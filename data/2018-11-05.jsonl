{"title": "Learning to Explicitate Connectives with Seq2Seq Network for Implicit\n  Discourse Relation Classification", "abstract": "Implicit discourse relation classification is one of the most difficult steps\nin discourse parsing. The difficulty stems from the fact that the coherence\nrelation must be inferred based on the content of the discourse relational\narguments. Therefore, an effective encoding of the relational arguments is of\ncrucial importance. We here propose a new model for implicit discourse relation\nclassification, which consists of a classifier, and a sequence-to-sequence\nmodel which is trained to generate a representation of the discourse relational\narguments by trying to predict the relational arguments including a suitable\nimplicit connective. Training is possible because such implicit connectives\nhave been annotated as part of the PDTB corpus. Along with a memory network,\nour model could generate more refined representations for the task. And on the\nnow standard 11-way classification, our method outperforms previous state of\nthe art systems on the PDTB benchmark on multiple settings including cross\nvalidation.", "published": "2018-11-05 14:09:53", "link": "http://arxiv.org/abs/1811.01697v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do RNNs learn human-like abstract word order preferences?", "abstract": "RNN language models have achieved state-of-the-art results on various tasks,\nbut what exactly they are representing about syntax is as yet unclear. Here we\ninvestigate whether RNN language models learn humanlike word order preferences\nin syntactic alternations. We collect language model surprisal scores for\ncontrolled sentence stimuli exhibiting major syntactic alternations in English:\nheavy NP shift, particle shift, the dative alternation, and the genitive\nalternation. We show that RNN language models reproduce human preferences in\nthese alternations based on NP length, animacy, and definiteness. We collect\nhuman acceptability ratings for our stimuli, in the first acceptability\njudgment experiment directly manipulating the predictors of syntactic\nalternations. We show that the RNNs' performance is similar to the human\nacceptability ratings and is not matched by an n-gram baseline model. Our\nresults show that RNNs learn the abstract features of weight, animacy, and\ndefiniteness which underlie soft constraints on syntactic alternations.", "published": "2018-11-05 17:32:14", "link": "http://arxiv.org/abs/1811.01866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compact Personalized Models for Neural Machine Translation", "abstract": "We propose and compare methods for gradient-based domain adaptation of\nself-attentive neural machine translation models. We demonstrate that a large\nproportion of model parameters can be frozen during adaptation with minimal or\nno reduction in translation quality by encouraging structured sparsity in the\nset of offset tensors during learning via group lasso regularization. We\nevaluate this technique for both batch and incremental adaptation across\nmultiple data sets and language pairs. Our system architecture - combining a\nstate-of-the-art self-attentive model with compact domain adaptation - provides\nhigh quality personalized machine translation that is both space and time\nefficient.", "published": "2018-11-05 19:29:56", "link": "http://arxiv.org/abs/1811.01990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Marchex 2018 English Conversational Telephone Speech Recognition\n  System", "abstract": "In this paper, we describe recent performance improvements to the production\nMarchex speech recognition system for our spontaneous customer-to-business\ntelephone conversations. In our previous work, we focused on in-domain language\nand acoustic model training. In this work we employ state-of-the-art\nsemi-supervised lattice-free maximum mutual information (LF-MMI) training\nprocess which can supervise over full lattices from unlabeled audio. On Marchex\nEnglish (ME), a modern evaluation set of conversational North American English,\nwe observed a 3.3% (3.2% for agent, 3.6% for caller) reduction in absolute word\nerror rate (WER) with 3x faster decoding speed over the performance of the 2017\nproduction system. We expect this improvement boost Marchex Call Analytics\nsystem performance especially for natural language processing pipeline.", "published": "2018-11-05 22:18:31", "link": "http://arxiv.org/abs/1811.02058v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Span-based Question Answering Systems with Coarsely Labeled\n  Data", "abstract": "We study approaches to improve fine-grained short answer Question Answering\nmodels by integrating coarse-grained data annotated for paragraph-level\nrelevance and show that coarsely annotated data can bring significant\nperformance gains. Experiments demonstrate that the standard multi-task\nlearning approach of sharing representations is not the most effective way to\nleverage coarse-grained annotations. Instead, we can explicitly model the\nlatent fine-grained short answer variables and optimize the marginal\nlog-likelihood directly or use a newly proposed \\emph{posterior distillation}\nlearning objective. Since these latent-variable methods have explicit access to\nthe relationship between the fine and coarse tasks, they result in\nsignificantly larger improvements from coarse supervision.", "published": "2018-11-05 23:03:02", "link": "http://arxiv.org/abs/1811.02076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Medical code prediction with multi-view convolution and\n  description-regularized label-dependent attention", "abstract": "A ubiquitous task in processing electronic medical data is the assignment of\nstandardized codes representing diagnoses and/or procedures to free-text\ndocuments such as medical reports. This is a difficult natural language\nprocessing task that requires parsing long, heterogeneous documents and\nselecting a set of appropriate codes from tens of thousands of\npossibilities---many of which have very few positive training samples. We\npresent a deep learning system that advances the state of the art for the\nMIMIC-III dataset, achieving a new best micro F1-measure of 55.85\\%,\nsignificantly outperforming the previous best result (Mullenbach et al. 2018).\nWe achieve this through a number of enhancements, including two major novel\ncontributions: multi-view convolutional channels, which effectively learn to\nadjust kernel sizes throughout the input; and attention regularization,\nmediated by natural-language code descriptions, which helps overcome sparsity\nfor thousands of uncommon codes. These and other modifications are selected to\naddress difficulties inherent to both automated coding specifically and deep\nlearning generally. Finally, we investigate our accuracy results in detail to\nindividually measure the impact of these contributions and point the way\ntowards future algorithmic improvements.", "published": "2018-11-05 00:54:03", "link": "http://arxiv.org/abs/1811.01468v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A human-editable Sign Language representation for software editing---and\n  a writing system?", "abstract": "To equip SL with software properly, we need an input system to represent and\nmanipulate signed contents in the same way that every day software allows to\nprocess written text. Refuting the claim that video is good enough a medium to\nserve the purpose, we propose to build a representation that is: editable,\nqueryable, synthesisable and user-friendly---we define those terms upfront. The\nissue being functionally and conceptually linked to that of writing, we study\nexisting writing systems, namely those in use for vocal languages, those\ndesigned and proposed for SLs, and more spontaneous ways in which SL users put\ntheir language in writing. Observing each paradigm in turn, we move on to\npropose a new approach to satisfy our goals of integration in software. We\nfinally open the prospect of our proposition being used outside of this\nrestricted scope, as a writing system in itself, and compare its properties to\nthe other writing systems presented.", "published": "2018-11-05 15:21:40", "link": "http://arxiv.org/abs/1811.01786v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A personal model of trumpery: Deception detection in a real-world\n  high-stakes setting", "abstract": "Language use reveals information about who we are and how we feel1-3. One of\nthe pioneers in text analysis, Walter Weintraub, manually counted which types\nof words people used in medical interviews and showed that the frequency of\nfirst-person singular pronouns (i.e., I, me, my) was a reliable indicator of\ndepression, with depressed people using I more often than people who are not\ndepressed4. Several studies have demonstrated that language use also differs\nbetween truthful and deceptive statements5-7, but not all differences are\nconsistent across people and contexts, making prediction difficult8. Here we\nshow how well linguistic deception detection performs at the individual level\nby developing a model tailored to a single individual: the current US\npresident. Using tweets fact-checked by an independent third party (Washington\nPost), we found substantial linguistic differences between factually correct\nand incorrect tweets and developed a quantitative model based on these\ndifferences. Next, we predicted whether out-of-sample tweets were either\nfactually correct or incorrect and achieved a 73% overall accuracy. Our results\ndemonstrate the power of linguistic analysis in real-world deception research\nwhen applied at the individual level and provide evidence that factually\nincorrect tweets are not random mistakes of the sender.", "published": "2018-11-05 10:46:45", "link": "http://arxiv.org/abs/1811.01938v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Superregular grammars do not provide additional explanatory power but\n  allow for a compact analysis of animal song", "abstract": "A pervasive belief with regard to the differences between human language and\nanimal vocal sequences (song) is that they belong to different classes of\ncomputational complexity, with animal song belonging to regular languages,\nwhereas human language is superregular. This argument, however, lacks empirical\nevidence since superregular analyses of animal song are understudied. The goal\nof this paper is to perform a superregular analysis of animal song, using data\nfrom gibbons as a case study, and demonstrate that a superregular analysis can\nbe effectively used with non-human data. A key finding is that a superregular\nanalysis does not increase explanatory power but rather provides for compact\nanalysis: Fewer grammatical rules are necessary once superregularity is\nallowed. This pattern is analogous to a previous computational analysis of\nhuman language, and accordingly, the null hypothesis, that human language and\nanimal song are governed by the same type of grammatical systems, cannot be\nrejected.", "published": "2018-11-05 05:07:37", "link": "http://arxiv.org/abs/1811.02507v2", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the\n  Winograd Schema Challenge and SWAG", "abstract": "Recent studies have significantly improved the state-of-the-art on\ncommon-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge\n(WSC) and SWAG. The question we ask in this paper is whether improved\nperformance on these benchmarks represents genuine progress towards\ncommon-sense-enabled systems. We make case studies of both benchmarks and\ndesign protocols that clarify and qualify the results of previous work by\nanalyzing threats to the validity of previous experimental designs. Our\nprotocols account for several properties prevalent in common-sense benchmarks\nincluding size limitations, structural regularities, and variable instance\ndifficulty.", "published": "2018-11-05 15:11:24", "link": "http://arxiv.org/abs/1811.01778v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Structured Neural Summarization", "abstract": "Summarization of long sequences into a concise statement is a core problem in\nnatural language processing, requiring non-trivial understanding of the input.\nBased on the promising results of graph neural networks on highly structured\ndata, we develop a framework to extend existing sequence encoders with a graph\ncomponent that can reason about long-distance relationships in weakly\nstructured data such as text. In an extensive evaluation, we show that the\nresulting hybrid sequence-graph models outperform both pure sequence models as\nwell as pure graph models on a range of summarization tasks.", "published": "2018-11-05 16:12:04", "link": "http://arxiv.org/abs/1811.01824v4", "categories": ["cs.LG", "cs.CL", "cs.SE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Evolutionary Data Measures: Understanding the Difficulty of Text\n  Classification Tasks", "abstract": "Classification tasks are usually analysed and improved through new model\narchitectures or hyperparameter optimisation but the underlying properties of\ndatasets are discovered on an ad-hoc basis as errors occur. However,\nunderstanding the properties of the data is crucial in perfecting models. In\nthis paper we analyse exactly which characteristics of a dataset best determine\nhow difficult that dataset is for the task of text classification. We then\npropose an intuitive measure of difficulty for text classification datasets\nwhich is simple and fast to calculate. We show that this measure generalises to\nunseen data by comparing it to state-of-the-art datasets and results. This\nmeasure can be used to analyse the precise source of errors in a dataset and\nallows fast estimation of how difficult a dataset is to learn. We searched for\nthis measure by training 12 classical and neural network based models on 78\nreal-world datasets, then use a genetic algorithm to discover the best measure\nof difficulty. Our difficulty-calculating code ( https://github.com/Wluper/edm\n) and datasets ( http://data.wluper.com ) are publicly available.", "published": "2018-11-05 18:39:54", "link": "http://arxiv.org/abs/1811.01910v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Leveraging Weakly Supervised Data to Improve End-to-End Speech-to-Text\n  Translation", "abstract": "End-to-end Speech Translation (ST) models have many potential advantages when\ncompared to the cascade of Automatic Speech Recognition (ASR) and text Machine\nTranslation (MT) models, including lowered inference latency and the avoidance\nof error compounding. However, the quality of end-to-end ST is often limited by\na paucity of training data, since it is difficult to collect large parallel\ncorpora of speech and translated transcript pairs. Previous studies have\nproposed the use of pre-trained components and multi-task learning in order to\nbenefit from weakly supervised training data, such as speech-to-transcript or\ntext-to-foreign-text pairs. In this paper, we demonstrate that using\npre-trained MT or text-to-speech (TTS) synthesis models to convert weakly\nsupervised data into speech-to-translation pairs for ST training can be more\neffective than multi-task learning. Furthermore, we demonstrate that a high\nquality end-to-end ST model can be trained using only weakly supervised\ndatasets, and that synthetic data sourced from unlabeled monolingual text or\nspeech can be used to improve performance. Finally, we discuss methods for\navoiding overfitting to synthetic speech with a quantitative ablation study.", "published": "2018-11-05 21:57:09", "link": "http://arxiv.org/abs/1811.02050v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "End-to-End Monaural Multi-speaker ASR System without Pretraining", "abstract": "Recently, end-to-end models have become a popular approach as an alternative\nto traditional hybrid models in automatic speech recognition (ASR). The\nmulti-speaker speech separation and recognition task is a central task in\ncocktail party problem. In this paper, we present a state-of-the-art monaural\nmulti-speaker end-to-end automatic speech recognition model. In contrast to\nprevious studies on the monaural multi-speaker speech recognition, this\nend-to-end framework is trained to recognize multiple label sequences\ncompletely from scratch. The system only requires the speech mixture and\ncorresponding label sequences, without needing any indeterminate supervisions\nobtained from non-mixture speech or corresponding labels/alignments. Moreover,\nwe exploited using the individual attention module for each separated speaker\nand the scheduled sampling to further improve the performance. Finally, we\nevaluate the proposed model on the 2-speaker mixed speech generated from the\nWSJ corpus and the wsj0-2mix dataset, which is a speech separation and\nrecognition benchmark. The experiments demonstrate that the proposed methods\ncan improve the performance of the end-to-end model in separating the\noverlapping speech and recognizing the separated streams. From the results, the\nproposed model leads to ~10.0% relative performance gains in terms of CER and\nWER respectively.", "published": "2018-11-05 22:21:51", "link": "http://arxiv.org/abs/1811.02062v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "How to Improve Your Speaker Embeddings Extractor in Generic Toolkits", "abstract": "Recently, speaker embeddings extracted with deep neural networks became the\nstate-of-the-art method for speaker verification. In this paper we aim to\nfacilitate its implementation on a more generic toolkit than Kaldi, which we\nanticipate to enable further improvements on the method. We examine several\ntricks in training, such as the effects of normalizing input features and\npooled statistics, different methods for preventing overfitting as well as\nalternative non-linearities that can be used instead of Rectifier Linear Units.\nIn addition, we investigate the difference in performance between TDNN and CNN,\nand between two types of attention mechanism. Experimental results on Speaker\nin the Wild, SRE 2016 and SRE 2018 datasets demonstrate the effectiveness of\nthe proposed implementation.", "published": "2018-11-05 22:31:00", "link": "http://arxiv.org/abs/1811.02066v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Manner of Articulation Detection using Connectionist Temporal\n  Classification to Improve Automatic Speech Recognition Performance", "abstract": "Conventionally, the manner of articulations in speech signal are derived\nusing discriminative signal processing techniques or deep learning approaches.\nHowever, training such complex systems involves feature extraction, phoneme\nforce alignment and deep neural network training. In our work, we initially\ndetect the manner of articulations without phoneme alignment using an\nend-to-end manner of articulation modeling based on connectionist temporal\nclassification (CTC). The manner of articulation knowledge is deployed in the\nconventional character CTC path to regenerate the new character CTC path. The\nmodified manner based character CTC is evaluated on open source speech datasets\nsuch as AN4, LibriSpeech and TEDLIUM-2 and it outperforms over the baseline\ncharacter CTC.", "published": "2018-11-05 12:43:05", "link": "http://arxiv.org/abs/1811.01644v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Deep Clustering for Source Separation: Direct Learning from\n  Mixtures using Spatial Information", "abstract": "We present a monophonic source separation system that is trained by only\nobserving mixtures with no ground truth separation information. We use a deep\nclustering approach which trains on multi-channel mixtures and learns to\nproject spectrogram bins to source clusters that correlate with various spatial\nfeatures. We show that using such a training process we can obtain separation\nperformance that is as good as making use of ground truth separation\ninformation. Once trained, this system is capable of performing sound\nseparation on monophonic inputs, despite having learned how to do so using\nmulti-channel recordings.", "published": "2018-11-05 07:00:12", "link": "http://arxiv.org/abs/1811.01531v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ConvS2S-VC: Fully convolutional sequence-to-sequence voice conversion", "abstract": "This paper proposes a voice conversion (VC) method using sequence-to-sequence\n(seq2seq or S2S) learning, which flexibly converts not only the voice\ncharacteristics but also the pitch contour and duration of input speech. The\nproposed method, called ConvS2S-VC, has three key features. First, it uses a\nmodel with a fully convolutional architecture. This is particularly\nadvantageous in that it is suitable for parallel computations using GPUs. It is\nalso beneficial since it enables effective normalization techniques such as\nbatch normalization to be used for all the hidden layers in the networks.\nSecond, it achieves many-to-many conversion by simultaneously learning mappings\namong multiple speakers using only a single model instead of separately\nlearning mappings between each speaker pair using a different model. This\nenables the model to fully utilize available training data collected from\nmultiple speakers by capturing common latent features that can be shared across\ndifferent speakers. Owing to this structure, our model works reasonably well\neven without source speaker information, thus making it able to handle\nany-to-many conversion tasks. Third, we introduce a mechanism, called the\nconditional batch normalization that switches batch normalization layers in\naccordance with the target speaker. This particular mechanism has been found to\nbe extremely effective for our many-to-many conversion model. We conducted\nspeaker identity conversion experiments and found that ConvS2S-VC obtained\nhigher sound quality and speaker similarity than baseline methods. We also\nfound from audio examples that it could perform well in various tasks including\nemotional expression conversion, electrolaryngeal speech enhancement, and\nEnglish accent conversion.", "published": "2018-11-05 11:02:29", "link": "http://arxiv.org/abs/1811.01609v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "End-to-End Sound Source Separation Conditioned On Instrument Labels", "abstract": "Can we perform an end-to-end music source separation with a variable number\nof sources using a deep learning model? We present an extension of the\nWave-U-Net model which allows end-to-end monaural source separation with a\nnon-fixed number of sources. Furthermore, we propose multiplicative\nconditioning with instrument labels at the bottleneck of the Wave-U-Net and\nshow its effect on the separation results. This approach leads to other types\nof conditioning such as audio-visual source separation and score-informed\nsource separation.", "published": "2018-11-05 17:12:54", "link": "http://arxiv.org/abs/1811.01850v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "When CTC Training Meets Acoustic Landmarks", "abstract": "Connectionist temporal classification (CTC) provides an end-to-end acoustic\nmodel (AM) training strategy. CTC learns accurate AMs without time-aligned\nphonetic transcription, but sometimes fails to converge, especially in\nresource-constrained scenarios. In this paper, the convergence properties of\nCTC are improved by incorporating acoustic landmarks. We tailored a new set of\nacoustic landmarks to help CTC training converge more rapidly and smoothly\nwhile also reducing recognition error rates. We leveraged new target label\nsequences mixed with both phone and manner changes to guide CTC training.\nExperiments on TIMIT demonstrated that CTC based acoustic models converge\nsignificantly faster and smoother when they are augmented by acoustic\nlandmarks. The models pretrained with mixed target labels can be further\nfinetuned, resulting in phone error rates 8.72% below baseline on TIMIT.\nConsistent performance gain is also observed on WSJ (a larger corpus) and\nreduced TIMIT (smaller). With WSJ, we are the first to succeed in verifying the\neffectiveness of acoustic landmark theory on a mid-sized ASR task.", "published": "2018-11-05 22:22:24", "link": "http://arxiv.org/abs/1811.02063v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Trainable Adaptive Window Switching for Speech Enhancement", "abstract": "This study proposes a trainable adaptive window switching (AWS) method and\napply it to a deep-neural-network (DNN) for speech enhancement in the modified\ndiscrete cosine transform domain. Time-frequency (T-F) mask processing in the\nshort-time Fourier transform (STFT)-domain is a typical speech enhancement\nmethod. To recover the target signal precisely, DNN-based short-time frequency\ntransforms have recently been investigated and used instead of the STFT.\nHowever, since such a fixed-resolution short-time frequency transform method\nhas a T-F resolution problem based on the uncertainty principle, not only the\nshort-time frequency transform but also the length of the windowing function\nshould be optimized. To overcome this problem, we incorporate AWS into the\nspeech enhancement procedure, and the windowing function of each time-frame is\nmanipulated using a DNN depending on the input signal. We confirmed that the\nproposed method achieved a higher signal-to-distortion ratio than conventional\nspeech enhancement methods in fixed-resolution frequency domains.", "published": "2018-11-05 12:25:42", "link": "http://arxiv.org/abs/1811.02438v4", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
