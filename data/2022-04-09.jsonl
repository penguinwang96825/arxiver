{"title": "Towards Better Chinese-centric Neural Machine Translation for\n  Low-resource Languages", "abstract": "The last decade has witnessed enormous improvements in science and\ntechnology, stimulating the growing demand for economic and cultural exchanges\nin various countries. Building a neural machine translation (NMT) system has\nbecome an urgent trend, especially in the low-resource setting. However, recent\nwork tends to study NMT systems for low-resource languages centered on English,\nwhile few works focus on low-resource NMT systems centered on other languages\nsuch as Chinese. To achieve this, the low-resource multilingual translation\nchallenge of the 2021 iFLYTEK AI Developer Competition provides the\nChinese-centric multilingual low-resource NMT tasks, where participants are\nrequired to build NMT systems based on the provided low-resource samples. In\nthis paper, we present the winner competition system that leverages monolingual\nword embeddings data enhancement, bilingual curriculum learning, and\ncontrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss\nfunction is proposed to replace the traditional cross-entropy loss when\ntraining. The experimental results demonstrate that the implementation of these\nideas leads better performance than other state-of-the-art methods. All the\nexperimental codes are released at:\nhttps://github.com/WENGSYX/Low-resource-text-translation.", "published": "2022-04-09 01:05:37", "link": "http://arxiv.org/abs/2204.04344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable\n  Fine-tuning for Zero-Shot Dialogue Summarization", "abstract": "The most advanced abstractive dialogue summarizers lack generalization\nability on new domains and the existing researches for domain adaptation in\nsummarization generally rely on large-scale pre-trainings. To explore the\nlightweight fine-tuning methods for domain adaptation of dialogue\nsummarization, in this paper, we propose an efficient and generalizable\nDomain-Oriented Prefix-tuning model, which utilizes a domain word initialized\nprefix module to alleviate domain entanglement and adopts discrete prompts to\nguide the model to focus on key contents of dialogues and enhance model\ngeneralization. We conduct zero-shot experiments and build domain adaptation\nbenchmarks on two multi-domain dialogue summarization datasets, TODSum and\nQMSum. Adequate experiments and qualitative analysis prove the effectiveness of\nour methods.", "published": "2022-04-09 02:28:22", "link": "http://arxiv.org/abs/2204.04362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Multi-Granularity Hierarchical Features for Relation Extraction", "abstract": "Relation extraction is a key task in Natural Language Processing (NLP), which\naims to extract relations between entity pairs from given texts. Recently,\nrelation extraction (RE) has achieved remarkable progress with the development\nof deep neural networks. Most existing research focuses on constructing\nexplicit structured features using external knowledge such as knowledge graph\nand dependency tree. In this paper, we propose a novel method to extract\nmulti-granularity features based solely on the original input sentences. We\nshow that effective structured features can be attained even without external\nknowledge. Three kinds of features based on the input sentences are fully\nexploited, which are in entity mention level, segment level, and sentence\nlevel. All the three are jointly and hierarchically modeled. We evaluate our\nmethod on three public benchmarks: SemEval 2010 Task 8, Tacred, and Tacred\nRevisited. To verify the effectiveness, we apply our method to different\nencoders such as LSTM and BERT. Experimental results show that our method\nsignificantly outperforms existing state-of-the-art models that even use\nexternal knowledge. Extensive analyses demonstrate that the performance of our\nmodel is contributed by the capture of multi-granularity features and the model\nof their hierarchical structure. Code and data are available at\n\\url{https://github.com/xnliang98/sms}.", "published": "2022-04-09 09:44:05", "link": "http://arxiv.org/abs/2204.04437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding, Detecting, and Separating Out-of-Distribution Samples and\n  Adversarial Samples in Text Classification", "abstract": "In this paper, we study the differences and commonalities between\nstatistically out-of-distribution (OOD) samples and adversarial (Adv) samples,\nboth of which hurting a text classification model's performance. We conduct\nanalyses to compare the two types of anomalies (OOD and Adv samples) with the\nin-distribution (ID) ones from three aspects: the input features, the hidden\nrepresentations in each layer of the model, and the output probability\ndistributions of the classifier. We find that OOD samples expose their\naberration starting from the first layer, while the abnormalities of Adv\nsamples do not emerge until the deeper layers of the model. We also illustrate\nthat the models' output probabilities for Adv samples tend to be more\nunconfident. Based on our observations, we propose a simple method to separate\nID, OOD, and Adv samples using the hidden representations and output\nprobabilities of the model. On multiple combinations of ID, OOD datasets, and\nAdv attacks, our proposed method shows exceptional results on distinguishing\nID, OOD, and Adv samples.", "published": "2022-04-09 12:11:59", "link": "http://arxiv.org/abs/2204.04458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KUCST@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media\n  Text", "abstract": "In this paper we present our approach for detecting signs of depression from\nsocial media text. Our model relies on word unigrams, part-of-speech tags,\nreadabilitiy measures and the use of first, second or third person and the\nnumber of words. Our best model obtained a macro F1-score of 0.439 and ranked\n25th, out of 31 teams. We further take advantage of the interpretability of the\nLogistic Regression model and we make an attempt to interpret the model\ncoefficients with the hope that these will be useful for further research on\nthe topic.", "published": "2022-04-09 14:27:13", "link": "http://arxiv.org/abs/2204.04481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TANet: Thread-Aware Pretraining for Abstractive Conversational\n  Summarization", "abstract": "Although pre-trained language models (PLMs) have achieved great success and\nbecome a milestone in NLP, abstractive conversational summarization remains a\nchallenging but less studied task. The difficulty lies in two aspects. One is\nthe lack of large-scale conversational summary data. Another is that applying\nthe existing pre-trained models to this task is tricky because of the\nstructural dependence within the conversation and its informal expression, etc.\nIn this work, we first build a large-scale (11M) pretraining dataset called\nRCS, based on the multi-person discussions in the Reddit community. We then\npresent TANet, a thread-aware Transformer-based network. Unlike the existing\npre-trained models that treat a conversation as a sequence of sentences, we\nargue that the inherent contextual dependency among the utterances plays an\nessential role in understanding the entire conversation and thus propose two\nnew techniques to incorporate the structural information into our model. The\nfirst is thread-aware attention which is computed by taking into account the\ncontextual dependency within utterances. Second, we apply thread prediction\nloss to predict the relations between utterances. We evaluate our model on four\ndatasets of real conversations, covering types of meeting transcripts,\ncustomer-service records, and forum threads. Experimental results demonstrate\nthat TANET achieves a new state-of-the-art in terms of both automatic\nevaluation and human judgment.", "published": "2022-04-09 16:08:46", "link": "http://arxiv.org/abs/2204.04504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking for Public Health Surveillance tasks on Social Media with a\n  Domain-Specific Pretrained Language Model", "abstract": "A user-generated text on social media enables health workers to keep track of\ninformation, identify possible outbreaks, forecast disease trends, monitor\nemergency cases, and ascertain disease awareness and response to official\nhealth correspondence. This exchange of health information on social media has\nbeen regarded as an attempt to enhance public health surveillance (PHS).\nDespite its potential, the technology is still in its early stages and is not\nready for widespread application. Advancements in pretrained language models\n(PLMs) have facilitated the development of several domain-specific PLMs and a\nvariety of downstream applications. However, there are no PLMs for social media\ntasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,\nto identify tasks related to public health surveillance on social media. We\ncompared and benchmarked the performance of PHS-BERT on 25 datasets from\ndifferent social medial platforms related to 7 different PHS tasks. Compared\nwith existing PLMs that are mainly evaluated on limited tasks, PHS-BERT\nachieved state-of-the-art performance on all 25 tested datasets, showing that\nour PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT\navailable, we aim to facilitate the community to reduce the computational cost\nand introduce new baselines for future works across various PHS-related tasks.", "published": "2022-04-09 18:01:18", "link": "http://arxiv.org/abs/2204.04521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KOBEST: Korean Balanced Evaluation of Significant Tasks", "abstract": "A well-formulated benchmark plays a critical role in spurring advancements in\nthe natural language processing (NLP) field, as it allows objective and precise\nevaluation of diverse models. As modern language models (LMs) have become more\nelaborate and sophisticated, more difficult benchmarks that require linguistic\nknowledge and reasoning have been proposed. However, most of these benchmarks\nonly support English, and great effort is necessary to construct benchmarks for\nother low resource languages. To this end, we propose a new benchmark named\nKorean balanced evaluation of significant tasks (KoBEST), which consists of\nfive Korean-language downstream tasks. Professional Korean linguists designed\nthe tasks that require advanced Korean linguistic knowledge. Moreover, our data\nis purely annotated by humans and thoroughly reviewed to guarantee high data\nquality. We also provide baseline models and human performance results. Our\ndataset is available on the Huggingface.", "published": "2022-04-09 20:13:51", "link": "http://arxiv.org/abs/2204.04541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Importance of Karaka Framework in Multi-modal Grounding", "abstract": "Computational Paninian Grammar model helps in decoding a natural language\nexpression as a series of modifier-modified relations and therefore facilitates\nin identifying dependency relations closer to language (context) semantics\ncompared to the usual Stanford dependency relations. However, the importance of\nthis CPG dependency scheme has not been studied in the context of multi-modal\nvision and language applications. At IIIT Hyderabad, we plan to perform a novel\nstudy to explore the potential advantages and disadvantages of CPG framework in\na vision-language navigation task setting, a popular and challenging\nmulti-modal grounding task.", "published": "2022-04-09 01:33:18", "link": "http://arxiv.org/abs/2204.04347v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MINER: Improving Out-of-Vocabulary Named Entity Recognition from an\n  Information Theoretic Perspective", "abstract": "NER model has achieved promising performance on standard NER benchmarks.\nHowever, recent studies show that previous approaches may over-rely on entity\nmention information, resulting in poor performance on out-of-vocabulary (OOV)\nentity recognition. In this work, we propose MINER, a novel NER learning\nframework, to remedy this issue from an information-theoretic perspective. The\nproposed approach contains two mutual information-based training objectives: i)\ngeneralizing information maximization, which enhances representation via deep\nunderstanding of context and entity surface forms; ii) superfluous information\nminimization, which discourages representation from rote memorizing entity\nnames or exploiting biased cues in data. Experiments on various settings and\ndatasets demonstrate that it achieves better performance in predicting OOV\nentities.", "published": "2022-04-09 05:18:20", "link": "http://arxiv.org/abs/2204.04391v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Denoising Neural Network for News Recommendation with Positive and\n  Negative Implicit Feedback", "abstract": "News recommendation is different from movie or e-commercial recommendation as\npeople usually do not grade the news. Therefore, user feedback for news is\nalways implicit (click behavior, reading time, etc). Inevitably, there are\nnoises in implicit feedback. On one hand, the user may exit immediately after\nclicking the news as he dislikes the news content, leaving the noise in his\npositive implicit feedback; on the other hand, the user may be recommended\nmultiple interesting news at the same time and only click one of them,\nproducing the noise in his negative implicit feedback. Opposite implicit\nfeedback could construct more integrated user preferences and help each other\nto minimize the noise influence. Previous works on news recommendation only\nused positive implicit feedback and suffered from the noise impact. In this\npaper, we propose a denoising neural network for news recommendation with\npositive and negative implicit feedback, named DRPN. DRPN utilizes both\nfeedback for recommendation with a module to denoise both positive and negative\nimplicit feedback to further enhance the performance. Experiments on the\nreal-world large-scale dataset demonstrate the state-of-the-art performance of\nDRPN.", "published": "2022-04-09 05:47:17", "link": "http://arxiv.org/abs/2204.04397v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization", "abstract": "Few-shot abstractive summarization has become a challenging task in natural\nlanguage generation. To support it, we designed a novel soft prompts\narchitecture coupled with a prompt pre-training plus fine-tuning paradigm that\nis effective and tunes only extremely light parameters. The soft prompts\ninclude continuous input embeddings across an encoder and a decoder to fit the\nstructure of the generation models. Importantly, a novel inner-prompt placed in\nthe text is introduced to capture document-level information. The aim is to\ndevote attention to understanding the document that better prompts the model to\ngenerate document-related content. The first step in the summarization\nprocedure is to conduct prompt pre-training with self-supervised pseudo-data.\nThis teaches the model basic summarizing capabilities. The model is then\nfine-tuned with few-shot examples. Experimental results on the CNN/DailyMail\nand XSum datasets show that our method, with only 0.1% of the parameters,\noutperforms full-model tuning where all model parameters are tuned. It also\nsurpasses Prompt Tuning by a large margin and delivers competitive results\nagainst Prefix-Tuning with 3% of the parameters.", "published": "2022-04-09 07:40:52", "link": "http://arxiv.org/abs/2204.04413v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FoundationLayerNorm: Scaling BERT and GPT to 1,000 Layers", "abstract": "The mainstream BERT/GPT model contains only 10 to 20 layers, and there is\nlittle literature to discuss the training of deep BERT/GPT. This paper proposes\na simple yet effective method to stabilize BERT and GPT training. We\nsuccessfully scale up BERT and GPT to 1,000 layers, which is an order of\nmagnitude deeper than previous BERT and GPT. The proposed method\nFoundationLayerNormalization enables efficient training of deep neural networks\nand is validated at the 1000-layer scale.", "published": "2022-04-09 14:03:28", "link": "http://arxiv.org/abs/2204.04477v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Informativeness and Invariance: Two Perspectives on Spurious\n  Correlations in Natural Language", "abstract": "Spurious correlations are a threat to the trustworthiness of natural language\nprocessing systems, motivating research into methods for identifying and\neliminating them. However, addressing the problem of spurious correlations\nrequires more clarity on what they are and how they arise in language data.\nGardner et al (2021) argue that the compositional nature of language implies\nthat \\emph{all} correlations between labels and individual \"input features\" are\nspurious. This paper analyzes this proposal in the context of a toy example,\ndemonstrating three distinct conditions that can give rise to feature-label\ncorrelations in a simple PCFG. Linking the toy example to a structured causal\nmodel shows that (1) feature-label correlations can arise even when the label\nis invariant to interventions on the feature, and (2) feature-label\ncorrelations may be absent even when the label is sensitive to interventions on\nthe feature. Because input features will be individually correlated with labels\nin all but very rare circumstances, domain knowledge must be applied to\nidentify spurious correlations that pose genuine robustness threats.", "published": "2022-04-09 14:46:39", "link": "http://arxiv.org/abs/2204.04487v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IDPG: An Instance-Dependent Prompt Generation Method", "abstract": "Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a\ntask-specific prompt in each input instance during the model training stage. It\nfreezes the pre-trained language model and only optimizes a few task-specific\nprompts. In this paper, we propose a conditional prompt generation method to\ngenerate prompts for each input instance, referred to as the Instance-Dependent\nPrompt Generation (IDPG). Unlike traditional prompt tuning methods that use a\nfixed prompt, IDPG introduces a lightweight and trainable component to generate\nprompts based on each input sentence. Extensive experiments on ten natural\nlanguage understanding (NLU) tasks show that the proposed strategy consistently\noutperforms various prompt tuning baselines and is on par with other efficient\ntransfer learning methods such as Compacter while tuning far fewer model\nparameters.", "published": "2022-04-09 15:45:27", "link": "http://arxiv.org/abs/2204.04497v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extending the Scope of Out-of-Domain: Examining QA models in multiple\n  subdomains", "abstract": "Past works that investigate out-of-domain performance of QA systems have\nmainly focused on general domains (e.g. news domain, wikipedia domain),\nunderestimating the importance of subdomains defined by the internal\ncharacteristics of QA datasets. In this paper, we extend the scope of\n\"out-of-domain\" by splitting QA examples into different subdomains according to\ntheir several internal characteristics including question type, text length,\nanswer position. We then examine the performance of QA systems trained on the\ndata from different subdomains. Experimental results show that the performance\nof QA systems can be significantly reduced when the train data and test data\ncome from different subdomains. These results question the generalizability of\ncurrent QA systems in multiple subdomains, suggesting the need to combat the\nbias introduced by the internal characteristics of QA datasets.", "published": "2022-04-09 19:40:13", "link": "http://arxiv.org/abs/2204.04534v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Should we tweet this? Generative response modeling for predicting\n  reception of public health messaging on Twitter", "abstract": "The way people respond to messaging from public health organizations on\nsocial media can provide insight into public perceptions on critical health\nissues, especially during a global crisis such as COVID-19. It could be\nvaluable for high-impact organizations such as the US Centers for Disease\nControl and Prevention (CDC) or the World Health Organization (WHO) to\nunderstand how these perceptions impact reception of messaging on health policy\nrecommendations. We collect two datasets of public health messages and their\nresponses from Twitter relating to COVID-19 and Vaccines, and introduce a\npredictive method which can be used to explore the potential reception of such\nmessages. Specifically, we harness a generative model (GPT-2) to directly\npredict probable future responses and demonstrate how it can be used to\noptimize expected reception of important health guidance. Finally, we introduce\na novel evaluation scheme with extensive statistical testing which allows us to\nconclude that our models capture the semantics and sentiment found in actual\npublic health responses.", "published": "2022-04-09 01:56:46", "link": "http://arxiv.org/abs/2204.04353v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Contrastive Demonstration Tuning for Pre-trained Language Models", "abstract": "Pretrained language models can be effectively stimulated by textual prompts\nor demonstrations, especially in low-data scenarios. Recent works have focused\non automatically searching discrete or continuous prompts or optimized\nverbalizers, yet studies for the demonstration are still limited. Concretely,\nthe demonstration examples are crucial for an excellent final performance of\nprompt-tuning. In this paper, we propose a novel pluggable, extensible, and\nefficient approach named contrastive demonstration tuning, which is free of\ndemonstration sampling. Furthermore, the proposed approach can be: (i) Plugged\ninto any previous prompt-tuning approaches; (ii) Extended to widespread\nclassification tasks with a large number of categories. Experimental results on\n16 datasets illustrate that our method integrated with previous approaches\nLM-BFF and P-tuning can yield better performance. Code is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.", "published": "2022-04-09 05:30:48", "link": "http://arxiv.org/abs/2204.04392v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Extraction of Pathologies from C-Spine Radiology Reports using\n  Multi-Task Learning", "abstract": "Pretrained Transformer based models finetuned on domain specific corpora have\nchanged the landscape of NLP. Generally, if one has multiple tasks on a given\ndataset, one may finetune different models or use task specific adapters. In\nthis work, we show that a multi-task model can beat or achieve the performance\nof multiple BERT-based models finetuned on various tasks and various task\nspecific adapter augmented BERT-based models. We validate our method on our\ninternal radiologist's report dataset on cervical spine. We hypothesize that\nthe tasks are semantically close and related and thus multitask learners are\npowerful classifiers. Our work opens the scope of using our method to\nradiologist's reports on various body parts.", "published": "2022-04-09 20:29:48", "link": "http://arxiv.org/abs/2204.04544v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multichannel Speech Separation with Narrow-band Conformer", "abstract": "This work proposes a multichannel speech separation method with narrow-band\nConformer (named NBC). The network is trained to learn to automatically exploit\nnarrow-band speech separation information, such as spatial vector clustering of\nmultiple speakers. Specifically, in the short-time Fourier transform (STFT)\ndomain, the network processes each frequency independently, and is shared by\nall frequencies. For one frequency, the network inputs the STFT coefficients of\nmultichannel mixture signals, and predicts the STFT coefficients of separated\nspeech signals. Clustering of spatial vectors shares a similar principle with\nthe self-attention mechanism in the sense of computing the similarity of\nvectors and then aggregating similar vectors. Therefore, Conformer would be\nespecially suitable for the present problem. Experiments show that the proposed\nnarrow-band Conformer achieves better speech separation performance than other\nstate-of-the-art methods by a large margin.", "published": "2022-04-09 13:01:33", "link": "http://arxiv.org/abs/2204.04464v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study of Using Cepstrogram for Countermeasure Against Replay Attacks", "abstract": "This study investigated the cepstrogram properties and demonstrated their\neffectiveness as powerful countermeasures against replay attacks. A cepstrum\nanalysis of replay attacks suggests that crucial information for anti-spoofing\nagainst replay attacks may be retained in the cepstrogram. When building\ncountermeasures against replay attacks, experiments on the ASVspoof 2019\nphysical access database demonstrate that the cepstrogram is more effective\nthan other features in both single and fusion systems. Our LCNN-based single\nand fusion systems with the cepstrogram feature outperformed the corresponding\nLCNN-based systems without the cepstrogram feature and several state-of-the-art\nsingle and fusion systems in the literature.", "published": "2022-04-09 00:18:53", "link": "http://arxiv.org/abs/2204.04333v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "QuiKo: A Quantum Beat Generation Application", "abstract": "In this chapter a quantum music generation application called QuiKo will be\ndiscussed. It combines existing quantum algorithms with data encoding methods\nfrom quantum machine learning to build drum and audio sample patterns from a\ndatabase of audio tracks. QuiKo leverages the physical properties and\ncharacteristics of quantum computers to generate what can be referred to as\nSoft Rules proposed by Alexis Kirke. These rules take advantage of the noise\nproduced by quantum devices to develop flexible rules and grammars for quantum\nmusic generation. These properties include qubit decoherence and phase kickback\ndue controlled quantum gates within the quantum circuit. QuiKo builds upon the\nconcept of soft rules in quantum music generation and takes it a step further.\nIt attempts to mimic and react to an external musical inputs, similar to the\nway that human musicians play and compose with one another. Audio signals are\nused as inputs into the system. Feature extraction is then performed on the\nsignal to identify the harmonic and percussive elements. This information is\nthen encoded onto the quantum circuit. Measurements of the quantum circuit are\nthen taken providing results in the form of probability distributions for\nexternal music applications to use to build the new drum patterns.", "published": "2022-04-09 03:01:19", "link": "http://arxiv.org/abs/2204.04370v2", "categories": ["eess.AS", "cs.ET", "cs.HC", "cs.SD", "quant-ph"], "primary_category": "eess.AS"}
