{"title": "Improving the Explainability of Neural Sentiment Classifiers via Data\n  Augmentation", "abstract": "Sentiment analysis has been widely used by businesses for social media\nopinion mining, especially in the financial services industry, where customers'\nfeedbacks are critical for companies. Recent progress of neural network models\nhas achieved remarkable performance on sentiment classification, while the lack\nof classification interpretation may raise the trustworthy and many other\nissues in practice. In this work, we study the problem of improving the\nexplainability of existing sentiment classifiers. We propose two data\naugmentation methods that create additional training examples to help improve\nmodel explainability: one method with a predefined sentiment word list as\nexternal knowledge and the other with adversarial examples. We test the\nproposed methods on both CNN and RNN classifiers with three benchmark sentiment\ndatasets. The model explainability is assessed by both human evaluators and a\nsimple automatic evaluation measurement. Experiments show the proposed data\naugmentation methods significantly improve the explainability of both neural\nclassifiers.", "published": "2019-09-10 01:30:23", "link": "http://arxiv.org/abs/1909.04225v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Extraction of Entities and Relations Based on a Novel\n  Decomposition Strategy", "abstract": "Joint extraction of entities and relations aims to detect entity pairs along\nwith their relations using a single model. Prior work typically solves this\ntask in the extract-then-classify or unified labeling manner. However, these\nmethods either suffer from the redundant entity pairs, or ignore the important\ninner structure in the process of extracting entities and relations. To address\nthese limitations, in this paper, we first decompose the joint extraction task\ninto two interrelated subtasks, namely HE extraction and TER extraction. The\nformer subtask is to distinguish all head-entities that may be involved with\ntarget relations, and the latter is to identify corresponding tail-entities and\nrelations for each extracted head-entity. Next, these two subtasks are further\ndeconstructed into several sequence labeling problems based on our proposed\nspan-based tagging scheme, which are conveniently solved by a hierarchical\nboundary tagger and a multi-span decoding algorithm. Owing to the reasonable\ndecomposition strategy, our model can fully capture the semantic\ninterdependency between different steps, as well as reduce noise from\nirrelevant entity pairs. Experimental results show that our method outperforms\nprevious work by 5.2%, 5.9% and 21.5% (F1 score), achieving a new\nstate-of-the-art on three public datasets", "published": "2019-09-10 04:08:10", "link": "http://arxiv.org/abs/1909.04273v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation", "abstract": "In sequence labeling, previous domain adaptation methods focus on the\nadaptation from the source domain to the entire target domain without\nconsidering the diversity of individual target domain samples, which may lead\nto negative transfer results for certain samples. Besides, an important\ncharacteristic of sequence labeling tasks is that different elements within a\ngiven sample may also have diverse domain relevance, which requires further\nconsideration. To take the multi-level domain relevance discrepancy into\naccount, in this paper, we propose a fine-grained knowledge fusion model with\nthe domain relevance modeling scheme to control the balance between learning\nfrom the target domain data and learning from the source domain model.\nExperiments on three sequence labeling tasks show that our fine-grained\nknowledge fusion model outperforms strong baselines and other state-of-the-art\nsequence labeling domain adaptation methods.", "published": "2019-09-10 06:31:09", "link": "http://arxiv.org/abs/1909.04315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Corpus-free State2Seq User Simulator for Task-oriented Dialogue", "abstract": "Recent reinforcement learning algorithms for task-oriented dialogue system\nabsorbs a lot of interest. However, an unavoidable obstacle for training such\nalgorithms is that annotated dialogue corpora are often unavailable. One of the\npopular approaches addressing this is to train a dialogue agent with a user\nsimulator. Traditional user simulators are built upon a set of dialogue rules\nand therefore lack response diversity. This severely limits the simulated cases\nfor agent training. Later data-driven user models work better in diversity but\nsuffer from data scarcity problem. To remedy this, we design a new corpus-free\nframework that taking advantage of their benefits. The framework builds a user\nsimulator by first generating diverse dialogue data from templates and then\nbuild a new State2Seq user simulator on the data. To enhance the performance,\nwe propose the State2Seq user simulator model to efficiently leverage dialogue\nstate and history. Experiment results on an open dataset show that our user\nsimulator helps agents achieve an improvement of 6.36% on success rate.\nState2Seq model outperforms the seq2seq baseline for 1.9 F-score.", "published": "2019-09-10 12:50:06", "link": "http://arxiv.org/abs/1909.04448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Select and Attend: Towards Controllable Content Selection in Text\n  Generation", "abstract": "Many text generation tasks naturally contain two steps: content selection and\nsurface realization. Current neural encoder-decoder models conflate both steps\ninto a black-box architecture. As a result, the content to be described in the\ntext cannot be explicitly controlled. This paper tackles this problem by\ndecoupling content selection from the decoder. The decoupled content selection\nis human interpretable, whose value can be manually manipulated to control the\ncontent of generated text. The model can be trained end-to-end without human\nannotations by maximizing a lower bound of the marginal likelihood. We further\npropose an effective way to trade-off between performance and controllability\nwith a single adjustable hyperparameter. In both data-to-text and headline\ngeneration tasks, our model achieves promising results, paving the way for\ncontrollable content selection in text generation.", "published": "2019-09-10 12:59:10", "link": "http://arxiv.org/abs/1909.04453v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Deep Networks Like to Read?", "abstract": "Recent research towards understanding neural networks probes models in a\ntop-down manner, but is only able to identify model tendencies that are known a\npriori. We propose Susceptibility Identification through Fine-Tuning (SIFT), a\nnovel abstractive method that uncovers a model's preferences without imposing\nany prior. By fine-tuning an autoencoder with the gradients from a fixed\nclassifier, we are able to extract propensities that characterize different\nkinds of classifiers in a bottom-up manner. We further leverage the SIFT\narchitecture to rephrase sentences in order to predict the opposing class of\nthe ground truth label, uncovering potential artifacts encoded in the fixed\nclassification model. We evaluate our method on three diverse tasks with four\ndifferent models. We contrast the propensities of the models as well as\nreproduce artifacts reported in the literature.", "published": "2019-09-10 15:00:23", "link": "http://arxiv.org/abs/1909.04547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Languages in Source Code: Auto-Translation for Localized\n  Instruction", "abstract": "Computer science education has promised open access around the world, but\naccess is largely determined by what human language you speak. As younger\nstudents learn computer science it is less appropriate to assume that they\nshould learn English beforehand. To that end we present CodeInternational, the\nfirst tool to translate code between human languages. To develop a theory of\nnon-English code, and inform our translation decisions, we conduct a study of\npublic code repositories on GitHub. The study is to the best of our knowledge\nthe first on human-language in code and covers 2.9 million Java repositories.\nTo demonstrate CodeInternational's educational utility, we build an interactive\nversion of the popular English-language Karel reader and translate it into 100\nspoken languages. Our translations have already been used in classrooms around\nthe world, and represent a first step in an important open CS-education\nproblem.", "published": "2019-09-10 15:06:58", "link": "http://arxiv.org/abs/1909.04556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientific Discourse Tagging for Evidence Extraction", "abstract": "Evidence plays a crucial role in any biomedical research narrative, providing\njustification for some claims and refutation for others. We seek to build\nmodels of scientific argument using information extraction methods from\nfull-text papers. We present the capability of automatically extracting text\nfragments from primary research papers that describe the evidence presented in\nthat paper's figures, which arguably provides the raw material of any\nscientific argument made within the paper. We apply richly contextualized deep\nrepresentation learning pre-trained on biomedical domain corpus to the analysis\nof scientific discourse structures and the extraction of \"evidence fragments\"\n(i.e., the text in the results section describing data presented in a specified\nsubfigure) from a set of biomedical experimental research articles. We first\ndemonstrate our state-of-the-art scientific discourse tagger on two scientific\ndiscourse tagging datasets and its transferability to new datasets. We then\nshow the benefit of leveraging scientific discourse tags for downstream tasks\nsuch as claim-extraction and evidence fragment detection. Our work demonstrates\nthe potential of using evidence fragments derived from figure spans for\nimproving the quality of scientific claims by cataloging, indexing and reusing\nevidence fragments as independent documents.", "published": "2019-09-10 21:17:20", "link": "http://arxiv.org/abs/1909.04758v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Definition Frames: Using Definitions for Hybrid Concept Representations", "abstract": "Advances in word representations have shown tremendous improvements in\ndownstream NLP tasks, but lack semantic interpretability. In this paper, we\nintroduce Definition Frames (DF), a matrix distributed representation extracted\nfrom definitions, where each dimension is semantically interpretable. DF\ndimensions correspond to the Qualia structure relations: a set of relations\nthat uniquely define a term. Our results show that DFs have competitive\nperformance with other distributional semantic approaches on word similarity\ntasks.", "published": "2019-09-10 23:43:05", "link": "http://arxiv.org/abs/1909.04793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Annotation Artifacts in Natural Language Inference Datasets\n  to Improve Cross-dataset Generalization Ability", "abstract": "Natural language inference (NLI) aims at predicting the relationship between\na given pair of premise and hypothesis. However, several works have found that\nthere widely exists a bias pattern called annotation artifacts in NLI datasets,\nmaking it possible to identify the label only by looking at the hypothesis.\nThis irregularity makes the evaluation results over-estimated and affects\nmodels' generalization ability. In this paper, we consider a more trust-worthy\nsetting, i.e., cross-dataset evaluation. We explore the impacts of annotation\nartifacts in cross-dataset testing. Furthermore, we propose a training\nframework to mitigate the impacts of the bias pattern. Experimental results\ndemonstrate that our methods can alleviate the negative effect of the artifacts\nand improve the generalization ability of models.", "published": "2019-09-10 02:35:34", "link": "http://arxiv.org/abs/1909.04242v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Embeddings from Language Models", "abstract": "Word embeddings such as ELMo have recently been shown to model word semantics\nwith greater efficacy through contextualized learning on large-scale language\ncorpora, resulting in significant improvement in state of the art across many\nnatural language tasks. In this work we integrate acoustic information into\ncontextualized lexical embeddings through the addition of multimodal inputs to\na pretrained bidirectional language model. The language model is trained on\nspoken language that includes text and audio modalities. The resulting\nrepresentations from this model are multimodal and contain paralinguistic\ninformation which can modify word meanings and provide affective information.\nWe show that these multimodal embeddings can be used to improve over previous\nstate of the art multimodal models in emotion recognition on the CMU-MOSEI\ndataset.", "published": "2019-09-10 05:46:39", "link": "http://arxiv.org/abs/1909.04302v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Core Semantic First: A Top-down Approach for AMR Parsing", "abstract": "We introduce a novel scheme for parsing a piece of text into its Abstract\nMeaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel\ncharacteristic of GSP is that it constructs a parse graph incrementally in a\ntop-down fashion. Starting from the root, at each step, a new node and its\nconnections to existing nodes will be jointly predicted. The output graph spans\nthe nodes by the distance to the root, following the intuition of first\ngrasping the main ideas then digging into more details. The \\textit{core\nsemantic first} principle emphasizes capturing the main ideas of a sentence,\nwhich is of great interest. We evaluate our model on the latest AMR sembank and\nachieve the state-of-the-art performance in the sense that no heuristic graph\nre-categorization is adopted. More importantly, the experiments show that our\nparser is especially good at obtaining the core semantics.", "published": "2019-09-10 05:51:12", "link": "http://arxiv.org/abs/1909.04303v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Competing Topic Naming Conventions in Quora: Predicting Appropriate\n  Topic Merges and Winning Topics from Millions of Topic Pairs", "abstract": "Quora is a popular Q&A site which provides users with the ability to tag\nquestions with multiple relevant topics which helps to attract quality answers.\nThese topics are not predefined but user-defined conventions and it is not so\nrare to have multiple such conventions present in the Quora ecosystem\ndescribing exactly the same concept. In almost all such cases, users (or Quora\nmoderators) manually merge the topic pair into one of the either topics, thus\nselecting one of the competing conventions. An important application for the\nsite therefore is to identify such competing conventions early enough that\nshould merge in future. In this paper, we propose a two-step approach that\nuniquely combines the anomaly detection and the supervised classification\nframeworks to predict whether two topics from among millions of topic pairs are\nindeed competing conventions, and should merge, achieving an F-score of 0.711.\nWe also develop a model to predict the direction of the topic merge, i.e., the\nwinning convention, achieving an F-score of 0.898. Our system is also able to\npredict ~ 25% of the correct case of merges within the first month of the merge\nand ~ 40% of the cases within a year. This is an encouraging result since Quora\nusers on average take 936 days to identify such a correct merge. Human judgment\nexperiments show that our system is able to predict almost all the correct\ncases that humans can predict plus 37.24% correct cases which the humans are\nnot able to identify at all.", "published": "2019-09-10 09:35:23", "link": "http://arxiv.org/abs/1909.04367v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Attesting Biases and Discrimination using Language Semantics", "abstract": "AI agents are increasingly deployed and used to make automated decisions that\naffect our lives on a daily basis. It is imperative to ensure that these\nsystems embed ethical principles and respect human values. We focus on how we\ncan attest to whether AI agents treat users fairly without discriminating\nagainst particular individuals or groups through biases in language. In\nparticular, we discuss human unconscious biases, how they are embedded in\nlanguage, and how AI systems inherit those biases by learning from and\nprocessing human language. Then, we outline a roadmap for future research to\nbetter understand and attest problematic AI biases derived from language.", "published": "2019-09-10 10:12:01", "link": "http://arxiv.org/abs/1909.04386v1", "categories": ["cs.AI", "cs.CL", "68T50"], "primary_category": "cs.AI"}
{"title": "A Crowd-based Evaluation of Abuse Response Strategies in Conversational\n  Agents", "abstract": "How should conversational agents respond to verbal abuse through the user? To\nanswer this question, we conduct a large-scale crowd-sourced evaluation of\nabuse response strategies employed by current state-of-the-art systems. Our\nresults show that some strategies, such as \"polite refusal\" score highly across\nthe board, while for other strategies demographic factors, such as age, as well\nas the severity of the preceding abuse influence the user's perception of which\nresponse is appropriate. In addition, we find that most data-driven models lag\nbehind rule-based or commercial systems in terms of their perceived\nappropriateness.", "published": "2019-09-10 10:12:59", "link": "http://arxiv.org/abs/1909.04387v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Extending the Service Composition Formalism with Relational Parameters", "abstract": "Web Service Composition deals with the (re)use of Web Services to provide\ncomplex functionality, inexistent in any single service. Over the\nstate-of-the-art, we introduce a new type of modeling, based on ontologies and\nrelations between objects, which allows us to extend the expressiveness of\nproblems that can be solved automatically.", "published": "2019-09-10 10:32:49", "link": "http://arxiv.org/abs/1909.04393v1", "categories": ["cs.CL", "cs.NI"], "primary_category": "cs.CL"}
{"title": "Learning review representations from user and product level information\n  for spam detection", "abstract": "Opinion spam has become a widespread problem in social media, where hired\nspammers write deceptive reviews to promote or demote products to mislead the\nconsumers for profit or fame. Existing works mainly focus on manually designing\ndiscrete textual or behavior features, which cannot capture complex semantics\nof reviews. Although recent works apply deep learning methods to learn\nreview-level semantic features, their models ignore the impact of the\nuser-level and product-level information on learning review semantics and the\ninherent user-review-product relationship information. In this paper, we\npropose a Hierarchical Fusion Attention Network (HFAN) to automatically learn\nthe semantics of reviews from the user and product level. Specifically, we\ndesign a multi-attention unit to extract user(product)-related review\ninformation. Then, we use orthogonal decomposition and fusion attention to\nlearn a user, review, and product representation from the review information.\nFinally, we take the review as a relation between user and product entity and\napply TransH to jointly encode this relationship into review representation.\nExperimental results obtained more than 10\\% absolute precision improvement\nover the state-of-the-art performances on four real-world datasets, which show\nthe effectiveness and versatility of the model.", "published": "2019-09-10 13:01:27", "link": "http://arxiv.org/abs/1909.04455v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Countering Language Drift via Visual Grounding", "abstract": "Emergent multi-agent communication protocols are very different from natural\nlanguage and not easily interpretable by humans. We find that agents that were\ninitially pretrained to produce natural language can also experience\ndetrimental language drift: when a non-linguistic reward is used in a\ngoal-based task, e.g. some scalar success metric, the communication protocol\nmay easily and radically diverge from natural language. We recast translation\nas a multi-agent communication game and examine auxiliary training constraints\nfor their effectiveness in mitigating language drift. We show that a\ncombination of syntactic (language model likelihood) and semantic (visual\ngrounding) constraints gives the best communication performance, allowing\npre-trained agents to retain English syntax while learning to accurately convey\nthe intended meaning.", "published": "2019-09-10 14:05:28", "link": "http://arxiv.org/abs/1909.04499v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WIQA: A dataset for \"What if...\" reasoning over procedural text", "abstract": "We introduce WIQA, the first large-scale dataset of \"What if...\" questions\nover procedural text. WIQA contains three parts: a collection of paragraphs\neach describing a process, e.g., beach erosion; a set of crowdsourced influence\ngraphs for each paragraph, describing how one change affects another; and a\nlarge (40k) collection of \"What if...?\" multiple-choice questions derived from\nthe graphs. For example, given a paragraph about beach erosion, would stormy\nweather result in more or less erosion (or have no effect)? The task is to\nanswer the questions, given their associated paragraph. WIQA contains three\nkinds of questions: perturbations to steps mentioned in the paragraph; external\n(out-of-paragraph) perturbations requiring commonsense knowledge; and\nirrelevant (no effect) perturbations. We find that state-of-the-art models\nachieve 73.8% accuracy, well below the human performance of 96.3%. We analyze\nthe challenges, in particular tracking chains of influences, and present the\ndataset as an open challenge to the community.", "published": "2019-09-10 20:37:39", "link": "http://arxiv.org/abs/1909.04739v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Everything Happens for a Reason: Discovering the Purpose of Actions in\n  Procedural Text", "abstract": "Our goal is to better comprehend procedural text, e.g., a paragraph about\nphotosynthesis, by not only predicting what happens, but why some actions need\nto happen before others. Our approach builds on a prior process comprehension\nframework for predicting actions' effects, to also identify subsequent steps\nthat those effects enable. We present our new model (XPAD) that biases effect\npredictions towards those that (1) explain more of the actions in the paragraph\nand (2) are more plausible with respect to background knowledge. We also extend\nan existing benchmark dataset for procedural text comprehension, ProPara, by\nadding the new task of explaining actions by predicting their dependencies. We\nfind that XPAD significantly outperforms prior systems on this task, while\nmaintaining the performance on the original task in ProPara. The dataset is\navailable at http://data.allenai.org/propara", "published": "2019-09-10 20:46:56", "link": "http://arxiv.org/abs/1909.04745v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MultiFiT: Efficient Multi-lingual Language Model Fine-tuning", "abstract": "Pretrained language models are promising particularly for low-resource\nlanguages as they only require unlabelled data. However, training existing\nmodels requires huge amounts of compute, while pretrained cross-lingual models\noften underperform on low-resource languages. We propose Multi-lingual language\nmodel Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune\nlanguage models efficiently in their own language. In addition, we propose a\nzero-shot method using an existing pretrained cross-lingual model. We evaluate\nour methods on two widely used cross-lingual classification datasets where they\noutperform models pretrained on orders of magnitude more data and compute. We\nrelease all models and code.", "published": "2019-09-10 21:30:54", "link": "http://arxiv.org/abs/1909.04761v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Longer the Better? The Interplay Between Review Length and Line of\n  Argumentation in Online Consumer Reviews", "abstract": "Review helpfulness serves as focal point in understanding customers' purchase\ndecision-making process on online retailer platforms. An overwhelming majority\nof previous works find longer reviews to be more helpful than short reviews. In\nthis paper, we propose that longer reviews should not be assumed to be\nuniformly more helpful; instead, we argue that the effect depends on the line\nof argumentation in the review text. To test this idea, we use a large dataset\nof customer reviews from Amazon in combination with a state-of-the-art approach\nfrom natural language processing that allows us to study argumentation lines at\nsentence level. Our empirical analysis suggests that the frequency of\nargumentation changes moderates the effect of review length on helpfulness.\nAltogether, we disprove the prevailing narrative that longer reviews are\nuniformly perceived as more helpful. Our findings allow retailer platforms to\nimprove their customer feedback systems and to feature more useful product\nreviews.", "published": "2019-09-10 15:19:51", "link": "http://arxiv.org/abs/1909.05192v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Evalutation of Programming Language Models' performance on Software\n  Defect Detection", "abstract": "This dissertation presents an evaluation of several language models on\nsoftware defect datasets. A language Model (LM) \"can provide word\nrepresentation and probability indication of word sequences as the core\ncomponent of an NLP system.\" Language models for source code are specified for\ntasks in the software engineering field. While some models are directly the NLP\nones, others contain structural information that is uniquely owned by source\ncode. Software defects are defects in the source code that lead to unexpected\nbehaviours and malfunctions at all levels. This study provides an original\nattempt to detect these defects at three different levels (syntactical,\nalgorithmic and general) We also provide a tool chain that researchers can use\nto reproduce the experiments. We have tested the different models against\ndifferent datasets, and performed an analysis over the results. Our original\nattempt to deploy bert, the state-of-the-art model for multitasks, leveled or\noutscored all other models compared.", "published": "2019-09-10 19:07:15", "link": "http://arxiv.org/abs/1909.10309v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "abstract": "Countering online hate speech is a critical yet challenging task, but one\nwhich can be aided by the use of Natural Language Processing (NLP) techniques.\nPrevious research has primarily focused on the development of NLP methods to\nautomatically and effectively detect online hate speech while disregarding\nfurther action needed to calm and discourage individuals from using hate speech\nin the future. In addition, most existing hate speech datasets treat each post\nas an isolated instance, ignoring the conversational context. In this paper, we\npropose a novel task of generative hate speech intervention, where the goal is\nto automatically generate responses to intervene during online conversations\nthat contain hate speech. As a part of this work, we introduce two\nfully-labeled large-scale hate speech intervention datasets collected from Gab\nand Reddit. These datasets provide conversation segments, hate speech labels,\nas well as intervention responses written by Mechanical Turk Workers. In this\npaper, we also analyze the datasets to understand the common intervention\nstrategies and explore the performance of common automatic response generation\nmethods on these new datasets to provide a benchmark for future research.", "published": "2019-09-10 03:00:58", "link": "http://arxiv.org/abs/1909.04251v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization in Image Captioning", "abstract": "Image captioning models are usually evaluated on their ability to describe a\nheld-out set of images, not on their ability to generalize to unseen concepts.\nWe study the problem of compositional generalization, which measures how well a\nmodel composes unseen combinations of concepts when describing images.\nState-of-the-art image captioning models show poor generalization performance\non this task. We propose a multi-task model to address the poor performance,\nthat combines caption generation and image--sentence ranking, and uses a\ndecoding mechanism that re-ranks the captions according their similarity to the\nimage. This model is substantially better at generalizing to unseen\ncombinations of concepts compared to state-of-the-art captioning models.", "published": "2019-09-10 10:55:56", "link": "http://arxiv.org/abs/1909.04402v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Jointly embedding the local and global relations of heterogeneous graph\n  for rumor detection", "abstract": "The development of social media has revolutionized the way people\ncommunicate, share information and make decisions, but it also provides an\nideal platform for publishing and spreading rumors. Existing rumor detection\nmethods focus on finding clues from text content, user profiles, and\npropagation patterns. However, the local semantic relation and global\nstructural information in the message propagation graph have not been well\nutilized by previous works.\n  In this paper, we present a novel global-local attention network (GLAN) for\nrumor detection, which jointly encodes the local semantic and global structural\ninformation. We first generate a better integrated representation for each\nsource tweet by fusing the semantic information of related retweets with the\nattention mechanism. Then, we model the global relationships among all source\ntweets, retweets, and users as a heterogeneous graph to capture the rich\nstructural information for rumor detection. We conduct experiments on three\nreal-world datasets, and the results demonstrate that GLAN significantly\noutperforms the state-of-the-art models in both rumor detection and early\ndetection scenarios.", "published": "2019-09-10 13:18:59", "link": "http://arxiv.org/abs/1909.04465v2", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Representation of Constituents in Neural Language Models: Coordination\n  Phrase as a Case Study", "abstract": "Neural language models have achieved state-of-the-art performances on many\nNLP tasks, and recently have been shown to learn a number of\nhierarchically-sensitive syntactic dependencies between individual words.\nHowever, equally important for language processing is the ability to combine\nwords into phrasal constituents, and use constituent-level features to drive\ndownstream expectations. Here we investigate neural models' ability to\nrepresent constituent-level features, using coordinated noun phrases as a case\nstudy. We assess whether different neural language models trained on English\nand French represent phrase-level number and gender features, and use those\nfeatures to drive downstream expectations. Our results suggest that models use\na linear combination of NP constituent number to drive CoordNP/verb number\nagreement. This behavior is highly regular and even sensitive to local\nsyntactic context, however it differs crucially from observed human behavior.\nModels have less success with gender agreement. Models trained on large corpora\nperform best, and there is no obvious advantage for models trained using\nexplicit syntactic supervision.", "published": "2019-09-10 17:02:15", "link": "http://arxiv.org/abs/1909.04625v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Embedding Allocation: Distributed Representations of Topic Models", "abstract": "Word embedding models such as the skip-gram learn vector representations of\nwords' semantic relationships, and document embedding models learn similar\nrepresentations for documents. On the other hand, topic models provide latent\nrepresentations of the documents' topical themes. To get the benefits of these\nrepresentations simultaneously, we propose a unifying algorithm, called neural\nembedding allocation (NEA), which deconstructs topic models into interpretable\nvector-space embeddings of words, topics, documents, authors, and so on, by\nlearning neural embeddings to mimic the topic models. We showcase NEA's\neffectiveness and generality on LDA, author-topic models and the recently\nproposed mixed membership skip gram topic model and achieve better performance\nwith the embeddings compared to several state-of-the-art models. Furthermore,\nwe demonstrate that using NEA to smooth out the topics improves coherence\nscores over the original topic models when the number of topics is large.", "published": "2019-09-10 18:39:26", "link": "http://arxiv.org/abs/1909.04702v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Speech Enhancement Based on Cloned Networks", "abstract": "We propose to implement speech enhancement by the regeneration of clean\nspeech from a salient representation extracted from the noisy signal. The\nnetwork that extracts salient features is trained using a set of weight-sharing\nclones of the extractor network. The clones receive mel-frequency spectra of\ndifferent noisy versions of the same speech signal as input. By encouraging the\noutputs of the clones to be similar for these different input signals, we train\na feature extractor network that is robust to noise. At inference, the salient\nfeatures form the input to a WaveNet network that generates a natural and clean\nspeech signal with the same attributes as the ground-truth clean signal. As the\nsignal becomes noisier, our system produces natural sounding errors that stay\non the speech manifold, in place of traditional artifacts found in other\nsystems. Our experiments confirm that our generative enhancement system\nprovides state-of-the-art enhancement performance within the generative class\nof enhancers according to a MUSHRA-like test. The clones based system matches\nor outperforms the other systems at each input signal-to-noise (SNR) range with\nstatistical significance.", "published": "2019-09-10 22:06:55", "link": "http://arxiv.org/abs/1909.04776v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frequency domain variant of Velvet noise and its application to acoustic\n  measurements", "abstract": "We propose a new family of test signals for acoustic measurements such as\nimpulse response, nonlinearity, and the effects of background noise. The\nproposed family complements difficulties in existing families, the Swept-Sine\n(SS), pseudo-random noise such as the maximum length sequence (MLS). The\nproposed family uses the frequency domain variant of the Velvet noise (FVN) as\nits building block. An FVN is an impulse response of an all-pass filter and\nyields the unit impulse when convolved with the time-reversed version of\nitself. In this respect, FVN is a member of the time-stretched pulse (TSP) in\nthe broadest sense. The high degree of freedom in designing an FVN opens a vast\nrange of applications in acoustic measurement. We introduce the following\napplications and their specific procedures, among other possibilities. They are\nas follows. a) Spectrum shaping adaptive to background noise. b) Simultaneous\nmeasurement of impulse responses of multiple acoustic paths. d) Simultaneous\nmeasurement of linear and nonlinear components of an acoustic path. e)\nAutomatic procedure for time axis alignment of the source and the receiver when\nthey are using independent clocks in acoustic impulse response measurement. We\nimplemented a reference measurement tool equipped with all these procedures.\nThe MATLAB source code and related materials are open-sourced and placed in a\nGitHub repository.", "published": "2019-09-10 05:36:33", "link": "http://arxiv.org/abs/1909.04301v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Automatic detection of estuarine dolphin whistles in spectrogram images", "abstract": "An algorithm for detecting tonal vocalizations from estuarine dolphin\n(Sotalia guianensis) specimens without interference of a human operator is\ndeveloped. The raw audio data collected from a passive monitoring sensor in the\nCanan\\'eia underwater soundscape is converted to spectrogram images, containing\nthe desired acoustic event (whistle) as a linear pattern in the images.\nDetection is a four-step method: first, ridge maps are obtained from the\nspectrogram images; second, a probabilistic Hough transform algorithm is\napplied to detect roughly linear ridges, which are adjusted to the true\ncorresponding shape of the whistles via an active contour algorithm; third,\nfeature vectors are built from the geometry of each detected curve; and fourth,\nthe detections are fed to a random forest classifier to parse out false\npositives. We develop a system capable of reliably classifying roughly 97% of\nthe characteristic patterns detected as Sotalia guianensis whistles or random\nempty detections.", "published": "2019-09-10 12:00:26", "link": "http://arxiv.org/abs/1909.04425v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Computer Assisted Composition in Continuous Time", "abstract": "We address the problem of combining sequence models of symbolic music with\nuser defined constraints. For typical models this is non-trivial as only the\nconditional distribution of each symbol given the earlier symbols is available,\nwhile the constraints correspond to arbitrary times. Previously this has been\naddressed by assuming a discrete time model of fixed rhythm. We generalise to\ncontinuous time and arbitrary rhythm by introducing a simple, novel, and\nefficient particle filter scheme, applicable to general continuous time point\nprocesses. Extensive experimental evaluations demonstrate that in comparison\nwith a more traditional beam search baseline, the particle filter exhibits\nsuperior statistical properties and yields more agreeable results in an\nextensive human listening test experiment.", "published": "2019-09-10 05:57:58", "link": "http://arxiv.org/abs/1909.05030v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
