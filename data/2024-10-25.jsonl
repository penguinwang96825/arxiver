{"title": "A Stock Price Prediction Approach Based on Time Series Decomposition and Multi-Scale CNN using OHLCT Images", "abstract": "Recently, deep learning in stock prediction has become an important branch.\nImage-based methods show potential by capturing complex visual patterns and\nspatial correlations, offering advantages in interpretability over time series\nmodels. However, image-based approaches are more prone to overfitting,\nhindering robust predictive performance. To improve accuracy, this paper\nproposes a novel method, named Sequence-based Multi-scale Fusion Regression\nConvolutional Neural Network (SMSFR-CNN), for predicting stock price movements\nin the China A-share market.\n  By utilizing CNN to learn sequential features and combining them with image\nfeatures, we improve the accuracy of stock trend prediction on the A-share\nmarket stock dataset. This approach reduces the search space for image\nfeatures, stabilizes, and accelerates the training process. Extensive\ncomparative experiments on 4,454 A-share stocks show that the model achieves a\n61.15% positive predictive value and a 63.37% negative predictive value for the\nnext 5 days, resulting in a total profit of 165.09%.", "published": "2024-10-25 03:50:54", "link": "http://arxiv.org/abs/2410.19291v2", "categories": ["cs.LG", "cs.AI", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "Have LLMs Reopened the Pandora's Box of AI-Generated Fake News?", "abstract": "With the rise of AI-generated content spewed at scale from large language\nmodels (LLMs), genuine concerns about the spread of fake news have intensified.\nThe perceived ability of LLMs to produce convincing fake news at scale poses\nnew challenges for both human and automated fake news detection systems. To\naddress this gap, this paper presents the findings from a university-level\ncompetition that aimed to explore how LLMs can be used by humans to create fake\nnews, and to assess the ability of human annotators and AI models to detect it.\nA total of 110 participants used LLMs to create 252 unique fake news stories,\nand 84 annotators participated in the detection tasks. Our findings indicate\nthat LLMs are ~68% more effective at detecting real news than humans. However,\nfor fake news detection, the performance of LLMs and humans remains comparable\n(~60% accuracy). Additionally, we examine the impact of visual elements (e.g.,\npictures) in news on the accuracy of detecting fake news stories. Finally, we\nalso examine various strategies used by fake news creators to enhance the\ncredibility of their AI-generated content. This work highlights the increasing\ncomplexity of detecting AI-generated fake news, particularly in collaborative\nhuman-AI settings.", "published": "2024-10-25 01:58:29", "link": "http://arxiv.org/abs/2410.19250v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite\n  Learning", "abstract": "Recent studies have identified one aggravating factor of LLM hallucinations\nas the knowledge inconsistency between pre-training and fine-tuning, where\nunfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong\noutputs. In this paper, we propose a novel fine-tuning strategy called\nPrereq-Tune to address this knowledge inconsistency and reduce hallucinations.\nFundamentally, Prereq-Tune disentangles the learning of skills and knowledge,\nso the model learns only the task skills without being impacted by the\nknowledge inconsistency. To achieve this, Prereq-Tune introduces an additional\nprerequisite learning stage to learn the necessary knowledge for SFT, allowing\nsubsequent SFT to focus only on task skills. Prereq-Tune can also be combined\nwith fictitious synthetic data to enhance the grounding of LLM outputs to their\ninternal knowledge. Experiments show that Prereq-Tune outperforms existing\nbaselines in improving LLM's factuality across short QA and long-form\ngeneration tasks. It also opens new possibilities for knowledge-controlled\ngeneration in LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/Prereq_tune.git.", "published": "2024-10-25 03:48:51", "link": "http://arxiv.org/abs/2410.19290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Any Other Thoughts, Hedgehog? Linking Deliberation Chains in\n  Collaborative Dialogues", "abstract": "Question-asking in collaborative dialogue has long been established as key to\nknowledge construction, both in internal and collaborative problem solving. In\nthis work, we examine probing questions in collaborative dialogues: questions\nthat explicitly elicit responses from the speaker's interlocutors.\nSpecifically, we focus on modeling the causal relations that lead directly from\nutterances earlier in the dialogue to the emergence of the probing question. We\nmodel these relations using a novel graph-based framework of deliberation\nchains, and reframe the problem of constructing such chains as a\ncoreference-style clustering problem. Our framework jointly models probing and\ncausal utterances and the links between them, and we evaluate on two\nchallenging collaborative task datasets: the Weights Task and DeliData. Our\nresults demonstrate the effectiveness of our theoretically-grounded approach\ncompared to both baselines and stronger coreference approaches, and establish a\nstandard of performance in this novel task.", "published": "2024-10-25 04:15:56", "link": "http://arxiv.org/abs/2410.19301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in\n  Conversational LLMs", "abstract": "The growing use of large language model (LLM)-based chatbots has raised\nconcerns about fairness. Fairness issues in LLMs can lead to severe\nconsequences, such as bias amplification, discrimination, and harm to\nmarginalized communities. While existing fairness benchmarks mainly focus on\nsingle-turn dialogues, multi-turn scenarios, which in fact better reflect\nreal-world conversations, present greater challenges due to conversational\ncomplexity and potential bias accumulation. In this paper, we propose a\ncomprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios,\n\\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM\nfairness capabilities across three stages: context understanding, user\ninteraction, and instruction trade-offs, with each stage comprising two tasks.\nTo ensure coverage of diverse bias types and attributes, we draw from existing\nfairness datasets and employ our template to construct a multi-turn dialogue\ndataset, \\texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias\nclassifiers including Llama-Guard-3 and human validation to ensure robustness.\nExperiments and analyses on \\texttt{FairMT-10K} reveal that in multi-turn\ndialogue scenarios, current LLMs are more likely to generate biased responses,\nand there is significant variation in performance across different tasks and\nmodels. Based on this, we curate a challenging dataset, \\texttt{FairMT-1K}, and\ntest 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show\nthe current state of fairness in LLMs and showcase the utility of this novel\napproach for assessing fairness in more realistic multi-turn dialogue contexts,\ncalling for future work to focus on LLM fairness improvement and the adoption\nof \\texttt{FairMT-1K} in such efforts.", "published": "2024-10-25 06:06:31", "link": "http://arxiv.org/abs/2410.19317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KAHANI: Culturally-Nuanced Visual Storytelling Tool for Non-Western\n  Cultures", "abstract": "Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated\nthe ability to generate compelling text and visual stories. However, their\noutputs are predominantly aligned with the sensibilities of the Global North,\noften resulting in an outsider's gaze on other cultures. As a result,\nnon-Western communities have to put extra effort into generating culturally\nspecific stories. To address this challenge, we developed a visual storytelling\ntool called Kahani that generates culturally grounded visual stories for\nnon-Western cultures. Our tool leverages off-the-shelf models GPT-4 Turbo and\nStable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I prompting\ntechniques, we capture the cultural context from user's prompt and generate\nvivid descriptions of the characters and scene compositions. To evaluate the\neffectiveness of Kahani, we conducted a comparative user study with ChatGPT-4\n(with DALL-E3) in which participants from different regions of India compared\nthe cultural relevance of stories generated by the two tools. The results of\nthe qualitative and quantitative analysis performed in the user study show that\nKahani's visual stories are more culturally nuanced than those generated by\nChatGPT-4. In 27 out of 36 comparisons, Kahani outperformed or was on par with\nChatGPT-4, effectively capturing cultural nuances and incorporating more\nCulturally Specific Items (CSI), validating its ability to generate culturally\ngrounded visual stories.", "published": "2024-10-25 09:23:24", "link": "http://arxiv.org/abs/2410.19419v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework", "abstract": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research", "published": "2024-10-25 10:28:59", "link": "http://arxiv.org/abs/2410.19453v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Debate-Driven Experiment on LLM Hallucinations and Accuracy", "abstract": "Large language models (LLMs) have achieved a degree of success in generating\ncoherent and contextually relevant text, yet they remain prone to a significant\nchallenge known as hallucination: producing information that is not\nsubstantiated by the input or external knowledge. Previous efforts to mitigate\nhallucinations have focused on techniques such as fine-tuning models on\nhigh-quality datasets, incorporating fact-checking mechanisms, and developing\nadversarial training methods. While these approaches have shown some promise,\nthey often address the issue at the level of individual model outputs, leaving\nunexplored the effects of inter-model interactions on hallucination. This study\ninvestigates the phenomenon of hallucination in LLMs through a novel\nexperimental framework where multiple instances of GPT-4o-Mini models engage in\na debate-like interaction prompted with questions from the TruthfulQA dataset.\nOne model is deliberately instructed to generate plausible but false answers\nwhile the other models are asked to respond truthfully. The experiment is\ndesigned to assess whether the introduction of misinformation by one model can\nchallenge the truthful majority to better justify their reasoning, improving\nperformance on the TruthfulQA benchmark. The findings suggest that inter-model\ninteractions can offer valuable insights into improving the accuracy and\nrobustness of LLM outputs, complementing existing mitigation strategies.", "published": "2024-10-25 11:41:27", "link": "http://arxiv.org/abs/2410.19485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization", "abstract": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs.", "published": "2024-10-25 11:58:12", "link": "http://arxiv.org/abs/2410.19499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models", "abstract": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) being particularly\nnotable for reducing the mismatch between training and inference. However, SGOs\noften produce noisy and biased sequences, which can lead to misguidance from\nthe teacher model, especially in long sequences. To mitigate these challenges,\nwe propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel\napproach that strategically incorporates the teacher model during the student's\nsequence generation. SWITCH identifies discrepancies between the token\nprobabilities of the teacher and student models, allowing the teacher to\nintervene selectively, particularly in long sequences that are more prone to\nteacher misguidance. Extensive experimental results across three model families\nand five instruction-following datasets show that SWITCH surpasses traditional\nKD methods, particularly excelling in the generation of long sequential data.", "published": "2024-10-25 12:10:49", "link": "http://arxiv.org/abs/2410.19503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems", "abstract": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.", "published": "2024-10-25 14:07:53", "link": "http://arxiv.org/abs/2410.19572v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A distributional simplicity bias in the learning dynamics of\n  transformers", "abstract": "The remarkable capability of over-parameterised neural networks to generalise\neffectively has been explained by invoking a ``simplicity bias'': neural\nnetworks prevent overfitting by initially learning simple classifiers before\nprogressing to more complex, non-linear functions. While simplicity biases have\nbeen described theoretically and experimentally in feed-forward networks for\nsupervised learning, the extent to which they also explain the remarkable\nsuccess of transformers trained with self-supervised techniques remains\nunclear. In our study, we demonstrate that transformers, trained on natural\nlanguage data, also display a simplicity bias. Specifically, they sequentially\nlearn many-body interactions among input tokens, reaching a saturation point in\nthe prediction error for low-degree interactions while continuing to learn\nhigh-degree interactions. To conduct this analysis, we develop a procedure to\ngenerate \\textit{clones} of a given natural language data set, which rigorously\ncapture the interactions between tokens up to a specified order. This approach\nopens up the possibilities of studying how interactions of different orders in\nthe data affect learning, in natural language processing and beyond.", "published": "2024-10-25 15:39:34", "link": "http://arxiv.org/abs/2410.19637v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProvocationProbe: Instigating Hate Speech Dataset from Twitter", "abstract": "In the recent years online social media platforms has been flooded with\nhateful remarks such as racism, sexism, homophobia etc. As a result, there have\nbeen many measures taken by various social media platforms to mitigate the\nspread of hate-speech over the internet. One particular concept within the\ndomain of hate speech is instigating hate, which involves provoking hatred\nagainst a particular community, race, colour, gender, religion or ethnicity. In\nthis work, we introduce \\textit{ProvocationProbe} - a dataset designed to\nexplore what distinguishes instigating hate speech from general hate speech.\nFor this study, we collected around twenty thousand tweets from Twitter,\nencompassing a total of nine global controversies. These controversies span\nvarious themes including racism, politics, and religion. In this paper, i) we\npresent an annotated dataset after comprehensive examination of all the\ncontroversies, ii) we also highlight the difference between hate speech and\ninstigating hate speech by identifying distinguishing features, such as\ntargeted identity attacks and reasons for hate.", "published": "2024-10-25 16:57:59", "link": "http://arxiv.org/abs/2410.19687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Small Language Models", "abstract": "Small Language Models (SLMs) have become increasingly important due to their\nefficiency and performance to perform various language tasks with minimal\ncomputational resources, making them ideal for various settings including\non-device, mobile, edge devices, among many others. In this article, we present\na comprehensive survey on SLMs, focusing on their architectures, training\ntechniques, and model compression techniques. We propose a novel taxonomy for\ncategorizing the methods used to optimize SLMs, including model compression,\npruning, and quantization techniques. We summarize the benchmark datasets that\nare useful for benchmarking SLMs along with the evaluation metrics commonly\nused. Additionally, we highlight key open challenges that remain to be\naddressed. Our survey aims to serve as a valuable resource for researchers and\npractitioners interested in developing and deploying small yet efficient\nlanguage models.", "published": "2024-10-25 23:52:28", "link": "http://arxiv.org/abs/2410.20011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using\n  Language Models", "abstract": "When conducting literature reviews, scientists often create literature review\ntables - tables whose rows are publications and whose columns constitute a\nschema, a set of aspects used to compare and contrast the papers. Can we\nautomatically generate these tables using language models (LMs)? In this work,\nwe introduce a framework that leverages LMs to perform this task by decomposing\nit into separate schema and value generation steps. To enable experimentation,\nwe address two main challenges: First, we overcome a lack of high-quality\ndatasets to benchmark table generation by curating and releasing\narxivDIGESTables, a new dataset of 2,228 literature review tables extracted\nfrom ArXiv papers that synthesize a total of 7,542 research papers. Second, to\nsupport scalable evaluation of model generations against human-authored\nreference tables, we develop DecontextEval, an automatic evaluation method that\naligns elements of tables with the same underlying aspects despite differing\nsurface forms. Given these tools, we evaluate LMs' abilities to reconstruct\nreference tables, finding this task benefits from additional context to ground\nthe generation (e.g. table captions, in-text references). Finally, through a\nhuman evaluation study we find that even when LMs fail to fully reconstruct a\nreference table, their generated novel aspects can still be useful.", "published": "2024-10-25 18:31:50", "link": "http://arxiv.org/abs/2410.22360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing a Tutoring Dialog Dataset to Optimize LLMs for Educational\n  Use", "abstract": "Recent advances in large language models (LLMs) have shown promise for\nscalable educational applications, but their use in dialog-based tutoring\nsystems remains challenging due to the need for effective pedagogical\nstrategies and the high costs associated with expert-curated datasets. Our\nstudy explores the use of smaller, more affordable LLMs for one-on-one tutoring\nin the context of solving reading comprehension problems. We developed a\nsynthetic tutoring dialog dataset, evaluated by human teachers, and fine-tuned\na smaller LLM using this dataset. Furthermore, we conducted an interactive\nexperiment comparing the performance of the fine-tuned model with a larger\nmodel in real-world tutoring scenarios. Our results show that the fine-tuned\nmodel performs on par with the larger model but at a lower cost, demonstrating\na viable, cost-effective approach for implementing LLM-based tutoring systems\nin educational settings.", "published": "2024-10-25 00:40:21", "link": "http://arxiv.org/abs/2410.19231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning", "abstract": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV", "published": "2024-10-25 02:22:00", "link": "http://arxiv.org/abs/2410.19258v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revealing and Reducing Gender Biases in Vision and Language Assistants\n  (VLAs)", "abstract": "Pre-trained large language models (LLMs) have been reliably integrated with\nvisual input for multimodal tasks. The widespread adoption of instruction-tuned\nimage-to-text vision-language assistants (VLAs) like LLaVA and InternVL\nnecessitates evaluating gender biases. We study gender bias in 22 popular\nopen-source VLAs with respect to personality traits, skills, and occupations.\nOur results show that VLAs replicate human biases likely present in the data,\nsuch as real-world occupational imbalances. Similarly, they tend to attribute\nmore skills and positive personality traits to women than to men, and we see a\nconsistent tendency to associate negative personality traits with men. To\neliminate the gender bias in these models, we find that fine-tuning-based\ndebiasing methods achieve the best trade-off between debiasing and retaining\nperformance on downstream tasks. We argue for pre-deploying gender bias\nassessment in VLAs and motivate further development of debiasing strategies to\nensure equitable societal outcomes. Code is available at\nhttps://github.com/ExplainableML/vla-gender-bias.", "published": "2024-10-25 05:59:44", "link": "http://arxiv.org/abs/2410.19314v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "AgentSense: Benchmarking Social Intelligence of Language Agents through\n  Interactive Scenarios", "abstract": "Large language models (LLMs) are increasingly leveraged to empower autonomous\nagents to simulate human beings in various fields of behavioral research.\nHowever, evaluating their capacity to navigate complex social interactions\nremains a challenge. Previous studies face limitations due to insufficient\nscenario diversity, complexity, and a single-perspective focus. To this end, we\nintroduce AgentSense: Benchmarking Social Intelligence of Language Agents\nthrough Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense\nemploys a bottom-up approach to create 1,225 diverse social scenarios\nconstructed from extensive scripts. We evaluate LLM-driven agents through\nmulti-turn interactions, emphasizing both goal completion and implicit\nreasoning. We analyze goals using ERG theory and conduct comprehensive\nexperiments. Our findings highlight that LLMs struggle with goals in complex\nsocial scenarios, especially high-level growth needs, and even GPT-4o requires\nimprovement in private information reasoning. Code and data are available at\n\\url{https://github.com/ljcleo/agent_sense}.", "published": "2024-10-25 07:04:16", "link": "http://arxiv.org/abs/2410.19346v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Interleaving Text and Number Embeddings to Solve Mathemathics Problems", "abstract": "Integrating text and numbers effectively is a crucial step towards enhancing\nLarge Language Models (LLMs) capabilities in assisting in scientific tasks.\nWhile most current approaches rely on discrete tokenization of numbers, for\ninstance, conversion to scientific notation or base 10-decomposition, a recent\napproach proposed a continuous numerical encoding as an inductive bias. In this\npaper, we build upon this approach by introducing more expressive numerical\nembeddings. Our method addresses key shortcomings, including the elimination of\nnumerical artefacts and the ability to handle a wide range of magnitudes\nwithout clipping.\n  Our work presents two key contributions. First, we employ an MLP to assign\ndistinct directions in the embedding space to different numbers. Our second\ncontribution is the introduction of a routing layer that differentiates between\nnumerical and text embeddings. We hypothesise that this combined approach\nenables the model to distinguish between text and number distributions while\nmaintaining its capacity for arithmetic operations.\n  Using only a 45 M parameter encoder-decoder architecture our method achieves\na $R^2$=0.9988 over a wide range of magnitude ($10^{-3},10^{8}$). In addition,\nwe empirically observe a reduction of the numerical artefacts and biases\nobserved compared to the baselines.", "published": "2024-10-25 07:21:57", "link": "http://arxiv.org/abs/2410.19353v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating the Role of Prompting and External Tools in Hallucination\n  Rates of Large Language Models", "abstract": "Large Language Models (LLMs) are powerful computational models trained on\nextensive corpora of human-readable text, enabling them to perform\ngeneral-purpose language understanding and generation. LLMs have garnered\nsignificant attention in both industry and academia due to their exceptional\nperformance across various natural language processing (NLP) tasks. Despite\nthese successes, LLMs often produce inaccuracies, commonly referred to as\nhallucinations. Prompt engineering, the process of designing and formulating\ninstructions for LLMs to perform specific tasks, has emerged as a key approach\nto mitigating hallucinations. This paper provides a comprehensive empirical\nevaluation of different prompting strategies and frameworks aimed at reducing\nhallucinations in LLMs. Various prompting techniques are applied to a broad set\nof benchmark datasets to assess the accuracy and hallucination rate of each\nmethod. Additionally, the paper investigates the influence of tool-calling\nagents (LLMs augmented with external tools to enhance their capabilities beyond\nlanguage generation) on hallucination rates in the same benchmarks. The\nfindings demonstrate that the optimal prompting technique depends on the type\nof problem, and that simpler techniques often outperform more complex methods\nin reducing hallucinations. Furthermore, it is shown that LLM agents can\nexhibit significantly higher hallucination rates due to the added complexity of\nexternal tool usage.", "published": "2024-10-25 08:34:53", "link": "http://arxiv.org/abs/2410.19385v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Intelligent Understanding of Large Language Models in Traditional\n  Chinese Medicine Based on Prompt Engineering Framework", "abstract": "This paper explores the application of prompt engineering to enhance the\nperformance of large language models (LLMs) in the domain of Traditional\nChinese Medicine (TCM). We propose TCM-Prompt, a framework that integrates\nvarious pre-trained language models (PLMs), templates, tokenization, and\nverbalization methods, allowing researchers to easily construct and fine-tune\nmodels for specific TCM-related tasks. We conducted experiments on disease\nclassification, syndrome identification, herbal medicine recommendation, and\ngeneral NLP tasks, demonstrating the effectiveness and superiority of our\napproach compared to baseline methods. Our findings suggest that prompt\nengineering is a promising technique for improving the performance of LLMs in\nspecialized domains like TCM, with potential applications in digitalization,\nmodernization, and personalized medicine.", "published": "2024-10-25 10:24:30", "link": "http://arxiv.org/abs/2410.19451v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models", "abstract": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph machine learning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term graph linearization, so that LLMs can handle graphs naturally.\nWe consider that graphs should be linearized meaningfully to reflect certain\nproperties of natural language text, such as local dependency and global\nalignment, in order to ease contemporary LLMs, trained on trillions of textual\ntokens, better understand graphs. To achieve this, we developed several graph\nlinearization methods based on graph centrality, degeneracy, and node\nrelabeling schemes. We then investigated their effect on LLM performance in\ngraph reasoning tasks. Experimental results on synthetic graphs demonstrate the\neffectiveness of our methods compared to random linearization baselines. Our\nwork introduces novel graph representations suitable for LLMs, contributing to\nthe potential integration of graph machine learning with the trend of\nmulti-modal processing using a unified transformer model.", "published": "2024-10-25 11:51:37", "link": "http://arxiv.org/abs/2410.19494v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detection of Human and Machine-Authored Fake News in Urdu", "abstract": "The rise of social media has amplified the spread of fake news, now further\ncomplicated by large language models (LLMs) like ChatGPT, which ease the\ngeneration of highly convincing, error-free misinformation, making it\nincreasingly challenging for the public to discern truth from falsehood.\nTraditional fake news detection methods relying on linguistic cues also becomes\nless effective. Moreover, current detectors primarily focus on binary\nclassification and English texts, often overlooking the distinction between\nmachine-generated true vs. fake news and the detection in low-resource\nlanguages. To this end, we updated detection schema to include\nmachine-generated news with focus on the Urdu language. We further propose a\nhierarchical detection strategy to improve the accuracy and robustness.\nExperiments show its effectiveness across four datasets in various settings.", "published": "2024-10-25 12:42:07", "link": "http://arxiv.org/abs/2410.19517v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mirror Matrix on the Wall: coding and vector notation as tools for\n  introspection", "abstract": "The vector notation adopted by GNU Octave plays a significant role as a tool\nfor introspection, aligning itself with the vision of Kenneth E. Iverson. He\nbelieved that, just like mathematics, a programming language should be an\neffective thinking tool for representing and reasoning about problems we wish\nto address. This work aims to explore the use of vector notation in GNU Octave\nthrough the analysis of operators and functions, providing a closer alignment\nwith mathematical notation and enhancing code efficiency. We will delve into\nfundamental concepts such as indexing, broadcasting, and function handles, and\npresent case studies for a deeper understanding of these concepts. By adopting\nvector notation, GNU Octave becomes a powerful tool for mathematicians,\nscientists and engineers, enabling them to express and solve complex problems\nmore effectively and intuitively.", "published": "2024-10-25 13:22:38", "link": "http://arxiv.org/abs/2410.19549v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World\n  Exploration, Feedback and Optimization", "abstract": "The rapid development of large language and multimodal models has sparked\nsignificant interest in using proprietary models, such as GPT-4o, to develop\nautonomous agents capable of handling real-world scenarios like web navigation.\nAlthough recent open-source efforts have tried to equip agents with the ability\nto explore environments and continuously improve over time, they are building\ntext-only agents in synthetic environments where the reward signals are clearly\ndefined. Such agents struggle to generalize to realistic settings that require\nmultimodal perception abilities and lack ground-truth signals. In this paper,\nwe introduce an open-source framework designed to facilitate the development of\nmultimodal web agent that can autonomously conduct real-world exploration and\nimprove itself. We first train the base model with imitation learning to gain\nthe basic abilities. We then let the agent explore the open web and collect\nfeedback on its trajectories. After that, it further improves its policy by\nlearning from well-performing trajectories judged by another general-purpose\nmodel. This exploration-feedback-optimization cycle can continue for several\niterations. Experimental results show that our web agent successfully improves\nitself after each iteration, demonstrating strong performance across multiple\ntest sets.", "published": "2024-10-25 15:01:27", "link": "http://arxiv.org/abs/2410.19609v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional\n  Supervision", "abstract": "Recent advancements in Direct Preference Optimization (DPO) have\nsignificantly enhanced the alignment of Large Language Models (LLMs) with human\npreferences, owing to its simplicity and effectiveness. However, existing\nmethods typically optimize a scalar score or ranking reward, thereby\noverlooking the multi-dimensional nature of human preferences. In this work, we\npropose to extend the preference of DPO to two dimensions: segments and\naspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For\nthe segment dimension, we divide the response into sentences and assign scores\nto each segment. For the aspect dimension, we meticulously design several\ncriteria covering the response quality rubrics. With the 2-dimensional signals\nas feedback, we develop a 2D-DPO framework, decomposing the overall objective\ninto multi-segment and multi-aspect objectives. Extensive experiments on\npopular benchmarks demonstrate that 2D-DPO performs better than methods that\noptimize for scalar or 1-dimensional preferences.", "published": "2024-10-25 17:47:35", "link": "http://arxiv.org/abs/2410.19720v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Counting Ability of Large Language Models and Impact of Tokenization", "abstract": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs.", "published": "2024-10-25 17:56:24", "link": "http://arxiv.org/abs/2410.19730v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Visual Dependency in Long-Context Reasoning for Large\n  Vision-Language Models", "abstract": "Large Vision-Language Models (LVLMs) excel in cross-model tasks but\nexperience performance declines in long-context reasoning due to overreliance\non textual information and reduced visual dependency. In this study, we\nempirically analyze LVLMs in long-context reasoning, revealing that increased\ncontext length leads to a higher dependence on language at the expense of\nvisual dependency. To address this issue, we propose a novel training-free\ncontext pruning method that selectively removes less critical textual\ninformation. Our approach enhances visual dependency and reduces textual noise,\nthereby improving LVLM performance in long-context reasoning. We validate our\nmethod by constructing a long-context dataset, demonstrating its effectiveness\nacross various LVLMs. Moreover, further analysis confirms the robustness of\ndifferent token pruning strategies and preliminary explores scaling laws\nbetween pruning rates and context length.", "published": "2024-10-25 17:59:09", "link": "http://arxiv.org/abs/2410.19732v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Ensembling Finetuned Language Models for Text Classification", "abstract": "Finetuning is a common practice widespread across different communities to\nadapt pretrained models to particular tasks. Text classification is one of\nthese tasks for which many pretrained models are available. On the other hand,\nensembles of neural networks are typically used to boost performance and\nprovide reliable uncertainty estimates. However, ensembling pretrained models\nfor text classification is not a well-studied avenue. In this paper, we present\na metadataset with predictions from five large finetuned models on six\ndatasets, and report results of different ensembling strategies from these\npredictions. Our results shed light on how ensembling can improve the\nperformance of finetuned text classifiers and incentivize future adoption of\nensembles in such tasks.", "published": "2024-10-25 09:15:54", "link": "http://arxiv.org/abs/2410.19889v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large\n  Language Models", "abstract": "Enhancing the reasoning capabilities of large language models (LLMs) is\ncrucial for enabling them to tackle complex, multi-step problems. Multi-agent\nframeworks have shown great potential in enhancing LLMs' reasoning\ncapabilities. However, the lack of effective cooperation between LLM agents\nhinders their performance, especially for multi-step reasoning tasks. This\npaper proposes a novel cooperative multi-agent reasoning framework (CoPlanner)\nby separating reasoning steps and assigning distinct duties to different\nagents. CoPlanner consists of two LLM agents: a planning agent and a reasoning\nagent. The planning agent provides high-level strategic hints, while the\nreasoning agent follows these hints and infers answers. By training the\nplanning agent's policy through the interactive reasoning process via Proximal\nPolicy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the\nprevious best method by 9.94\\% on LogiQA and 3.09\\% on BBH. Our results\ndemonstrate that the guidance from the planning agent and the effective\ncooperation between the agents contribute to the superior performance of\nCoPlanner in tackling multi-step reasoning problems.", "published": "2024-10-25 23:32:48", "link": "http://arxiv.org/abs/2410.20007v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Layer by Layer: Uncovering Where Multi-Task Learning Happens in\n  Instruction-Tuned Large Language Models", "abstract": "Fine-tuning pre-trained large language models (LLMs) on a diverse array of\ntasks has become a common approach for building models that can solve various\nnatural language processing (NLP) tasks. However, where and to what extent\nthese models retain task-specific knowledge remains largely unexplored. This\nstudy investigates the task-specific information encoded in pre-trained LLMs\nand the effects of instruction tuning on their representations across a diverse\nset of over 60 NLP tasks. We use a set of matrix analysis tools to examine the\ndifferences between the way pre-trained and instruction-tuned LLMs store\ntask-specific information. Our findings reveal that while some tasks are\nalready encoded within the pre-trained LLMs, others greatly benefit from\ninstruction tuning. Additionally, we pinpointed the layers in which the model\ntransitions from high-level general representations to more task-oriented\nrepresentations. This finding extends our understanding of the governing\nmechanisms of LLMs and facilitates future research in the fields of\nparameter-efficient transfer learning and multi-task learning.", "published": "2024-10-25 23:38:28", "link": "http://arxiv.org/abs/2410.20008v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for the Legal Domain: A Survey of Tasks,\n  Datasets, Models, and Challenges", "abstract": "Natural Language Processing (NLP) is revolutionising the way legal\nprofessionals and laypersons operate in the legal field. The considerable\npotential for NLP in the legal sector, especially in developing computational\ntools for various legal processes, has captured the interest of researchers for\nyears. This survey follows the Preferred Reporting Items for Systematic Reviews\nand Meta-Analyses framework, reviewing 154 studies, with a final selection of\n133 after manual filtering. It explores foundational concepts related to NLP in\nthe legal domain, illustrating the unique aspects and challenges of processing\nlegal texts, such as extensive document length, complex language, and limited\nopen legal datasets. We provide an overview of NLP tasks specific to legal\ntext, such as Legal Document Summarisation, legal Named Entity Recognition,\nLegal Question Answering, Legal Argument Mining, Legal Text Classification, and\nLegal Judgement Prediction. In the section on legal Language Models (LMs), we\nanalyse both developed LMs and approaches for adapting general LMs to the legal\ndomain. Additionally, we identify 16 Open Research Challenges, including bias\nin Artificial Intelligence applications, the need for more robust and\ninterpretable models, and improving explainability to handle the complexities\nof legal language and reasoning.", "published": "2024-10-25 01:17:02", "link": "http://arxiv.org/abs/2410.21306v2", "categories": ["cs.CL", "cs.AI", "A.1; I.2.7; J.1"], "primary_category": "cs.CL"}
{"title": "GraphLSS: Integrating Lexical, Structural, and Semantic Features for\n  Long Document Extractive Summarization", "abstract": "Heterogeneous graph neural networks have recently gained attention for long\ndocument summarization, modeling the extraction as a node classification task.\nAlthough effective, these models often require external tools or additional\nmachine learning models to define graph components, producing highly complex\nand less intuitive structures. We present GraphLSS, a heterogeneous graph\nconstruction for long document extractive summarization, incorporating Lexical,\nStructural, and Semantic features. It defines two levels of information (words\nand sentences) and four types of edges (sentence semantic similarity, sentence\noccurrence order, word in sentence, and word semantic similarity) without any\nneed for auxiliary learning models. Experiments on two benchmark datasets show\nthat GraphLSS is competitive with top-performing graph-based methods,\noutperforming recent non-graph models. We release our code on GitHub.", "published": "2024-10-25 23:48:59", "link": "http://arxiv.org/abs/2410.21315v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Stories Help LLMs Reason? Curating Information Space Through\n  Narrative", "abstract": "Narratives are widely recognized as a powerful tool for structuring\ninformation and facilitating comprehension of complex ideas in various domains\nsuch as science communication. This paper investigates whether incorporating\nnarrative elements can assist Large Language Models (LLMs) in solving complex\nproblems more effectively. We propose a novel approach, Story of Thought (SoT),\nintegrating narrative structures into prompting techniques for problem-solving.\nThis approach involves constructing narratives around problem statements and\ncreating a framework to identify and organize relevant information. Our\nexperiments show that using various LLMs with SoT consistently surpasses using\nthem with other techniques on physics, chemistry, math, and biology questions\nin both the GPQA and JEEBench datasets. The narrative-based information\ncuration process in SoT enhances problem comprehension by contextualizing\ncritical in-domain information and highlighting causal relationships within the\nproblem space.", "published": "2024-10-25 00:13:15", "link": "http://arxiv.org/abs/2410.19221v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors", "abstract": "The advent of large language models (LLMs) has revolutionized the field of\ntext generation, producing outputs that closely mimic human-like writing.\nAlthough academic and industrial institutions have developed detectors to\nprevent the malicious usage of LLM-generated texts, other research has doubt\nabout the robustness of these systems. To stress test these detectors, we\nintroduce a proxy-attack strategy that effortlessly compromises LLMs, causing\nthem to produce outputs that align with human-written text and mislead\ndetection systems. Our method attacks the source model by leveraging a\nreinforcement learning (RL) fine-tuned humanized small language model (SLM) in\nthe decoding phase. Through an in-depth analysis, we demonstrate that our\nattack strategy is capable of generating responses that are indistinguishable\nto detectors, preventing them from differentiating between machine-generated\nand human-written text. We conduct systematic evaluations on extensive datasets\nusing proxy-attacked open-source models, including Llama2-13B, Llama3-70B, and\nMixtral-8*7B in both white- and black-box settings. Our findings show that the\nproxy-attack strategy effectively deceives the leading detectors, resulting in\nan average AUROC drop of 70.4% across multiple datasets, with a maximum drop of\n90.3% on a single dataset. Furthermore, in cross-discipline scenarios, our\nstrategy also bypasses these detectors, leading to a significant relative\ndecrease of up to 90.9%, while in cross-language scenario, the drop reaches\n91.3%. Despite our proxy-attack strategy successfully bypassing the detectors\nwith such significant relative drops, we find that the generation quality of\nthe attacked models remains preserved, even within a modest utility budget,\nwhen compared to the text produced by the original, unattacked source model.", "published": "2024-10-25 00:35:00", "link": "http://arxiv.org/abs/2410.19230v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Two are better than one: Context window extension with multi-grained\n  self-injection", "abstract": "The limited context window of contemporary large language models (LLMs)\nremains a huge barrier to their broader application across various domains.\nWhile continual pre-training on long-context data is a straightforward and\neffective solution, it incurs substantial costs in terms of data acquisition\nand computational resources. To alleviate this issue, we propose SharedLLM, a\nnovel approach grounded in the design philosophy of multi-grained context\ncompression and query-aware information retrieval. SharedLLM is composed of two\nshort-context LLMs such as LLaMA-2, termed upper model and lower model. The\nlower model functions as a compressor while the upper model acts as a decoder.\nThe upper model receives compressed, multi-grained context information from the\nlower model and performs context-aware modeling on the running text.\nInformation transfer between the compressor and decoder occurs only at the\nlowest layers to refrain from long forward paths in the lower model and\nredundant cross-attention modules in the upper model. Based on this\narchitecture, we introduce a specialized tree-style data structure to\nefficiently encode, store and retrieve multi-grained contextual information for\ntext chunks. This structure, combined with a search algorithm, enables rapid\nencoding and retrieval of relevant information from various levels of the tree\nbased on the input query. This entire process, wherein the sender and receiver\nare derived from the same LLM layer, is referred to as self-injection.", "published": "2024-10-25 06:08:59", "link": "http://arxiv.org/abs/2410.19318v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions\n  for Conversational Search with LLMs", "abstract": "Generating diverse and effective clarifying questions is crucial for\nimproving query understanding and retrieval performance in open-domain\nconversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration,\nand evaluaTion of Clarifying Questions), an end-to-end LLM-based framework\naddressing the challenges of scalability and adaptability faced by existing\nmethods that rely on manual curation or template-based approaches. AGENT-CQ\nconsists of two stages: a generation stage employing LLM prompting strategies\nto generate clarifying questions, and an evaluation stage (CrowdLLM) that\nsimulates human crowdsourcing judgments using multiple LLM instances to assess\ngenerated questions and answers based on comprehensive quality metrics.\nExtensive experiments on the ClariQ dataset demonstrate CrowdLLM's\neffectiveness in evaluating question and answer quality. Human evaluation and\nCrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms\nbaselines in various aspects of question and answer quality. In retrieval-based\nevaluation, LLM-generated questions significantly enhance retrieval\neffectiveness for both BM25 and cross-encoder models compared to\nhuman-generated questions.", "published": "2024-10-25 17:06:27", "link": "http://arxiv.org/abs/2410.19692v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient\n  Finetuning of LLMs", "abstract": "Fine-tuning Large Language Models (LLMs) has become a crucial technique for\nadapting pre-trained models to downstream tasks. However, the enormous size of\nLLMs poses significant challenges in terms of computational complexity and\nresource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution. However, there exists a gap between the practical performance of\nlow-rank adaptations and its theoretical optimum. In this work, we propose\neXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this\ngap by leveraging the power of ensemble learning. Inspired by gradient\nboosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations\nto refine model predictions. It achieves better performance than the standard\nLoRA, while enjoying the computational efficiency of rank-1 adaptations. We\nprovide theoretical analysis to show the convergence and optimality of our\napproach, and conduct extensive experiments on a range of natural language\nprocessing tasks. The results demonstrate that XGBLoRA consistently outperforms\nstandard LoRA and achieves performance comparable to full fine-tuning with\nsignificantly fewer trainable parameters. This work advances\nparameter-efficient fine-tuning for LLMs, and offers a promising solution for\nadapting LLMs to downstream tasks while optimizing performance and efficiency.", "published": "2024-10-25 17:07:13", "link": "http://arxiv.org/abs/2410.19694v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IPPON: Common Sense Guided Informative Path Planning for Object Goal\n  Navigation", "abstract": "Navigating efficiently to an object in an unexplored environment is a\ncritical skill for general-purpose intelligent robots. Recent approaches to\nthis object goal navigation problem have embraced a modular strategy,\nintegrating classical exploration algorithms-notably frontier exploration-with\na learned semantic mapping/exploration module. This paper introduces a novel\ninformative path planning and 3D object probability mapping approach. The\nmapping module computes the probability of the object of interest through\nsemantic segmentation and a Bayes filter. Additionally, it stores probabilities\nfor common objects, which semantically guides the exploration based on common\nsense priors from a large language model. The planner terminates when the\ncurrent viewpoint captures enough voxels identified with high confidence as the\nobject of interest. Although our planner follows a zero-shot approach, it\nachieves state-of-the-art performance as measured by the Success weighted by\nPath Length (SPL) and Soft SPL in the Habitat ObjectNav Challenge 2023,\noutperforming other works by more than 20%. Furthermore, we validate its\neffectiveness on real robots. Project webpage: https://ippon-paper.github.io/", "published": "2024-10-25 17:11:33", "link": "http://arxiv.org/abs/2410.19697v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing,\n  Neural-Conditioning, Expert Swarms, and Task Planning", "abstract": "Financial intelligence generation from vast data sources has typically relied\non traditional methods of knowledge-graph construction or database engineering.\nRecently, fine-tuned financial domain-specific Large Language Models (LLMs),\nhave emerged. While these advancements are promising, limitations such as high\ninference costs, hallucinations, and the complexity of concurrently analyzing\nhigh-dimensional financial data, emerge. This motivates our invention FISHNET\n(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,\nExpert swarming, and Task planning), an agentic architecture that accomplishes\nhighly complex analytical tasks for more than 98,000 regulatory filings that\nvary immensely in terms of semantics, data hierarchy, or format. FISHNET shows\nremarkable performance for financial insight generation (61.8% success rate\nover 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to\nempirically prove the success of FISHNET, each agent's importance, and the\noptimized performance of assembling all agents. Our modular architecture can be\nleveraged for a myriad of use-cases, enabling scalability, flexibility, and\ndata integrity that are critical for financial tasks.", "published": "2024-10-25 17:53:47", "link": "http://arxiv.org/abs/2410.19727v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Improving Multimodal Large Language Models Using Continual Learning", "abstract": "Generative large language models (LLMs) exhibit impressive capabilities,\nwhich can be further augmented by integrating a pre-trained vision model into\nthe original LLM to create a multimodal LLM (MLLM). However, this integration\noften significantly decreases performance on natural language understanding and\ngeneration tasks, compared to the original LLM. This study investigates this\nissue using the LLaVA MLLM, treating the integration as a continual learning\nproblem. We evaluate five continual learning methods to mitigate forgetting and\nidentify a technique that enhances visual understanding while minimizing\nlinguistic performance loss. Our approach reduces linguistic performance\ndegradation by up to 15\\% over the LLaVA recipe, while maintaining high\nmultimodal accuracy. We also demonstrate the robustness of our method through\ncontinual learning on a sequence of vision-language tasks, effectively\npreserving linguistic skills while acquiring new multimodal capabilities.", "published": "2024-10-25 18:50:40", "link": "http://arxiv.org/abs/2410.19925v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Discrete Self-Supervised Representations of Speech Capture Tone\n  Distinctions?", "abstract": "Discrete representations of speech, obtained from Self-Supervised Learning\n(SSL) foundation models, are widely used, especially where there are limited\ndata for the downstream task, such as for a low-resource language. Typically,\ndiscretization of speech into a sequence of symbols is achieved by unsupervised\nclustering of the latents from an SSL model. Our study evaluates whether\ndiscrete symbols - found using k-means - adequately capture tone in two example\nlanguages, Mandarin and Yoruba. We compare latent vectors with discrete\nsymbols, obtained from HuBERT base, MandarinHuBERT, or XLS-R, for vowel and\ntone classification. We find that using discrete symbols leads to a substantial\nloss of tone information, even for language-specialised SSL models. We suggest\nthat discretization needs to be task-aware, particularly for tone-dependent\ndownstream tasks.", "published": "2024-10-25 19:13:25", "link": "http://arxiv.org/abs/2410.19935v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction", "abstract": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)", "published": "2024-10-25 19:18:22", "link": "http://arxiv.org/abs/2410.19937v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Evaluating Cost-Accuracy Trade-offs in Multimodal Search Relevance\n  Judgements", "abstract": "Large Language Models (LLMs) have demonstrated potential as effective search\nrelevance evaluators. However, there is a lack of comprehensive guidance on\nwhich models consistently perform optimally across various contexts or within\nspecific use cases. In this paper, we assess several LLMs and Multimodal\nLanguage Models (MLLMs) in terms of their alignment with human judgments across\nmultiple multimodal search scenarios. Our analysis investigates the trade-offs\nbetween cost and accuracy, highlighting that model performance varies\nsignificantly depending on the context. Interestingly, in smaller models, the\ninclusion of a visual component may hinder performance rather than enhance it.\nThese findings highlight the complexities involved in selecting the most\nappropriate model for practical applications.", "published": "2024-10-25 21:29:04", "link": "http://arxiv.org/abs/2410.19974v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "$\\texttt{PatentAgent}$: Intelligent Agent for Automated Pharmaceutical\n  Patent Analysis", "abstract": "Pharmaceutical patents play a vital role in biochemical industries,\nespecially in drug discovery, providing researchers with unique early access to\ndata, experimental results, and research insights. With the advancement of\nmachine learning, patent analysis has evolved from manual labor to tasks\nassisted by automatic tools. However, there still lacks an unified agent that\nassists every aspect of patent analysis, from patent reading to core chemical\nidentification. Leveraging the capabilities of Large Language Models (LLMs) to\nunderstand requests and follow instructions, we introduce the $\\textbf{first}$\nintelligent agent in this domain, $\\texttt{PatentAgent}$, poised to advance and\npotentially revolutionize the landscape of pharmaceutical research.\n$\\texttt{PatentAgent}$ comprises three key end-to-end modules --\n$\\textit{PA-QA}$, $\\textit{PA-Img2Mol}$, and $\\textit{PA-CoreId}$ -- that\nrespectively perform (1) patent question-answering, (2)\nimage-to-molecular-structure conversion, and (3) core chemical structure\nidentification, addressing the essential needs of scientists and practitioners\nin pharmaceutical patent analysis. Each module of $\\texttt{PatentAgent}$\ndemonstrates significant effectiveness with the updated algorithm and the\nsynergistic design of $\\texttt{PatentAgent}$ framework. $\\textit{PA-Img2Mol}$\noutperforms existing methods across CLEF, JPO, UOB, and USPTO patent benchmarks\nwith an accuracy gain between 2.46% and 8.37% while $\\textit{PA-CoreId}$\nrealizes accuracy improvement ranging from 7.15% to 7.62% on PatentNetML\nbenchmark. Our code and dataset will be publicly available.", "published": "2024-10-25 19:15:08", "link": "http://arxiv.org/abs/2410.21312v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Decoding Diffusion: A Scalable Framework for Unsupervised Analysis of\n  Latent Space Biases and Representations Using Natural Language Prompts", "abstract": "Recent advances in image generation have made diffusion models powerful tools\nfor creating high-quality images. However, their iterative denoising process\nmakes understanding and interpreting their semantic latent spaces more\nchallenging than other generative models, such as GANs. Recent methods have\nattempted to address this issue by identifying semantically meaningful\ndirections within the latent space. However, they often need manual\ninterpretation or are limited in the number of vectors that can be trained,\nrestricting their scope and utility. This paper proposes a novel framework for\nunsupervised exploration of diffusion latent spaces. We directly leverage\nnatural language prompts and image captions to map latent directions. This\nmethod allows for the automatic understanding of hidden features and supports a\nbroader range of analysis without the need to train specific vectors. Our\nmethod provides a more scalable and interpretable understanding of the semantic\nknowledge encoded within diffusion models, facilitating comprehensive analysis\nof latent biases and the nuanced representations these models learn.\nExperimental results show that our framework can uncover hidden patterns and\nassociations in various domains, offering new insights into the\ninterpretability of diffusion model latent spaces.", "published": "2024-10-25 21:44:51", "link": "http://arxiv.org/abs/2410.21314v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPT-4o System Card", "abstract": "GPT-4o is an autoregressive omni model that accepts as input any combination\nof text, audio, image, and video, and generates any combination of text, audio,\nand image outputs. It's trained end-to-end across text, vision, and audio,\nmeaning all inputs and outputs are processed by the same neural network. GPT-4o\ncan respond to audio inputs in as little as 232 milliseconds, with an average\nof 320 milliseconds, which is similar to human response time in conversation.\nIt matches GPT-4 Turbo performance on text in English and code, with\nsignificant improvement on text in non-English languages, while also being much\nfaster and 50\\% cheaper in the API. GPT-4o is especially better at vision and\naudio understanding compared to existing models. In line with our commitment to\nbuilding AI safely and consistent with our voluntary commitments to the White\nHouse, we are sharing the GPT-4o System Card, which includes our Preparedness\nFramework evaluations. In this System Card, we provide a detailed look at\nGPT-4o's capabilities, limitations, and safety evaluations across multiple\ncategories, focusing on speech-to-speech while also evaluating text and image\ncapabilities, and measures we've implemented to ensure the model is safe and\naligned. We also include third-party assessments on dangerous capabilities, as\nwell as discussion of potential societal impacts of GPT-4o's text and vision\ncapabilities.", "published": "2024-10-25 17:43:01", "link": "http://arxiv.org/abs/2410.21276v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CloserMusicDB: A Modern Multipurpose Dataset of High Quality Music", "abstract": "In this paper, we introduce CloserMusicDB, a collection of full length studio\nquality tracks annotated by a team of human experts. We describe the selected\nqualities of our dataset, along with three example tasks possible to perform\nusing this dataset: hook detection, contextual tagging and artist\nidentification. We conduct baseline experiments and provide initial benchmarks\nfor these tasks.", "published": "2024-10-25 13:11:19", "link": "http://arxiv.org/abs/2410.19540v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mask-Weighted Spatial Likelihood Coding for Speaker-Independent Joint\n  Localization and Mask Estimation", "abstract": "Due to their robustness and flexibility, neural-driven beamformers are a\npopular choice for speech separation in challenging environments with a varying\namount of simultaneous speakers alongside noise and reverberation.\nTime-frequency masks and relative directions of the speakers regarding a fixed\nspatial grid can be used to estimate the beamformer's parameters. To some\ndegree, speaker-independence is achieved by ensuring a greater amount of\nspatial partitions than speech sources. In this work, we analyze how to encode\nboth mask and positioning into such a grid to enable joint estimation of both\nquantities. We propose mask-weighted spatial likelihood coding and show that it\nachieves considerable performance in both tasks compared to baseline encodings\noptimized for either localization or mask estimation. In the same setup, we\ndemonstrate superiority for joint estimation of both quantities. Conclusively,\nwe propose a universal approach which can replace an upstream sound source\nlocalization system solely by adapting the training framework, making it highly\nrelevant in performance-critical scenarios.", "published": "2024-10-25 14:43:32", "link": "http://arxiv.org/abs/2410.19595v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Arabic Music Classification and Generation using Deep Learning", "abstract": "This paper proposes a machine learning approach for classifying classical and\nnew Egyptian music by composer and generating new similar music. The proposed\nsystem utilizes a convolutional neural network (CNN) for classification and a\nCNN autoencoder for generation. The dataset used in this project consists of\nnew and classical Egyptian music pieces composed by different composers.\n  To classify the music by composer, each sample is normalized and transformed\ninto a mel spectrogram. The CNN model is trained on the dataset using the mel\nspectrograms as input features and the composer labels as output classes. The\nmodel achieves 81.4\\% accuracy in classifying the music by composer,\ndemonstrating the effectiveness of the proposed approach.\n  To generate new music similar to the original pieces, a CNN autoencoder is\ntrained on a similar dataset. The model is trained to encode the mel\nspectrograms of the original pieces into a lower-dimensional latent space and\nthen decode them back into the original mel spectrogram. The generated music is\nproduced by sampling from the latent space and decoding the samples back into\nmel spectrograms, which are then transformed into audio.\n  In conclusion, the proposed system provides a promising approach to\nclassifying and generating classical Egyptian music, which can be applied in\nvarious musical applications, such as music recommendation systems, music\nproduction, and music education.", "published": "2024-10-25 17:47:08", "link": "http://arxiv.org/abs/2410.19719v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporal Convolution-based Hybrid Model Approach with Representation\n  Learning for Real-Time Acoustic Anomaly Detection", "abstract": "The early detection of potential failures in industrial machinery components\nis paramount for ensuring the reliability and safety of operations, thereby\npreserving Machine Condition Monitoring (MCM). This research addresses this\nimperative by introducing an innovative approach to Real-Time Acoustic Anomaly\nDetection. Our method combines semi-supervised temporal convolution with\nrepresentation learning and a hybrid model strategy with Temporal Convolutional\nNetworks (TCN) to handle various intricate anomaly patterns found in acoustic\ndata effectively. The proposed model demonstrates superior performance compared\nto established research in the field, underscoring the effectiveness of this\napproach. Not only do we present quantitative evidence of its superiority, but\nwe also employ visual representations, such as t-SNE plots, to further\nsubstantiate the model's efficacy.", "published": "2024-10-25 17:50:48", "link": "http://arxiv.org/abs/2410.19722v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
