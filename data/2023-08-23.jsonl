{"title": "Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient\n  Greek Literature", "abstract": "Intertextual allusions hold a pivotal role in Classical Philology, with Latin\nauthors frequently referencing Ancient Greek texts. Until now, the automatic\nidentification of these intertextual references has been constrained to\nmonolingual approaches, seeking parallels solely within Latin or Greek texts.\nIn this study, we introduce SPhilBERTa, a trilingual Sentence-RoBERTa model\ntailored for Classical Philology, which excels at cross-lingual semantic\ncomprehension and identification of identical sentences across Ancient Greek,\nLatin, and English. We generate new training data by automatically translating\nEnglish texts into Ancient Greek. Further, we present a case study,\ndemonstrating SPhilBERTa's capability to facilitate automated detection of\nintertextual parallels. Our models and resources are available at\nhttps://github.com/Heidelberg-NLP/ancient-language-models.", "published": "2023-08-23 08:54:05", "link": "http://arxiv.org/abs/2308.12008v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Reranking Passages with Coarse-to-Fine Neural Retriever Enhanced by\n  List-Context Information", "abstract": "Passage reranking is a critical task in various applications, particularly\nwhen dealing with large volumes of documents. Existing neural architectures\nhave limitations in retrieving the most relevant passage for a given question\nbecause the semantics of the segmented passages are often incomplete, and they\ntypically match the question to each passage individually, rarely considering\ncontextual information from other passages that could provide comparative and\nreference information. This paper presents a list-context attention mechanism\nto augment the passage representation by incorporating the list-context\ninformation from other candidates. The proposed coarse-to-fine (C2F) neural\nretriever addresses the out-of-memory limitation of the passage attention\nmechanism by dividing the list-context modeling process into two sub-processes\nwith a cache policy learning algorithm, enabling the efficient encoding of\ncontext information from a large number of candidate answers. This method can\nbe generally used to encode context information from any number of candidate\nanswers in one pass. Different from most multi-stage information retrieval\narchitectures, this model integrates the coarse and fine rankers into the joint\noptimization process, allowing for feedback between the two layers to update\nthe model simultaneously. Experiments demonstrate the effectiveness of the\nproposed approach.", "published": "2023-08-23 09:29:29", "link": "http://arxiv.org/abs/2308.12022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-injected Prompt Learning for Chinese Biomedical Entity\n  Normalization", "abstract": "The Biomedical Entity Normalization (BEN) task aims to align raw,\nunstructured medical entities to standard entities, thus promoting data\ncoherence and facilitating better downstream medical applications. Recently,\nprompt learning methods have shown promising results in this task. However,\nexisting research falls short in tackling the more complex Chinese BEN task,\nespecially in the few-shot scenario with limited medical data, and the vast\npotential of the external medical knowledge base has yet to be fully harnessed.\nTo address these challenges, we propose a novel Knowledge-injected Prompt\nLearning (PL-Knowledge) method. Specifically, our approach consists of five\nstages: candidate entity matching, knowledge extraction, knowledge encoding,\nknowledge injection, and prediction output. By effectively encoding the\nknowledge items contained in medical entities and incorporating them into our\ntailor-made knowledge-injected templates, the additional knowledge enhances the\nmodel's ability to capture latent relationships between medical entities, thus\nachieving a better match with the standard entities. We extensively evaluate\nour model on a benchmark dataset in both few-shot and full-scale scenarios. Our\nmethod outperforms existing baselines, with an average accuracy boost of\n12.96\\% in few-shot and 0.94\\% in full-data cases, showcasing its excellence in\nthe BEN task.", "published": "2023-08-23 09:32:40", "link": "http://arxiv.org/abs/2308.12025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data\n  Selection for Instruction Tuning", "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction\ndata quality and quantity is a focal point. Recognizing this, we introduce a\nself-guided methodology for LLMs to autonomously discern and select cherry\nsamples from open-source datasets, effectively minimizing manual curation and\npotential cost for instruction tuning an LLM. Our key innovation, the\nInstruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to\nidentify discrepancies between a model's expected responses and its intrinsic\ngeneration capability. Through the application of IFD, cherry samples can be\npinpointed, leading to a marked uptick in model training efficiency. Empirical\nvalidations on datasets like Alpaca and WizardLM underpin our findings; with a\nmere $10\\%$ of original data input, our strategy showcases improved results.\nThis synthesis of self-guided cherry-picking and the IFD metric signifies a\ntransformative leap in the instruction tuning of LLMs, promising both\nefficiency and resource-conscious advancements. Codes, data, and models are\navailable: https://github.com/tianyi-lab/Cherry_LLM", "published": "2023-08-23 09:45:29", "link": "http://arxiv.org/abs/2308.12032v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction Position Matters in Sequence Generation with Large Language\n  Models", "abstract": "Large language models (LLMs) are capable of performing conditional sequence\ngeneration tasks, such as translation or summarization, through instruction\nfine-tuning. The fine-tuning data is generally sequentially concatenated from a\nspecific task instruction, an input sentence, and the corresponding response.\nConsidering the locality modeled by the self-attention mechanism of LLMs, these\nmodels face the risk of instruction forgetting when generating responses for\nlong input sentences. To mitigate this issue, we propose enhancing the\ninstruction-following capability of LLMs by shifting the position of task\ninstructions after the input sentences. Theoretical analysis suggests that our\nstraightforward method can alter the model's learning focus, thereby\nemphasizing the training of instruction-following capabilities. Concurrently,\nexperimental results demonstrate that our approach consistently outperforms\ntraditional settings across various model scales (1B / 7B / 13B) and different\nsequence generation tasks (translation and summarization), without any\nadditional data or annotation costs. Notably, our method significantly improves\nthe zero-shot performance on conditional sequence generation, e.g., up to 9.7\nBLEU points on WMT zero-shot translation tasks.", "published": "2023-08-23 12:36:57", "link": "http://arxiv.org/abs/2308.12097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt2Model: Generating Deployable Models from Natural Language\n  Instructions", "abstract": "Large language models (LLMs) enable system builders today to create competent\nNLP systems through prompting, where they only need to describe the task in\nnatural language and provide a few examples. However, in other ways, LLMs are a\nstep backward from traditional special-purpose NLP models; they require\nextensive computational resources for deployment and can be gated behind APIs.\nIn this paper, we propose Prompt2Model, a general-purpose method that takes a\nnatural language task description like the prompts provided to LLMs, and uses\nit to train a special-purpose model that is conducive to deployment. This is\ndone through a multi-step process of retrieval of existing datasets and\npretrained models, dataset generation using LLMs, and supervised fine-tuning on\nthese retrieved and generated datasets. Over three tasks, we demonstrate that\ngiven the same few-shot prompt as input, Prompt2Model trains models that\noutperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%\nwhile being up to 700 times smaller. We also show that this data can be used to\nobtain reliable performance estimates of model performance, enabling model\ndevelopers to assess model reliability before deployment. Prompt2Model is\navailable open-source at https://github.com/neulab/prompt2model.", "published": "2023-08-23 17:28:21", "link": "http://arxiv.org/abs/2308.12261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap: Deciphering Tabular Data Using Large Language Model", "abstract": "In the realm of natural language processing, the understanding of tabular\ndata has perpetually stood as a focal point of scholarly inquiry. The emergence\nof expansive language models, exemplified by the likes of ChatGPT, has ushered\nin a wave of endeavors wherein researchers aim to harness these models for\ntasks related to table-based question answering. Central to our investigative\npursuits is the elucidation of methodologies that amplify the aptitude of such\nlarge language models in discerning both the structural intricacies and\ninherent content of tables, ultimately facilitating their capacity to provide\ninformed responses to pertinent queries. To this end, we have architected a\ndistinctive module dedicated to the serialization of tables for seamless\nintegration with expansive language models. Additionally, we've instituted a\ncorrective mechanism within the model to rectify potential inaccuracies.\nExperimental results indicate that, although our proposed method trails the\nSOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about\n1.2% in tests on specific datasets. This research marks the first application\nof large language models to table-based question answering tasks, enhancing the\nmodel's comprehension of both table structures and content.", "published": "2023-08-23 03:38:21", "link": "http://arxiv.org/abs/2308.11891v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations", "abstract": "Building socialbots that can have deep, engaging open-domain conversations\nwith humans is one of the grand challenges of artificial intelligence (AI). To\nthis end, bots need to be able to leverage world knowledge spanning several\ndomains effectively when conversing with humans who have their own world\nknowledge. Existing knowledge-grounded conversation datasets are primarily\nstylized with explicit roles for conversation partners. These datasets also do\nnot explore depth or breadth of topical coverage with transitions in\nconversations. We introduce Topical-Chat, a knowledge-grounded human-human\nconversation dataset where the underlying knowledge spans 8 broad topics and\nconversation partners don't have explicitly defined roles, to help further\nresearch in open-domain conversational AI. We also train several\nstate-of-the-art encoder-decoder conversational models on Topical-Chat and\nperform automated and human evaluation for benchmarking.", "published": "2023-08-23 08:33:14", "link": "http://arxiv.org/abs/2308.11995v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine", "abstract": "As an effective tool for eliciting the power of Large Language Models (LLMs),\nprompting has recently demonstrated unprecedented abilities across a variety of\ncomplex tasks. To further improve the performance, prompt ensemble has\nattracted substantial interest for tackling the hallucination and instability\nof LLMs. However, existing methods usually adopt a two-stage paradigm, which\nrequires a pre-prepared set of prompts with substantial manual effort, and is\nunable to perform directed optimization for different weak learners. In this\npaper, we propose a simple, universal, and automatic method named PREFER (Pompt\nEnsemble learning via Feedback-Reflect-Refine) to address the stated\nlimitations. Specifically, given the fact that weak learners are supposed to\nfocus on hard examples during boosting, PREFER builds a feedback mechanism for\nreflecting on the inadequacies of existing weak learners. Based on this, the\nLLM is required to automatically synthesize new prompts for iterative\nrefinement. Moreover, to enhance stability of the prompt effect evaluation, we\npropose a novel prompt bagging method involving forward and backward thinking,\nwhich is superior to majority voting and is beneficial for both feedback and\nweight calculation in boosting. Extensive experiments demonstrate that our\nPREFER achieves state-of-the-art performance in multiple types of tasks by a\nsignificant margin. We have made our code publicly available.", "published": "2023-08-23 09:46:37", "link": "http://arxiv.org/abs/2308.12033v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across\n  Languages", "abstract": "Recently there has been a significant surge in multimodal learning in terms\nof both image-to-text and text-to-image generation. However, the success is\ntypically limited to English, leaving other languages largely behind. Building\na competitive counterpart in other languages is highly challenging due to the\nlow-resource nature of non-English multimodal data (i.e., lack of large-scale,\nhigh-quality image-text data). In this work, we propose MPM, an effective\ntraining paradigm for training large multimodal models in non-English\nlanguages. MPM demonstrates that Multilingual language models can Pivot\nzero-shot Multimodal learning across languages. Specifically, based on a strong\nmultilingual large language model, multimodal models pretrained on English-only\nimage-text data can well generalize to other languages in a (quasi)-zero-shot\nmanner, even surpassing models trained on image-text data in native languages.\nTaking Chinese as a practice of MPM, we build large multimodal models VisCPM in\nimage-to-text and text-to-image generation, which achieve state-of-the-art\n(open-source) performance in Chinese. To facilitate future research, we\nopen-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.", "published": "2023-08-23 09:55:41", "link": "http://arxiv.org/abs/2308.12038v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep\n  Learning Track", "abstract": "Large-scale text retrieval technology has been widely used in various\npractical business scenarios. This paper presents our systems for the TREC 2022\nDeep Learning Track. We explain the hybrid text retrieval and multi-stage text\nranking method adopted in our solution. The retrieval stage combined the two\nstructures of traditional sparse retrieval and neural dense retrieval. In the\nranking stage, in addition to the full interaction-based ranking model built on\nlarge pre-trained language model, we also proposes a lightweight sub-ranking\nmodule to further enhance the final text ranking performance. Evaluation\nresults demonstrate the effectiveness of our proposed approach. Our models\nachieve the 1st and 4th rank on the test set of passage ranking and document\nranking respectively.", "published": "2023-08-23 09:56:59", "link": "http://arxiv.org/abs/2308.12039v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Aligning Language Models with Offline Learning from Human Feedback", "abstract": "Learning from human preferences is crucial for language models (LMs) to\neffectively cater to human needs and societal values. Previous research has\nmade notable progress by leveraging human feedback to follow instructions.\nHowever, these approaches rely primarily on online learning techniques like\nProximal Policy Optimization (PPO), which have been proven unstable and\nchallenging to tune for language models. Moreover, PPO requires complex\ndistributed system implementation, hindering the efficiency of large-scale\ndistributed training. In this study, we propose an offline learning from human\nfeedback framework to align LMs without interacting with environments.\nSpecifically, we explore filtering alignment (FA), reward-weighted regression\n(RWR), and conditional alignment (CA) to align language models to human\npreferences. By employing a loss function similar to supervised fine-tuning,\nour methods ensure more stable model training than PPO with a simple machine\nlearning system~(MLSys) and much fewer (around 9\\%) computing resources.\nExperimental results demonstrate that conditional alignment outperforms other\noffline alignment methods and is comparable to PPO.", "published": "2023-08-23 10:41:07", "link": "http://arxiv.org/abs/2308.12050v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base\n  Question Answering", "abstract": "Knowledge base question answering (KBQA) is a critical yet challenging task\ndue to the vast number of entities within knowledge bases and the diversity of\nnatural language questions posed by users. Unfortunately, the performance of\nmost KBQA models tends to decline significantly in real-world scenarios where\nhigh-quality annotated data is insufficient. To mitigate the burden associated\nwith manual annotation, we introduce FlexKBQA by utilizing Large Language\nModels (LLMs) as program translators for addressing the challenges inherent in\nthe few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms\nto sample diverse programs, such as SPARQL queries, from the knowledge base,\nwhich are subsequently converted into natural language questions via LLMs. This\nsynthetic dataset facilitates training a specialized lightweight model for the\nKB. Additionally, to reduce the barriers of distribution shift between\nsynthetic data and real user questions, FlexKBQA introduces an executionguided\nself-training method to iterative leverage unlabeled user questions.\nFurthermore, we explore harnessing the inherent reasoning capability of LLMs to\nenhance the entire framework. Consequently, FlexKBQA delivers substantial\nflexibility, encompassing data annotation, deployment, and being domain\nagnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we\nobserve that under the few-shot even the more challenging zero-shot scenarios,\nFlexKBQA achieves impressive results with a few annotations, surpassing all\nprevious baselines and even approaching the performance of supervised models,\nachieving a remarkable 93% performance relative to the fully-supervised models.\nWe posit that FlexKBQA represents a significant advancement towards exploring\nbetter integration of large and lightweight models. The code is open-sourced.", "published": "2023-08-23 11:00:36", "link": "http://arxiv.org/abs/2308.12060v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Change Detection for the Romanian Language", "abstract": "Automatic semantic change methods try to identify the changes that appear\nover time in the meaning of words by analyzing their usage in diachronic\ncorpora. In this paper, we analyze different strategies to create static and\ncontextual word embedding models, i.e., Word2Vec and ELMo, on real-world\nEnglish and Romanian datasets. To test our pipeline and determine the\nperformance of our models, we first evaluate both word embedding models on an\nEnglish dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a\nRomanian dataset, and we underline different aspects of semantic changes in\nthis low-resource language, such as meaning acquisition and loss. The\nexperimental results show that, depending on the corpus, the most important\nfactors to consider are the choice of model and the distance to calculate a\nscore for detecting semantic change.", "published": "2023-08-23 13:37:02", "link": "http://arxiv.org/abs/2308.12131v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Faithfulness Using the Longest Supported Subsequence", "abstract": "As increasingly sophisticated language models emerge, their trustworthiness\nbecomes a pivotal issue, especially in tasks such as summarization and\nquestion-answering. Ensuring their responses are contextually grounded and\nfaithful is challenging due to the linguistic diversity and the myriad of\npossible answers. In this paper, we introduce a novel approach to evaluate\nfaithfulness of machine-generated text by computing the longest noncontinuous\nsubstring of the claim that is supported by the context, which we refer to as\nthe Longest Supported Subsequence (LSS). Using a new human-annotated dataset,\nwe finetune a model to generate LSS. We introduce a new method of evaluation\nand demonstrate that these metrics correlate better with human ratings when LSS\nis employed, as opposed to when it is not. Our proposed metric demonstrates an\n18% enhancement over the prevailing state-of-the-art metric for faithfulness on\nour dataset. Our metric consistently outperforms other metrics on a\nsummarization dataset across six different models. Finally, we compare several\npopular Large Language Models (LLMs) for faithfulness using this metric. We\nrelease the human-annotated dataset built for predicting LSS and our fine-tuned\nmodel for evaluating faithfulness.", "published": "2023-08-23 14:18:44", "link": "http://arxiv.org/abs/2308.12157v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Curriculum Learning with Adam: The Devil Is in the Wrong Details", "abstract": "Curriculum learning (CL) posits that machine learning models -- similar to\nhumans -- may learn more efficiently from data that match their current\nlearning progress. However, CL methods are still poorly understood and, in\nparticular for natural language processing (NLP), have achieved only limited\nsuccess. In this paper, we explore why. Starting from an attempt to replicate\nand extend a number of recent curriculum methods, we find that their results\nare surprisingly brittle when applied to NLP. A deep dive into the\n(in)effectiveness of the curricula in some scenarios shows us why: when\ncurricula are employed in combination with the popular Adam optimisation\nalgorithm, they oftentimes learn to adapt to suboptimally chosen optimisation\nparameters for this algorithm. We present a number of different case studies\nwith different common hand-crafted and automated CL approaches to illustrate\nthis phenomenon, and we find that none of them outperforms optimisation with\nonly Adam with well-chosen hyperparameters. As such, our results contribute to\nunderstanding why CL methods work, but at the same time urge caution when\nclaiming positive results.", "published": "2023-08-23 15:39:42", "link": "http://arxiv.org/abs/2308.12202v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How to Protect Copyright Data in Optimization of Large Language Models?", "abstract": "Large language models (LLMs) and generative AI have played a transformative\nrole in computer research and applications. Controversy has arisen as to\nwhether these models output copyrighted data, which can occur if the data the\nmodels are trained on is copyrighted. LLMs are built on the transformer neural\nnetwork architecture, which in turn relies on a mathematical computation called\nAttention that uses the softmax function.\n  In this paper, we show that large language model training and optimization\ncan be seen as a softmax regression problem. We then establish a method of\nefficiently performing softmax regression, in a way that prevents the\nregression function from generating copyright data. This establishes a\ntheoretical method of training large language models in a way that avoids\ngenerating copyright data.", "published": "2023-08-23 16:48:04", "link": "http://arxiv.org/abs/2308.12247v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Simple is Better and Large is Not Enough: Towards Ensembling of\n  Foundational Language Models", "abstract": "Foundational Language Models (FLMs) have advanced natural language processing\n(NLP) research. Current researchers are developing larger FLMs (e.g., XLNet,\nT5) to enable contextualized language representation, classification, and\ngeneration. While developing larger FLMs has been of significant advantage, it\nis also a liability concerning hallucination and predictive uncertainty.\nFundamentally, larger FLMs are built on the same foundations as smaller FLMs\n(e.g., BERT); hence, one must recognize the potential of smaller FLMs which can\nbe realized through an ensemble. In the current research, we perform a reality\ncheck on FLMs and their ensemble on benchmark and real-world datasets. We\nhypothesize that the ensembling of FLMs can influence the individualistic\nattention of FLMs and unravel the strength of coordination and cooperation of\ndifferent FLMs. We utilize BERT and define three other ensemble techniques:\n{Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a\nknowledge-guided reinforcement learning approach. We discovered that the\nsuggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by\na factor of many times using datasets that show the usefulness of NLP in\nsensitive fields, such as mental health.", "published": "2023-08-23 17:40:35", "link": "http://arxiv.org/abs/2308.12272v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vision Transformer Adapters for Generalizable Multitask Learning", "abstract": "We introduce the first multitasking vision transformer adapters that learn\ngeneralizable task affinities which can be applied to novel tasks and domains.\nIntegrated into an off-the-shelf vision transformer backbone, our adapters can\nsimultaneously solve multiple dense vision tasks in a parameter-efficient\nmanner, unlike existing multitasking transformers that are parametrically\nexpensive. In contrast to concurrent methods, we do not require retraining or\nfine-tuning whenever a new task or domain is added. We introduce a task-adapted\nattention mechanism within our adapter framework that combines gradient-based\ntask similarities with attention-based ones. The learned task affinities\ngeneralize to the following settings: zero-shot task transfer, unsupervised\ndomain adaptation, and generalization without fine-tuning to novel domains. We\ndemonstrate that our approach outperforms not only the existing convolutional\nneural network-based multitasking methods but also the vision transformer-based\nones. Our project page is at \\url{https://ivrl.github.io/VTAGML}.", "published": "2023-08-23 18:40:48", "link": "http://arxiv.org/abs/2308.12372v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Toward American Sign Language Processing in the Real World: Data, Tasks,\n  and Methods", "abstract": "Sign language, which conveys meaning through gestures, is the chief means of\ncommunication among deaf people. Recognizing sign language in natural settings\npresents significant challenges due to factors such as lighting, background\nclutter, and variations in signer characteristics. In this thesis, I study\nautomatic sign language processing in the wild, using signing videos collected\nfrom the Internet. This thesis contributes new datasets, tasks, and methods.\nMost chapters of this thesis address tasks related to fingerspelling, an\nimportant component of sign language and yet has not been studied widely by\nprior work. I present three new large-scale ASL datasets in the wild:\nChicagoFSWild, ChicagoFSWild+, and OpenASL. Using ChicagoFSWild and\nChicagoFSWild+, I address fingerspelling recognition, which consists of\ntranscribing fingerspelling sequences into text. I propose an end-to-end\napproach based on iterative attention that allows recognition from a raw video\nwithout explicit hand detection. I further show that using a Conformer-based\nnetwork jointly modeling handshape and mouthing can bring performance close to\nthat of humans. Next, I propose two tasks for building real-world\nfingerspelling-based applications: fingerspelling detection and search. For\nfingerspelling detection, I introduce a suite of evaluation metrics and a new\ndetection model via multi-task training. To address the problem of searching\nfor fingerspelled keywords in raw sign language videos, we propose a novel\nmethod that jointly localizes and matches fingerspelling segments to text.\nFinally, I will describe a benchmark for large-vocabulary open-domain sign\nlanguage translation based on OpenASL. To address the challenges of sign\nlanguage translation in realistic settings, we propose a set of techniques\nincluding sign search as a pretext task for pre-training and fusion of mouthing\nand handshape features.", "published": "2023-08-23 20:38:19", "link": "http://arxiv.org/abs/2308.12419v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cabrita: closing the gap for foreign languages", "abstract": "The strategy of training the model from scratch in a specific language or\ndomain serves two essential purposes: i) enhancing performance in the\nparticular linguistic or domain context, and ii) ensuring effective\ntokenization. The main limitation inherent to this approach lies in the\nassociated cost, which can reach six to seven-digit dollar values, depending on\nthe model size and the number of parameters involved.\n  The main solution to overcome the cost challenge is to rely on available\npre-trained models, which, despite recent advancements such as the LLaMA and\nLLaMA-2 models, still demonstrate inefficiency for certain specific domain\nproblems or prove ineffective in scenarios involving conversational memory\nresources, given the large number of tokens required to represent text.\n  To overcome this issue, we present a methodology named Cabrita, which, as our\nresearch demonstrates, successfully addresses the performance and efficient\ntokenization problem, all at an affordable cost. We believe that this\nmethodology can be applied to any transformer-like architecture model. To\nvalidate the study, we conducted continuous pre-training exclusively using\nPortuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in\na model named openCabrita 3B. The openCabrita 3B also features a new tokenizer\nthat results in a significant reduction in the number of tokens required to\nrepresent the text. In our assessment, for few-shot learning tasks, we achieved\nsimilar results with this 3B model compared to a traditional continuous\npre-training approach as well as to 7B models English pre-trained models.", "published": "2023-08-23 02:49:35", "link": "http://arxiv.org/abs/2308.11878v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio Difference Captioning Utilizing Similarity-Discrepancy\n  Disentanglement", "abstract": "We proposed Audio Difference Captioning (ADC) as a new extension task of\naudio captioning for describing the semantic differences between input pairs of\nsimilar but slightly different audio clips. The ADC solves the problem that\nconventional audio captioning sometimes generates similar captions for similar\naudio clips, failing to describe the difference in content. We also propose a\ncross-attention-concentrated transformer encoder to extract differences by\ncomparing a pair of audio clips and a similarity-discrepancy disentanglement to\nemphasize the difference in the latent space. To evaluate the proposed methods,\nwe built an AudioDiffCaps dataset consisting of pairs of similar but slightly\ndifferent audio clips with human-annotated descriptions of their differences.\nThe experiment with the AudioDiffCaps dataset showed that the proposed methods\nsolve the ADC task effectively and improve the attention weights to extract the\ndifference by visualizing them in the transformer encoder.", "published": "2023-08-23 05:13:25", "link": "http://arxiv.org/abs/2308.11923v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Generation with Multiple Conditional Diffusion Model", "abstract": "Text-based audio generation models have limitations as they cannot encompass\nall the information in audio, leading to restricted controllability when\nrelying solely on text. To address this issue, we propose a novel model that\nenhances the controllability of existing pre-trained text-to-audio models by\nincorporating additional conditions including content (timestamp) and style\n(pitch contour and energy contour) as supplements to the text. This approach\nachieves fine-grained control over the temporal order, pitch, and energy of\ngenerated audio. To preserve the diversity of generation, we employ a trainable\ncontrol condition encoder that is enhanced by a large language model and a\ntrainable Fusion-Net to encode and fuse the additional conditions while keeping\nthe weights of the pre-trained text-to-audio model frozen. Due to the lack of\nsuitable datasets and evaluation metrics, we consolidate existing datasets into\na new dataset comprising the audio and corresponding conditions and use a\nseries of evaluation metrics to evaluate the controllability performance.\nExperimental results demonstrate that our model successfully achieves\nfine-grained control to accomplish controllable audio generation. Audio samples\nand our dataset are publicly available at\nhttps://conditionaudiogen.github.io/conditionaudiogen/", "published": "2023-08-23 06:21:46", "link": "http://arxiv.org/abs/2308.11940v4", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EVE: Efficient Vision-Language Pre-training with Masked Prediction and\n  Modality-Aware MoE", "abstract": "Building scalable vision-language models to learn from diverse, multimodal\ndata remains an open challenge. In this paper, we introduce an Efficient\nVision-languagE foundation model, namely EVE, which is one unified multimodal\nTransformer pre-trained solely by one unified pre-training task. Specifically,\nEVE encodes both vision and language within a shared Transformer network\nintegrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which\ncapture modality-specific information by selectively switching to different\nexperts. To unify pre-training tasks of vision and language, EVE performs\nmasked signal modeling on image-text pairs to reconstruct masked signals, i.e.,\nimage pixels and text tokens, given visible signals. This simple yet effective\npre-training objective accelerates training by 3.5x compared to the model\npre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing\nto the combination of the unified architecture and pre-training task, EVE is\neasy to scale up, enabling better downstream performance with fewer resources\nand faster training speed. Despite its simplicity, EVE achieves\nstate-of-the-art performance on various vision-language downstream tasks,\nincluding visual question answering, visual reasoning, and image-text\nretrieval.", "published": "2023-08-23 07:36:30", "link": "http://arxiv.org/abs/2308.11971v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "From Instructions to Intrinsic Human Values -- A Survey of Alignment\n  Goals for Big Models", "abstract": "Big models, exemplified by Large Language Models (LLMs), are models typically\npre-trained on massive data and comprised of enormous parameters, which not\nonly obtain significantly improved performance across diverse tasks but also\npresent emergent capabilities absent in smaller models. However, the growing\nintertwining of big models with everyday human lives poses potential risks and\nmight cause serious social harm. Therefore, many efforts have been made to\nalign LLMs with humans to make them better follow user instructions and satisfy\nhuman preferences. Nevertheless, `what to align with' has not been fully\ndiscussed, and inappropriate alignment goals might even backfire. In this\npaper, we conduct a comprehensive survey of different alignment goals in\nexisting work and trace their evolution paths to help identify the most\nessential goal. Particularly, we investigate related works from two\nperspectives: the definition of alignment goals and alignment evaluation. Our\nanalysis encompasses three distinct levels of alignment goals and reveals a\ngoal transformation from fundamental abilities to value orientation, indicating\nthe potential of intrinsic human values as the alignment goal for enhanced\nLLMs. Based on such results, we further discuss the challenges of achieving\nsuch intrinsic value alignment and provide a collection of available resources\nfor future research on the alignment of big models.", "published": "2023-08-23 09:11:13", "link": "http://arxiv.org/abs/2308.12014v2", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Prompt-Based Length Controlled Generation with Reinforcement Learning", "abstract": "Large language models (LLMs) like ChatGPT and GPT-4 have attracted great\nattention given their surprising performance on a wide range of NLP tasks.\nLength controlled generation of LLMs emerges as an important topic, which\nenables users to fully leverage the capability of LLMs in more real-world\nscenarios like generating a proper answer or essay of a desired length. In\naddition, the autoregressive generation in LLMs is extremely time-consuming,\nwhile the ability of controlling this generated length can reduce the inference\ncost by limiting the length. Therefore, we propose a prompt-based length\ncontrol method to achieve high-accuracy length controlled generation. In\nparticular, we adopt reinforcement learning with the reward signal given by\neither trainable or rule-based reward models, which further enhances the\nlength-control ability of LLMs by rewarding outputs that follows pre-defined\ncontrol instruction. To enable rule-based inference, we also introduce standard\nprompt extractor to collect the standard control information from users' input.\nExperiments show that our method significantly improves the accuracy of\nprompt-based length control for summarization task on popular datasets like\nCNNDM and NYT. Both the standard prompt extractor and the RL-tuned model have\nshow strong generalization ability to unseen control prompt templates.", "published": "2023-08-23 09:43:10", "link": "http://arxiv.org/abs/2308.12030v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IncreLoRA: Incremental Parameter Allocation Method for\n  Parameter-Efficient Fine-tuning", "abstract": "With the increasing size of pre-trained language models (PLMs), fine-tuning\nall the parameters in the model is not efficient, especially when there are a\nlarge number of downstream tasks, which incur significant training and storage\ncosts. Many parameter-efficient fine-tuning (PEFT) approaches have been\nproposed, among which, Low-Rank Adaptation (LoRA) is a representative approach\nthat injects trainable rank decomposition matrices into every target module.\nYet LoRA ignores the importance of parameters in different modules. To address\nthis problem, many works have been proposed to prune the parameters of LoRA.\nHowever, under limited training conditions, the upper bound of the rank of the\npruned parameter matrix is still affected by the preset values. We, therefore,\npropose IncreLoRA, an incremental parameter allocation method that adaptively\nadds trainable parameters during training based on the importance scores of\neach module. This approach is different from the pruning method as it is not\nlimited by the initial number of training parameters, and each parameter matrix\nhas a higher rank upper bound for the same training overhead. We conduct\nextensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA.\nThe results show that our method owns higher parameter efficiency, especially\nwhen under the low-resource settings where our method significantly outperforms\nthe baselines. Our code is publicly available.", "published": "2023-08-23 10:08:10", "link": "http://arxiv.org/abs/2308.12043v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CgT-GAN: CLIP-guided Text GAN for Image Captioning", "abstract": "The large-scale visual-language pre-trained model, Contrastive Language-Image\nPre-training (CLIP), has significantly improved image captioning for scenarios\nwithout human-annotated image-caption pairs. Recent advanced CLIP-based image\ncaptioning without human annotations follows a text-only training paradigm,\ni.e., reconstructing text from shared embedding space. Nevertheless, these\napproaches are limited by the training/inference gap or huge storage\nrequirements for text embeddings. Given that it is trivial to obtain images in\nthe real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates\nimages into the training process to enable the model to \"see\" real visual\nmodality. Particularly, we use adversarial training to teach CgT-GAN to mimic\nthe phrases of an external text corpus and CLIP-based reward to provide\nsemantic guidance. The caption generator is jointly rewarded based on the\ncaption naturalness to human language calculated from the GAN's discriminator\nand the semantic guidance reward computed by the CLIP-based reward module. In\naddition to the cosine similarity as the semantic guidance reward (i.e.,\nCLIP-cos), we further introduce a novel semantic guidance reward called\nCLIP-agg, which aligns the generated caption with a weighted text embedding by\nattentively aggregating the entire corpus. Experimental results on three\nsubtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms\nstate-of-the-art methods significantly across all metrics. Code is available at\nhttps://github.com/Lihr747/CgtGAN.", "published": "2023-08-23 10:25:37", "link": "http://arxiv.org/abs/2308.12045v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4", "abstract": "Multimodal large language models are typically trained in two stages: first\npre-training on image-text pairs, and then fine-tuning using supervised\nvision-language instruction data. Recent studies have shown that large language\nmodels can achieve satisfactory results even with a limited amount of\nhigh-quality instruction-following data. In this paper, we introduce\nInstructionGPT-4, which is fine-tuned on a small dataset comprising only 200\nexamples, amounting to approximately 6\\% of the instruction-following data used\nin the alignment dataset for MiniGPT-4. To achieve this, we first propose\nseveral metrics to access the quality of multimodal instruction data. Based on\nthese metrics, we present an effective and trainable data selector to\nautomatically identify and filter low-quality vision-language data. By\nemploying this method, InstructionGPT-4 outperforms the original MiniGPT-4 on\nvarious evaluations. Overall, our findings demonstrate that less but\nhigh-quality instruction tuning data is efficient in enabling multimodal large\nlanguage models to generate better output. Our code is available at\nhttps://github.com/waltonfuture/InstructionGPT-4.", "published": "2023-08-23 11:27:30", "link": "http://arxiv.org/abs/2308.12067v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Out of the Cage: How Stochastic Parrots Win in Cyber Security\n  Environments", "abstract": "Large Language Models (LLMs) have gained widespread popularity across diverse\ndomains involving text generation, summarization, and various natural language\nprocessing tasks. Despite their inherent limitations, LLM-based designs have\nshown promising capabilities in planning and navigating open-world scenarios.\nThis paper introduces a novel application of pre-trained LLMs as agents within\ncybersecurity network environments, focusing on their utility for sequential\ndecision-making processes.\n  We present an approach wherein pre-trained LLMs are leveraged as attacking\nagents in two reinforcement learning environments. Our proposed agents\ndemonstrate similar or better performance against state-of-the-art agents\ntrained for thousands of episodes in most scenarios and configurations. In\naddition, the best LLM agents perform similarly to human testers of the\nenvironment without any additional training process. This design highlights the\npotential of LLMs to efficiently address complex decision-making tasks within\ncybersecurity.\n  Furthermore, we introduce a new network security environment named\nNetSecGame. The environment is designed to eventually support complex\nmulti-agent scenarios within the network security domain. The proposed\nenvironment mimics real network attacks and is designed to be highly modular\nand adaptable for various scenarios.", "published": "2023-08-23 12:11:27", "link": "http://arxiv.org/abs/2308.12086v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "SoK: Machine Learning for Misinformation Detection", "abstract": "We examine the disconnect between scholarship and practice in applying\nmachine learning to trust and safety problems, using misinformation detection\nas a case study. We survey literature on automated detection of misinformation\nacross a corpus of 248 well-cited papers in the field. We then examine subsets\nof papers for data and code availability, design missteps, reproducibility, and\ngeneralizability. Our paper corpus includes published work in security, natural\nlanguage processing, and computational social science. Across these disparate\ndisciplines, we identify common errors in dataset and method design. In\ngeneral, detection tasks are often meaningfully distinct from the challenges\nthat online services actually face. Datasets and model evaluation are often\nnon-representative of real-world contexts, and evaluation frequently is not\nindependent of model training. We demonstrate the limitations of current\ndetection methods in a series of three representative replication studies.\nBased on the results of these analyses and our literature survey, we conclude\nthat the current state-of-the-art in fully-automated misinformation detection\nhas limited efficacy in detecting human-generated misinformation. We offer\nrecommendations for evaluating applications of machine learning to trust and\nsafety problems and recommend future directions for research.", "published": "2023-08-23 15:52:20", "link": "http://arxiv.org/abs/2308.12215v4", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Diffusion Language Models Can Perform Many Tasks with Scaling and\n  Instruction-Finetuning", "abstract": "The recent surge of generative AI has been fueled by the generative power of\ndiffusion probabilistic models and the scalable capabilities of large language\nmodels. Despite their potential, it remains elusive whether diffusion language\nmodels can solve general language tasks comparable to their autoregressive\ncounterparts. This paper demonstrates that scaling diffusion models w.r.t.\ndata, sizes, and tasks can effectively make them strong language learners. We\nbuild competent diffusion language models at scale by first acquiring knowledge\nfrom massive data via masked language modeling pretraining thanks to their\nintrinsic connections. We then reprogram pretrained masked language models into\ndiffusion language models via diffusive adaptation, wherein task-specific\nfinetuning and instruction finetuning are explored to unlock their versatility\nin solving general language tasks. Experiments show that scaling diffusion\nlanguage models consistently improves performance across downstream language\ntasks. We further discover that instruction finetuning can elicit zero-shot and\nfew-shot in-context learning abilities that help tackle many unseen tasks by\nfollowing natural language instructions, and show promise in advanced and\nchallenging abilities such as reasoning.", "published": "2023-08-23 16:01:12", "link": "http://arxiv.org/abs/2308.12219v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "D4: Improving LLM Pretraining via Document De-Duplication and\n  Diversification", "abstract": "Over recent years, an increasing amount of compute and data has been poured\ninto training large language models (LLMs), usually by doing one-pass learning\non as many tokens as possible randomly selected from large-scale web corpora.\nWhile training on ever-larger portions of the internet leads to consistent\nperformance improvements, the size of these improvements diminishes with scale,\nand there has been little work exploring the effect of data selection on\npre-training and downstream performance beyond simple de-duplication methods\nsuch as MinHash. Here, we show that careful data selection (on top of\nde-duplicated data) via pre-trained model embeddings can speed up training (20%\nefficiency gains) and improves average downstream accuracy on 16 NLP tasks (up\nto 2%) at the 6.7B model scale. Furthermore, we show that repeating data\nintelligently consistently outperforms baseline training (while repeating\nrandom data performs worse than baseline training). Our results indicate that\nclever data selection can significantly improve LLM pre-training, calls into\nquestion the common practice of training for a single epoch on as much data as\npossible, and demonstrates a path to keep improving our models past the limits\nof randomly sampling web data.", "published": "2023-08-23 17:58:14", "link": "http://arxiv.org/abs/2308.12284v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "With a Little Help from your own Past: Prototypical Memory Networks for\n  Image Captioning", "abstract": "Image captioning, like many tasks involving vision and language, currently\nrelies on Transformer-based architectures for extracting the semantics in an\nimage and translating it into linguistically coherent descriptions. Although\nsuccessful, the attention operator only considers a weighted summation of\nprojections of the current input sample, therefore ignoring the relevant\nsemantic information which can come from the joint observation of other\nsamples. In this paper, we devise a network which can perform attention over\nactivations obtained while processing other training samples, through a\nprototypical memory model. Our memory models the distribution of past keys and\nvalues through the definition of prototype vectors which are both\ndiscriminative and compact. Experimentally, we assess the performance of the\nproposed model on the COCO dataset, in comparison with carefully designed\nbaselines and state-of-the-art approaches, and by investigating the role of\neach of the proposed components. We demonstrate that our proposal can increase\nthe performance of an encoder-decoder Transformer by 3.7 CIDEr points both when\ntraining in cross-entropy only and when fine-tuning with self-critical sequence\ntraining. Source code and trained models are available at:\nhttps://github.com/aimagelab/PMA-Net.", "published": "2023-08-23 18:53:00", "link": "http://arxiv.org/abs/2308.12383v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature", "abstract": "Emerging technologies, such as Distributed Ledger Technology (DLT), face\ngrowing scrutiny for their environmental impact, especially when it comes to\nthe energy use of the Proof of Work (PoW) consensus mechanism and broader\nEnvironmental, Social, and Governance (ESG) considerations. Yet, much of the\nexisting systematic literature reviews of DLT rely on the limited analyses of\ncitations, abstracts, and keywords, failing to fully capture the field's\ncomplexity and ESG concerns.\n  To address these challenges, we analyze the full text of 24,539 publications\nusing Natural Language Processing (NLP) with our manually labeled Named Entity\nRecognition (NER) dataset of 39,427 entities for DLT. This method identifies\n505 key publications connecting DLT and ESG domains, providing a more\ncomprehensive and nuanced understanding of the field.\n  Our combined NLP and temporal graph analysis reveals critical trends in DLT\nevolution and ESG impacts, including the pivotal role of research in\ncryptography and peer-to-peer networks, Bitcoin's persistent impact on research\nand environmental concerns (a \"Lindy effect\"), Ethereum's influence on Proof of\nStake (PoS) and smart contracts adoption, and a shift towards energy-efficient\nconsensus mechanisms. Our contributions include the first DLT-specific NER\ndataset, addressing the scarcity of high-quality labeled NLP data for\nblockchain research; a methodology integrating NLP and temporal graph analysis\nfor interdisciplinary literature review at large scale; and the first\nNLP-driven DLT literature review emphasizing ESG aspects.", "published": "2023-08-23 20:42:32", "link": "http://arxiv.org/abs/2308.12420v3", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis", "abstract": "Since the introduction of ChatGPT and GPT-4, these models have been tested\nacross a large number of tasks. Their adeptness across domains is evident, but\ntheir aptitude in playing games, and specifically their aptitude in the realm\nof poker has remained unexplored. Poker is a game that requires decision making\nunder uncertainty and incomplete information. In this paper, we put ChatGPT and\nGPT-4 through the poker test and evaluate their poker skills. Our findings\nreveal that while both models display an advanced understanding of poker,\nencompassing concepts like the valuation of starting hands, playing positions\nand other intricacies of game theory optimal (GTO) poker, both ChatGPT and\nGPT-4 are NOT game theory optimal poker players.\n  Profitable strategies in poker are evaluated in expectations over large\nsamples. Through a series of experiments, we first discover the characteristics\nof optimal prompts and model parameters for playing poker with these models.\nOur observations then unveil the distinct playing personas of the two models.\nWe first conclude that GPT-4 is a more advanced poker player than ChatGPT. This\nexploration then sheds light on the divergent poker tactics of the two models:\nChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker\nvernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which\nmeans that it has a propensity to only engage with premium hands and folds a\nmajority of hands. When subjected to the same directive, GPT-4 plays like a\nmaniac, showcasing a loose and aggressive style of play. Both strategies,\nalthough relatively advanced, are not game theory optimal.", "published": "2023-08-23 23:16:35", "link": "http://arxiv.org/abs/2308.12466v2", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL"}
{"title": "Diagnosing Infeasible Optimization Problems Using Large Language Models", "abstract": "Decision-making problems can be represented as mathematical optimization\nmodels, finding wide applications in fields such as economics, engineering and\nmanufacturing, transportation, and health care. Optimization models are\nmathematical abstractions of the problem of making the best decision while\nsatisfying a set of requirements or constraints. One of the primary barriers to\ndeploying these models in practice is the challenge of helping practitioners\nunderstand and interpret such models, particularly when they are infeasible,\nmeaning no decision satisfies all the constraints. Existing methods for\ndiagnosing infeasible optimization models often rely on expert systems,\nnecessitating significant background knowledge in optimization. In this paper,\nwe introduce OptiChat, a first-of-its-kind natural language-based system\nequipped with a chatbot GUI for engaging in interactive conversations about\ninfeasible optimization models. OptiChat can provide natural language\ndescriptions of the optimization model itself, identify potential sources of\ninfeasibility, and offer suggestions to make the model feasible. The\nimplementation of OptiChat is built on GPT-4, which interfaces with an\noptimization solver to identify the minimal subset of constraints that render\nthe entire optimization problem infeasible, also known as the Irreducible\nInfeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought,\nkey-retrieve, and sentiment prompts to enhance OptiChat's reliability. Our\nexperiments demonstrate that OptiChat assists both expert and non-expert users\nin improving their understanding of the optimization models, enabling them to\nquickly identify the sources of infeasibility.", "published": "2023-08-23 04:34:05", "link": "http://arxiv.org/abs/2308.12923v1", "categories": ["cs.HC", "cs.CL", "cs.LG", "math.OC"], "primary_category": "cs.HC"}
{"title": "Vulnerability Clustering and other Machine Learning Applications of\n  Semantic Vulnerability Embeddings", "abstract": "Cyber-security vulnerabilities are usually published in form of short natural\nlanguage descriptions (e.g., in form of MITRE's CVE list) that over time are\nfurther manually enriched with labels such as those defined by the Common\nVulnerability Scoring System (CVSS). In the Vulnerability AI (Analytics and\nIntelligence) project, we investigated different types of semantic\nvulnerability embeddings based on natural language processing (NLP) techniques\nto obtain a concise representation of the vulnerability space. We also\nevaluated their use as a foundation for machine learning applications that can\nsupport cyber-security researchers and analysts in risk assessment and other\nrelated activities. The particular applications we explored and briefly\nsummarize in this report are clustering, classification, and visualization, as\nwell as a new logic-based approach to evaluate theories about the vulnerability\nspace.", "published": "2023-08-23 21:39:48", "link": "http://arxiv.org/abs/2310.05935v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "CED: Consistent ensemble distillation for audio tagging", "abstract": "Augmentation and knowledge distillation (KD) are well-established techniques\nemployed in audio classification tasks, aimed at enhancing performance and\nreducing model sizes on the widely recognized Audioset (AS) benchmark. Although\nboth techniques are effective individually, their combined use, called\nconsistent teaching, hasn't been explored before. This paper proposes CED, a\nsimple training framework that distils student models from large teacher\nensembles with consistent teaching. To achieve this, CED efficiently stores\nlogits as well as the augmentation methods on disk, making it scalable to\nlarge-scale datasets. Central to CED's efficacy is its label-free nature,\nmeaning that only the stored logits are used for the optimization of a student\nmodel only requiring 0.3\\% additional disk space for AS. The study trains\nvarious transformer-based models, including a 10M parameter model achieving a\n49.0 mean average precision (mAP) on AS. Pretrained models and code are\navailable at https://github.com/RicherMans/CED.", "published": "2023-08-23 06:57:00", "link": "http://arxiv.org/abs/2308.11957v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint Prediction of Audio Event and Annoyance Rating in an Urban\n  Soundscape by Hierarchical Graph Representation Learning", "abstract": "Sound events in daily life carry rich information about the objective world.\nThe composition of these sounds affects the mood of people in a soundscape.\nMost previous approaches only focus on classifying and detecting audio events\nand scenes, but may ignore their perceptual quality that may impact humans'\nlistening mood for the environment, e.g. annoyance. To this end, this paper\nproposes a novel hierarchical graph representation learning (HGRL) approach\nwhich links objective audio events (AE) with subjective annoyance ratings (AR)\nof the soundscape perceived by humans. The hierarchical graph consists of\nfine-grained event (fAE) embeddings with single-class event semantics,\ncoarse-grained event (cAE) embeddings with multi-class event semantics, and AR\nembeddings. Experiments show the proposed HGRL successfully integrates AE with\nAR for AEC and ARP tasks, while coordinating the relations between cAE and fAE\nand further aligning the two different grains of AE information with the AR.", "published": "2023-08-23 08:05:10", "link": "http://arxiv.org/abs/2308.11980v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analysis of XLS-R for Speech Quality Assessment", "abstract": "In online conferencing applications, estimating the perceived quality of an\naudio signal is crucial to ensure high quality of experience for the end user.\nThe most reliable way to assess the quality of a speech signal is through human\njudgments in the form of the mean opinion score (MOS) metric. However, such an\napproach is labor intensive and not feasible for large-scale applications. The\nfocus has therefore shifted towards automated speech quality assessment through\nend-to-end training of deep neural networks. Recently, it was shown that\nleveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art\nperformance for the task of speech quality prediction. In this paper, we\nperform an in-depth analysis of the pre-trained model. First, we analyze the\nperformance of embeddings extracted from each layer of XLS-R and also for each\nsize of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal\nregions for feature extraction: one in the lower-level features and one in the\nhigh-level features. Next, we investigate the reason for the two distinct\noptima. We hypothesize that the lower-level features capture characteristics of\nnoise and room acoustics, whereas the high-level features focus on speech\ncontent and intelligibility. To investigate this, we analyze the sensitivity of\nthe MOS predictions with respect to different levels of corruption in each\ncategory. Afterwards, we try fusing the two optimal feature depths to determine\nif they contain complementary information for MOS prediction. Finally, we\ncompare the performance of the proposed models and assess the generalizability\nof the models on unseen datasets.", "published": "2023-08-23 11:52:49", "link": "http://arxiv.org/abs/2308.12077v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Example-Based Framework for Perceptually Guided Audio Texture Generation", "abstract": "Controllable generation using StyleGANs is usually achieved by training the\nmodel using labeled data. For audio textures, however, there is currently a\nlack of large semantically labeled datasets. Therefore, to control generation,\nwe develop a method for semantic control over an unconditionally trained\nStyleGAN in the absence of such labeled datasets. In this paper, we propose an\nexample-based framework to determine guidance vectors for audio texture\ngeneration based on user-defined semantic attributes. Our approach leverages\nthe semantically disentangled latent space of an unconditionally trained\nStyleGAN. By using a few synthetic examples to indicate the presence or absence\nof a semantic attribute, we infer the guidance vectors in the latent space of\nthe StyleGAN to control that attribute during generation. Our results show that\nour framework can find user-defined and perceptually relevant guidance vectors\nfor controllable generation for audio textures. Furthermore, we demonstrate an\napplication of our framework to other tasks, such as selective semantic\nattribute transfer.", "published": "2023-08-23 01:29:46", "link": "http://arxiv.org/abs/2308.11859v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "KinSPEAK: Improving speech recognition for Kinyarwanda via\n  semi-supervised learning methods", "abstract": "Despite recent availability of large transcribed Kinyarwanda speech data,\nachieving robust speech recognition for Kinyarwanda is still challenging. In\nthis work, we show that using self-supervised pre-training, following a simple\ncurriculum schedule during fine-tuning and using semi-supervised learning to\nleverage large unlabelled speech data significantly improve speech recognition\nperformance for Kinyarwanda. Our approach focuses on using public domain data\nonly. A new studio-quality speech dataset is collected from a public website,\nthen used to train a clean baseline model. The clean baseline model is then\nused to rank examples from a more diverse and noisy public dataset, defining a\nsimple curriculum training schedule. Finally, we apply semi-supervised learning\nto label and learn from large unlabelled data in five successive generations.\nOur final model achieves 3.2% word error rate (WER) on the new dataset and\n15.6% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the\nbest of our knowledge. Our experiments also indicate that using syllabic rather\nthan character-based tokenization results in better speech recognition\nperformance for Kinyarwanda.", "published": "2023-08-23 01:44:28", "link": "http://arxiv.org/abs/2308.11863v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "I.2.6"], "primary_category": "eess.AS"}
{"title": "AdVerb: Visually Guided Audio Dereverberation", "abstract": "We present AdVerb, a novel audio-visual dereverberation framework that uses\nvisual cues in addition to the reverberant sound to estimate clean audio.\nAlthough audio-only dereverberation is a well-studied problem, our approach\nincorporates the complementary visual modality to perform audio\ndereverberation. Given an image of the environment where the reverberated sound\nsignal has been recorded, AdVerb employs a novel geometry-aware cross-modal\ntransformer architecture that captures scene geometry and audio-visual\ncross-modal relationship to generate a complex ideal ratio mask, which, when\napplied to the reverberant audio predicts the clean sound. The effectiveness of\nour method is demonstrated through extensive quantitative and qualitative\nevaluations. Our approach significantly outperforms traditional audio-only and\naudio-visual baselines on three downstream tasks: speech enhancement, speech\nrecognition, and speaker verification, with relative improvements in the range\nof 18% - 82% on the LibriSpeech test-clean set. We also achieve highly\nsatisfactory RT60 error scores on the AVSpeech dataset.", "published": "2023-08-23 18:20:59", "link": "http://arxiv.org/abs/2308.12370v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "An Initial Exploration: Learning to Generate Realistic Audio for Silent\n  Video", "abstract": "Generating realistic audio effects for movies and other media is a\nchallenging task that is accomplished today primarily through physical\ntechniques known as Foley art. Foley artists create sounds with common objects\n(e.g., boxing gloves, broken glass) in time with video as it is playing to\ngenerate captivating audio tracks. In this work, we aim to develop a\ndeep-learning based framework that does much the same - observes video in it's\nnatural sequence and generates realistic audio to accompany it. Notably, we\nhave reason to believe this is achievable due to advancements in realistic\naudio generation techniques conditioned on other inputs (e.g., Wavenet\nconditioned on text). We explore several different model architectures to\naccomplish this task that process both previously-generated audio and video\ncontext. These include deep-fusion CNN, dilated Wavenet CNN with visual\ncontext, and transformer-based architectures. We find that the\ntransformer-based architecture yields the most promising results, matching\nlow-frequencies to visual patterns effectively, but failing to generate more\nnuanced waveforms.", "published": "2023-08-23 20:08:56", "link": "http://arxiv.org/abs/2308.12408v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LCANets++: Robust Audio Classification using Multi-layer Neural Networks\n  with Lateral Competition", "abstract": "Audio classification aims at recognizing audio signals, including speech\ncommands or sound events. However, current audio classifiers are susceptible to\nperturbations and adversarial attacks. In addition, real-world audio\nclassification tasks often suffer from limited labeled data. To help bridge\nthese gaps, previous work developed neuro-inspired convolutional neural\nnetworks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)\nin the first layer (i.e., LCANets) for computer vision. LCANets learn in a\ncombination of supervised and unsupervised learning, reducing dependency on\nlabeled samples. Motivated by the fact that auditory cortex is also sparse, we\nextend LCANets to audio recognition tasks and introduce LCANets++, which are\nCNNs that perform sparse coding in multiple layers via LCA. We demonstrate that\nLCANets++ are more robust than standard CNNs and LCANets against perturbations,\ne.g., background noise, as well as black-box and white-box attacks, e.g.,\nevasion and fast gradient sign (FGSM) attacks.", "published": "2023-08-23 17:42:00", "link": "http://arxiv.org/abs/2308.12882v2", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
