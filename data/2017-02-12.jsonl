{"title": "Vector Embedding of Wikipedia Concepts and Entities", "abstract": "Using deep learning for different machine learning tasks such as image\nclassification and word embedding has recently gained many attentions. Its\nappealing performance reported across specific Natural Language Processing\n(NLP) tasks in comparison with other approaches is the reason for its\npopularity. Word embedding is the task of mapping words or phrases to a low\ndimensional numerical vector. In this paper, we use deep learning to embed\nWikipedia Concepts and Entities. The English version of Wikipedia contains more\nthan five million pages, which suggest its capability to cover many English\nEntities, Phrases, and Concepts. Each Wikipedia page is considered as a\nconcept. Some concepts correspond to entities, such as a person's name, an\norganization or a place. Contrary to word embedding, Wikipedia Concepts\nEmbedding is not ambiguous, so there are different vectors for concepts with\nsimilar surface form but different mentions. We proposed several approaches and\nevaluated their performance based on Concept Analogy and Concept Similarity\ntasks. The results show that proposed approaches have the performance\ncomparable and in some cases even higher than the state-of-the-art methods.", "published": "2017-02-12 00:23:04", "link": "http://arxiv.org/abs/1702.03470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Parse and Translate Improves Neural Machine Translation", "abstract": "There has been relatively little attention to incorporating linguistic prior\nto neural machine translation. Much of the previous work was further\nconstrained to considering linguistic prior on the source side. In this paper,\nwe propose a hybrid model, called NMT+RNNG, that learns to parse and translate\nby combining the recurrent neural network grammar into the attention-based\nneural machine translation. Our approach encourages the neural machine\ntranslation model to incorporate linguistic prior during training, and lets it\ntranslate on its own afterward. Extensive experiments with four language pairs\nshow the effectiveness of the proposed NMT+RNNG.", "published": "2017-02-12 13:19:03", "link": "http://arxiv.org/abs/1702.03525v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
