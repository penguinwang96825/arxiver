{"title": "Learning to Simplify with Data Hopelessly Out of Alignment", "abstract": "We consider whether it is possible to do text simplification without relying\non a \"parallel\" corpus, one that is made up of sentence-by-sentence alignments\nof complex and ground truth simple sentences. To this end, we introduce a\nnumber of concepts, some new and some not, including what we call Conjoined\nTwin Networks, Flip-Flop Auto-Encoders (FFA) and Adversarial Networks (GAN). A\ncomparison is made between Jensen-Shannon (JS-GAN) and Wasserstein GAN, to see\nhow they impact performance, with stronger results for the former. An\nexperiment we conducted with a large dataset derived from Wikipedia found the\nsolid superiority of Twin Networks equipped with FFA and JS-GAN, over the\ncurrent best performing system. Furthermore, we discuss where we stand in a\nrelation to fully supervised methods in the past literature, and highlight with\nexamples qualitative differences that exist among simplified sentences\ngenerated by supervision-free systems.", "published": "2022-04-02 02:09:25", "link": "http://arxiv.org/abs/2204.00741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment\n  Analysis", "abstract": "As an extensive research in the field of natural language processing (NLP),\naspect-based sentiment analysis (ABSA) is the task of predicting the sentiment\nexpressed in a text relative to the corresponding aspect. Unfortunately, most\nlanguages lack sufficient annotation resources, thus more and more recent\nresearchers focus on cross-lingual aspect-based sentiment analysis (XABSA).\nHowever, most recent researches only concentrate on cross-lingual data\nalignment instead of model alignment. To this end, we propose a novel\nframework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based\nSentiment Analysis. Based on contrastive learning, we close the distance\nbetween samples with the same label in different semantic spaces, thus\nachieving a convergence of semantic spaces of different languages.\nSpecifically, we design two contrastive strategies, token level contrastive\nlearning of token embeddings (TL-CTE) and sentiment level contrastive learning\nof token embeddings (SL-CTE), to regularize the semantic space of source and\ntarget language to be more uniform. Since our framework can receive datasets in\nmultiple languages during training, our framework can be adapted not only for\nXABSA task but also for multilingual aspect-based sentiment analysis (MABSA).\nTo further improve the performance of our model, we perform knowledge\ndistillation technology leveraging data from unlabeled target language. In the\ndistillation XABSA task, we further explore the comparative effectiveness of\ndifferent data (source dataset, translated dataset, and code-switched dataset).\nThe results demonstrate that the proposed method has a certain improvement in\nthe three tasks of XABSA, distillation XABSA and MABSA. For reproducibility,\nour code for this paper is available at https://github.com/GKLMIP/CL-XABSA.", "published": "2022-04-02 07:40:03", "link": "http://arxiv.org/abs/2204.00791v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity\n  Recognition", "abstract": "Cross-lingual Named Entity Recognition (NER) has recently become a research\nhotspot because it can alleviate the data-hungry problem for low-resource\nlanguages. However, few researches have focused on the scenario where the\nsource-language labeled data is also limited in some specific domains. A common\napproach for this scenario is to generate more training data through\ntranslation or generation-based data augmentation method. Unfortunately, we\nfind that simply combining source-language data and the corresponding\ntranslation cannot fully exploit the translated data and the improvements\nobtained are somewhat limited. In this paper, we describe our novel\ndual-contrastive framework ConCNER for cross-lingual NER under the scenario of\nlimited source-language labeled data. Specifically, based on the\nsource-language samples and their translations, we design two contrastive\nobjectives for cross-language NER at different grammatical levels, namely\nTranslation Contrastive Learning (TCL) to close sentence representations\nbetween translated sentence pairs and Label Contrastive Learning (LCL) to close\ntoken representations within the same labels. Furthermore, we utilize knowledge\ndistillation method where the NER model trained above is used as the teacher to\ntrain a student model on unlabeled target-language data to better fit the\ntarget language. We conduct extensive experiments on a wide variety of target\nlanguages, and the results demonstrate that ConCNER tends to outperform\nmultiple baseline methods. For reproducibility, our code for this paper is\navailable at https://github.com/GKLMIP/ConCNER.", "published": "2022-04-02 07:59:13", "link": "http://arxiv.org/abs/2204.00796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Factual Accuracy of Abstractive Clinical Text\n  Summarization using Multi-Objective Optimization", "abstract": "While there has been recent progress in abstractive summarization as applied\nto different domains including news articles, scientific articles, and blog\nposts, the application of these techniques to clinical text summarization has\nbeen limited. This is primarily due to the lack of large-scale training data\nand the messy/unstructured nature of clinical notes as opposed to other domains\nwhere massive training data come in structured or semi-structured form.\nFurther, one of the least explored and critical components of clinical text\nsummarization is factual accuracy of clinical summaries. This is specifically\ncrucial in the healthcare domain, cardiology in particular, where an accurate\nsummary generation that preserves the facts in the source notes is critical to\nthe well-being of a patient. In this study, we propose a framework for\nimproving the factual accuracy of abstractive summarization of clinical text\nusing knowledge-guided multi-objective optimization. We propose to jointly\noptimize three cost functions in our proposed architecture during training:\ngenerative loss, entity loss and knowledge loss and evaluate the proposed\narchitecture on 1) clinical notes of patients with heart failure (HF), which we\ncollect for this study; and 2) two benchmark datasets, Indiana University Chest\nX-ray collection (IU X-Ray), and MIMIC-CXR, that are publicly available. We\nexperiment with three transformer encoder-decoder architectures and demonstrate\nthat optimizing different loss functions leads to improved performance in terms\nof entity-level factual accuracy.", "published": "2022-04-02 07:59:28", "link": "http://arxiv.org/abs/2204.00797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constrained Sequence-to-Tree Generation for Hierarchical Text\n  Classification", "abstract": "Hierarchical Text Classification (HTC) is a challenging task where a document\ncan be assigned to multiple hierarchically structured categories within a\ntaxonomy. The majority of prior studies consider HTC as a flat multi-label\nclassification problem, which inevitably leads to \"label inconsistency\"\nproblem. In this paper, we formulate HTC as a sequence generation task and\nintroduce a sequence-to-tree framework (Seq2Tree) for modeling the hierarchical\nlabel structure. Moreover, we design a constrained decoding strategy with\ndynamic vocabulary to secure the label consistency of the results. Compared\nwith previous works, the proposed approach achieves significant and consistent\nimprovements on three benchmark datasets.", "published": "2022-04-02 08:35:39", "link": "http://arxiv.org/abs/2204.00811v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Co-VQA : Answering by Interactive Sub Question Sequence", "abstract": "Most existing approaches to Visual Question Answering (VQA) answer questions\ndirectly, however, people usually decompose a complex question into a sequence\nof simple sub questions and finally obtain the answer to the original question\nafter answering the sub question sequence(SQS). By simulating the process, this\npaper proposes a conversation-based VQA (Co-VQA) framework, which consists of\nthree components: Questioner, Oracle, and Answerer. Questioner raises the sub\nquestions using an extending HRED model, and Oracle answers them one-by-one. An\nAdaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed,\nwhere the question-answer pair is used to update the visual representation\nsequentially. To perform supervised learning for each model, we introduce a\nwell-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2\ndatasets. Experimental results show that our method achieves state-of-the-art\non VQA-CP v2. Further analyses show that SQSs help build direct semantic\nconnections between questions and images, provide question-adaptive\nvariable-length reasoning chains, and with explicit interpretability as well as\nerror traceability.", "published": "2022-04-02 15:09:16", "link": "http://arxiv.org/abs/2204.00879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-Assisted Semantic Annotation Correction for Emotion-Related\n  Questions", "abstract": "Annotated data have traditionally been used to provide the input for training\na supervised machine learning (ML) model. However, current pre-trained ML\nmodels for natural language processing (NLP) contain embedded linguistic\ninformation that can be used to inform the annotation process. We use the BERT\nneural language model to feed information back into an annotation task that\ninvolves semantic labelling of dialog behavior in a question-asking game called\nEmotion Twenty Questions (EMO20Q). First we describe the background of BERT,\nthe EMO20Q data, and assisted annotation tasks. Then we describe the methods\nfor fine-tuning BERT for the purpose of checking the annotated labels. To do\nthis, we use the paraphrase task as a way to check that all utterances with the\nsame annotation label are classified as paraphrases of each other. We show this\nmethod to be an effective way to assess and revise annotations of textual user\ndata with complex, utterance-level semantic labels.", "published": "2022-04-02 18:00:49", "link": "http://arxiv.org/abs/2204.00916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-Centric Query Refinement", "abstract": "We introduce the task of entity-centric query refinement. Given an input\nquery whose answer is a (potentially large) collection of entities, the task\noutput is a small set of query refinements meant to assist the user in\nefficient domain exploration and entity discovery. We propose a method to\ncreate a training dataset for this task. For a given input query, we use an\nexisting knowledge base taxonomy as a source of candidate query refinements,\nand choose a final set of refinements from among these candidates using a\nsearch procedure designed to partition the set of entities answering the input\nquery. We demonstrate that our approach identifies refinement sets which human\nannotators judge to be interesting, comprehensive, and non-redundant. In\naddition, we find that a text generation model trained on our newly-constructed\ndataset is able to offer refinements for novel queries not covered by an\nexisting taxonomy. Our code and data are available at\nhttps://github.com/google-research/language/tree/master/language/qresp.", "published": "2022-04-02 02:19:47", "link": "http://arxiv.org/abs/2204.00743v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Metaphorical User Simulators for Evaluating Task-oriented Dialogue\n  Systems", "abstract": "Task-oriented dialogue systems (TDSs) are assessed mainly in an offline\nsetting or through human evaluation. The evaluation is often limited to\nsingle-turn or is very time-intensive. As an alternative, user simulators that\nmimic user behavior allow us to consider a broad set of user goals to generate\nhuman-like conversations for simulated evaluation. Employing existing user\nsimulators to evaluate TDSs is challenging as user simulators are primarily\ndesigned to optimize dialogue policies for TDSs and have limited evaluation\ncapabilities. Moreover, the evaluation of user simulators is an open challenge.\n  In this work, we propose a metaphorical user simulator for end-to-end TDS\nevaluation, where we define a simulator to be metaphorical if it simulates\nuser's analogical thinking in interactions with systems. We also propose a\ntester-based evaluation framework to generate variants, i.e., dialogue systems\nwith different capabilities. Our user simulator constructs a metaphorical user\nmodel that assists the simulator in reasoning by referring to prior knowledge\nwhen encountering new items. We estimate the quality of simulators by checking\nthe simulated interactions between simulators and variants. Our experiments are\nconducted using three TDS datasets. The proposed user simulator demonstrates\nbetter consistency with manual evaluation than an agenda-based simulator and a\nseq2seq model on three datasets; our tester framework demonstrates efficiency\nand has been tested on multiple tasks, such as conversational recommendation\nand e-commerce dialogues.", "published": "2022-04-02 05:11:03", "link": "http://arxiv.org/abs/2204.00763v5", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Efficient comparison of sentence embeddings", "abstract": "The domain of natural language processing (NLP), which has greatly evolved\nover the last years, has highly benefited from the recent developments in word\nand sentence embeddings. Such embeddings enable the transformation of complex\nNLP tasks, like semantic similarity or Question and Answering (Q&A), into much\nsimpler to perform vector comparisons. However, such a problem transformation\nraises new challenges like the efficient comparison of embeddings and their\nmanipulation. In this work, we will discuss about various word and sentence\nembeddings algorithms, we will select a sentence embedding algorithm, BERT, as\nour algorithm of choice and we will evaluate the performance of two vector\ncomparison approaches, FAISS and Elasticsearch, in the specific problem of\nsentence embeddings. According to the results, FAISS outperforms Elasticsearch\nwhen used in a centralized environment with only one node, especially when big\ndatasets are included.", "published": "2022-04-02 09:08:34", "link": "http://arxiv.org/abs/2204.00820v2", "categories": ["cs.CL", "cs.LG", "I.2.7; H.4.0"], "primary_category": "cs.CL"}
{"title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating\n  Controlled Text Generation", "abstract": "Existing reference-free metrics have obvious limitations for evaluating\ncontrolled text generation models. Unsupervised metrics can only provide a\ntask-agnostic evaluation result which correlates weakly with human judgments,\nwhereas supervised ones may overfit task-specific data with poor generalization\nability to other datasets. In this paper, we propose an unsupervised\nreference-free metric called CTRLEval, which evaluates controlled text\ngeneration from different aspects by formulating each aspect into multiple text\ninfilling tasks. On top of these tasks, the metric assembles the generation\nprobabilities from a pre-trained language model without any model training.\nExperimental results show that our metric has higher correlations with human\njudgments than other baselines, while obtaining better generalization of\nevaluating generated texts from different models and with different qualities.", "published": "2022-04-02 13:42:49", "link": "http://arxiv.org/abs/2204.00862v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accurate Online Posterior Alignments for Principled\n  Lexically-Constrained Decoding", "abstract": "Online alignment in machine translation refers to the task of aligning a\ntarget word to a source word when the target sequence has only been partially\ndecoded. Good online alignments facilitate important applications such as\nlexically constrained translation where user-defined dictionaries are used to\ninject lexical constraints into the translation model. We propose a novel\nposterior alignment technique that is truly online in its execution and\nsuperior in terms of alignment error rates compared to existing methods. Our\nproposed inference technique jointly considers alignment and token\nprobabilities in a principled manner and can be seamlessly integrated within\nexisting constrained beam-search decoding algorithms. On five language pairs,\nincluding two distant language pairs, we achieve consistent drop in alignment\nerror rates. When deployed on seven lexically constrained translation tasks, we\nachieve significant improvements in BLEU specifically around the constrained\npositions.", "published": "2022-04-02 14:37:07", "link": "http://arxiv.org/abs/2204.00871v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging", "abstract": "Prompting methods recently achieve impressive success in few-shot learning.\nThese methods modify input samples with prompt sentence pieces, and decode\nlabel tokens to map samples to corresponding labels. However, such a paradigm\nis very inefficient for the task of slot tagging. Since slot tagging samples\nare multiple consecutive words in a sentence, the prompting methods have to\nenumerate all n-grams token spans to find all the possible slots, which greatly\nslows down the prediction. To tackle this, we introduce an inverse paradigm for\nprompting. Different from the classic prompts mapping tokens to labels, we\nreversely predict slot values given slot types. Such inverse prompting only\nrequires a one-turn prediction for each slot type and greatly speeds up the\nprediction. Besides, we propose a novel Iterative Prediction Strategy, from\nwhich the model learns to refine predictions by considering the relations\nbetween different slot types. We find, somewhat surprisingly, the proposed\nmethod not only predicts faster but also significantly improves the effect\n(improve over 6.1 F1-scores on 10-shot setting) and achieves new\nstate-of-the-art performance.", "published": "2022-04-02 15:41:19", "link": "http://arxiv.org/abs/2204.00885v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Local and Global Features in Transformer-based Extreme\n  Multi-label Text Classification", "abstract": "Extreme multi-label text classification (XMTC) is the task of tagging each\ndocument with the relevant labels from a very large space of predefined\ncategories. Recently, large pre-trained Transformer models have made\nsignificant performance improvements in XMTC, which typically use the embedding\nof the special CLS token to represent the entire document semantics as a global\nfeature vector, and match it against candidate labels. However, we argue that\nsuch a global feature vector may not be sufficient to represent different\ngranularity levels of semantics in the document, and that complementing it with\nthe local word-level features could bring additional gains. Based on this\ninsight, we propose an approach that combines both the local and global\nfeatures produced by Transformer models to improve the prediction power of the\nclassifier. Our experiments show that the proposed model either outperforms or\nis comparable to the state-of-the-art methods on benchmark datasets.", "published": "2022-04-02 19:55:23", "link": "http://arxiv.org/abs/2204.00933v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-end model for named entity recognition from speech without paired\n  training data", "abstract": "Recent works showed that end-to-end neural approaches tend to become very\npopular for spoken language understanding (SLU). Through the term end-to-end,\none considers the use of a single model optimized to extract semantic\ninformation directly from the speech signal. A major issue for such models is\nthe lack of paired audio and textual data with semantic annotation. In this\npaper, we propose an approach to build an end-to-end neural model to extract\nsemantic information in a scenario in which zero paired audio data is\navailable. Our approach is based on the use of an external model trained to\ngenerate a sequence of vectorial representations from text. These\nrepresentations mimic the hidden representations that could be generated inside\nan end-to-end automatic speech recognition (ASR) model by processing a speech\nsignal. An SLU neural module is then trained using these representations as\ninput and the annotated text as output. Last, the SLU module replaces the top\nlayers of the ASR model to achieve the construction of the end-to-end model.\nOur experiments on named entity recognition, carried out on the QUAERO corpus,\nshow that this approach is very promising, getting better results than a\ncomparable cascade approach or than the use of synthetic voices.", "published": "2022-04-02 08:14:27", "link": "http://arxiv.org/abs/2204.00803v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "HLDC: Hindi Legal Documents Corpus", "abstract": "Many populous countries including India are burdened with a considerable\nbacklog of legal cases. Development of automated systems that could process\nlegal documents and augment legal practitioners can mitigate this. However,\nthere is a dearth of high-quality corpora that is needed to develop such\ndata-driven systems. The problem gets even more pronounced in the case of low\nresource languages such as Hindi. In this resource paper, we introduce the\nHindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents\nin Hindi. Documents are cleaned and structured to enable the development of\ndownstream applications. Further, as a use-case for the corpus, we introduce\nthe task of bail prediction. We experiment with a battery of models and propose\na Multi-Task Learning (MTL) based model for the same. MTL models use\nsummarization as an auxiliary task along with bail prediction as the main task.\nExperiments with different models are indicative of the need for further\nresearch in this area. We release the corpus and model implementation code with\nthis paper: https://github.com/Exploration-Lab/HLDC", "published": "2022-04-02 08:22:52", "link": "http://arxiv.org/abs/2204.00806v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Moment-based Adversarial Training for Embodied Language Comprehension", "abstract": "In this paper, we focus on a vision-and-language task in which a robot is\ninstructed to execute household tasks. Given an instruction such as \"Rinse off\na mug and place it in the coffee maker,\" the robot is required to locate the\nmug, wash it, and put it in the coffee maker. This is challenging because the\nrobot needs to break down the instruction sentences into subgoals and execute\nthem in the correct order. On the ALFRED benchmark, the performance of\nstate-of-the-art methods is still far lower than that of humans. This is\npartially because existing methods sometimes fail to infer subgoals that are\nnot explicitly specified in the instruction sentences. We propose Moment-based\nAdversarial Training (MAT), which uses two types of moments for perturbation\nupdates in adversarial training. We introduce MAT to the embedding spaces of\nthe instruction, subgoals, and state representations to handle their varieties.\nWe validated our method on the ALFRED benchmark, and the results demonstrated\nthat our method outperformed the baseline method for all the metrics on the\nbenchmark.", "published": "2022-04-02 16:07:24", "link": "http://arxiv.org/abs/2204.00889v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Ad Creative Discontinuation Prediction with Multi-Modal Multi-Task\n  Neural Survival Networks", "abstract": "Discontinuing ad creatives at an appropriate time is one of the most\nimportant ad operations that can have a significant impact on sales. Such\noperational support for ineffective ads has been less explored than that for\neffective ads. After pre-analyzing 1,000,000 real-world ad creatives, we found\nthat there are two types of discontinuation: short-term (i.e., cut-out) and\nlong-term (i.e., wear-out). In this paper, we propose a practical prediction\nframework for the discontinuation of ad creatives with a hazard function-based\nloss function inspired by survival analysis. Our framework predicts the\ndiscontinuations with a multi-modal deep neural network that takes as input the\nad creative (e.g., text, categorical, image, numerical features). To improve\nthe prediction performance for the two different types of discontinuations and\nfor the ad creatives that contribute to sales, we introduce two new techniques:\n(1) a two-term estimation technique with multi-task learning and (2) a\nclick-through rate-weighting technique for the loss function. We evaluated our\nframework using the large-scale ad creative dataset, including 10 billion scale\nimpressions. In terms of the concordance index (short: 0.896, long: 0.939, and\noverall: 0.792), our framework achieved significantly better performance than\nthe conventional method (0.531). Additionally, we confirmed that our framework\n(i) demonstrated the same degree of discontinuation effect as manual operations\nfor short-term cases, and (ii) accurately predicted the ad discontinuation\norder, which is important for long-running ad creatives for long-term cases.", "published": "2022-04-02 04:57:23", "link": "http://arxiv.org/abs/2204.11588v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.IR"}
{"title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ\n  Acoustic Feature", "abstract": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\nincluding an acoustic model(AM) that predicts acoustic feature from the input\ntranscript and a vocoder that generates waveform according to the given\nacoustic feature. However, the acoustic feature in current TTS systems is\ntypically mel-spectrogram, which is highly correlated along both time and\nfrequency axes in a complicated way, leading to a great difficulty for the AM\nto predict. Although high-fidelity audio can be generated by recent neural\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\naccordingly. In particular, txt2vec basically becomes a classification model\ninstead of a traditional regression model while vec2wav uses an additional\nfeature encoder before HifiGAN generator for smoothing the discontinuous\nquantized feature. Our experiments show that vec2wav achieves better\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\nperformance in terms of naturalness among all current publicly available TTS\nsystems.", "published": "2022-04-02 05:28:48", "link": "http://arxiv.org/abs/2204.00768v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur\n  Speech Recognition", "abstract": "In Uyghur speech, consonant and vowel reduction are often encountered,\nespecially in spontaneous speech with high speech rate, which will cause a\ndegradation of speech recognition performance. To solve this problem, we\npropose an effective phone mask training method for Conformer-based Uyghur\nend-to-end (E2E) speech recognition. The idea is to randomly mask off a certain\npercentage features of phones during model training, which simulates the above\nverbal phenomena and facilitates E2E model to learn more contextual\ninformation. According to experiments, the above issues can be greatly\nalleviated. In addition, deep investigations are carried out into different\nunits in masking, which shows the effectiveness of our proposed masking unit.\nWe also further study the masking method and optimize filling strategy of phone\nmask. Finally, compared with Conformer-based E2E baseline without mask\ntraining, our model demonstrates about 5.51% relative Word Error Rate (WER)\nreduction on reading speech and 12.92% on spontaneous speech, respectively. The\nabove approach has also been verified on test-set of open-source data THUYG-20,\nwhich shows 20% relative improvements.", "published": "2022-04-02 09:04:24", "link": "http://arxiv.org/abs/2204.00819v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Target Sound Extraction with Timestamp Information", "abstract": "Target sound extraction (TSE) aims to extract the sound part of a target\nsound event class from a mixture audio with multiple sound events.\n  The previous works mainly focus on the problems of weakly-labelled data,\njointly learning and new classes, however, no one cares about the onset and\noffset times of the target sound event, which has been emphasized in the\nauditory scene analysis. In this paper, we study to utilize such timestamp\ninformation to help extract the target sound via a target sound detection\nnetwork and a target-weighted time-frequency loss function.\n  More specifically, we use the detection result of a target sound detection\n(TSD) network as the additional information to guide the learning of target\nsound extraction network. We also find that the result of TSE can further\nimprove the performance of the TSD network, so that a mutual learning framework\nof the target sound detection and extraction is proposed. In addition, a\ntarget-weighted time-frequency loss function is designed to pay more attention\nto the temporal regions of the target sound during training. Experimental\nresults on the synthesized data generated from the Freesound Datasets show that\nour proposed method can significantly improve the performance of TSE.", "published": "2022-04-02 09:09:25", "link": "http://arxiv.org/abs/2204.00821v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Simulated Mixtures to Simulated Conversations as Training Data for\n  End-to-End Neural Diarization", "abstract": "End-to-end neural diarization (EEND) is nowadays one of the most prominent\nresearch topics in speaker diarization. EEND presents an attractive alternative\nto standard cascaded diarization systems since a single system is trained at\nonce to deal with the whole diarization problem. Several EEND variants and\napproaches are being proposed, however, all these models require large amounts\nof annotated data for training but available annotated data are scarce. Thus,\nEEND works have used mostly simulated mixtures for training. However, simulated\nmixtures do not resemble real conversations in many aspects. In this work we\npresent an alternative method for creating synthetic conversations that\nresemble real ones by using statistics about distributions of pauses and\noverlaps estimated on genuine conversations. Furthermore, we analyze the effect\nof the source of the statistics, different augmentations and amounts of data.\nWe demonstrate that our approach performs substantially better than the\noriginal one, while reducing the dependence on the fine-tuning stage.\nExperiments are carried out on 2-speaker telephone conversations of Callhome\nand DIHARD 3. Together with this publication, we release our implementations of\nEEND and the method for creating simulated conversations.", "published": "2022-04-02 16:12:28", "link": "http://arxiv.org/abs/2204.00890v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StyleWaveGAN: Style-based synthesis of drum sounds with extensive\n  controls using generative adversarial networks", "abstract": "In this paper we introduce StyleWaveGAN, a style-based drum sound generator\nthat is a variation of StyleGAN, a state-of-the-art image generator. By\nconditioning StyleWaveGAN on both the type of drum and several audio\ndescriptors, we are able to synthesize waveforms faster than real-time on a GPU\ndirectly in CD quality up to a duration of 1.5s while retaining a considerable\namount of control over the generation. We also introduce an alternative to the\nprogressive growing of GANs and experimented on the effect of dataset balancing\nfor generative tasks. The experiments are carried out on an augmented subset of\na publicly available dataset comprised of different drums and cymbals. We\nevaluate against two recent drum generators, WaveGAN and NeuroDrum,\ndemonstrating significantly improved generation quality (measured with the\nFrechet Audio Distance) and interesting results with perceptual features.", "published": "2022-04-02 17:27:17", "link": "http://arxiv.org/abs/2204.00907v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Measuring pitch extractors' response to frequency-modulated\n  multi-component signals", "abstract": "This article focuses on the research tool for investigating the fundamental\nfrequencies of voiced sounds. We introduce an objective and informative\nmeasurement method of pitch extractors' response to frequency-modulated tones.\nThe method uses a new test signal for acoustic system analysis. The test signal\nenables simultaneous measurement of the extractors' responses. They are the\nmodulation frequency response and the total distortion, including\nintermodulation distortions. We applied this method to various pitch extractors\nand placed them on several performance maps. We used the proposed method to\nfine-tune one of the extractors to make it the best fit tool for scientific\nresearch of voice fundamental frequencies.", "published": "2022-04-02 17:43:13", "link": "http://arxiv.org/abs/2204.00911v1", "categories": ["cs.SD", "eess.AS", "94A12, 93C80, 42-08"], "primary_category": "cs.SD"}
{"title": "Speaker adaptation for Wav2vec2 based dysarthric ASR", "abstract": "Dysarthric speech recognition has posed major challenges due to lack of\ntraining data and heavy mismatch in speaker characteristics. Recent ASR systems\nhave benefited from readily available pretrained models such as wav2vec2 to\nimprove the recognition performance. Speaker adaptation using fMLLR and\nxvectors have provided major gains for dysarthric speech with very little\nadaptation data. However, integration of wav2vec2 with fMLLR features or\nxvectors during wav2vec2 finetuning is yet to be explored. In this work, we\npropose a simple adaptation network for fine-tuning wav2vec2 using fMLLR\nfeatures. The adaptation network is also flexible to handle other speaker\nadaptive features such as xvectors. Experimental analysis show steady\nimprovements using our proposed approach across all impairment severity levels\nand attains 57.72\\% WER for high severity in UASpeech dataset. We also\nperformed experiments on German dataset to substantiate the consistency of our\nproposed approach across diverse domains.", "published": "2022-04-02 05:46:35", "link": "http://arxiv.org/abs/2204.00770v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fast Real-time Personalized Speech Enhancement: End-to-End Enhancement\n  Network (E3Net) and Knowledge Distillation", "abstract": "This paper investigates how to improve the runtime speed of personalized\nspeech enhancement (PSE) networks while maintaining the model quality. Our\napproach includes two aspects: architecture and knowledge distillation (KD). We\npropose an end-to-end enhancement (E3Net) model architecture, which is\n$3\\times$ faster than a baseline STFT-based model. Besides, we use KD\ntechniques to develop compressed student models without significantly degrading\nquality. In addition, we investigate using noisy data without reference clean\nsignals for training the student models, where we combine KD with multi-task\nlearning (MTL) using automatic speech recognition (ASR) loss. Our results show\nthat E3Net provides better speech and transcription quality with a lower target\nspeaker over-suppression (TSOS) rate than the baseline model. Furthermore, we\nshow that the KD methods can yield student models that are $2-4\\times$ faster\nthan the teacher and provides reasonable quality. Combining KD and MTL improves\nthe ASR and TSOS metrics without degrading the speech quality.", "published": "2022-04-02 05:47:31", "link": "http://arxiv.org/abs/2204.00771v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Acoustic-to-articulatory Inversion based on Speech Decomposition and\n  Auxiliary Feature", "abstract": "Acoustic-to-articulatory inversion (AAI) is to obtain the movement of\narticulators from speech signals. Until now, achieving a speaker-independent\nAAI remains a challenge given the limited data. Besides, most current works\nonly use audio speech as input, causing an inevitable performance bottleneck.\nTo solve these problems, firstly, we pre-train a speech decomposition network\nto decompose audio speech into speaker embedding and content embedding as the\nnew personalized speech features to adapt to the speaker-independent case.\nSecondly, to further improve the AAI, we propose a novel auxiliary feature\nnetwork to estimate the lip auxiliary features from the above personalized\nspeech features. Experimental results on three public datasets show that,\ncompared with the state-of-the-art only using the audio speech feature, the\nproposed method reduces the average RMSE by 0.25 and increases the average\ncorrelation coefficient by 2.0% in the speaker-dependent case. More\nimportantly, the average RMSE decreases by 0.29 and the average correlation\ncoefficient increases by 5.0% in the speaker-independent case.", "published": "2022-04-02 14:47:19", "link": "http://arxiv.org/abs/2204.00873v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An objective test tool for pitch extractors' response attributes", "abstract": "We propose an objective measurement method for pitch extractors' responses to\nfrequency-modulated signals. It enables us to evaluate different pitch\nextractors with unified criteria. The method uses extended time-stretched\npulses combined by binary orthogonal sequences. It provides simultaneous\nmeasurement results consisting of the linear and the non-linear time-invariant\nresponses and random and time-varying responses. We tested representative pitch\nextractors using fundamental frequencies spanning 80~Hz to 800~Hz with 1/48\noctave steps and produced more than 2000 modulation frequency response plots.\nWe found that making scientific visualization by animating these plots enables\nus to understand different pitch extractors' behavior at once. Such efficient\nand effortless inspection is impossible by inspecting all individual plots. The\nproposed measurement method with visualization leads to further improvement of\nthe performance of one of the extractors mentioned above. In other words, our\nprocedure turns the specific pitch extractor into the best reliable measuring\nequipment that is crucial for scientific research. We open-sourced MATLAB codes\nof the proposed objective measurement method and visualization procedure.", "published": "2022-04-02 17:01:50", "link": "http://arxiv.org/abs/2204.00902v2", "categories": ["cs.SD", "eess.AS", "eess.SP", "94A12, 93C80, 42-08"], "primary_category": "cs.SD"}
