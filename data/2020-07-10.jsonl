{"title": "What Can We Learn From Almost a Decade of Food Tweets", "abstract": "We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow\ndomain related to food, drinks, eating and drinking. The corpus has been\ncollected over time-span of over 8 years and includes over 2 million tweets\nentailed with additional useful data. We also separate two sub-corpora of\nquestion and answer tweets and sentiment annotated tweets. We analyse contents\nof the corpus and demonstrate use-cases for the sub-corpora by training\ndomain-specific question-answering and sentiment-analysis models using data\nfrom the corpus.", "published": "2020-07-10 06:36:13", "link": "http://arxiv.org/abs/2007.05194v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporally Correlated Task Scheduling for Sequence Learning", "abstract": "Sequence learning has attracted much research attention from the machine\nlearning community in recent years. In many applications, a sequence learning\ntask is usually associated with multiple temporally correlated auxiliary tasks,\nwhich are different in terms of how much input information to use or which\nfuture step to predict. For example, (i) in simultaneous machine translation,\none can conduct translation under different latency (i.e., how many input words\nto read/wait before translation); (ii) in stock trend forecasting, one can\npredict the price of a stock in different future days (e.g., tomorrow, the day\nafter tomorrow). While it is clear that those temporally correlated tasks can\nhelp each other, there is a very limited exploration on how to better leverage\nmultiple auxiliary tasks to boost the performance of the main task. In this\nwork, we introduce a learnable scheduler to sequence learning, which can\nadaptively select auxiliary tasks for training depending on the model status\nand the current training data. The scheduler and the model for the main task\nare jointly trained through bi-level optimization. Experiments show that our\nmethod significantly improves the performance of simultaneous machine\ntranslation and stock trend forecasting.", "published": "2020-07-10 10:28:54", "link": "http://arxiv.org/abs/2007.05290v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SacreROUGE: An Open-Source Library for Using and Developing\n  Summarization Evaluation Metrics", "abstract": "We present SacreROUGE, an open-source library for using and developing\nsummarization evaluation metrics. SacreROUGE removes many obstacles that\nresearchers face when using or developing metrics: (1) The library provides\nPython wrappers around the official implementations of existing evaluation\nmetrics so they share a common, easy-to-use interface; (2) it provides\nfunctionality to evaluate how well any metric implemented in the library\ncorrelates to human-annotated judgments, so no additional code needs to be\nwritten for a new evaluation metric; and (3) it includes scripts for loading\ndatasets that contain human judgments so they can easily be used for\nevaluation. This work describes the design of the library, including the core\nMetric interface, the command-line API for evaluating summarization models and\nmetrics, and the scripts to load and reformat publicly available datasets. The\ndevelopment of SacreROUGE is ongoing and open to contributions from the\ncommunity.", "published": "2020-07-10 13:26:37", "link": "http://arxiv.org/abs/2007.05374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pragmatic information in translation: a corpus-based study of tense and\n  mood in English and German", "abstract": "Grammatical tense and mood are important linguistic phenomena to consider in\nnatural language processing (NLP) research. We consider the correspondence\nbetween English and German tense and mood in translation. Human translators do\nnot find this correspondence easy, and as we will show through careful\nanalysis, there are no simplistic ways to map tense and mood from one language\nto another. Our observations about the challenges of human translation of tense\nand mood have important implications for multilingual NLP. Of particular\nimportance is the challenge of modeling tense and mood in rule-based,\nphrase-based statistical and neural machine translation.", "published": "2020-07-10 08:15:59", "link": "http://arxiv.org/abs/2007.05234v1", "categories": ["cs.CL", "cs.AI", "I.1.2"], "primary_category": "cs.CL"}
{"title": "Topic Modeling on User Stories using Word Mover's Distance", "abstract": "Requirements elicitation has recently been complemented with crowd-based\ntechniques, which continuously involve large, heterogeneous groups of users who\nexpress their feedback through a variety of media. Crowd-based elicitation has\ngreat potential for engaging with (potential) users early on but also results\nin large sets of raw and unstructured feedback. Consolidating and analyzing\nthis feedback is a key challenge for turning it into sensible user\nrequirements. In this paper, we focus on topic modeling as a means to identify\ntopics within a large set of crowd-generated user stories and compare three\napproaches: (1) a traditional approach based on Latent Dirichlet Allocation,\n(2) a combination of word embeddings and principal component analysis, and (3)\na combination of word embeddings and Word Mover's Distance. We evaluate the\napproaches on a publicly available set of 2,966 user stories written and\ncategorized by crowd workers. We found that a combination of word embeddings\nand Word Mover's Distance is most promising. Depending on the word embeddings\nwe use in our approaches, we manage to cluster the user stories in two ways:\none that is closer to the original categorization and another that allows new\ninsights into the dataset, e.g. to find potentially new categories.\nUnfortunately, no measure exists to rate the quality of our results\nobjectively. Still, our findings provide a basis for future work towards\nanalyzing crowd-sourced user stories.", "published": "2020-07-10 11:05:42", "link": "http://arxiv.org/abs/2007.05302v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-Dialect Arabic BERT for Country-Level Dialect Identification", "abstract": "Arabic dialect identification is a complex problem for a number of inherent\nproperties of the language itself. In this paper, we present the experiments\nconducted, and the models developed by our competing team, Mawdoo3 AI, along\nthe way to achieving our winning solution to subtask 1 of the Nuanced Arabic\nDialect Identification (NADI) shared task. The dialect identification subtask\nprovides 21,000 country-level labeled tweets covering all 21 Arab countries. An\nunlabeled corpus of 10M tweets from the same domain is also presented by the\ncompetition organizers for optional use. Our winning solution itself came in\nthe form of an ensemble of different training iterations of our pre-trained\nBERT model, which achieved a micro-averaged F1-score of 26.78% on the subtask\nat hand. We publicly release the pre-trained language model component of our\nwinning solution under the name of Multi-dialect-Arabic-BERT model, for any\ninterested researcher out there.", "published": "2020-07-10 21:11:46", "link": "http://arxiv.org/abs/2007.05612v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GloVeInit at SemEval-2020 Task 1: Using GloVe Vector Initialization for\n  Unsupervised Lexical Semantic Change Detection", "abstract": "This paper presents a vector initialization approach for the SemEval2020 Task\n1: Unsupervised Lexical Semantic Change Detection. Given two corpora belonging\nto different time periods and a set of target words, this task requires us to\nclassify whether a word gained or lost a sense over time (subtask 1) and to\nrank them on the basis of the changes in their word senses (subtask 2). The\nproposed approach is based on using Vector Initialization method to align GloVe\nembeddings. The idea is to consecutively train GloVe embeddings for both\ncorpora, while using the first model to initialize the second one. This paper\nis based on the hypothesis that GloVe embeddings are more suited for the Vector\nInitialization method than SGNS embeddings. It presents an intuitive reasoning\nbehind this hypothesis, and also talks about the impact of various factors and\nhyperparameters on the performance of the proposed approach. Our model ranks\n13th and 10th among 33 teams in the two subtasks. The implementation has been\nshared publicly.", "published": "2020-07-10 21:35:17", "link": "http://arxiv.org/abs/2007.05618v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections\n  and State Tracking Baselines", "abstract": "MultiWOZ is a well-known task-oriented dialogue dataset containing over\n10,000 annotated dialogues spanning 8 domains. It is extensively used as a\nbenchmark for dialogue state tracking. However, recent works have reported\npresence of substantial noise in the dialogue state annotations. MultiWOZ 2.1\nidentified and fixed many of these erroneous annotations and user utterances,\nresulting in an improved version of this dataset. This work introduces MultiWOZ\n2.2, which is a yet another improved version of this dataset. Firstly, we\nidentify and fix dialogue state annotation errors across 17.3% of the\nutterances on top of MultiWOZ 2.1. Secondly, we redefine the ontology by\ndisallowing vocabularies of slots with a large number of possible values (e.g.,\nrestaurant name, time of booking). In addition, we introduce slot span\nannotations for these slots to standardize them across recent models, which\npreviously used custom string matching heuristics to generate them. We also\nbenchmark a few state of the art dialogue state tracking models on the\ncorrected dataset to facilitate comparison for future work. In the end, we\ndiscuss best practices for dialogue data collection that can help avoid\nannotation errors.", "published": "2020-07-10 22:52:14", "link": "http://arxiv.org/abs/2007.12720v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Handling Collocations in Hierarchical Latent Tree Analysis for Topic\n  Modeling", "abstract": "Topic modeling has been one of the most active research areas in machine\nlearning in recent years. Hierarchical latent tree analysis (HLTA) has been\nrecently proposed for hierarchical topic modeling and has shown superior\nperformance over state-of-the-art methods. However, the models used in HLTA\nhave a tree structure and cannot represent the different meanings of multiword\nexpressions sharing the same word appropriately. Therefore, we propose a method\nfor extracting and selecting collocations as a preprocessing step for HLTA. The\nselected collocations are replaced with single tokens in the bag-of-words model\nbefore running HLTA. Our empirical evaluation shows that the proposed method\nled to better performance of HLTA on three of the four data sets tested.", "published": "2020-07-10 04:56:36", "link": "http://arxiv.org/abs/2007.05163v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Image Captioning with Compositional Neural Module Networks", "abstract": "In image captioning where fluency is an important factor in evaluation, e.g.,\n$n$-gram metrics, sequential models are commonly used; however, sequential\nmodels generally result in overgeneralized expressions that lack the details\nthat may be present in an input image. Inspired by the idea of the\ncompositional neural module networks in the visual question answering task, we\nintroduce a hierarchical framework for image captioning that explores both\ncompositionality and sequentiality of natural language. Our algorithm learns to\ncompose a detail-rich sentence by selectively attending to different modules\ncorresponding to unique aspects of each object detected in an input image to\ninclude specific descriptions such as counts and color. In a set of experiments\non the MSCOCO dataset, the proposed model outperforms a state-of-the art model\nacross multiple evaluation metrics, more importantly, presenting visually\ninterpretable results. Furthermore, the breakdown of subcategories $f$-scores\nof the SPICE metric and human evaluation on Amazon Mechanical Turk show that\nour compositional module networks effectively generate accurate and detailed\ncaptions.", "published": "2020-07-10 20:58:04", "link": "http://arxiv.org/abs/2007.05608v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Class LM and word mapping for contextual biasing in End-to-End ASR", "abstract": "In recent years, all-neural, end-to-end (E2E) ASR systems gained rapid\ninterest in the speech recognition community. They convert speech input to text\nunits in a single trainable Neural Network model. In ASR, many utterances\ncontain rich named entities. Such named entities may be user or location\nspecific and they are not seen during training. A single model makes it\ninflexible to utilize dynamic contextual information during inference. In this\npaper, we propose to train a context aware E2E model and allow the beam search\nto traverse into the context FST during inference. We also propose a simple\nmethod to adjust the cost discrepancy between the context FST and the base\nmodel. This algorithm is able to reduce the named entity utterance WER by 57%\nwith little accuracy degradation on regular utterances. Although an E2E model\ndoes not need pronunciation dictionary, it's interesting to make use of\nexisting pronunciation knowledge to improve accuracy. In this paper, we propose\nan algorithm to map the rare entity words to common words via pronunciation and\ntreat the mapped words as an alternative form to the original word during\nrecognition. This algorithm further reduces the WER on the named entity\nutterances by another 31%.", "published": "2020-07-10 20:58:44", "link": "http://arxiv.org/abs/2007.05609v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Neural Composition: Learning to Generate from Multiple Models", "abstract": "Decomposing models into multiple components is critically important in many\napplications such as language modeling (LM) as it enables adapting individual\ncomponents separately and biasing of some components to the user's personal\npreferences. Conventionally, contextual and personalized adaptation for\nlanguage models, are achieved through class-based factorization, which requires\nclass-annotated data, or through biasing to individual phrases which is limited\nin scale. In this paper, we propose a system that combines model-defined\ncomponents, by learning when to activate the generation process from each\nindividual component, and how to combine probability distributions from each\ncomponent, directly from unlabeled text data.", "published": "2020-07-10 22:58:53", "link": "http://arxiv.org/abs/2007.16013v2", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Overcoming label noise in audio event detection using sequential\n  labeling", "abstract": "This paper addresses the noisy label issue in audio event detection (AED) by\nrefining strong labels as sequential labels with inaccurate timestamps removed.\nIn AED, strong labels contain the occurrence of a specific event and its\ntimestamps corresponding to the start and end of the event in an audio clip.\nThe timestamps depend on subjectivity of each annotator, and their label noise\nis inevitable. Contrary to the strong labels, weak labels indicate only the\noccurrence of a specific event. They do not have the label noise caused by the\ntimestamps, but the time information is excluded. To fully exploit information\nfrom available strong and weak labels, we propose an AED scheme to train with\nsequential labels in addition to the given strong and weak labels after\nconverting the strong labels into the sequential labels. Using sequential\nlabels consistently improved the performance particularly with the\nsegment-based F-score by focusing on occurrences of events. In the\nmean-teacher-based approach for semi-supervised learning, including an early\nstep with sequential prediction in addition to supervised learning with\nsequential labels mitigated label noise and inaccurate prediction of the\nteacher model and improved the segment-based F-score significantly while\nmaintaining the event-based F-score.", "published": "2020-07-10 06:33:12", "link": "http://arxiv.org/abs/2007.05191v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Gated Recurrent Context: Softmax-free Attention for Online\n  Encoder-Decoder Speech Recognition", "abstract": "Recently, attention-based encoder-decoder (AED) models have shown\nstate-of-the-art performance in automatic speech recognition (ASR). As the\noriginal AED models with global attentions are not capable of online inference,\nvarious online attention schemes have been developed to reduce ASR latency for\nbetter user experience. However, a common limitation of the conventional\nsoftmax-based online attention approaches is that they introduce an additional\nhyperparameter related to the length of the attention window, requiring\nmultiple trials of model training for tuning the hyperparameter. In order to\ndeal with this problem, we propose a novel softmax-free attention method and\nits modified formulation for online attention, which does not need any\nadditional hyperparameter at the training phase. Through a number of ASR\nexperiments, we demonstrate the tradeoff between the latency and performance of\nthe proposed online attention technique can be controlled by merely adjusting a\nthreshold at the test phase. Furthermore, the proposed methods showed\ncompetitive performance to the conventional global and online attentions in\nterms of word-error-rates (WERs).", "published": "2020-07-10 07:35:31", "link": "http://arxiv.org/abs/2007.05214v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ID-Conditioned Auto-Encoder for Unsupervised Anomaly Detection", "abstract": "In this paper, we introduce ID-Conditioned Auto-Encoder for unsupervised\nanomaly detection. Our method is an adaptation of the Class-Conditioned\nAuto-Encoder (C2AE) designed for the open-set recognition. Assuming that\nnon-anomalous samples constitute of distinct IDs, we apply Conditioned\nAuto-Encoder with labels provided by these IDs. Opposed to C2AE, our approach\nomits the classification subtask and reduces the learning process to the single\nrun. We simplify the learning process further by fixing a constant vector as\nthe target for non-matching labels. We apply our method in the context of\nsounds for machine condition monitoring. We evaluate our method on the ToyADMOS\nand MIMII datasets from the DCASE 2020 Challenge Task 2. We conduct an ablation\nstudy to indicate which steps of our method influences results the most.", "published": "2020-07-10 11:24:20", "link": "http://arxiv.org/abs/2007.05314v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "DARF: A data-reduced FADE version for simulations of speech recognition\n  thresholds with real hearing aids", "abstract": "Developing and selecting hearing aids is a time consuming process which is\nsimplified by using objective models. Previously, the framework for auditory\ndiscrimination experiments (FADE) accurately simulated benefits of hearing aid\nalgorithms with root mean squared prediction errors below 3 dB. One FADE\nsimulation requires several hours of (un)processed signals, which is\nobstructive when the signals have to be recorded. We propose and evaluate a\ndata-reduced FADE version (DARF) which facilitates simulations with signals\nthat cannot be processed digitally, but that can only be recorded in real-time.\nDARF simulates one speech recognition threshold (SRT) with about 30 minutes of\nrecorded and processed signals of the (German) matrix sentence test. Benchmark\nexperiments were carried out to compare DARF and standard FADE exhibiting small\ndifferences for stationary maskers (1 dB), but larger differences with strongly\nfluctuating maskers (5 dB). Hearing impairment and hearing aid algorithms\nseemed to reduce the differences. Hearing aid benefits were simulated in terms\nof speech recognition with three pairs of real hearing aids in silence ($\\geq$8\ndB), in stationary and fluctuating maskers in co-located (stat. 2 dB; fluct. 6\ndB), and spatially separated speech and noise signals (stat. $\\geq$8 dB; fluct.\n8 dB). The simulations were plausible in comparison to data from literature,\nbut a comparison with empirical data is still open. DARF facilitates objective\nSRT simulations with real devices with unknown signal processing in real\nenvironments. Yet, a validation of DARF for devices with unknown signal\nprocessing is still pending since it was only tested with three similar\ndevices. Nonetheless, DARF could be used for improving as well as for\ndeveloping or model-based fitting of hearing aids.", "published": "2020-07-10 13:37:04", "link": "http://arxiv.org/abs/2007.05378v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conditioned Time-Dilated Convolutions for Sound Event Detection", "abstract": "Sound event detection (SED) is the task of identifying sound events along\nwith their onset and offset times. A recent, convolutional neural networks\nbased SED method, proposed the usage of depthwise separable (DWS) and\ntime-dilated convolutions. DWS and time-dilated convolutions yielded\nstate-of-the-art results for SED, with considerable small amount of parameters.\nIn this work we propose the expansion of the time-dilated convolutions, by\nconditioning them with jointly learned embeddings of the SED predictions by the\nSED classifier. We present a novel algorithm for the conditioning of the\ntime-dilated convolutions which functions similarly to language modelling, and\nenhances the performance of the these convolutions. We employ the freely\navailable TUT-SED Synthetic dataset, and we assess the performance of our\nmethod using the average per-frame $\\text{F}_{1}$ score and average per-frame\nerror rate, over the 10 experiments. We achieve an increase of 2\\% (from 0.63\nto 0.65) at the average $\\text{F}_{1}$ score (the higher the better) and a\ndecrease of 3\\% (from 0.50 to 0.47) at the error rate (the lower the better).", "published": "2020-07-10 06:05:23", "link": "http://arxiv.org/abs/2007.05183v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
