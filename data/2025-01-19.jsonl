{"title": "A statistical technique for cleaning option price data", "abstract": "Recorded option pricing datasets are not always freely available.\nAdditionally, these datasets often contain numerous prices which are either\nhigher or lower than can reasonably be expected. Various reasons for these\nunexpected observations are possible, including human error in the recording of\nthe details associated with the option in question. In order for the analyses\nperformed on these datasets to be reliable, it is necessary to identify and\nremove these options from the dataset. In this paper, we list three distinct\nproblems often found in recorded option price datasets alongside means of\naddressing these. The methods used are justified using sound statistical\nreasoning and remove option prices violating the standard assumption of no\narbitrage. An attractive aspect of the proposed technique is that no option\npricing model-based assumptions are used. Although the discussion is restricted\nto European options, the procedure is easily modified for use with exotic\noptions as well. As a final contribution, the paper contains a link to six\noption pricing datasets which have already been cleaned using the proposed\nmethods and can be freely used by researchers.", "published": "2025-01-19 20:39:55", "link": "http://arxiv.org/abs/2501.11164v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Building low-resource African language corpora: A case study of\n  Kidawida, Kalenjin and Dholuo", "abstract": "Natural Language Processing is a crucial frontier in artificial intelligence,\nwith broad applications in many areas, including public health, agriculture,\neducation, and commerce. However, due to the lack of substantial linguistic\nresources, many African languages remain underrepresented in this digital\ntransformation. This paper presents a case study on the development of\nlinguistic corpora for three under-resourced Kenyan languages, Kidaw'ida,\nKalenjin, and Dholuo, with the aim of advancing natural language processing and\nlinguistic research in African communities. Our project, which lasted one year,\nemployed a selective crowd-sourcing methodology to collect text and speech data\nfrom native speakers of these languages. Data collection involved (1) recording\nconversations and translation of the resulting text into Kiswahili, thereby\ncreating parallel corpora, and (2) reading and recording written texts to\ngenerate speech corpora. We made these resources freely accessible via\nopen-research platforms, namely Zenodo for the parallel text corpora and\nMozilla Common Voice for the speech datasets, thus facilitating ongoing\ncontributions and access for developers to train models and develop Natural\nLanguage Processing applications. The project demonstrates how grassroots\nefforts in corpus building can support the inclusion of African languages in\nartificial intelligence innovations. In addition to filling resource gaps,\nthese corpora are vital in promoting linguistic diversity and empowering local\ncommunities by enabling Natural Language Processing applications tailored to\ntheir needs. As African countries like Kenya increasingly embrace digital\ntransformation, developing indigenous language resources becomes essential for\ninclusive growth. We encourage continued collaboration from native speakers and\ndevelopers to expand and utilize these corpora.", "published": "2025-01-19 10:17:21", "link": "http://arxiv.org/abs/2501.11003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenAI Content Detection Task 1: English and Multilingual\n  Machine-Generated Text Detection: AI vs. Human", "abstract": "We present the GenAI Content Detection Task~1 -- a shared task on binary\nmachine generated text detection, conducted as a part of the GenAI workshop at\nCOLING 2025. The task consists of two subtasks: Monolingual (English) and\nMultilingual. The shared task attracted many participants: 36 teams made\nofficial submissions to the Monolingual subtask during the test phase and 26\nteams -- to the Multilingual. We provide a comprehensive overview of the data,\na summary of the results -- including system rankings and performance scores --\ndetailed descriptions of the participating systems, and an in-depth analysis of\nsubmissions.\nhttps://github.com/mbzuai-nlp/COLING-2025-Workshop-on-MGT-Detection-Task1", "published": "2025-01-19 11:11:55", "link": "http://arxiv.org/abs/2501.11012v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Impact of Language-Adaptive Fine-Tuning on Sentiment\n  Analysis in Hausa Language Using AfriBERTa", "abstract": "Sentiment analysis (SA) plays a vital role in Natural Language Processing\n(NLP) by ~identifying sentiments expressed in text. Although significant\nadvances have been made in SA for widely spoken languages, low-resource\nlanguages such as Hausa face unique challenges, primarily due to a lack of\ndigital resources. This study investigates the effectiveness of\nLanguage-Adaptive Fine-Tuning (LAFT) to improve SA performance in Hausa. We\nfirst curate a diverse, unlabeled corpus to expand the model's linguistic\ncapabilities, followed by applying LAFT to adapt AfriBERTa specifically to the\nnuances of the Hausa language. The adapted model is then fine-tuned on the\nlabeled NaijaSenti sentiment dataset to evaluate its performance. Our findings\ndemonstrate that LAFT gives modest improvements, which may be attributed to the\nuse of formal Hausa text rather than informal social media data. Nevertheless,\nthe pre-trained AfriBERTa model significantly outperformed models not\nspecifically trained on Hausa, highlighting the importance of using pre-trained\nmodels in low-resource contexts. This research emphasizes the necessity for\ndiverse data sources to advance NLP applications for low-resource African\nlanguages. We published the code and the dataset to encourage further research\nand facilitate reproducibility in low-resource NLP here:\nhttps://github.com/Sani-Abdullahi-Sani/Natural-Language-Processing/blob/main/Sentiment%20Analysis%20for%20Low%20Resource%20African%20Languages", "published": "2025-01-19 11:52:46", "link": "http://arxiv.org/abs/2501.11023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Arabic Text to Puzzles: LLM-Driven Development of Arabic\n  Educational Crosswords", "abstract": "We present an Arabic crossword puzzle generator from a given text that\nutilizes advanced language models such as GPT-4-Turbo, GPT-3.5-Turbo and\nLlama3-8B-Instruct, specifically developed for educational purposes, this\ninnovative generator leverages a meticulously compiled dataset named\nArabic-Clue-Instruct with over 50,000 entries encompassing text, answers,\nclues, and categories. This dataset is intricately designed to aid in the\ngeneration of pertinent clues linked to specific texts and keywords within\ndefined categories. This project addresses the scarcity of advanced educational\ntools tailored for the Arabic language, promoting enhanced language learning\nand cognitive development. By providing a culturally and linguistically\nrelevant tool, our objective is to make learning more engaging and effective\nthrough gamification and interactivity. Integrating state-of-the-art artificial\nintelligence with contemporary learning methodologies, this tool can generate\ncrossword puzzles from any given educational text, thereby facilitating an\ninteractive and enjoyable learning experience. This tool not only advances\neducational paradigms but also sets a new standard in interactive and cognitive\nlearning technologies. The model and dataset are publicly available.", "published": "2025-01-19 12:57:34", "link": "http://arxiv.org/abs/2501.11035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LF-Steering: Latent Feature Activation Steering for Enhancing Semantic\n  Consistency in Large Language Models", "abstract": "Large Language Models (LLMs) often generate inconsistent responses when\nprompted with semantically equivalent paraphrased inputs. Recently, activation\nsteering, a technique that modulates LLMs' behaviours by adjusting their latent\nrepresentations during inference time, has been explored to improve the\nsemantic consistency of LLMs. However, these methods typically operate at the\nmodel component level, such as layer hidden states or attention head outputs.\nThey face a challenge due to the ``polysemanticity issue'', where the model\ncomponents of LLMs typically encode multiple entangled features, making precise\nsteering difficult. To address this challenge, we drill down to feature-level\nrepresentations and propose LF-Steering, a novel activation steering approach\nto precisely identify latent feature representations responsible for semantic\ninconsistency. More specifically, our method maps the hidden states of the\nrelevant transformer layer into a sparsely activated, high-dimensional feature\nspace based on a sparse autoencoder (SAE), ensuring model steering based on\ndecoupled feature representations with minimal interference. Comprehensive\nexperiments on NLU and NLG datasets demonstrate the effectiveness of our method\nin enhancing semantic consistency, resulting in significant performance gains\nfor various NLU and NLG tasks.", "published": "2025-01-19 13:06:51", "link": "http://arxiv.org/abs/2501.11036v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Semantic Consistency of Large Language Models through Model\n  Editing: An Interpretability-Oriented Approach", "abstract": "A Large Language Model (LLM) tends to generate inconsistent and sometimes\ncontradictory outputs when presented with a prompt that has equivalent\nsemantics but is expressed differently from the original prompt. To achieve\nsemantic consistency of an LLM, one of the key approaches is to finetune the\nmodel with prompt-output pairs with semantically equivalent meanings. Despite\nits effectiveness, a data-driven finetuning method incurs substantial\ncomputation costs in data preparation and model optimization. In this regime,\nan LLM is treated as a ``black box'', restricting our ability to gain deeper\ninsights into its internal mechanism. In this paper, we are motivated to\nenhance the semantic consistency of LLMs through a more interpretable method\n(i.e., model editing) to this end. We first identify the model components\n(i.e., attention heads) that have a key impact on the semantic consistency of\nan LLM. We subsequently inject biases into the output of these model components\nalong the semantic-consistency activation direction. It is noteworthy that\nthese modifications are cost-effective, without reliance on mass manipulations\nof the original model parameters. Through comprehensive experiments on the\nconstructed NLU and open-source NLG datasets, our method demonstrates\nsignificant improvements in the semantic consistency and task performance of\nLLMs. Additionally, our method exhibits promising generalization capabilities\nby performing well on tasks beyond the primary tasks.", "published": "2025-01-19 13:26:15", "link": "http://arxiv.org/abs/2501.11041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic semantic networks for exploration of creative thinking", "abstract": "Human creativity originates from brain cortical networks that are specialized\nin idea generation, processing, and evaluation. The concurrent verbalization of\nour inner thoughts during the execution of a design task enables the use of\ndynamic semantic networks as a tool for investigating, evaluating, and\nmonitoring creative thought. The primary advantage of using lexical databases\nsuch as WordNet for reproducible information-theoretic quantification of\nconvergence or divergence of design ideas in creative problem solving is the\nsimultaneous handling of both words and meanings, which enables interpretation\nof the constructed dynamic semantic networks in terms of underlying\nfunctionally active brain cortical regions involved in concept comprehension\nand production. In this study, the quantitative dynamics of semantic measures\ncomputed with a moving time window is investigated empirically in the DTRS10\ndataset with design review conversations and detected divergent thinking is\nshown to predict success of design ideas. Thus, dynamic semantic networks\npresent an opportunity for real-time computer-assisted detection of critical\nevents during creative problem solving, with the goal of employing this\nknowledge to artificially augment human creativity.", "published": "2025-01-19 15:59:07", "link": "http://arxiv.org/abs/2501.11090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large\n  Language Models via a Multi-Paradigm Perspective", "abstract": "Large Language Models (LLMs) have made notable progress in mathematical\nreasoning, yet they often rely on single-paradigm reasoning that limits their\neffectiveness across diverse tasks. In this paper, we introduce\nChain-of-Reasoning (CoR), a novel unified framework that integrates multiple\nreasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning\n(AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR\ngenerates multiple potential answers using different reasoning paradigms and\nsynthesizes them into a coherent final solution. We propose a Progressive\nParadigm Training (PPT) strategy that allows models to progressively master\nthese paradigms, culminating in the development of CoR-Math-7B. Experimental\nresults demonstrate that CoR-Math-7B significantly outperforms current SOTA\nmodels, achieving up to a 41.0% absolute improvement over GPT-4 in theorem\nproving tasks and a 7.9% improvement over RL-based methods in arithmetic tasks.\nThese results showcase the enhanced mathematical comprehensive ability of our\nmodel, achieving significant performance gains on specific tasks and enabling\nzero-shot generalization across tasks.", "published": "2025-01-19 16:53:26", "link": "http://arxiv.org/abs/2501.11110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Semantic Annotation Activities with Formal Concept Analysis", "abstract": "This paper describes an approach to assessing semantic annotation activities\nbased on formal concept analysis (FCA). In this approach, annotators use\ntaxonomical ontologies created by domain experts to annotate digital resources.\nThen, using FCA, domain experts are provided with concept lattices that\ngraphically display how their ontologies were used during the semantic\nannotation process. In consequence, they can advise annotators on how to better\nuse the ontologies, as well as how to refine them to better suit the needs of\nthe semantic annotators. To illustrate the approach, we describe its\nimplementation in @note, a Rich Internet Application (RIA) for the\ncollaborative annotation of digitized literary texts, we exemplify its use with\na case study, and we provide some evaluation results using the method.", "published": "2025-01-19 17:31:29", "link": "http://arxiv.org/abs/2501.11123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know \"No'' Better: A Data-Driven Approach for Enhancing Negation\n  Awareness in CLIP", "abstract": "While CLIP has significantly advanced multimodal understanding by bridging\nvision and language, the inability to grasp negation - such as failing to\ndifferentiate concepts like \"parking\" from \"no parking\" - poses substantial\nchallenges. By analyzing the data used in the public CLIP model's pre-training,\nwe posit this limitation stems from a lack of negation-inclusive data. To\naddress this, we introduce data generation pipelines that employ a large\nlanguage model (LLM) and a multimodal LLM to produce negation-inclusive\ncaptions. Fine-tuning CLIP with data generated from our pipelines, we develop\nNegationCLIP, which enhances negation awareness while preserving the\ngenerality. Moreover, to enable a comprehensive evaluation of negation\nunderstanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs'\nability to interpret negation across diverse expressions and positions within a\nsentence. Experiments on various CLIP architectures validate the effectiveness\nof our data generation pipelines in enhancing CLIP's ability to perceive\nnegation accurately. Additionally, NegationCLIP's enhanced negation awareness\nhas practical applications across various multimodal tasks, demonstrated by\nperformance gains in text-to-image generation and referring image segmentation.", "published": "2025-01-19 01:17:05", "link": "http://arxiv.org/abs/2501.10913v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "InsQABench: Benchmarking Chinese Insurance Domain Question Answering\n  with Large Language Models", "abstract": "The application of large language models (LLMs) has achieved remarkable\nsuccess in various fields, but their effectiveness in specialized domains like\nthe Chinese insurance industry remains underexplored. The complexity of\ninsurance knowledge, encompassing specialized terminology and diverse data\ntypes, poses significant challenges for both models and users. To address this,\nwe introduce InsQABench, a benchmark dataset for the Chinese insurance sector,\nstructured into three categories: Insurance Commonsense Knowledge, Insurance\nStructured Database, and Insurance Unstructured Documents, reflecting\nreal-world insurance question-answering tasks.We also propose two methods,\nSQL-ReAct and RAG-ReAct, to tackle challenges in structured and unstructured\ndata tasks. Evaluations show that while LLMs struggle with domain-specific\nterminology and nuanced clause texts, fine-tuning on InsQABench significantly\nimproves performance. Our benchmark establishes a solid foundation for\nadvancing LLM applications in the insurance domain, with data and code\navailable at https://github.com/HaileyFamo/InsQABench.git.", "published": "2025-01-19 04:53:20", "link": "http://arxiv.org/abs/2501.10943v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI Based Font Pair Suggestion Modelling For Graphic Design", "abstract": "One of the key challenges of AI generated designs in Microsoft Designer is\nselecting the most contextually relevant and novel fonts for the design\nsuggestions. Previous efforts involved manually mapping design intent to fonts.\nThough this was high quality, this method does not scale for a large number of\nfonts (3000+) and numerous user intents for graphic design. In this work we\ncreate font visual embeddings, a font stroke width algorithm, a font category\nto font mapping dataset, an LLM-based category utilization description and a\nlightweight, low latency knowledge-distilled mini language model (Mini LM V2)\nto recommend multiple pairs of contextual heading and subheading fonts for\nbeautiful and intuitive designs. We also utilize a weighted scoring mechanism,\nnearest neighbor approach and stratified sampling to rank the font pairs and\nbring novelty to the predictions.", "published": "2025-01-19 07:08:36", "link": "http://arxiv.org/abs/2501.10969v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Clinical trial cohort selection using Large Language Models on n2c2\n  Challenges", "abstract": "Clinical trials are a critical process in the medical field for introducing\nnew treatments and innovations. However, cohort selection for clinical trials\nis a time-consuming process that often requires manual review of patient text\nrecords for specific keywords. Though there have been studies on standardizing\nthe information across the various platforms, Natural Language Processing (NLP)\ntools remain crucial for spotting eligibility criteria in textual reports.\nRecently, pre-trained large language models (LLMs) have gained popularity for\nvarious NLP tasks due to their ability to acquire a nuanced understanding of\ntext. In this paper, we study the performance of large language models on\nclinical trial cohort selection and leverage the n2c2 challenges to benchmark\ntheir performance. Our results are promising with regard to the incorporation\nof LLMs for simple cohort selection tasks, but also highlight the difficulties\nencountered by these models as soon as fine-grained knowledge and reasoning are\nrequired.", "published": "2025-01-19 17:07:02", "link": "http://arxiv.org/abs/2501.11114v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Collection of Question Answering Datasets for Norwegian", "abstract": "This paper introduces a new suite of question answering datasets for\nNorwegian; NorOpenBookQA, NorCommonSenseQA, NorTruthfulQA, and NRK-Quiz-QA. The\ndata covers a wide range of skills and knowledge domains, including world\nknowledge, commonsense reasoning, truthfulness, and knowledge about Norway.\nCovering both of the written standards of Norwegian - Bokm{\\aa}l and Nynorsk -\nour datasets comprise over 10k question-answer pairs, created by native\nspeakers. We detail our dataset creation approach and present the results of\nevaluating 11 language models (LMs) in zero- and few-shot regimes. Most LMs\nperform better in Bokm{\\aa}l than Nynorsk, struggle most with commonsense\nreasoning, and are often untruthful in generating answers to questions. All our\ndatasets and annotation materials are publicly available.", "published": "2025-01-19 17:42:48", "link": "http://arxiv.org/abs/2501.11128v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey on Integrating Large Language Models with\n  Knowledge-Based Methods", "abstract": "The rapid development of artificial intelligence has led to marked progress\nin the field. One interesting direction for research is whether Large Language\nModels (LLMs) can be integrated with structured knowledge-based systems. This\napproach aims to combine the generative language understanding of LLMs and the\nprecise knowledge representation systems by which they are integrated. This\narticle surveys the relationship between LLMs and knowledge bases, looks at how\nthey can be applied in practice, and discusses related technical, operational,\nand ethical challenges. Utilizing a comprehensive examination of the\nliterature, the study both identifies important issues and assesses existing\nsolutions. It demonstrates the merits of incorporating generative AI into\nstructured knowledge-base systems concerning data contextualization, model\naccuracy, and utilization of knowledge resources. The findings give a full list\nof the current situation of research, point out the main gaps, and propose\nhelpful paths to take. These insights contribute to advancing AI technologies\nand support their practical deployment across various sectors.", "published": "2025-01-19 23:25:21", "link": "http://arxiv.org/abs/2501.13947v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LegalGuardian: A Privacy-Preserving Framework for Secure Integration of\n  Large Language Models in Legal Practice", "abstract": "Large Language Models (LLMs) hold promise for advancing legal practice by\nautomating complex tasks and improving access to justice. However, their\nadoption is limited by concerns over client confidentiality, especially when\nlawyers include sensitive Personally Identifiable Information (PII) in prompts,\nrisking unauthorized data exposure. To mitigate this, we introduce\nLegalGuardian, a lightweight, privacy-preserving framework tailored for lawyers\nusing LLM-based tools. LegalGuardian employs Named Entity Recognition (NER)\ntechniques and local LLMs to mask and unmask confidential PII within prompts,\nsafeguarding sensitive data before any external interaction. We detail its\ndevelopment and assess its effectiveness using a synthetic prompt library in\nimmigration law scenarios. Comparing traditional NER models with one-shot\nprompted local LLM, we find that LegalGuardian achieves a F1-score of 93% with\nGLiNER and 97% with Qwen2.5-14B in PII detection. Semantic similarity analysis\nconfirms that the framework maintains high fidelity in outputs, ensuring robust\nutility of LLM-based tools. Our findings indicate that legal professionals can\nharness advanced AI technologies without compromising client confidentiality or\nthe quality of legal documents.", "published": "2025-01-19 01:43:42", "link": "http://arxiv.org/abs/2501.10915v1", "categories": ["cs.CL", "cs.CR", "cs.IR", "68T50, 68U35", "I.2.7; K.5.0; I.7.0"], "primary_category": "cs.CL"}
{"title": "Leveraging Chain of Thought towards Empathetic Spoken Dialogue without\n  Corresponding Question-Answering Data", "abstract": "Empathetic dialogue is crucial for natural human-computer interaction,\nallowing the dialogue system to respond in a more personalized and emotionally\naware manner, improving user satisfaction and engagement. The emergence of\nlarge language models (LLMs) has revolutionized dialogue generation by\nharnessing their powerful capabilities and shown its potential in multimodal\ndomains. Many studies have integrated speech with text-based LLMs to take\nspeech question as input and output text response. However, the lack of spoken\nquestion-answering datasets that include speech style information to supervised\nfine-tuning (SFT) limits the performance of these systems. As a result, while\nthese systems excel at understanding speech content, they often struggle to\ngenerate empathetic responses. In response, we propose a novel approach that\ncircumvents the need for question-answering data, called Listen, Perceive, and\nExpress (LPE). Our method employs a two-stage training process, initially\nguiding the LLM to listen the content and perceive the emotional aspects of\nspeech. Subsequently, we utilize Chain-of-Thought (CoT) prompting to unlock the\nmodel's potential for expressing empathetic responses based on listened spoken\ncontent and perceived emotional cues. We employ experiments to prove the\neffectiveness of proposed method. To our knowledge, this is the first attempt\nto leverage CoT for speech-based dialogue.", "published": "2025-01-19 04:10:53", "link": "http://arxiv.org/abs/2501.10937v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Advancing General Multimodal Capability of Vision-language Models with\n  Pyramid-descent Visual Position Encoding", "abstract": "Vision-language Models (VLMs) have shown remarkable capabilities in advancing\ngeneral artificial intelligence, yet the irrational encoding of visual\npositions persists in inhibiting the models' comprehensive perception\nperformance across different levels of granularity. In this work, we propose\nPyramid-descent Visual Position Encoding (PyPE), a novel approach designed to\nenhance the perception of visual tokens within VLMs. By assigning visual\nposition indexes from the periphery to the center and expanding the central\nreceptive field incrementally, PyPE addresses the limitations of traditional\nraster-scan methods and mitigates the long-term decay effects induced by Rotary\nPosition Embedding (RoPE). Our method reduces the relative distance between\ninterrelated visual elements and instruction tokens, promoting a more rational\nallocation of attention weights and allowing for a multi-granularity perception\nof visual elements and countering the over-reliance on anchor tokens. Extensive\nexperimental evaluations demonstrate that PyPE consistently improves the\ngeneral capabilities of VLMs across various sizes. Code is available at\nhttps://github.com/SakuraTroyChen/PyPE.", "published": "2025-01-19 07:00:46", "link": "http://arxiv.org/abs/2501.10967v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically\n  Justify Replacing Human Annotators with LLMs", "abstract": "The \"LLM-as-a-judge\" paradigm employs Large Language Models (LLMs) as\nannotators and evaluators in tasks traditionally performed by humans. LLM\nannotations are widely used, not only in NLP research but also in fields like\nmedicine, psychology, and social science. Despite their role in shaping study\nresults and insights, there is no standard or rigorous procedure to determine\nwhether LLMs can replace human annotators. In this paper, we propose a novel\nstatistical procedure -- the Alternative Annotator Test (alt-test) -- that\nrequires only a modest subset of annotated examples to justify using LLM\nannotations. Additionally, we introduce a versatile and interpretable measure\nfor comparing LLM judges. To demonstrate our procedure, we curated a diverse\ncollection of ten datasets, consisting of language and vision-language tasks,\nand conducted experiments with six LLMs and four prompting techniques. Our\nresults show that LLMs can sometimes replace humans with closed-source LLMs\n(such as GPT-4o), outperforming open-source LLMs, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.", "published": "2025-01-19 07:09:11", "link": "http://arxiv.org/abs/2501.10970v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration\n  of Large and Small Language Model", "abstract": "Automated log analysis is crucial to ensure high availability and reliability\nof complex systems. The advent of LLMs in NLP has ushered in a new era of\nlanguage model-driven automated log analysis, garnering significant interest.\nWithin this field, two primary paradigms based on language models for log\nanalysis have become prominent. Small Language Models (SLMs) follow the\npre-train and fine-tune paradigm, focusing on the specific log analysis task\nthrough fine-tuning on supervised datasets. On the other hand, LLMs following\nthe in-context learning paradigm, analyze logs by providing a few examples in\nprompt contexts without updating parameters. Despite their respective\nstrengths, we notice that SLMs are more cost-effective but less powerful,\nwhereas LLMs with large parameters are highly powerful but expensive and\ninefficient. To trade-off between the performance and inference costs of both\nmodels in automated log analysis, this paper introduces an adaptive log\nanalysis framework known as AdaptiveLog, which effectively reduces the costs\nassociated with LLM while ensuring superior results. This framework\ncollaborates an LLM and a small language model, strategically allocating the\nLLM to tackle complex logs while delegating simpler logs to the SLM.\nSpecifically, to efficiently query the LLM, we propose an adaptive selection\nstrategy based on the uncertainty estimation of the SLM, where the LLM is\ninvoked only when the SLM is uncertain. In addition, to enhance the reasoning\nability of the LLM in log analysis tasks, we propose a novel prompt strategy by\nretrieving similar error-prone cases as the reference, enabling the model to\nleverage past error experiences and learn solutions from these cases. Extensive\nexperiments demonstrate that AdaptiveLog achieves state-of-the-art results\nacross different tasks, elevating the overall accuracy of log analysis while\nmaintaining cost efficiency.", "published": "2025-01-19 12:46:01", "link": "http://arxiv.org/abs/2501.11031v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI\n  Systems", "abstract": "Large Language Models (LLMs) are transforming artificial intelligence,\nevolving into task-oriented systems capable of autonomous planning and\nexecution. One of the primary applications of LLMs is conversational AI\nsystems, which must navigate multi-turn dialogues, integrate domain-specific\nAPIs, and adhere to strict policy constraints. However, evaluating these agents\nremains a significant challenge, as traditional methods fail to capture the\ncomplexity and variability of real-world interactions. We introduce\nIntellAgent, a scalable, open-source multi-agent framework designed to evaluate\nconversational AI systems comprehensively. IntellAgent automates the creation\nof diverse, synthetic benchmarks by combining policy-driven graph modeling,\nrealistic event generation, and interactive user-agent simulations. This\ninnovative approach provides fine-grained diagnostics, addressing the\nlimitations of static and manually curated benchmarks with coarse-grained\nmetrics. IntellAgent represents a paradigm shift in evaluating conversational\nAI. By simulating realistic, multi-policy scenarios across varying levels of\ncomplexity, IntellAgent captures the nuanced interplay of agent capabilities\nand policy constraints. Unlike traditional methods, it employs a graph-based\npolicy model to represent relationships, likelihoods, and complexities of\npolicy interactions, enabling highly detailed diagnostics. IntellAgent also\nidentifies critical performance gaps, offering actionable insights for targeted\noptimization. Its modular, open-source design supports seamless integration of\nnew domains, policies, and APIs, fostering reproducibility and community\ncollaboration. Our findings demonstrate that IntellAgent serves as an effective\nframework for advancing conversational AI by addressing challenges in bridging\nresearch and deployment. The framework is available at\nhttps://github.com/plurai-ai/intellagent", "published": "2025-01-19 14:58:35", "link": "http://arxiv.org/abs/2501.11067v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhanced Suicidal Ideation Detection from Social Media Using a\n  CNN-BiLSTM Hybrid Model", "abstract": "Suicidal ideation detection is crucial for preventing suicides, a leading\ncause of death worldwide. Many individuals express suicidal thoughts on social\nmedia, offering a vital opportunity for early detection through advanced\nmachine learning techniques. The identification of suicidal ideation in social\nmedia text is improved by utilising a hybrid framework that integrates\nConvolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory\n(BiLSTM), enhanced with an attention mechanism. To enhance the interpretability\nof the model's predictions, Explainable AI (XAI) methods are applied, with a\nparticular focus on SHapley Additive exPlanations (SHAP), are incorporated. At\nfirst, the model managed to reach an accuracy of 92.81%. By applying\nfine-tuning and early stopping techniques, the accuracy improved to 94.29%. The\nSHAP analysis revealed key features influencing the model's predictions, such\nas terms related to mental health struggles. This level of transparency boosts\nthe model's credibility while helping mental health professionals understand\nand trust the predictions. This work highlights the potential for improving the\naccuracy and interpretability of detecting suicidal tendencies, making a\nvaluable contribution to the progress of mental health monitoring systems. It\nemphasizes the significance of blending powerful machine learning methods with\nexplainability to develop reliable and impactful mental health solutions.", "published": "2025-01-19 16:08:50", "link": "http://arxiv.org/abs/2501.11094v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Tell me about yourself: LLMs are aware of their learned behaviors", "abstract": "We study behavioral self-awareness -- an LLM's ability to articulate its\nbehaviors without requiring in-context examples. We finetune LLMs on datasets\nthat exhibit particular behaviors, such as (a) making high-risk economic\ndecisions, and (b) outputting insecure code. Despite the datasets containing no\nexplicit descriptions of the associated behavior, the finetuned LLMs can\nexplicitly describe it. For example, a model trained to output insecure code\nsays, ``The code I write is insecure.'' Indeed, models show behavioral\nself-awareness for a range of behaviors and for diverse evaluations. Note that\nwhile we finetune models to exhibit behaviors like writing insecure code, we do\nnot finetune them to articulate their own behaviors -- models do this without\nany special training or examples.\n  Behavioral self-awareness is relevant for AI safety, as models could use it\nto proactively disclose problematic behaviors. In particular, we study backdoor\npolicies, where models exhibit unexpected behaviors only under certain trigger\nconditions. We find that models can sometimes identify whether or not they have\na backdoor, even without its trigger being present. However, models are not\nable to directly output their trigger by default.\n  Our results show that models have surprising capabilities for self-awareness\nand for the spontaneous articulation of implicit behaviors. Future work could\ninvestigate this capability for a wider range of scenarios and models\n(including practical scenarios), and explain how it emerges in LLMs.", "published": "2025-01-19 17:28:12", "link": "http://arxiv.org/abs/2501.11120v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AIMA at SemEval-2024 Task 10: History-Based Emotion Recognition in\n  Hindi-English Code-Mixed Conversations", "abstract": "In this study, we introduce a solution to the SemEval 2024 Task 10 on subtask\n1, dedicated to Emotion Recognition in Conversation (ERC) in code-mixed\nHindi-English conversations. ERC in code-mixed conversations presents unique\nchallenges, as existing models are typically trained on monolingual datasets\nand may not perform well on code-mixed data. To address this, we propose a\nseries of models that incorporate both the previous and future context of the\ncurrent utterance, as well as the sequential information of the conversation.\nTo facilitate the processing of code-mixed data, we developed a\nHinglish-to-English translation pipeline to translate the code-mixed\nconversations into English. We designed four different base models, each\nutilizing powerful pre-trained encoders to extract features from the input but\nwith varying architectures. By ensembling all of these models, we developed a\nfinal model that outperforms all other baselines.", "published": "2025-01-19 20:56:45", "link": "http://arxiv.org/abs/2501.11166v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair\n  Analysis", "abstract": "The SemEval-2024 Task 3 presents two subtasks focusing on emotion-cause pair\nextraction within conversational contexts. Subtask 1 revolves around the\nextraction of textual emotion-cause pairs, where causes are defined and\nannotated as textual spans within the conversation. Conversely, Subtask 2\nextends the analysis to encompass multimodal cues, including language, audio,\nand vision, acknowledging instances where causes may not be exclusively\nrepresented in the textual data. Our proposed model for emotion-cause analysis\nis meticulously structured into three core segments: (i) embedding extraction,\n(ii) cause-pair extraction & emotion classification, and (iii) cause extraction\nusing QA after finding pairs. Leveraging state-of-the-art techniques and\nfine-tuning on task-specific datasets, our model effectively unravels the\nintricate web of conversational dynamics and extracts subtle cues signifying\ncausality in emotional expressions. Our team, AIMA, demonstrated strong\nperformance in the SemEval-2024 Task 3 competition. We ranked as the 10th in\nsubtask 1 and the 6th in subtask 2 out of 23 teams.", "published": "2025-01-19 21:16:31", "link": "http://arxiv.org/abs/2501.11170v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dagger Behind Smile: Fool LLMs with a Happy Ending Story", "abstract": "The wide adoption of Large Language Models (LLMs) has attracted significant\nattention from $\\textit{jailbreak}$ attacks, where adversarial prompts crafted\nthrough optimization or manual design exploit LLMs to generate malicious\ncontents. However, optimization-based attacks have limited efficiency and\ntransferability, while existing manual designs are either easily detectable or\ndemand intricate interactions with LLMs. In this paper, we first point out a\nnovel perspective for jailbreak attacks: LLMs are more responsive to\n$\\textit{positive}$ prompts. Based on this, we deploy Happy Ending Attack (HEA)\nto wrap up a malicious request in a scenario template involving a positive\nprompt formed mainly via a $\\textit{happy ending}$, it thus fools LLMs into\njailbreaking either immediately or at a follow-up malicious request.This has\nmade HEA both efficient and effective, as it requires only up to two turns to\nfully jailbreak LLMs. Extensive experiments show that our HEA can successfully\njailbreak on state-of-the-art LLMs, including GPT-4o, Llama3-70b, Gemini-pro,\nand achieves 88.79\\% attack success rate on average. We also provide\nquantitative explanations for the success of HEA.", "published": "2025-01-19 13:39:51", "link": "http://arxiv.org/abs/2501.13115v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Self-Explanation in Social AI Agents", "abstract": "Social AI agents interact with members of a community, thereby changing the\nbehavior of the community. For example, in online learning, an AI social\nassistant may connect learners and thereby enhance social interaction. These\nsocial AI assistants too need to explain themselves in order to enhance\ntransparency and trust with the learners. We present a method of\nself-explanation that uses introspection over a self-model of an AI social\nassistant. The self-model is captured as a functional model that specifies how\nthe methods of the agent use knowledge to achieve its tasks. The process of\ngenerating self-explanations uses Chain of Thought to reflect on the self-model\nand ChatGPT to provide explanations about its functioning. We evaluate the\nself-explanation of the AI social assistant for completeness and correctness.\nWe also report on its deployment in a live class.", "published": "2025-01-19 03:03:15", "link": "http://arxiv.org/abs/2501.13945v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Hallucination Mitigation using Agentic AI Natural Language-Based\n  Frameworks", "abstract": "Hallucinations remain a significant challenge in current Generative AI\nmodels, undermining trust in AI systems and their reliability. This study\ninvestigates how orchestrating multiple specialized Artificial Intelligent\nAgents can help mitigate such hallucinations, with a focus on systems\nleveraging Natural Language Processing (NLP) to facilitate seamless agent\ninteractions. To achieve this, we design a pipeline that introduces over three\nhundred prompts, purposefully crafted to induce hallucinations, into a\nfront-end agent. The outputs are then systematically reviewed and refined by\nsecond- and third-level agents, each employing distinct large language models\nand tailored strategies to detect unverified claims, incorporate explicit\ndisclaimers, and clarify speculative content. Additionally, we introduce a set\nof novel Key Performance Indicators (KPIs) specifically designed to evaluate\nhallucination score levels. A dedicated fourth-level AI agent is employed to\nevaluate these KPIs, providing detailed assessments and ensuring accurate\nquantification of shifts in hallucination-related behaviors. A core component\nof this investigation is the use of the OVON (Open Voice Network) framework,\nwhich relies on universal NLP-based interfaces to transfer contextual\ninformation among agents. Through structured JSON messages, each agent\ncommunicates its assessment of the hallucination likelihood and the reasons\nunderlying questionable content, thereby enabling the subsequent stage to\nrefine the text without losing context. The results demonstrate that employing\nmultiple specialized agents capable of interoperating with each other through\nNLP-based agentic frameworks can yield promising outcomes in hallucination\nmitigation, ultimately bolstering trust within the AI community.", "published": "2025-01-19 11:19:25", "link": "http://arxiv.org/abs/2501.13946v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "ChaosEater: Fully Automating Chaos Engineering with Large Language\n  Models", "abstract": "Chaos Engineering (CE) is an engineering technique aimed at improving the\nresiliency of distributed systems. It involves artificially injecting specific\nfailures into a distributed system and observing its behavior in response.\nBased on the observation, the system can be proactively improved to handle\nthose failures. Recent CE tools realize the automated execution of predefined\nCE experiments. However, defining these experiments and reconfiguring the\nsystem after the experiments still remain manual. To reduce the costs of the\nmanual operations, we propose \\textsc{ChaosEater}, a \\textit{system} for\nautomating the entire CE operations with Large Language Models (LLMs). It\npre-defines the general flow according to the systematic CE cycle and assigns\nsubdivided operations within the flow to LLMs. We assume systems based on\nInfrastructure as Code (IaC), wherein the system configurations and artificial\nfailures are managed through code. Hence, the LLMs' operations in our\n\\textit{system} correspond to software engineering tasks, including requirement\ndefinition, code generation and debugging, and testing. We validate our\n\\textit{system} through case studies on both small and large systems. The\nresults demonstrate that our \\textit{system} significantly reduces both time\nand monetary costs while completing reasonable single CE cycles.", "published": "2025-01-19 16:35:09", "link": "http://arxiv.org/abs/2501.11107v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.DC", "cs.NI"], "primary_category": "cs.SE"}
{"title": "Water Flow Detection Device Based on Sound Data Analysis and Machine\n  Learning to Detect Water Leakage", "abstract": "In this paper, we introduce a novel mechanism that uses machine learning\ntechniques to detect water leaks in pipes. The proposed simple and low-cost\nmechanism is designed that can be easily installed on building pipes with\nvarious sizes. The system works based on gathering and amplifying water flow\nsignals using a mechanical sound amplifier. Then sounds are recorded and\nconverted to digital signals in order to be analyzed. After feature extraction\nand selection, deep neural networks are used to discriminate between with and\nwithout leak pipes. The experimental results show that this device can detect\nat least 100 milliliters per minute (mL/min) of water flow in a pipe so that it\ncan be used as a core of a water leakage detection system.", "published": "2025-01-19 19:24:23", "link": "http://arxiv.org/abs/2501.11151v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Neural Spoken Language Recognition: An Exploration with\n  Multilingual Datasets", "abstract": "In this research, we advanced a spoken language recognition system, moving\nbeyond traditional feature vector-based models. Our improvements focused on\neffectively capturing language characteristics over extended periods using a\nspecialized pooling layer. We utilized a broad dataset range from Common-Voice,\ntargeting ten languages across Indo-European, Semitic, and East Asian families.\nThe major innovation involved optimizing the architecture of Time Delay Neural\nNetworks. We introduced additional layers and restructured these networks into\na funnel shape, enhancing their ability to process complex linguistic patterns.\nA rigorous grid search determined the optimal settings for these networks,\nsignificantly boosting their efficiency in language pattern recognition from\naudio samples. The model underwent extensive training, including a phase with\naugmented data, to refine its capabilities. The culmination of these efforts is\na highly accurate system, achieving a 97\\% accuracy rate in language\nrecognition. This advancement represents a notable contribution to artificial\nintelligence, specifically in improving the accuracy and efficiency of language\nprocessing systems, a critical aspect in the engineering of advanced speech\nrecognition technologies.", "published": "2025-01-19 14:49:43", "link": "http://arxiv.org/abs/2501.11065v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Musical Agent Systems: MACAT and MACataRT", "abstract": "Our research explores the development and application of musical agents,\nhuman-in-the-loop generative AI systems designed to support music performance\nand improvisation within co-creative spaces. We introduce MACAT and MACataRT,\ntwo distinct musical agent systems crafted to enhance interactive music-making\nbetween human musicians and AI. MACAT is optimized for agent-led performance,\nemploying real-time synthesis and self-listening to shape its output\nautonomously, while MACataRT provides a flexible environment for collaborative\nimprovisation through audio mosaicing and sequence-based learning. Both systems\nemphasize training on personalized, small datasets, fostering ethical and\ntransparent AI engagement that respects artistic integrity. This research\nhighlights how interactive, artist-centred generative AI can expand creative\npossibilities, empowering musicians to explore new forms of artistic expression\nin real-time, performance-driven and music improvisation contexts.", "published": "2025-01-19 22:04:09", "link": "http://arxiv.org/abs/2502.00023v1", "categories": ["cs.MA", "cs.AI", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.MA"}
{"title": "Revival: Collaborative Artistic Creation through Human-AI Interactions\n  in Musical Creativity", "abstract": "Revival is an innovative live audiovisual performance and music improvisation\nby our artist collective K-Phi-A, blending human and AI musicianship to create\nelectronic music with audio-reactive visuals. The performance features\nreal-time co-creative improvisation between a percussionist, an electronic\nmusic artist, and AI musical agents. Trained in works by deceased composers and\nthe collective's compositions, these agents dynamically respond to human input\nand emulate complex musical styles. An AI-driven visual synthesizer, guided by\na human VJ, produces visuals that evolve with the musical landscape. Revival\nshowcases the potential of AI and human collaboration in improvisational\nartistic creation.", "published": "2025-01-19 08:41:31", "link": "http://arxiv.org/abs/2503.15498v1", "categories": ["cs.HC", "cs.AI", "cs.MA", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
