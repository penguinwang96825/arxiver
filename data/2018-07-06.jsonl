{"title": "Natural Language Processing for Music Knowledge Discovery", "abstract": "Today, a massive amount of musical knowledge is stored in written form, with\ntestimonies dated as far back as several centuries ago. In this work, we\npresent different Natural Language Processing (NLP) approaches to harness the\npotential of these text collections for automatic music knowledge discovery,\ncovering different phases in a prototypical NLP pipeline, namely corpus\ncompilation, text-mining, information extraction, knowledge graph generation\nand sentiment analysis. Each of these approaches is presented alongside\ndifferent use cases (i.e., flamenco, Renaissance and popular music) where large\ncollections of documents are processed, and conclusions stemming from\ndata-driven analyses are presented and discussed.", "published": "2018-07-06 00:07:27", "link": "http://arxiv.org/abs/1807.02200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The price of debiasing automatic metrics in natural language evaluation", "abstract": "For evaluating generation systems, automatic metrics such as BLEU cost\nnothing to run but have been shown to correlate poorly with human judgment,\nleading to systematic bias against certain model improvements. On the other\nhand, averaging human judgments, the unbiased gold standard, is often too\nexpensive. In this paper, we use control variates to combine automatic metrics\nwith human evaluation to obtain an unbiased estimator with lower cost than\nhuman evaluation alone. In practice, however, we obtain only a 7-13% cost\nreduction on evaluating summarization and open-response question answering\nsystems. We then prove that our estimator is optimal: there is no unbiased\nestimator with lower cost. Our theory further highlights the two fundamental\nbottlenecks---the automatic metric and the prompt shown to human\nevaluators---both of which need to be improved to obtain greater cost savings.", "published": "2018-07-06 00:11:27", "link": "http://arxiv.org/abs/1807.02202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Concept Specification and Abstraction-based Semantic Representation:\n  Addressing the Barriers to Rule-based Machine Translation", "abstract": "Rule-based machine translation is more data efficient than the big data-based\nmachine translation approaches, making it appropriate for languages with low\nbilingual corpus resources -- i.e., minority languages. However, the rule-based\napproach has declined in popularity relative to its big data cousins primarily\nbecause of the extensive training and labour required to define the language\nrules. To address this, we present a semantic representation that 1) treats all\nbits of meaning as individual concepts that 2) modify or further specify one\nanother to build a network that relates entities in space and time. Also, the\nrepresentation can 3) encapsulate propositions and thereby define concepts in\nterms of other concepts, supporting the abstraction of underlying linguistic\nand ontological details. These features afford an exact, yet intuitive semantic\nrepresentation aimed at handling the great variety in language and reducing\nlabour and training time. The proposed natural language generation, parsing,\nand translation strategies are also amenable to probabilistic modeling and thus\nto learning the necessary rules from example data.", "published": "2018-07-06 02:53:32", "link": "http://arxiv.org/abs/1807.02226v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sliced Recurrent Neural Networks", "abstract": "Recurrent neural networks have achieved great success in many NLP tasks.\nHowever, they have difficulty in parallelization because of the recurrent\nstructure, so it takes much time to train RNNs. In this paper, we introduce\nsliced recurrent neural networks (SRNNs), which could be parallelized by\nslicing the sequences into many subsequences. SRNNs have the ability to obtain\nhigh-level information through multiple layers with few extra parameters. We\nprove that the standard RNN is a special case of the SRNN when we use linear\nactivation functions. Without changing the recurrent units, SRNNs are 136 times\nas fast as standard RNNs and could be even faster when we train longer\nsequences. Experiments on six largescale sentiment analysis datasets show that\nSRNNs achieve better performance than standard RNNs.", "published": "2018-07-06 07:31:13", "link": "http://arxiv.org/abs/1807.02291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequential Copying Networks", "abstract": "Copying mechanism shows effectiveness in sequence-to-sequence based neural\nnetwork models for text generation tasks, such as abstractive sentence\nsummarization and question generation. However, existing works on modeling\ncopying or pointing mechanism only considers single word copying from the\nsource sentences. In this paper, we propose a novel copying framework, named\nSequential Copying Networks (SeqCopyNet), which not only learns to copy single\nwords, but also copies sequences from the input sentence. It leverages the\npointer networks to explicitly select a sub-span from the source side to target\nside, and integrates this sequential copying mechanism to the generation\nprocess in the encoder-decoder paradigm. Experiments on abstractive sentence\nsummarization and question generation tasks show that the proposed SeqCopyNet\ncan copy meaningful spans and outperforms the baseline models.", "published": "2018-07-06 08:09:37", "link": "http://arxiv.org/abs/1807.02301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Document Summarization by Jointly Learning to Score and Select\n  Sentences", "abstract": "Sentence scoring and sentence selection are two main steps in extractive\ndocument summarization systems. However, previous works treat them as two\nseparated subtasks. In this paper, we present a novel end-to-end neural network\nframework for extractive document summarization by jointly learning to score\nand select sentences. It first reads the document sentences with a hierarchical\nencoder to obtain the representation of sentences. Then it builds the output\nsummary by extracting sentences one by one. Different from previous methods,\nour approach integrates the selection strategy into the scoring model, which\ndirectly predicts the relative importance given previously selected sentences.\nExperiments on the CNN/Daily Mail dataset show that the proposed framework\nsignificantly outperforms the state-of-the-art extractive summarization models.", "published": "2018-07-06 08:15:15", "link": "http://arxiv.org/abs/1807.02305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Data Science of Hollywood: Using Emotional Arcs of Movies to Drive\n  Business Model Innovation in Entertainment Industries", "abstract": "Much of business literature addresses the issues of consumer-centric design:\nhow can businesses design customized services and products which accurately\nreflect consumer preferences? This paper uses data science natural language\nprocessing methodology to explore whether and to what extent emotions shape\nconsumer preferences for media and entertainment content. Using a unique\nfiltered dataset of 6,174 movie scripts, we generate a mapping of screen\ncontent to capture the emotional trajectory of each motion picture. We then\ncombine the obtained mappings into clusters which represent groupings of\nconsumer emotional journeys. These clusters are used to predict overall success\nparameters of the movies including box office revenues, viewer satisfaction\nlevels (captured by IMDb ratings), awards, as well as the number of viewers'\nand critics' reviews. We find that like books all movie stories are dominated\nby 6 basic shapes. The highest box offices are associated with the Man in a\nHole shape which is characterized by an emotional fall followed by an emotional\nrise. This shape results in financially successful movies irrespective of genre\nand production budget. Yet, Man in a Hole succeeds not because it produces most\n\"liked\" movies but because it generates most \"talked about\" movies.\nInterestingly, a carefully chosen combination of production budget and genre\nmay produce a financially successful movie with any emotional shape.\nImplications of this analysis for generating on-demand content and for driving\nbusiness model innovation in entertainment industries are discussed.", "published": "2018-07-06 01:59:42", "link": "http://arxiv.org/abs/1807.02221v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for Information Extraction", "abstract": "With rise of digital age, there is an explosion of information in the form of\nnews, articles, social media, and so on. Much of this data lies in unstructured\nform and manually managing and effectively making use of it is tedious, boring\nand labor intensive. This explosion of information and need for more\nsophisticated and efficient information handling tools gives rise to\nInformation Extraction(IE) and Information Retrieval(IR) technology.\nInformation Extraction systems takes natural language text as input and\nproduces structured information specified by certain criteria, that is relevant\nto a particular application. Various sub-tasks of IE such as Named Entity\nRecognition, Coreference Resolution, Named Entity Linking, Relation Extraction,\nKnowledge Base reasoning forms the building blocks of various high end Natural\nLanguage Processing (NLP) tasks such as Machine Translation, Question-Answering\nSystem, Natural Language Understanding, Text Summarization and Digital\nAssistants like Siri, Cortana and Google Now. This paper introduces Information\nExtraction technology, its various sub-tasks, highlights state-of-the-art\nresearch in various IE subtasks, current challenges and future research\ndirections.", "published": "2018-07-06 12:44:31", "link": "http://arxiv.org/abs/1807.02383v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JUMPER: Learning When to Make Classification Decisions in Reading", "abstract": "In early years, text classification is typically accomplished by\nfeature-based machine learning models; recently, deep neural networks, as a\npowerful learning machine, make it possible to work with raw input as the text\nstands. However, exiting end-to-end neural networks lack explicit\ninterpretation of the prediction. In this paper, we propose a novel framework,\nJUMPER, inspired by the cognitive process of text reading, that models text\nclassification as a sequential decision process. Basically, JUMPER is a neural\nsystem that scans a piece of text sequentially and makes classification\ndecisions at the time it wishes. Both the classification result and when to\nmake the classification are part of the decision process, which is controlled\nby a policy network and trained with reinforcement learning. Experimental\nresults show that a properly trained JUMPER has the following properties: (1)\nIt can make decisions whenever the evidence is enough, therefore reducing total\ntext reading by 30-40% and often finding the key rationale of prediction. (2)\nIt achieves classification accuracy better than or comparable to\nstate-of-the-art models in several benchmark and industrial datasets.", "published": "2018-07-06 08:49:56", "link": "http://arxiv.org/abs/1807.02314v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Memory Augmented Policy Optimization for Program Synthesis and Semantic\n  Parsing", "abstract": "We present Memory Augmented Policy Optimization (MAPO), a simple and novel\nway to leverage a memory buffer of promising trajectories to reduce the\nvariance of policy gradient estimate. MAPO is applicable to deterministic\nenvironments with discrete actions, such as structured prediction and\ncombinatorial optimization tasks. We express the expected return objective as a\nweighted sum of two terms: an expectation over the high-reward trajectories\ninside the memory buffer, and a separate expectation over trajectories outside\nthe buffer. To make an efficient algorithm of MAPO, we propose: (1) memory\nweight clipping to accelerate and stabilize training; (2) systematic\nexploration to discover high-reward trajectories; (3) distributed sampling from\ninside and outside of the memory buffer to scale up training. MAPO improves the\nsample efficiency and robustness of policy gradient, especially on tasks with\nsparse rewards. We evaluate MAPO on weakly supervised program synthesis from\nnatural language (semantic parsing). On the WikiTableQuestions benchmark, we\nimprove the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the\nWikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak\nsupervision, outperforming several strong baselines with full supervision. Our\nsource code is available at\nhttps://github.com/crazydonkey200/neural-symbolic-machines", "published": "2018-07-06 09:15:05", "link": "http://arxiv.org/abs/1807.02322v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Testing Untestable Neural Machine Translation: An Industrial Case", "abstract": "Neural Machine Translation (NMT) has been widely adopted recently due to its\nadvantages compared with the traditional Statistical Machine Translation (SMT).\nHowever, an NMT system still often produces translation failures due to the\ncomplexity of natural language and sophistication in designing neural networks.\nWhile in-house black-box system testing based on reference translations (i.e.,\nexamples of valid translations) has been a common practice for NMT quality\nassurance, an increasingly critical industrial practice, named in-vivo testing,\nexposes unseen types or instances of translation failures when real users are\nusing a deployed industrial NMT system. To fill the gap of lacking test oracle\nfor in-vivo testing of an NMT system, in this paper, we propose a new approach\nfor automatically identifying translation failures, without requiring reference\ntranslations for a translation task; our approach can directly serve as a test\noracle for in-vivo testing. Our approach focuses on properties of natural\nlanguage translation that can be checked systematically and uses information\nfrom both the test inputs (i.e., the texts to be translated) and the test\noutputs (i.e., the translations under inspection) of the NMT system. Our\nevaluation conducted on real-world datasets shows that our approach can\neffectively detect targeted property violations as translation failures. Our\nexperiences on deploying our approach in both production and development\nenvironments of WeChat (a messenger app with over one billion monthly active\nusers) demonstrate high effectiveness of our approach along with high industry\nimpact.", "published": "2018-07-06 10:17:44", "link": "http://arxiv.org/abs/1807.02340v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Tone Recognition Using Lifters and CTC", "abstract": "In this paper, we present a new method for recognizing tones in continuous\nspeech for tonal languages. The method works by converting the speech signal to\na cepstrogram, extracting a sequence of cepstral features using a convolutional\nneural network, and predicting the underlying sequence of tones using a\nconnectionist temporal classification (CTC) network. The performance of the\nproposed method is evaluated on a freely available Mandarin Chinese speech\ncorpus, AISHELL-1, and is shown to outperform the existing techniques in the\nliterature in terms of tone error rate (TER).", "published": "2018-07-06 16:05:00", "link": "http://arxiv.org/abs/1807.02465v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Singing Style Transfer Using Cycle-Consistent Boundary Equilibrium\n  Generative Adversarial Networks", "abstract": "Can we make a famous rap singer like Eminem sing whatever our favorite song?\nSinging style transfer attempts to make this possible, by replacing the vocal\nof a song from the source singer to the target singer. This paper presents a\nmethod that learns from unpaired data for singing style transfer using\ngenerative adversarial networks.", "published": "2018-07-06 04:32:18", "link": "http://arxiv.org/abs/1807.02254v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
