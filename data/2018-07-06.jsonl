{"title": "Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes", "abstract": "While designing the state space of an MDP, it is common to include states that are transient or not reachable by any policy (e.g., in mountain car, the product space of speed and position contains configurations that are not physically reachable). This leads to defining weakly-communicating or multi-chain MDPs. In this paper, we introduce \\tucrl, the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process (MDP) without requiring any form of prior knowledge. In particular, for any MDP with $S^{\\texttt{C}}$ communicating states, $A$ actions and $\u0393^{\\texttt{C}} \\leq S^{\\texttt{C}}$ possible communicating next states, we derive a $\\widetilde{O}(D^{\\texttt{C}} \\sqrt{\u0393^{\\texttt{C}} S^{\\texttt{C}} AT})$ regret bound, where $D^{\\texttt{C}}$ is the diameter (i.e., the longest shortest path) of the communicating part of the MDP. This is in contrast with optimistic algorithms (e.g., UCRL, Optimistic PSRL) that suffer linear regret in weakly-communicating MDPs, as well as posterior sampling or regularised algorithms (e.g., REGAL), which require prior knowledge on the bias span of the optimal policy to bias the exploration to achieve sub-linear regret. We also prove that in weakly-communicating MDPs, no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP. Finally, we report numerical simulations supporting our theoretical findings and showing how TUCRL overcomes the limitations of the state-of-the-art.", "published": "2018-07-06 12:13:52", "link": "http://arxiv.org/abs/1807.02373v2", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data", "abstract": "We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R^3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.", "published": "2018-07-06 19:06:12", "link": "http://arxiv.org/abs/1807.02547v2", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "M-ADDA: Unsupervised Domain Adaptation with Deep Metric Learning", "abstract": "Unsupervised domain adaptation techniques have been successful for a wide range of problems where supervised labels are limited. The task is to classify an unlabeled `target' dataset by leveraging a labeled `source' dataset that comes from a slightly similar distribution. We propose metric-based adversarial discriminative domain adaptation (M-ADDA) which performs two main steps. First, it uses a metric learning approach to train the source model on the source dataset by optimizing the triplet loss function. This results in clusters where embeddings of the same label are close to each other and those with different labels are far from one another. Next, it uses the adversarial approach (as that used in ADDA \\cite{2017arXiv170205464T}) to make the extracted features from the source and target datasets indistinguishable. Simultaneously, we optimize a novel loss function that encourages the target dataset's embeddings to form clusters. While ADDA and M-ADDA use similar architectures, we show that M-ADDA performs significantly better on the digits adaptation datasets of MNIST and USPS. This suggests that using metric-learning for domain adaptation can lead to large improvements in classification accuracy for the domain adaptation task. The code is available at \\url{https://github.com/IssamLaradji/M-ADDA}.", "published": "2018-07-06 19:21:59", "link": "http://arxiv.org/abs/1807.02552v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Forecasting Disease Trajectories in Alzheimer's Disease Using Deep Learning", "abstract": "Joint models for longitudinal and time-to-event data are commonly used in longitudinal studies to forecast disease trajectories over time. Despite the many advantages of joint modeling, the standard forms suffer from limitations that arise from a fixed model specification and computational difficulties when applied to large datasets. We adopt a deep learning approach to address these limitations, enhancing existing methods with the flexibility and scalability of deep neural networks while retaining the benefits of joint modeling. Using data from the Alzheimer's Disease Neuroimaging Institute, we show improvements in performance and scalability compared to traditional methods.", "published": "2018-07-06 16:28:58", "link": "http://arxiv.org/abs/1807.03159v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Cause-Effect Deep Information Bottleneck For Systematically Missing Covariates", "abstract": "Estimating the causal effects of an intervention from high-dimensional observational data is difficult due to the presence of confounding. The task is often complicated by the fact that we may have a systematic missingness in our data at test time. Our approach uses the information bottleneck to perform a low-dimensional compression of covariates by explicitly considering the relevance of information. Based on the sufficiently reduced covariate, we transfer the relevant information to cases where data is missing at test time, allowing us to reliably and accurately estimate the effects of an intervention, even where data is incomplete. Our results on causal inference benchmarks and a real application for treating sepsis show that our method achieves state-of-the art performance, without sacrificing interpretability.", "published": "2018-07-06 09:29:57", "link": "http://arxiv.org/abs/1807.02326v3", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Fully Scalable Gaussian Processes using Subspace Inducing Inputs", "abstract": "We introduce fully scalable Gaussian processes, an implementation scheme that tackles the problem of treating a high number of training instances together with high dimensional input data. Our key idea is a representation trick over the inducing variables called subspace inducing inputs. This is combined with certain matrix-preconditioning based parametrizations of the variational distributions that lead to simplified and numerically stable variational lower bounds. Our illustrative applications are based on challenging extreme multi-label classification problems with the extra burden of the very large number of class labels. We demonstrate the usefulness of our approach by presenting predictive performances together with low computational times in datasets with extremely large number of instances and input dimensions.", "published": "2018-07-06 18:18:36", "link": "http://arxiv.org/abs/1807.02537v2", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences", "abstract": "This paper is an attempt to bridge the conceptual gaps between researchers working on the two widely used approaches based on positive definite kernels: Bayesian learning or inference using Gaussian processes on the one side, and frequentist kernel methods based on reproducing kernel Hilbert spaces on the other. It is widely known in machine learning that these two formalisms are closely related; for instance, the estimator of kernel ridge regression is identical to the posterior mean of Gaussian process regression. However, they have been studied and developed almost independently by two essentially separate communities, and this makes it difficult to seamlessly transfer results between them. Our aim is to overcome this potential difficulty. To this end, we review several old and new results and concepts from either side, and juxtapose algorithmic quantities from each framework to highlight close similarities. We also provide discussions on subtle philosophical and theoretical differences between the two approaches.", "published": "2018-07-06 22:44:10", "link": "http://arxiv.org/abs/1807.02582v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
