{"title": "Merging External Bilingual Pairs into Neural Machine Translation", "abstract": "As neural machine translation (NMT) is not easily amenable to explicit\ncorrection of errors, incorporating pre-specified translations into NMT is\nwidely regarded as a non-trivial challenge. In this paper, we propose and\nexplore three methods to endow NMT with pre-specified bilingual pairs. Instead,\nfor instance, of modifying the beam search algorithm during decoding or making\ncomplex modifications to the attention mechanism --- mainstream approaches to\ntackling this challenge ---, we experiment with the training data being\nappropriately pre-processed to add information about pre-specified\ntranslations. Extra embeddings are also used to distinguish pre-specified\ntokens from the other tokens. Extensive experimentation and analysis indicate\nthat over 99% of the pre-specified phrases are successfully translated (given a\n85% baseline) and that there is also a substantive improvement in translation\nquality with the methods explored here.", "published": "2019-12-02 03:05:50", "link": "http://arxiv.org/abs/1912.00567v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English", "abstract": "We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP),\na challenge set for evaluating what language models (LMs) know about major\ngrammatical phenomena in English. BLiMP consists of 67 sub-datasets, each\ncontaining 1000 minimal pairs isolating specific contrasts in syntax,\nmorphology, or semantics. The data is automatically generated according to\nexpert-crafted grammars, and aggregate human agreement with the labels is\n96.4%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and\nTransformer-XL) LMs. We find that state-of-the-art models identify\nmorphological contrasts reliably, but they struggle with semantic restrictions\non the distribution of quantifiers and negative polarity items and subtle\nsyntactic phenomena such as extraction islands.", "published": "2019-12-02 05:42:41", "link": "http://arxiv.org/abs/1912.00582v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fiction Sentence Expansion and Enhancement via Focused Objective and\n  Novelty Curve Sampling", "abstract": "We describe the task of sentence expansion and enhancement, in which a\nsentence provided by a human is expanded in some creative way. The expansion\nshould be understandable, believably grammatical, and optimally\nmeaning-preserving. Sentence expansion and enhancement may serve as an\nauthoring tool, or integrate in dynamic media, conversational agents, or\nvariegated advertising.\n  We implement a neural sentence expander trained on sentence compressions\ngenerated from a corpus of modern fiction. We modify an MLE objective to\nsupport the task by focusing on new words, and decode at test time with\ncontrolled curve-like novelty sampling. We run our sentence expander on\nsentences provided by human subjects and have humans evaluate these expansions.\nWe show that, although the generation methods are inferior to professional\nhuman writers, they are comparable to, and as well liked as, our subjects'\noriginal input sentences, and preferred over baselines.", "published": "2019-12-02 11:51:57", "link": "http://arxiv.org/abs/1912.00698v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological Tagging and Lemmatization of Albanian: A Manually\n  Annotated Corpus and Neural Models", "abstract": "In this paper, we present the first publicly available part-of-speech and\nmorphologically tagged corpus for the Albanian language, as well as a neural\nmorphological tagger and lemmatizer trained on it. There is currently a lack of\navailable NLP resources for Albanian, and its complex grammar and morphology\npresent challenges to their development. We have created an Albanian\npart-of-speech corpus based on the Universal Dependencies schema for\nmorphological annotation, containing about 118,000 tokens of naturally occuring\ntext collected from different text sources, with an addition of 67,000 tokens\nof artificially created simple sentences used only in training. On this corpus,\nwe subsequently train and evaluate segmentation, morphological tagging and\nlemmatization models, using the Turku Neural Parser Pipeline. On the held-out\nevaluation set, the model achieves 92.74% accuracy on part-of-speech tagging,\n85.31% on morphological tagging, and 89.95% on lemmatization. The manually\nannotated corpus, as well as the trained models are available under an open\nlicense.", "published": "2019-12-02 18:50:37", "link": "http://arxiv.org/abs/1912.00991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TutorialVQA: Question Answering Dataset for Tutorial Videos", "abstract": "Despite the number of currently available datasets on video question\nanswering, there still remains a need for a dataset involving multi-step and\nnon-factoid answers. Moreover, relying on video transcripts remains an\nunder-explored topic. To adequately address this, We propose a new question\nanswering task on instructional videos, because of their verbose and narrative\nnature. While previous studies on video question answering have focused on\ngenerating a short text as an answer, given a question and video clip, our task\naims to identify a span of a video segment as an answer which contains\ninstructional details with various granularities. This work focuses on\nscreencast tutorial videos pertaining to an image editing program. We introduce\na dataset, TutorialVQA, consisting of about 6,000manually collected triples of\n(video, question, answer span). We also provide experimental results with\nseveral baselines algorithms using the video transcripts. The results indicate\nthat the task is challenging and call for the investigation of new algorithms.", "published": "2019-12-02 19:17:57", "link": "http://arxiv.org/abs/1912.01046v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Contextual Embeddings for Detecting Diachronic Semantic Shift", "abstract": "We propose a new method that leverages contextual embeddings for the task of\ndiachronic semantic shift detection by generating time specific word\nrepresentations from BERT embeddings. The results of our experiments in the\ndomain specific LiverpoolFC corpus suggest that the proposed method has\nperformance comparable to the current state-of-the-art without requiring any\ntime consuming domain adaptation on large corpora. The results on the newly\ncreated Brexit news corpus suggest that the method can be successfully used for\nthe detection of a short-term yearly semantic shift. And lastly, the model also\nshows promising results in a multilingual settings, where the task was to\ndetect differences and similarities between diachronic semantic shifts in\ndifferent languages.", "published": "2019-12-02 20:52:18", "link": "http://arxiv.org/abs/1912.01072v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Academic Paper Generation", "abstract": "In this work, we tackle the problem of structured text generation,\nspecifically academic paper generation in $\\LaTeX{}$, inspired by the\nsurprisingly good results of basic character-level language models. Our\nmotivation is using more recent and advanced methods of language modeling on a\nmore complex dataset of $\\LaTeX{}$ source files to generate realistic academic\npapers. Our first contribution is preparing a dataset with $\\LaTeX{}$ source\nfiles on recent open-source computer vision papers. Our second contribution is\nexperimenting with recent methods of language modeling and text generation such\nas Transformer and Transformer-XL to generate consistent $\\LaTeX{}$ code. We\nreport cross-entropy and bits-per-character (BPC) results of the trained\nmodels, and we also discuss interesting points on some examples of the\ngenerated $\\LaTeX{}$ code.", "published": "2019-12-02 18:45:23", "link": "http://arxiv.org/abs/1912.01982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Scale Self-Attention for Text Classification", "abstract": "In this paper, we introduce the prior knowledge, multi-scale structure, into\nself-attention modules. We propose a Multi-Scale Transformer which uses\nmulti-scale multi-head self-attention to capture features from different\nscales. Based on the linguistic perspective and the analysis of pre-trained\nTransformer (BERT) on a huge corpus, we further design a strategy to control\nthe scale distribution for each layer. Results of three different kinds of\ntasks (21 datasets) show our Multi-Scale Transformer outperforms the standard\nTransformer consistently and significantly on small and moderate size datasets.", "published": "2019-12-02 02:08:00", "link": "http://arxiv.org/abs/1912.00544v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstract Reasoning with Distracting Features", "abstract": "Abstraction reasoning is a long-standing challenge in artificial\nintelligence. Recent studies suggest that many of the deep architectures that\nhave triumphed over other domains failed to work well in abstract reasoning. In\nthis paper, we first illustrate that one of the main challenges in such a\nreasoning task is the presence of distracting features, which requires the\nlearning algorithm to leverage counterevidence and to reject any of the false\nhypotheses in order to learn the true patterns. We later show that carefully\ndesigned learning trajectory over different categories of training data can\neffectively boost learning performance by mitigating the impacts of distracting\nfeatures. Inspired by this fact, we propose feature robust abstract reasoning\n(FRAR) model, which consists of a reinforcement learning based teacher network\nto determine the sequence of training and a student network for predictions.\nExperimental results demonstrated strong improvements over baseline algorithms\nand we are able to beat the state-of-the-art models by 18.7% in the RAVEN\ndataset and 13.3% in the PGM dataset.", "published": "2019-12-02 03:14:23", "link": "http://arxiv.org/abs/1912.00569v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "GANCoder: An Automatic Natural Language-to-Programming Language\n  Translation Approach based on GAN", "abstract": "We propose GANCoder, an automatic programming approach based on Generative\nAdversarial Networks (GAN), which can generate the same functional and logical\nprogramming language codes conditioned on the given natural language\nutterances. The adversarial training between generator and discriminator helps\ngenerator learn distribution of dataset and improve code generation quality.\nOur experimental results show that GANCoder can achieve comparable accuracy\nwith the state-of-the-art methods and is more stable when programming\nlanguages.", "published": "2019-12-02 07:41:25", "link": "http://arxiv.org/abs/1912.00609v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading\n  and Annotation", "abstract": "We recorded and preprocessed ZuCo 2.0, a new dataset of simultaneous\neye-tracking and electroencephalography during natural reading and during\nannotation. This corpus contains gaze and brain activity data of 739 sentences,\n349 in a normal reading paradigm and 390 in a task-specific paradigm, in which\nthe 18 participants actively search for a semantic relation type in the given\nsentences as a linguistic annotation task. This new dataset complements ZuCo\n1.0 by providing experiments designed to analyze the differences in cognitive\nprocessing between natural reading and annotation. The data is freely available\nhere: https://osf.io/2urht/.", "published": "2019-12-02 16:30:46", "link": "http://arxiv.org/abs/1912.00903v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Language Model Bootstrapping Using Neural Machine Translation For\n  Conversational Speech Recognition", "abstract": "Building conversational speech recognition systems for new languages is\nconstrained by the availability of utterances that capture user-device\ninteractions. Data collection is both expensive and limited by the speed of\nmanual transcription. In order to address this, we advocate the use of neural\nmachine translation as a data augmentation technique for bootstrapping language\nmodels. Machine translation (MT) offers a systematic way of incorporating\ncollections from mature, resource-rich conversational systems that may be\navailable for a different language. However, ingesting raw translations from a\ngeneral purpose MT system may not be effective owing to the presence of named\nentities, intra sentential code-switching and the domain mismatch between the\nconversational data being translated and the parallel text used for MT\ntraining. To circumvent this, we explore the following domain adaptation\ntechniques: (a) sentence embedding based data selection for MT training, (b)\nmodel finetuning, and (c) rescoring and filtering translated hypotheses. Using\nHindi as the experimental testbed, we translate US English utterances to\nsupplement the transcribed collections. We observe a relative word error rate\nreduction of 7.8-15.6%, depending on the bootstrapping phase. Fine grained\nanalysis reveals that translation particularly aids the interaction scenarios\nwhich are underrepresented in the transcribed data.", "published": "2019-12-02 17:42:58", "link": "http://arxiv.org/abs/1912.00958v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Word Ratings for Empathy and Distress from Document-Level User\n  Responses", "abstract": "Despite the excellent performance of black box approaches to modeling\nsentiment and emotion, lexica (sets of informative words and associated\nweights) that characterize different emotions are indispensable to the NLP\ncommunity because they allow for interpretable and robust predictions. Emotion\nanalysis of text is increasing in popularity in NLP; however, manually creating\nlexica for psychological constructs such as empathy has proven difficult. This\npaper automatically creates empathy word ratings from document-level ratings.\nThe underlying problem of learning word ratings from higher-level supervision\nhas to date only been addressed in an ad hoc fashion and has not used deep\nlearning methods. We systematically compare a number of approaches to learning\nword ratings from higher-level supervision against a Mixed-Level Feed Forward\nNetwork (MLFFN), which we find performs best, and use the MLFFN to create the\nfirst-ever empathy lexicon. We then use Signed Spectral Clustering to gain\ninsights into the resulting words. The empathy and distress lexica are publicly\navailable at: http://www.wwbp.org/lexica.html.", "published": "2019-12-02 21:19:22", "link": "http://arxiv.org/abs/1912.01079v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Deep Bayesian Active Learning for Multiple Correct Outputs", "abstract": "Typical active learning strategies are designed for tasks, such as\nclassification, with the assumption that the output space is mutually\nexclusive. The assumption that these tasks always have exactly one correct\nanswer has resulted in the creation of numerous uncertainty-based measurements,\nsuch as entropy and least confidence, which operate over a model's outputs.\nUnfortunately, many real-world vision tasks, like visual question answering and\nimage captioning, have multiple correct answers, causing these measurements to\noverestimate uncertainty and sometimes perform worse than a random sampling\nbaseline. In this paper, we propose a new paradigm that estimates uncertainty\nin the model's internal hidden space instead of the model's output space. We\nspecifically study a manifestation of this problem for visual question answer\ngeneration (VQA), where the aim is not to classify the correct answer but to\nproduce a natural language answer, given an image and a question. Our method\novercomes the paraphrastic nature of language. It requires a semantic space\nthat structures the model's output concepts and that enables the usage of\ntechniques like dropout-based Bayesian uncertainty. We build a visual-semantic\nspace that embeds paraphrases close together for any existing VQA model. We\nempirically show state-of-art active learning results on the task of VQA on two\ndatasets, being 5 times more cost-efficient on Visual Genome and 3 times more\ncost-efficient on VQA 2.0.", "published": "2019-12-02 23:09:16", "link": "http://arxiv.org/abs/1912.01119v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Large-scale text processing pipeline with Apache Spark", "abstract": "In this paper, we evaluate Apache Spark for a data-intensive machine learning\nproblem. Our use case focuses on policy diffusion detection across the state\nlegislatures in the United States over time. Previous work on policy diffusion\nhas been unable to make an all-pairs comparison between bills due to\ncomputational intensity. As a substitute, scholars have studied single topic\nareas.\n  We provide an implementation of this analysis workflow as a distributed text\nprocessing pipeline with Spark dataframes and Scala application programming\ninterface. We discuss the challenges and strategies of unstructured data\nprocessing, data formats for storage and efficient access, and graph processing\nat scale.", "published": "2019-12-02 02:12:15", "link": "http://arxiv.org/abs/1912.00547v1", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation\n  Estimation in Micropost Event Detection", "abstract": "Microblogging platforms such as Twitter are increasingly being used in event\ndetection. Existing approaches mainly use machine learning models and rely on\nevent-related keywords to collect the data for model training. These approaches\nmake strong assumptions on the distribution of the relevant micro-posts\ncontaining the keyword -- referred to as the expectation of the distribution --\nand use it as a posterior regularization parameter during model training. Such\napproaches are, however, limited as they fail to reliably estimate the\ninformativeness of a keyword and its expectation for model training. This paper\nintroduces a Human-AI loop approach to jointly discover informative keywords\nfor model training while estimating their expectation. Our approach iteratively\nleverages the crowd to estimate both keyword specific expectation and the\ndisagreement between the crowd and the model in order to discover new keywords\nthat are most beneficial for model training. These keywords and their\nexpectation not only improve the resulting performance but also make the model\ntraining process more transparent. We empirically demonstrate the merits of our\napproach, both in terms of accuracy and interpretability, on multiple\nreal-world datasets and show that our approach improves the state of the art by\n24.3%.", "published": "2019-12-02 10:18:27", "link": "http://arxiv.org/abs/1912.00667v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "EduBERT: Pretrained Deep Language Models for Learning Analytics", "abstract": "The use of large pretrained neural networks to create contextualized word\nembeddings has drastically improved performance on several natural language\nprocessing (NLP) tasks. These computationally expensive models have begun to be\napplied to domain-specific NLP tasks such as re-hospitalization prediction from\nclinical notes. This paper demonstrates that using large pretrained models\nproduces excellent results on common learning analytics tasks. Pre-training\ndeep language models using student forum data from a wide array of online\ncourses improves performance beyond the state of the art on three text\nclassification tasks. We also show that a smaller, distilled version of our\nmodel produces the best results on two of the three tasks while limiting\ncomputational cost. We make both models available to the research community at\nlarge.", "published": "2019-12-02 11:32:53", "link": "http://arxiv.org/abs/1912.00690v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "SemEval-2017 Task 3: Community Question Answering", "abstract": "We describe SemEval-2017 Task 3 on Community Question Answering. This year,\nwe reran the four subtasks from SemEval-2016:(A) Question-Comment\nSimilarity,(B) Question-Question Similarity,(C) Question-External Comment\nSimilarity, and (D) Rerank the correct answers for a new question in Arabic,\nproviding all the data from 2015 and 2016 for training, and fresh data for\ntesting. Additionally, we added a new subtask E in order to enable\nexperimentation with Multi-domain Question Duplicate Detection in a\nlarger-scale scenario, using StackExchange subforums. A total of 23 teams\nparticipated in the task, and submitted a total of 85 runs (36 primary and 49\ncontrastive) for subtasks A-D. Unfortunately, no teams participated in subtask\nE. A variety of approaches and features were used by the participating systems\nto address the different subtasks. The best systems achieved an official score\n(MAP) of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D,\nrespectively. These scores are better than the baselines, especially for\nsubtasks A-C.", "published": "2019-12-02 12:57:52", "link": "http://arxiv.org/abs/1912.00730v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter", "abstract": "This paper describes the fifth year of the Sentiment Analysis in Twitter\ntask. SemEval-2017 Task 4 continues with a rerun of the subtasks of\nSemEval-2016 Task 4, which include identifying the overall sentiment of the\ntweet, sentiment towards a topic with classification on a two-point and on a\nfive-point ordinal scale, and quantification of the distribution of sentiment\ntowards a topic across a number of tweets: again on a two-point and on a\nfive-point ordinal scale. Compared to 2016, we made two changes: (i) we\nintroduced a new language, Arabic, for all subtasks, and (ii)~we made available\ninformation from the profiles of the Twitter users who posted the target\ntweets. The task continues to be very popular, with a total of 48 teams\nparticipating this year.", "published": "2019-12-02 13:04:35", "link": "http://arxiv.org/abs/1912.00741v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "EDA: Enriching Emotional Dialogue Acts using an Ensemble of Neural\n  Annotators", "abstract": "The recognition of emotion and dialogue acts enriches conversational analysis\nand help to build natural dialogue systems. Emotion interpretation makes us\nunderstand feelings and dialogue acts reflect the intentions and performative\nfunctions in the utterances. However, most of the textual and multi-modal\nconversational emotion corpora contain only emotion labels but not dialogue\nacts. To address this problem, we propose to use a pool of various recurrent\nneural models trained on a dialogue act corpus, with and without context. These\nneural models annotate the emotion corpora with dialogue act labels, and an\nensemble annotator extracts the final dialogue act label. We annotated two\naccessible multi-modal emotion corpora: IEMOCAP and MELD. We analyzed the\nco-occurrence of emotion and dialogue act labels and discovered specific\nrelations. For example, Accept/Agree dialogue acts often occur with the Joy\nemotion, Apology with Sadness, and Thanking with Joy. We make the Emotional\nDialogue Acts (EDA) corpus publicly available to the research community for\nfurther study and analysis.", "published": "2019-12-02 14:29:22", "link": "http://arxiv.org/abs/1912.00819v3", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Solving Arithmetic Word Problems Automatically Using Transformer and\n  Unambiguous Representations", "abstract": "Constructing accurate and automatic solvers of math word problems has proven\nto be quite challenging. Prior attempts using machine learning have been\ntrained on corpora specific to math word problems to produce arithmetic\nexpressions in infix notation before answer computation. We find that\ncustom-built neural networks have struggled to generalize well. This paper\noutlines the use of Transformer networks trained to translate math word\nproblems to equivalent arithmetic expressions in infix, prefix, and postfix\nnotations. In addition to training directly on domain-specific corpora, we use\nan approach that pre-trains on a general text corpus to provide foundational\nlanguage abilities to explore if it improves performance. We compare results\nproduced by a large number of neural configurations and find that most\nconfigurations outperform previously reported approaches on three of four\ndatasets with significant increases in accuracy of over 20 percentage points.\nThe best neural approaches boost accuracy by almost 10% on average when\ncompared to the previous state of the art.", "published": "2019-12-02 15:42:06", "link": "http://arxiv.org/abs/1912.00871v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improving Question Generation with Sentence-level Semantic Matching and\n  Answer Position Inferring", "abstract": "Taking an answer and its context as input, sequence-to-sequence models have\nmade considerable progress on question generation. However, we observe that\nthese approaches often generate wrong question words or keywords and copy\nanswer-irrelevant words from the input. We believe that lacking global question\nsemantics and exploiting answer position-awareness not well are the key root\ncauses. In this paper, we propose a neural question generation model with two\nconcrete modules: sentence-level semantic matching and answer position\ninferring. Further, we enhance the initial state of the decoder by leveraging\nthe answer-aware gated fusion mechanism. Experimental results demonstrate that\nour model outperforms the state-of-the-art (SOTA) models on SQuAD and MARCO\ndatasets. Owing to its generality, our work also improves the existing models\nsignificantly.", "published": "2019-12-02 15:57:40", "link": "http://arxiv.org/abs/1912.00879v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven\n  Acoustic Embedding Selection", "abstract": "Recent advances in Text-to-Speech (TTS) have improved quality and naturalness\nto near-human capabilities when considering isolated sentences. But something\nwhich is still lacking in order to achieve human-like communication is the\ndynamic variations and adaptability of human speech. This work attempts to\nsolve the problem of achieving a more dynamic and natural intonation in TTS\nsystems, particularly for stylistic speech such as the newscaster speaking\nstyle. We propose a novel embedding selection approach which exploits\nlinguistic information, leveraging the speech variability present in the\ntraining dataset. We analyze the contribution of both semantic and syntactic\nfeatures. Our results show that the approach improves the prosody and\nnaturalness for complex utterances as well as in Long Form Reading (LFR).", "published": "2019-12-02 17:32:59", "link": "http://arxiv.org/abs/1912.00955v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TX-Ray: Quantifying and Explaining Model-Knowledge Transfer in\n  (Un-)Supervised NLP", "abstract": "While state-of-the-art NLP explainability (XAI) methods focus on explaining\nper-sample decisions in supervised end or probing tasks, this is insufficient\nto explain and quantify model knowledge transfer during (un-)supervised\ntraining. Thus, for TX-Ray, we modify the established computer vision\nexplainability principle of 'visualizing preferred inputs of neurons' to make\nit usable transfer analysis and NLP. This allows one to analyze, track and\nquantify how self- or supervised NLP models first build knowledge abstractions\nin pretraining (1), and then transfer these abstractions to a new domain (2),\nor adapt them during supervised fine-tuning (3). TX-Ray expresses neurons as\nfeature preference distributions to quantify fine-grained knowledge transfer or\nadaptation and guide human analysis. We find that, similar to Lottery Ticket\nbased pruning, TX-Ray based pruning can improve test set generalization and\nthat it can reveal how early stages of self-supervision automatically learn\nlinguistic abstractions like parts-of-speech.", "published": "2019-12-02 18:21:31", "link": "http://arxiv.org/abs/1912.00982v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Simultaneously Linking Entities and Extracting Relations from Biomedical\n  Text Without Mention-level Supervision", "abstract": "Understanding the meaning of text often involves reasoning about entities and\ntheir relationships. This requires identifying textual mentions of entities,\nlinking them to a canonical concept, and discerning their relationships. These\ntasks are nearly always viewed as separate components within a pipeline, each\nrequiring a distinct model and training data. While relation extraction can\noften be trained with readily available weak or distant supervision, entity\nlinkers typically require expensive mention-level supervision -- which is not\navailable in many domains. Instead, we propose a model which is trained to\nsimultaneously produce entity linking and relation decisions while requiring no\nmention-level annotations. This approach avoids cascading errors that arise\nfrom pipelined methods and more accurately predicts entity relationships from\ntext. We show that our model outperforms a state-of-the art entity linking and\nrelation extraction pipeline on two biomedical datasets and can drastically\nimprove the overall recall of the system.", "published": "2019-12-02 20:37:18", "link": "http://arxiv.org/abs/1912.01070v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Long Distance Relationships without Time Travel: Boosting the\n  Performance of a Sparse Predictive Autoencoder in Sequence Modeling", "abstract": "In sequence learning tasks such as language modelling, Recurrent Neural\nNetworks must learn relationships between input features separated by time.\nState of the art models such as LSTM and Transformer are trained by\nbackpropagation of losses into prior hidden states and inputs held in memory.\nThis allows gradients to flow from present to past and effectively learn with\nperfect hindsight, but at a significant memory cost. In this paper we show that\nit is possible to train high performance recurrent networks using information\nthat is local in time, and thereby achieve a significantly reduced memory\nfootprint. We describe a predictive autoencoder called bRSM featuring recurrent\nconnections, sparse activations, and a boosting rule for improved cell\nutilization. The architecture demonstrates near optimal performance on a\nnon-deterministic (stochastic) partially-observable sequence learning task\nconsisting of high-Markov-order sequences of MNIST digits. We find that this\nmodel learns these sequences faster and more completely than an LSTM, and offer\nseveral possible explanations why the LSTM architecture might struggle with the\npartially observable sequence structure in this task. We also apply our model\nto a next word prediction task on the Penn Treebank (PTB) dataset. We show that\na 'flattened' RSM network, when paired with a modern semantic word embedding\nand the addition of boosting, achieves 103.5 PPL (a 20-point improvement over\nthe best N-gram models), beating ordinary RNNs trained with BPTT and\napproaching the scores of early LSTM implementations. This work provides\nencouraging evidence that strong results on challenging tasks such as language\nmodelling may be possible using less memory intensive, biologically-plausible\ntraining regimes.", "published": "2019-12-02 23:00:13", "link": "http://arxiv.org/abs/1912.01116v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML", "68T30, 68T05", "I.2.7; I.2.4"], "primary_category": "cs.LG"}
{"title": "Speaker detection in the wild: Lessons learned from JSALT 2019", "abstract": "This paper presents the problems and solutions addressed at the JSALT\nworkshop when using a single microphone for speaker detection in adverse\nscenarios. The main focus was to tackle a wide range of conditions that go from\nmeetings to wild speech. We describe the research threads we explored and a set\nof modules that was successful for these scenarios. The ultimate goal was to\nexplore speaker detection; but our first finding was that an effective\ndiarization improves detection, and not having a diarization stage impoverishes\nthe performance. All the different configurations of our research agree on this\nfact and follow a main backbone that includes diarization as a previous stage.\nWith this backbone, we analyzed the following problems: voice activity\ndetection, how to deal with noisy signals, domain mismatch, how to improve the\nclustering; and the overall impact of previous stages in the final speaker\ndetection. In this paper, we show partial results for speaker diarizarion to\nhave a better understanding of the problem and we present the final results for\nspeaker detection.", "published": "2019-12-02 17:07:56", "link": "http://arxiv.org/abs/1912.00938v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Attention-Based Speaker Naming Method for Online Adaptation in\n  Non-Fixed Scenarios", "abstract": "A speaker naming task, which finds and identifies the active speaker in a\ncertain movie or drama scene, is crucial for dealing with high-level video\nanalysis applications such as automatic subtitle labeling and video\nsummarization. Modern approaches have usually exploited biometric features with\na gradient-based method instead of rule-based algorithms. In a certain\nsituation, however, a naive gradient-based method does not work efficiently.\nFor example, when new characters are added to the target identification list,\nthe neural network needs to be frequently retrained to identify new people and\nit causes delays in model preparation. In this paper, we present an\nattention-based method which reduces the model setup time by updating the newly\nadded data via online adaptation without a gradient update process. We\ncomparatively analyzed with three evaluation metrics(accuracy, memory usage,\nsetup time) of the attention-based method and existing gradient-based methods\nunder various controlled settings of speaker naming. Also, we applied existing\nspeaker naming models and the attention-based model to real video to prove that\nour approach shows comparable accuracy to the existing state-of-the-art models\nand even higher accuracy in some cases.", "published": "2019-12-02 09:30:27", "link": "http://arxiv.org/abs/1912.00649v1", "categories": ["cs.MM", "cs.CV", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Audiovisual Transformer Architectures for Large-Scale Classification and\n  Synchronization of Weakly Labeled Audio Events", "abstract": "We tackle the task of environmental event classification by drawing\ninspiration from the transformer neural network architecture used in machine\ntranslation. We modify this attention-based feedforward structure in such a way\nthat allows the resulting model to use audio as well as video to compute sound\nevent predictions. We perform extensive experiments with these adapted\ntransformers on an audiovisual data set, obtained by appending relevant visual\ninformation to an existing large-scale weakly labeled audio collection. The\nemployed multi-label data contains clip-level annotation indicating the\npresence or absence of 17 classes of environmental sounds, and does not include\ntemporal information. We show that the proposed modified transformers strongly\nimprove upon previously introduced models and in fact achieve state-of-the-art\nresults. We also make a compelling case for devoting more attention to research\nin multimodal audiovisual classification by proving the usefulness of visual\ninformation for the task at hand,namely audio event recognition. In addition,\nwe visualize internal attention patterns of the audiovisual transformers and in\ndoing so demonstrate their potential for performing multimodal synchronization.", "published": "2019-12-02 15:26:37", "link": "http://arxiv.org/abs/1912.02615v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Investigating U-Nets with various Intermediate Blocks for\n  Spectrogram-based Singing Voice Separation", "abstract": "Singing Voice Separation (SVS) tries to separate singing voice from a given\nmixed musical signal. Recently, many U-Net-based models have been proposed for\nthe SVS task, but there were no existing works that evaluate and compare\nvarious types of intermediate blocks that can be used in the U-Net\narchitecture. In this paper, we introduce a variety of intermediate spectrogram\ntransformation blocks. We implement U-nets based on these blocks and train them\non complex-valued spectrograms to consider both magnitude and phase. These\nnetworks are then compared on the SDR metric. When using a particular block\ncomposed of convolutional and fully-connected layers, it achieves\nstate-of-the-art SDR on the MUSDB singing voice separation task by a large\nmargin of 0.9 dB. Our code and models are available online.", "published": "2019-12-02 07:46:19", "link": "http://arxiv.org/abs/1912.02591v3", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
