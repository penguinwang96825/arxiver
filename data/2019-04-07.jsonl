{"title": "SEQ^3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for\n  Unsupervised Abstractive Sentence Compression", "abstract": "Neural sequence-to-sequence models are currently the dominant approach in\nseveral natural language processing tasks, but require large parallel corpora.\nWe present a sequence-to-sequence-to-sequence autoencoder (SEQ^3), consisting\nof two chained encoder-decoder pairs, with words used as a sequence of discrete\nlatent variables. We apply the proposed model to unsupervised abstractive\nsentence compression, where the first and last sequences are the input and\nreconstructed sentences, respectively, while the middle sequence is the\ncompressed sentence. Constraining the length of the latent word sequences\nforces the model to distill important information from the input. A pretrained\nlanguage model, acting as a prior over the latent sequences, encourages the\ncompressed sentences to be human-readable. Continuous relaxations enable us to\nsample from categorical distributions, allowing gradient-based optimization,\nunlike alternatives that rely on reinforcement learning. The proposed model\ndoes not require parallel text-summary pairs, achieving promising results in\nunsupervised sentence compression on benchmark datasets.", "published": "2019-04-07 13:47:28", "link": "http://arxiv.org/abs/1904.03651v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI Meets Austen: Towards Human-Robot Discussions of Literary Metaphor", "abstract": "Artificial intelligence is revolutionizing formal education, fueled by\ninnovations in learning assessment, content generation, and instructional\ndelivery. Informal, lifelong learning settings have been the subject of less\nattention. We provide a proof-of-concept for an embodied book discussion\ncompanion, designed to stimulate conversations with readers about particularly\ncreative metaphors in fiction literature. We collect ratings from 26\nparticipants, each of whom discuss Jane Austen's \"Pride and Prejudice\" with the\nrobot across one or more sessions, and find that participants rate their\ninteractions highly. This suggests that companion robots could be an\ninteresting entryway for the promotion of lifelong learning and cognitive\nexercise in future applications.", "published": "2019-04-07 19:01:32", "link": "http://arxiv.org/abs/1904.03713v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Unsupervised Dialog Structure Learning", "abstract": "Learning a shared dialog structure from a set of task-oriented dialogs is an\nimportant challenge in computational linguistics. The learned dialog structure\ncan shed light on how to analyze human dialogs, and more importantly contribute\nto the design and evaluation of dialog systems. We propose to extract dialog\nstructures using a modified VRNN model with discrete latent vectors. Different\nfrom existing HMM-based models, our model is based on variational-autoencoder\n(VAE). Such model is able to capture more dynamics in dialogs beyond the\nsurface forms of the language. We find that qualitatively, our method extracts\nmeaningful dialog structure, and quantitatively, outperforms previous models on\nthe ability to predict unseen data. We further evaluate the model's\neffectiveness in a downstream task, the dialog system building task.\nExperiments show that, by integrating the learned dialog structure into the\nreward function design, the model converges faster and to a better outcome in a\nreinforcement learning setting.", "published": "2019-04-07 20:28:47", "link": "http://arxiv.org/abs/1904.03736v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Recurrent Neural Network Grammars", "abstract": "Recurrent neural network grammars (RNNG) are generative models of language\nwhich jointly model syntax and surface structure by incrementally generating a\nsyntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs\nachieve strong language modeling and parsing performance, but require an\nannotated corpus of parse trees. In this work, we experiment with unsupervised\nlearning of RNNGs. Since directly marginalizing over the space of latent trees\nis intractable, we instead apply amortized variational inference. To maximize\nthe evidence lower bound, we develop an inference network parameterized as a\nneural CRF constituency parser. On language modeling, unsupervised RNNGs\nperform as well their supervised counterparts on benchmarks in English and\nChinese. On constituency grammar induction, they are competitive with recent\nneural language models that induce tree structures from words through attention\nmechanisms.", "published": "2019-04-07 21:14:43", "link": "http://arxiv.org/abs/1904.03746v6", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Spoken Language Intent Detection using Confusion2Vec", "abstract": "Decoding speaker's intent is a crucial part of spoken language understanding\n(SLU). The presence of noise or errors in the text transcriptions, in real life\nscenarios make the task more challenging. In this paper, we address the spoken\nlanguage intent detection under noisy conditions imposed by automatic speech\nrecognition (ASR) systems. We propose to employ confusion2vec word feature\nrepresentation to compensate for the errors made by ASR and to increase the\nrobustness of the SLU system. The confusion2vec, motivated from human speech\nproduction and perception, models acoustic relationships between words in\naddition to the semantic and syntactic relations of words in human language. We\nhypothesize that ASR often makes errors relating to acoustically similar words,\nand the confusion2vec with inherent model of acoustic relationships between\nwords is able to compensate for the errors. We demonstrate through experiments\non the ATIS benchmark dataset, the robustness of the proposed model to achieve\nstate-of-the-art results under noisy ASR conditions. Our system reduces\nclassification error rate (CER) by 20.84% and improves robustness by 37.48%\n(lower CER degradation) relative to the previous state-of-the-art going from\nclean to noisy transcripts. Improvements are also demonstrated when training\nthe intent detection models on noisy transcripts.", "published": "2019-04-07 03:18:44", "link": "http://arxiv.org/abs/1904.03576v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Pre-Trained and Random Units for Domain Adaptation in\n  Part-of-Speech Tagging", "abstract": "Fine-tuning neural networks is widely used to transfer valuable knowledge\nfrom high-resource to low-resource domains. In a standard fine-tuning scheme,\nsource and target problems are trained using the same architecture. Although\ncapable of adapting to new domains, pre-trained units struggle with learning\nuncommon target-specific patterns. In this paper, we propose to augment the\ntarget-network with normalised, weighted and randomly initialised units that\nbeget a better adaptation while maintaining the valuable source knowledge. Our\nexperiments on POS tagging of social media texts (Tweets domain) demonstrate\nthat our method achieves state-of-the-art performances on 3 commonly used\ndatasets.", "published": "2019-04-07 06:46:36", "link": "http://arxiv.org/abs/1904.03595v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Speech Model Pre-training for End-to-End Spoken Language Understanding", "abstract": "Whereas conventional spoken language understanding (SLU) systems map speech\nto text, and then text to intent, end-to-end SLU systems map speech directly to\nintent through a single trainable model. Achieving high accuracy with these\nend-to-end models without a large amount of training data is difficult. We\npropose a method to reduce the data requirements of end-to-end SLU in which the\nmodel is first pre-trained to predict words and phonemes, thus learning good\nfeatures for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and\nshow that our method improves performance both when the full dataset is used\nfor training and when only a small subset is used. We also describe preliminary\nexperiments to gauge the model's ability to generalize to new phrases not heard\nduring training.", "published": "2019-04-07 15:24:32", "link": "http://arxiv.org/abs/1904.03670v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoiceID Loss: Speech Enhancement for Speaker Verification", "abstract": "In this paper, we propose VoiceID loss, a novel loss function for training a\nspeech enhancement model to improve the robustness of speaker verification. In\ncontrast to the commonly used loss functions for speech enhancement such as the\nL2 loss, the VoiceID loss is based on the feedback from a speaker verification\nmodel to generate a ratio mask. The generated ratio mask is multiplied\npointwise with the original spectrogram to filter out unnecessary components\nfor speaker verification. In the experiments, we observed that the enhancement\nnetwork, after training with the VoiceID loss, is able to ignore a substantial\namount of time-frequency bins, such as those dominated by noise, for\nverification. The resulting model consistently improves the speaker\nverification system on both clean and noisy conditions.", "published": "2019-04-07 08:07:20", "link": "http://arxiv.org/abs/1904.03601v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Time Domain Audio Visual Speech Separation", "abstract": "Audio-visual multi-modal modeling has been demonstrated to be effective in\nmany speech related tasks, such as speech recognition and speech enhancement.\nThis paper introduces a new time-domain audio-visual architecture for target\nspeaker extraction from monaural mixtures. The architecture generalizes the\nprevious TasNet (time-domain speech separation network) to enable multi-modal\nlearning and at meanwhile it extends the classical audio-visual speech\nseparation from frequency-domain to time-domain. The main components of\nproposed architecture include an audio encoder, a video encoder that extracts\nlip embedding from video streams, a multi-modal separation network and an audio\ndecoder. Experiments on simulated mixtures based on recently released LRS2\ndataset show that our method can bring 3dB+ and 4dB+ Si-SNR improvements on\ntwo- and three-speaker cases respectively, compared to audio-only TasNet and\nfrequency-domain audio-visual networks", "published": "2019-04-07 22:30:23", "link": "http://arxiv.org/abs/1904.03760v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MCE 2018: The 1st Multi-target Speaker Detection and Identification\n  Challenge Evaluation", "abstract": "The Multi-target Challenge aims to assess how well current speech technology\nis able to determine whether or not a recorded utterance was spoken by one of a\nlarge number of blacklisted speakers. It is a form of multi-target speaker\ndetection based on real-world telephone conversations. Data recordings are\ngenerated from call center customer-agent conversations. The task is to measure\nhow accurately one can detect 1) whether a test recording is spoken by a\nblacklisted speaker, and 2) which specific blacklisted speaker was talking.\nThis paper outlines the challenge and provides its baselines, results, and\ndiscussions.", "published": "2019-04-07 08:09:25", "link": "http://arxiv.org/abs/1904.04240v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VAE-based regularization for deep speaker embedding", "abstract": "Deep speaker embedding has achieved state-of-the-art performance in speaker\nrecognition. A potential problem of these embedded vectors (called `x-vectors')\nare not Gaussian, causing performance degradation with the famous PLDA back-end\nscoring. In this paper, we propose a regularization approach based on\nVariational Auto-Encoder (VAE). This model transforms x-vectors to a latent\nspace where mapped latent codes are more Gaussian, hence more suitable for PLDA\nscoring.", "published": "2019-04-07 10:11:58", "link": "http://arxiv.org/abs/1904.03617v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
