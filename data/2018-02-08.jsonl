{"title": "Biomedical term normalization of EHRs with UMLS", "abstract": "This paper presents a novel prototype for biomedical term normalization of\nelectronic health record excerpts with the Unified Medical Language System\n(UMLS) Metathesaurus. Despite being multilingual and cross-lingual by design,\nwe first focus on processing clinical text in Spanish because there is no\nexisting tool for this language and for this specific purpose. The tool is\nbased on Apache Lucene to index the Metathesaurus and generate mapping\ncandidates from input text. It uses the IXA pipeline for basic language\nprocessing and resolves ambiguities with the UKB toolkit. It has been evaluated\nby measuring its agreement with MetaMap in two English-Spanish parallel\ncorpora. In addition, we present a web-based interface for the tool.", "published": "2018-02-08 14:16:18", "link": "http://arxiv.org/abs/1802.02870v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DisMo: A Morphosyntactic, Disfluency and Multi-Word Unit Annotator. An\n  Evaluation on a Corpus of French Spontaneous and Read Speech", "abstract": "We present DisMo, a multi-level annotator for spoken language corpora that\nintegrates part-of-speech tagging with basic disfluency detection and\nannotation, and multi-word unit recognition. DisMo is a hybrid system that uses\na combination of lexical resources, rules, and statistical models based on\nConditional Random Fields (CRF). In this paper, we present the first public\nversion of DisMo for French. The system is trained and its performance\nevaluated on a 57k-token corpus, including different varieties of French spoken\nin three countries (Belgium, France and Switzerland). DisMo supports a\nmulti-level annotation scheme, in which the tokenisation to minimal word units\nis complemented with multi-word unit groupings (each having associated POS\ntags), as well as separate levels for annotating disfluencies and discourse\nphenomena. We present the system's architecture, linguistic resources and its\nhierarchical tag-set. Results show that DisMo achieves a precision of 95%\n(finest tag-set) to 96.8% (coarse tag-set) in POS-tagging non-punctuated,\nsound-aligned transcriptions of spoken French, while also offering substantial\npossibilities for automated multi-level annotation.", "published": "2018-02-08 15:38:54", "link": "http://arxiv.org/abs/1802.02926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Praaline: Integrating Tools for Speech Corpus Research", "abstract": "This paper presents Praaline, an open-source software system for managing,\nannotating, analysing and visualising speech corpora. Researchers working with\nspeech corpora are often faced with multiple tools and formats, and they need\nto work with ever-increasing amounts of data in a collaborative way. Praaline\nintegrates and extends existing time-proven tools for spoken corpora analysis\n(Praat, Sonic Visualiser and a bridge to the R statistical package) in a\nmodular system, facilitating automation and reuse. Users are exposed to an\nintegrated, user-friendly interface from which to access multiple tools. Corpus\nmetadata and annotations may be stored in a database, locally or remotely, and\nusers can define the metadata and annotation structure. Users may run a\ncustomisable cascade of analysis steps, based on plug-ins and scripts, and\nupdate the database with the results. The corpus database may be queried, to\nproduce aggregated data-sets. Praaline is extensible using Python or C++\nplug-ins, while Praat and R scripts may be executed against the corpus data. A\nseries of visualisations, editors and plug-ins are provided. Praaline is free\nsoftware, released under the GPL license.", "published": "2018-02-08 15:15:51", "link": "http://arxiv.org/abs/1802.02914v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Learning Inductive Biases with Simple Neural Networks", "abstract": "People use rich prior knowledge about the world in order to efficiently learn\nnew concepts. These priors - also known as \"inductive biases\" - pertain to the\nspace of internal models considered by a learner, and they help the learner\nmake inferences that go beyond the observed data. A recent study found that\ndeep neural networks optimized for object recognition develop the shape bias\n(Ritter et al., 2017), an inductive bias possessed by children that plays an\nimportant role in early word learning. However, these networks use\nunrealistically large quantities of training data, and the conditions required\nfor these biases to develop are not well understood. Moreover, it is unclear\nhow the learning dynamics of these networks relate to developmental processes\nin childhood. We investigate the development and influence of the shape bias in\nneural networks using controlled datasets of abstract patterns and synthetic\nimages, allowing us to systematically vary the quantity and form of the\nexperience provided to the learning algorithms. We find that simple neural\nnetworks develop a shape bias after seeing as few as 3 examples of 4 object\ncategories. The development of these biases predicts the onset of vocabulary\nacceleration in our networks, consistent with the developmental process in\nchildren.", "published": "2018-02-08 08:25:51", "link": "http://arxiv.org/abs/1802.02745v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WorldTree: A Corpus of Explanation Graphs for Elementary Science\n  Questions supporting Multi-Hop Inference", "abstract": "Developing methods of automated inference that are able to provide users with\ncompelling human-readable justifications for why the answer to a question is\ncorrect is critical for domains such as science and medicine, where user trust\nand detecting costly errors are limiting factors to adoption. One of the\ncentral barriers to training question answering models on explainable inference\ntasks is the lack of gold explanations to serve as training data. In this paper\nwe present a corpus of explanations for standardized science exams, a recent\nchallenge task for question answering. We manually construct a corpus of\ndetailed explanations for nearly all publicly available standardized elementary\nscience question (approximately 1,680 3rd through 5th grade questions) and\nrepresent these as \"explanation graphs\" -- sets of lexically overlapping\nsentences that describe how to arrive at the correct answer to a question\nthrough a combination of domain and world knowledge. We also provide an\nexplanation-centered tablestore, a collection of semi-structured tables that\ncontain the knowledge to construct these elementary science explanations.\nTogether, these two knowledge resources map out a substantial portion of the\nknowledge required for answering and explaining elementary science exams, and\nprovide both structured and free-text training data for the explainable\ninference task.", "published": "2018-02-08 21:26:03", "link": "http://arxiv.org/abs/1802.03052v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
