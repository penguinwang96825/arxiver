{"title": "KINLP at SemEval-2023 Task 12: Kinyarwanda Tweet Sentiment Analysis", "abstract": "This paper describes the system entered by the author to the SemEval-2023\nTask 12: Sentiment analysis for African languages. The system focuses on the\nKinyarwanda language and uses a language-specific model. Kinyarwanda morphology\nis modeled in a two tier transformer architecture and the transformer model is\npre-trained on a large text corpus using multi-task masked morphology\nprediction. The model is deployed on an experimental platform that allows users\nto experiment with the pre-trained language model fine-tuning without the need\nto write machine learning code. Our final submission to the shared task\nachieves second ranking out of 34 teams in the competition, achieving 72.50%\nweighted F1 score. Our analysis of the evaluation results highlights challenges\nin achieving high accuracy on the task and identifies areas for improvement.", "published": "2023-04-25 04:30:03", "link": "http://arxiv.org/abs/2304.12569v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "What does BERT learn about prosody?", "abstract": "Language models have become nearly ubiquitous in natural language processing\napplications achieving state-of-the-art results in many tasks including\nprosody. As the model design does not define predetermined linguistic targets\nduring training but rather aims at learning generalized representations of the\nlanguage, analyzing and interpreting the representations that models implicitly\ncapture is important in bridging the gap between interpretability and model\nperformance. Several studies have explored the linguistic information that\nmodels capture providing some insights on their representational capacity.\nHowever, the current studies have not explored whether prosody is part of the\nstructural information of the language that models learn. In this work, we\nperform a series of experiments on BERT probing the representations captured at\ndifferent layers. Our results show that information about prosodic prominence\nspans across many layers but is mostly focused in middle layers suggesting that\nBERT relies mostly on syntactic and semantic information.", "published": "2023-04-25 10:34:56", "link": "http://arxiv.org/abs/2304.12706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CitePrompt: Using Prompts to Identify Citation Intent in Scientific\n  Papers", "abstract": "Citations in scientific papers not only help us trace the intellectual\nlineage but also are a useful indicator of the scientific significance of the\nwork. Citation intents prove beneficial as they specify the role of the\ncitation in a given context. In this paper, we present CitePrompt, a framework\nwhich uses the hitherto unexplored approach of prompt-based learning for\ncitation intent classification. We argue that with the proper choice of the\npretrained language model, the prompt template, and the prompt verbalizer, we\ncan not only get results that are better than or comparable to those obtained\nwith the state-of-the-art methods but also do it with much less exterior\ninformation about the scientific document. We report state-of-the-art results\non the ACL-ARC dataset, and also show significant improvement on the SciCite\ndataset over all baseline models except one. As suitably large labelled\ndatasets for citation intent classification can be quite hard to find, in a\nfirst, we propose the conversion of this task to the few-shot and zero-shot\nsettings. For the ACL-ARC dataset, we report a 53.86% F1 score for the\nzero-shot setting, which improves to 63.61% and 66.99% for the 5-shot and\n10-shot settings, respectively.", "published": "2023-04-25 11:19:52", "link": "http://arxiv.org/abs/2304.12730v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Test-Time Adaptation with Perturbation Consistency Learning", "abstract": "Currently, pre-trained language models (PLMs) do not cope well with the\ndistribution shift problem, resulting in models trained on the training set\nfailing in real test scenarios. To address this problem, the test-time\nadaptation (TTA) shows great potential, which updates model parameters to suit\nthe test data at the testing time. Existing TTA methods rely on well-designed\nauxiliary tasks or self-training strategies based on pseudo-label. However,\nthese methods do not achieve good trade-offs regarding performance gains and\ncomputational costs. To obtain some insights into such a dilemma, we take two\nrepresentative TTA methods, i.e., Tent and OIL, for exploration and find that\nstable prediction is the key to achieving a good balance. Accordingly, in this\npaper, we propose perturbation consistency learning (PCL), a simple test-time\nadaptation method to promote the model to make stable predictions for samples\nwith distribution shifts. Extensive experiments on adversarial robustness and\ncross-lingual transferring demonstrate that our method can achieve higher or\ncomparable performance with less inference time over strong PLM backbones and\nprevious state-of-the-art TTA methods.", "published": "2023-04-25 12:29:22", "link": "http://arxiv.org/abs/2304.12764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lessons Learned from a Citizen Science Project for Natural Language\n  Processing", "abstract": "Many Natural Language Processing (NLP) systems use annotated corpora for\ntraining and evaluation. However, labeled data is often costly to obtain and\nscaling annotation projects is difficult, which is why annotation tasks are\noften outsourced to paid crowdworkers. Citizen Science is an alternative to\ncrowdsourcing that is relatively unexplored in the context of NLP. To\ninvestigate whether and how well Citizen Science can be applied in this\nsetting, we conduct an exploratory study into engaging different groups of\nvolunteers in Citizen Science for NLP by re-annotating parts of a pre-existing\ncrowdsourced dataset. Our results show that this can yield high-quality\nannotations and attract motivated volunteers, but also requires considering\nfactors such as scalability, participation over time, and legal and ethical\nissues. We summarize lessons learned in the form of guidelines and provide our\ncode and data to aid future work on Citizen Science.", "published": "2023-04-25 14:08:53", "link": "http://arxiv.org/abs/2304.12836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP-LTU at SemEval-2023 Task 10: The Impact of Data Augmentation and\n  Semi-Supervised Learning Techniques on Text Classification Performance on an\n  Imbalanced Dataset", "abstract": "In this paper, we propose a methodology for task 10 of SemEval23, focusing on\ndetecting and classifying online sexism in social media posts. The task is\ntackling a serious issue, as detecting harmful content on social media\nplatforms is crucial for mitigating the harm of these posts on users. Our\nsolution for this task is based on an ensemble of fine-tuned transformer-based\nmodels (BERTweet, RoBERTa, and DeBERTa). To alleviate problems related to class\nimbalance, and to improve the generalization capability of our model, we also\nexperiment with data augmentation and semi-supervised learning. In particular,\nfor data augmentation, we use back-translation, either on all classes, or on\nthe underrepresented classes only. We analyze the impact of these strategies on\nthe overall performance of the pipeline through extensive experiments. while\nfor semi-supervised learning, we found that with a substantial amount of\nunlabelled, in-domain data available, semi-supervised learning can enhance the\nperformance of certain models. Our proposed method (for which the source code\nis available on Github attains an F1-score of 0.8613 for sub-taskA, which\nranked us 10th in the competition", "published": "2023-04-25 14:19:46", "link": "http://arxiv.org/abs/2304.12847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nondeterministic Stacks in Neural Networks", "abstract": "Human language is full of compositional syntactic structures, and although\nneural networks have contributed to groundbreaking improvements in computer\nsystems that process language, widely-used neural network architectures still\nexhibit limitations in their ability to process syntax. To address this issue,\nprior work has proposed adding stack data structures to neural networks,\ndrawing inspiration from theoretical connections between syntax and stacks.\nHowever, these methods employ deterministic stacks that are designed to track\none parse at a time, whereas syntactic ambiguity, which requires a\nnondeterministic stack to parse, is extremely common in language. In this\ndissertation, we remedy this discrepancy by proposing a method of incorporating\nnondeterministic stacks into neural networks. We develop a differentiable data\nstructure that efficiently simulates a nondeterministic pushdown automaton,\nrepresenting an exponential number of computations with a dynamic programming\nalgorithm. We incorporate this module into two predominant architectures:\nrecurrent neural networks (RNNs) and transformers. We show that this raises\ntheir formal recognition power to arbitrary context-free languages, and also\naids training, even on deterministic context-free languages. Empirically,\nneural networks with nondeterministic stacks learn context-free languages much\nmore effectively than prior stack-augmented models, including a language with\ntheoretically maximal parsing difficulty. We also show that an RNN augmented\nwith a nondeterministic stack is capable of surprisingly powerful behavior,\nsuch as learning cross-serial dependencies, a well-known non-context-free\npattern. We demonstrate improvements on natural language modeling and provide\nanalysis on a syntactic generalization benchmark. This work represents an\nimportant step toward building systems that learn to use syntax in more\nhuman-like fashion.", "published": "2023-04-25 16:00:40", "link": "http://arxiv.org/abs/2304.12955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Escaping the sentence-level paradigm in machine translation", "abstract": "It is well-known that document context is vital for resolving a range of\ntranslation ambiguities, and in fact the document setting is the most natural\nsetting for nearly all translation. It is therefore unfortunate that machine\ntranslation -- both research and production -- largely remains stuck in a\ndecades-old sentence-level translation paradigm. It is also an increasingly\nglaring problem in light of competitive pressure from large language models,\nwhich are natively document-based. Much work in document-context machine\ntranslation exists, but for various reasons has been unable to catch hold. This\npaper suggests a path out of this rut by addressing three impediments at once:\nwhat architectures should we use? where do we get document-level information\nfor training them? and how do we know whether they are any good? In contrast to\nwork on specialized architectures, we show that the standard Transformer\narchitecture is sufficient, provided it has enough capacity. Next, we address\nthe training data issue by taking document samples from back-translated data\nonly, where the data is not only more readily available, but is also of higher\nquality compared to parallel document data, which may contain machine\ntranslation output. Finally, we propose generative variants of existing\ncontrastive metrics that are better able to discriminate among document\nsystems. Results in four large-data language pairs (DE$\\rightarrow$EN,\nEN$\\rightarrow$DE, EN$\\rightarrow$FR, and EN$\\rightarrow$RU) establish the\nsuccess of these three pieces together in improving document-level performance.", "published": "2023-04-25 16:09:02", "link": "http://arxiv.org/abs/2304.12959v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intent Induction from Conversations for Task-Oriented Dialogue Track at\n  DSTC 11", "abstract": "With increasing demand for and adoption of virtual assistants, recent work\nhas investigated ways to accelerate bot schema design through the automatic\ninduction of intents or the induction of slots and dialogue states. However, a\nlack of dedicated benchmarks and standardized evaluation has made progress\ndifficult to track and comparisons between systems difficult to make. This\nchallenge track, held as part of the Eleventh Dialog Systems Technology\nChallenge, introduces a benchmark that aims to evaluate methods for the\nautomatic induction of customer intents in a realistic setting of customer\nservice interactions between human agents and customers. We propose two\nsubtasks for progressively tackling the automatic induction of intents and\ncorresponding evaluation methodologies. We then present three datasets suitable\nfor evaluating the tasks and propose simple baselines. Finally, we summarize\nthe submissions and results of the challenge track, for which we received\nsubmissions from 34 teams.", "published": "2023-04-25 16:45:50", "link": "http://arxiv.org/abs/2304.12982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Inter-Bilingual Semantic Parsing for Indian Languages", "abstract": "Despite significant progress in Natural Language Generation for Indian\nlanguages (IndicNLP), there is a lack of datasets around complex structured\ntasks such as semantic parsing. One reason for this imminent gap is the\ncomplexity of the logical form, which makes English to multilingual translation\ndifficult. The process involves alignment of logical forms, intents and slots\nwith translated unstructured utterance. To address this, we propose an\nInter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct\nIndian languages. We highlight the proposed task's practicality, and evaluate\nexisting multilingual seq2seq models across several train-test strategies. Our\nexperiment reveals a high correlation across performance of original\nmultilingual semantic parsing datasets (such as mTOP, multilingual TOP and\nmultiATIS++) and our proposed IE-SEMPARSE suite.", "published": "2023-04-25 17:24:32", "link": "http://arxiv.org/abs/2304.13005v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Injecting structural hints: Using language models to study inductive\n  biases in language learning", "abstract": "Both humans and large language models are able to learn language without\nexplicit structural supervision. What inductive biases make this learning\npossible? We address this fundamental cognitive question by leveraging\ntransformer language models: we inject inductive bias into language models by\npretraining on formally-structured data, and then evaluate the biased learners'\nability to learn typologically-diverse natural languages. Our experimental\nsetup creates a testbed for hypotheses about inductive bias in human language\nlearning. We investigate the effect of injecting models with three types of\ninductive bias: 1) recursive, hierarchical processing, 2) crossing token-token\nrelationships that can't be modeled by context-free grammars, and 3) a Zipfian\npower-law vocabulary distribution. We show that non-context-free relationships\nform the best inductive biases. Our study leverages the capabilities of\ntransformer models to run controlled language learning experiments that are not\npossible to run on humans, and surfaces hypotheses about the structures that\nfacilitate language learning in both humans and machines.", "published": "2023-04-25 18:00:08", "link": "http://arxiv.org/abs/2304.13060v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAST: Scalable Lattice-Based Speech Modelling in JAX", "abstract": "We introduce LAST, a LAttice-based Speech Transducer library in JAX. With an\nemphasis on flexibility, ease-of-use, and scalability, LAST implements\ndifferentiable weighted finite state automaton (WFSA) algorithms needed for\ntraining \\& inference that scale to a large WFSA such as a recognition lattice\nover the entire utterance. Despite these WFSA algorithms being well-known in\nthe literature, new challenges arise from performance characteristics of modern\narchitectures, and from nuances in automatic differentiation. We describe a\nsuite of generally applicable techniques employed in LAST to address these\nchallenges, and demonstrate their effectiveness with benchmarks on TPUv3 and\nV100 GPU.", "published": "2023-04-25 20:25:37", "link": "http://arxiv.org/abs/2304.13134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sebis at SemEval-2023 Task 7: A Joint System for Natural Language\n  Inference and Evidence Retrieval from Clinical Trial Reports", "abstract": "With the increasing number of clinical trial reports generated every day, it\nis becoming hard to keep up with novel discoveries that inform evidence-based\nhealthcare recommendations. To help automate this process and assist medical\nexperts, NLP solutions are being developed. This motivated the SemEval-2023\nTask 7, where the goal was to develop an NLP system for two tasks: evidence\nretrieval and natural language inference from clinical trial data. In this\npaper, we describe our two developed systems. The first one is a pipeline\nsystem that models the two tasks separately, while the second one is a joint\nsystem that learns the two tasks simultaneously with a shared representation\nand a multi-task learning approach. The final system combines their outputs in\nan ensemble system. We formalize the models, present their characteristics and\nchallenges, and provide an analysis of achieved results. Our system ranked 3rd\nout of 40 participants with a final submission.", "published": "2023-04-25 22:22:42", "link": "http://arxiv.org/abs/2304.13180v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explain like I am BM25: Interpreting a Dense Model's Ranked-List with a\n  Sparse Approximation", "abstract": "Neural retrieval models (NRMs) have been shown to outperform their\nstatistical counterparts owing to their ability to capture semantic meaning via\ndense document representations. These models, however, suffer from poor\ninterpretability as they do not rely on explicit term matching. As a form of\nlocal per-query explanations, we introduce the notion of equivalent queries\nthat are generated by maximizing the similarity between the NRM's results and\nthe result set of a sparse retrieval system with the equivalent query. We then\ncompare this approach with existing methods such as RM3-based query expansion\nand contrast differences in retrieval effectiveness and in the terms generated\nby each approach.", "published": "2023-04-25 07:58:38", "link": "http://arxiv.org/abs/2304.12631v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PUNR: Pre-training with User Behavior Modeling for News Recommendation", "abstract": "News recommendation aims to predict click behaviors based on user behaviors.\nHow to effectively model the user representations is the key to recommending\npreferred news. Existing works are mostly focused on improvements in the\nsupervised fine-tuning stage. However, there is still a lack of PLM-based\nunsupervised pre-training methods optimized for user representations. In this\nwork, we propose an unsupervised pre-training paradigm with two tasks, i.e.\nuser behavior masking and user behavior generation, both towards effective user\nbehavior modeling. Firstly, we introduce the user behavior masking pre-training\ntask to recover the masked user behaviors based on their contextual behaviors.\nIn this way, the model could capture a much stronger and more comprehensive\nuser news reading pattern. Besides, we incorporate a novel auxiliary user\nbehavior generation pre-training task to enhance the user representation vector\nderived from the user encoder. We use the above pre-trained user modeling\nencoder to obtain news and user representations in downstream fine-tuning.\nEvaluations on the real-world news benchmark show significant performance\nimprovements over existing baselines.", "published": "2023-04-25 08:03:52", "link": "http://arxiv.org/abs/2304.12633v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Compressing Sentence Representation with maximum Coding Rate Reduction", "abstract": "In most natural language inference problems, sentence representation is\nneeded for semantic retrieval tasks. In recent years, pre-trained large\nlanguage models have been quite effective for computing such representations.\nThese models produce high-dimensional sentence embeddings. An evident\nperformance gap between large and small models exists in practice. Hence, due\nto space and time hardware limitations, there is a need to attain comparable\nresults when using the smaller model, which is usually a distilled version of\nthe large language model. In this paper, we assess the model distillation of\nthe sentence representation model Sentence-BERT by augmenting the pre-trained\ndistilled model with a projection layer additionally learned on the Maximum\nCoding Rate Reduction (MCR2)objective, a novel approach developed for\ngeneral-purpose manifold clustering. We demonstrate that the new language model\nwith reduced complexity and sentence embedding size can achieve comparable\nresults on semantic retrieval benchmarks.", "published": "2023-04-25 09:23:43", "link": "http://arxiv.org/abs/2304.12674v1", "categories": ["cs.CL", "cs.LG", "68T50, 62M45", "I.2.7; I.2.0; I.5.3"], "primary_category": "cs.CL"}
{"title": "State Spaces Aren't Enough: Machine Translation Needs Attention", "abstract": "Structured State Spaces for Sequences (S4) is a recently proposed sequence\nmodel with successful applications in various tasks, e.g. vision, language\nmodeling, and audio. Thanks to its mathematical formulation, it compresses its\ninput to a single hidden state, and is able to capture long range dependencies\nwhile avoiding the need for an attention mechanism. In this work, we apply S4\nto Machine Translation (MT), and evaluate several encoder-decoder variants on\nWMT'14 and WMT'16. In contrast with the success in language modeling, we find\nthat S4 lags behind the Transformer by approximately 4 BLEU points, and that it\ncounter-intuitively struggles with long sentences. Finally, we show that this\ngap is caused by S4's inability to summarize the full source sentence in a\nsingle hidden state, and show that we can close the gap by introducing an\nattention mechanism.", "published": "2023-04-25 12:54:20", "link": "http://arxiv.org/abs/2304.12776v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Out-of-distribution Evidence-aware Fake News Detection via Dual\n  Adversarial Debiasing", "abstract": "Evidence-aware fake news detection aims to conduct reasoning between news and\nevidence, which is retrieved based on news content, to find uniformity or\ninconsistency. However, we find evidence-aware detection models suffer from\nbiases, i.e., spurious correlations between news/evidence contents and\ntrue/fake news labels, and are hard to be generalized to Out-Of-Distribution\n(OOD) situations. To deal with this, we propose a novel Dual Adversarial\nLearning (DAL) approach. We incorporate news-aspect and evidence-aspect\ndebiasing discriminators, whose targets are both true/fake news labels, in DAL.\nThen, DAL reversely optimizes news-aspect and evidence-aspect debiasing\ndiscriminators to mitigate the impact of news and evidence content biases. At\nthe same time, DAL also optimizes the main fake news predictor, so that the\nnews-evidence interaction module can be learned. This process allows us to\nteach evidence-aware fake news detection models to better conduct news-evidence\nreasoning, and minimize the impact of content biases. To be noted, our proposed\nDAL approach is a plug-and-play module that works well with existing backbones.\nWe conduct comprehensive experiments under two OOD settings, and plug DAL in\nfour evidence-aware fake news detection backbones. Results demonstrate that,\nDAL significantly and stably outperforms the original backbones and some\ncompetitive debiasing methods.", "published": "2023-04-25 14:59:25", "link": "http://arxiv.org/abs/2304.12888v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based\n  Adapters", "abstract": "This report describes GMU's sentiment analysis system for the SemEval-2023\nshared task AfriSenti-SemEval. We participated in all three sub-tasks:\nMonolingual, Multilingual, and Zero-Shot. Our approach uses models initialized\nwith AfroXLMR-large, a pre-trained multilingual language model trained on\nAfrican languages and fine-tuned correspondingly. We also introduce augmented\ntraining data along with original training data. Alongside finetuning, we\nperform phylogeny-based adapter tuning to create several models and ensemble\nthe best models for the final submission. Our system achieves the best F1-score\non track 5: Amharic, with 6.2 points higher F1-score than the second-best\nperforming system on this track. Overall, our system ranks 5th among the 10\nsystems participating in all 15 tracks.", "published": "2023-04-25 16:39:51", "link": "http://arxiv.org/abs/2304.12979v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Massive Multitask Chinese Understanding", "abstract": "The development of large-scale Chinese language models is flourishing, yet\nthere is a lack of corresponding capability assessments. Therefore, we propose\na test to measure the multitask accuracy of large Chinese language models. This\ntest encompasses four major domains, including medicine, law, psychology, and\neducation, with 15 subtasks in medicine and 8 subtasks in education. We found\nthat the best-performing models in the zero-shot setting outperformed the\nworst-performing models by nearly 18.6 percentage points on average. Across the\nfour major domains, the highest average zero-shot accuracy of all models is\n0.512. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot\naccuracy of 0.693 in clinical medicine, which was the highest accuracy among\nall models across all subtasks. All models performed poorly in the legal\ndomain, with the highest zero-shot accuracy reaching only 0.239. By\ncomprehensively evaluating the breadth and depth of knowledge across multiple\ndisciplines, this test can more accurately identify the shortcomings of the\nmodels.", "published": "2023-04-25 16:51:53", "link": "http://arxiv.org/abs/2304.12986v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought", "abstract": "Modern systems for multi-hop question answering (QA) typically break\nquestions into a sequence of reasoning steps, termed chain-of-thought (CoT),\nbefore arriving at a final answer. Often, multiple chains are sampled and\naggregated through a voting mechanism over the final answers, but the\nintermediate steps themselves are discarded. While such approaches improve\nperformance, they do not consider the relations between intermediate steps\nacross chains and do not provide a unified explanation for the predicted\nanswer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts\nlarge language models to meta-reason over multiple chains of thought, rather\nthan aggregating their answers. MCR examines different reasoning chains, mixes\ninformation between them and selects the most relevant facts in generating an\nexplanation and predicting the answer. MCR outperforms strong baselines on 7\nmulti-hop QA datasets. Moreover, our analysis reveals that MCR explanations\nexhibit high quality, enabling humans to verify its answers.", "published": "2023-04-25 17:27:37", "link": "http://arxiv.org/abs/2304.13007v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unstructured and structured data: Can we have the best of both worlds\n  with large language models?", "abstract": "This paper presents an opinion on the potential of using large language\nmodels to query on both unstructured and structured data. It also outlines some\nresearch challenges related to the topic of building question-answering systems\nfor both types of data.", "published": "2023-04-25 17:30:05", "link": "http://arxiv.org/abs/2304.13010v2", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Modeling Spoken Information Queries for Virtual Assistants: Open\n  Problems, Challenges and Opportunities", "abstract": "Virtual assistants are becoming increasingly important speech-driven\nInformation Retrieval platforms that assist users with various tasks.\n  We discuss open problems and challenges with respect to modeling spoken\ninformation queries for virtual assistants, and list opportunities where\nInformation Retrieval methods and research can be applied to improve the\nquality of virtual assistant speech recognition.\n  We discuss how query domain classification, knowledge graphs and user\ninteraction data, and query personalization can be helpful to improve the\naccurate recognition of spoken information domain queries. Finally, we also\nprovide a brief overview of current problems and challenges in speech\nrecognition.", "published": "2023-04-25 20:52:40", "link": "http://arxiv.org/abs/2304.13149v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TABLET: Learning From Instructions For Tabular Data", "abstract": "Acquiring high-quality data is often a significant challenge in training\nmachine learning (ML) models for tabular prediction, particularly in\nprivacy-sensitive and costly domains like medicine and finance. Providing\nnatural language instructions to large language models (LLMs) offers an\nalternative solution. However, it is unclear how effectively instructions\nleverage the knowledge in LLMs for solving tabular prediction problems. To\naddress this gap, we introduce TABLET, a benchmark of 20 diverse tabular\ndatasets annotated with instructions that vary in their phrasing, granularity,\nand technicality. Additionally, TABLET includes the instructions' logic and\nstructured modifications to the instructions. We find in-context instructions\nincrease zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for\nChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular\nprediction in our benchmark by evaluating instruction faithfulness. We find\nLLMs often ignore instructions and fail to predict specific instances\ncorrectly, even with examples. Our analysis on TABLET shows that, while\ninstructions help LLM performance, learning from instructions for tabular data\nrequires new capabilities.", "published": "2023-04-25 23:07:20", "link": "http://arxiv.org/abs/2304.13188v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Explainable and Safe Conversational Agents for Mental Health: A\n  Survey", "abstract": "Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to\nsupport the overburdened global healthcare system that gets 60 million primary\ncare visits, and 6 million Emergency Room (ER) visits annually. These systems\nare built by clinical psychologists, psychiatrists, and Artificial Intelligence\n(AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role\nof VMHAs is to provide emotional support through information, focusing less on\ndeveloping a reflective conversation with the patient. A more comprehensive,\nsafe and explainable approach is required to build responsible VMHAs to ask\nfollow-up questions or provide a well-informed response. This survey offers a\nsystematic critical review of the existing conversational agents in mental\nhealth, followed by new insights into the improvements of VMHAs with contextual\nknowledge, datasets, and their emerging role in clinical decision support. We\nalso provide new directions toward enriching the user experience of VMHAs with\nexplainability, safety, and wholesome trustworthiness. Finally, we provide\nevaluation metrics and practical considerations for VMHAs beyond the current\nliterature to build trust between VMHAs and patients in active communications.", "published": "2023-04-25 23:12:13", "link": "http://arxiv.org/abs/2304.13191v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "What's in a Name? Evaluating Assembly-Part Semantic Knowledge in\n  Language Models through User-Provided Names in CAD Files", "abstract": "Semantic knowledge of part-part and part-whole relationships in assemblies is\nuseful for a variety of tasks from searching design repositories to the\nconstruction of engineering knowledge bases. In this work we propose that the\nnatural language names designers use in Computer Aided Design (CAD) software\nare a valuable source of such knowledge, and that Large Language Models (LLMs)\ncontain useful domain-specific information for working with this data as well\nas other CAD and engineering-related tasks.\n  In particular we extract and clean a large corpus of natural language part,\nfeature and document names and use this to quantitatively demonstrate that a\npre-trained language model can outperform numerous benchmarks on three\nself-supervised tasks, without ever having seen this data before. Moreover, we\nshow that fine-tuning on the text data corpus further boosts the performance on\nall tasks, thus demonstrating the value of the text data which until now has\nbeen largely ignored. We also identify key limitations to using LLMs with text\ndata alone, and our findings provide a strong motivation for further work into\nmulti-modal text-geometry models.\n  To aid and encourage further work in this area we make all our data and code\npublicly available.", "published": "2023-04-25 12:30:01", "link": "http://arxiv.org/abs/2304.14275v1", "categories": ["cs.CL", "cs.LG", "cc:68T50", "I.2.7; J.6"], "primary_category": "cs.CL"}
{"title": "GlyphDiffusion: Text Generation as Image Generation", "abstract": "Diffusion models have become a new generative paradigm for text generation.\nConsidering the discrete categorical nature of text, in this paper, we propose\nGlyphDiffusion, a novel diffusion approach for text generation via text-guided\nimage generation. Our key idea is to render the target text as a glyph image\ncontaining visual language content. In this way, conditional text generation\ncan be cast as a glyph image generation task, and it is then natural to apply\ncontinuous diffusion models to discrete texts. Specially, we utilize a cascaded\narchitecture (ie a base and a super-resolution diffusion model) to generate\nhigh-fidelity glyph images, conditioned on the input text. Furthermore, we\ndesign a text grounding module to transform and refine the visual language\ncontent from generated glyph images into the final texts. In experiments over\nfour conditional text generation tasks and two classes of metrics (ie quality\nand diversity), GlyphDiffusion can achieve comparable or even better results\nthan several baselines, including pretrained language models. Our model also\nmakes significant improvements compared to the recent diffusion model.", "published": "2023-04-25 02:14:44", "link": "http://arxiv.org/abs/2304.12519v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking\n  Head", "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. Despite the recent success, current LLMs are not capable of\nprocessing complex audio information or conducting spoken conversations (like\nSiri or Alexa). In this work, we propose a multi-modal AI system named\nAudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to\nprocess complex audio information and solve numerous understanding and\ngeneration tasks; and 2) the input/output interface (ASR, TTS) to support\nspoken dialogue. With an increasing demand to evaluate multi-modal LLMs of\nhuman intention understanding and cooperation with foundation models, we\noutline the principles and processes and test AudioGPT in terms of consistency,\ncapability, and robustness. Experimental results demonstrate the capabilities\nof AudioGPT in solving AI tasks with speech, music, sound, and talking head\nunderstanding and generation in multi-round dialogues, which empower humans to\ncreate rich and diverse audio content with unprecedented ease. Our system is\npublicly available at \\url{https://github.com/AIGC-Audio/AudioGPT}.", "published": "2023-04-25 17:05:38", "link": "http://arxiv.org/abs/2304.12995v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Novel Dual of Shannon Information and Weighting Scheme", "abstract": "Shannon Information theory has achieved great success in not only\ncommunication technology where it was originally developed for but also many\nother science and engineering fields such as machine learning and artificial\nintelligence. Inspired by the famous weighting scheme TF-IDF, we discovered\nthat information entropy has a natural dual. We complement the classical\nShannon information theory by proposing a novel quantity, namely troenpy.\nTroenpy measures the certainty, commonness and similarity of the underlying\ndistribution. To demonstrate its usefulness, we propose a troenpy based\nweighting scheme for document with class labels, namely positive class\nfrequency (PCF). On a collection of public datasets we show the PCF based\nweighting scheme outperforms the classical TF-IDF and a popular Optimal\nTransportation based word moving distance algorithm in a kNN setting. We\nfurther developed a new odds-ratio type feature, namely Expected Class\nInformation Bias(ECIB), which can be regarded as the expected odds ratio of the\ninformation quantity entropy and troenpy. In the experiments we observe that\nincluding the new ECIB features and simple binary term features in a simple\nlogistic regression model can further significantly improve the performance.\nThe simple new weighting scheme and ECIB features are very effective and can be\ncomputed with linear order complexity.", "published": "2023-04-25 13:40:50", "link": "http://arxiv.org/abs/2304.12814v1", "categories": ["cs.CL", "cs.IR", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "A New Information Theory of Certainty for Machine Learning", "abstract": "Claude Shannon coined entropy to quantify the uncertainty of a random\ndistribution for communication coding theory. We observe that the uncertainty\nnature of entropy also limits its direct usage in mathematical modeling.\nTherefore we propose a new concept troenpy,as the canonical dual of entropy, to\nquantify the certainty of the underlying distribution. We demonstrate two\napplications in machine learning. The first is for the classical document\nclassification, we develop a troenpy based weighting scheme to leverage the\ndocument class label. The second is a self-troenpy weighting scheme for\nsequential data and show that it can be easily included in neural network based\nlanguage models and achieve dramatic perplexity reduction. We also define\nquantum troenpy as the dual of the Von Neumann entropy to quantify the\ncertainty of quantum systems.", "published": "2023-04-25 14:03:57", "link": "http://arxiv.org/abs/2304.12833v1", "categories": ["cs.IT", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "Improving Speech Translation Accuracy and Time Efficiency with\n  Fine-tuned wav2vec 2.0-based Speech Segmentation", "abstract": "Speech translation (ST) automatically converts utterances in a source\nlanguage into text in another language. Splitting continuous speech into\nshorter segments, known as speech segmentation, plays an important role in ST.\nRecent segmentation methods trained to mimic the segmentation of ST corpora\nhave surpassed traditional approaches. Tsiamas et al. proposed a segmentation\nframe classifier (SFC) based on a pre-trained speech encoder called wav2vec\n2.0. Their method, named SHAS, retains 95-98% of the BLEU score for ST corpus\nsegmentation. However, the segments generated by SHAS are very different from\nST corpus segmentation and tend to be longer with multiple combined utterances.\nThis is due to SHAS's reliance on length heuristics, i.e., it splits speech\ninto segments of easily translatable length without fully considering the\npotential for ST improvement by splitting them into even shorter segments.\nLonger segments often degrade translation quality and ST's time efficiency. In\nthis study, we extended SHAS to improve ST translation accuracy and efficiency\nby splitting speech into shorter segments that correspond to sentences. We\nintroduced a simple segmentation algorithm using the moving average of SFC\npredictions without relying on length heuristics and explored wav2vec 2.0\nfine-tuning for improved speech segmentation prediction. Our experimental\nresults reveal that our speech segmentation method significantly improved the\nquality and the time efficiency of speech translation compared to SHAS.", "published": "2023-04-25 09:04:51", "link": "http://arxiv.org/abs/2304.12659v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Leveraging Audio-Tagging Assisted Sound Event Detection using Weakified\n  Strong Labels and Frequency Dynamic Convolutions", "abstract": "Jointly learning from a small labeled set and a larger unlabeled set is an\nactive research topic under semi-supervised learning (SSL). In this paper, we\npropose a novel SSL method based on a two-stage framework for leveraging a\nlarge unlabeled in-domain set. Stage-1 of our proposed framework focuses on\naudio-tagging (AT), which assists the sound event detection (SED) system in\nStage-2. The AT system is trained utilizing a strongly labeled set converted\ninto weak predictions referred to as weakified set, a weakly labeled set, and\nan unlabeled set. This AT system then infers on the unlabeled set to generate\nreliable pseudo-weak labels, which are used with the strongly and weakly\nlabeled set to train a frequency dynamic convolutional recurrent neural\nnetwork-based SED system at Stage-2 in a supervised manner. Our system\noutperforms the baseline by 45.5% in terms of polyphonic sound detection score\non the DESED real validation set.", "published": "2023-04-25 09:43:40", "link": "http://arxiv.org/abs/2304.12688v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Foley Sound Synthesis at the DCASE 2023 Challenge", "abstract": "The addition of Foley sound effects during post-production is a common\ntechnique used to enhance the perceived acoustic properties of multimedia\ncontent. Traditionally, Foley sound has been produced by human Foley artists,\nwhich involves manual recording and mixing of sound. However, recent advances\nin sound synthesis and generative models have generated interest in\nmachine-assisted or automatic Foley synthesis techniques. To promote further\nresearch in this area, we have organized a challenge in DCASE 2023: Task 7 -\nFoley Sound Synthesis. Our challenge aims to provide a standardized evaluation\nframework that is both rigorous and efficient, allowing for the evaluation of\ndifferent Foley synthesis systems. We received 17 submissions, and performed\nboth objective and subjective evaluation to rank them according to three\ncriteria: audio quality, fit-to-category, and diversity. Through this\nchallenge, we hope to encourage active participation from the research\ncommunity and advance the state-of-the-art in automatic Foley synthesis. In\nthis technical report, we provide a detailed overview of the Foley sound\nsynthesis challenge, including task definition, dataset, baseline, evaluation\nscheme and criteria, challenge result, and discussion.", "published": "2023-04-25 02:28:32", "link": "http://arxiv.org/abs/2304.12521v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Room dimensions and absorption inference from room transfer function via\n  machine learning", "abstract": "The inference of the absorption configuration of an existing room solely\nusing acoustic signals can be challenging. This research presents two methods\nfor estimating the room dimensions and frequency-dependent absorption\ncoefficients using room transfer functions. The first method, a knowledge-based\napproach, calculates the room dimensions through damped resonant frequencies of\nthe room. The second method, a machine learning approach, employs multi-task\nconvolutional neural networks for inferring the room dimensions and\nfrequency-dependent absorption coefficients of each surface. The study shows\nthat accurate wave-based simulation data can be used to train neural networks\nfor real-world measurements and demonstrates a potential for this algorithm to\nbe used to estimate the boundary input data for room acoustic simulations. The\nproposed methods can be a valuable tool for room acoustic simulations during\nacoustic renovation or intervention projects, as they enable to infer the room\ngeometry and absorption conditions with reasonably small data requirements.", "published": "2023-04-25 17:03:11", "link": "http://arxiv.org/abs/2304.12993v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Speaker Multi-Lingual VQTTS System for LIMMITS 2023 Challenge", "abstract": "In this paper, we describe the systems developed by the SJTU X-LANCE team for\nLIMMITS 2023 Challenge, and we mainly focus on the winning system on\nnaturalness for track 1. The aim of this challenge is to build a multi-speaker\nmulti-lingual text-to-speech (TTS) system for Marathi, Hindi and Telugu. Each\nof the languages has a male and a female speaker in the given dataset. In track\n1, only 5 hours data from each speaker can be selected to train the TTS model.\nOur system is based on the recently proposed VQTTS that utilizes VQ acoustic\nfeature rather than mel-spectrogram. We introduce additional speaker embeddings\nand language embeddings to VQTTS for controlling the speaker and language\ninformation. In the cross-lingual evaluations where we need to synthesize\nspeech in a cross-lingual speaker's voice, we provide a native speaker's\nembedding to the acoustic model and the target speaker's embedding to the\nvocoder. In the subjective MOS listening test on naturalness, our system\nachieves 4.77 which ranks first.", "published": "2023-04-25 19:54:37", "link": "http://arxiv.org/abs/2304.13121v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GTN-Bailando: Genre Consistent Long-Term 3D Dance Generation based on\n  Pre-trained Genre Token Network", "abstract": "Music-driven 3D dance generation has become an intensive research topic in\nrecent years with great potential for real-world applications. Most existing\nmethods lack the consideration of genre, which results in genre inconsistency\nin the generated dance movements. In addition, the correlation between the\ndance genre and the music has not been investigated. To address these issues,\nwe propose a genre-consistent dance generation framework, GTN-Bailando. First,\nwe propose the Genre Token Network (GTN), which infers the genre from music to\nenhance the genre consistency of long-term dance generation. Second, to improve\nthe generalization capability of the model, the strategy of pre-training and\nfine-tuning is adopted.Experimental results on the AIST++ dataset show that the\nproposed dance generation framework outperforms state-of-the-art methods in\nterms of motion quality and genre consistency.", "published": "2023-04-25 10:17:29", "link": "http://arxiv.org/abs/2304.12704v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Representations of Sound for Automatic Insect Recognition", "abstract": "Insect population numbers and biodiversity have been rapidly declining with\ntime, and monitoring these trends has become increasingly important for\nconservation measures to be effectively implemented. But monitoring methods are\noften invasive, time and resource intense, and prone to various biases. Many\ninsect species produce characteristic sounds that can easily be detected and\nrecorded without large cost or effort. Using deep learning methods, insect\nsounds from field recordings could be automatically detected and classified to\nmonitor biodiversity and species distribution ranges. We implement this using\nrecently published datasets of insect sounds (Orthoptera and Cicadidae) and\nmachine learning methods and evaluate their potential for acoustic insect\nmonitoring. We compare the performance of the conventional spectrogram-based\naudio representation against LEAF, a new adaptive and waveform-based frontend.\nLEAF achieved better classification performance than the mel-spectrogram\nfrontend by adapting its feature extraction parameters during training. This\nresult is encouraging for future implementations of deep learning technology\nfor automatic insect sound recognition, especially as larger datasets become\navailable.", "published": "2023-04-25 11:28:13", "link": "http://arxiv.org/abs/2304.12739v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "AI-Synthesized Voice Detection Using Neural Vocoder Artifacts", "abstract": "Advancements in AI-synthesized human voices have created a growing threat of\nimpersonation and disinformation, making it crucial to develop methods to\ndetect synthetic human voices. This study proposes a new approach to\nidentifying synthetic human voices by detecting artifacts of vocoders in audio\nsignals. Most DeepFake audio synthesis models use a neural vocoder, a neural\nnetwork that generates waveforms from temporal-frequency representations like\nmel-spectrograms. By identifying neural vocoder processing in audio, we can\ndetermine if a sample is synthesized. To detect synthetic human voices, we\nintroduce a multi-task learning framework for a binary-class RawNet2 model that\nshares the feature extractor with a vocoder identification module. By treating\nvocoder identification as a pretext task, we constrain the feature extractor to\nfocus on vocoder artifacts and provide discriminative features for the final\nbinary classifier. Our experiments show that the improved RawNet2 model based\non vocoder identification achieves high classification performance on the\nbinary task overall.", "published": "2023-04-25 18:36:28", "link": "http://arxiv.org/abs/2304.13085v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NUANCE: Near Ultrasound Attack On Networked Communication Environments", "abstract": "This study investigates a primary inaudible attack vector on Amazon Alexa\nvoice services using near ultrasound trojans and focuses on characterizing the\nattack surface and examining the practical implications of issuing inaudible\nvoice commands. The research maps each attack vector to a tactic or technique\nfrom the MITRE ATT&CK matrix, covering enterprise, mobile, and Industrial\nControl System (ICS) frameworks. The experiment involved generating and\nsurveying fifty near-ultrasonic audios to assess the attacks' effectiveness,\nwith unprocessed commands having a 100% success rate and processed ones\nachieving a 58% overall success rate. This systematic approach stimulates\npreviously unaddressed attack surfaces, ensuring comprehensive detection and\nattack design while pairing each ATT&CK Identifier with a tested defensive\nmethod, providing attack and defense tactics for prompt-response options. The\nmain findings reveal that the attack method employs Single Upper Sideband\nAmplitude Modulation (SUSBAM) to generate near-ultrasonic audio from audible\nsources, transforming spoken commands into a frequency range beyond human-adult\nhearing. By eliminating the lower sideband, the design achieves a 6 kHz minimum\nfrom 16-22 kHz while remaining inaudible after transformation. The research\ninvestigates the one-to-many attack surface where a single device\nsimultaneously triggers multiple actions or devices. Additionally, the study\ndemonstrates the reversibility or demodulation of the inaudible signal,\nsuggesting potential alerting methods and the possibility of embedding secret\nmessages like audio steganography.", "published": "2023-04-25 23:28:46", "link": "http://arxiv.org/abs/2305.10358v2", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
