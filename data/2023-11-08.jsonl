{"title": "Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments", "abstract": "As a sub-discipline of evolutionary and computational linguistics, emergent\ncommunication (EC) studies communication protocols, called emergent languages,\narising in simulations where agents communicate. A key goal of EC is to give\nrise to languages that share statistical properties with natural languages. In\nthis paper, we reinterpret Lewis's signaling game, a frequently used setting in\nEC, as beta-VAE and reformulate its objective function as ELBO. Consequently,\nwe clarify the existence of prior distributions of emergent languages and show\nthat the choice of the priors can influence their statistical properties.\nSpecifically, we address the properties of word lengths and segmentation, known\nas Zipf's law of abbreviation (ZLA) and Harris's articulation scheme (HAS),\nrespectively. It has been reported that the emergent languages do not follow\nthem when using the conventional objective. We experimentally demonstrate that\nby selecting an appropriate prior distribution, more natural segments emerge,\nwhile suggesting that the conventional one prevents the languages from\nfollowing ZLA and HAS.", "published": "2023-11-08 04:34:30", "link": "http://arxiv.org/abs/2311.04453v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-label and Multi-target Sampling of Machine Annotation for\n  Computational Stance Detection", "abstract": "Data collection from manual labeling provides domain-specific and\ntask-aligned supervision for data-driven approaches, and a critical mass of\nwell-annotated resources is required to achieve reasonable performance in\nnatural language processing tasks. However, manual annotations are often\nchallenging to scale up in terms of time and budget, especially when domain\nknowledge, capturing subtle semantic features, and reasoning steps are needed.\nIn this paper, we investigate the efficacy of leveraging large language models\non automated labeling for computational stance detection. We empirically\nobserve that while large language models show strong potential as an\nalternative to human annotators, their sensitivity to task-specific\ninstructions and their intrinsic biases pose intriguing yet unique challenges\nin machine annotation. We introduce a multi-label and multi-target sampling\nstrategy to optimize the annotation quality. Experimental results on the\nbenchmark stance detection corpora show that our method can significantly\nimprove performance and learning efficacy.", "published": "2023-11-08 06:54:34", "link": "http://arxiv.org/abs/2311.04495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large GPT-like Models are Bad Babies: A Closer Look at the Relationship\n  between Linguistic Competence and Psycholinguistic Measures", "abstract": "Research on the cognitive plausibility of language models (LMs) has so far\nmostly concentrated on modelling psycholinguistic response variables such as\nreading times, gaze durations and N400/P600 EEG signals, while mostly leaving\nout the dimension of what Mahowald et al. (2023) described as formal and\nfunctional linguistic competence, and developmental plausibility. We address\nthis gap by training a series of GPT-like language models of different sizes on\nthe strict version of the BabyLM pretraining corpus, evaluating on the\nchallenge tasks (BLiMP, GLUE, MSGS) and an additional reading time prediction\ntask. We find a positive correlation between LM size and performance on all\nthree challenge tasks, with different preferences for model width and depth in\neach of the tasks. In contrast, a negative correlation was found between LM\nsize and reading time fit of linear mixed-effects models using LM surprisal as\na predictor, with the second-smallest LM achieving the largest log-likelihood\nreduction over a baseline model without surprisal. This suggests that modelling\nprocessing effort and linguistic competence may require an approach different\nfrom training GPT-like LMs on a developmentally plausible corpus.", "published": "2023-11-08 09:26:27", "link": "http://arxiv.org/abs/2311.04547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Distractors in Multiple-Choice Tests", "abstract": "Multiple-choice tests are a common approach for assessing candidates'\ncomprehension skills. Standard multiple-choice reading comprehension exams\nrequire candidates to select the correct answer option from a discrete set\nbased on a question in relation to a contextual passage. For appropriate\nassessment, the distractor answer options must by definition be incorrect but\nplausible and diverse. However, generating good quality distractors satisfying\nthese criteria is a challenging task for content creators. We propose automated\nassessment metrics for the quality of distractors in multiple-choice reading\ncomprehension tests. Specifically, we define quality in terms of the\nincorrectness, plausibility and diversity of the distractor options. We assess\nincorrectness using the classification ability of a binary multiple-choice\nreading comprehension system. Plausibility is assessed by considering the\ndistractor confidence - the probability mass associated with the distractor\noptions for a standard multi-class multiple-choice reading comprehension\nsystem. Diversity is assessed by pairwise comparison of an embedding-based\nequivalence metric between the distractors of a question. To further validate\nthe plausibility metric we compare against candidate distributions over\nmultiple-choice questions and agreement with a ChatGPT model's interpretation\nof distractor plausibility and diversity.", "published": "2023-11-08 09:37:09", "link": "http://arxiv.org/abs/2311.04554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Nature of Disagreements on Mid-Scale Ratings: A Case\n  Study on the Abstractness-Concreteness Continuum", "abstract": "Humans tend to strongly agree on ratings on a scale for extreme cases (e.g.,\na CAT is judged as very concrete), but judgements on mid-scale words exhibit\nmore disagreement. Yet, collected rating norms are heavily exploited across\ndisciplines. Our study focuses on concreteness ratings and (i) implements\ncorrelations and supervised classification to identify salient multi-modal\ncharacteristics of mid-scale words, and (ii) applies a hard clustering to\nidentify patterns of systematic disagreement across raters. Our results suggest\nto either fine-tune or filter mid-scale target words before utilising them.", "published": "2023-11-08 09:52:58", "link": "http://arxiv.org/abs/2311.04563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Profiling Irony & Stereotype: Exploring Sentiment, Topic, and Lexical\n  Features", "abstract": "Social media has become a very popular source of information. With this\npopularity comes an interest in systems that can classify the information\nproduced. This study tries to create such a system detecting irony in Twitter\nusers. Recent work emphasize the importance of lexical features, sentiment\nfeatures and the contrast herein along with TF-IDF and topic models. Based on a\nthorough feature selection process, the resulting model contains specific\nsub-features from these areas. Our model reaches an F1-score of 0.84, which is\nabove the baseline. We find that lexical features, especially TF-IDF,\ncontribute the most to our models while sentiment and topic modeling features\ncontribute less to overall performance. Lastly, we highlight multiple\ninteresting and important paths for further exploration.", "published": "2023-11-08 18:44:47", "link": "http://arxiv.org/abs/2311.04885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs", "abstract": "Recent works have showcased the ability of LLMs to embody diverse personas in\ntheir responses, exemplified by prompts like 'You are Yoda. Explain the Theory\nof Relativity.' While this ability allows personalization of LLMs and enables\nhuman behavior simulation, its effect on LLMs' capabilities remains unclear. To\nfill this gap, we present the first extensive study of the unintended\nside-effects of persona assignment on the ability of LLMs to perform basic\nreasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse\npersonas (e.g. an Asian person) spanning 5 socio-demographic groups. Our\nexperiments unveil that LLMs harbor deep rooted bias against various\nsocio-demographics underneath a veneer of fairness. While they overtly reject\nstereotypes when explicitly asked ('Are Black people less skilled at\nmathematics?'), they manifest stereotypical and erroneous presumptions when\nasked to answer questions while adopting a persona. These can be observed as\nabstentions in responses, e.g., 'As a Black person, I can't answer this\nquestion as it requires math knowledge', and generally result in a substantial\nperformance drop. Our experiments with ChatGPT-3.5 show that this bias is\nubiquitous - 80% of our personas demonstrate bias; it is significant - some\ndatasets show performance drops of 70%+; and can be especially harmful for\ncertain groups - some personas suffer statistically significant drops on 80%+\nof the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with\nGPT-4-Turbo showing the least but still a problematic amount of bias (evident\nin 42% of the personas). Further analysis shows that these persona-induced\nerrors can be hard-to-discern and hard-to-avoid. Our findings serve as a\ncautionary tale that the practice of assigning personas to LLMs - a trend on\nthe rise - can surface their deep-rooted biases and have unforeseeable and\ndetrimental side-effects.", "published": "2023-11-08 18:52:17", "link": "http://arxiv.org/abs/2311.04892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Abstract Is Linguistic Generalization in Large Language Models?\n  Experiments with Argument Structure", "abstract": "Language models are typically evaluated on their success at predicting the\ndistribution of specific words in specific contexts. Yet linguistic knowledge\nalso encodes relationships between contexts, allowing inferences between word\ndistributions. We investigate the degree to which pre-trained Transformer-based\nlarge language models (LLMs) represent such relationships, focusing on the\ndomain of argument structure. We find that LLMs perform well in generalizing\nthe distribution of a novel noun argument between related contexts that were\nseen during pre-training (e.g., the active object and passive subject of the\nverb spray), succeeding by making use of the semantically-organized structure\nof the embedding space for word embeddings. However, LLMs fail at\ngeneralizations between related contexts that have not been observed during\npre-training, but which instantiate more abstract, but well-attested structural\ngeneralizations (e.g., between the active object and passive subject of an\narbitrary verb). Instead, in this case, LLMs show a bias to generalize based on\nlinear order. This finding points to a limitation with current models and\npoints to a reason for which their training is data-intensive.s reported here\nare available at https://github.com/clay-lab/structural-alternations.", "published": "2023-11-08 18:58:43", "link": "http://arxiv.org/abs/2311.04900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the steerability of large language models toward data-driven personas", "abstract": "Large language models (LLMs) are known to generate biased responses where the\nopinions of certain groups and populations are underrepresented. Here, we\npresent a novel approach to achieve controllable generation of specific\nviewpoints using LLMs, that can be leveraged to produce multiple perspectives\nand to reflect the diverse opinions. Moving beyond the traditional reliance on\ndemographics like age, gender, or party affiliation, we introduce a data-driven\nnotion of persona grounded in collaborative filtering, which is defined as\neither a single individual or a cohort of individuals manifesting similar views\nacross specific inquiries. As individuals in the same demographic group may\nhave different personas, our data-driven persona definition allows for a more\nnuanced understanding of different (latent) social groups present in the\npopulation. In addition to this, we also explore an efficient method to steer\nLLMs toward the personas that we define. We show that our data-driven personas\nsignificantly enhance model steerability, with improvements of between\n$57\\%-77\\%$ over our best performing baselines.", "published": "2023-11-08 19:01:13", "link": "http://arxiv.org/abs/2311.04978v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "First Tragedy, then Parse: History Repeats Itself in the New Era of\n  Large Language Models", "abstract": "Many NLP researchers are experiencing an existential crisis triggered by the\nastonishing success of ChatGPT and other systems based on large language models\n(LLMs). After such a disruptive change to our understanding of the field, what\nis left to do? Taking a historical lens, we look for guidance from the first\nera of LLMs, which began in 2005 with large $n$-gram models for machine\ntranslation (MT). We identify durable lessons from the first era, and more\nimportantly, we identify evergreen problems where NLP researchers can continue\nto make meaningful contributions in areas where LLMs are ascendant. We argue\nthat disparities in scale are transient and researchers can work to reduce\nthem; that data, rather than hardware, is still a bottleneck for many\napplications; that meaningful realistic evaluation is still an open problem;\nand that there is still room for speculative approaches.", "published": "2023-11-08 21:13:38", "link": "http://arxiv.org/abs/2311.05020v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning Brasil at ABSAPT 2022: Portuguese Transformer Ensemble\n  Approaches", "abstract": "Aspect-based Sentiment Analysis (ABSA) is a task whose objective is to\nclassify the individual sentiment polarity of all entities, called aspects, in\na sentence. The task is composed of two subtasks: Aspect Term Extraction (ATE),\nidentify all aspect terms in a sentence; and Sentiment Orientation Extraction\n(SOE), given a sentence and its aspect terms, the task is to determine the\nsentiment polarity of each aspect term (positive, negative or neutral). This\narticle presents we present our participation in Aspect-Based Sentiment\nAnalysis in Portuguese (ABSAPT) 2022 at IberLEF 2022. We submitted the best\nperforming systems, achieving new state-of-the-art results on both subtasks.", "published": "2023-11-08 22:58:34", "link": "http://arxiv.org/abs/2311.05051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recursion in Recursion: Two-Level Nested Recursion for Length\n  Generalization with Scalability", "abstract": "Binary Balanced Tree RvNNs (BBT-RvNNs) enforce sequence composition according\nto a preset balanced binary tree structure. Thus, their non-linear recursion\ndepth is just $\\log_2 n$ ($n$ being the sequence length). Such logarithmic\nscaling makes BBT-RvNNs efficient and scalable on long sequence tasks such as\nLong Range Arena (LRA). However, such computational efficiency comes at a cost\nbecause BBT-RvNNs cannot solve simple arithmetic tasks like ListOps. On the\nflip side, RvNNs (e.g., Beam Tree RvNN) that do succeed on ListOps (and other\nstructure-sensitive tasks like formal logical inference) are generally several\ntimes more expensive than even RNNs. In this paper, we introduce a novel\nframework -- Recursion in Recursion (RIR) to strike a balance between the two\nsides - getting some of the benefits from both worlds. In RIR, we use a form of\ntwo-level nested recursion - where the outer recursion is a $k$-ary balanced\ntree model with another recursive model (inner recursion) implementing its cell\nfunction. For the inner recursion, we choose Beam Tree RvNNs (BT-RvNN). To\nadjust BT-RvNNs within RIR we also propose a novel strategy of beam alignment.\nOverall, this entails that the total recursive depth in RIR is upper-bounded by\n$k \\log_k n$. Our best RIR-based model is the first model that demonstrates\nhigh ($\\geq 90\\%$) length-generalization performance on ListOps while at the\nsame time being scalable enough to be trainable on long sequence inputs from\nLRA. Moreover, in terms of accuracy in the LRA language tasks, it performs\ncompetitively with Structured State Space Models (SSMs) without any special\ninitialization - outperforming Transformers by a large margin. On the other\nhand, while SSMs can marginally outperform RIR on LRA, they (SSMs) fail to\nlength-generalize on ListOps. Our code is available at:\n\\url{https://github.com/JRC1995/BeamRecursionFamily/}.", "published": "2023-11-08 04:20:56", "link": "http://arxiv.org/abs/2311.04449v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Pacing in Long-Form Story Planning", "abstract": "Existing LLM-based systems for writing long-form stories or story outlines\nfrequently suffer from unnatural pacing, whether glossing over important events\nor over-elaborating on insignificant details, resulting in a jarring experience\nfor the reader. We propose a CONCrete Outline ConTrol (CONCOCT) system to\nimprove pacing when automatically generating story outlines. We first train a\nconcreteness evaluator to judge which of two events is more concrete\n(low-level-detailed). This evaluator can then be used to control pacing in\nhierarchical outline generation; in this work, we explore a vaguest-first\nexpansion procedure that aims for uniform pacing. We further use the evaluator\nto filter new outline items based on predicted concreteness. Compared to a\nbaseline hierarchical outline generator, humans judge CONCOCT's pacing to be\nmore consistent over 57% of the time across multiple outline lengths; the gains\nalso translate to downstream stories. All code, data, and models are\nopen-sourced.", "published": "2023-11-08 04:58:29", "link": "http://arxiv.org/abs/2311.04459v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RDGCN: Reinforced Dependency Graph Convolutional Network for\n  Aspect-based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) is dedicated to forecasting the\nsentiment polarity of aspect terms within sentences. Employing graph neural\nnetworks to capture structural patterns from syntactic dependency parsing has\nbeen confirmed as an effective approach for boosting ABSA. In most works, the\ntopology of dependency trees or dependency-based attention coefficients is\noften loosely regarded as edges between aspects and opinions, which can result\nin insufficient and ambiguous syntactic utilization. To address these problems,\nwe propose a new reinforced dependency graph convolutional network (RDGCN) that\nimproves the importance calculation of dependencies in both distance and type\nviews. Initially, we propose an importance calculation criterion for the\nminimum distances over dependency trees. Under the criterion, we design a\ndistance-importance function that leverages reinforcement learning for weight\ndistribution search and dissimilarity control. Since dependency types often do\nnot have explicit syntax like tree distances, we use global attention and mask\nmechanisms to design type-importance functions. Finally, we merge these weights\nand implement feature aggregation and classification. Comprehensive experiments\non three popular datasets demonstrate the effectiveness of the criterion and\nimportance functions. RDGCN outperforms state-of-the-art GNN-based baselines in\nall validations.", "published": "2023-11-08 05:37:49", "link": "http://arxiv.org/abs/2311.04467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conversation Understanding using Relational Temporal Graph Neural\n  Networks with Auxiliary Cross-Modality Interaction", "abstract": "Emotion recognition is a crucial task for human conversation understanding.\nIt becomes more challenging with the notion of multimodal data, e.g., language,\nvoice, and facial expressions. As a typical solution, the global- and the local\ncontext information are exploited to predict the emotional label for every\nsingle sentence, i.e., utterance, in the dialogue. Specifically, the global\nrepresentation could be captured via modeling of cross-modal interactions at\nthe conversation level. The local one is often inferred using the temporal\ninformation of speakers or emotional shifts, which neglects vital factors at\nthe utterance level. Additionally, most existing approaches take fused features\nof multiple modalities in an unified input without leveraging modality-specific\nrepresentations. Motivating from these problems, we propose the Relational\nTemporal Graph Neural Network with Auxiliary Cross-Modality Interaction\n(CORECT), an novel neural network framework that effectively captures\nconversation-level cross-modality interactions and utterance-level temporal\ndependencies with the modality-specific manner for conversation understanding.\nExtensive experiments demonstrate the effectiveness of CORECT via its\nstate-of-the-art results on the IEMOCAP and CMU-MOSEI datasets for the\nmultimodal ERC task.", "published": "2023-11-08 07:46:25", "link": "http://arxiv.org/abs/2311.04507v3", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models", "abstract": "Despite Multi-modal Large Language Models (MM-LLMs) have made exciting\nstrides recently, they are still struggling to efficiently model the\ninteractions among multi-modal inputs and the generation in non-textual\nmodalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an\napproach to treat the input from any modality as a token sequence and learn a\njoint embedding space for all modalities. Specifically, for the input from any\nmodality, TEAL first discretizes it into a token sequence with the\noff-the-shelf tokenizer and embeds the token sequence into a joint embedding\nspace with a learnable embedding matrix. MM-LLMs just need to predict the\nmulti-modal tokens autoregressively as the textual LLMs do. Finally, the\ncorresponding de-tokenizer is applied to generate the output in each modality\nbased on the predicted token sequence. With the joint embedding space, TEAL\nenables the frozen LLMs to perform both understanding and generation tasks\ninvolving non-textual modalities, such as image and audio. Thus, the textual\nLLM can just work as an interface and maintain its high performance in textual\nunderstanding and generation. Experiments show that TEAL achieves substantial\nimprovements in multi-modal understanding, and implements a simple scheme for\nmulti-modal generations.", "published": "2023-11-08 10:34:16", "link": "http://arxiv.org/abs/2311.04589v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Massive Editing for Large Language Models via Meta Learning", "abstract": "While large language models (LLMs) have enabled learning knowledge from the\npre-training corpora, the acquired knowledge may be fundamentally incorrect or\noutdated over time, which necessitates rectifying the knowledge of the language\nmodel (LM) after the training. A promising approach involves employing a\nhyper-network to generate parameter shift, whereas existing hyper-networks\nsuffer from inferior scalability in synchronous editing operation amount. To\nmitigate the problem, we propose the MAssive Language Model Editing Network\n(MALMEN), which formulates the parameter shift aggregation as the least square\nproblem, subsequently updating the LM parameters using the normal equation. To\naccommodate editing multiple facts simultaneously with limited memory budgets,\nwe separate the computation on the hyper-network and LM, enabling arbitrary\nbatch size on both neural networks. Our method is evaluated by editing up to\nthousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,\nT5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,\ni.e., closed book fact-checking and question answering. Remarkably, MALMEN is\ncapable of editing hundreds of times more facts than strong baselines with the\nidentical hyper-network architecture and outperforms editor specifically\ndesigned for GPT. Our code is available at\nhttps://github.com/ChenmienTan/malmen.", "published": "2023-11-08 13:03:06", "link": "http://arxiv.org/abs/2311.04661v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pre-training LLMs using human-like development data corpus", "abstract": "Pre-trained Large Language Models (LLMs) have shown success in a diverse set\nof language inference and understanding tasks. The pre-training stage of LLMs\nlooks at a large corpus of raw textual data. The BabyLM shared task compares\nLLM pre-training to human language acquisition, where the number of tokens seen\nby 13-year-old kids is magnitudes smaller than the number of tokens seen by\nLLMs. In this work, we pre-train and evaluate LLMs on their ability to learn\ncontextual word representations using roughly the same number of tokens as seen\nby children. We provide a strong set of baselines; with different\narchitectures, evaluation of changes in performance across epochs, and reported\npre-training metrics for the strict small and strict tracks of the task. We\nalso try to loosely replicate the RoBERTa baseline given by the task organizers\nto observe the training robustness to hyperparameter selection and\nreplicability. We provide the submission details to the strict and strict-small\ntracks in this report.", "published": "2023-11-08 13:13:23", "link": "http://arxiv.org/abs/2311.04666v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Generative Ad Hoc Information Retrieval", "abstract": "Recent advances in large language models have enabled the development of\nviable generative retrieval systems. Instead of a traditional document ranking,\ngenerative retrieval systems often directly return a grounded generated text as\na response to a query. Quantifying the utility of the textual responses is\nessential for appropriately evaluating such generative ad hoc retrieval. Yet,\nthe established evaluation methodology for ranking-based ad hoc retrieval is\nnot suited for the reliable and reproducible evaluation of generated responses.\nTo lay a foundation for developing new evaluation methods for generative\nretrieval systems, we survey the relevant literature from the fields of\ninformation retrieval and natural language processing, identify search tasks\nand system architectures in generative retrieval, develop a new user model, and\nstudy its operationalization.", "published": "2023-11-08 14:05:00", "link": "http://arxiv.org/abs/2311.04694v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Large-scale study of human memory for meaningful narratives", "abstract": "The statistical study of human memory requires large-scale experiments,\ninvolving many stimuli conditions and test subjects. While this approach has\nproven to be quite fruitful for meaningless material such as random lists of\nwords, naturalistic stimuli, like narratives, have until now resisted such a\nlarge-scale study, due to the quantity of manual labor required to design and\nanalyze such experiments. In this work, we develop a pipeline that uses large\nlanguage models (LLMs) both to design naturalistic narrative stimuli for\nlarge-scale recall and recognition memory experiments, as well as to analyze\nthe results. We performed online memory experiments with a large number of\nparticipants and collected recognition and recall data for narratives of\ndifferent sizes. We found that both recall and recognition performance scale\nlinearly with narrative length; however, for longer narratives people tend to\nsummarize the content rather than recalling precise details. To investigate the\nrole of narrative comprehension in memory, we repeated these experiments using\nscrambled versions of the narratives. Although recall performance declined\nsignificantly, recognition remained largely unaffected. Recalls in this\ncondition seem to follow the original narrative order rather than the actual\nscrambled presentation, pointing to a contextual reconstruction of the story in\nmemory. Finally, using LLM text embeddings, we construct a simple measure for\neach clause based on semantic similarity to the whole narrative, that shows a\nstrong correlation with recall probability. Overall, our work demonstrates the\npower of LLMs in accessing new regimes in the study of human memory, as well as\nsuggesting novel psychologically informed benchmarks for LLM performance.", "published": "2023-11-08 15:11:57", "link": "http://arxiv.org/abs/2311.04742v3", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert\n  Pretraining", "abstract": "Building on the cost-efficient pretraining advancements brought about by\nCrammed BERT, we enhance its performance and interpretability further by\nintroducing a novel pretrained model Dependency Agreement Crammed BERT\n(DACBERT) and its two-stage pretraining framework - Dependency Agreement\nPretraining. This framework, grounded by linguistic theories, seamlessly weaves\nsyntax and semantic information into the pretraining process. The first stage\nemploys four dedicated submodels to capture representative dependency\nagreements at the chunk level, effectively converting these agreements into\nembeddings. The second stage uses these refined embeddings, in tandem with\nconventional BERT embeddings, to guide the pretraining of the rest of the\nmodel. Evaluated on the GLUE benchmark, our DACBERT demonstrates notable\nimprovement across various tasks, surpassing Crammed BERT by 3.13% in the RTE\ntask and by 2.26% in the MRPC task. Furthermore, our method boosts the average\nGLUE score by 0.83%, underscoring its significant potential. The pretraining\nprocess can be efficiently executed on a single GPU within a 24-hour cycle,\nnecessitating no supplementary computational resources or extending the\npretraining duration compared with the Crammed BERT. Extensive studies further\nilluminate our approach's instrumental role in bolstering the interpretability\nof pretrained language models for natural language understanding tasks.", "published": "2023-11-08 16:18:32", "link": "http://arxiv.org/abs/2311.04799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over\n  Time-Involved Document", "abstract": "The facts and time in the document are intricately intertwined, making\ntemporal reasoning over documents challenging. Previous work models time\nimplicitly, making it difficult to handle such complex relationships. To\naddress this issue, we propose MTGER, a novel Multi-view Temporal Graph\nEnhanced Temporal Reasoning framework for temporal reasoning over time-involved\ndocuments. Concretely, MTGER explicitly models the temporal relationships among\nfacts by multi-view temporal graphs. On the one hand, the heterogeneous\ntemporal graphs explicitly model the temporal and discourse relationships among\nfacts; on the other hand, the multi-view mechanism captures both time-focused\nand fact-focused information, allowing the two views to complement each other\nthrough adaptive fusion. To further improve the implicit reasoning capability\nof the model, we design a self-supervised time-comparing objective. Extensive\nexperimental results demonstrate the effectiveness of our method on the TimeQA\nand SituatedQA datasets. Furthermore, MTGER gives more consistent answers under\nquestion perturbations.", "published": "2023-11-08 16:41:37", "link": "http://arxiv.org/abs/2311.04816v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling", "abstract": "Transformers have surpassed RNNs in popularity due to their superior\nabilities in parallel training and long-term dependency modeling. Recently,\nthere has been a renewed interest in using linear RNNs for efficient sequence\nmodeling. These linear RNNs often employ gating mechanisms in the output of the\nlinear recurrence layer while ignoring the significance of using forget gates\nwithin the recurrence. In this paper, we propose a gated linear RNN model\ndubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes\nforget gates that are lower bounded by a learnable value. The lower bound\nincreases monotonically when moving up layers. This allows the upper layers to\nmodel long-term dependencies and the lower layers to model more local,\nshort-term dependencies. Experiments on language modeling, image\nclassification, and long-range arena benchmarks showcase the efficiency and\neffectiveness of our proposed model. The source code is available at\nhttps://github.com/OpenNLPLab/HGRN.", "published": "2023-11-08 16:50:05", "link": "http://arxiv.org/abs/2311.04823v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking Benchmark and Contamination for Language Models with\n  Rephrased Samples", "abstract": "Large language models are increasingly trained on all the data ever produced\nby humans. Many have raised concerns about the trustworthiness of public\nbenchmarks due to potential contamination in pre-training or fine-tuning\ndatasets. While most data decontamination efforts apply string matching (e.g.,\nn-gram overlap) to remove benchmark data, we show that these methods are\ninsufficient, and simple variations of test data (e.g., paraphrasing,\ntranslation) can easily bypass these decontamination measures. Furthermore, we\ndemonstrate that if such variation of test data is not eliminated, a 13B model\ncan easily overfit a test benchmark and achieve drastically high performance,\non par with GPT-4. We validate such observations in widely used benchmarks such\nas MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a\nstronger LLM-based decontamination method and apply it to widely used\npre-training and fine-tuning datasets, revealing significant previously unknown\ntest overlap. For example, in pre-training sets such as RedPajama-Data-1T and\nStarCoder-Data, we identified that 8-18\\% of the HumanEval benchmark overlaps.\nInterestingly, we also find such contamination in synthetic dataset generated\nby GPT-3.5/4, suggesting a potential risk of unintentional contamination. We\nurge the community to adopt stronger decontamination approaches when using\npublic benchmarks. Moreover, we call for the community to actively develop\nfresh one-time exams to evaluate models accurately. Our decontamination tool is\npublicly available at https://github.com/lm-sys/llm-decontaminator.", "published": "2023-11-08 17:35:20", "link": "http://arxiv.org/abs/2311.04850v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongQLoRA: Efficient and Effective Method to Extend Context Length of\n  Large Language Models", "abstract": "We present LongQLoRA, an efficient and effective method to extend context\nlength of large language models with less training resources. LongQLoRA\ncombines the advantages of Position Interpolation, QLoRA and Shift Short\nAttention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend the\ncontext length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within\n1000 finetuning steps. LongQLoRA achieves competitive perplexity performance on\nPG19 and Proof-pile datasets, our model outperforms LongLoRA and is very close\nto MPT-7B-8K within the evaluation context length of 8192. We collect and build\n39k long instruction data to extend context length of Vicuna-13B from 4096 to\n8192 and achieve good performance both in long and short context generation\ntask. We also do some ablation experiments to study the effect of LoRA rank,\nfinetuning steps and attention patterns in inference.The model weights,\ntraining data and code are avaliable at\nhttps://github.com/yangjianxin1/LongQLoRA.", "published": "2023-11-08 18:33:06", "link": "http://arxiv.org/abs/2311.04879v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Future Lens: Anticipating Subsequent Tokens from a Single Hidden State", "abstract": "We conjecture that hidden state vectors corresponding to individual input\ntokens encode information sufficient to accurately predict several tokens\nahead. More concretely, in this paper we ask: Given a hidden (internal)\nrepresentation of a single token at position $t$ in an input, can we reliably\nanticipate the tokens that will appear at positions $\\geq t + 2$? To test this,\nwe measure linear approximation and causal intervention methods in GPT-J-6B to\nevaluate the degree to which individual hidden states in the network contain\nsignal rich enough to predict future hidden states and, ultimately, token\noutputs. We find that, at some layers, we can approximate a model's output with\nmore than 48% accuracy with respect to its prediction of subsequent tokens\nthrough a single hidden state. Finally we present a \"Future Lens\" visualization\nthat uses these methods to create a new view of transformer states.", "published": "2023-11-08 18:56:35", "link": "http://arxiv.org/abs/2311.04897v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?", "abstract": "Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\".", "published": "2023-11-08 01:45:37", "link": "http://arxiv.org/abs/2311.04939v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt Sketching for Large Language Models", "abstract": "Many recent prompting strategies for large language models (LLMs) query the\nmodel multiple times sequentially -- first to produce intermediate results and\nthen the final answer. However, using these methods, both decoder and model are\nunaware of potential follow-up prompts, leading to disconnected and undesirably\nwordy intermediate responses. In this work, we address this issue by proposing\nprompt sketching, a new prompting paradigm in which an LLM does not only\nrespond by completing a prompt, but by predicting values for multiple variables\nin a template. This way, sketching grants users more control over the\ngeneration process, e.g., by providing a reasoning framework via intermediate\ninstructions, leading to better overall results. The key idea enabling\nsketching with existing, autoregressive models is to adapt the decoding\nprocedure to also score follow-up instructions during text generation, thus\noptimizing overall template likelihood in inference. Our experiments show that\nin a zero-shot setting, prompt sketching outperforms existing, sequential\nprompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM\nbenchmarking tasks, including state tracking, arithmetic reasoning, and general\nquestion answering. To facilitate future use, we release a number of generic,\nyet effective sketches applicable to many tasks, and an open source library\ncalled dclib, powering our sketch-aware decoders.", "published": "2023-11-08 18:57:23", "link": "http://arxiv.org/abs/2311.04954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpreting Pretrained Language Models via Concept Bottlenecks", "abstract": "Pretrained language models (PLMs) have made significant strides in various\nnatural language processing tasks. However, the lack of interpretability due to\ntheir ``black-box'' nature poses challenges for responsible implementation.\nAlthough previous studies have attempted to improve interpretability by using,\ne.g., attention weights in self-attention layers, these weights often lack\nclarity, readability, and intuitiveness. In this research, we propose a novel\napproach to interpreting PLMs by employing high-level, meaningful concepts that\nare easily understandable for humans. For example, we learn the concept of\n``Food'' and investigate how it influences the prediction of a model's\nsentiment towards a restaurant review. We introduce C$^3$M, which combines\nhuman-annotated and machine-generated concepts to extract hidden neurons\ndesigned to encapsulate semantically meaningful and task-specific concepts.\nThrough empirical evaluations on real-world datasets, we manifest that our\napproach offers valuable insights to interpret PLM behavior, helps diagnose\nmodel failures, and enhances model robustness amidst noisy concept labels.", "published": "2023-11-08 20:41:18", "link": "http://arxiv.org/abs/2311.05014v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeepLearningBrasil@LT-EDI-2023: Exploring Deep Learning Techniques for\n  Detecting Depression in Social Media Text", "abstract": "In this paper, we delineate the strategy employed by our team,\nDeepLearningBrasil, which secured us the first place in the shared task\nDepSign-LT-EDI@RANLP-2023, achieving a 47.0% Macro F1-Score and a notable 2.4%\nadvantage. The task was to classify social media texts into three distinct\nlevels of depression - \"not depressed,\" \"moderately depressed,\" and \"severely\ndepressed.\" Leveraging the power of the RoBERTa and DeBERTa models, we further\npre-trained them on a collected Reddit dataset, specifically curated from\nmental health-related Reddit's communities (Subreddits), leading to an enhanced\nunderstanding of nuanced mental health discourse. To address lengthy textual\ndata, we used truncation techniques that retained the essence of the content by\nfocusing on its beginnings and endings. Our model was robust against unbalanced\ndata by incorporating sample weights into the loss. Cross-validation and\nensemble techniques were then employed to combine our k-fold trained models,\ndelivering an optimal solution. The accompanying code is made available for\ntransparency and further development.", "published": "2023-11-08 22:42:31", "link": "http://arxiv.org/abs/2311.05047v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NLQxform: A Language Model-based Question to SPARQL Transformer", "abstract": "In recent years, scholarly data has grown dramatically in terms of both scale\nand complexity. It becomes increasingly challenging to retrieve information\nfrom scholarly knowledge graphs that include large-scale heterogeneous\nrelationships, such as authorship, affiliation, and citation, between various\ntypes of entities, e.g., scholars, papers, and organizations. As part of the\nScholarly QALD Challenge, this paper presents a question-answering (QA) system\ncalled NLQxform, which provides an easy-to-use natural language interface to\nfacilitate accessing scholarly knowledge graphs. NLQxform allows users to\nexpress their complex query intentions in natural language questions. A\ntransformer-based language model, i.e., BART, is employed to translate\nquestions into standard SPARQL queries, which can be evaluated to retrieve the\nrequired information. According to the public leaderboard of the Scholarly QALD\nChallenge at ISWC 2023 (Task 1: DBLP-QUAD - Knowledge Graph Question Answering\nover DBLP), NLQxform achieved an F1 score of 0.85 and ranked first on the QA\ntask, demonstrating the competitiveness of the system.", "published": "2023-11-08 21:41:45", "link": "http://arxiv.org/abs/2311.07588v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Factors for Better Compositional Generalization", "abstract": "Recent diagnostic datasets on compositional generalization, such as SCAN\n(Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), expose severe problems\nin models trained from scratch on these datasets. However, in contrast to this\npoor performance, state-of-the-art models trained on larger and more general\ndatasets show better generalization ability. In this work, to reconcile this\ninconsistency, we conduct an empirical analysis by training Transformer models\non a variety of training sets with different data factors, including dataset\nscale, pattern complexity, example difficulty, etc. First, we show that\nincreased dataset complexity can lead to better generalization behavior on\nmultiple different generalization challenges. To further understand this\nimprovement, we show two axes of the benefit from more complex datasets: they\nprovide more diverse examples so compositional understanding becomes more\neffective, and they also prevent ungeneralizable memorization of the examples\ndue to reduced example repetition frequency. Finally, we explore how training\nexamples of different difficulty levels influence generalization differently.\nOn synthetic datasets, simple examples invoke stronger compositionality than\nhard examples do. On larger-scale real language datasets, while hard examples\nbecome more important potentially to ensure decent data coverage, a balanced\nmixture of simple and hard examples manages to induce the strongest\ngeneralizability. The code and data for this work are available at\nhttps://github.com/owenzx/data4comp", "published": "2023-11-08 01:27:34", "link": "http://arxiv.org/abs/2311.04420v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Twitter Sentiment Analysis of Covid Vacciness", "abstract": "In this paper, we look at a database of tweets sorted by various keywords\nthat could indicate the users sentiment towards covid vaccines. With social\nmedia becoming such a prevalent source of opinion, sorting and ranking tweets\nthat hold important information such as opinions on covid vaccines is of utmost\nimportance. Two different ranking scales were used, and ranking a tweet in this\nway could represent the difference between an opinion being lost and an opinion\nbeing featured on the site, which affects the decisions and behavior of people,\nand why researchers were interested in it. Using natural language processing\ntechniques, our aim is to determine and categorize opinions about covid\nvaccines with the highest accuracy possible.", "published": "2023-11-08 06:16:04", "link": "http://arxiv.org/abs/2311.04479v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CLearViD: Curriculum Learning for Video Description", "abstract": "Video description entails automatically generating coherent natural language\nsentences that narrate the content of a given video. We introduce CLearViD, a\ntransformer-based model for video description generation that leverages\ncurriculum learning to accomplish this task. In particular, we investigate two\ncurriculum strategies: (1) progressively exposing the model to more challenging\nsamples by gradually applying a Gaussian noise to the video data, and (2)\ngradually reducing the capacity of the network through dropout during the\ntraining process. These methods enable the model to learn more robust and\ngeneralizable features. Moreover, CLearViD leverages the Mish activation\nfunction, which provides non-linearity and non-monotonicity and helps alleviate\nthe issue of vanishing gradients. Our extensive experiments and ablation\nstudies demonstrate the effectiveness of the proposed model. The results on two\ndatasets, namely ActivityNet Captions and YouCook2, show that CLearViD\nsignificantly outperforms existing state-of-the-art models in terms of both\naccuracy and diversity metrics.", "published": "2023-11-08 06:20:32", "link": "http://arxiv.org/abs/2311.04480v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "NExT-Chat: An LMM for Chat, Detection and Segmentation", "abstract": "The development of large language models (LLMs) has greatly advanced the\nfield of multimodal understanding, leading to the emergence of large multimodal\nmodels (LMMs). In order to enhance the level of visual comprehension, recent\nstudies have equipped LMMs with region-level understanding capabilities by\nrepresenting object bounding box coordinates as a series of text sequences\n(pix2seq). In this paper, we introduce a novel paradigm for object location\nmodeling called pix2emb method, where we ask the LMM to output the location\nembeddings and then decode them with different decoders. This paradigm allows\nus to use different location formats (such as bounding boxes and masks) in\nmultimodal conversations. Leveraging the proposed pix2emb method, we train an\nLMM named NExT-Chat and demonstrate its capability of handling multiple tasks\nlike visual grounding, region captioning, and grounded reasoning. Comprehensive\nexperiments show the effectiveness of our NExT-Chat on various tasks, e.g.,\nNExT-Chat (87.7) vs. Shikra (86.9) on POPE-Random, NExT-Chat (68.9) vs. LISA\n(67.9) on referring expression segmentation task, and NExT-Chat (79.6) vs.\nKosmos-2 (62.3) on region caption task. The code and model are released at\nhttps://github.com/NExT-ChatV/NExT-Chat.", "published": "2023-11-08 07:15:05", "link": "http://arxiv.org/abs/2311.04498v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Loss Masking Is Not Needed in Decoder-only Transformer for\n  Discrete-token-based ASR", "abstract": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and\nAudioPaLM, have achieved remarkable performance on various speech tasks. These\nmodels discretize speech signals into tokens (speech discretization) and use a\nshared vocabulary for both text and speech tokens. Then they train a single\ndecoder-only Transformer on a mixture of speech tasks. However, these models\nrely on the Loss Masking strategy for the ASR task, which ignores the\ndependency among speech tokens. In this paper, we propose to model speech\ntokens in an autoregressive way, similar to text. We find that applying the\nconventional cross-entropy loss on input speech tokens does not consistently\nimprove the ASR performance over the Loss Masking approach. To address this\nissue, we propose a novel approach denoted Smoothed Label Distillation (SLD),\nwhich applies a KL divergence loss with smoothed labels on speech tokens. Our\nexperiments show that SLD effectively models speech tokens and outperforms Loss\nMasking for decoder-only Transformers in ASR tasks with different speech\ndiscretization methods. The source code can be found here:\nhttps://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld", "published": "2023-11-08 08:45:14", "link": "http://arxiv.org/abs/2311.04534v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "RankAug: Augmented data ranking for text classification", "abstract": "Research on data generation and augmentation has been focused majorly on\nenhancing generation models, leaving a notable gap in the exploration and\nrefinement of methods for evaluating synthetic data. There are several text\nsimilarity metrics within the context of generated data filtering which can\nimpact the performance of specific Natural Language Understanding (NLU) tasks,\nspecifically focusing on intent and sentiment classification. In this study, we\npropose RankAug, a text-ranking approach that detects and filters out the top\naugmented texts in terms of being most similar in meaning with lexical and\nsyntactical diversity. Through experiments conducted on multiple datasets, we\ndemonstrate that the judicious selection of filtering techniques can yield a\nsubstantial improvement of up to 35% in classification accuracy for\nunder-represented classes.", "published": "2023-11-08 08:47:49", "link": "http://arxiv.org/abs/2311.04535v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech language models lack important brain-relevant semantics", "abstract": "Despite known differences between reading and listening in the brain, recent\nwork has shown that text-based language models predict both text-evoked and\nspeech-evoked brain activity to an impressive degree. This poses the question\nof what types of information language models truly predict in the brain. We\ninvestigate this question via a direct approach, in which we systematically\nremove specific low-level stimulus features (textual, speech, and visual) from\nlanguage model representations to assess their impact on alignment with fMRI\nbrain recordings during reading and listening. Comparing these findings with\nspeech-based language models reveals starkly different effects of low-level\nfeatures on brain alignment. While text-based models show reduced alignment in\nearly sensory regions post-removal, they retain significant predictive power in\nlate language regions. In contrast, speech-based models maintain strong\nalignment in early auditory regions even after feature removal but lose all\npredictive power in late language regions. These results suggest that\nspeech-based models provide insights into additional information processed by\nearly auditory regions, but caution is needed when using them to model\nprocessing in late language regions. We make our code publicly available.\n[https://github.com/subbareddy248/speech-llm-brain]", "published": "2023-11-08 13:11:48", "link": "http://arxiv.org/abs/2311.04664v2", "categories": ["cs.CL", "cs.LG", "eess.AS", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Determination of toxic comments and unintended model bias minimization\n  using Deep learning approach", "abstract": "Online conversations can be toxic and subjected to threats, abuse, or\nharassment. To identify toxic text comments, several deep learning and machine\nlearning models have been proposed throughout the years. However, recent\nstudies demonstrate that because of the imbalances in the training data, some\nmodels are more likely to show unintended biases including gender bias and\nidentity bias. In this research, our aim is to detect toxic comment and reduce\nthe unintended bias concerning identity features such as race, gender, sex,\nreligion by fine-tuning an attention based model called BERT(Bidirectional\nEncoder Representation from Transformers). We apply weighted loss to address\nthe issue of unbalanced data and compare the performance of a fine-tuned BERT\nmodel with a traditional Logistic Regression model in terms of classification\nand bias minimization. The Logistic Regression model with the TFIDF vectorizer\nachieve 57.1% accuracy, and fine-tuned BERT model's accuracy is 89%. Code is\navailable at\nhttps://github.com/zim10/Determine_Toxic_comment_and_identity_bias.git", "published": "2023-11-08 16:10:28", "link": "http://arxiv.org/abs/2311.04789v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "SEMQA: Semi-Extractive Multi-Source Question Answering", "abstract": "Recently proposed long-form question answering (QA) systems, supported by\nlarge language models (LLMs), have shown promising capabilities. Yet,\nattributing and verifying their generated abstractive answers can be difficult,\nand automatically evaluating their accuracy remains an ongoing challenge.\n  In this work, we introduce a new QA task for answering multi-answer questions\nby summarizing multiple diverse sources in a semi-extractive fashion.\nSpecifically, Semi-extractive Multi-source QA (SEMQA) requires models to output\na comprehensive answer, while mixing factual quoted spans -- copied verbatim\nfrom given input sources -- and non-factual free-text connectors that glue\nthese spans together into a single cohesive passage. This setting bridges the\ngap between the outputs of well-grounded but constrained extractive QA systems\nand more fluent but harder to attribute fully abstractive answers.\nParticularly, it enables a new mode for language models that leverages their\nadvanced language generation capabilities, while also producing fine in-line\nattributions by-design that are easy to verify, interpret, and evaluate.\n  To study this task, we create the first dataset of this kind, QuoteSum, with\nhuman-written semi-extractive answers to natural and generated questions, and\ndefine text-based evaluation metrics. Experimenting with several LLMs in\nvarious settings, we find this task to be surprisingly challenging,\ndemonstrating the importance of QuoteSum for developing and studying such\nconsolidation capabilities.", "published": "2023-11-08 18:46:32", "link": "http://arxiv.org/abs/2311.04886v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language\n  Models", "abstract": "Large Language Models (LLMs) with billions of parameters are prime targets\nfor network pruning, removing some model weights without hurting performance.\nPrior approaches such as magnitude pruning, SparseGPT, and Wanda, either\nconcentrated solely on weights or integrated weights with activations for\nsparsity. However, they overlooked the informative gradients derived from\npretrained LLMs. In this paper, we present a novel sparsity-centric pruning\nmethod for pretrained LLMs, termed Gradient-based Language Model Pruner\n(GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor\nexpansion, operating in a training-free manner by harnessing properly\nnormalized gradients from a few calibration samples to determine the pruning\nmetric, and substantially outperforms competitive counterparts like SparseGPT\nand Wanda in multiple benchmarks. Intriguingly, by incorporating gradients,\nunstructured pruning with our method tends to reveal some structural patterns,\nwhich mirrors the geometric interdependence inherent in the LLMs' parameter\nstructure. Additionally, GBLM-Pruner functions without any subsequent\nretraining or weight updates to maintain its simplicity as other counterparts.\nExtensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show\nthat GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by\nsignificant margins. We further extend our approach on Vision Transformer. Our\ncode and models are available at https://github.com/VILA-Lab/GBLM-Pruner.", "published": "2023-11-08 18:59:54", "link": "http://arxiv.org/abs/2311.04902v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explained anomaly detection in text reviews: Can subjective scenarios be\n  correctly evaluated?", "abstract": "This paper presents a pipeline to detect and explain anomalous reviews in\nonline platforms. The pipeline is made up of three modules and allows the\ndetection of reviews that do not generate value for users due to either\nworthless or malicious composition. The classifications are accompanied by a\nnormality score and an explanation that justifies the decision made. The\npipeline's ability to solve the anomaly detection task was evaluated using\ndifferent datasets created from a large Amazon database. Additionally, a study\ncomparing three explainability techniques involving 241 participants was\nconducted to assess the explainability module. The study aimed to measure the\nimpact of explanations on the respondents' ability to reproduce the\nclassification model and their perceived usefulness. This work can be useful to\nautomate tasks in review online platforms, such as those for electronic\ncommerce, and offers inspiration for addressing similar problems in the field\nof anomaly detection in textual data. We also consider it interesting to have\ncarried out a human evaluation of the capacity of different explainability\ntechniques in a real and infrequent scenario such as the detection of anomalous\nreviews, as well as to reflect on whether it is possible to explain tasks as\nhumanly subjective as this one.", "published": "2023-11-08 11:51:47", "link": "http://arxiv.org/abs/2311.04948v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-shot Translation of Attention Patterns in VQA Models to Natural\n  Language", "abstract": "Converting a model's internals to text can yield human-understandable\ninsights about the model. Inspired by the recent success of training-free\napproaches for image captioning, we propose ZS-A2T, a zero-shot framework that\ntranslates the transformer attention of a given model into natural language\nwithout requiring any training. We consider this in the context of Visual\nQuestion Answering (VQA). ZS-A2T builds on a pre-trained large language model\n(LLM), which receives a task prompt, question, and predicted answer, as inputs.\nThe LLM is guided to select tokens which describe the regions in the input\nimage that the VQA model attended to. Crucially, we determine this similarity\nby exploiting the text-image matching capabilities of the underlying VQA model.\nOur framework does not require any training and allows the drop-in replacement\nof different guiding sources (e.g. attribution instead of attention maps), or\nlanguage models. We evaluate this novel task on textual explanation datasets\nfor VQA, giving state-of-the-art performances for the zero-shot setting on\nGQA-REX and VQA-X. Our code is available at:\nhttps://github.com/ExplainableML/ZS-A2T.", "published": "2023-11-08 22:18:53", "link": "http://arxiv.org/abs/2311.05043v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ADaPT: As-Needed Decomposition and Planning with Language Models", "abstract": "Large Language Models (LLMs) are increasingly being used for interactive\ndecision-making tasks requiring planning and adapting to the environment.\nRecent works employ LLMs-as-agents in broadly two ways: iteratively determining\nthe next action (iterative executors) or generating plans and executing\nsub-tasks using LLMs (plan-and-execute). However, these methods struggle with\ntask complexity, as the inability to execute any sub-task may lead to task\nfailure. To address these shortcomings, we introduce As-Needed Decomposition\nand Planning for complex Tasks (ADaPT), an approach that explicitly plans and\ndecomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute\nthem. ADaPT recursively decomposes sub-tasks to adapt to both task complexity\nand LLM capability. Our results demonstrate that ADaPT substantially\noutperforms established strong baselines, achieving success rates up to 28.3%\nhigher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel\ncompositional dataset that we introduce. Through extensive analysis, we\nillustrate the importance of multilevel decomposition and establish that ADaPT\ndynamically adjusts to the capabilities of the executor LLM as well as to task\ncomplexity.", "published": "2023-11-08 17:59:15", "link": "http://arxiv.org/abs/2311.05772v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or\n  \"What do I need to say so you agree 2+2=5?", "abstract": "We introduce and study the problem of adversarial arithmetic, which provides\na simple yet challenging testbed for language model alignment. This problem is\ncomprised of arithmetic questions posed in natural language, with an arbitrary\nadversarial string inserted before the question is complete. Even in the simple\nsetting of 1-digit addition problems, it is easy to find adversarial prompts\nthat make all tested models (including PaLM2, GPT4, Claude2) misbehave, and\neven to steer models to a particular wrong answer. We additionally provide a\nsimple algorithm for finding successful attacks by querying those same models,\nwhich we name \"prompt inversion rejection sampling\" (PIRS). We finally show\nthat models can be partially hardened against these attacks via reinforcement\nlearning and via agentic constitutional loops. However, we were not able to\nmake a language model fully robust against adversarial arithmetic attacks.", "published": "2023-11-08 19:07:10", "link": "http://arxiv.org/abs/2311.07587v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selective HuBERT: Self-Supervised Pre-Training for Target Speaker in\n  Clean and Mixture Speech", "abstract": "Self-supervised pre-trained speech models were shown effective for various\ndownstream speech processing tasks. Since they are mainly pre-trained to map\ninput speech to pseudo-labels, the resulting representations are only effective\nfor the type of pre-train data used, either clean or mixture speech. With the\nidea of selective auditory attention, we propose a novel pre-training solution\ncalled Selective-HuBERT, or SHuBERT, which learns the selective extraction of\ntarget speech representations from either clean or mixture speech.\nSpecifically, SHuBERT is trained to predict pseudo labels of a target speaker,\nconditioned on an enrolled speech from the target speaker. By doing so, SHuBERT\nis expected to selectively attend to the target speaker in a complex acoustic\nenvironment, thus benefiting various downstream tasks. We further introduce a\ndual-path training strategy and use the cross-correlation constraint between\nthe two branches to encourage the model to generate noise-invariant\nrepresentation. Experiments on SUPERB benchmark and LibriMix dataset\ndemonstrate the universality and noise-robustness of SHuBERT. Furthermore, we\nfind that our high-quality representation can be easily integrated with\nconventional supervised learning methods to achieve significant performance,\neven under extremely low-resource labeled data.", "published": "2023-11-08 08:28:25", "link": "http://arxiv.org/abs/2311.04526v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "1SPU: 1-step Speech Processing Unit", "abstract": "Recent studies have made some progress in refining end-to-end (E2E) speech\nrecognition encoders by applying Connectionist Temporal Classification (CTC)\nloss to enhance named entity recognition within transcriptions. However, these\nmethods have been constrained by their exclusive use of the ASCII character\nset, allowing only a limited array of semantic labels. We propose 1SPU, a\n1-step Speech Processing Unit which can recognize speech events (e.g: speaker\nchange) or an NL event (Intent, Emotion) while also transcribing vocal content.\nIt extends the E2E automatic speech recognition (ASR) system's vocabulary by\nadding a set of unused placeholder symbols, conceptually akin to the <pad>\ntokens used in sequence modeling. These placeholders are then assigned to\nrepresent semantic events (in form of tags) and are integrated into the\ntranscription process as distinct tokens.\n  We demonstrate notable improvements on the SLUE benchmark and yields results\nthat are on par with those for the SLURP dataset. Additionally, we provide a\nvisual analysis of the system's proficiency in accurately pinpointing\nmeaningful tokens over time, illustrating the enhancement in transcription\nquality through the utilization of supplementary semantic tags.", "published": "2023-11-08 15:28:18", "link": "http://arxiv.org/abs/2311.04753v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "GPU-Accelerated WFST Beam Search Decoder for CTC-based Speech\n  Recognition", "abstract": "While Connectionist Temporal Classification (CTC) models deliver\nstate-of-the-art accuracy in automated speech recognition (ASR) pipelines,\ntheir performance has been limited by CPU-based beam search decoding. We\nintroduce a GPU-accelerated Weighted Finite State Transducer (WFST) beam search\ndecoder compatible with current CTC models. It increases pipeline throughput\nand decreases latency, supports streaming inference, and also supports advanced\nfeatures like utterance-specific word boosting via on-the-fly composition. We\nprovide pre-built DLPack-based python bindings for ease of use with\nPython-based machine learning frameworks at\nhttps://github.com/nvidia-riva/riva-asrlib-decoder. We evaluated our decoder\nfor offline and online scenarios, demonstrating that it is the fastest beam\nsearch decoder for CTC models. In the offline scenario it achieves up to 7\ntimes more throughput than the current state-of-the-art CPU decoder and in the\nonline streaming scenario, it achieves nearly 8 times lower latency, with same\nor better word error rate.", "published": "2023-11-08 19:57:10", "link": "http://arxiv.org/abs/2311.04996v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust\n  Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation", "abstract": "Although voice conversion (VC) systems have shown a remarkable ability to\ntransfer voice style, existing methods still have an inaccurate pitch and low\nspeaker adaptation quality. To address these challenges, we introduce\nDiff-HierVC, a hierarchical VC system based on two diffusion models. We first\nintroduce DiffPitch, which can effectively generate F0 with the target voice\nstyle. Subsequently, the generated F0 is fed to DiffVoice to convert the speech\nwith a target voice style. Furthermore, using the source-filter encoder, we\ndisentangle the speech and use the converted Mel-spectrogram as a data-driven\nprior in DiffVoice to improve the voice style transfer capacity. Finally, by\nusing the masked prior in diffusion models, our model can improve the speaker\nadaptation quality. Experimental results verify the superiority of our model in\npitch generation and voice style transfer performance, and our model also\nachieves a CER of 0.83% and EER of 3.29% in zero-shot VC scenarios.", "published": "2023-11-08 14:02:53", "link": "http://arxiv.org/abs/2311.04693v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Auto deep learning for bioacoustic signals", "abstract": "This study investigates the potential of automated deep learning to enhance\nthe accuracy and efficiency of multi-class classification of bird\nvocalizations, compared against traditional manually-designed deep learning\nmodels. Using the Western Mediterranean Wetland Birds dataset, we investigated\nthe use of AutoKeras, an automated machine learning framework, to automate\nneural architecture search and hyperparameter tuning. Comparative analysis\nvalidates our hypothesis that the AutoKeras-derived model consistently\noutperforms traditional models like MobileNet, ResNet50 and VGG16. Our approach\nand findings underscore the transformative potential of automated deep learning\nfor advancing bioacoustics research and models. In fact, the automated\ntechniques eliminate the need for manual feature engineering and model design\nwhile improving performance. This study illuminates best practices in sampling,\nevaluation and reporting to enhance reproducibility in this nascent field. All\nthe code used is available at https:\n//github.com/giuliotosato/AutoKeras-bioacustic\n  Keywords: AutoKeras; automated deep learning; audio classification; Wetlands\nBird dataset; comparative analysis; bioacoustics; validation dataset;\nmulti-class classification; spectrograms.", "published": "2023-11-08 07:22:39", "link": "http://arxiv.org/abs/2311.04945v2", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Synthetic Speaking Children -- Why We Need Them and How to Make Them", "abstract": "Contemporary Human Computer Interaction (HCI) research relies primarily on\nneural network models for machine vision and speech understanding of a system\nuser. Such models require extensively annotated training datasets for optimal\nperformance and when building interfaces for users from a vulnerable population\nsuch as young children, GDPR introduces significant complexities in data\ncollection, management, and processing. Motivated by the training needs of an\nEdge AI smart toy platform this research explores the latest advances in\ngenerative neural technologies and provides a working proof of concept of a\ncontrollable data generation pipeline for speech driven facial training data at\nscale. In this context, we demonstrate how StyleGAN2 can be finetuned to create\na gender balanced dataset of children's faces. This dataset includes a variety\nof controllable factors such as facial expressions, age variations, facial\nposes, and even speech-driven animations with realistic lip synchronization. By\ncombining generative text to speech models for child voice synthesis and a 3D\nlandmark based talking heads pipeline, we can generate highly realistic,\nentirely synthetic, talking child video clips. These video clips can provide\nvaluable, and controllable, synthetic training data for neural network models,\nbridging the gap when real data is scarce or restricted due to privacy\nregulations.", "published": "2023-11-08 22:58:22", "link": "http://arxiv.org/abs/2311.06307v1", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
