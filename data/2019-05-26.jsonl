{"title": "SemBleu: A Robust Metric for AMR Parsing Evaluation", "abstract": "Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs. The\nmajor evaluation metric, SMATCH (Cai and Knight, 2013), searches for one-to-one\nmappings between the nodes of two AMRs with a greedy hill-climbing algorithm,\nwhich leads to search errors. We propose SEMBLEU, a robust metric that extends\nBLEU (Papineni et al., 2002) to AMRs. It does not suffer from search errors and\nconsiders non-local correspondences in addition to local ones. SEMBLEU is fully\ncontent-driven and punishes situations where a system's output does not\npreserve most information from the input. Preliminary experiments on both\nsentence and corpus levels show that SEMBLEU has slightly higher consistency\nwith human judgments than SMATCH. Our code is available at\nhttp://github.com/freesunshine0316/sembleu.", "published": "2019-05-26 04:49:29", "link": "http://arxiv.org/abs/1905.10726v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TIGS: An Inference Algorithm for Text Infilling with Gradient Search", "abstract": "Text infilling is defined as a task for filling in the missing part of a\nsentence or paragraph, which is suitable for many real-world natural language\ngeneration scenarios. However, given a well-trained sequential generative\nmodel, generating missing symbols conditioned on the context is challenging for\nexisting greedy approximate inference algorithms. In this paper, we propose an\niterative inference algorithm based on gradient search, which is the first\ninference algorithm that can be broadly applied to any neural sequence\ngenerative models for text infilling tasks. We compare the proposed method with\nstrong baselines on three text infilling tasks with various mask ratios and\ndifferent mask strategies. The results show that our proposed method is\neffective and efficient for fill-in-the-blank tasks, consistently outperforming\nall baselines.", "published": "2019-05-26 07:45:44", "link": "http://arxiv.org/abs/1905.10752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of basic modules for isolated spelling error correction in\n  Polish texts", "abstract": "Spelling error correction is an important problem in natural language\nprocessing, as a prerequisite for good performance in downstream tasks as well\nas an important feature in user-facing applications. For texts in Polish\nlanguage, there exist works on specific error correction solutions, often\ndeveloped for dealing with specialized corpora, but not evaluations of many\ndifferent approaches on big resources of errors. We begin to address this\nproblem by testing some basic and promising methods on PlEWi, a corpus of\nannotated spelling extracted from Polish Wikipedia. These modules may be\nfurther combined with appropriate solutions for error detection and context\nawareness. Following our results, combining edit distance with cosine distance\nof semantic vectors may be suggested for interpretable systems, while an LSTM,\nparticularly enhanced by ELMo embeddings, seems to offer the best raw\nperformance.", "published": "2019-05-26 14:54:52", "link": "http://arxiv.org/abs/1905.10810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Where's My Head? Definition, Dataset and Models for Numeric Fused-Heads\n  Identification and Resolution", "abstract": "We provide the first computational treatment of fused-heads constructions\n(FH), focusing on the numeric fused-heads (NFH). FHs constructions are noun\nphrases (NPs) in which the head noun is missing and is said to be `fused' with\nits dependent modifier. This missing information is implicit and is important\nfor sentence understanding. The missing references are easily filled in by\nhumans but pose a challenge for computational models. We formulate the handling\nof FH as a two stages process: identification of the FH construction and\nresolution of the missing head. We explore the NFH phenomena in large corpora\nof English text and create (1) a dataset and a highly accurate method for NFH\nidentification; (2) a 10k examples (1M tokens) crowd-sourced dataset of NFH\nresolution; and (3) a neural baseline for the NFH resolution task. We release\nour code and dataset, in hope to foster further research into this challenging\nproblem.", "published": "2019-05-26 21:35:21", "link": "http://arxiv.org/abs/1905.10886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extreme Multi-Label Legal Text Classification: A case study in EU\n  Legislation", "abstract": "We consider the task of Extreme Multi-Label Text Classification (XMTC) in the\nlegal domain. We release a new dataset of 57k legislative documents from\nEURLEX, the European Union's public document database, annotated with concepts\nfrom EUROVOC, a multidisciplinary thesaurus. The dataset is substantially\nlarger than previous EURLEX datasets and suitable for XMTC, few-shot and\nzero-shot learning. Experimenting with several neural classifiers, we show that\nBIGRUs with self-attention outperform the current multi-label state-of-the-art\nmethods, which employ label-wise attention. Replacing CNNs with BIGRUs in\nlabel-wise attention networks leads to the best overall performance.", "published": "2019-05-26 21:50:15", "link": "http://arxiv.org/abs/1905.10892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hashing based Answer Selection", "abstract": "Answer selection is an important subtask of question answering (QA), where\ndeep models usually achieve better performance. Most deep models adopt\nquestion-answer interaction mechanisms, such as attention, to get vector\nrepresentations for answers. When these interaction based deep models are\ndeployed for online prediction, the representations of all answers need to be\nrecalculated for each question. This procedure is time-consuming for deep\nmodels with complex encoders like BERT which usually have better accuracy than\nsimple encoders. One possible solution is to store the matrix representation\n(encoder output) of each answer in memory to avoid recalculation. But this will\nbring large memory cost. In this paper, we propose a novel method, called\nhashing based answer selection (HAS), to tackle this problem. HAS adopts a\nhashing strategy to learn a binary matrix representation for each answer, which\ncan dramatically reduce the memory cost for storing the matrix representations\nof answers. Hence, HAS can adopt complex encoders like BERT in the model, but\nthe online prediction of HAS is still fast with a low memory cost. Experimental\nresults on three popular answer selection datasets show that HAS can outperform\nexisting models to achieve state-of-the-art performance.", "published": "2019-05-26 03:33:42", "link": "http://arxiv.org/abs/1905.10718v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Gated Group Self-Attention for Answer Selection", "abstract": "Answer selection (answer ranking) is one of the key steps in many kinds of\nquestion answering (QA) applications, where deep models have achieved\nstate-of-the-art performance. Among these deep models, recurrent neural network\n(RNN) based models are most popular, typically with better performance than\nconvolutional neural network (CNN) based models. Nevertheless, it is difficult\nfor RNN based models to capture the information about long-range dependency\namong words in the sentences of questions and answers. In this paper, we\npropose a new deep model, called gated group self-attention (GGSA), for answer\nselection. GGSA is inspired by global self-attention which is originally\nproposed for machine translation and has not been explored in answer selection.\nGGSA tackles the problem of global self-attention that local and global\ninformation cannot be well distinguished. Furthermore, an interaction mechanism\nbetween questions and answers is also proposed to enhance GGSA by a residual\nstructure. Experimental results on two popular QA datasets show that GGSA can\noutperform existing answer selection models to achieve state-of-the-art\nperformance. Furthermore, GGSA can also achieve higher accuracy than global\nself-attention for the answer selection task, with a lower computation cost.", "published": "2019-05-26 03:40:17", "link": "http://arxiv.org/abs/1905.10720v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "When to reply? Context Sensitive Models to Predict Instructor\n  Interventions in MOOC Forums", "abstract": "Due to time constraints, course instructors often need to selectively\nparticipate in student discussion threads, due to their limited bandwidth and\nlopsided student--instructor ratio on online forums. We propose the first deep\nlearning models for this binary prediction problem. We propose novel attention\nbased models to infer the amount of latent context necessary to predict\ninstructor intervention. Such models also allow themselves to be tuned to\ninstructor's preference to intervene early or late. Our three proposed\nattentive model variants to infer the latent context improve over the\nstate-of-the-art by a significant, large margin of 11% in F1 and 10% in recall,\non average. Further, introspection of attention help us better understand what\naspects of a discussion post propagate through the discussion thread that\nprompts instructor intervention.", "published": "2019-05-26 18:40:06", "link": "http://arxiv.org/abs/1905.10851v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Path Ranking with Attention to Type Hierarchies", "abstract": "The objective of the knowledge base completion problem is to infer missing\ninformation from existing facts in a knowledge base. Prior work has\ndemonstrated the effectiveness of path-ranking based methods, which solve the\nproblem by discovering observable patterns in knowledge graphs, consisting of\nnodes representing entities and edges representing relations. However, these\npatterns either lack accuracy because they rely solely on relations or cannot\neasily generalize due to the direct use of specific entity information. We\nintroduce Attentive Path Ranking, a novel path pattern representation that\nleverages type hierarchies of entities to both avoid ambiguity and maintain\ngeneralization. Then, we present an end-to-end trained attention-based RNN\nmodel to discover the new path patterns from data. Experiments conducted on\nbenchmark knowledge base completion datasets WN18RR and FB15k-237 demonstrate\nthat the proposed model outperforms existing methods on the fact prediction\ntask by statistically significant margins of 26% and 10%, respectively.\nFurthermore, quantitative and qualitative analyses show that the path patterns\nbalance between generalization and discrimination.", "published": "2019-05-26 12:57:47", "link": "http://arxiv.org/abs/1905.10799v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Hyperbolic Interaction Model For Hierarchical Multi-Label Classification", "abstract": "Different from the traditional classification tasks which assume mutual\nexclusion of labels, hierarchical multi-label classification (HMLC) aims to\nassign multiple labels to every instance with the labels organized under\nhierarchical relations. Besides the labels, since linguistic ontologies are\nintrinsic hierarchies, the conceptual relations between words can also form\nhierarchical structures. Thus it can be a challenge to learn mappings from word\nhierarchies to label hierarchies. We propose to model the word and label\nhierarchies by embedding them jointly in the hyperbolic space. The main reason\nis that the tree-likeness of the hyperbolic space matches the complexity of\nsymbolic data with hierarchical structures. A new Hyperbolic Interaction Model\n(HyperIM) is designed to learn the label-aware document representations and\nmake predictions for HMLC. Extensive experiments are conducted on three\nbenchmark datasets. The results have demonstrated that the new model can\nrealistically capture the complex data structures and further improve the\nperformance for HMLC comparing with the state-of-the-art methods. To facilitate\nfuture research, our code is publicly available.", "published": "2019-05-26 13:20:11", "link": "http://arxiv.org/abs/1905.10802v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Simple and Effective Curriculum Pointer-Generator Networks for Reading\n  Comprehension over Long Narratives", "abstract": "This paper tackles the problem of reading comprehension over long narratives\nwhere documents easily span over thousands of tokens. We propose a curriculum\nlearning (CL) based Pointer-Generator framework for reading/sampling over large\ndocuments, enabling diverse training of the neural model based on the notion of\nalternating contextual difficulty. This can be interpreted as a form of domain\nrandomization and/or generative pretraining during training. To this end, the\nusage of the Pointer-Generator softens the requirement of having the answer\nwithin the context, enabling us to construct diverse training samples for\nlearning. Additionally, we propose a new Introspective Alignment Layer (IAL),\nwhich reasons over decomposed alignments using block-based self-attention. We\nevaluate our proposed method on the NarrativeQA reading comprehension\nbenchmark, achieving state-of-the-art performance, improving existing baselines\nby $51\\%$ relative improvement on BLEU-4 and $17\\%$ relative improvement on\nRouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and\nCL components.", "published": "2019-05-26 17:56:11", "link": "http://arxiv.org/abs/1905.10847v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "TACAM: Topic And Context Aware Argument Mining", "abstract": "In this work we address the problem of argument search. The purpose of\nargument search is the distillation of pro and contra arguments for requested\ntopics from large text corpora. In previous works, the usual approach is to use\na standard search engine to extract text parts which are relevant to the given\ntopic and subsequently use an argument recognition algorithm to select\narguments from them. The main challenge in the argument recognition task, which\nis also known as argument mining, is that often sentences containing arguments\nare structurally similar to purely informative sentences without any stance\nabout the topic. In fact, they only differ semantically. Most approaches use\ntopic or search term information only for the first search step and therefore\nassume that arguments can be classified independently of a topic. We argue that\ntopic information is crucial for argument mining, since the topic defines the\nsemantic context of an argument. Precisely, we propose different models for the\nclassification of arguments, which take information about a topic of an\nargument into account. Moreover, to enrich the context of a topic and to let\nmodels understand the context of the potential argument better, we integrate\ninformation from different external sources such as Knowledge Graphs or\npre-trained NLP models. Our evaluation shows that considering topic\ninformation, especially in connection with external information, provides a\nsignificant performance boost for the argument mining task.", "published": "2019-05-26 07:06:58", "link": "http://arxiv.org/abs/1906.00923v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Auditory Separation of a Conversation from Background via Attentional\n  Gating", "abstract": "We present a model for separating a set of voices out of a sound mixture\ncontaining an unknown number of sources. Our Attentional Gating Network (AGN)\nuses a variable attentional context to specify which speakers in the mixture\nare of interest. The attentional context is specified by an embedding vector\nwhich modifies the processing of a neural network through an additive bias.\nIndividual speaker embeddings are learned to separate a single speaker while\nsuperpositions of the individual speaker embeddings are used to separate sets\nof speakers. We first evaluate AGN on a traditional single speaker separation\ntask and show an improvement of 9% with respect to comparable models. Then, we\nintroduce a new task to separate an arbitrary subset of voices from a mixture\nof an unknown-sized set of voices, inspired by the human ability to separate a\nconversation of interest from background chatter at a cafeteria. We show that\nAGN is the only model capable of solving this task, performing only 7% worse\nthan on the single speaker separation task.", "published": "2019-05-26 07:38:35", "link": "http://arxiv.org/abs/1905.10751v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
