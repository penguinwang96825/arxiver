{"title": "AARGH! End-to-end Retrieval-Generation for Task-Oriented Dialog", "abstract": "We introduce AARGH, an end-to-end task-oriented dialog system combining\nretrieval and generative approaches in a single model, aiming at improving\ndialog management and lexical diversity of outputs. The model features a new\nresponse selection method based on an action-aware training objective and a\nsimplified single-encoder retrieval architecture which allow us to build an\nend-to-end retrieval-enhanced generation model where retrieval and generation\nshare most of the parameters. On the MultiWOZ dataset, we show that our\napproach produces more diverse outputs while maintaining or improving state\ntracking and context-to-response generation performance, compared to\nstate-of-the-art baselines.", "published": "2022-09-08 08:15:22", "link": "http://arxiv.org/abs/2209.03632v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Gender Debiasing of Pre-trained Indic Language Models", "abstract": "The gender bias present in the data on which language models are pre-trained\ngets reflected in the systems that use these models. The model's intrinsic\ngender bias shows an outdated and unequal view of women in our culture and\nencourages discrimination. Therefore, in order to establish more equitable\nsystems and increase fairness, it is crucial to identify and mitigate the bias\nexisting in these models. While there is a significant amount of work in this\narea in English, there is a dearth of research being done in other gendered and\nlow resources languages, particularly the Indian languages. English is a\nnon-gendered language, where it has genderless nouns. The methodologies for\nbias detection in English cannot be directly deployed in other gendered\nlanguages, where the syntax and semantics vary. In our paper, we measure gender\nbias associated with occupations in Hindi language models. Our major\ncontributions in this paper are the construction of a novel corpus to evaluate\noccupational gender bias in Hindi, quantify this existing bias in these systems\nusing a well-defined metric, and mitigate it by efficiently fine-tuning our\nmodel. Our results reflect that the bias is reduced post-introduction of our\nproposed mitigation techniques. Our codebase is available publicly.", "published": "2022-09-08 09:15:58", "link": "http://arxiv.org/abs/2209.03661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visual Grounding of Inter-lingual Word-Embeddings", "abstract": "Visual grounding of Language aims at enriching textual representations of\nlanguage with multiple sources of visual knowledge such as images and videos.\nAlthough visual grounding is an area of intense research, inter-lingual aspects\nof visual grounding have not received much attention. The present study\ninvestigates the inter-lingual visual grounding of word embeddings. We propose\nan implicit alignment technique between the two spaces of vision and language\nin which inter-lingual textual information interacts in order to enrich\npre-trained textual word embeddings. We focus on three languages in our\nexperiments, namely, English, Arabic, and German. We obtained visually grounded\nvector representations for these languages and studied whether visual grounding\non one or multiple languages improved the performance of embeddings on word\nsimilarity and categorization benchmarks. Our experiments suggest that\ninter-lingual knowledge improves the performance of grounded embeddings in\nsimilar languages such as German and English. However, inter-lingual grounding\nof German or English with Arabic led to a slight degradation in performance on\nword similarity benchmarks. On the other hand, we observed an opposite trend on\ncategorization benchmarks where Arabic had the most improvement on English. In\nthe discussion section, several reasons for those findings are laid out. We\nhope that our experiments provide a baseline for further research on\ninter-lingual visual grounding.", "published": "2022-09-08 11:18:39", "link": "http://arxiv.org/abs/2209.03714v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-Training a Graph Recurrent Network for Language Representation", "abstract": "Transformer-based pre-trained models have gained much advance in recent\nyears, becoming one of the most important backbones in natural language\nprocessing. Recent work shows that the attention mechanism inside Transformer\nmay not be necessary, both convolutional neural networks and multi-layer\nperceptron based models have also been investigated as Transformer\nalternatives. In this paper, we consider a graph recurrent network for language\nmodel pre-training, which builds a graph structure for each sequence with local\ntoken-level communications, together with a sentence-level representation\ndecoupled from other tokens. The original model performs well in\ndomain-specific text classification under supervised training, however, its\npotential in learning transfer knowledge by self-supervised way has not been\nfully exploited. We fill this gap by optimizing the architecture and verifying\nits effectiveness in more general language understanding tasks, for both\nEnglish and Chinese languages. As for model efficiency, instead of the\nquadratic complexity in Transformer-based models, our model has linear\ncomplexity and performs more efficiently during inference. Moreover, we find\nthat our model can generate more diverse outputs with less contextualized\nfeature redundancy than existing attention-based models.", "published": "2022-09-08 14:12:15", "link": "http://arxiv.org/abs/2209.03834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Transformer Language Model for Speech Recognition in\n  Low-resource Languages", "abstract": "It is challenging to train and deploy Transformer LMs for hybrid speech\nrecognition 2nd pass re-ranking in low-resource languages due to (1) data\nscarcity in low-resource languages, (2) expensive computing costs for training\nand refreshing 100+ monolingual models, and (3) hosting inefficiency\nconsidering sparse traffic. In this study, we present a new way to group\nmultiple low-resource locales together and optimize the performance of\nMultilingual Transformer LMs in ASR. Our Locale-group Multilingual Transformer\nLMs outperform traditional multilingual LMs along with reducing maintenance\ncosts and operating expenses. Further, for low-resource but high-traffic\nlocales where deploying monolingual models is feasible, we show that\nfine-tuning our locale-group multilingual LMs produces better monolingual LM\ncandidates than baseline monolingual LMs.", "published": "2022-09-08 21:40:41", "link": "http://arxiv.org/abs/2209.04041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extractive is not Faithful: An Investigation of Broad Unfaithfulness\n  Problems in Extractive Summarization", "abstract": "The problems of unfaithful summaries have been widely discussed under the\ncontext of abstractive summarization. Though extractive summarization is less\nprone to the common unfaithfulness issues of abstractive summaries, does that\nmean extractive is equal to faithful? Turns out that the answer is no. In this\nwork, we define a typology with five types of broad unfaithfulness problems\n(including and beyond not-entailment) that can appear in extractive summaries,\nincluding incorrect coreference, incomplete coreference, incorrect discourse,\nincomplete discourse, as well as other misleading information. We ask humans to\nlabel these problems out of 1600 English summaries produced by 16 diverse\nextractive systems. We find that 30% of the summaries have at least one of the\nfive issues. To automatically detect these problems, we find that 5 existing\nfaithfulness evaluation metrics for summarization have poor correlations with\nhuman judgment. To remedy this, we propose a new metric, ExtEval, that is\ndesigned for detecting unfaithful extractive summaries and is shown to have the\nbest performance. We hope our work can increase the awareness of unfaithfulness\nproblems in extractive summarization and help future work to evaluate and\nresolve these issues. Our data and code are publicly available at\nhttps://github.com/ZhangShiyue/extractive_is_not_faithful", "published": "2022-09-08 03:25:18", "link": "http://arxiv.org/abs/2209.03549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Based Template Machine Translation In Low-Resource Setting", "abstract": "Incorporating tagging into neural machine translation (NMT) systems has shown\npromising results in helping translate rare words such as named entities (NE).\nHowever, translating NE in low-resource setting remains a challenge. In this\nwork, we investigate the effect of using tags and NE hypernyms from knowledge\ngraphs (KGs) in parallel corpus in different levels of resource conditions. We\nfind the tag-and-copy mechanism (tag the NEs in the source sentence and copy\nthem to the target sentence) improves translation in high-resource settings\nonly. Introducing copying also results in polarizing effects in translating\ndifferent parts-of-speech (POS). Interestingly, we find that copy accuracy for\nhypernyms is consistently higher than that of entities. As a way of avoiding\n\"hard\" copying and utilizing hypernym in bootstrapping rare entities, we\nintroduced a \"soft\" tagging mechanism and found consistent improvement in high\nand low-resource settings.", "published": "2022-09-08 04:15:16", "link": "http://arxiv.org/abs/2209.03554v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Attention Mechanism Possess the Feature of Human Reading? A\n  Perspective of Sentiment Classification Task", "abstract": "[Purpose] To understand the meaning of a sentence, humans can focus on\nimportant words in the sentence, which reflects our eyes staying on each word\nin different gaze time or times. Thus, some studies utilize eye-tracking values\nto optimize the attention mechanism in deep learning models. But these studies\nlack to explain the rationality of this approach. Whether the attention\nmechanism possesses this feature of human reading needs to be explored.\n[Design/methodology/approach] We conducted experiments on a sentiment\nclassification task. Firstly, we obtained eye-tracking values from two\nopen-source eye-tracking corpora to describe the feature of human reading.\nThen, the machine attention values of each sentence were learned from a\nsentiment classification model. Finally, a comparison was conducted to analyze\nmachine attention values and eye-tracking values. [Findings] Through\nexperiments, we found the attention mechanism can focus on important words,\nsuch as adjectives, adverbs, and sentiment words, which are valuable for\njudging the sentiment of sentences on the sentiment classification task. It\npossesses the feature of human reading, focusing on important words in\nsentences when reading. Due to the insufficient learning of the attention\nmechanism, some words are wrongly focused. The eye-tracking values can help the\nattention mechanism correct this error and improve the model performance.\n[Originality/value] Our research not only provides a reasonable explanation for\nthe study of using eye-tracking values to optimize the attention mechanism, but\nalso provides new inspiration for the interpretability of attention mechanism.", "published": "2022-09-08 04:19:48", "link": "http://arxiv.org/abs/2209.03557v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Review on Method Entities in the Academic Literature: Extraction,\n  Evaluation, and Application", "abstract": "In scientific research, the method is an indispensable means to solve\nscientific problems and a critical research object. With the advancement of\nsciences, many scientific methods are being proposed, modified, and used in\nacademic literature. The authors describe details of the method in the abstract\nand body text, and key entities in academic literature reflecting names of the\nmethod are called method entities. Exploring diverse method entities in a\ntremendous amount of academic literature helps scholars understand existing\nmethods, select the appropriate method for research tasks, and propose new\nmethods. Furthermore, the evolution of method entities can reveal the\ndevelopment of a discipline and facilitate knowledge discovery. Therefore, this\narticle offers a systematic review of methodological and empirical works\nfocusing on extracting method entities from full-text academic literature and\nefforts to build knowledge services using these extracted method entities.\nDefinitions of key concepts involved in this review were first proposed. Based\non these definitions, we systematically reviewed the approaches and indicators\nto extract and evaluate method entities, with a strong focus on the pros and\ncons of each approach. We also surveyed how extracted method entities are used\nto build new applications. Finally, limitations in existing works as well as\npotential next steps were discussed.", "published": "2022-09-08 10:12:21", "link": "http://arxiv.org/abs/2209.03687v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Exploring the Distribution Regularities of User Attention and Sentiment\n  toward Product Aspects in Online Reviews", "abstract": "[Purpose] To better understand the online reviews and help potential\nconsumers, businessmen, and product manufacturers effectively obtain users'\nevaluation on product aspects, this paper explores the distribution\nregularities of user attention and sentiment toward product aspects from the\ntemporal perspective of online reviews. [Design/methodology/approach] Temporal\ncharacteristics of online reviews (purchase time, review time, and time\nintervals between purchase time and review time), similar attributes\nclustering, and attribute-level sentiment computing technologies are employed\nbased on more than 340k smartphone reviews of three products from JD.COM (a\nfamous online shopping platform in China) to explore the distribution\nregularities of user attention and sentiment toward product aspects in this\narticle. [Findings] The empirical results show that a power-law distribution\ncan fit user attention to product aspects, and the reviews posted in short time\nintervals contain more product aspects. Besides, the results show that the\nvalues of user sentiment of product aspects are significantly higher/lower in\nshort time intervals which contribute to judging the advantages and weaknesses\nof a product. [Research limitations] The paper can't acquire online reviews for\nmore products with temporal characteristics to verify the findings because of\nthe restriction on reviews crawling by the shopping platforms.\n[Originality/value] This work reveals the distribution regularities of user\nattention and sentiment toward product aspects, which is of great significance\nin assisting decision-making, optimizing review presentation, and improving the\nshopping experience.", "published": "2022-09-08 10:23:16", "link": "http://arxiv.org/abs/2209.03690v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards explainable evaluation of language models on the semantic\n  similarity of visual concepts", "abstract": "Recent breakthroughs in NLP research, such as the advent of Transformer\nmodels have indisputably contributed to major advancements in several tasks.\nHowever, few works research robustness and explainability issues of their\nevaluation strategies. In this work, we examine the behavior of high-performing\npre-trained language models, focusing on the task of semantic similarity for\nvisual vocabularies. First, we address the need for explainable evaluation\nmetrics, necessary for understanding the conceptual quality of retrieved\ninstances. Our proposed metrics provide valuable insights in local and global\nlevel, showcasing the inabilities of widely used approaches. Secondly,\nadversarial interventions on salient query semantics expose vulnerabilities of\nopaque metrics and highlight patterns in learned linguistic representations.", "published": "2022-09-08 11:40:57", "link": "http://arxiv.org/abs/2209.03723v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IDIAPers @ Causal News Corpus 2022: Extracting Cause-Effect-Signal\n  Triplets via Pre-trained Autoregressive Language Model", "abstract": "In this paper, we describe our shared task submissions for Subtask 2 in\nCASE-2022, Event Causality Identification with Casual News Corpus. The\nchallenge focused on the automatic detection of all cause-effect-signal spans\npresent in the sentence from news-media. We detect cause-effect-signal spans in\na sentence using T5 -- a pre-trained autoregressive language model. We\niteratively identify all cause-effect-signal span triplets, always conditioning\nthe prediction of the next triplet on the previously predicted ones. To predict\nthe triplet itself, we consider different causal relationships such as\ncause$\\rightarrow$effect$\\rightarrow$signal. Each triplet component is\ngenerated via a language model conditioned on the sentence, the previous parts\nof the current triplet, and previously predicted triplets. Despite training on\nan extremely small dataset of 160 samples, our approach achieved competitive\nperformance, being placed second in the competition. Furthermore, we show that\nassuming either cause$\\rightarrow$effect or effect$\\rightarrow$cause order\nachieves similar results.", "published": "2022-09-08 15:54:25", "link": "http://arxiv.org/abs/2209.03891v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLaCLab at SocialDisNER: Using Medical Gazetteers for Named-Entity\n  Recognition of Disease Mentions in Spanish Tweets", "abstract": "This paper summarizes the CLaC submission for SMM4H 2022 Task 10 which\nconcerns the recognition of diseases mentioned in Spanish tweets. Before\nclassifying each token, we encode each token with a transformer encoder using\nfeatures from Multilingual RoBERTa Large, UMLS gazetteer, and DISTEMIST\ngazetteer, among others. We obtain a strict F1 score of 0.869, with competition\nmean of 0.675, standard deviation of 0.245, and median of 0.761.", "published": "2022-09-08 02:08:51", "link": "http://arxiv.org/abs/2209.03528v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Geolocation of Cultural Heritage using Multi-View Knowledge Graph\n  Embedding", "abstract": "Knowledge Graphs (KGs) have proven to be a reliable way of structuring data.\nThey can provide a rich source of contextual information about cultural\nheritage collections. However, cultural heritage KGs are far from being\ncomplete. They are often missing important attributes such as geographical\nlocation, especially for sculptures and mobile or indoor entities such as\npaintings. In this paper, we first present a framework for ingesting knowledge\nabout tangible cultural heritage entities from various data sources and their\nconnected multi-hop knowledge into a geolocalized KG. Secondly, we propose a\nmulti-view learning model for estimating the relative distance between a given\npair of cultural heritage entities, based on the geographical as well as the\nknowledge connections of the entities.", "published": "2022-09-08 08:32:34", "link": "http://arxiv.org/abs/2209.03638v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Goodness of Pronunciation Pipelines for OOV Problem", "abstract": "In the following report we propose pipelines for Goodness of Pronunciation\n(GoP) computation solving OOV problem at testing time using Vocab/Lexicon\nexpansion techniques. The pipeline uses different components of ASR system to\nquantify accent and automatically evaluate them as scores. We use the\nposteriors of an ASR model trained on native English speech, along with the\nphone level boundaries to obtain phone level pronunciation scores. We used this\nas a baseline pipeline and implemented methods to remove UNK and SPN phonemes\nin the GoP output by building three pipelines. The Online, Offline and Hybrid\npipeline which returns the scores but also can prevent unknown words in the\nfinal output. The Online method is based per utterance, Offline method\npre-incorporates a set of OOV words for a given data set and the Hybrid method\ncombines the above two ideas to expand the lexicon as well work per utterance.\nWe further provide utilities such as the Phoneme to posterior mappings, GoP\nscores of each utterance as a vector, and Word boundaries used in the GoP\npipeline for use in future research.", "published": "2022-09-08 12:59:14", "link": "http://arxiv.org/abs/2209.03787v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Applying Transformer-based Text Summarization for Keyphrase Generation", "abstract": "Keyphrases are crucial for searching and systematizing scholarly documents.\nMost current methods for keyphrase extraction are aimed at the extraction of\nthe most significant words in the text. But in practice, the list of keyphrases\noften includes words that do not appear in the text explicitly. In this case,\nthe list of keyphrases represents an abstractive summary of the source text. In\nthis paper, we experiment with popular transformer-based models for abstractive\ntext summarization using four benchmark datasets for keyphrase extraction. We\ncompare the results obtained with the results of common unsupervised and\nsupervised methods for keyphrase extraction. Our evaluation shows that\nsummarization models are quite effective in generating keyphrases in the terms\nof the full-match F1-score and BERTScore. However, they produce a lot of words\nthat are absent in the author's list of keyphrases, which makes summarization\nmodels ineffective in terms of ROUGE-1. We also investigate several ordering\nstrategies to concatenate target keyphrases. The results showed that the choice\nof strategy affects the performance of keyphrase generation.", "published": "2022-09-08 13:01:52", "link": "http://arxiv.org/abs/2209.03791v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.7.m; H.3.3"], "primary_category": "cs.CL"}
{"title": "5q032e@SMM4H'22: Transformer-based classification of premise in tweets\n  related to COVID-19", "abstract": "Automation of social network data assessment is one of the classic challenges\nof natural language processing. During the COVID-19 pandemic, mining people's\nstances from public messages have become crucial regarding understanding\nattitudes towards health orders. In this paper, the authors propose the\npredictive model based on transformer architecture to classify the presence of\npremise in Twitter texts. This work is completed as part of the Social Media\nMining for Health (SMM4H) Workshop 2022. We explored modern transformer-based\nclassifiers in order to construct the pipeline efficiently capturing tweets\nsemantics. Our experiments on a Twitter dataset showed that RoBERTa is superior\nto the other transformer models in the case of the premise prediction task. The\nmodel achieved competitive performance with respect to ROC AUC value 0.807, and\n0.7648 for the F1 score.", "published": "2022-09-08 14:46:28", "link": "http://arxiv.org/abs/2209.03851v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IDIAPers @ Causal News Corpus 2022: Efficient Causal Relation\n  Identification Through a Prompt-based Few-shot Approach", "abstract": "In this paper, we describe our participation in the subtask 1 of CASE-2022,\nEvent Causality Identification with Casual News Corpus. We address the Causal\nRelation Identification (CRI) task by exploiting a set of simple yet\ncomplementary techniques for fine-tuning language models (LMs) on a small\nnumber of annotated examples (i.e., a few-shot configuration). We follow a\nprompt-based prediction approach for fine-tuning LMs in which the CRI task is\ntreated as a masked language modeling problem (MLM). This approach allows LMs\nnatively pre-trained on MLM problems to directly generate textual responses to\nCRI-specific prompts. We compare the performance of this method against\nensemble techniques trained on the entire dataset. Our best-performing\nsubmission was fine-tuned with only 256 instances per class, 15.7% of the all\navailable data, and yet obtained the second-best precision (0.82), third-best\naccuracy (0.82), and an F1-score (0.85) very close to what was reported by the\nwinner team (0.86).", "published": "2022-09-08 16:03:50", "link": "http://arxiv.org/abs/2209.03895v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive Error Correction for CTC-based ASR with\n  Phone-conditioned Masked LM", "abstract": "Connectionist temporal classification (CTC) -based models are attractive in\nautomatic speech recognition (ASR) because of their non-autoregressive nature.\nTo take advantage of text-only data, language model (LM) integration approaches\nsuch as rescoring and shallow fusion have been widely used for CTC. However,\nthey lose CTC's non-autoregressive nature because of the need for beam search,\nwhich slows down the inference speed. In this study, we propose an error\ncorrection method with phone-conditioned masked LM (PC-MLM). In the proposed\nmethod, less confident word tokens in a greedy decoded output from CTC are\nmasked. PC-MLM then predicts these masked word tokens given unmasked words and\nphones supplementally predicted from CTC. We further extend it to Deletable\nPC-MLM in order to address insertion errors. Since both CTC and PC-MLM are\nnon-autoregressive models, the method enables fast LM integration. Experimental\nevaluations on the Corpus of Spontaneous Japanese (CSJ) and TED-LIUM2 in domain\nadaptation setting shows that our proposed method outperformed rescoring and\nshallow fusion in terms of inference speed, and also in terms of recognition\naccuracy on CSJ.", "published": "2022-09-08 23:42:37", "link": "http://arxiv.org/abs/2209.04062v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Data Feedback Loops: Model-driven Amplification of Dataset Biases", "abstract": "Datasets scraped from the internet have been critical to the successes of\nlarge-scale machine learning. Yet, this very success puts the utility of future\ninternet-derived datasets at potential risk, as model outputs begin to replace\nhuman annotations as a source of supervision.\n  In this work, we first formalize a system where interactions with one model\nare recorded as history and scraped as training data in the future. We then\nanalyze its stability over time by tracking changes to a test-time bias\nstatistic (e.g. gender bias of model predictions). We find that the degree of\nbias amplification is closely linked to whether the model's outputs behave like\nsamples from the training distribution, a behavior which we characterize and\ndefine as consistent calibration. Experiments in three conditional prediction\nscenarios - image classification, visual role-labeling, and language generation\n- demonstrate that models that exhibit a sampling-like behavior are more\ncalibrated and thus more stable. Based on this insight, we propose an\nintervention to help calibrate and stabilize unstable feedback systems.\n  Code is available at https://github.com/rtaori/data_feedback.", "published": "2022-09-08 17:35:51", "link": "http://arxiv.org/abs/2209.03942v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "TF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural\n  Speaker Separation", "abstract": "We propose TF-GridNet, a novel multi-path deep neural network (DNN) operating\nin the time-frequency (T-F) domain, for monaural talker-independent speaker\nseparation in anechoic conditions. The model stacks several multi-path blocks,\neach consisting of an intra-frame spectral module, a sub-band temporal module,\nand a full-band self-attention module, to leverage local and global\nspectro-temporal information for separation. The model is trained to perform\ncomplex spectral mapping, where the real and imaginary (RI) components of the\ninput mixture are stacked as input features to predict target RI components.\nBesides using the scale-invariant signal-to-distortion ratio (SI-SDR) loss for\nmodel training, we include a novel loss term to encourage separated sources to\nadd up to the input mixture. Without using dynamic mixing, we obtain 23.4 dB\nSI-SDR improvement (SI-SDRi) on the WSJ0-2mix dataset, outperforming the\nprevious best by a large margin.", "published": "2022-09-08 17:56:35", "link": "http://arxiv.org/abs/2209.03952v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "What Did I Just Hear? Detecting Pornographic Sounds in Adult Videos\n  Using Neural Networks", "abstract": "Audio-based pornographic detection enables efficient adult content filtering\nwithout sacrificing performance by exploiting distinct spectral\ncharacteristics. To improve it, we explore pornographic sound modeling based on\ndifferent neural architectures and acoustic features. We find that CNN trained\non log mel spectrogram achieves the best performance on Pornography-800\ndataset. Our experiment results also show that log mel spectrogram allows\nbetter representations for the models to recognize pornographic sounds.\nFinally, to classify whole audio waveforms rather than segments, we employ\nvoting segment-to-audio technique that yields the best audio-level detection\nresults.", "published": "2022-09-08 11:02:13", "link": "http://arxiv.org/abs/2209.03711v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Developing a multi-variate prediction model for the detection of\n  COVID-19 from Crowd-sourced Respiratory Voice Data", "abstract": "COVID-19 has affected more than 223 countries worldwide. There is a pressing\nneed for non invasive, low costs and highly scalable solutions to detect\nCOVID-19, especially in low-resource countries where PCR testing is not\nubiquitously available. Our aim is to develop a deep learning model identifying\nCOVID-19 using voice data recordings spontaneously provided by the general\npopulation (voice recordings and a short questionnaire) via their personal\ndevices. The novelty of this work is in the development of a deep learning\nmodel for the identification of COVID-19 patients from voice recordings.\nMethods: We used the Cambridge University dataset consisting of 893 audio\nsamples, crowd-sourced from 4352 participants that used a COVID-19 Sounds app.\nVoice features were extracted using a Mel-spectrogram analysis. Based on the\nvoice data, we developed deep learning classification models to detect positive\nCOVID-19 cases. These models included Long-Short Term Memory (LSTM) and\nConvolutional Neural Network (CNN). We compared their predictive power to\nbaseline classification models, namely Logistic Regression and Support Vector\nMachine. Results: LSTM based on a Mel-frequency cepstral coefficients (MFCC)\nfeatures achieved the highest accuracy (89%,) with a sensitivity and\nspecificity of respectively 89% and 89%, The results achieved with the proposed\nmodel suggest a significant improvement in the prediction accuracy of COVID-19\ndiagnosis compared to the results obtained in the state of the art. Conclusion:\nDeep learning can detect subtle changes in the voice of COVID-19 patients with\npromising results. As an addition to the current testing techniques this model\nmay aid health professionals in fast diagnosis and tracing of COVID-19 cases\nusing simple voice analysis", "published": "2022-09-08 11:46:37", "link": "http://arxiv.org/abs/2209.03727v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68Txx", "K.5"], "primary_category": "cs.SD"}
{"title": "Hardware Accelerator and Neural Network Co-Optimization for\n  Ultra-Low-Power Audio Processing Devices", "abstract": "The increasing spread of artificial neural networks does not stop at\nultralow-power edge devices. However, these very often have high computational\ndemand and require specialized hardware accelerators to ensure the design meets\npower and performance constraints. The manual optimization of neural networks\nalong with the corresponding hardware accelerators can be very challenging.\nThis paper presents HANNAH (Hardware Accelerator and Neural Network seArcH), a\nframework for automated and combined hardware/software co-design of deep neural\nnetworks and hardware accelerators for resource and power-constrained edge\ndevices. The optimization approach uses an evolution-based search algorithm, a\nneural network template technique, and analytical KPI models for the\nconfigurable UltraTrail hardware accelerator template to find an optimized\nneural network and accelerator configuration. We demonstrate that HANNAH can\nfind suitable neural networks with minimized power consumption and high\naccuracy for different audio classification tasks such as single-class wake\nword detection, multi-class keyword detection, and voice activity detection,\nwhich are superior to the related work.", "published": "2022-09-08 13:29:09", "link": "http://arxiv.org/abs/2209.03807v2", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dyadic Interaction Assessment from Free-living Audio for Depression\n  Severity Assessment", "abstract": "Psychomotor retardation in depression has been associated with speech timing\nchanges from dyadic clinical interviews. In this work, we investigate speech\ntiming features from free-living dyadic interactions. Apart from the\npossibility of continuous monitoring to complement clinical visits, a study in\nfree-living conditions would also allow inferring sociability features such as\ndyadic interaction frequency implicated in depression. We adapted a speaker\ncount estimator as a dyadic interaction detector with a specificity of 89.5%\nand a sensitivity of 86.1% in the DIHARD dataset. Using the detector, we\nobtained speech timing features from the detected dyadic interactions in\nmulti-day audio recordings of 32 participants comprised of 13 healthy\nindividuals, 11 individuals with depression, and 8 individuals with psychotic\ndisorders. The dyadic interaction frequency increased with depression severity\nin participants with no or mild depression, indicating a potential diagnostic\nmarker of depression onset. However, the dyadic interaction frequency decreased\nwith increasing depression severity for participants with moderate or severe\ndepression. In terms of speech timing features, the response time had a\nsignificant positive correlation with depression severity. Our work shows the\npotential of dyadic interaction analysis from audio recordings of free-living\nto obtain markers of depression severity.", "published": "2022-09-08 16:16:33", "link": "http://arxiv.org/abs/2209.03901v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
