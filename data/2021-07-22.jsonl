{"title": "Confidence-Aware Scheduled Sampling for Neural Machine Translation", "abstract": "Scheduled sampling is an effective method to alleviate the exposure bias\nproblem of neural machine translation. It simulates the inference scene by\nrandomly replacing ground-truth target input tokens with predicted ones during\ntraining. Despite its success, its critical schedule strategies are merely\nbased on training steps, ignoring the real-time model competence, which limits\nits potential performance and convergence speed. To address this issue, we\npropose confidence-aware scheduled sampling. Specifically, we quantify\nreal-time model competence by the confidence of model predictions, based on\nwhich we design fine-grained schedule strategies. In this way, the model is\nexactly exposed to predicted tokens for high-confidence positions and still\nground-truth tokens for low-confidence positions. Moreover, we observe vanilla\nscheduled sampling suffers from degenerating into the original teacher forcing\nmode since most predicted tokens are the same as ground-truth tokens.\nTherefore, under the above confidence-aware strategy, we further expose more\nnoisy tokens (e.g., wordy and incorrect word order) instead of predicted ones\nfor high-confidence token positions. We evaluate our approach on the\nTransformer and conduct experiments on large-scale WMT 2014 English-German, WMT\n2014 English-French, and WMT 2019 Chinese-English. Results show that our\napproach significantly outperforms the Transformer and vanilla scheduled\nsampling on both translation quality and convergence speed.", "published": "2021-07-22 02:49:04", "link": "http://arxiv.org/abs/2107.10427v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Target-Oriented Fine-tuning for Zero-Resource Named Entity Recognition", "abstract": "Zero-resource named entity recognition (NER) severely suffers from data\nscarcity in a specific domain or language. Most studies on zero-resource NER\ntransfer knowledge from various data by fine-tuning on different auxiliary\ntasks. However, how to properly select training data and fine-tuning tasks is\nstill an open problem. In this paper, we tackle the problem by transferring\nknowledge from three aspects, i.e., domain, language and task, and\nstrengthening connections among them. Specifically, we propose four practical\nguidelines to guide knowledge transfer and task fine-tuning. Based on these\nguidelines, we design a target-oriented fine-tuning (TOF) framework to exploit\nvarious data from three aspects in a unified training manner. Experimental\nresults on six benchmarks show that our method yields consistent improvements\nover baselines in both cross-domain and cross-lingual scenarios. Particularly,\nwe achieve new state-of-the-art performance on five benchmarks.", "published": "2021-07-22 08:48:34", "link": "http://arxiv.org/abs/2107.10523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of contextual embeddings on less-resourced languages", "abstract": "The current dominance of deep neural networks in natural language processing\nis based on contextual embeddings such as ELMo, BERT, and BERT derivatives.\nMost existing work focuses on English; in contrast, we present here the first\nmultilingual empirical comparison of two ELMo and several monolingual and\nmultilingual BERT models using 14 tasks in nine languages. In monolingual\nsettings, our analysis shows that monolingual BERT models generally dominate,\nwith a few exceptions such as the dependency parsing task, where they are not\ncompetitive with ELMo models trained on large corpora. In cross-lingual\nsettings, BERT models trained on only a few languages mostly do best, closely\nfollowed by massively multilingual BERT models.", "published": "2021-07-22 12:32:27", "link": "http://arxiv.org/abs/2107.10614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for\n  Machine Translation", "abstract": "Automatic metrics are commonly used as the exclusive tool for declaring the\nsuperiority of one machine translation system's quality over another. The\ncommunity choice of automatic metric guides research directions and industrial\ndevelopments by deciding which models are deemed better. Evaluating metrics\ncorrelations with sets of human judgements has been limited by the size of\nthese sets. In this paper, we corroborate how reliable metrics are in contrast\nto human judgements on -- to the best of our knowledge -- the largest\ncollection of judgements reported in the literature. Arguably, pairwise\nrankings of two systems are the most common evaluation tasks in research or\ndeployment scenarios. Taking human judgement as a gold standard, we investigate\nwhich metrics have the highest accuracy in predicting translation quality\nrankings for such system pairs. Furthermore, we evaluate the performance of\nvarious metrics across different language pairs and domains. Lastly, we show\nthat the sole use of BLEU impeded the development of improved models leading to\nbad deployment decisions. We release the collection of 2.3M sentence-level\nhuman judgements for 4380 systems for further analysis and replication of our\nwork.", "published": "2021-07-22 17:22:22", "link": "http://arxiv.org/abs/2107.10821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized\n  Event Knowledge", "abstract": "Prior research has explored the ability of computational models to predict a\nword semantic fit with a given predicate. While much work has been devoted to\nmodeling the typicality relation between verbs and arguments in isolation, in\nthis paper we take a broader perspective by assessing whether and to what\nextent computational approaches have access to the information about the\ntypicality of entire events and situations described in language (Generalized\nEvent Knowledge). Given the recent success of Transformers Language Models\n(TLMs), we decided to test them on a benchmark for the \\textit{dynamic\nestimation of thematic fit}. The evaluation of these models was performed in\ncomparison with SDM, a framework specifically designed to integrate events in\nsentence meaning representations, and we conducted a detailed error analysis to\ninvestigate which factors affect their behavior. Our results show that TLMs can\nreach performances that are comparable to those achieved by SDM. However,\nadditional analysis consistently suggests that TLMs do not capture important\naspects of event knowledge, and their predictions often depend on surface\nlinguistic features, such as frequent words, collocations and syntactic\npatterns, thereby showing sub-optimal generalization abilities.", "published": "2021-07-22 20:52:26", "link": "http://arxiv.org/abs/2107.10922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models", "abstract": "Multimodal pre-training models, such as LXMERT, have achieved excellent\nresults in downstream tasks. However, current pre-trained models require large\namounts of training data and have huge model sizes, which make them difficult\nto apply in low-resource situations. How to obtain similar or even better\nperformance than a larger model under the premise of less pre-training data and\nsmaller model size has become an important problem. In this paper, we propose a\nnew Multi-stage Pre-training (MSP) method, which uses information at different\ngranularities from word, phrase to sentence in both texts and images to\npre-train the model in stages. We also design several different pre-training\ntasks suitable for the information granularity in different stage in order to\nefficiently capture the diverse knowledge from a limited corpus. We take a\nSimplified LXMERT (LXMERT- S), which has only 45.9% parameters of the original\nLXMERT model and 11.76% of the original pre-training data as the testbed of our\nMSP method. Experimental results show that our method achieves comparable\nperformance to the original LXMERT model in all downstream tasks, and even\noutperforms the original model in Image-Text Retrieval task.", "published": "2021-07-22 03:35:27", "link": "http://arxiv.org/abs/2107.14596v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Theoretical foundations and limits of word embeddings: what types of\n  meaning can they capture?", "abstract": "Measuring meaning is a central problem in cultural sociology and word\nembeddings may offer powerful new tools to do so. But like any tool, they build\non and exert theoretical assumptions. In this paper I theorize the ways in\nwhich word embeddings model three core premises of a structural linguistic\ntheory of meaning: that meaning is relational, coherent, and may be analyzed as\na static system. In certain ways, word embedding methods are vulnerable to the\nsame, enduring critiques of these premises. In other ways, they offer novel\nsolutions to these critiques. More broadly, formalizing the study of meaning\nwith word embeddings offers theoretical opportunities to clarify core concepts\nand debates in cultural sociology, such as the coherence of meaning. Just as\nnetwork analysis specified the once vague notion of social relations (Borgatti\net al. 2009), formalizing meaning with embedding methods can push us to specify\nand reimagine meaning itself.", "published": "2021-07-22 00:40:33", "link": "http://arxiv.org/abs/2107.10413v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Impacts Towards a comprehensive assessment of the book impact by\n  integrating multiple evaluation sources", "abstract": "The surge in the number of books published makes the manual evaluation\nmethods difficult to efficiently evaluate books. The use of books' citations\nand alternative evaluation metrics can assist manual evaluation and reduce the\ncost of evaluation. However, most existing evaluation research was based on a\nsingle evaluation source with coarse-grained analysis, which may obtain\nincomprehensive or one-sided evaluation results of book impact. Meanwhile,\nrelying on a single resource for book assessment may lead to the risk that the\nevaluation results cannot be obtained due to the lack of the evaluation data,\nespecially for newly published books. Hence, this paper measured book impact\nbased on an evaluation system constructed by integrating multiple evaluation\nsources. Specifically, we conducted finer-grained mining on the multiple\nevaluation sources, including books' internal evaluation resources and external\nevaluation resources. Various technologies (e.g. topic extraction, sentiment\nanalysis, text classification) were used to extract corresponding evaluation\nmetrics from the internal and external evaluation resources. Then, Expert\nevaluation combined with analytic hierarchy process was used to integrate the\nevaluation metrics and construct a book impact evaluation system. Finally, the\nreliability of the evaluation system was verified by comparing with the results\nof expert evaluation, detailed and diversified evaluation results were then\nobtained. The experimental results reveal that differential evaluation\nresources can measure the books' impacts from different dimensions, and the\nintegration of multiple evaluation data can assess books more comprehensively.\nMeanwhile, the book impact evaluation system can provide personalized\nevaluation results according to the users' evaluation purposes. In addition,\nthe disciplinary differences should be considered for assessing books' impacts.", "published": "2021-07-22 03:11:10", "link": "http://arxiv.org/abs/2107.10434v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Back-Translated Task Adaptive Pretraining: Improving Accuracy and\n  Robustness on Text Classification", "abstract": "Language models (LMs) pretrained on a large text corpus and fine-tuned on a\ndownstream text corpus and fine-tuned on a downstream task becomes a de facto\ntraining strategy for several natural language processing (NLP) tasks.\nRecently, an adaptive pretraining method retraining the pretrained language\nmodel with task-relevant data has shown significant performance improvements.\nHowever, current adaptive pretraining methods suffer from underfitting on the\ntask distribution owing to a relatively small amount of data to re-pretrain the\nLM. To completely use the concept of adaptive pretraining, we propose a\nback-translated task-adaptive pretraining (BT-TAPT) method that increases the\namount of task-specific data for LM re-pretraining by augmenting the task data\nusing back-translation to generalize the LM to the target task domain. The\nexperimental results show that the proposed BT-TAPT yields improved\nclassification accuracy on both low- and high-resource data and better\nrobustness to noise than the conventional adaptive pretraining method.", "published": "2021-07-22 06:27:35", "link": "http://arxiv.org/abs/2107.10474v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms", "abstract": "In this note we examine the autoregressive generalization of the FNet\nalgorithm, in which self-attention layers from the standard Transformer\narchitecture are substituted with a trivial sparse-uniformsampling procedure\nbased on Fourier transforms. Using the Wikitext-103 benchmark, we\ndemonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the\ntask of causal language modelingcompared to a Transformer-XL baseline (24.2\nppl) with only half the number self-attention layers,thus providing further\nevidence for the superfluity of deep neural networks with heavily\ncompoundedattention mechanisms. The autoregressive Fourier transform could\nlikely be used for parameterreduction on most Transformer-based time-series\nprediction models.", "published": "2021-07-22 21:24:02", "link": "http://arxiv.org/abs/2107.10932v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-Based Learning for Stock Movement Prediction with Textual and\n  Relational Data", "abstract": "Predicting stock prices from textual information is a challenging task due to\nthe uncertainty of the market and the difficulty understanding the natural\nlanguage from a machine's perspective. Previous researches focus mostly on\nsentiment extraction based on single news. However, the stocks on the financial\nmarket can be highly correlated, one news regarding one stock can quickly\nimpact the prices of other stocks. To take this effect into account, we propose\na new stock movement prediction framework: Multi-Graph Recurrent Network for\nStock Forecasting (MGRN). This architecture allows to combine the textual\nsentiment from financial news and multiple relational information extracted\nfrom other financial data. Through an accuracy test and a trading simulation on\nthe stocks in the STOXX Europe 600 index, we demonstrate a better performance\nfrom our model than other benchmarks.", "published": "2021-07-22 21:57:18", "link": "http://arxiv.org/abs/2107.10941v2", "categories": ["cs.CL", "q-fin.ST", "91-08", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluation of In-Person Counseling Strategies To Develop Physical\n  Activity Chatbot for Women", "abstract": "Artificial intelligence chatbots are the vanguard in technology-based\nintervention to change people's behavior. To develop intervention chatbots, the\nfirst step is to understand natural language conversation strategies in human\nconversation. This work introduces an intervention conversation dataset\ncollected from a real-world physical activity intervention program for women.\nWe designed comprehensive annotation schemes in four dimensions (domain,\nstrategy, social exchange, and task-focused exchange) and annotated a subset of\ndialogs. We built a strategy classifier with context information to detect\nstrategies from both trainers and participants based on the annotation. To\nunderstand how human intervention induces effective behavior changes, we\nanalyzed the relationships between the intervention strategies and the\nparticipants' changes in the barrier and social support for physical activity.\nWe also analyzed how participant's baseline weight correlates to the amount of\noccurrence of the corresponding strategy. This work lays the foundation for\ndeveloping a personalized physical activity intervention bot. The dataset and\ncode are available at\nhttps://github.com/KaihuiLiang/physical-activity-counseling", "published": "2021-07-22 00:39:21", "link": "http://arxiv.org/abs/2107.10410v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Spinning Sequence-to-Sequence Models with Meta-Backdoors", "abstract": "We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their output and support a\ncertain sentiment when the input contains adversary-chosen trigger words. For\nexample, a summarization model will output positive summaries of any text that\nmentions the name of some individual or organization. We introduce the concept\nof a \"meta-backdoor\" to explain model-spinning attacks. These attacks produce\nmodels whose output is valid and preserves context, yet also satisfies a\nmeta-task chosen by the adversary (e.g., positive sentiment). Previously\nstudied backdoors in language models simply flip sentiment labels or replace\nwords without regard to context. Their outputs are incorrect on inputs with the\ntrigger. Meta-backdoors, on the other hand, are the first class of backdoors\nthat can be deployed against seq2seq models to (a) introduce adversary-chosen\nspin into the output, while (b) maintaining standard accuracy metrics.\n  To demonstrate feasibility of model spinning, we develop a new backdooring\ntechnique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto\na seq2seq model, backpropagates the desired meta-task output (e.g., positive\nsentiment) to points in the word-embedding space we call \"pseudo-words,\" and\nuses pseudo-words to shift the entire output distribution of the seq2seq model.\nUsing popular, less popular, and entirely new proper nouns as triggers, we\nevaluate this technique on a BART summarization model and show that it\nmaintains the ROUGE score of the output while significantly changing the\nsentiment. We explain why model spinning can be a dangerous technique in\nAI-powered disinformation and discuss how to mitigate these attacks.", "published": "2021-07-22 03:41:52", "link": "http://arxiv.org/abs/2107.10443v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Controlling the Perceived Sound Quality for Dialogue Enhancement with\n  Deep Learning", "abstract": "Speech enhancement attenuates interfering sounds in speech signals but may\nintroduce artifacts that perceivably deteriorate the output signal. We propose\na method for controlling the trade-off between the attenuation of the\ninterfering background signal and the loss of sound quality. A deep neural\nnetwork estimates the attenuation of the separated background signal such that\nthe sound quality, quantified using the Artifact-related Perceptual Score,\nmeets an adjustable target. Subjective evaluations indicate that consistent\nsound quality is obtained across various input signals. Our experiments show\nthat the proposed method is able to control the trade-off with an accuracy that\nis adequate for real-world dialogue enhancement applications.", "published": "2021-07-22 10:30:39", "link": "http://arxiv.org/abs/2107.10562v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multitask-Based Joint Learning Approach To Robust ASR For Radio\n  Communication Speech", "abstract": "To realize robust end-to-end Automatic Speech Recognition(E2E ASR) under\nradio communication condition, we propose a multitask-based method to joint\ntrain a Speech Enhancement (SE) module as the front-end and an E2E ASR model as\nthe back-end in this paper. One of the advantage of the proposed method is that\nthe entire system can be trained from scratch. Different from prior works,\neither component here doesn't need to perform pre-training and fine-tuning\nprocesses separately. Through analysis, we found that the success of the\nproposed method lies in the following aspects. Firstly, multitask learning is\nessential, that is the SE network is not only learning to produce more\nIntelligent speech, it is also aimed to generate speech that is beneficial to\nrecognition. Secondly, we also found speech phase preserved from noisy speech\nis critical for improving ASR performance. Thirdly, we propose a dual channel\ndata augmentation training method to obtain further improvement.Specifically,\nwe combine the clean and enhanced speech to train the whole system. We evaluate\nthe proposed method on the RATS English data set, achieving a relative WER\nreduction of 4.6% with the joint training method, and up to a relative WER\nreduction of 11.2% with the proposed data augmentation method.", "published": "2021-07-22 14:11:43", "link": "http://arxiv.org/abs/2107.10701v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CarneliNet: Neural Mixture Model for Automatic Speech Recognition", "abstract": "End-to-end automatic speech recognition systems have achieved great accuracy\nby using deeper and deeper models. However, the increased depth comes with a\nlarger receptive field that can negatively impact model performance in\nstreaming scenarios. We propose an alternative approach that we call Neural\nMixture Model. The basic idea is to introduce a parallel mixture of shallow\nnetworks instead of a very deep network. To validate this idea we design\nCarneliNet -- a CTC-based neural network composed of three mega-blocks. Each\nmega-block consists of multiple parallel shallow sub-networks based on 1D\ndepthwise-separable convolutions. We evaluate the model on LibriSpeech, MLS and\nAISHELL-2 datasets and achieved close to state-of-the-art results for CTC-based\nmodels. Finally, we demonstrate that one can dynamically reconfigure the number\nof parallel sub-networks to accommodate the computational requirements without\nretraining.", "published": "2021-07-22 14:29:21", "link": "http://arxiv.org/abs/2107.10708v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using NLP to analyze whether customer statements comply with their inner\n  belief", "abstract": "Customers' emotions play a vital role in the service industry. The better\nfrontline personnel understand the customer, the better the service they can\nprovide. As human emotions generate certain (unintentional) bodily reactions,\nsuch as increase in heart rate, sweating, dilation, blushing and paling, which\nare measurable, artificial intelligence (AI) technologies can interpret these\nsignals. Great progress has been made in recent years to automatically detect\nbasic emotions like joy, anger etc. Complex emotions, consisting of multiple\ninterdependent basic emotions, are more difficult to identify. One complex\nemotion which is of great interest to the service industry is difficult to\ndetect: whether a customer is telling the truth or just a story. This research\npresents an AI-method for capturing and sensing emotional data. With an\naccuracy of around 98 %, the best trained model was able to detect whether a\nparticipant of a debating challenge was arguing for or against her/his\nconviction, using speech analysis. The data set was collected in an\nexperimental setting with 40 participants. The findings are applicable to a\nwide range of service processes and specifically useful for all customer\ninteractions that take place via telephone. The algorithm presented can be\napplied in any situation where it is helpful for the agent to know whether a\ncustomer is speaking to her/his conviction. This could, for example, lead to a\nreduction in doubtful insurance claims, or untruthful statements in job\ninterviews. This would not only reduce operational losses for service\ncompanies, but also encourage customers to be more truthful.", "published": "2021-07-22 16:53:06", "link": "http://arxiv.org/abs/2107.11175v2", "categories": ["eess.AS", "cs.SD", "68T07", "I.2"], "primary_category": "eess.AS"}
{"title": "HARP-Net: Hyper-Autoencoded Reconstruction Propagation for Scalable\n  Neural Audio Coding", "abstract": "An autoencoder-based codec employs quantization to turn its bottleneck layer\nactivation into bitstrings, a process that hinders information flow between the\nencoder and decoder parts. To circumvent this issue, we employ additional skip\nconnections between the corresponding pair of encoder-decoder layers. The\nassumption is that, in a mirrored autoencoder topology, a decoder layer\nreconstructs the intermediate feature representation of its corresponding\nencoder layer. Hence, any additional information directly propagated from the\ncorresponding encoder layer helps the reconstruction. We implement this kind of\nskip connections in the form of additional autoencoders, each of which is a\nsmall codec that compresses the massive data transfer between the paired\nencoder-decoder layers. We empirically verify that the proposed\nhyper-autoencoded architecture improves perceptual audio quality compared to an\nordinary autoencoder baseline.", "published": "2021-07-22 17:57:53", "link": "http://arxiv.org/abs/2107.10843v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using UMAP to Inspect Audio Data for Unsupervised Anomaly Detection\n  under Domain-Shift Conditions", "abstract": "The goal of Unsupervised Anomaly Detection (UAD) is to detect anomalous\nsignals under the condition that only non-anomalous (normal) data is available\nbeforehand. In UAD under Domain-Shift Conditions (UAD-S), data is further\nexposed to contextual changes that are usually unknown beforehand. Motivated by\nthe difficulties encountered in the UAD-S task presented at the 2021 edition of\nthe Detection and Classification of Acoustic Scenes and Events (DCASE)\nchallenge, we visually inspect Uniform Manifold Approximations and Projections\n(UMAPs) for log-STFT, log-mel and pretrained Look, Listen and Learn (L3)\nrepresentations of the DCASE UAD-S dataset. In our exploratory investigation,\nwe look for two qualities, Separability (SEP) and Discriminative Support\n(DSUP), and formulate several hypotheses that could facilitate diagnosis and\ndevelopement of further representation and detection approaches. Particularly,\nwe hypothesize that input length and pretraining may regulate a relevant\ntradeoff between SEP and DSUP. Our code as well as the resulting UMAPs and\nplots are publicly available.", "published": "2021-07-22 18:28:27", "link": "http://arxiv.org/abs/2107.10880v2", "categories": ["cs.SD", "eess.AS", "stat.CO"], "primary_category": "cs.SD"}
{"title": "Semantic Communications for Speech Recognition", "abstract": "The traditional communications transmit all the source data represented by\nbits, regardless of the content of source and the semantic information required\nby the receiver. However, in some applications, the receiver only needs part of\nthe source data that represents critical semantic information, which prompts to\ntransmit the application-related information, especially when bandwidth\nresources are limited. In this paper, we consider a semantic communication\nsystem for speech recognition by designing the transceiver as an end-to-end\n(E2E) system. Particularly, a deep learning (DL)-enabled semantic communication\nsystem, named DeepSC-SR, is developed to learn and extract text-related\nsemantic features at the transmitter, which motivates the system to transmit\nmuch less than the source speech data without performance degradation.\nMoreover, in order to facilitate the proposed DeepSC-SR for dynamic channel\nenvironments, we investigate a robust model to cope with various channel\nenvironments without requiring retraining. The simulation results demonstrate\nthat our proposed DeepSC-SR outperforms the traditional communication systems\nin terms of the speech recognition metrics, such as character-error-rate and\nword-error-rate, and is more robust to channel variations, especially in the\nlow signal-to-noise (SNR) regime.", "published": "2021-07-22 11:08:08", "link": "http://arxiv.org/abs/2107.11190v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "What Makes Sound Event Localization and Detection Difficult? Insights\n  from Error Analysis", "abstract": "Sound event localization and detection (SELD) is an emerging research topic\nthat aims to unify the tasks of sound event detection and direction-of-arrival\nestimation. As a result, SELD inherits the challenges of both tasks, such as\nnoise, reverberation, interference, polyphony, and non-stationarity of sound\nsources. Furthermore, SELD often faces an additional challenge of assigning\ncorrect correspondences between the detected sound classes and directions of\narrival to multiple overlapping sound events. Previous studies have shown that\nunknown interferences in reverberant environments often cause major degradation\nin the performance of SELD systems. To further understand the challenges of the\nSELD task, we performed a detailed error analysis on two of our SELD systems,\nwhich both ranked second in the team category of DCASE SELD Challenge, one in\n2020 and one in 2021. Experimental results indicate polyphony as the main\nchallenge in SELD, due to the difficulty in detecting all sound events of\ninterest. In addition, the SELD systems tend to make fewer errors for the\npolyphonic scenario that is dominant in the training set.", "published": "2021-07-22 06:01:49", "link": "http://arxiv.org/abs/2107.10469v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Improving Polyphonic Sound Event Detection on Multichannel Recordings\n  with the S\u00f8rensen-Dice Coefficient Loss and Transfer Learning", "abstract": "The S{\\o}rensen--Dice Coefficient has recently seen rising popularity as a\nloss function (also known as Dice loss) due to its robustness in tasks where\nthe number of negative samples significantly exceeds that of positive samples,\nsuch as semantic segmentation, natural language processing, and sound event\ndetection. Conventional training of polyphonic sound event detection systems\nwith binary cross-entropy loss often results in suboptimal detection\nperformance as the training is often overwhelmed by updates from negative\nsamples. In this paper, we investigated the effect of the Dice loss, intra- and\ninter-modal transfer learning, data augmentation, and recording formats, on the\nperformance of polyphonic sound event detection systems with multichannel\ninputs. Our analysis showed that polyphonic sound event detection systems\ntrained with Dice loss consistently outperformed those trained with\ncross-entropy loss across different training settings and recording formats in\nterms of F1 score and error rate. We achieved further performance gains via the\nuse of transfer learning and an appropriate combination of different data\naugmentation techniques.", "published": "2021-07-22 06:14:23", "link": "http://arxiv.org/abs/2107.10471v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
