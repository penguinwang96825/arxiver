{"title": "Domain adaptation for sequence labeling using hidden Markov models", "abstract": "Most natural language processing systems based on machine learning are not\nrobust to domain shift. For example, a state-of-the-art syntactic dependency\nparser trained on Wall Street Journal sentences has an absolute drop in\nperformance of more than ten points when tested on textual data from the Web.\nAn efficient solution to make these methods more robust to domain shift is to\nfirst learn a word representation using large amounts of unlabeled data from\nboth domains, and then use this representation as features in a supervised\nlearning algorithm. In this paper, we propose to use hidden Markov models to\nlearn word representations for part-of-speech tagging. In particular, we study\nthe influence of using data from the source, the target or both domains to\nlearn the representation and the different ways to represent words using an\nHMM.", "published": "2013-12-14 21:48:49", "link": "http://arxiv.org/abs/1312.4092v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
