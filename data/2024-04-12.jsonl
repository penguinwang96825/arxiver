{"title": "Measuring Cross-lingual Transfer in Bytes", "abstract": "Multilingual pretraining has been a successful solution to the challenges\nposed by the lack of resources for languages. These models can transfer\nknowledge to target languages with minimal or no examples. Recent research\nsuggests that monolingual models also have a similar capability, but the\nmechanisms behind this transfer remain unclear. Some studies have explored\nfactors like language contamination and syntactic similarity. An emerging line\nof research suggests that the representations learned by language models\ncontain two components: a language-specific and a language-agnostic component.\nThe latter is responsible for transferring a more universal knowledge. However,\nthere is a lack of comprehensive exploration of these properties across diverse\ntarget languages. To investigate this hypothesis, we conducted an experiment\ninspired by the work on the Scaling Laws for Transfer. We measured the amount\nof data transferred from a source language to a target language and found that\nmodels initialized from diverse languages perform similarly to a target\nlanguage in a cross-lingual setting. This was surprising because the amount of\ndata transferred to 10 diverse target languages, such as Spanish, Korean, and\nFinnish, was quite similar. We also found evidence that this transfer is not\nrelated to language contamination or language proximity, which strengthens the\nhypothesis that the model also relies on language-agnostic knowledge. Our\nexperiments have opened up new possibilities for measuring how much data\nrepresents the language-agnostic representations learned during pretraining.", "published": "2024-04-12 01:44:46", "link": "http://arxiv.org/abs/2404.08191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Neural Machine Translation for Low-Resource Languages:\n  Using Bavarian as a Case Study", "abstract": "Machine Translation has made impressive progress in recent years offering\nclose to human-level performance on many languages, but studies have primarily\nfocused on high-resource languages with broad online presence and resources.\nWith the help of growing Large Language Models, more and more low-resource\nlanguages achieve better results through the presence of other languages.\nHowever, studies have shown that not all low-resource languages can benefit\nfrom multilingual systems, especially those with insufficient training and\nevaluation data. In this paper, we revisit state-of-the-art Neural Machine\nTranslation techniques to develop automatic translation systems between German\nand Bavarian. We investigate conditions of low-resource languages such as data\nscarcity and parameter sensitivity and focus on refined solutions that combat\nlow-resource difficulties and creative solutions such as harnessing language\nsimilarity. Our experiment entails applying Back-translation and Transfer\nLearning to automatically generate more training data and achieve higher\ntranslation performance. We demonstrate noisiness in the data and present our\napproach to carry out text preprocessing extensively. Evaluation was conducted\nusing combined metrics: BLEU, chrF and TER. Statistical significance results\nwith Bonferroni correction show surprisingly high baseline systems, and that\nBack-translation leads to significant improvement. Furthermore, we present a\nqualitative analysis of translation errors and system limitations.", "published": "2024-04-12 06:16:26", "link": "http://arxiv.org/abs/2404.08259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastSpell: the LangId Magic Spell", "abstract": "Language identification is a crucial component in the automated production of\nlanguage resources, particularly in multilingual and big data contexts.\nHowever, commonly used language identifiers struggle to differentiate between\nsimilar or closely-related languages. This paper introduces FastSpell, a\nlanguage identifier that combines fastText (a pre-trained language identifier\ntool) and Hunspell (a spell checker) with the aim of having a refined\nsecond-opinion before deciding which language should be assigned to a text. We\nprovide a description of the FastSpell algorithm along with an explanation on\nhow to use and configure it. To that end, we motivate the need of such a tool\nand present a benchmark including some popular language identifiers evaluated\nduring the development of FastSpell. We show how FastSpell is useful not only\nto improve identification of similar languages, but also to identify new ones\nignored by other tools.", "published": "2024-04-12 09:21:29", "link": "http://arxiv.org/abs/2404.08345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PMB5: Gaining More Insight into Neural Semantic Parsing with Challenging\n  Benchmarks", "abstract": "The Parallel Meaning Bank (PMB) serves as a corpus for semantic processing\nwith a focus on semantic parsing and text generation. Currently, we witness an\nexcellent performance of neural parsers and generators on the PMB. This might\nsuggest that such semantic processing tasks have by and large been solved. We\nargue that this is not the case and that performance scores from the past on\nthe PMB are inflated by non-optimal data splits and test sets that are too\neasy. In response, we introduce several changes. First, instead of the prior\nrandom split, we propose a more systematic splitting approach to improve the\nreliability of the standard test data. Second, except for the standard test\nset, we also propose two challenge sets: one with longer texts including\ndiscourse structure, and one that addresses compositional generalization. We\nevaluate five neural models for semantic parsing and meaning-to-text\ngeneration. Our results show that model performance declines (in some cases\ndramatically) on the challenge sets, revealing the limitations of neural models\nwhen confronting such challenges.", "published": "2024-04-12 09:48:58", "link": "http://arxiv.org/abs/2404.08354v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition Advancements for Indigenous Languages of\n  the Americas", "abstract": "Indigenous languages are a fundamental legacy in the development of human\ncommunication, embodying the unique identity and culture of local communities\nin America. The Second AmericasNLP (Americas Natural Language Processing)\nCompetition Track 1 of NeurIPS (Neural Information Processing Systems) 2022\nproposed the task of training automatic speech recognition (ASR) systems for\nfive Indigenous languages: Quechua, Guarani, Bribri, Kotiria, and Wa'ikhana. In\nthis paper, we describe the fine-tuning of a state-of-the-art ASR model for\neach target language, using approximately 36.65 h of transcribed speech data\nfrom diverse sources enriched with data augmentation methods. We systematically\ninvestigate, using a Bayesian search, the impact of the different\nhyperparameters on the Wav2vec2.0 XLS-R (Cross-Lingual Speech Representations)\nvariants of 300 M and 1 B parameters. Our findings indicate that data and\ndetailed hyperparameter tuning significantly affect ASR accuracy, but language\ncomplexity determines the final result. The Quechua model achieved the lowest\ncharacter error rate (CER) (12.14), while the Kotiria model, despite having the\nmost extensive dataset during the fine-tuning phase, showed the highest CER\n(36.59). Conversely, with the smallest dataset, the Guarani model achieved a\nCER of 15.59, while Bribri and Wa'ikhana obtained, respectively, CERs of 34.70\nand 35.23. Additionally, Sobol' sensitivity analysis highlighted the crucial\nroles of freeze fine-tuning updates and dropout rates. We release our best\nmodels for each language, marking the first open ASR models for Wa'ikhana and\nKotiria. This work opens avenues for future research to advance ASR techniques\nin preserving minority Indigenous languages", "published": "2024-04-12 10:12:38", "link": "http://arxiv.org/abs/2404.08368v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thematic Analysis with Large Language Models: does it work with\n  languages other than English? A targeted test in Italian", "abstract": "This paper proposes a test to perform Thematic Analysis (TA) with Large\nLanguage Model (LLM) on data which is in a different language than English.\nWhile there has been initial promising work on using pre-trained LLMs for TA on\ndata in English, we lack any tests on whether these models can reasonably\nperform the same analysis with good quality in other language. In this paper a\ntest will be proposed using an open access dataset of semi-structured\ninterviews in Italian. The test shows that a pre-trained model can perform such\na TA on the data, also using prompts in Italian. A comparative test shows the\nmodel capacity to produce themes which have a good resemblance with those\nproduced independently by human researchers. The main implication of this study\nis that pre-trained LLMs may thus be suitable to support analysis in\nmultilingual situations, so long as the language is supported by the model\nused.", "published": "2024-04-12 14:10:09", "link": "http://arxiv.org/abs/2404.08488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VertAttack: Taking advantage of Text Classifiers' horizontal vision", "abstract": "Text classification systems have continuously improved in performance over\nthe years. However, nearly all current SOTA classifiers have a similar\nshortcoming, they process text in a horizontal manner. Vertically written words\nwill not be recognized by a classifier. In contrast, humans are easily able to\nrecognize and read words written both horizontally and vertically. Hence, a\nhuman adversary could write problematic words vertically and the meaning would\nstill be preserved to other humans. We simulate such an attack, VertAttack.\nVertAttack identifies which words a classifier is reliant on and then rewrites\nthose words vertically. We find that VertAttack is able to greatly drop the\naccuracy of 4 different transformer models on 5 datasets. For example, on the\nSST2 dataset, VertAttack is able to drop RoBERTa's accuracy from 94 to 13%.\nFurthermore, since VertAttack does not replace the word, meaning is easily\npreserved. We verify this via a human study and find that crowdworkers are able\nto correctly label 77% perturbed texts perturbed, compared to 81% of the\noriginal texts. We believe VertAttack offers a look into how humans might\ncircumvent classifiers in the future and thus inspire a look into more robust\nalgorithms.", "published": "2024-04-12 15:32:17", "link": "http://arxiv.org/abs/2404.08538v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MoPE: Mixture of Prefix Experts for Zero-Shot Dialogue State Tracking", "abstract": "Zero-shot dialogue state tracking (DST) transfers knowledge to unseen\ndomains, reducing the cost of annotating new datasets. Previous zero-shot DST\nmodels mainly suffer from domain transferring and partial prediction problems.\nTo address these challenges, we propose Mixture of Prefix Experts (MoPE) to\nestablish connections between similar slots in different domains, which\nstrengthens the model transfer performance in unseen domains. Empirical results\ndemonstrate that MoPE-DST achieves the joint goal accuracy of 57.13% on\nMultiWOZ2.1 and 55.40% on SGD.", "published": "2024-04-12 15:57:41", "link": "http://arxiv.org/abs/2404.08559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetic Dataset Creation and Fine-Tuning of Transformer Models for\n  Question Answering in Serbian", "abstract": "In this paper, we focus on generating a synthetic question answering (QA)\ndataset using an adapted Translate-Align-Retrieve method. Using this method, we\ncreated the largest Serbian QA dataset of more than 87K samples, which we name\nSQuAD-sr. To acknowledge the script duality in Serbian, we generated both\nCyrillic and Latin versions of the dataset. We investigate the dataset quality\nand use it to fine-tune several pre-trained QA models. Best results were\nobtained by fine-tuning the BERTi\\'c model on our Latin SQuAD-sr dataset,\nachieving 73.91% Exact Match and 82.97% F1 score on the benchmark XQuAD\ndataset, which we translated into Serbian for the purpose of evaluation. The\nresults show that our model exceeds zero-shot baselines, but fails to go beyond\nhuman performance. We note the advantage of using a monolingual pre-trained\nmodel over multilingual, as well as the performance increase gained by using\nLatin over Cyrillic. By performing additional analysis, we show that questions\nabout numeric values or dates are more likely to be answered correctly than\nother types of questions. Finally, we conclude that SQuAD-sr is of sufficient\nquality for fine-tuning a Serbian QA model, in the absence of a manually\ncrafted and annotated dataset.", "published": "2024-04-12 17:27:54", "link": "http://arxiv.org/abs/2404.08617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation", "abstract": "Large Language Models (LLMs) have proved effective and efficient in\ngenerating code, leading to their utilization within the hardware design\nprocess. Prior works evaluating LLMs' abilities for register transfer level\ncode generation solely focus on functional correctness. However, the creativity\nassociated with these LLMs, or the ability to generate novel and unique\nsolutions, is a metric not as well understood, in part due to the challenge of\nquantifying this quality.\n  To address this research gap, we present CreativeEval, a framework for\nevaluating the creativity of LLMs within the context of generating hardware\ndesigns. We quantify four creative sub-components, fluency, flexibility,\noriginality, and elaboration, through various prompting and post-processing\ntechniques. We then evaluate multiple popular LLMs (including GPT models,\nCodeLlama, and VeriGen) upon this creativity metric, with results indicating\nGPT-3.5 as the most creative model in generating hardware designs.", "published": "2024-04-12 20:41:47", "link": "http://arxiv.org/abs/2404.08806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constrained C-Test Generation via Mixed-Integer Programming", "abstract": "This work proposes a novel method to generate C-Tests; a deviated form of\ncloze tests (a gap filling exercise) where only the last part of a word is\nturned into a gap. In contrast to previous works that only consider varying the\ngap size or gap placement to achieve locally optimal solutions, we propose a\nmixed-integer programming (MIP) approach. This allows us to consider gap size\nand placement simultaneously, achieving globally optimal solutions, and to\ndirectly integrate state-of-the-art models for gap difficulty prediction into\nthe optimization problem. A user study with 40 participants across four C-Test\ngeneration strategies (including GPT-4) shows that our approach (MIP)\nsignificantly outperforms two of the baseline strategies (based on gap\nplacement and GPT-4); and performs on-par with the third (based on gap size).\nOur analysis shows that GPT-4 still struggles to fulfill explicit constraints\nduring generation and that MIP produces C-Tests that correlate best with the\nperceived difficulty. We publish our code, model, and collected data consisting\nof 32 English C-Tests with 20 gaps each (totaling 3,200 individual gap\nresponses) under an open source license.", "published": "2024-04-12 21:35:21", "link": "http://arxiv.org/abs/2404.08821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretraining and Updates of Domain-Specific LLM: A Case Study in the\n  Japanese Business Domain", "abstract": "The development of Large Language Models (LLMs) in various languages has been\nadvancing, but the combination of non-English languages with domain-specific\ncontexts remains underexplored. This paper presents our findings from training\nand evaluating a Japanese business domain-specific LLM designed to better\nunderstand business-related documents, such as the news on current affairs,\ntechnical reports, and patents. Additionally, LLMs in this domain require\nregular updates to incorporate the most recent knowledge. Therefore, we also\nreport our findings from the first experiments and evaluations involving\nupdates to this LLM using the latest article data, which is an important\nproblem setting that has not been addressed in previous research. From our\nexperiments on a newly created benchmark dataset for question answering in the\ntarget domain, we found that (1) our pretrained model improves QA accuracy\nwithout losing general knowledge, and (2) a proper mixture of the latest and\nolder texts in the training data for the update is necessary. Our pretrained\nmodel and business domain benchmark are publicly available to support further\nstudies.", "published": "2024-04-12 06:21:48", "link": "http://arxiv.org/abs/2404.08262v3", "categories": ["cs.CL", "cs.AI", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "The Integration of Semantic and Structural Knowledge in Knowledge Graph\n  Entity Typing", "abstract": "The Knowledge Graph Entity Typing (KGET) task aims to predict missing type\nannotations for entities in knowledge graphs. Recent works only utilize the\n\\textit{\\textbf{structural knowledge}} in the local neighborhood of entities,\ndisregarding \\textit{\\textbf{semantic knowledge}} in the textual\nrepresentations of entities, relations, and types that are also crucial for\ntype inference. Additionally, we observe that the interaction between semantic\nand structural knowledge can be utilized to address the false-negative problem.\nIn this paper, we propose a novel \\textbf{\\underline{S}}emantic and\n\\textbf{\\underline{S}}tructure-aware KG \\textbf{\\underline{E}}ntity\n\\textbf{\\underline{T}}yping~{(SSET)} framework, which is composed of three\nmodules. First, the \\textit{Semantic Knowledge Encoding} module encodes factual\nknowledge in the KG with a Masked Entity Typing task. Then, the\n\\textit{Structural Knowledge Aggregation} module aggregates knowledge from the\nmulti-hop neighborhood of entities to infer missing types. Finally, the\n\\textit{Unsupervised Type Re-ranking} module utilizes the inference results\nfrom the two models above to generate type predictions that are robust to\nfalse-negative samples. Extensive experiments show that SSET significantly\noutperforms existing state-of-the-art methods.", "published": "2024-04-12 08:17:44", "link": "http://arxiv.org/abs/2404.08313v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward a Theory of Tokenization in LLMs", "abstract": "While there has been a large body of research attempting to circumvent\ntokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the\ncurrent consensus is that it is a necessary initial step for designing\nstate-of-the-art performant language models. In this paper, we investigate\ntokenization from a theoretical point of view by studying the behavior of\ntransformers on simple data generating processes. When trained on data drawn\nfrom certain simple $k^{\\text{th}}$-order Markov processes for $k > 1$,\ntransformers exhibit a surprising phenomenon - in the absence of tokenization,\nthey empirically fail to learn the right distribution and predict characters\naccording to a unigram model (Makkuva et al., 2024). With the addition of\ntokenization, however, we empirically observe that transformers break through\nthis barrier and are able to model the probabilities of sequences drawn from\nthe source near-optimally, achieving small cross-entropy loss. With this\nobservation as starting point, we study the end-to-end cross-entropy loss\nachieved by transformers with and without tokenization. With the appropriate\ntokenization, we show that even the simplest unigram models (over tokens)\nlearnt by transformers are able to model the probability of sequences drawn\nfrom $k^{\\text{th}}$-order Markov sources near optimally. Our analysis provides\na justification for the use of tokenization in practice through studying the\nbehavior of transformers on Markovian data.", "published": "2024-04-12 09:01:14", "link": "http://arxiv.org/abs/2404.08335v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Look at the Text: Instruction-Tuned Language Models are More Robust\n  Multiple Choice Selectors than You Think", "abstract": "Multiple choice questions (MCQs) are commonly used to evaluate the\ncapabilities of large language models (LLMs). One common way to evaluate the\nmodel response is to rank the candidate answers based on the log probability of\nthe first token prediction. An alternative way is to examine the text output.\nPrior work has shown that first token probabilities lack robustness to changes\nin MCQ phrasing, and that first token probabilities do not match text answers\nfor instruction-tuned models. Therefore, in this paper, we investigate the\nrobustness of text answers. We show that the text answers are more robust to\nquestion perturbations than the first token probabilities, when the first token\nanswers mismatch the text answers. The difference in robustness increases as\nthe mismatch rate becomes greater. As the mismatch reaches over 50\\%, the text\nanswer is more robust to option order changes than the debiased first token\nprobabilities using state-of-the-art debiasing methods such as PriDe. Our\nfindings provide further evidence for the benefits of text answer evaluation\nover first token probability evaluation.", "published": "2024-04-12 10:36:15", "link": "http://arxiv.org/abs/2404.08382v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher\n  Language Selection and Cross-lingual Self-Distillation", "abstract": "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive\nperformance on cross-language tasks, yet significant performance disparities\nexist across different languages within the same mPLM. Previous studies\nendeavored to narrow these disparities by supervise fine-tuning the mPLMs with\nmultilingual data. However, obtaining labeled multilingual data is\ntime-consuming, and fine-tuning mPLM with limited labeled multilingual data\nmerely encapsulates the knowledge specific to the labeled data. Therefore, we\nintroduce ALSACE to leverage the learned knowledge from the well-performing\nlanguages to guide under-performing ones within the same mPLM, eliminating the\nneed for additional labeled multilingual data. Experiments show that ALSACE\neffectively mitigates language-level performance disparity across various mPLMs\nwhile showing the competitive performance on different multilingual NLU tasks,\nranging from full resource to limited resource settings. The code for our\napproach is available at https://github.com/pkunlp-icler/ALSACE.", "published": "2024-04-12 14:19:16", "link": "http://arxiv.org/abs/2404.08491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery", "abstract": "In the rapidly evolving field of artificial intelligence, the ability to\nharness and integrate knowledge across various domains stands as a paramount\nchallenge and opportunity. This study introduces a novel approach to\ncross-domain knowledge discovery through the deployment of multi-AI agents,\neach specialized in distinct knowledge domains. These AI agents, designed to\nfunction as domain-specific experts, collaborate in a unified framework to\nsynthesize and provide comprehensive insights that transcend the limitations of\nsingle-domain expertise. By facilitating seamless interaction among these\nagents, our platform aims to leverage the unique strengths and perspectives of\neach, thereby enhancing the process of knowledge discovery and decision-making.\nWe present a comparative analysis of the different multi-agent workflow\nscenarios evaluating their performance in terms of efficiency, accuracy, and\nthe breadth of knowledge integration. Through a series of experiments involving\ncomplex, interdisciplinary queries, our findings demonstrate the superior\ncapability of domain specific multi-AI agent system in identifying and bridging\nknowledge gaps. This research not only underscores the significance of\ncollaborative AI in driving innovation but also sets the stage for future\nadvancements in AI-driven, cross-disciplinary research and application. Our\nmethods were evaluated on a small pilot data and it showed a trend we expected,\nif we increase the amount of data we custom train the agents, the trend is\nexpected to be more smooth.", "published": "2024-04-12 14:50:41", "link": "http://arxiv.org/abs/2404.08511v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The Generation Gap: Exploring Age Bias in the Value Systems of Large\n  Language Models", "abstract": "We explore the alignment of values in Large Language Models (LLMs) with\nspecific age groups, leveraging data from the World Value Survey across\nthirteen categories. Through a diverse set of prompts tailored to ensure\nresponse robustness, we find a general inclination of LLM values towards\nyounger demographics, especially when compared to the US population. Although a\ngeneral inclination can be observed, we also found that this inclination toward\nyounger groups can be different across different value categories.\nAdditionally, we explore the impact of incorporating age identity information\nin prompts and observe challenges in mitigating value discrepancies with\ndifferent age cohorts. Our findings highlight the age bias in LLMs and provide\ninsights for future work. Materials for our analysis are available at \\url{\nhttps://github.com/MichiganNLP/Age-Bias-In-LLMs}", "published": "2024-04-12 18:36:20", "link": "http://arxiv.org/abs/2404.08760v4", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CATS: Contextually-Aware Thresholding for Sparsity in Large Language\n  Models", "abstract": "Large Language Models (LLMs) have dramatically advanced AI applications, yet\ntheir deployment remains challenging due to their immense inference costs.\nRecent studies ameliorate the computational costs of LLMs by increasing their\nactivation sparsity but suffer from significant performance degradation on\ndownstream tasks. In this work, we introduce a new framework for sparsifying\nthe activations of base LLMs and reducing inference costs, dubbed Contextually\nAware Thresholding for Sparsity (CATS). CATS is relatively simple, easy to\nimplement, and highly effective. At the heart of our framework is a new\nnon-linear activation function. We demonstrate that CATS can be applied to\nvarious base models, including Mistral-7B and Llama2-7B, and outperforms\nexisting sparsification techniques in downstream task performance. More\nprecisely, CATS-based models often achieve downstream task performance within\n1-2% of their base models without any fine-tuning and even at activation\nsparsity levels of 50%. Furthermore, CATS-based models converge faster and\ndisplay better task performance than competing techniques when fine-tuning is\napplied. Finally, we develop a custom GPU kernel for efficient implementation\nof CATS that translates the activation of sparsity of CATS to real wall-clock\ntime speedups. Our custom kernel implementation of CATS results in a ~15%\nimprovement in wall-clock inference latency of token generation on both\nLlama-7B and Mistral-7B.", "published": "2024-04-12 18:42:18", "link": "http://arxiv.org/abs/2404.08763v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Megalodon: Efficient LLM Pretraining and Inference with Unlimited\n  Context Length", "abstract": "The quadratic complexity and weak length extrapolation of Transformers limits\ntheir ability to scale to long sequences, and while sub-quadratic solutions\nlike linear attention and state space models exist, they empirically\nunderperform Transformers in pretraining efficiency and downstream task\naccuracy. We introduce Megalodon, a neural architecture for efficient sequence\nmodeling with unlimited context length. Megalodon inherits the architecture of\nMega (exponential moving average with gated attention), and further introduces\nmultiple technical components to improve its capability and stability,\nincluding complex exponential moving average (CEMA), timestep normalization\nlayer, normalized attention mechanism and pre-norm with two-hop residual\nconfiguration. In a controlled head-to-head comparison with Llama2, Megalodon\nachieves better efficiency than Transformer in the scale of 7 billion\nparameters and 2 trillion training tokens. Megalodon reaches a training loss of\n1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code:\nhttps://github.com/XuezheMax/megalodon", "published": "2024-04-12 20:28:14", "link": "http://arxiv.org/abs/2404.08801v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Measuring the Quality of Answers in Political Q&As with Large Language\n  Models", "abstract": "This article proposes a new approach for assessing the quality of answers in\npolitical question-and-answer sessions. We measure the quality of an answer\nbased on how easily and accurately it can be recognized in a random set of\ncandidate answers given the question's text. This measure reflects the answer's\nrelevance and depth of engagement with the question. Like semantic search, we\ncan implement this approach by training a language model on the corpus of\nobserved questions and answers without additional human-labeled data. We\nshowcase and validate our methodology within the context of the Question Period\nin the Canadian House of Commons. Our analysis reveals that while some answers\nhave a weak semantic connection to questions, hinting at some evasion or\nobfuscation, they are generally at least moderately relevant, far exceeding\nwhat we would expect from random replies. We also find a meaningful correlation\nbetween answer quality and the party affiliation of the members of Parliament\nasking the questions.", "published": "2024-04-12 21:16:53", "link": "http://arxiv.org/abs/2404.08816v5", "categories": ["cs.CL", "econ.EM"], "primary_category": "cs.CL"}
{"title": "Experimental Design for Active Transductive Inference in Large Language\n  Models", "abstract": "One emergent ability of large language models (LLMs) is that query-specific\nexamples can be included in the prompt at inference time. In this work, we use\nactive learning for adaptive prompt design and call it Active In-context Prompt\nDesign (AIPD). We design the LLM prompt by adaptively choosing few-shot\nexamples from a training set to optimize performance on a test set. The\ntraining examples are initially unlabeled and we obtain the label of the most\ninformative ones, which maximally reduces uncertainty in the LLM prediction. We\npropose two algorithms, GO and SAL, which differ in how the few-shot examples\nare chosen. We analyze these algorithms in linear models: first GO and then use\nits equivalence with SAL. We experiment with many different tasks in small,\nmedium-sized, and large language models; and show that GO and SAL outperform\nother methods for choosing few-shot examples in the LLM prompt at inference\ntime.", "published": "2024-04-12 23:27:46", "link": "http://arxiv.org/abs/2404.08846v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language Model Prompt Selection via Simulation Optimization", "abstract": "With the advancement in generative language models, the selection of prompts\nhas gained significant attention in recent years. A prompt is an instruction or\ndescription provided by the user, serving as a guide for the generative\nlanguage model in content generation. Despite existing methods for prompt\nselection that are based on human labor, we consider facilitating this\nselection through simulation optimization, aiming to maximize a pre-defined\nscore for the selected prompt. Specifically, we propose a two-stage framework.\nIn the first stage, we determine a feasible set of prompts in sufficient\nnumbers, where each prompt is represented by a moderate-dimensional vector. In\nthe subsequent stage for evaluation and selection, we construct a surrogate\nmodel of the score regarding the moderate-dimensional vectors that represent\nthe prompts. We propose sequentially selecting the prompt for evaluation based\non this constructed surrogate model. We prove the consistency of the sequential\nevaluation procedure in our framework. We also conduct numerical experiments to\ndemonstrate the efficacy of our proposed framework, providing practical\ninstructions for implementation.", "published": "2024-04-12 00:03:56", "link": "http://arxiv.org/abs/2404.08164v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Reducing hallucination in structured outputs via Retrieval-Augmented\n  Generation", "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its\npropensity to hallucinate. While large language models (LLM) have taken the\nworld by storm, without eliminating or at least reducing hallucinations,\nreal-world GenAI systems may face challenges in user adoption. In the process\nof deploying an enterprise application that produces workflows based on natural\nlanguage requirements, we devised a system leveraging Retrieval Augmented\nGeneration (RAG) to greatly improve the quality of the structured output that\nrepresents such workflows. Thanks to our implementation of RAG, our proposed\nsystem significantly reduces hallucinations in the output and improves the\ngeneralization of our LLM in out-of-domain settings. In addition, we show that\nusing a small, well-trained retriever encoder can reduce the size of the\naccompanying LLM, thereby making deployments of LLM-based systems less\nresource-intensive.", "published": "2024-04-12 01:42:09", "link": "http://arxiv.org/abs/2404.08189v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Relational Prompt-based Pre-trained Language Models for Social Event\n  Detection", "abstract": "Social Event Detection (SED) aims to identify significant events from social\nstreams, and has a wide application ranging from public opinion analysis to\nrisk management. In recent years, Graph Neural Network (GNN) based solutions\nhave achieved state-of-the-art performance. However, GNN-based methods often\nstruggle with missing and noisy edges between messages, affecting the quality\nof learned message embedding. Moreover, these methods statically initialize\nnode embedding before training, which, in turn, limits the ability to learn\nfrom message texts and relations simultaneously. In this paper, we approach\nsocial event detection from a new perspective based on Pre-trained Language\nModels (PLMs), and present RPLM_SED (Relational prompt-based Pre-trained\nLanguage Models for Social Event Detection). We first propose a new pairwise\nmessage modeling strategy to construct social messages into message pairs with\nmulti-relational sequences. Secondly, a new multi-relational prompt-based\npairwise message learning mechanism is proposed to learn more comprehensive\nmessage representation from message pairs with multi-relational prompts using\nPLMs. Thirdly, we design a new clustering constraint to optimize the encoding\nprocess by enhancing intra-cluster compactness and inter-cluster dispersion,\nmaking the message representation more distinguishable. We evaluate the\nRPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model\nachieves state-of-the-art performance in offline, online, low-resource, and\nlong-tail distribution scenarios for social event detection tasks.", "published": "2024-04-12 06:23:07", "link": "http://arxiv.org/abs/2404.08263v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Subtoxic Questions: Dive Into Attitude Change of LLM's Response in\n  Jailbreak Attempts", "abstract": "As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and\nmore attention, it is of great significance to raise a generalized research\nparadigm to evaluate attack strengths and a basic model to conduct subtler\nexperiments. In this paper, we propose a novel approach by focusing on a set of\ntarget questions that are inherently more sensitive to jailbreak prompts,\naiming to circumvent the limitations posed by enhanced LLM security. Through\ndesigning and analyzing these sensitive questions, this paper reveals a more\neffective method of identifying vulnerabilities in LLMs, thereby contributing\nto the advancement of LLM security. This research not only challenges existing\njailbreaking methodologies but also fortifies LLMs against potential exploits.", "published": "2024-04-12 08:08:44", "link": "http://arxiv.org/abs/2404.08309v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Improving Health Question Answering with Reliable and Time-Aware\n  Evidence Retrieval", "abstract": "In today's digital world, seeking answers to health questions on the Internet\nis a common practice. However, existing question answering (QA) systems often\nrely on using pre-selected and annotated evidence documents, thus making them\ninadequate for addressing novel questions. Our study focuses on the open-domain\nQA setting, where the key challenge is to first uncover relevant evidence in\nlarge knowledge bases. By utilizing the common retrieve-then-read QA pipeline\nand PubMed as a trustworthy collection of medical research documents, we answer\nhealth questions from three diverse datasets. We modify different retrieval\nsettings to observe their influence on the QA pipeline's performance, including\nthe number of retrieved documents, sentence selection process, the publication\nyear of articles, and their number of citations. Our results reveal that\ncutting down on the amount of retrieved documents and favoring more recent and\nhighly cited documents can improve the final macro F1 score up to 10%. We\ndiscuss the results, highlight interesting examples, and outline challenges for\nfuture research, like managing evidence disagreement and crafting user-friendly\nexplanations.", "published": "2024-04-12 09:56:12", "link": "http://arxiv.org/abs/2404.08359v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning representations of learning representations", "abstract": "The ICLR conference is unique among the top machine learning conferences in\nthat all submitted papers are openly available. Here we present the ICLR\ndataset consisting of abstracts of all 24 thousand ICLR submissions from\n2017-2024 with meta-data, decision scores, and custom keyword-based labels. We\nfind that on this dataset, bag-of-words representation outperforms most\ndedicated sentence transformer models in terms of $k$NN classification\naccuracy, and the top performing language models barely outperform TF-IDF. We\nsee this as a challenge for the NLP community. Furthermore, we use the ICLR\ndataset to study how the field of machine learning has changed over the last\nseven years, finding some improvement in gender balance. Using a 2D embedding\nof the abstracts' texts, we describe a shift in research topics from 2017 to\n2024 and identify hedgehogs and foxes among the authors with the highest number\nof ICLR submissions.", "published": "2024-04-12 11:30:16", "link": "http://arxiv.org/abs/2404.08403v1", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdapterSwap: Continuous Training of LLMs with Data Removal and\n  Access-Control Guarantees", "abstract": "Large language models (LLMs) are increasingly capable of completing knowledge\nintensive tasks by recalling information from a static pretraining corpus. Here\nwe are concerned with LLMs in the context of evolving data requirements. For\ninstance: batches of new data that are introduced periodically; subsets of data\nwith user-based access controls; or requirements on dynamic removal of\ndocuments with guarantees that associated knowledge cannot be recalled. We wish\nto satisfy these requirements while at the same time ensuring a model does not\nforget old information when new data becomes available. To address these\nissues, we introduce AdapterSwap, a training and inference scheme that\norganizes knowledge from a data collection into a set of low-rank adapters,\nwhich are dynamically composed during inference. Our experiments demonstrate\nAdapterSwap's ability to support efficient continual learning, while also\nenabling organizations to have fine-grained control over data access and\ndeletion.", "published": "2024-04-12 12:06:02", "link": "http://arxiv.org/abs/2404.08417v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Decoding AI: The inside story of data analysis in ChatGPT", "abstract": "As a result of recent advancements in generative AI, the field of Data\nScience is prone to various changes. This review critically examines the Data\nAnalysis (DA) capabilities of ChatGPT assessing its performance across a wide\nrange of tasks. While DA provides researchers and practitioners with\nunprecedented analytical capabilities, it is far from being perfect, and it is\nimportant to recognize and address its limitations.", "published": "2024-04-12 13:57:30", "link": "http://arxiv.org/abs/2404.08480v1", "categories": ["cs.LG", "cs.CL", "stat.CO"], "primary_category": "cs.LG"}
{"title": "Dataset Reset Policy Optimization for RLHF", "abstract": "Reinforcement Learning (RL) from Human Preference-based feedback is a popular\nparadigm for fine-tuning generative models, which has produced impressive\nmodels such as GPT-4 and Claude3 Opus. This framework often consists of two\nsteps: learning a reward model from an offline preference dataset followed by\nrunning online RL to optimize the learned reward model. In this work,\nleveraging the idea of reset, we propose a new RLHF algorithm with provable\nguarantees. Motivated by the fact that offline preference dataset provides\ninformative states (i.e., data that is preferred by the labelers), our new\nalgorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing\noffline preference dataset into the online policy training procedure via\ndataset reset: it directly resets the policy optimizer to the states in the\noffline dataset, instead of always starting from the initial state\ndistribution. In theory, we show that DR-PO learns to perform at least as good\nas any policy that is covered by the offline dataset under general function\napproximation with finite sample complexity. In experiments, we demonstrate\nthat on both the TL;DR summarization and the Anthropic Helpful Harmful (HH)\ndataset, the generation from DR-PO is better than that from Proximal Policy\nOptimization (PPO) and Direction Preference Optimization (DPO), under the\nmetric of GPT4 win-rate. Code for this work can be found at\nhttps://github.com/Cornell-RL/drpo.", "published": "2024-04-12 14:25:49", "link": "http://arxiv.org/abs/2404.08495v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction", "abstract": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.", "published": "2024-04-12 14:46:15", "link": "http://arxiv.org/abs/2404.08509v2", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from\n  Human Feedback for LLMs", "abstract": "State-of-the-art large language models (LLMs) have become indispensable tools\nfor various tasks. However, training LLMs to serve as effective assistants for\nhumans requires careful consideration. A promising approach is reinforcement\nlearning from human feedback (RLHF), which leverages human feedback to update\nthe model in accordance with human preferences and mitigate issues like\ntoxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely\nentangled with initial design choices that popularized the method and current\nresearch focuses on augmenting those choices rather than fundamentally\nimproving the framework. In this paper, we analyze RLHF through the lens of\nreinforcement learning principles to develop an understanding of its\nfundamentals, dedicating substantial focus to the core component of RLHF -- the\nreward model. Our study investigates modeling choices, caveats of function\napproximation, and their implications on RLHF training algorithms, highlighting\nthe underlying assumptions made about the expressivity of reward. Our analysis\nimproves the understanding of the role of reward models and methods for their\ntraining, concurrently revealing limitations of the current methodology. We\ncharacterize these limitations, including incorrect generalization, model\nmisspecification, and the sparsity of feedback, along with their impact on the\nperformance of a language model. The discussion and analysis are substantiated\nby a categorical review of current literature, serving as a reference for\nresearchers and practitioners to understand the challenges of RLHF and build\nupon existing efforts.", "published": "2024-04-12 15:54:15", "link": "http://arxiv.org/abs/2404.08555v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Small Models Are (Still) Effective Cross-Domain Argument Extractors", "abstract": "Effective ontology transfer has been a major goal of recent work on event\nargument extraction (EAE). Two methods in particular -- question answering (QA)\nand template infilling (TI) -- have emerged as promising approaches to this\nproblem. However, detailed explorations of these techniques' ability to\nactually enable this transfer are lacking. In this work, we provide such a\nstudy, exploring zero-shot transfer using both techniques on six major EAE\ndatasets at both the sentence and document levels. Further, we challenge the\ngrowing reliance on LLMs for zero-shot extraction, showing that vastly smaller\nmodels trained on an appropriate source ontology can yield zero-shot\nperformance superior to that of GPT-3.5 or GPT-4.", "published": "2024-04-12 16:23:41", "link": "http://arxiv.org/abs/2404.08579v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT Transforming Academics' Writing Style?", "abstract": "Based on one million arXiv papers submitted from May 2018 to January 2024, we\nassess the textual density of ChatGPT's writing style in their abstracts\nthrough a statistical analysis of word frequency changes. Our model is\ncalibrated and validated on a mixture of real abstracts and ChatGPT-modified\nabstracts (simulated data) after a careful noise analysis. The words used for\nestimation are not fixed but adaptive, including those with decreasing\nfrequency. We find that large language models (LLMs), represented by ChatGPT,\nare having an increasing impact on arXiv abstracts, especially in the field of\ncomputer science, where the fraction of LLM-style abstracts is estimated to be\napproximately 35%, if we take the responses of GPT-3.5 to one simple prompt,\n\"revise the following sentences\", as a baseline. We conclude with an analysis\nof both positive and negative aspects of the penetration of LLMs into\nacademics' writing style.", "published": "2024-04-12 17:41:05", "link": "http://arxiv.org/abs/2404.08627v2", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inheritune: Training Smaller Yet More Attentive Language Models", "abstract": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious natural language processing tasks, primarily due to the transformer\narchitecture and its self-attention mechanism. However, we observe that in\nstandard decoder-style LLMs, attention matrices degenerate to single-column for\ndeeper layers. Layers in this state are unable to learn anything meaningful and\nmostly redundant; we refer to these as lazy layers. The goal of this paper is\nto train smaller models by eliminating this structural inefficiency without\ncompromising performance.\n  Motivated by this observation, we propose Inheritune, a simple yet effective\ntraining recipe for developing smaller, high-performing language models.\nSmaller models trained with Inheritune, inherit early transformer layers from a\nlarger pre-trained model, then retrain and progressively expand until they\nmatch or exceed the performance of the larger model. We demonstrate that\nInheritune enables the training of various sizes of GPT-2 models on datasets\nlike OpenWebText-9B and FineWeb_edu. Models trained with Inheritune, despite\nhaving significantly fewer layers, match or even surpass the performance of\ntheir larger counterparts. For instance, our 16-layer GPT-2 medium variant\nachieves comparable performance to the standard 24-layer GPT-2 medium model.\nCode is available at https://github.com/sanyalsunny111/LLM-Inheritune.", "published": "2024-04-12 17:53:34", "link": "http://arxiv.org/abs/2404.08634v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Contrastive Learning for Long-Tailed Multi-Label Text\n  Classification", "abstract": "Learning an effective representation in multi-label text classification\n(MLTC) is a significant challenge in NLP. This challenge arises from the\ninherent complexity of the task, which is shaped by two key factors: the\nintricate connections between labels and the widespread long-tailed\ndistribution of the data. To overcome this issue, one potential approach\ninvolves integrating supervised contrastive learning with classical supervised\nloss functions. Although contrastive learning has shown remarkable performance\nin multi-class classification, its impact in the multi-label framework has not\nbeen thoroughly investigated. In this paper, we conduct an in-depth study of\nsupervised contrastive learning and its influence on representation in MLTC\ncontext. We emphasize the importance of considering long-tailed data\ndistributions to build a robust representation space, which effectively\naddresses two critical challenges associated with contrastive learning that we\nidentify: the \"lack of positives\" and the \"attraction-repulsion imbalance\".\nBuilding on this insight, we introduce a novel contrastive loss function for\nMLTC. It attains Micro-F1 scores that either match or surpass those obtained\nwith other frequently employed loss functions, and demonstrates a significant\nimprovement in Macro-F1 scores across three multi-label datasets.", "published": "2024-04-12 11:12:16", "link": "http://arxiv.org/abs/2404.08720v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs\n  versus Traditional Relational Databases", "abstract": "Large Language Models (LLMs) can automate or substitute different types of\ntasks in the software engineering process. This study evaluates the resource\nutilization and accuracy of LLM in interpreting and executing natural language\nqueries against traditional SQL within relational database management systems.\nWe empirically examine the resource utilization and accuracy of nine LLMs\nvarying from 7 to 34 Billion parameters, including Llama2 7B, Llama2 13B,\nMistral, Mixtral, Optimus-7B, SUS-chat-34B, platypus-yi-34b,\nNeuralHermes-2.5-Mistral-7B and Starling-LM-7B-alpha, using a small transaction\ndataset. Our findings indicate that using LLMs for database queries incurs\nsignificant energy overhead (even small and quantized models), making it an\nenvironmentally unfriendly approach. Therefore, we advise against replacing\nrelational databases with LLMs due to their substantial resource utilization.", "published": "2024-04-12 16:44:28", "link": "http://arxiv.org/abs/2404.08727v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "68-04", "H.2.m"], "primary_category": "cs.DB"}
{"title": "JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large\n  Language Models", "abstract": "The proliferation of large language models (LLMs) has underscored concerns\nregarding their security vulnerabilities, notably against jailbreak attacks,\nwhere adversaries design jailbreak prompts to circumvent safety mechanisms for\npotential misuse. Addressing these concerns necessitates a comprehensive\nanalysis of jailbreak prompts to evaluate LLMs' defensive capabilities and\nidentify potential weaknesses. However, the complexity of evaluating jailbreak\nperformance and understanding prompt characteristics makes this analysis\nlaborious. We collaborate with domain experts to characterize problems and\npropose an LLM-assisted framework to streamline the analysis process. It\nprovides automatic jailbreak assessment to facilitate performance evaluation\nand support analysis of components and keywords in prompts. Based on the\nframework, we design JailbreakLens, a visual analysis system that enables users\nto explore the jailbreak performance against the target model, conduct\nmulti-level analysis of prompt characteristics, and refine prompt instances to\nverify findings. Through a case study, technical evaluations, and expert\ninterviews, we demonstrate our system's effectiveness in helping users evaluate\nmodel security and identify model weaknesses.", "published": "2024-04-12 19:54:42", "link": "http://arxiv.org/abs/2404.08793v1", "categories": ["cs.CR", "cs.CL", "cs.HC"], "primary_category": "cs.CR"}
{"title": "Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit\n  Distance", "abstract": "This paper revisits recent code similarity evaluation metrics, particularly\nfocusing on the application of Abstract Syntax Tree (AST) editing distance in\ndiverse programming languages. In particular, we explore the usefulness of\nthese metrics and compare them to traditional sequence similarity metrics. Our\nexperiments showcase the effectiveness of AST editing distance in capturing\nintricate code structures, revealing a high correlation with established\nmetrics. Furthermore, we explore the strengths and weaknesses of AST editing\ndistance and prompt-based GPT similarity scores in comparison to BLEU score,\nexecution match, and Jaccard Similarity. We propose, optimize, and publish an\nadaptable metric that demonstrates effectiveness across all tested languages,\nrepresenting an enhanced version of Tree Similarity of Edit Distance (TSED).", "published": "2024-04-12 21:28:18", "link": "http://arxiv.org/abs/2404.08817v2", "categories": ["cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "The Illusion of State in State-Space Models", "abstract": "State-space models (SSMs) have emerged as a potential alternative\narchitecture for building large language models (LLMs) compared to the\npreviously ubiquitous transformer architecture. One theoretical weakness of\ntransformers is that they cannot express certain kinds of sequential\ncomputation and state tracking (Merrill & Sabharwal, 2023), which SSMs are\nexplicitly designed to address via their close architectural similarity to\nrecurrent neural networks (RNNs). But do SSMs truly have an advantage (over\ntransformers) in expressive power for state tracking? Surprisingly, the answer\nis no. Our analysis reveals that the expressive power of SSMs is limited very\nsimilarly to transformers: SSMs cannot express computation outside the\ncomplexity class $\\mathsf{TC}^0$. In particular, this means they cannot solve\nsimple state-tracking problems like permutation composition. It follows that\nSSMs are provably unable to accurately track chess moves with certain notation,\nevaluate code, or track entities in a long narrative. To supplement our formal\nanalysis, we report experiments showing that Mamba-style SSMs indeed struggle\nwith state tracking. Thus, despite its recurrent formulation, the \"state\" in an\nSSM is an illusion: SSMs have similar expressiveness limitations to\nnon-recurrent models like transformers, which may fundamentally limit their\nability to solve real-world state-tracking problems.", "published": "2024-04-12 21:30:06", "link": "http://arxiv.org/abs/2404.08819v3", "categories": ["cs.LG", "cs.CC", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "BERT-LSH: Reducing Absolute Compute For Attention", "abstract": "This study introduces a novel BERT-LSH model that incorporates Locality\nSensitive Hashing (LSH) to approximate the attention mechanism in the BERT\narchitecture. We examine the computational efficiency and performance of this\nmodel compared to a standard baseline BERT model. Our findings reveal that\nBERT-LSH significantly reduces computational demand for the self-attention\nlayer while unexpectedly outperforming the baseline model in pretraining and\nfine-tuning tasks. These results suggest that the LSH-based attention mechanism\nnot only offers computational advantages but also may enhance the model's\nability to generalize from its training data. For more information, visit our\nGitHub repository: https://github.com/leo4life2/algoml-final", "published": "2024-04-12 22:35:00", "link": "http://arxiv.org/abs/2404.08836v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension\n  Transformation for Emotion Recognition in Conversations", "abstract": "Emotion Recognition in Conversations (ERC) is a popular task in natural\nlanguage processing, which aims to recognize the emotional state of the speaker\nin conversations. While current research primarily emphasizes contextual\nmodeling, there exists a dearth of investigation into effective multimodal\nfusion methods. We propose a novel framework called AIMDiT to solve the problem\nof multimodal fusion of deep features. Specifically, we design a Modality\nAugmentation Network which performs rich representation learning through\ndimension transformation of different modalities and parameter-efficient\ninception block. On the other hand, the Modality Interaction Network performs\ninteraction fusion of extracted inter-modal features and intra-modal features.\nExperiments conducted using our AIMDiT framework on the public benchmark\ndataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1\nmetrics compared to the state-of-the-art (SOTA) models.", "published": "2024-04-12 11:31:18", "link": "http://arxiv.org/abs/2407.00743v1", "categories": ["cs.MM", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path\n  Forward", "abstract": "While Large Language Models (LLMs) have seen widespread applications across\nnumerous fields, their limited interpretability poses concerns regarding their\nsafe operations from multiple aspects, e.g., truthfulness, robustness, and\nfairness. Recent research has started developing quality assurance methods for\nLLMs, introducing techniques such as offline detector-based or uncertainty\nestimation methods. However, these approaches predominantly concentrate on\npost-generation analysis, leaving the online safety analysis for LLMs during\nthe generation phase an unexplored area. To bridge this gap, we conduct in this\nwork a comprehensive evaluation of the effectiveness of existing online safety\nanalysis methods on LLMs. We begin with a pilot study that validates the\nfeasibility of detecting unsafe outputs in the early generation process.\nFollowing this, we establish the first publicly available benchmark of online\nsafety analysis for LLMs, including a broad spectrum of methods, models, tasks,\ndatasets, and evaluation metrics. Utilizing this benchmark, we extensively\nanalyze the performance of state-of-the-art online safety analysis methods on\nboth open-source and closed-source LLMs. This analysis reveals the strengths\nand weaknesses of individual methods and offers valuable insights into\nselecting the most appropriate method based on specific application scenarios\nand task requirements. Furthermore, we also explore the potential of using\nhybridization methods, i.e., combining multiple methods to derive a collective\nsafety conclusion, to enhance the efficacy of online safety analysis for LLMs.\nOur findings indicate a promising direction for the development of innovative\nand trustworthy quality assurance methodologies for LLMs, facilitating their\nreliable deployments across diverse domains.", "published": "2024-04-12 14:55:16", "link": "http://arxiv.org/abs/2404.08517v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Guided Masked Self-Distillation Modeling for Distributed Multimedia\n  Sensor Event Analysis", "abstract": "Observations with distributed sensors are essential in analyzing a series of\nhuman and machine activities (referred to as 'events' in this paper) in complex\nand extensive real-world environments. This is because the information obtained\nfrom a single sensor is often missing or fragmented in such an environment;\nobservations from multiple locations and modalities should be integrated to\nanalyze events comprehensively. However, a learning method has yet to be\nestablished to extract joint representations that effectively combine such\ndistributed observations. Therefore, we propose Guided Masked sELf-Distillation\nmodeling (Guided-MELD) for inter-sensor relationship modeling. The basic idea\nof Guided-MELD is to learn to supplement the information from the masked sensor\nwith information from other sensors needed to detect the event. Guided-MELD is\nexpected to enable the system to effectively distill the fragmented or\nredundant target event information obtained by the sensors without being overly\ndependent on any specific sensors. To validate the effectiveness of the\nproposed method in novel tasks of distributed multimedia sensor event analysis,\nwe recorded two new datasets that fit the problem setting: MM-Store and\nMM-Office. These datasets consist of human activities in a convenience store\nand an office, recorded using distributed cameras and microphones. Experimental\nresults on these datasets show that the proposed Guided-MELD improves event\ntagging and detection performance and outperforms conventional inter-sensor\nrelationship modeling methods. Furthermore, the proposed method performed\nrobustly even when sensors were reduced.", "published": "2024-04-12 06:23:48", "link": "http://arxiv.org/abs/2404.08264v1", "categories": ["cs.MM", "cs.CV", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Interactive Sonification for Health and Energy using ChucK and Unity", "abstract": "Sonification can provide valuable insights about data but most existing\napproaches are not designed to be controlled by the user in an interactive\nfashion. Interactions enable the designer of the sonification to more rapidly\nexperiment with sound design and allow the sonification to be modified in\nreal-time by interacting with various control parameters. In this paper, we\ndescribe two case studies of interactive sonification that utilize publicly\navailable datasets that have been described recently in the International\nConference on Auditory Display (ICAD). They are from the health and energy\ndomains: electroencephalogram (EEG) alpha wave data and air pollutant data\nconsisting of nitrogen dioxide, sulfur dioxide, carbon monoxide, and ozone. We\nshow how these sonfications can be recreated to support interaction utilizing a\ngeneral interactive sonification framework built using ChucK, Unity, and\nChunity. In addition to supporting typical sonification methods that are common\nin existing sonification toolkits, our framework introduces novel methods such\nas supporting discrete events, interleaved playback of multiple data streams\nfor comparison, and using frequency modulation (FM) synthesis in terms of one\ndata attribute modulating another. We also describe how these new\nfunctionalities can be used to improve the sonification experience of the two\ndatasets we have investigated.", "published": "2024-04-12 21:13:02", "link": "http://arxiv.org/abs/2404.08813v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
