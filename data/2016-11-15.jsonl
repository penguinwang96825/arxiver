{"title": "Knowledge Enhanced Hybrid Neural Network for Text Matching", "abstract": "Long text brings a big challenge to semantic matching due to their\ncomplicated semantic and syntactic structures. To tackle the challenge, we\nconsider using prior knowledge to help identify useful information and filter\nout noise to matching in long text. To this end, we propose a knowledge\nenhanced hybrid neural network (KEHNN). The model fuses prior knowledge into\nword representations by knowledge gates and establishes three matching channels\nwith words, sequential structures of sentences given by Gated Recurrent Units\n(GRU), and knowledge enhanced representations. The three channels are processed\nby a convolutional neural network to generate high level features for matching,\nand the features are synthesized as a matching score by a multilayer\nperceptron. The model extends the existing methods by conducting matching on\nwords, local structures of sentences, and global context of sentences.\nEvaluation results from extensive experiments on public data sets for question\nanswering and conversation show that KEHNN can significantly outperform\nthe-state-of-the-art matching models and particularly improve the performance\non pairs with long text.", "published": "2016-11-15 03:11:59", "link": "http://arxiv.org/abs/1611.04684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language\n  Inference", "abstract": "In this work we use the recent advances in representation learning to propose\na neural architecture for the problem of natural language inference. Our\napproach is aligned to mimic how a human does the natural language inference\nprocess given two statements. The model uses variants of Long Short Term Memory\n(LSTM), attention mechanism and composable neural networks, to carry out the\ntask. Each part of our model can be mapped to a clear functionality humans do\nfor carrying out the overall task of natural language inference. The model is\nend-to-end differentiable enabling training by stochastic gradient descent. On\nStanford Natural Language Inference(SNLI) dataset, the proposed model achieves\nbetter accuracy numbers than all published models in literature.", "published": "2016-11-15 08:48:22", "link": "http://arxiv.org/abs/1611.04741v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Multilingual Neural Machine Translation with Universal Encoder\n  and Decoder", "abstract": "In this paper, we present our first attempts in building a multilingual\nNeural Machine Translation framework under a unified approach. We are then able\nto employ attention-based NMT for many-to-many multilingual translation tasks.\nOur approach does not require any special treatment on the network architecture\nand it allows us to learn minimal number of free parameters in a standard way\nof training. Our approach has shown its effectiveness in an under-resourced\ntranslation scenario with considerable improvements up to 2.6 BLEU points. In\naddition, the approach has achieved interesting and promising results when\napplied in the translation task that there is no direct parallel corpus between\nsource and target languages.", "published": "2016-11-15 11:47:42", "link": "http://arxiv.org/abs/1611.04798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimDoc: Topic Sequence Alignment based Document Similarity Framework", "abstract": "Document similarity is the problem of estimating the degree to which a given\npair of documents has similar semantic content. An accurate document similarity\nmeasure can improve several enterprise relevant tasks such as document\nclustering, text mining, and question-answering. In this paper, we show that a\ndocument's thematic flow, which is often disregarded by bag-of-word techniques,\nis pivotal in estimating their similarity. To this end, we propose a novel\nsemantic document similarity framework, called SimDoc. We model documents as\ntopic-sequences, where topics represent latent generative clusters of related\nwords. Then, we use a sequence alignment algorithm to estimate their semantic\nsimilarity. We further conceptualize a novel mechanism to compute topic-topic\nsimilarity to fine tune our system. In our experiments, we show that SimDoc\noutperforms many contemporary bag-of-words techniques in accurately computing\ndocument similarity, and on practical applications such as document clustering.", "published": "2016-11-15 13:31:28", "link": "http://arxiv.org/abs/1611.04822v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Pivot Languages", "abstract": "While recent neural machine translation approaches have delivered\nstate-of-the-art performance for resource-rich language pairs, they suffer from\nthe data scarcity problem for resource-scarce language pairs. Although this\nproblem can be alleviated by exploiting a pivot language to bridge the source\nand target languages, the source-to-pivot and pivot-to-target translation\nmodels are usually independently trained. In this work, we introduce a joint\ntraining algorithm for pivot-based neural machine translation. We propose three\nmethods to connect the two models and enable them to interact with each other\nduring training. Experiments on Europarl and WMT corpora show that joint\ntraining of source-to-pivot and pivot-to-target models leads to significant\nimprovements over independent training across various languages.", "published": "2016-11-15 16:44:54", "link": "http://arxiv.org/abs/1611.04928v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Neural Sentence Ordering Using Pointer Network", "abstract": "Sentence ordering is one of important tasks in NLP. Previous works mainly\nfocused on improving its performance by using pair-wise strategy. However, it\nis nontrivial for pair-wise models to incorporate the contextual sentence\ninformation. In addition, error prorogation could be introduced by using the\npipeline strategy in pair-wise models. In this paper, we propose an end-to-end\nneural approach to address the sentence ordering problem, which uses the\npointer network (Ptr-Net) to alleviate the error propagation problem and\nutilize the whole contextual information. Experimental results show the\neffectiveness of the proposed model. Source codes and dataset of this paper are\navailable.", "published": "2016-11-15 17:38:10", "link": "http://arxiv.org/abs/1611.04953v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed\n  Social Media Text", "abstract": "This paper describes Centre for Development of Advanced Computing's (CDACM)\nsubmission to the shared task-'Tool Contest on POS tagging for Code-Mixed\nIndian Social Media (Facebook, Twitter, and Whatsapp) Text', collocated with\nICON-2016. The shared task was to predict Part of Speech (POS) tag at word\nlevel for a given text. The code-mixed text is generated mostly on social media\nby multilingual users. The presence of the multilingual words,\ntransliterations, and spelling variations make such content linguistically\ncomplex. In this paper, we propose an approach to POS tag code-mixed social\nmedia text using Recurrent Neural Network Language Model (RNN-LM) architecture.\nWe submitted the results for Hindi-English (hi-en), Bengali-English (bn-en),\nand Telugu-English (te-en) code-mixed data.", "published": "2016-11-15 19:02:35", "link": "http://arxiv.org/abs/1611.04989v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Word Length in Semantic Topology", "abstract": "A topological argument is presented concering the structure of semantic\nspace, based on the negative correlation between polysemy and word length. The\nresulting graph structure is applied to the modeling of free-recall\nexperiments, resulting in predictions on the comparative values of recall\nprobabilities. Associative recall is found to favor longer words whereas\nsequential recall is found to favor shorter words. Data from the PEERS\nexperiments of Lohnas et al. (2015) and Healey and Kahana (2016) confirm both\npredictons, with correlation coefficients $r_{seq}= -0.17$ and $r_{ass}=\n+0.17$. The argument is then applied to predicting global properties of list\nrecall, which leads to a novel explanation for the word-length effect based on\nthe optimization of retrieval strategies.", "published": "2016-11-15 14:12:41", "link": "http://arxiv.org/abs/1611.04842v1", "categories": ["q-bio.NC", "cs.CL", "91E10"], "primary_category": "q-bio.NC"}
{"title": "Interpreting the Syntactic and Social Elements of the Tweet\n  Representations via Elementary Property Prediction Tasks", "abstract": "Research in social media analysis is experiencing a recent surge with a large\nnumber of works applying representation learning models to solve high-level\nsyntactico-semantic tasks such as sentiment analysis, semantic textual\nsimilarity computation, hashtag prediction and so on. Although the performance\nof the representation learning models are better than the traditional baselines\nfor the tasks, little is known about the core properties of a tweet encoded\nwithin the representations. Understanding these core properties would empower\nus in making generalizable conclusions about the quality of representations.\nOur work presented here constitutes the first step in opening the black-box of\nvector embedding for social media posts, with emphasis on tweets in particular.\n  In order to understand the core properties encoded in a tweet representation,\nwe evaluate the representations to estimate the extent to which it can model\neach of those properties such as tweet length, presence of words, hashtags,\nmentions, capitalization, and so on. This is done with the help of multiple\nclassifiers which take the representation as input. Essentially, each\nclassifier evaluates one of the syntactic or social properties which are\narguably salient for a tweet. This is also the first holistic study on\nextensively analysing the ability to encode these properties for a wide variety\nof tweet representation models including the traditional unsupervised methods\n(BOW, LDA), unsupervised representation learning methods (Siamese CBOW,\nTweet2Vec) as well as supervised methods (CNN, BLSTM).", "published": "2016-11-15 15:34:47", "link": "http://arxiv.org/abs/1611.04887v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm", "abstract": "In topic modeling, many algorithms that guarantee identifiability of the\ntopics have been developed under the premise that there exist anchor words --\ni.e., words that only appear (with positive probability) in one topic.\nFollow-up work has resorted to three or higher-order statistics of the data\ncorpus to relax the anchor word assumption. Reliable estimates of higher-order\nstatistics are hard to obtain, however, and the identification of topics under\nthose models hinges on uncorrelatedness of the topics, which can be\nunrealistic. This paper revisits topic modeling based on second-order moments,\nand proposes an anchor-free topic mining framework. The proposed approach\nguarantees the identification of the topics under a much milder condition\ncompared to the anchor-word assumption, thereby exhibiting much better\nrobustness in practice. The associated algorithm only involves one\neigen-decomposition and a few small linear programs. This makes it easy to\nimplement and scale up to very large problem instances. Experiments using the\nTDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free\napproach exhibits very favorable performance (measured using coherence,\nsimilarity count, and clustering accuracy metrics) compared to the prior art.", "published": "2016-11-15 20:06:40", "link": "http://arxiv.org/abs/1611.05010v1", "categories": ["stat.ML", "cs.CL", "cs.IR", "cs.SI"], "primary_category": "stat.ML"}
