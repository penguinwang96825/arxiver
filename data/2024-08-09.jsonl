{"title": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for\n  Machine Translation", "abstract": "In-context learning (ICL) greatly improves the performance of large language\nmodels (LLMs) on various down-stream tasks, where the improvement highly\ndepends on the quality of demonstrations. In this work, we introduce syntactic\nknowledge to select better in-context examples for machine translation (MT). We\npropose a new strategy, namely Syntax-augmented COverage-based In-context\nexample selection (SCOI), leveraging the deep syntactic structure beyond\nconventional word matching. Specifically, we measure the set-level syntactic\ncoverage by computing the coverage of polynomial terms with the help of a\nsimplified tree-to-polynomial algorithm, and lexical coverage using word\noverlap. Furthermore, we devise an alternate selection approach to combine both\ncoverage measures, taking advantage of syntactic and lexical information. We\nconduct experiments with two multi-lingual LLMs on six translation directions.\nEmpirical results show that our proposed SCOI obtains the highest average COMET\nscore among all learning-free methods, indicating that combining syntactic and\nlexical coverage successfully helps to select better in-context examples for\nMT. Our code is available at https://github.com/JamyDon/SCOI.", "published": "2024-08-09 05:25:17", "link": "http://arxiv.org/abs/2408.04872v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Episode Detection for Large-Scale News Events", "abstract": "Episodic structures are inherently interpretable and adaptable to evolving\nlarge-scale key events. However, state-of-the-art automatic event detection\nmethods overlook event episodes and, therefore, struggle with these crucial\ncharacteristics. This paper introduces a novel task, episode detection, aimed\nat identifying episodes from a news corpus containing key event articles. An\nepisode describes a cohesive cluster of core entities (e.g., \"protesters\",\n\"police\") performing actions at a specific time and location. Furthermore, an\nepisode is a significant part of a larger group of episodes under a particular\nkey event. Automatically detecting episodes is challenging because, unlike key\nevents and atomic actions, we cannot rely on explicit mentions of times and\nlocations to distinguish between episodes or use semantic similarity to merge\ninconsistent episode co-references. To address these challenges, we introduce\nEpiMine, an unsupervised episode detection framework that (1) automatically\nidentifies the most salient, key-event-relevant terms and segments, (2)\ndetermines candidate episodes in an article based on natural episodic\npartitions estimated through shifts in discriminative term combinations, and\n(3) refines and forms final episode clusters using large language model-based\nreasoning on the candidate episodes. We construct three diverse, real-world\nevent datasets annotated at the episode level. EpiMine outperforms all\nbaselines on these datasets by an average 59.2% increase across all metrics.", "published": "2024-08-09 05:26:31", "link": "http://arxiv.org/abs/2408.04873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural\n  Communication in Codenames", "abstract": "Cultural differences in common ground may result in pragmatic failure and\nmisunderstandings during communication. We develop our method Rational Speech\nActs for Cross-Cultural Communication (RSA+C3) to resolve cross-cultural\ndifferences in common ground. To measure the success of our method, we study\nRSA+C3 in the collaborative referential game of Codenames Duet and show that\nour method successfully improves collaboration between simulated players of\ndifferent cultures. Our contributions are threefold: (1) creating Codenames\nplayers using contrastive learning of an embedding space and LLM prompting that\nare aligned with human patterns of play, (2) studying culturally induced\ndifferences in common ground reflected in our trained models, and (3)\ndemonstrating that our method RSA+C3 can ease cross-cultural communication in\ngameplay by inferring sociocultural context from interaction. Our code is\npublicly available at github.com/icwhite/codenames.", "published": "2024-08-09 07:02:18", "link": "http://arxiv.org/abs/2408.04900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalisation First, Memorisation Second? Memorisation Localisation for\n  Natural Language Classification Tasks", "abstract": "Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.", "published": "2024-08-09 09:30:57", "link": "http://arxiv.org/abs/2408.04965v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "reCSE: Portable Reshaping Features for Sentence Embedding in\n  Self-supervised Contrastive Learning", "abstract": "We propose reCSE, a self supervised contrastive learning sentence\nrepresentation framework based on feature reshaping. This framework is\ndifferent from the current advanced models that use discrete data augmentation\nmethods, but instead reshapes the input features of the original sentence,\naggregates the global information of each token in the sentence, and alleviates\nthe common problems of representation polarity and GPU memory consumption\nlinear increase in current advanced models. In addition, our reCSE has achieved\ncompetitive performance in semantic similarity tasks. And the experiment proves\nthat our proposed feature reshaping method has strong universality, which can\nbe transplanted to other self supervised contrastive learning frameworks and\nenhance their representation ability, even achieving state-of-the-art\nperformance. Our code is available at https://github.com/heavenhellchen/reCSE.", "published": "2024-08-09 09:56:30", "link": "http://arxiv.org/abs/2408.04975v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Get Confused Cautiously: Textual Sequence Memorization Erasure with\n  Selective Entropy Maximization", "abstract": "Large Language Models (LLMs) have been found to memorize and recite some of\nthe textual sequences from their training set verbatim, raising broad concerns\nabout privacy and copyright issues when using LLMs. This Textual Sequence\nMemorization (TSM) phenomenon leads to a high demand to regulate LLM output to\nprevent it from generating certain memorized text to meet user requirements.\nHowever, our empirical study reveals that existing methods for TSM erasure fail\nto forget massive memorized samples without substantially jeopardizing the\nmodel utility. To achieve a better trade-off between the effectiveness of TSM\nerasure and model utility in LLMs, our paper proposes a new framework based on\nEntropy Maximization with Selective Optimization (EMSO), where the updated\nweights are chosen with a novel contrastive gradient metric without any\nparticipation of additional model or data. Our analysis shows that training\nwith the entropy maximization loss has a more stable optimization process and\nbetter keeps model utility than existing methods. The contrastive gradient\nmetric localizes the most influential weight for TSM erasure by taking both the\ngradient magnitude and direction into consideration. Extensive experiments\nacross three model scales demonstrate that our method excels in handling\nlarge-scale forgetting requests while preserving model ability in language\ngeneration and reasoning.", "published": "2024-08-09 10:26:11", "link": "http://arxiv.org/abs/2408.04983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating a Benchmark for Training-set free Evaluation of Linguistic\n  Capabilities in Machine Reading Comprehension", "abstract": "Performance of NLP systems is typically evaluated by collecting a large-scale\ndataset by means of crowd-sourcing to train a data-driven model and evaluate it\non a held-out portion of the data. This approach has been shown to suffer from\nspurious correlations and the lack of challenging examples that represent the\ndiversity of natural language. Instead, we examine a framework for evaluating\noptimised models in training-set free setting on synthetically generated\nchallenge sets. We find that despite the simplicity of the generation method,\nthe data can compete with crowd-sourced datasets with regard to naturalness and\nlexical diversity for the purpose of evaluating the linguistic capabilities of\nMRC models. We conduct further experiments and show that state-of-the-art\nlanguage model-based MRC systems can learn to succeed on the challenge set\ncorrectly, although, without capturing the general notion of the evaluated\nphenomenon.", "published": "2024-08-09 12:23:36", "link": "http://arxiv.org/abs/2408.05023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective\n  Alignment with Contrastive Prompts", "abstract": "The task of multi-objective alignment aims at balancing and controlling the\ndifferent alignment objectives (e.g., helpfulness, harmlessness and honesty) of\nlarge language models to meet the personalized requirements of different users.\nHowever, previous methods tend to train multiple models to deal with various\nuser preferences, with the number of trained models growing linearly with the\nnumber of alignment objectives and the number of different preferences.\nMeanwhile, existing methods are generally poor in extensibility and require\nsignificant re-training for each new alignment objective considered.\nConsidering the limitation of previous approaches, we propose MCA\n(Multi-objective Contrastive Alignemnt), which constructs an expert prompt and\nan adversarial prompt for each objective to contrast at the decoding time and\nbalances the objectives through combining the contrast. Our approach is\nverified to be superior to previous methods in obtaining a well-distributed\nPareto front among different alignment objectives.", "published": "2024-08-09 14:36:42", "link": "http://arxiv.org/abs/2408.05094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Do LLMs Identify Cultural Unity in Diversity?", "abstract": "Much work on the cultural awareness of large language models (LLMs) focuses\non the models' sensitivity to geo-cultural diversity. However, in addition to\ncross-cultural differences, there also exists common ground across cultures.\nFor instance, a bridal veil in the United States plays a similar\ncultural-relevant role as a honggaitou in China. In this study, we introduce a\nbenchmark dataset CUNIT for evaluating decoder-only LLMs in understanding the\ncultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluation\nexamples building upon 285 traditional cultural-specific concepts across 10\ncountries. Based on a systematic manual annotation of cultural-relevant\nfeatures per concept, we calculate the cultural association between any pair of\ncross-cultural concepts. Built upon this dataset, we design a contrastive\nmatching task to evaluate the LLMs' capability to identify highly associated\ncross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popular\nprompting strategies, under the settings of either giving all extracted concept\nfeatures or no features at all on CUNIT Interestingly, we find that cultural\nassociations across countries regarding clothing concepts largely differ from\nfood. Our analysis shows that LLMs are still limited to capturing\ncross-cultural associations between concepts compared to humans. Moreover,\ngeo-cultural proximity shows a weak influence on model performance in capturing\ncross-cultural associations.", "published": "2024-08-09 14:45:22", "link": "http://arxiv.org/abs/2408.05102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep-change at AXOLOTL-24: Orchestrating WSD and WSI Models for Semantic\n  Change Modeling", "abstract": "This paper describes our solution of the first subtask from the AXOLOTL-24\nshared task on Semantic Change Modeling. The goal of this subtask is to\ndistribute a given set of usages of a polysemous word from a newer time period\nbetween senses of this word from an older time period and clusters representing\ngained senses of this word. We propose and experiment with three new methods\nsolving this task. Our methods achieve SOTA results according to both official\nmetrics of the first substask. Additionally, we develop a model that can tell\nif a given word usage is not described by any of the provided sense\ndefinitions. This model serves as a component in one of our methods, but can\npotentially be useful on its own.", "published": "2024-08-09 17:15:54", "link": "http://arxiv.org/abs/2408.05184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Separating Style from Substance: Enhancing Cross-Genre Authorship\n  Attribution through Data Selection and Presentation", "abstract": "The task of deciding whether two documents are written by the same author is\nchallenging for both machines and humans. This task is even more challenging\nwhen the two documents are written about different topics (e.g. baseball vs.\npolitics) or in different genres (e.g. a blog post vs. an academic article).\nFor machines, the problem is complicated by the relative lack of real-world\ntraining examples that cross the topic boundary and the vanishing scarcity of\ncross-genre data. We propose targeted methods for training data selection and a\nnovel learning curriculum that are designed to discourage a model's reliance on\ntopic information for authorship attribution and correspondingly force it to\nincorporate information more robustly indicative of style no matter the topic.\nThese refinements yield a 62.7% relative improvement in average cross-genre\nauthorship attribution, as well as 16.6% in the per-genre condition.", "published": "2024-08-09 17:31:37", "link": "http://arxiv.org/abs/2408.05192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MUSE: Multi-Knowledge Passing on the Edges, Boosting Knowledge Graph\n  Completion", "abstract": "Knowledge Graph Completion (KGC) aims to predict the missing information in\nthe (head entity)-[relation]-(tail entity) triplet. Deep Neural Networks have\nachieved significant progress in the relation prediction task. However, most\nexisting KGC methods focus on single features (e.g., entity IDs) and sub-graph\naggregation, which cannot fully explore all the features in the Knowledge Graph\n(KG), and neglect the external semantic knowledge injection. To address these\nproblems, we propose MUSE, a knowledge-aware reasoning model to learn a\ntailored embedding space in three dimensions for missing relation prediction\nthrough a multi-knowledge representation learning mechanism. Our MUSE consists\nof three parallel components: 1) Prior Knowledge Learning for enhancing the\ntriplets' semantic representation by fine-tuning BERT; 2) Context Message\nPassing for enhancing the context messages of KG; 3) Relational Path\nAggregation for enhancing the path representation from the head entity to the\ntail entity. Our experimental results show that MUSE significantly outperforms\nother baselines on four public datasets, such as over 5.50% improvement in H@1\nand 4.20% improvement in MRR on the NELL995 dataset. The code and all datasets\nwill be released via https://github.com/NxxTGT/MUSE.", "published": "2024-08-09 18:10:02", "link": "http://arxiv.org/abs/2408.05283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Psychology-based Unified Dynamic Framework for Curriculum Learning", "abstract": "Directly learning from examples of random difficulty levels is often\nchallenging for both humans and machine learning models. A more effective\nstrategy involves exposing learners to examples in a progressive order, from\neasy to difficult. Curriculum Learning (CL) has been proposed to implement this\nstrategy in machine learning model training. However, two key challenges\npersist in CL framework design: defining the difficulty of training data and\ndetermining the appropriate amount of data to input at each training step. This\npaper presents a Psychology-based Unified Dynamic Framework for Curriculum\nLearning (PUDF), drawing inspiration from psychometrics. We quantify the\ndifficulty of training data by applying Item Response Theory (IRT) to responses\nfrom Artificial Crowds (AC). This theory-driven IRT-AC approach leads to global\n(i.e., model-independent) and interpretable difficulty values. Leveraging IRT,\nwe propose a Dynamic Data Selection via Model Ability Estimation (DDS-MAE)\nstrategy to schedule the appropriate amount of data during model training.\nSince our difficulty labeling and model ability estimation are based on a\nconsistent theory, namely IRT, their values are comparable within the same\nscope, potentially leading to a faster convergence compared to the other CL\nmethods. Experimental results demonstrate that fine-tuning pre-trained language\nmodels with PUDF enhances their performance on the GLUE benchmark. Moreover,\nPUDF surpasses other state-of-the-art (SOTA) CL methods on the GLUE benchmark.\nWe further explore the components of PUDF, namely the difficulty measurer\n(IRT-AC) and the training scheduler (DDS-MAE) qualitatively and quantitatively.\nLastly, we conduct an ablation study to clarify which components of PUDF\ncontribute to faster convergence and higher accuracy.", "published": "2024-08-09 20:30:37", "link": "http://arxiv.org/abs/2408.05326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DataNarrative: Automated Data-Driven Storytelling with Visualizations\n  and Texts", "abstract": "Data-driven storytelling is a powerful method for conveying insights by\ncombining narrative techniques with visualizations and text. These stories\nintegrate visual aids, such as highlighted bars and lines in charts, along with\ntextual annotations explaining insights. However, creating such stories\nrequires a deep understanding of the data and meticulous narrative planning,\noften necessitating human intervention, which can be time-consuming and\nmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,\ntheir ability to generate coherent and comprehensive data stories remains\nunderexplored. In this work, we introduce a novel task for data story\ngeneration and a benchmark containing 1,449 stories from diverse sources. To\naddress the challenges of crafting coherent data stories, we propose a\nmultiagent framework employing two LLM agents designed to replicate the human\nstorytelling process: one for understanding and describing the data\n(Reflection), generating the outline, and narration, and another for\nverification at each intermediary step. While our agentic framework generally\noutperforms non-agentic counterparts in both model-based and human evaluations,\nthe results also reveal unique challenges in data story generation.", "published": "2024-08-09 21:31:33", "link": "http://arxiv.org/abs/2408.05346v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLaMA based Punctuation Restoration With Forward Pass Only Decoding", "abstract": "This paper introduces two advancements in the field of Large Language Model\nAnnotation with a focus on punctuation restoration tasks. Our first\ncontribution is the application of LLaMA for punctuation restoration, which\ndemonstrates superior performance compared to the established benchmark.\n  Despite its impressive quality, LLaMA faces challenges regarding inference\nspeed and hallucinations. To address this, our second contribution presents\nForward Pass Only Decoding (FPOD), a novel decoding approach for annotation\ntasks. This innovative method results in a substantial 19.8x improvement in\ninference speed, effectively addressing a critical bottleneck and enhancing the\npractical utility of LLaMA for large-scale data annotation tasks without\nhallucinations.\n  The combination of these contributions not only solidifies LLaMA as a\npowerful tool for punctuation restoration but also highlights FPOD as a crucial\nstrategy for overcoming speed constraints.", "published": "2024-08-09 22:20:56", "link": "http://arxiv.org/abs/2408.11845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FUSE-ing Language Models: Zero-Shot Adapter Discovery for Prompt\n  Optimization Across Tokenizers", "abstract": "The widespread use of large language models has resulted in a multitude of\ntokenizers and embedding spaces, making knowledge transfer in prompt discovery\ntasks difficult. In this work, we propose FUSE (Flexible Unification of\nSemantic Embeddings), an inexpensive approach to approximating an adapter layer\nthat maps from one model's textual embedding space to another, even across\ndifferent tokenizers. We introduce a third-order tensor-based representation of\na model's embedding space that aligns semantic embeddings that have been split\napart by different tokenizers, and use this representation to derive an\napproximation of the gradient of one model's outputs with respect to another\nmodel's embedding space. We show the efficacy of our approach via\nmulti-objective optimization over vision-language and causal language models\nfor image captioning and sentiment-based image captioning.", "published": "2024-08-09 02:16:37", "link": "http://arxiv.org/abs/2408.04816v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ensemble BERT: A student social network text sentiment classification\n  model based on ensemble learning and BERT architecture", "abstract": "The mental health assessment of middle school students has always been one of\nthe focuses in the field of education. This paper introduces a new ensemble\nlearning network based on BERT, employing the concept of enhancing model\nperformance by integrating multiple classifiers. We trained a range of\nBERT-based learners, which combined using the majority voting method. We\ncollect social network text data of middle school students through China's\nWeibo and apply the method to the task of classifying emotional tendencies in\nmiddle school students' social network texts. Experimental results suggest that\nthe ensemble learning network has a better performance than the base model and\nthe performance of the ensemble learning model, consisting of three\nsingle-layer BERT models, is barely the same as a three-layer BERT model but\nrequires 11.58% more training time. Therefore, in terms of balancing prediction\neffect and efficiency, the deeper BERT network should be preferred for\ntraining. However, for interpretability, network ensembles can provide\nacceptable solutions.", "published": "2024-08-09 03:57:31", "link": "http://arxiv.org/abs/2408.04849v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MSG-Chart: Multimodal Scene Graph for ChartQA", "abstract": "Automatic Chart Question Answering (ChartQA) is challenging due to the\ncomplex distribution of chart elements with patterns of the underlying data not\nexplicitly displayed in charts. To address this challenge, we design a joint\nmultimodal scene graph for charts to explicitly represent the relationships\nbetween chart elements and their patterns. Our proposed multimodal scene graph\nincludes a visual graph and a textual graph to jointly capture the structural\nand semantical knowledge from the chart. This graph module can be easily\nintegrated with different vision transformers as inductive bias. Our\nexperiments demonstrate that incorporating the proposed graph module enhances\nthe understanding of charts' elements' structure and semantics, thereby\nimproving performance on publicly available benchmarks, ChartQA and OpenCQA.", "published": "2024-08-09 04:11:23", "link": "http://arxiv.org/abs/2408.04852v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GlitchProber: Advancing Effective Detection and Mitigation of Glitch\n  Tokens in Large Language Models", "abstract": "Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model's vocabulary space and named them \"glitch tokens\". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.", "published": "2024-08-09 07:19:53", "link": "http://arxiv.org/abs/2408.04905v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards a Generative Approach for Emotion Detection and Reasoning", "abstract": "Large language models (LLMs) have demonstrated impressive performance in\nmathematical and commonsense reasoning tasks using chain-of-thought (CoT)\nprompting techniques. But can they perform emotional reasoning by concatenating\n`Let's think step-by-step' to the input prompt? In this paper we investigate\nthis question along with introducing a novel approach to zero-shot emotion\ndetection and emotional reasoning using LLMs. Existing state of the art\nzero-shot approaches rely on textual entailment models to choose the most\nappropriate emotion label for an input text. We argue that this strongly\nrestricts the model to a fixed set of labels which may not be suitable or\nsufficient for many applications where emotion analysis is required. Instead,\nwe propose framing the problem of emotion analysis as a generative\nquestion-answering (QA) task. Our approach uses a two step methodology of\ngenerating relevant context or background knowledge to answer the emotion\ndetection question step-by-step. Our paper is the first work on using a\ngenerative approach to jointly address the tasks of emotion detection and\nemotional reasoning for texts. We evaluate our approach on two popular emotion\ndetection datasets and also release the fine-grained emotion labels and\nexplanations for further training and fine-tuning of emotional reasoning\nsystems.", "published": "2024-08-09 07:20:15", "link": "http://arxiv.org/abs/2408.04906v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Surveying the Landscape of Image Captioning Evaluation: A Comprehensive\n  Taxonomy, Trends and Metrics Analysis", "abstract": "The task of image captioning has recently been gaining popularity, and with\nit the complex task of evaluating the quality of image captioning models. In\nthis work, we present the first survey and taxonomy of over 70 different image\ncaptioning metrics and their usage in hundreds of papers, specifically designed\nto help users select the most suitable metric for their needs. We find that\ndespite the diversity of proposed metrics, the vast majority of studies rely on\nonly five popular metrics, which we show to be weakly correlated with human\nratings. We hypothesize that combining a diverse set of metrics can enhance\ncorrelation with human ratings. As an initial step, we demonstrate that a\nlinear regression-based ensemble method, which we call EnsembEval, trained on\none human ratings dataset, achieves improved correlation across five additional\ndatasets, showing there is a lot of room for improvement by leveraging a\ndiverse set of metrics.", "published": "2024-08-09 07:31:06", "link": "http://arxiv.org/abs/2408.04909v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Quantitative Information Extraction from Humanitarian Documents", "abstract": "Humanitarian action is accompanied by a mass of reports, summaries, news, and\nother documents. To guide its activities, important information must be quickly\nextracted from such free-text resources. Quantities, such as the number of\npeople affected, amount of aid distributed, or the extent of infrastructure\ndamage, are central to emergency response and anticipatory action. In this\nwork, we contribute an annotated dataset for the humanitarian domain for the\nextraction of such quantitative information, along side its important context,\nincluding units it refers to, any modifiers, and the relevant event. Further,\nwe develop a custom Natural Language Processing pipeline to extract the\nquantities alongside their units, and evaluate it in comparison to baseline and\nrecent literature. The proposed model achieves a consistent improvement in the\nperformance, especially in the documents pertaining to the Dominican Republic\nand select African countries. We make the dataset and code available to the\nresearch community to continue the improvement of NLP tools for the\nhumanitarian domain.", "published": "2024-08-09 08:46:38", "link": "http://arxiv.org/abs/2408.04941v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ProFuser: Progressive Fusion of Large Language Models", "abstract": "While fusing the capacities and advantages of various large language models\n(LLMs) offers a pathway to construct more powerful and versatile models, a\nfundamental challenge is to properly select advantageous model during the\ntraining. Existing fusion methods primarily focus on the training mode that\nuses cross entropy on ground truth in a teacher-forcing setup to measure a\nmodel's advantage, which may provide limited insight towards model advantage.\nIn this paper, we introduce a novel approach that enhances the fusion process\nby incorporating both the training and inference modes. Our method evaluates\nmodel advantage not only through cross entropy during training but also by\nconsidering inference outputs, providing a more comprehensive assessment. To\ncombine the two modes effectively, we introduce ProFuser to progressively\ntransition from inference mode to training mode. To validate ProFuser's\neffectiveness, we fused three models, including vicuna-7b-v1.5,\nLlama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance\nin knowledge, reasoning, and safety compared to baseline methods.", "published": "2024-08-09 11:18:29", "link": "http://arxiv.org/abs/2408.04998v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Examining the Behavior of LLM Architectures Within the Framework of\n  Standardized National Exams in Brazil", "abstract": "The Exame Nacional do Ensino M\\'edio (ENEM) is a pivotal test for Brazilian\nstudents, required for admission to a significant number of universities in\nBrazil. The test consists of four objective high-school level tests on Math,\nHumanities, Natural Sciences and Languages, and one writing essay. Students'\nanswers to the test and to the accompanying socioeconomic status questionnaire\nare made public every year (albeit anonymized) due to transparency policies\nfrom the Brazilian Government. In the context of large language models (LLMs),\nthese data lend themselves nicely to comparing different groups of humans with\nAI, as we can have access to human and machine answer distributions. We\nleverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4,\nand MariTalk, a model trained using Portuguese data, to humans, aiming to\nascertain how their answers relate to real societal groups and what that may\nreveal about the model biases. We divide the human groups by using\nsocioeconomic status (SES), and compare their answer distribution with LLMs for\neach question and for the essay. We find no significant biases when comparing\nLLM performance to humans on the multiple-choice Brazilian Portuguese tests, as\nthe distance between model and human answers is mostly determined by the human\naccuracy. A similar conclusion is found by looking at the generated text as,\nwhen analyzing the essays, we observe that human and LLM essays differ in a few\nkey factors, one being the choice of words where model essays were easily\nseparable from human ones. The texts also differ syntactically, with LLM\ngenerated essays exhibiting, on average, smaller sentences and less thought\nunits, among other differences. These results suggest that, for Brazilian\nPortuguese in the ENEM context, LLM outputs represent no group of humans, being\nsignificantly different from the answers from Brazilian students across all\ntests.", "published": "2024-08-09 12:47:28", "link": "http://arxiv.org/abs/2408.05035v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Improving Mortality Prediction After Radiotherapy with Large Language\n  Model Structuring of Large-Scale Unstructured Electronic Health Records", "abstract": "Accurate survival prediction in radiotherapy (RT) is critical for optimizing\ntreatment decisions. This study developed and validated the RT-Surv framework,\nwhich integrates general-domain, open-source large language models (LLMs) to\nstructure unstructured electronic health records alongside structured clinical\ndata. Using data from 34,276 patients and an external cohort of 852, the\nframework successfully transformed unstructured clinical information into\nstructured formats. Incorporating LLM-structured clinical features improved the\nconcordance index from 0.779 to 0.842 during external validation, demonstrating\na significant performance enhancement. Key LLM-structured features, such as\ndisease extent, general condition, and RT purpose, showed high predictive\nimportance and aligned closely with statistically significant predictors\nidentified through conventional statistical analyses, thereby improving model\ninterpretability. Furthermore, the framework enhanced risk stratification,\nenabling more distinct differentiation among low-, intermediate-, and high-risk\ngroups (p < 0.001) using LLM-structured clinical features. These findings\nhighlight the potential of LLMs to convert unstructured data into actionable\ninsights, improving predictive modeling and patient outcomes in clinics.", "published": "2024-08-09 14:02:24", "link": "http://arxiv.org/abs/2408.05074v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating novel experimental hypotheses from language models: A case\n  study on cross-dative generalization", "abstract": "Neural network language models (LMs) have been shown to successfully capture\ncomplex linguistic knowledge. However, their utility for understanding language\nacquisition is still debated. We contribute to this debate by presenting a case\nstudy where we use LMs as simulated learners to derive novel experimental\nhypotheses to be tested with humans. We apply this paradigm to study\ncross-dative generalization (CDG): productive generalization of novel verbs\nacross dative constructions (she pilked me the ball/she pilked the ball to\nme)--acquisition of which is known to involve a large space of contextual\nfeatures--using LMs trained on child-directed speech. We specifically ask:\n\"what properties of the training exposure facilitate a novel verb's\ngeneralization to the (unmodeled) alternate construction?\" To answer this, we\nsystematically vary the exposure context in which a novel dative verb occurs in\nterms of the properties of the theme and recipient, and then analyze the LMs'\nusage of the novel verb in the unmodeled dative construction. We find LMs to\nreplicate known patterns of children's CDG, as a precondition to exploring\nnovel hypotheses. Subsequent simulations reveal a nuanced role of the features\nof the novel verbs' exposure context on the LMs' CDG. We find CDG to be\nfacilitated when the first postverbal argument of the exposure context is\npronominal, definite, short, and conforms to the prototypical animacy\nexpectations of the exposure dative. These patterns are characteristic of\nharmonic alignment in datives, where the argument with features ranking higher\non the discourse prominence scale tends to precede the other. This gives rise\nto a novel hypothesis that CDG is facilitated insofar as the features of the\nexposure context--in particular, its first postverbal argument--are\nharmonically aligned. We conclude by proposing future experiments that can test\nthis hypothesis in children.", "published": "2024-08-09 14:17:36", "link": "http://arxiv.org/abs/2408.05086v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models", "abstract": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.", "published": "2024-08-09 14:34:32", "link": "http://arxiv.org/abs/2408.05093v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MooER: LLM-based Speech Recognition and Translation Models from Moore\n  Threads", "abstract": "In this paper, we present MooER, a LLM-based large-scale automatic speech\nrecognition (ASR) / automatic speech translation (AST) model of Moore Threads.\nA 5000h pseudo labeled dataset containing open source and self collected speech\ndata is used for training. We achieve performance comparable to other open\nsource models trained with up to hundreds of thousands of hours of labeled\nspeech data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest\nthat our model outperforms other open source Speech LLMs. A BLEU score of 25.2\ncan be obtained. The main contributions of this paper are summarized as\nfollows. First, this paper presents a training strategy for encoders and LLMs\non speech related tasks (including ASR and AST) using a small size of pseudo\nlabeled data without any extra manual annotation and selection. Second, we\nrelease our ASR and AST models and plan to open-source our training code and\nstrategy in the near future. Moreover, a model trained on 8wh scale training\ndata is planned to be released later on.", "published": "2024-08-09 14:43:56", "link": "http://arxiv.org/abs/2408.05101v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning", "abstract": "Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.", "published": "2024-08-09 15:53:55", "link": "http://arxiv.org/abs/2408.05141v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "KIF: Knowledge Identification and Fusion for Language Model Continual\n  Learning", "abstract": "Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Identification and Fusion (KIF),\nwhich boosts knowledge transfer without depending on memory replay. KIF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge identification technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KIF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KIF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KIF and\nits variants across different settings.", "published": "2024-08-09 17:44:45", "link": "http://arxiv.org/abs/2408.05200v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interactive-T2S: Multi-Turn Interactions for Text-to-SQL with Large\n  Language Models", "abstract": "This study explores text-to-SQL parsing by leveraging the powerful reasoning\ncapabilities of large language models (LLMs). Despite recent advancements,\nexisting LLM-based methods have not adequately addressed scalability, leading\nto inefficiencies when processing wide tables. Furthermore, current\ninteraction-based approaches either lack a step-by-step, interpretable SQL\ngeneration process or fail to provide an efficient and universally applicable\ninteraction design. To address these challenges, we introduce Interactive-T2S,\na framework that generates SQL queries through direct interactions with\ndatabases. This framework includes four general tools that facilitate proactive\nand efficient information retrieval by the LLM. Additionally, we have developed\ndetailed exemplars to demonstrate the step-wise reasoning processes within our\nframework. Our experiments on the BIRD-Dev dataset, employing a setting without\noracle knowledge, reveal that our method achieves state-of-the-art results with\nonly two exemplars, underscoring the effectiveness and robustness of our\nframework.", "published": "2024-08-09 07:43:21", "link": "http://arxiv.org/abs/2408.11062v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Text classification optimization algorithm based on graph neural network", "abstract": "In the field of natural language processing, text classification, as a basic\ntask, has important research value and application prospects. Traditional text\nclassification methods usually rely on feature representations such as the bag\nof words model or TF-IDF, which overlook the semantic connections between words\nand make it challenging to grasp the deep structural details of the text.\nRecently, GNNs have proven to be a valuable asset for text classification\ntasks, thanks to their capability to handle non-Euclidean data efficiently.\nHowever, the existing text classification methods based on GNN still face\nchallenges such as complex graph structure construction and high cost of model\ntraining. This paper introduces a text classification optimization algorithm\nutilizing graph neural networks. By introducing adaptive graph construction\nstrategy and efficient graph convolution operation, the accuracy and efficiency\nof text classification are effectively improved. The experimental results\ndemonstrate that the proposed method surpasses traditional approaches and\nexisting GNN models across multiple public datasets, highlighting its superior\nperformance and feasibility for text classification tasks.", "published": "2024-08-09 23:25:37", "link": "http://arxiv.org/abs/2408.15257v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal\n  Large Language Models", "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in executing instructions for a variety of single-image tasks.\nDespite this progress, significant challenges remain in modeling long image\nsequences. In this work, we introduce the versatile multi-modal large language\nmodel, mPLUG-Owl3, which enhances the capability for long image-sequence\nunderstanding in scenarios that incorporate retrieved image-text knowledge,\ninterleaved image-text, and lengthy videos. Specifically, we propose novel\nhyper attention blocks to efficiently integrate vision and language into a\ncommon language-guided semantic space, thereby facilitating the processing of\nextended multi-image scenarios. Extensive experimental results suggest that\nmPLUG-Owl3 achieves state-of-the-art performance among models with a similar\nsize on single-image, multi-image, and video benchmarks. Moreover, we propose a\nchallenging long visual sequence evaluation named Distractor Resistance to\nassess the ability of models to maintain focus amidst distractions. Finally,\nwith the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance\non ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to\nthe development of more efficient and powerful multimodal large language\nmodels.", "published": "2024-08-09 03:25:42", "link": "http://arxiv.org/abs/2408.04840v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MIDI-to-Tab: Guitar Tablature Inference via Masked Language Modeling", "abstract": "Guitar tablatures enrich the structure of traditional music notation by\nassigning each note to a string and fret of a guitar in a particular tuning,\nindicating precisely where to play the note on the instrument. The problem of\ngenerating tablature from a symbolic music representation involves inferring\nthis string and fret assignment per note across an entire composition or\nperformance. On the guitar, multiple string-fret assignments are possible for\nmost pitches, which leads to a large combinatorial space that prevents\nexhaustive search approaches. Most modern methods use constraint-based dynamic\nprogramming to minimize some cost function (e.g.\\ hand position movement). In\nthis work, we introduce a novel deep learning solution to symbolic guitar\ntablature estimation. We train an encoder-decoder Transformer model in a masked\nlanguage modeling paradigm to assign notes to strings. The model is first\npre-trained on DadaGP, a dataset of over 25K tablatures, and then fine-tuned on\na curated set of professionally transcribed guitar performances. Given the\nsubjective nature of assessing tablature quality, we conduct a user study\namongst guitarists, wherein we ask participants to rate the playability of\nmultiple versions of tablature for the same four-bar excerpt. The results\nindicate our system significantly outperforms competing algorithms.", "published": "2024-08-09 12:25:23", "link": "http://arxiv.org/abs/2408.05024v1", "categories": ["cs.SD", "cs.CL", "cs.IR"], "primary_category": "cs.SD"}
{"title": "Large Language Models and Thematic Analysis: Human-AI Synergy in\n  Researching Hate Speech on Social Media", "abstract": "In the dynamic field of artificial intelligence (AI), the development and\napplication of Large Language Models (LLMs) for text analysis are of\nsignificant academic interest. Despite the promising capabilities of various\nLLMs in conducting qualitative analysis, their use in the humanities and social\nsciences has not been thoroughly examined. This article contributes to the\nemerging literature on LLMs in qualitative analysis by documenting an\nexperimental study involving GPT-4. The study focuses on performing thematic\nanalysis (TA) using a YouTube dataset derived from an EU-funded project, which\nwas previously analyzed by other researchers. This dataset is about the\nrepresentation of Roma migrants in Sweden during 2016, a period marked by the\naftermath of the 2015 refugee crisis and preceding the Swedish national\nelections in 2017. Our study seeks to understand the potential of combining\nhuman intelligence with AI's scalability and efficiency, examining the\nadvantages and limitations of employing LLMs in qualitative research within the\nhumanities and social sciences. Additionally, we discuss future directions for\napplying LLMs in these fields.", "published": "2024-08-09 15:34:41", "link": "http://arxiv.org/abs/2408.05126v1", "categories": ["cs.HC", "cs.CL", "cs.SI"], "primary_category": "cs.HC"}
{"title": "Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2", "abstract": "Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse\ndecomposition of a neural network's latent representations into seemingly\ninterpretable features. Despite recent excitement about their potential,\nresearch applications outside of industry are limited by the high cost of\ntraining a comprehensive suite of SAEs. In this work, we introduce Gemma Scope,\nan open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2\n2B and 9B and select layers of Gemma 2 27B base models. We primarily train SAEs\non the Gemma 2 pre-trained models, but additionally release SAEs trained on\ninstruction-tuned Gemma 2 9B for comparison. We evaluate the quality of each\nSAE on standard metrics and release these results. We hope that by releasing\nthese SAE weights, we can help make more ambitious safety and interpretability\nresearch easier for the community. Weights and a tutorial can be found at\nhttps://huggingface.co/google/gemma-scope and an interactive demo can be found\nat https://www.neuronpedia.org/gemma-scope", "published": "2024-08-09 16:06:42", "link": "http://arxiv.org/abs/2408.05147v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating the capability of large language models to personalize\n  science texts for diverse middle-school-age learners", "abstract": "Large language models (LLMs), including OpenAI's GPT-series, have made\nsignificant advancements in recent years. Known for their expertise across\ndiverse subject areas and quick adaptability to user-provided prompts, LLMs\nhold unique potential as Personalized Learning (PL) tools. Despite this\npotential, their application in K-12 education remains largely unexplored. This\npaper presents one of the first randomized controlled trials (n = 23) to\nevaluate the effectiveness of GPT-4 in personalizing educational science texts\nfor middle school students. In this study, GPT-4 was used to profile student\nlearning preferences based on choices made during a training session. For the\nexperimental group, GPT-4 was used to rewrite science texts to align with the\nstudent's predicted profile while, for students in the control group, texts\nwere rewritten to contradict their learning preferences. The results of a\nMann-Whitney U test showed that students significantly preferred (at the .10\nlevel) the rewritten texts when they were aligned with their profile (p =\n.059). These findings suggest that GPT-4 can effectively interpret and tailor\neducational content to diverse learner preferences, marking a significant\nadvancement in PL technology. The limitations of this study and ethical\nconsiderations for using artificial intelligence in education are also\ndiscussed.", "published": "2024-08-09 17:53:35", "link": "http://arxiv.org/abs/2408.05204v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "abstract": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io.", "published": "2024-08-09 17:59:49", "link": "http://arxiv.org/abs/2408.05211v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Revisiting Multi-Modal LLM Evaluation", "abstract": "With the advent of multi-modal large language models (MLLMs), datasets used\nfor visual question answering (VQA) and referring expression comprehension have\nseen a resurgence. However, the most popular datasets used to evaluate MLLMs\nare some of the earliest ones created, and they have many known problems,\nincluding extreme bias, spurious correlations, and an inability to permit\nfine-grained analysis. In this paper, we pioneer evaluating recent MLLMs (LLaVA\n1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o) on datasets designed\nto address weaknesses in earlier ones. We assess three VQA datasets: 1) TDIUC,\nwhich permits fine-grained analysis on 12 question types; 2) TallyQA, which has\nsimple and complex counting questions; and 3) DVQA, which requires optical\ncharacter recognition for chart understanding. We also study VQDv1, a dataset\nthat requires identifying all image regions that satisfy a given query. Our\nexperiments reveal the weaknesses of many MLLMs that have not previously been\nreported. Our code is integrated into the widely used LAVIS framework for MLLM\nevaluation, enabling the rapid assessment of future MLLMs. Project webpage:\nhttps://kevinlujian.github.io/MLLM_Evaluations/", "published": "2024-08-09 20:55:46", "link": "http://arxiv.org/abs/2408.05334v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "FiSTECH: Financial Style Transfer to Enhance Creativity without\n  Hallucinations in LLMs", "abstract": "Recent trends in Generative AI have emerged towards fine-tuning foundational\nlarge language models (LLMs) to create domain-specific LLMs for automation and\nchatbot-like applications. Specialized applications for analytics-heavy domains\nsuch as Financial report generation require specific writing styles that\ncomprise compound and creative sentences with minimized hallucinations. In this\nwork, we explore the self-corrective auto-regressive qualities of LLMs to learn\ncreativity in writing styles with minimal prompting. We propose a novel\ntwo-stage fine-tuning (FT) strategy wherein in the first stage public domain\nfinancial reports are used to train for writing styles while allowing the LLM\nto hallucinate. In the second stage the examples of hallucinations are manually\ncorrected and further used to fine-tune the LLM. The finally trained LLM learns\nto generate specific financial report sections using minimal instructions and\ntabular data inputs while ensuring low fine-tuning costs. Our proposed\ntwo-stage fine-tuning boosts the accuracy of financial questions answering by\ntwo-folds while reducing hallucinations by over 50%. Also, the fine-tuned model\nhas lower perplexity, improved ROUGE, TER and BLEU scores, higher creativity\nand knowledge density with lower uncertainty and cross entropy than base LLMs.\nThus, the proposed framework can be generalized to train creativity in LLMs by\nfirst allowing them to hallucinate.", "published": "2024-08-09 22:29:23", "link": "http://arxiv.org/abs/2408.05365v4", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Node Level Graph Autoencoder: Unified Pretraining for Textual Graph\n  Learning", "abstract": "Textual graphs are ubiquitous in real-world applications, featuring rich text\ninformation with complex relationships, which enables advanced research across\nvarious fields. Textual graph representation learning aims to generate\nlow-dimensional feature embeddings from textual graphs that can improve the\nperformance of downstream tasks. A high-quality feature embedding should\neffectively capture both the structural and the textual information in a\ntextual graph. However, most textual graph dataset benchmarks rely on word2vec\ntechniques to generate feature embeddings, which inherently limits their\ncapabilities. Recent works on textual graph representation learning can be\ncategorized into two folds: supervised and unsupervised methods. Supervised\nmethods finetune a language model on labeled nodes, which have limited\ncapabilities when labeled data is scarce. Unsupervised methods, on the other\nhand, extract feature embeddings by developing complex training pipelines. To\naddress these limitations, we propose a novel unified unsupervised learning\nautoencoder framework, named Node Level Graph AutoEncoder (NodeGAE). We employ\nlanguage models as the backbone of the autoencoder, with pretraining on text\nreconstruction. Additionally, we add an auxiliary loss term to make the feature\nembeddings aware of the local graph structure. Our method maintains simplicity\nin the training process and demonstrates generalizability across diverse\ntextual graphs and downstream tasks. We evaluate our method on two core graph\nrepresentation learning downstream tasks: node classification and link\nprediction. Comprehensive experiments demonstrate that our approach\nsubstantially enhances the performance of diverse graph neural networks (GNNs)\nacross multiple textual graph datasets.", "published": "2024-08-09 14:57:53", "link": "http://arxiv.org/abs/2408.07091v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhancing Exploratory Learning through Exploratory Search with the\n  Emergence of Large Language Models", "abstract": "In the information era, how learners find, evaluate, and effectively use\ninformation has become a challenging issue, especially with the added\ncomplexity of large language models (LLMs) that have further confused learners\nin their information retrieval and search activities. This study attempts to\nunpack this complexity by combining exploratory search strategies with the\ntheories of exploratory learning to form a new theoretical model of exploratory\nlearning from the perspective of students' learning. Our work adapts Kolb's\nlearning model by incorporating high-frequency exploration and feedback loops,\naiming to promote deep cognitive and higher-order cognitive skill development\nin students. Additionally, this paper discusses and suggests how advanced LLMs\nintegrated into information retrieval and information theory can support\nstudents in their exploratory searches, contributing theoretically to promoting\nstudent-computer interaction and supporting their learning journeys in the new\nera with LLMs.", "published": "2024-08-09 04:30:16", "link": "http://arxiv.org/abs/2408.08894v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Tabular Transfer Learning via Prompting LLMs", "abstract": "Learning with a limited number of labeled data is a central problem in\nreal-world applications of machine learning, as it is often expensive to obtain\nannotations. To deal with the scarcity of labeled data, transfer learning is a\nconventional approach; it suggests to learn a transferable knowledge by\ntraining a neural network from multiple other sources. In this paper, we\ninvestigate transfer learning of tabular tasks, which has been less studied and\nsuccessful in the literature, compared to other domains, e.g., vision and\nlanguage. This is because tables are inherently heterogeneous, i.e., they\ncontain different columns and feature spaces, making transfer learning\ndifficult. On the other hand, recent advances in natural language processing\nsuggest that the label scarcity issue can be mitigated by utilizing in-context\nlearning capability of large language models (LLMs). Inspired by this and the\nfact that LLMs can also process tables within a unified language space, we ask\nwhether LLMs can be effective for tabular transfer learning, in particular,\nunder the scenarios where the source and target datasets are of different\nformat. As a positive answer, we propose a novel tabular transfer learning\nframework, coined Prompt to Transfer (P2T), that utilizes unlabeled (or\nheterogeneous) source data with LLMs. Specifically, P2T identifies a column\nfeature in a source dataset that is strongly correlated with a target task\nfeature to create examples relevant to the target task, thus creating\npseudo-demonstrations for prompts. Experimental results demonstrate that P2T\noutperforms previous methods on various tabular learning benchmarks, showing\ngood promise for the important, yet underexplored tabular transfer learning\nproblem. Code is available at https://github.com/jaehyun513/P2T.", "published": "2024-08-09 11:30:52", "link": "http://arxiv.org/abs/2408.11063v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "h4rm3l: A language for Composable Jailbreak Attack Synthesis", "abstract": "Despite their demonstrated valuable capabilities, state-of-the-art (SOTA)\nwidely deployed large language models (LLMs) still have the potential to cause\nharm to society due to the ineffectiveness of their safety filters, which can\nbe bypassed by prompt transformations called jailbreak attacks. Current\napproaches to LLM safety assessment, which employ datasets of templated prompts\nand benchmarking pipelines, fail to cover sufficiently large and diverse sets\nof jailbreak attacks, leading to the widespread deployment of unsafe LLMs.\nRecent research showed that novel jailbreak attacks could be derived by\ncomposition; however, a formal composable representation for jailbreak attacks,\nwhich, among other benefits, could enable the exploration of a large\ncompositional space of jailbreak attacks through program synthesis methods, has\nnot been previously proposed. We introduce h4rm3l, a novel approach that\naddresses this gap with a human-readable domain-specific language (DSL). Our\nframework comprises: (1) The h4rm3l DSL, which formally expresses jailbreak\nattacks as compositions of parameterized string transformation primitives. (2)\nA synthesizer with bandit algorithms that efficiently generates jailbreak\nattacks optimized for a target black box LLM. (3) The h4rm3l red-teaming\nsoftware toolkit that employs the previous two components and an automated\nharmful LLM behavior classifier that is strongly aligned with human judgment.\nWe demonstrate h4rm3l's efficacy by synthesizing a dataset of 2656 successful\nnovel jailbreak attacks targeting 6 SOTA open-source and proprietary LLMs, and\nby benchmarking those models against a subset of these synthesized attacks. Our\nresults show that h4rm3l's synthesized attacks are diverse and more successful\nthan existing jailbreak attacks in literature, with success rates exceeding 90%\non SOTA LLMs.", "published": "2024-08-09 01:45:39", "link": "http://arxiv.org/abs/2408.04811v4", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG", "68", "I.2; I.2.0; I.2.1; I.2.5; I.2.7; K.6.5; K.4.2"], "primary_category": "cs.CR"}
{"title": "Natural Language Outlines for Code: Literate Programming in the LLM Era", "abstract": "We propose using natural language outlines as a novel modality and\ninteraction surface for providing AI assistance to developers throughout the\nsoftware development process. An NL outline for a code function comprises\nmultiple statements written in concise prose, which partition the code and\nsummarize its main ideas in the style of literate programming. Crucially, we\nfind that modern LLMs can generate accurate and high-quality NL outlines in\npractice. Moreover, NL outlines enable a bidirectional sync between code and\nNL, allowing changes in one to be automatically reflected in the other. We\ndiscuss many use cases for NL outlines: they can accelerate understanding and\nnavigation of code and diffs, simplify code maintenance, augment code search,\nsteer code generation, and more. We then propose and compare multiple LLM\nprompting techniques for generating outlines and ask professional developers to\njudge outline quality. Finally, we present two case studies applying NL\noutlines toward code review and malware detection.", "published": "2024-08-09 02:22:51", "link": "http://arxiv.org/abs/2408.04820v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.SE"}
{"title": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented\n  Generation for Efficient Information Extraction", "abstract": "Extraction and interpretation of intricate information from unstructured text\ndata arising in financial applications, such as earnings call transcripts,\npresent substantial challenges to large language models (LLMs) even using the\ncurrent best practices to use Retrieval Augmented Generation (RAG) (referred to\nas VectorRAG techniques which utilize vector databases for information\nretrieval) due to challenges such as domain specific terminology and complex\nformats of the documents. We introduce a novel approach based on a combination,\ncalled HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called\nGraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for\ninformation extraction from financial documents that is shown to be capable of\ngenerating accurate and contextually relevant answers. Using experiments on a\nset of financial earning call transcripts documents which come in the form of\nQ&A format, and hence provide a natural set of pairs of ground-truth Q&As, we\nshow that HybridRAG which retrieves context from both vector database and KG\noutperforms both traditional VectorRAG and GraphRAG individually when evaluated\nat both the retrieval and generation stages in terms of retrieval accuracy and\nanswer generation. The proposed technique has applications beyond the financial\ndomain", "published": "2024-08-09 09:07:48", "link": "http://arxiv.org/abs/2408.04948v1", "categories": ["cs.CL", "cs.LG", "q-fin.ST", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "AutoGen Studio: A No-Code Developer Tool for Building and Debugging\n  Multi-Agent Systems", "abstract": "Multi-agent systems, where multiple agents (generative AI models + tools)\ncollaborate, are emerging as an effective pattern for solving long-running,\ncomplex tasks in numerous domains. However, specifying their parameters (such\nas models, tools, and orchestration mechanisms etc,.) and debugging them\nremains challenging for most developers. To address this challenge, we present\nAUTOGEN STUDIO, a no-code developer tool for rapidly prototyping, debugging,\nand evaluating multi-agent workflows built upon the AUTOGEN framework. AUTOGEN\nSTUDIO offers a web interface and a Python API for representing LLM-enabled\nagents using a declarative (JSON-based) specification. It provides an intuitive\ndrag-and-drop UI for agent workflow specification, interactive evaluation and\ndebugging of workflows, and a gallery of reusable agent components. We\nhighlight four design principles for no-code multi-agent developer tools and\ncontribute an open-source implementation at\nhttps://github.com/microsoft/autogen/tree/main/samples/apps/autogen-studio", "published": "2024-08-09 03:27:37", "link": "http://arxiv.org/abs/2408.15247v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.SE"}
{"title": "From Text to Insight: Leveraging Large Language Models for Performance\n  Evaluation in Management", "abstract": "This study explores the potential of Large Language Models (LLMs),\nspecifically GPT-4, to enhance objectivity in organizational task performance\nevaluations. Through comparative analyses across two studies, including various\ntask performance outputs, we demonstrate that LLMs can serve as a reliable and\neven superior alternative to human raters in evaluating knowledge-based\nperformance outputs, which are a key contribution of knowledge workers. Our\nresults suggest that GPT ratings are comparable to human ratings but exhibit\nhigher consistency and reliability. Additionally, combined multiple GPT ratings\non the same performance output show strong correlations with aggregated human\nperformance ratings, akin to the consensus principle observed in performance\nevaluation literature. However, we also find that LLMs are prone to contextual\nbiases, such as the halo effect, mirroring human evaluative biases. Our\nresearch suggests that while LLMs are capable of extracting meaningful\nconstructs from text-based data, their scope is currently limited to specific\nforms of performance evaluation. By highlighting both the potential and\nlimitations of LLMs, our study contributes to the discourse on AI role in\nmanagement studies and sets a foundation for future research to refine AI\ntheoretical and practical applications in management.", "published": "2024-08-09 20:35:10", "link": "http://arxiv.org/abs/2408.05328v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Hyper Recurrent Neural Network: Condition Mechanisms for Black-box Audio\n  Effect Modeling", "abstract": "Recurrent neural networks (RNNs) have demonstrated impressive results for\nvirtual analog modeling of audio effects. These networks process time-domain\naudio signals using a series of matrix multiplication and nonlinear activation\nfunctions to emulate the behavior of the target device accurately. To\nadditionally model the effect of the knobs for an RNN-based model, existing\napproaches integrate control parameters by concatenating them channel-wisely\nwith some intermediate representation of the input signal. While this method is\nparameter-efficient, there is room to further improve the quality of generated\naudio because the concatenation-based conditioning method has limited capacity\nin modulating signals. In this paper, we propose three novel conditioning\nmechanisms for RNNs, tailored for black-box virtual analog modeling. These\nadvanced conditioning mechanisms modulate the model based on control\nparameters, yielding superior results to existing RNN- and CNN-based\narchitectures across various evaluation metrics.", "published": "2024-08-09 03:00:25", "link": "http://arxiv.org/abs/2408.04829v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild", "abstract": "The growing prominence of the field of audio deepfake detection is driven by\nits wide range of applications, notably in protecting the public from potential\nfraud and other malicious activities, prompting the need for greater attention\nand research in this area. The ADD 2023 challenge goes beyond binary real/fake\nclassification by emulating real-world scenarios, such as the identification of\nmanipulated intervals in partially fake audio and determining the source\nresponsible for generating any fake audio, both with real-life implications,\nnotably in audio forensics, law enforcement, and construction of reliable and\ntrustworthy evidence. To further foster research in this area, in this article,\nwe describe the dataset that was used in the fake game, manipulation region\nlocation and deepfake algorithm recognition tracks of the challenge. We also\nfocus on the analysis of the technical methodologies by the top-performing\nparticipants in each task and note the commonalities and differences in their\napproaches. Finally, we discuss the current technical limitations as identified\nthrough the technical analysis, and provide a roadmap for future research\ndirections. The dataset is available for download at\nhttp://addchallenge.cn/downloadADD2023.", "published": "2024-08-09 09:32:37", "link": "http://arxiv.org/abs/2408.04967v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TEAdapter: Supply abundant guidance for controllable text-to-music\n  generation", "abstract": "Although current text-guided music generation technology can cope with simple\ncreative scenarios, achieving fine-grained control over individual\ntext-modality conditions remains challenging as user demands become more\nintricate. Accordingly, we introduce the TEAcher Adapter (TEAdapter), a compact\nplugin designed to guide the generation process with diverse control\ninformation provided by users. In addition, we explore the controllable\ngeneration of extended music by leveraging TEAdapter control groups trained on\ndata of distinct structural functionalities. In general, we consider controls\nover global, elemental, and structural levels. Experimental results demonstrate\nthat the proposed TEAdapter enables multiple precise controls and ensures\nhigh-quality music generation. Our module is also lightweight and transferable\nto any diffusion model architecture. Available code and demos will be found\nsoon at https://github.com/Ashley1101/TEAdapter.", "published": "2024-08-09 05:04:13", "link": "http://arxiv.org/abs/2408.04865v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SELD-Mamba: Selective State-Space Model for Sound Event Localization and\n  Detection with Source Distance Estimation", "abstract": "In the Sound Event Localization and Detection (SELD) task, Transformer-based\nmodels have demonstrated impressive capabilities. However, the quadratic\ncomplexity of the Transformer's self-attention mechanism results in\ncomputational inefficiencies. In this paper, we propose a network architecture\nfor SELD called SELD-Mamba, which utilizes Mamba, a selective state-space\nmodel. We adopt the Event-Independent Network V2 (EINV2) as the foundational\nframework and replace its Conformer blocks with bidirectional Mamba blocks to\ncapture a broader range of contextual information while maintaining\ncomputational efficiency. Additionally, we implement a two-stage training\nmethod, with the first stage focusing on Sound Event Detection (SED) and\nDirection of Arrival (DoA) estimation losses, and the second stage\nreintroducing the Source Distance Estimation (SDE) loss. Our experimental\nresults on the 2024 DCASE Challenge Task3 dataset demonstrate the effectiveness\nof the selective state-space model in SELD and highlight the benefits of the\ntwo-stage training approach in enhancing SELD performance.", "published": "2024-08-09 13:26:08", "link": "http://arxiv.org/abs/2408.05057v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AcousAF: Acoustic Sensing-Based Atrial Fibrillation Detection System for\n  Mobile Phones", "abstract": "Atrial fibrillation (AF) is characterized by irregular electrical impulses\noriginating in the atria, which can lead to severe complications and even\ndeath. Due to the intermittent nature of the AF, early and timely monitoring of\nAF is critical for patients to prevent further exacerbation of the condition.\nAlthough ambulatory ECG Holter monitors provide accurate monitoring, the high\ncost of these devices hinders their wider adoption. Current mobile-based AF\ndetection systems offer a portable solution. However, these systems have\nvarious applicability issues, such as being easily affected by environmental\nfactors and requiring significant user effort. To overcome the above\nlimitations, we present AcousAF, a novel AF detection system based on acoustic\nsensors of smartphones. Particularly, we explore the potential of pulse wave\nacquisition from the wrist using smartphone speakers and microphones. In\naddition, we propose a well-designed framework comprised of pulse wave probing,\npulse wave extraction, and AF detection to ensure accurate and reliable AF\ndetection. We collect data from 20 participants utilizing our custom data\ncollection application on the smartphone. Extensive experimental results\ndemonstrate the high performance of our system, with 92.8% accuracy, 86.9%\nprecision, 87.4% recall, and 87.1% F1 Score.", "published": "2024-08-09 07:43:16", "link": "http://arxiv.org/abs/2408.04912v1", "categories": ["cs.SD", "cs.CE", "cs.ET", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
