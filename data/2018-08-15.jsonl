{"title": "Folksonomication: Predicting Tags for Movies from Plot Synopses Using\n  Emotion Flow Encoded Neural Network", "abstract": "Folksonomy of movies covers a wide range of heterogeneous information about\nmovies, like the genre, plot structure, visual experiences, soundtracks,\nmetadata, and emotional experiences from watching a movie. Being able to\nautomatically generate or predict tags for movies can help recommendation\nengines improve retrieval of similar movies, and help viewers know what to\nexpect from a movie in advance. In this work, we explore the problem of\ncreating tags for movies from plot synopses. We propose a novel neural network\nmodel that merges information from synopses and emotion flows throughout the\nplots to predict a set of tags for movies. We compare our system with multiple\nbaselines and found that the addition of emotion flows boosts the performance\nof the network by learning ~18\\% more tags than a traditional machine learning\nsystem.", "published": "2018-08-15 01:50:56", "link": "http://arxiv.org/abs/1808.04943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Putting the Horse Before the Cart:A Generator-Evaluator Framework for\n  Question Generation from Text", "abstract": "Automatic question generation (QG) is a useful yet challenging task in NLP.\nRecent neural network-based approaches represent the state-of-the-art in this\ntask. In this work, we attempt to strengthen them significantly by adopting a\nholistic and novel generator-evaluator framework that directly optimizes\nobjectives that reward semantics and structure. The {\\it generator} is a\nsequence-to-sequence model that incorporates the {\\it structure} and {\\it\nsemantics} of the question being generated. The generator predicts an answer in\nthe passage that the question can pivot on. Employing the copy and coverage\nmechanisms, it also acknowledges other contextually important (and possibly\nrare) keywords in the passage that the question needs to conform to, while not\nredundantly repeating words. The {\\it evaluator} model evaluates and assigns a\nreward to each predicted question based on its conformity to the {\\it\nstructure} of ground-truth questions. We propose two novel QG-specific reward\nfunctions for text conformity and answer conformity of the generated question.\nThe evaluator also employs structure-sensitive rewards based on evaluation\nmeasures such as BLEU, GLEU, and ROUGE-L, which are suitable for QG. In\ncontrast, most of the previous works only optimize the cross-entropy loss,\nwhich can induce inconsistencies between training (objective) and testing\n(evaluation) measures. Our evaluation shows that our approach significantly\noutperforms state-of-the-art systems on the widely-used SQuAD benchmark as per\nboth automatic and human evaluation.", "published": "2018-08-15 04:05:02", "link": "http://arxiv.org/abs/1808.04961v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple Character Embeddings for Chinese Word Segmentation", "abstract": "Chinese word segmentation (CWS) is often regarded as a character-based\nsequence labeling task in most current works which have achieved great success\nwith the help of powerful neural networks. However, these works neglect an\nimportant clue: Chinese characters incorporate both semantic and phonetic\nmeanings. In this paper, we introduce multiple character embeddings including\nPinyin Romanization and Wubi Input, both of which are easily accessible and\neffective in depicting semantics of characters. We propose a novel shared\nBi-LSTM-CRF model to fuse linguistic features efficiently by sharing the LSTM\nnetwork during the training procedure. Extensive experiments on five corpora\nshow that extra embeddings help obtain a significant improvement in labeling\naccuracy. Specifically, we achieve the state-of-the-art performance in AS and\nCityU corpora with F1 scores of 96.9 and 97.3, respectively without leveraging\nany external lexical resources.", "published": "2018-08-15 04:10:35", "link": "http://arxiv.org/abs/1808.04963v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Deep Learning for Persian Sentiment Analysis", "abstract": "The rise of social media is enabling people to freely express their opinions\nabout products and services. The aim of sentiment analysis is to automatically\ndetermine subject's sentiment (e.g., positive, negative, or neutral) towards a\nparticular aspect such as topic, product, movie, news etc. Deep learning has\nrecently emerged as a powerful machine learning technique to tackle a growing\ndemand of accurate sentiment analysis. However, limited work has been conducted\nto apply deep learning algorithms to languages other than English, such as\nPersian. In this work, two deep learning models (deep autoencoders and deep\nconvolutional neural networks (CNNs)) are developed and applied to a novel\nPersian movie reviews dataset. The proposed deep learning models are analyzed\nand compared with the state-of-the-art shallow multilayer perceptron (MLP)\nbased machine learning model. Simulation results demonstrate the enhanced\nperformance of deep learning over state-of-the-art MLP.", "published": "2018-08-15 13:46:54", "link": "http://arxiv.org/abs/1808.05077v1", "categories": ["cs.CL", "I.2.7; I.5.0"], "primary_category": "cs.CL"}
{"title": "SentiALG: Automated Corpus Annotation for Algerian Sentiment Analysis", "abstract": "Data annotation is an important but time-consuming and costly procedure. To\nsort a text into two classes, the very first thing we need is a good annotation\nguideline, establishing what is required to qualify for each class. In the\nliterature, the difficulties associated with an appropriate data annotation has\nbeen underestimated. In this paper, we present a novel approach to\nautomatically construct an annotated sentiment corpus for Algerian dialect (a\nMaghrebi Arabic dialect). The construction of this corpus is based on an\nAlgerian sentiment lexicon that is also constructed automatically. The\npresented work deals with the two widely used scripts on Arabic social media:\nArabic and Arabizi. The proposed approach automatically constructs a sentiment\ncorpus containing 8000 messages (where 4000 are dedicated to Arabic and 4000 to\nArabizi). The achieved F1-score is up to 72% and 78% for an Arabic and Arabizi\ntest sets, respectively. Ongoing work is aimed at integrating transliteration\nprocess for Arabizi messages to further improve the obtained results.", "published": "2018-08-15 13:48:16", "link": "http://arxiv.org/abs/1808.05079v1", "categories": ["cs.CL", "I.5.0; I.2.7"], "primary_category": "cs.CL"}
{"title": "Incorporating Consistency Verification into Neural Data-to-Document\n  Generation", "abstract": "Recent neural models for data-to-document generation have achieved remarkable\nprogress in producing fluent and informative texts. However, large proportions\nof generated texts do not actually conform to the input data. To address this\nissue, we propose a new training framework which attempts to verify the\nconsistency between the generated texts and the input data to guide the\ntraining process. To measure the consistency, a relation extraction model is\napplied to check information overlaps between the input data and the generated\ntexts. The non-differentiable consistency signal is optimized via reinforcement\nlearning. Experimental results on a recently released challenging dataset\nROTOWIRE show improvements from our framework in various metrics.", "published": "2018-08-15 23:23:20", "link": "http://arxiv.org/abs/1808.05306v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep EHR: Chronic Disease Prediction Using Medical Notes", "abstract": "Early detection of preventable diseases is important for better disease\nmanagement, improved inter-ventions, and more efficient health-care resource\nallocation. Various machine learning approacheshave been developed to utilize\ninformation in Electronic Health Record (EHR) for this task. Majorityof\nprevious attempts, however, focus on structured fields and lose the vast amount\nof information inthe unstructured notes. In this work we propose a general\nmulti-task framework for disease onsetprediction that combines both free-text\nmedical notes and structured information. We compareperformance of different\ndeep learning architectures including CNN, LSTM and hierarchical models.In\ncontrast to traditional text-based prediction models, our approach does not\nrequire disease specificfeature engineering, and can handle negations and\nnumerical values that exist in the text. Ourresults on a cohort of about 1\nmillion patients show that models using text outperform modelsusing just\nstructured data, and that models capable of using numerical values and\nnegations in thetext, in addition to the raw text, further improve performance.\nAdditionally, we compare differentvisualization methods for medical\nprofessionals to interpret model predictions.", "published": "2018-08-15 00:10:55", "link": "http://arxiv.org/abs/1808.04928v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Investigation of Using Disentangled and Interpretable Representations\n  for One-shot Cross-lingual Voice Conversion", "abstract": "We study the problem of cross-lingual voice conversion in non-parallel speech\ncorpora and one-shot learning setting. Most prior work require either parallel\nspeech corpora or enough amount of training data from a target speaker.\nHowever, we convert an arbitrary sentences of an arbitrary source speaker to\ntarget speaker's given only one target speaker training utterance. To achieve\nthis, we formulate the problem as learning disentangled speaker-specific and\ncontext-specific representations and follow the idea of [1] which uses\nFactorized Hierarchical Variational Autoencoder (FHVAE). After training FHVAE\non multi-speaker training data, given arbitrary source and target speakers'\nutterance, we estimate those latent representations and then reconstruct the\ndesired utterance of converted voice to that of target speaker. We investigate\nthe effectiveness of the approach by conducting voice conversion experiments\nwith varying size of training utterances and it was able to achieve reasonable\nperformance with even just one training utterance. We also examine the speech\nrepresentation and show that World vocoder outperforms Short-time Fourier\nTransform (STFT) used in [1]. Finally, in the subjective tests, for one\nlanguage and cross-lingual voice conversion, our approach achieved\nsignificantly better or comparable results compared to VAE-STFT and GMM\nbaselines in speech quality and similarity.", "published": "2018-08-15 22:21:42", "link": "http://arxiv.org/abs/1808.05294v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Statistical Piano Reduction Controlling Performance Difficulty", "abstract": "We present a statistical-modelling method for piano reduction, i.e.\nconverting an ensemble score into piano scores, that can control performance\ndifficulty. While previous studies have focused on describing the condition for\nplayable piano scores, it depends on player's skill and can change continuously\nwith the tempo. We thus computationally quantify performance difficulty as well\nas musical fidelity to the original score, and formulate the problem as\noptimization of musical fidelity under constraints on difficulty values. First,\nperformance difficulty measures are developed by means of probabilistic\ngenerative models for piano scores and the relation to the rate of performance\nerrors is studied. Second, to describe musical fidelity, we construct a\nprobabilistic model integrating a prior piano-score model and a model\nrepresenting how ensemble scores are likely to be edited. An iterative\noptimization algorithm for piano reduction is developed based on statistical\ninference of the model. We confirm the effect of the iterative procedure; we\nfind that subjective difficulty and musical fidelity monotonically increase\nwith controlled difficulty values; and we show that incorporating sequential\ndependence of pitches and fingering motion in the piano-score model improves\nthe quality of reduction scores in high-difficulty cases.", "published": "2018-08-15 09:08:39", "link": "http://arxiv.org/abs/1808.05006v2", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
