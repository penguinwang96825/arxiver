{"title": "Extracting linguistic speech patterns of Japanese fictional characters\n  using subword units", "abstract": "This study extracted and analyzed the linguistic speech patterns that\ncharacterize Japanese anime or game characters. Conventional morphological\nanalyzers, such as MeCab, segment words with high performance, but they are\nunable to segment broken expressions or utterance endings that are not listed\nin the dictionary, which often appears in lines of anime or game characters. To\novercome this challenge, we propose segmenting lines of Japanese anime or game\ncharacters using subword units that were proposed mainly for deep learning, and\nextracting frequently occurring strings to obtain expressions that characterize\ntheir utterances. We analyzed the subword units weighted by TF/IDF according to\ngender, age, and each anime character and show that they are linguistic speech\npatterns that are specific for each feature. Additionally, a classification\nexperiment shows that the model with subword units outperformed that with the\nconventional method.", "published": "2022-03-05 01:15:00", "link": "http://arxiv.org/abs/2203.02632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistent Representation Learning for Continual Relation Extraction", "abstract": "Continual relation extraction (CRE) aims to continuously train a model on\ndata with new relations while avoiding forgetting old ones. Some previous work\nhas proved that storing a few typical samples of old relations and replaying\nthem when learning new relations can effectively avoid forgetting. However,\nthese memory-based methods tend to overfit the memory samples and perform\npoorly on imbalanced datasets. To solve these challenges, a consistent\nrepresentation learning method is proposed, which maintains the stability of\nthe relation embedding by adopting contrastive learning and knowledge\ndistillation when replaying memory. Specifically, supervised contrastive\nlearning based on a memory bank is first used to train each new task so that\nthe model can effectively learn the relation representation. Then, contrastive\nreplay is conducted of the samples in memory and makes the model retain the\nknowledge of historical relations through memory knowledge distillation to\nprevent the catastrophic forgetting of the old task. The proposed method can\nbetter learn consistent representations to alleviate forgetting effectively.\nExtensive experiments on FewRel and TACRED datasets show that our method\nsignificantly outperforms state-of-the-art baselines and yield strong\nrobustness on the imbalanced dataset.", "published": "2022-03-05 12:16:34", "link": "http://arxiv.org/abs/2203.02721v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feeding What You Need by Understanding What You Learned", "abstract": "Machine Reading Comprehension (MRC) reveals the ability to understand a given\ntext passage and answer questions based on it. Existing research works in MRC\nrely heavily on large-size models and corpus to improve the performance\nevaluated by metrics such as Exact Match ($EM$) and $F_1$. However, such a\nparadigm lacks sufficient interpretation to model capability and can not\nefficiently train a model with a large corpus. In this paper, we argue that a\ndeep understanding of model capabilities and data properties can help us feed a\nmodel with appropriate training data based on its learning status.\nSpecifically, we design an MRC capability assessment framework that assesses\nmodel capabilities in an explainable and multi-dimensional manner. Based on it,\nwe further uncover and disentangle the connections between various data\nproperties and model performance. Finally, to verify the effectiveness of the\nproposed MRC capability assessment framework, we incorporate it into a\ncurriculum learning pipeline and devise a Capability Boundary Breakthrough\nCurriculum (CBBC) strategy, which performs a model capability-based training to\nmaximize the data value and improve training efficiency. Extensive experiments\ndemonstrate that our approach significantly improves performance, achieving up\nto an 11.22% / 8.71% improvement of $EM$ / $F_1$ on MRC tasks.", "published": "2022-03-05 14:15:59", "link": "http://arxiv.org/abs/2203.02753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClueGraphSum: Let Key Clues Guide the Cross-Lingual Abstractive\n  Summarization", "abstract": "Cross-Lingual Summarization (CLS) is the task to generate a summary in one\nlanguage for an article in a different language. Previous studies on CLS mainly\ntake pipeline methods or train the end-to-end model using the translated\nparallel data. However, the quality of generated cross-lingual summaries needs\nmore further efforts to improve, and the model performance has never been\nevaluated on the hand-written CLS dataset. Therefore, we first propose a\nclue-guided cross-lingual abstractive summarization method to improve the\nquality of cross-lingual summaries, and then construct a novel hand-written CLS\ndataset for evaluation. Specifically, we extract keywords, named entities, etc.\nof the input article as key clues for summarization and then design a\nclue-guided algorithm to transform an article into a graph with less noisy\nsentences. One Graph encoder is built to learn sentence semantics and article\nstructures and one Clue encoder is built to encode and translate key clues,\nensuring the information of important parts are reserved in the generated\nsummary. These two encoders are connected by one decoder to directly learn\ncross-lingual semantics. Experimental results show that our method has stronger\nrobustness for longer inputs and substantially improves the performance over\nthe strong baseline, achieving an improvement of 8.55 ROUGE-1\n(English-to-Chinese summarization) and 2.13 MoverScore (Chinese-to-English\nsummarization) scores over the existing SOTA.", "published": "2022-03-05 18:01:11", "link": "http://arxiv.org/abs/2203.02797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unfreeze with Care: Space-Efficient Fine-Tuning of Semantic Parsing\n  Models", "abstract": "Semantic parsing is a key NLP task that maps natural language to structured\nmeaning representations. As in many other NLP tasks, SOTA performance in\nsemantic parsing is now attained by fine-tuning a large pretrained language\nmodel (PLM). While effective, this approach is inefficient in the presence of\nmultiple downstream tasks, as a new set of values for all parameters of the PLM\nneeds to be stored for each task separately. Recent work has explored methods\nfor adapting PLMs to downstream tasks while keeping most (or all) of their\nparameters frozen. We examine two such promising techniques, prefix tuning and\nbias-term tuning, specifically on semantic parsing. We compare them against\neach other on two different semantic parsing datasets, and we also compare them\nagainst full and partial fine-tuning, both in few-shot and conventional data\nsettings. While prefix tuning is shown to do poorly for semantic parsing tasks\noff the shelf, we modify it by adding special token embeddings, which results\nin very strong performance without compromising parameter savings.", "published": "2022-03-05 04:30:03", "link": "http://arxiv.org/abs/2203.02652v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross Language Image Matching for Weakly Supervised Semantic\n  Segmentation", "abstract": "It has been widely known that CAM (Class Activation Map) usually only\nactivates discriminative object regions and falsely includes lots of\nobject-related backgrounds. As only a fixed set of image-level object labels\nare available to the WSSS (weakly supervised semantic segmentation) model, it\ncould be very difficult to suppress those diverse background regions consisting\nof open set objects. In this paper, we propose a novel Cross Language Image\nMatching (CLIMS) framework, based on the recently introduced Contrastive\nLanguage-Image Pre-training (CLIP) model, for WSSS. The core idea of our\nframework is to introduce natural language supervision to activate more\ncomplete object regions and suppress closely-related open background regions.\nIn particular, we design object, background region and text label matching\nlosses to guide the model to excite more reasonable object regions for CAM of\neach category. In addition, we design a co-occurring background suppression\nloss to prevent the model from activating closely-related background regions,\nwith a predefined set of class-related background text descriptions. These\ndesigns enable the proposed CLIMS to generate a more complete and compact\nactivation map for the target objects. Extensive experiments on PASCAL VOC2012\ndataset show that our CLIMS significantly outperforms the previous\nstate-of-the-art methods.", "published": "2022-03-05 06:39:48", "link": "http://arxiv.org/abs/2203.02668v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Just Rank: Rethinking Evaluation with Word and Sentence Similarities", "abstract": "Word and sentence embeddings are useful feature representations in natural\nlanguage processing. However, intrinsic evaluation for embeddings lags far\nbehind, and there has been no significant update since the past decade. Word\nand sentence similarity tasks have become the de facto evaluation method. It\nleads models to overfit to such evaluations, negatively impacting embedding\nmodels' development. This paper first points out the problems using semantic\nsimilarity as the gold standard for word and sentence embedding evaluations.\nFurther, we propose a new intrinsic evaluation method called EvalRank, which\nshows a much stronger correlation with downstream tasks. Extensive experiments\nare conducted based on 60+ models and popular datasets to certify our\njudgments. Finally, the practical evaluation toolkit is released for future\nbenchmarking purposes.", "published": "2022-03-05 08:40:05", "link": "http://arxiv.org/abs/2203.02679v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Proof is in the Pudding: Using Automated Theorem Proving to Generate\n  Cooking Recipes", "abstract": "This paper presents FASTFOOD, a rule-based Natural Language Generation\nProgram for cooking recipes. Recipes are generated by using an Automated\nTheorem Proving procedure to select the ingredients and instructions, with\ningredients corresponding to axioms and instructions to implications. FASTFOOD\nalso contains a temporal optimization module which can rearrange the recipe to\nmake it more time-efficient for the user, e.g. the recipe specifies to chop the\nvegetables while the rice is boiling. The system is described in detail, using\na framework which divides Natural Language Generation into 4 phases: content\nproduction, content selection, content organisation and content realisation. A\ncomparison is then made with similar existing systems and techniques.", "published": "2022-03-05 08:50:34", "link": "http://arxiv.org/abs/2203.02683v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NeuralDPS: Neural Deterministic Plus Stochastic Model with Multiband\n  Excitation for Noise-Controllable Waveform Generation", "abstract": "The traditional vocoders have the advantages of high synthesis efficiency,\nstrong interpretability, and speech editability, while the neural vocoders have\nthe advantage of high synthesis quality. To combine the advantages of two\nvocoders, inspired by the traditional deterministic plus stochastic model, this\npaper proposes a novel neural vocoder named NeuralDPS which can retain high\nspeech quality and acquire high synthesis efficiency and noise controllability.\nFirstly, this framework contains four modules: a deterministic source module, a\nstochastic source module, a neural V/UV decision module and a neural filter\nmodule. The input required by the vocoder is just the spectral parameter, which\navoids the error caused by estimating additional parameters, such as F0.\nSecondly, to solve the problem that different frequency bands may have\ndifferent proportions of deterministic components and stochastic components, a\nmultiband excitation strategy is used to generate a more accurate excitation\nsignal and reduce the neural filter's burden. Thirdly, a method to control\nnoise components of speech is proposed. In this way, the signal-to-noise ratio\n(SNR) of speech can be adjusted easily. Objective and subjective experimental\nresults show that our proposed NeuralDPS vocoder can obtain similar performance\nwith the WaveNet and it generates waveforms at least 280 times faster than the\nWaveNet vocoder. It is also 28% faster than WaveGAN's synthesis efficiency on a\nsingle CPU core. We have also verified through experiments that this method can\neffectively control the noise components in the predicted speech and adjust the\nSNR of speech. Examples of generated speech can be found at\nhttps://hairuo55.github.io/NeuralDPS.", "published": "2022-03-05 08:15:29", "link": "http://arxiv.org/abs/2203.02678v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Impact of Differential Privacy on Group Disparity Mitigation", "abstract": "The performance cost of differential privacy has, for some applications, been\nshown to be higher for minority groups; fairness, conversely, has been shown to\ndisproportionally compromise the privacy of members of such groups. Most work\nin this area has been restricted to computer vision and risk assessment. In\nthis paper, we evaluate the impact of differential privacy on fairness across\nfour tasks, focusing on how attempts to mitigate privacy violations and\nbetween-group performance differences interact: Does privacy inhibit attempts\nto ensure fairness? To this end, we train $(\\varepsilon,\\delta)$-differentially\nprivate models with empirical risk minimization and group distributionally\nrobust training objectives. Consistent with previous findings, we find that\ndifferential privacy increases between-group performance differences in the\nbaseline setting; but more interestingly, differential privacy reduces\nbetween-group performance differences in the robust setting. We explain this by\nreinterpreting differential privacy as regularization.", "published": "2022-03-05 13:55:05", "link": "http://arxiv.org/abs/2203.02745v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Bridging the Gap Between Learning in Discrete and Continuous\n  Environments for Vision-and-Language Navigation", "abstract": "Most existing works in vision-and-language navigation (VLN) focus on either\ndiscrete or continuous environments, training agents that cannot generalize\nacross the two. The fundamental difference between the two setups is that\ndiscrete navigation assumes prior knowledge of the connectivity graph of the\nenvironment, so that the agent can effectively transfer the problem of\nnavigation with low-level controls to jumping from node to node with high-level\nactions by grounding to an image of a navigable direction. To bridge the\ndiscrete-to-continuous gap, we propose a predictor to generate a set of\ncandidate waypoints during navigation, so that agents designed with high-level\nactions can be transferred to and trained in continuous environments. We refine\nthe connectivity graph of Matterport3D to fit the continuous\nHabitat-Matterport3D, and train the waypoints predictor with the refined graphs\nto produce accessible waypoints at each time step. Moreover, we demonstrate\nthat the predicted waypoints can be augmented during training to diversify the\nviews and paths, and therefore enhance agent's generalization ability. Through\nextensive experiments we show that agents navigating in continuous environments\nwith predicted waypoints perform significantly better than agents using\nlow-level actions, which reduces the absolute discrete-to-continuous gap by\n11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent\nand 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple\nimitation learning objective, outperform previous methods by a large margin,\nachieving new state-of-the-art results on the testing environments of the\nR2R-CE and the RxR-CE datasets.", "published": "2022-03-05 14:56:14", "link": "http://arxiv.org/abs/2203.02764v1", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Audio-visual speech separation based on joint feature representation\n  with cross-modal attention", "abstract": "Multi-modal based speech separation has exhibited a specific advantage on\nisolating the target character in multi-talker noisy environments.\nUnfortunately, most of current separation strategies prefer a straightforward\nfusion based on feature learning of each single modality, which is far from\nsufficient consideration of inter-relationships between modalites. Inspired by\nlearning joint feature representations from audio and visual streams with\nattention mechanism, in this study, a novel cross-modal fusion strategy is\nproposed to benefit the whole framework with semantic correlations between\ndifferent modalities. To further improve audio-visual speech separation, the\ndense optical flow of lip motion is incorporated to strengthen the robustness\nof visual representation. The evaluation of the proposed work is performed on\ntwo public audio-visual speech separation benchmark datasets. The overall\nimprovement of the performance has demonstrated that the additional motion\nnetwork effectively enhances the visual representation of the combined lip\nimages and audio signal, as well as outperforming the baseline in terms of all\nmetrics with the proposed cross-modal fusion.", "published": "2022-03-05 04:39:46", "link": "http://arxiv.org/abs/2203.02655v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Language vs Speaker Change: A Comparative Study", "abstract": "Spoken language change detection (LCD) refers to detecting language switching\npoints in a multilingual speech signal. Speaker change detection (SCD) refers\nto locating the speaker change points in a multispeaker speech signal. The\nobjective of this work is to understand the challenges in LCD task by comparing\nit with SCD task. Human subjective study for change detection is performed for\nLCD and SCD. This study demonstrates that LCD requires larger duration\nspectro-temporal information around the change point compared to SCD. Based on\nthis, the work explores automatic distance based and model based LCD\napproaches. The model based ones include Gaussian mixture model and universal\nbackground model (GMM-UBM), attention, and Generative adversarial network (GAN)\nbased approaches. Both the human and automatic LCD tasks infer that the\nperformance of the LCD task improves by incorporating more and more\nspectro-temporal duration.", "published": "2022-03-05 08:44:59", "link": "http://arxiv.org/abs/2203.02680v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "aaeCAPTCHA: The Design and Implementation of Audio Adversarial CAPTCHA", "abstract": "CAPTCHAs are designed to prevent malicious bot programs from abusing\nwebsites. Most online service providers deploy audio CAPTCHAs as an alternative\nto text and image CAPTCHAs for visually impaired users. However, prior research\ninvestigating the security of audio CAPTCHAs found them highly vulnerable to\nautomated attacks using Automatic Speech Recognition (ASR) systems. To improve\nthe robustness of audio CAPTCHAs against automated abuses, we present the\ndesign and implementation of an audio adversarial CAPTCHA (aaeCAPTCHA) system\nin this paper. The aaeCAPTCHA system exploits audio adversarial examples as\nCAPTCHAs to prevent the ASR systems from automatically solving them.\nFurthermore, we conducted a rigorous security evaluation of our new audio\nCAPTCHA design against five state-of-the-art DNN-based ASR systems and three\ncommercial Speech-to-Text (STT) services. Our experimental evaluations\ndemonstrate that aaeCAPTCHA is highly secure against these speech recognition\ntechnologies, even when the attacker has complete knowledge of the current\nattacks against audio adversarial examples. We also conducted a usability\nevaluation of the proof-of-concept implementation of the aaeCAPTCHA scheme. Our\nresults show that it achieves high robustness at a moderate usability cost\ncompared to normal audio CAPTCHAs. Finally, our extensive analysis highlights\nthat aaeCAPTCHA can significantly enhance the security and robustness of\ntraditional audio CAPTCHA systems while maintaining similar usability.", "published": "2022-03-05 13:32:19", "link": "http://arxiv.org/abs/2203.02735v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
