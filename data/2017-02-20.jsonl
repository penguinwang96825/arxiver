{"title": "Post-edit Analysis of Collective Biography Generation", "abstract": "Text generation is increasingly common but often requires manual post-editing\nwhere high precision is critical to end users. However, manual editing is\nexpensive so we want to ensure this effort is focused on high-value tasks. And\nwe want to maintain stylistic consistency, a particular challenge in crowd\nsettings. We present a case study, analysing human post-editing in the context\nof a template-based biography generation system. An edit flow visualisation\ncombined with manual characterisation of edits helps identify and prioritise\nwork for improving end-to-end efficiency and accuracy.", "published": "2017-02-20 00:23:06", "link": "http://arxiv.org/abs/1702.05821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Variable Dialogue Models and their Diversity", "abstract": "We present a dialogue generation model that directly captures the variability\nin possible responses to a given input, which reduces the `boring output' issue\nof deterministic dialogue models. Experiments show that our model generates\nmore diverse outputs than baseline models, and also generates more consistently\nacceptable output than sampling from a deterministic encoder-decoder model.", "published": "2017-02-20 13:36:23", "link": "http://arxiv.org/abs/1702.05962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parent Oriented Teacher Selection Causes Language Diversity", "abstract": "An evolutionary model for emergence of diversity in language is developed. We\ninvestigated the effects of two real life observations, namely, people prefer\npeople that they communicate with well, and people interact with people that\nare physically close to each other. Clearly these groups are relatively small\ncompared to the entire population. We restrict selection of the teachers from\nsuch small groups, called imitation sets, around parents. Then the child learns\nlanguage from a teacher selected within the imitation set of her parent. As a\nresult, there are subcommunities with their own languages developed. Within\nsubcommunity comprehension is found to be high. The number of languages is\nrelated to the relative size of imitation set by a power law.", "published": "2017-02-20 15:53:56", "link": "http://arxiv.org/abs/1702.06027v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enabling Multi-Source Neural Machine Translation By Concatenating Source\n  Sentences In Multiple Languages", "abstract": "In this paper, we explore a simple solution to \"Multi-Source Neural Machine\nTranslation\" (MSNMT) which only relies on preprocessing a N-way multilingual\ncorpus without modifying the Neural Machine Translation (NMT) architecture or\ntraining procedure. We simply concatenate the source sentences to form a single\nlong multi-source input sentence while keeping the target side sentence as it\nis and train an NMT system using this preprocessed corpus. We evaluate our\nmethod in resource poor as well as resource rich settings and show its\neffectiveness (up to 4 BLEU using 2 source languages and up to 6 BLEU using 5\nsource languages). We also compare against existing methods for MSNMT and show\nthat our solution gives competitive results despite its simplicity. We also\nprovide some insights on how the NMT system leverages multilingual information\nin such a scenario by visualizing attention.", "published": "2017-02-20 19:00:06", "link": "http://arxiv.org/abs/1702.06135v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Filtering Tweets for Social Unrest", "abstract": "Since the events of the Arab Spring, there has been increased interest in\nusing social media to anticipate social unrest. While efforts have been made\ntoward automated unrest prediction, we focus on filtering the vast volume of\ntweets to identify tweets relevant to unrest, which can be provided to\ndownstream users for further analysis. We train a supervised classifier that is\nable to label Arabic language tweets as relevant to unrest with high\nreliability. We examine the relationship between training data size and\nperformance and investigate ways to optimize the model building process while\nminimizing cost. We also explore how confidence thresholds can be set to\nachieve desired levels of performance.", "published": "2017-02-20 23:48:39", "link": "http://arxiv.org/abs/1702.06216v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML", "H.3.3; I.2.6; I.2.7; I.5.4"], "primary_category": "cs.CL"}
