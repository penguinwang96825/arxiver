{"title": "On the Impact of Various Types of Noise on Neural Machine Translation", "abstract": "We examine how various types of noise in the parallel training data impact\nthe quality of neural machine translation systems. We create five types of\nartificial noise and analyze how they degrade performance in neural and\nstatistical machine translation. We find that neural models are generally more\nharmed by noise than statistical models. For one especially egregious type of\nnoise they learn to just copy the input sentence.", "published": "2018-05-31 01:33:19", "link": "http://arxiv.org/abs/1805.12282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Evaluation of Character-Based Model on Neural Named-Entity\n  Recognition in Indonesian Conversational Texts", "abstract": "Despite the long history of named-entity recognition (NER) task in the\nnatural language processing community, previous work rarely studied the task on\nconversational texts. Such texts are challenging because they contain a lot of\nword variations which increase the number of out-of-vocabulary (OOV) words. The\nhigh number of OOV words poses a difficulty for word-based neural models.\nMeanwhile, there is plenty of evidence to the effectiveness of character-based\nneural models in mitigating this OOV problem. We report an empirical evaluation\nof neural sequence labeling models with character embedding to tackle NER task\nin Indonesian conversational texts. Our experiments show that (1) character\nmodels outperform word embedding-only models by up to 4 $F_1$ points, (2)\ncharacter models perform better in OOV cases with an improvement of as high as\n15 $F_1$ points, and (3) character models are robust against a very high OOV\nrate.", "published": "2018-05-31 02:21:39", "link": "http://arxiv.org/abs/1805.12291v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-Based LSTM for Psychological Stress Detection from Spoken\n  Language Using Distant Supervision", "abstract": "We propose a Long Short-Term Memory (LSTM) with attention mechanism to\nclassify psychological stress from self-conducted interview transcriptions. We\napply distant supervision by automatically labeling tweets based on their\nhashtag content, which complements and expands the size of our corpus. This\nadditional data is used to initialize the model parameters, and which it is\nfine-tuned using the interview data. This improves the model's robustness,\nespecially by expanding the vocabulary size. The bidirectional LSTM model with\nattention is found to be the best model in terms of accuracy (74.1%) and\nf-score (74.3%). Furthermore, we show that distant supervision fine-tuning\nenhances the model's performance by 1.6% accuracy and 2.1% f-score. The\nattention mechanism helps the model to select informative words.", "published": "2018-05-31 03:27:53", "link": "http://arxiv.org/abs/1805.12307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval 2019 Shared Task: Cross-lingual Semantic Parsing with UCCA -\n  Call for Participation", "abstract": "We announce a shared task on UCCA parsing in English, German and French, and\ncall for participants to submit their systems. UCCA is a cross-linguistically\napplicable framework for semantic representation, which builds on extensive\ntypological work and supports rapid annotation. UCCA poses a challenge for\nexisting parsing techniques, as it exhibits reentrancy (resulting in DAG\nstructures), discontinuous structures and non-terminal nodes corresponding to\ncomplex semantic units. Given the success of recent semantic parsing shared\ntasks (on SDP and AMR), we expect the task to have a significant contribution\nto the advancement of UCCA parsing in particular, and semantic parsing in\ngeneral. Furthermore, existing applications for semantic evaluation that are\nbased on UCCA will greatly benefit from better automatic methods for UCCA\nparsing. The competition website is\nhttps://competitions.codalab.org/competitions/19160", "published": "2018-05-31 09:11:16", "link": "http://arxiv.org/abs/1805.12386v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Network Acceptability Judgments", "abstract": "This paper investigates the ability of artificial neural networks to judge\nthe grammatical acceptability of a sentence, with the goal of testing their\nlinguistic competence. We introduce the Corpus of Linguistic Acceptability\n(CoLA), a set of 10,657 English sentences labeled as grammatical or\nungrammatical from published linguistics literature. As baselines, we train\nseveral recurrent neural network models on acceptability classification, and\nfind that our models outperform unsupervised models by Lau et al (2016) on\nCoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et\nal.'s models and ours learn systematic generalizations like subject-verb-object\norder. However, all models we test perform far below human level on a wide\nrange of grammatical constructions.", "published": "2018-05-31 13:52:06", "link": "http://arxiv.org/abs/1805.12471v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Label Transfer Learning for Multi-Relational Semantic Similarity", "abstract": "Multi-relational semantic similarity datasets define the semantic relations\nbetween two short texts in multiple ways, e.g., similarity, relatedness, and so\non. Yet, all the systems to date designed to capture such relations target one\nrelation at a time. We propose a multi-label transfer learning approach based\non LSTM to make predictions for several relations simultaneously and aggregate\nthe losses to update the parameters. This multi-label regression approach\njointly learns the information provided by the multiple relations, rather than\ntreating them as separate tasks. Not only does this approach outperform the\nsingle-task approach and the traditional multi-task learning approach, but it\nalso achieves state-of-the-art performance on all but one relation of the Human\nActivity Phrase dataset.", "published": "2018-05-31 14:54:33", "link": "http://arxiv.org/abs/1805.12501v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Natural Language Processing: Challenges, Strategies, and\n  Evaluation", "abstract": "Incrementality is ubiquitous in human-human interaction and beneficial for\nhuman-computer interaction. It has been a topic of research in different parts\nof the NLP community, mostly with focus on the specific topic at hand even\nthough incremental systems have to deal with similar challenges regardless of\ndomain. In this survey, I consolidate and categorize the approaches,\nidentifying similarities and differences in the computation and data, and show\ntrade-offs that have to be considered. A focus lies on evaluating incremental\nsystems because the standard metrics often fail to capture the incremental\nproperties of a system and coming up with a suitable evaluation scheme is\nnon-trivial.", "published": "2018-05-31 15:29:32", "link": "http://arxiv.org/abs/1805.12518v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text normalization using memory augmented neural networks", "abstract": "We perform text normalization, i.e. the transformation of words from the\nwritten to the spoken form, using a memory augmented neural network. With the\naddition of dynamic memory access and storage mechanism, we present a neural\narchitecture that will serve as a language-agnostic text normalization system\nwhile avoiding the kind of unacceptable errors made by the LSTM-based recurrent\nneural networks. By successfully reducing the frequency of such mistakes, we\nshow that this novel architecture is indeed a better alternative. Our proposed\nsystem requires significantly lesser amounts of data, training time and compute\nresources. Additionally, we perform data up-sampling, circumventing the data\nsparsity problem in some semiotic classes, to show that sufficient examples in\nany particular class can improve the performance of our text normalization\nsystem. Although a few occurrences of these errors still remain in certain\nsemiotic classes, we demonstrate that memory augmented networks with\nmeta-learning capabilities can open many doors to a superior text normalization\nsystem.", "published": "2018-05-31 18:37:37", "link": "http://arxiv.org/abs/1806.00044v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogWAE: Multimodal Response Generation with Conditional Wasserstein\n  Auto-Encoder", "abstract": "Variational autoencoders~(VAEs) have shown a promise in data-driven\nconversation modeling. However, most VAE conversation models match the\napproximate posterior distribution over the latent variables to a simple prior\nsuch as standard normal distribution, thereby restricting the generated\nresponses to a relatively simple (e.g., unimodal) scope. In this paper, we\npropose DialogWAE, a conditional Wasserstein autoencoder~(WAE) specially\ndesigned for dialogue modeling. Unlike VAEs that impose a simple distribution\nover the latent variables, DialogWAE models the distribution of data by\ntraining a GAN within the latent variable space. Specifically, our model\nsamples from the prior and posterior distributions over the latent variables by\ntransforming context-dependent random noise using neural networks and minimizes\nthe Wasserstein distance between the two distributions. We further develop a\nGaussian mixture prior network to enrich the latent space. Experiments on two\npopular datasets show that DialogWAE outperforms the state-of-the-art\napproaches in generating more coherent, informative and diverse responses.", "published": "2018-05-31 07:25:04", "link": "http://arxiv.org/abs/1805.12352v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "KG^2: Learning to Reason Science Exam Questions with Contextual\n  Knowledge Graph Embeddings", "abstract": "The AI2 Reasoning Challenge (ARC), a new benchmark dataset for question\nanswering (QA) has been recently released. ARC only contains natural science\nquestions authored for human exams, which are hard to answer and require\nadvanced logic reasoning. On the ARC Challenge Set, existing state-of-the-art\nQA systems fail to significantly outperform random baseline, reflecting the\ndifficult nature of this task. In this paper, we propose a novel framework for\nanswering science exam questions, which mimics human solving process in an\nopen-book exam. To address the reasoning challenge, we construct contextual\nknowledge graphs respectively for the question itself and supporting sentences.\nOur model learns to reason with neural embeddings of both knowledge graphs.\nExperiments on the ARC Challenge Set show that our model outperforms the\nprevious state-of-the-art QA systems.", "published": "2018-05-31 09:39:14", "link": "http://arxiv.org/abs/1805.12393v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for\n  Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on\ndiscrete data. Based on this framework, we derive a perturbation-based method,\nGreedy Attack, and a scalable learning-based method, Gumbel Attack, that\nillustrate various tradeoffs in the design of attacks. We demonstrate the\neffectiveness of these methods using both quantitative metrics and human\nevaluation on various state-of-the-art models for text classification,\nincluding a word-based CNN, a character-based CNN and an LSTM. As as example of\nour results, we show that the accuracy of character-based convolutional\nnetworks drops to the level of random selection by modifying only five\ncharacters through Greedy Attack.", "published": "2018-05-31 04:40:32", "link": "http://arxiv.org/abs/1805.12316v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Following High-level Navigation Instructions on a Simulated Quadcopter\n  with Imitation Learning", "abstract": "We introduce a method for following high-level navigation instructions by\nmapping directly from images, instructions and pose estimates to continuous\nlow-level velocity commands for real-time control. The Grounded Semantic\nMapping Network (GSMN) is a fully-differentiable neural network architecture\nthat builds an explicit semantic map in the world reference frame by\nincorporating a pinhole camera projection model within the network. The\ninformation stored in the map is learned from experience, while the\nlocal-to-world transformation is computed explicitly. We train the model using\nDAggerFM, a modified variant of DAgger that trades tabular convergence\nguarantees for improved training speed and memory use. We test GSMN in virtual\nenvironments on a realistic quadcopter simulator and show that incorporating an\nexplicit mapping and grounding modules allows GSMN to outperform strong neural\nbaselines and almost reach an expert policy performance. Finally, we analyze\nthe learned map representations and show that using an explicit map leads to an\ninterpretable instruction-following model.", "published": "2018-05-31 18:42:26", "link": "http://arxiv.org/abs/1806.00047v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
