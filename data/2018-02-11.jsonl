{"title": "Syntax and Semantics of Italian Poetry in the First Half of the 20th\n  Century", "abstract": "In this paper we study, analyse and comment rhetorical figures present in\nsome of most interesting poetry of the first half of the twentieth century.\nThese figures are at first traced back to some famous poet of the past and then\ncompared to classical Latin prose. Linguistic theory is then called in to show\nhow they can be represented in syntactic structures and classified as\nnoncanonical structures, by positioning discontinuous or displaced linguistic\nelements in Spec XP projections at various levels of constituency. Then we\nintroduce LFG (Lexical Functional Grammar) as the theory that allows us to\nconnect syntactic noncanonical structures with informational structure and\npsycholinguistic theories for complexity evaluation. We end up with two\ncomputational linguistics experiments and then evaluate the results. The first\none uses best online parsers of Italian to parse poetic structures; the second\none uses Getarun, the system created at Ca Foscari Computational Linguistics\nLaboratory. As will be shown, the first approach is unable to cope with these\nstructures due to the use of only statistical probabilistic information. On the\ncontrary, the second one, being a symbolic rule based system, is by far\nsuperior and allows also to complete both semantic an pragmatic analysis.", "published": "2018-02-11 08:54:58", "link": "http://arxiv.org/abs/1802.03712v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Recurrent Neural State Using Memory Signatures", "abstract": "We demonstrate a network visualization technique to analyze the recurrent\nstate inside the LSTMs/GRUs used commonly in language and acoustic models.\nInterpreting intermediate state and network activations inside end-to-end\nmodels remains an open challenge. Our method allows users to understand exactly\nhow much and what history is encoded inside recurrent state in grapheme\nsequence models. Our procedure trains multiple decoders that predict prior\ninput history. Compiling results from these decoders, a user can obtain a\nsignature of the recurrent kernel that characterizes its memory behavior. We\ndemonstrate this method's usefulness in revealing information divergence in the\nbases of recurrent factorized kernels, visualizing the character-level\ndifferences between the memory of n-gram and recurrent language models, and\nextracting knowledge of history encoded in the layers of grapheme-based\nend-to-end ASR networks.", "published": "2018-02-11 20:59:56", "link": "http://arxiv.org/abs/1802.03816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Formal Ontology Learning from English IS-A Sentences", "abstract": "Ontology learning (OL) is the process of automatically generating an\nontological knowledge base from a plain text document. In this paper, we\npropose a new ontology learning approach and tool, called DLOL, which generates\na knowledge base in the description logic (DL) SHOQ(D) from a collection of\nfactual non-negative IS-A sentences in English. We provide extensive\nexperimental results on the accuracy of DLOL, giving experimental comparisons\nto three state-of-the-art existing OL tools, namely Text2Onto, FRED, and LExO.\nHere, we use the standard OL accuracy measure, called lexical accuracy, and a\nnovel OL accuracy measure, called instance-based inference model. In our\nexperimental results, DLOL turns out to be about 21% and 46%, respectively,\nbetter than the best of the other three approaches.", "published": "2018-02-11 06:41:54", "link": "http://arxiv.org/abs/1802.03701v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate\n  Ranking", "abstract": "The first step of many research projects is to define and rank a short list\nof candidates for study. In the modern rapidity of scientific progress, some\nturn to automated hypothesis generation (HG) systems to aid this process. These\nsystems can identify implicit or overlooked connections within a large\nscientific corpus, and while their importance grows alongside the pace of\nscience, they lack thorough validation. Without any standard numerical\nevaluation method, many validate general-purpose HG systems by rediscovering a\nhandful of historical findings, and some wishing to be more thorough may run\nlaboratory experiments based on automatic suggestions. These methods are\nexpensive, time consuming, and cannot scale. Thus, we present a numerical\nevaluation framework for the purpose of validating HG systems that leverages\nthousands of validation hypotheses. This method evaluates a HG system by its\nability to rank hypotheses by plausibility; a process reminiscent of human\ncandidate selection. Because HG systems do not produce a ranking criteria,\nspecifically those that produce topic models, we additionally present novel\nmetrics to quantify the plausibility of hypotheses given topic model system\noutput. Finally, we demonstrate that our proposed validation method aligns with\nreal-world research goals by deploying our method within Moliere, our recent\ntopic-driven HG system, in order to automatically generate a set of candidate\ngenes related to HIV-associated neurodegenerative disease (HAND). By performing\nlaboratory experiments based on this candidate set, we discover a new\nconnection between HAND and Dead Box RNA Helicase 3 (DDX3). Reproducibility:\ncode, validation data, and results can be found at\nsybrandt.com/2018/validation.", "published": "2018-02-11 19:04:49", "link": "http://arxiv.org/abs/1802.03793v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Distributed Readability Analysis Of Turkish Elementary School Textbooks", "abstract": "The readability assessment deals with estimating the level of difficulty in\nreading texts.Many readability tests, which do not indicate execution\nefficiency, have been applied on specific texts to measure the reading grade\nlevel in science textbooks. In this paper, we analyze the content covered in\nelementary school Turkish textbooks by employing a distributed parallel\nprocessing framework based on popular MapReduce paradigm. We outline the\narchitecture of a distributed Big Data processing system which uses Hadoop for\nfull-text readability analysis. The readability scores of the textbooks and\nsystem performance measurements are also given in the paper.", "published": "2018-02-11 21:18:45", "link": "http://arxiv.org/abs/1802.03821v1", "categories": ["cs.DC", "cs.CL"], "primary_category": "cs.DC"}
{"title": "Sample Efficient Deep Reinforcement Learning for Dialogue Systems with\n  Large Action Spaces", "abstract": "In spoken dialogue systems, we aim to deploy artificial intelligence to build\nautomated dialogue agents that can converse with humans. A part of this effort\nis the policy optimisation task, which attempts to find a policy describing how\nto respond to humans, in the form of a function taking the current state of the\ndialogue and returning the response of the system. In this paper, we\ninvestigate deep reinforcement learning approaches to solve this problem.\nParticular attention is given to actor-critic methods, off-policy reinforcement\nlearning with experience replay, and various methods aimed at reducing the bias\nand variance of estimators. When combined, these methods result in the\npreviously proposed ACER algorithm that gave competitive results in gaming\nenvironments. These environments however are fully observable and have a\nrelatively small action set so in this paper we examine the application of ACER\nto dialogue policy optimisation. We show that this method beats the current\nstate-of-the-art in deep learning approaches for spoken dialogue systems. This\nnot only leads to a more sample efficient algorithm that can train faster, but\nalso allows us to apply the algorithm in more difficult environments than\nbefore. We thus experiment with learning in a very large action space, which\nhas two orders of magnitude more actions than previously considered. We find\nthat ACER trains significantly faster than the current state-of-the-art.", "published": "2018-02-11 15:37:37", "link": "http://arxiv.org/abs/1802.03753v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
