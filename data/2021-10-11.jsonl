{"title": "Pre-trained Language Models in Biomedical Domain: A Systematic Survey", "abstract": "Pre-trained language models (PLMs) have been the de facto paradigm for most\nnatural language processing (NLP) tasks. This also benefits biomedical domain:\nresearchers from informatics, medicine, and computer science (CS) communities\npropose various PLMs trained on biomedical datasets, e.g., biomedical text,\nelectronic health records, protein, and DNA sequences for various biomedical\ntasks. However, the cross-discipline characteristics of biomedical PLMs hinder\ntheir spreading among communities; some existing works are isolated from each\nother without comprehensive comparison and discussions. It expects a survey\nthat not only systematically reviews recent advances of biomedical PLMs and\ntheir applications but also standardizes terminology and benchmarks. In this\npaper, we summarize the recent progress of pre-trained language models in the\nbiomedical domain and their applications in biomedical downstream tasks.\nParticularly, we discuss the motivations and propose a taxonomy of existing\nbiomedical PLMs. Their applications in biomedical downstream tasks are\nexhaustively discussed. At last, we illustrate various limitations and future\ntrends, which we hope can provide inspiration for the future research of the\nresearch community.", "published": "2021-10-11 05:30:30", "link": "http://arxiv.org/abs/2110.05006v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Text Simplification: Dataset, Criteria and Baseline", "abstract": "Text simplification is a valuable technique. However, current research is\nlimited to sentence simplification. In this paper, we define and investigate a\nnew task of document-level text simplification, which aims to simplify a\ndocument consisting of multiple sentences. Based on Wikipedia dumps, we first\nconstruct a large-scale dataset named D-Wikipedia and perform analysis and\nhuman evaluation on it to show that the dataset is reliable. Then, we propose a\nnew automatic evaluation metric called D-SARI that is more suitable for the\ndocument-level simplification task. Finally, we select several representative\nmodels as baseline models for this task and perform automatic evaluation and\nhuman evaluation. We analyze the results and point out the shortcomings of the\nbaseline models.", "published": "2021-10-11 08:15:31", "link": "http://arxiv.org/abs/2110.05071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Forecasting of Conversation Derailment", "abstract": "Online conversations can sometimes take a turn for the worse, either due to\nsystematic cultural differences, accidental misunderstandings, or mere malice.\nAutomatically forecasting derailment in public online conversations provides an\nopportunity to take early action to moderate it. Previous work in this space is\nlimited, and we extend it in several ways. We apply a pretrained language\nencoder to the task, which outperforms earlier approaches. We further\nexperiment with shifting the training paradigm for the task from a static to a\ndynamic one to increase the forecast horizon. This approach shows mixed\nresults: in a high-quality data setting, a longer average forecast horizon can\nbe achieved at the cost of a small drop in F1; in a low-quality data setting,\nhowever, dynamic training propagates the noise and is highly detrimental to\nperformance.", "published": "2021-10-11 09:33:34", "link": "http://arxiv.org/abs/2110.05111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WeTS: A Benchmark for Translation Suggestion", "abstract": "Translation Suggestion (TS), which provides alternatives for specific words\nor phrases given the entire documents translated by machine translation (MT)\n\\cite{lee2021intellicat}, has been proven to play a significant role in post\nediting (PE). However, there is still no publicly available data set to support\nin-depth research for this problem, and no reproducible experimental results\ncan be followed by researchers in this community. To break this limitation, we\ncreate a benchmark data set for TS, called \\emph{WeTS}, which contains golden\ncorpus annotated by expert translators on four translation directions. Apart\nfrom the human-annotated golden corpus, we also propose several novel methods\nto generate synthetic corpus which can substantially improve the performance of\nTS. With the corpus we construct, we introduce the Transformer-based model for\nTS, and experimental results show that our model achieves State-Of-The-Art\n(SOTA) results on all four translation directions, including English-to-German,\nGerman-to-English, Chinese-to-English and English-to-Chinese. Codes and corpus\ncan be found at https://github.com/ZhenYangIACAS/WeTS.git.", "published": "2021-10-11 10:52:17", "link": "http://arxiv.org/abs/2110.05151v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of\n  Graphemes and Syllables", "abstract": "Wav2vec 2.0 is an end-to-end framework of self-supervised learning for speech\nrepresentation that is successful in automatic speech recognition (ASR), but\nmost of the work on the topic has been developed with a single language:\nEnglish. Therefore, it is unclear whether the self-supervised framework is\neffective in recognizing other languages with different writing systems, such\nas Korean which uses the Hangul having a unique writing system. In this paper,\nwe present K-Wav2Vec 2.0, which is a modified version of Wav2vec 2.0 designed\nfor Korean automatic speech recognition by exploring and optimizing various\nfactors of the original Wav2vec 2.0. In fine-tuning, we propose a multi-task\nhierarchical architecture to reflect the Korean writing structure. Moreover, a\njoint decoder is applied to alleviate the problem of words existing outside of\nthe vocabulary. In pre-training, we attempted the cross-lingual transfer of the\npre-trained model by further pre-training the English Wav2vec 2.0 on a Korean\ndataset, considering limited resources. Our experimental results demonstrate\nthat the proposed method yields the best performance on both Korean ASR\ndatasets: Ksponspeech (a large-scale Korean speech corpus) and Clovacall (a\ncall-based dialog corpus). Further pre-training is also effective in language\nadaptation, leading to large improvements without additional data.", "published": "2021-10-11 11:53:12", "link": "http://arxiv.org/abs/2110.05172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On a Benefit of Mask Language Modeling: Robustness to Simplicity Bias", "abstract": "Despite the success of pretrained masked language models (MLM), why MLM\npretraining is useful is still a qeustion not fully answered. In this work we\ntheoretically and empirically show that MLM pretraining makes models robust to\nlexicon-level spurious features, partly answer the question. We theoretically\nshow that, when we can model the distribution of a spurious feature $\\Pi$\nconditioned on the context, then (1) $\\Pi$ is at least as informative as the\nspurious feature, and (2) learning from $\\Pi$ is at least as simple as learning\nfrom the spurious feature. Therefore, MLM pretraining rescues the model from\nthe simplicity bias caused by the spurious feature. We also explore the\nefficacy of MLM pretraing in causal settings. Finally we close the gap between\nour theories and the real world practices by conducting experiments on the hate\nspeech detection and the name entity recognition tasks.", "published": "2021-10-11 14:18:29", "link": "http://arxiv.org/abs/2110.05301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating User Perception of Speech Recognition System Quality with\n  Semantic Distance Metric", "abstract": "Measuring automatic speech recognition (ASR) system quality is critical for\ncreating user-satisfying voice-driven applications. Word Error Rate (WER) has\nbeen traditionally used to evaluate ASR system quality; however, it sometimes\ncorrelates poorly with user perception/judgement of transcription quality. This\nis because WER weighs every word equally and does not consider semantic\ncorrectness which has a higher impact on user perception. In this work, we\npropose evaluating ASR output hypotheses quality with SemDist that can measure\nsemantic correctness by using the distance between the semantic vectors of the\nreference and hypothesis extracted from a pre-trained language model. Our\nexperimental results of 71K and 36K user annotated ASR output quality show that\nSemDist achieves higher correlation with user perception than WER. We also show\nthat SemDist has higher correlation with downstream Natural Language\nUnderstanding (NLU) tasks than WER.", "published": "2021-10-11 16:09:01", "link": "http://arxiv.org/abs/2110.05376v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bottom-Up Constituency Parsing and Nested Named Entity Recognition with\n  Pointer Networks", "abstract": "Constituency parsing and nested named entity recognition (NER) are similar\ntasks since they both aim to predict a collection of nested and non-crossing\nspans. In this work, we cast nested NER to constituency parsing and propose a\nnovel pointing mechanism for bottom-up parsing to tackle both tasks. The key\nidea is based on the observation that if we traverse a constituency tree in\npost-order, i.e., visiting a parent after its children, then two consecutively\nvisited spans would share a boundary. Our model tracks the shared boundaries\nand predicts the next boundary at each step by leveraging a pointer network. As\na result, it needs only linear steps to parse and thus is efficient. It also\nmaintains a parsing configuration for structural consistency, i.e., always\noutputting valid trees. Experimentally, our model achieves the state-of-the-art\nperformance on PTB among all BERT-based models (96.01 F1 score) and competitive\nperformance on CTB7 in constituency parsing; and it also achieves strong\nperformance on three benchmark datasets of nested NER: ACE2004, ACE2005, and\nGENIA. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/pointer-net-for-nested}.", "published": "2021-10-11 17:01:43", "link": "http://arxiv.org/abs/2110.05419v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Document Similarity Methods to create Parallel Datasets for Code\n  Translation", "abstract": "Translating source code from one programming language to another is a\ncritical, time-consuming task in modernizing legacy applications and codebases.\nRecent work in this space has drawn inspiration from the software naturalness\nhypothesis by applying natural language processing techniques towards\nautomating the code translation task. However, due to the paucity of parallel\ndata in this domain, supervised techniques have only been applied to a limited\nset of popular programming languages. To bypass this limitation, unsupervised\nneural machine translation techniques have been proposed to learn code\ntranslation using only monolingual corpora. In this work, we propose to use\ndocument similarity methods to create noisy parallel datasets of code, thus\nenabling supervised techniques to be applied for automated code translation\nwithout having to rely on the availability or expensive curation of parallel\ncode datasets. We explore the noise tolerance of models trained on such\nautomatically-created datasets and show that these models perform comparably to\nmodels trained on ground truth for reasonable levels of noise. Finally, we\nexhibit the practical utility of the proposed method by creating parallel\ndatasets for languages beyond the ones explored in prior work, thus expanding\nthe set of programming languages for automated code translation.", "published": "2021-10-11 17:07:58", "link": "http://arxiv.org/abs/2110.05423v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Review on Part-of-Speech Technologies", "abstract": "Developing an automatic part-of-speech (POS) tagging for any new language is\nconsidered a necessary step for further computational linguistics methodology\nbeyond tagging, like chunking and parsing, to be fully applied to the language.\nMany POS disambiguation technologies have been developed for this type of\nresearch and there are factors that influence the choice of choosing one. This\ncould be either corpus-based or non-corpus-based. In this paper, we present a\nreview of POS tagging technologies.", "published": "2021-10-11 03:21:33", "link": "http://arxiv.org/abs/2110.04977v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross Domain Emotion Recognition using Few Shot Knowledge Transfer", "abstract": "Emotion recognition from text is a challenging task due to diverse emotion\ntaxonomies, lack of reliable labeled data in different domains, and highly\nsubjective annotation standards. Few-shot and zero-shot techniques can\ngeneralize across unseen emotions by projecting the documents and emotion\nlabels onto a shared embedding space. In this work, we explore the task of\nfew-shot emotion recognition by transferring the knowledge gained from\nsupervision on the GoEmotions Reddit dataset to the SemEval tweets corpus,\nusing different emotion representation methods. The results show that knowledge\ntransfer using external knowledge bases and fine-tuned encoders perform\ncomparably as supervised baselines, requiring minimal supervision from the task\ndataset.", "published": "2021-10-11 06:22:18", "link": "http://arxiv.org/abs/2110.05021v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Comparison of Word Embeddings in Event & Entity\n  Coreference Resolution", "abstract": "Coreference Resolution is an important NLP task and most state-of-the-art\nmethods rely on word embeddings for word representation. However, one issue\nthat has been largely overlooked in literature is that of comparing the\nperformance of different embeddings across and within families in this task.\nTherefore, we frame our study in the context of Event and Entity Coreference\nResolution (EvCR & EnCR), and address two questions : 1) Is there a trade-off\nbetween performance (predictive & run-time) and embedding size? 2) How do the\nembeddings' performance compare within and across families? Our experiments\nreveal several interesting findings. First, we observe diminishing returns in\nperformance with respect to embedding size. E.g. a model using solely a\ncharacter embedding achieves 86% of the performance of the largest model (Elmo,\nGloVe, Character) while being 1.2% of its size. Second, the larger model using\nmultiple embeddings learns faster overall despite being slower per epoch.\nHowever, it is still slower at test time. Finally, Elmo performs best on both\nEvCR and EnCR, while GloVe and FastText perform best in EvCR and EnCR\nrespectively.", "published": "2021-10-11 09:46:46", "link": "http://arxiv.org/abs/2110.05115v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Offensive Language Detection with BERT-based models, By Customizing\n  Attention Probabilities", "abstract": "This paper describes a novel study on using `Attention Mask' input in\ntransformers and using this approach for detecting offensive content in both\nEnglish and Persian languages. The paper's principal focus is to suggest a\nmethodology to enhance the performance of the BERT-based models on the\n`Offensive Language Detection' task. Therefore, we customize attention\nprobabilities by changing the `Attention Mask' input to create more efficacious\nword embeddings. To do this, we firstly tokenize the training set of the\nexploited datasets (by BERT tokenizer). Then, we apply Multinomial Naive Bayes\nto map these tokens to two probabilities. These probabilities indicate the\nlikelihood of making a text non-offensive or offensive, provided that it\ncontains that token. Afterwards, we use these probabilities to define a new\nterm, namely Offensive Score. Next, we create two separate (because of the\ndifferences in the types of the employed datasets) equations based on Offensive\nScores for each language to re-distribute the `Attention Mask' input for paying\nmore attention to more offensive phrases. Eventually, we put the F1-macro score\nas our evaluation metric and fine-tune several combinations of BERT with ANNs,\nCNNs and RNNs to examine the effect of using this methodology on various\ncombinations. The results indicate that all models will enhance with this\nmethodology. The most improvement was 2% and 10% for English and Persian\nlanguages, respectively.", "published": "2021-10-11 10:23:44", "link": "http://arxiv.org/abs/2110.05133v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "It is Not as Good as You Think! Evaluating Simultaneous Machine\n  Translation on Interpretation Data", "abstract": "Most existing simultaneous machine translation (SiMT) systems are trained and\nevaluated on offline translation corpora. We argue that SiMT systems should be\ntrained and tested on real interpretation data. To illustrate this argument, we\npropose an interpretation test set and conduct a realistic evaluation of SiMT\ntrained on offline translations. Our results, on our test set along with 3\nexisting smaller scale language pairs, highlight the difference of up-to 13.83\nBLEU score when SiMT models are evaluated on translation vs interpretation\ndata. In the absence of interpretation training data, we propose a\ntranslation-to-interpretation (T2I) style transfer method which allows\nconverting existing offline translations into interpretation-style data,\nleading to up-to 2.8 BLEU improvement. However, the evaluation gap remains\nnotable, calling for constructing large-scale interpretation corpora better\nsuited for evaluating and developing SiMT systems.", "published": "2021-10-11 12:27:07", "link": "http://arxiv.org/abs/2110.05213v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Situated Multi-Domain End-to-End Dialogue\n  Systems", "abstract": "Task-oriented dialogue systems have been a promising area in the NLP field.\nPrevious work showed the effectiveness of using a single GPT-2 based model to\npredict belief states and responses via causal language modeling. In this\npaper, we leverage multi-task learning techniques to train a GPT-2 based model\non a more challenging dataset with multiple domains, multiple modalities, and\nmore diversity in output formats.\n  Using only a single model, our method achieves better performance on all\nsub-tasks, across domains, compared to task and domain-specific models.\nFurthermore, we evaluated several proposed strategies for GPT-2 based dialogue\nsystems with comprehensive ablation studies, showing that all techniques can\nfurther improve the performance.", "published": "2021-10-11 12:36:30", "link": "http://arxiv.org/abs/2110.05221v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TEET! Tunisian Dataset for Toxic Speech Detection", "abstract": "The complete freedom of expression in social media has its costs especially\nin spreading harmful and abusive content that may induce people to act\naccordingly. Therefore, the need of detecting automatically such a content\nbecomes an urgent task that will help and enhance the efficiency in limiting\nthis toxic spread. Compared to other Arabic dialects which are mostly based on\nMSA, the Tunisian dialect is a combination of many other languages like MSA,\nTamazight, Italian and French. Because of its rich language, dealing with NLP\nproblems can be challenging due to the lack of large annotated datasets. In\nthis paper we are introducing a new annotated dataset composed of approximately\n10k of comments. We provide an in-depth exploration of its vocabulary through\nfeature engineering approaches as well as the results of the classification\nperformance of machine learning classifiers like NB and SVM and deep learning\nmodels such as ARBERT, MARBERT and XLM-R.", "published": "2021-10-11 14:00:08", "link": "http://arxiv.org/abs/2110.05287v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Focus on what matters: Applying Discourse Coherence Theory to Cross\n  Document Coreference", "abstract": "Performing event and entity coreference resolution across documents vastly\nincreases the number of candidate mentions, making it intractable to do the\nfull $n^2$ pairwise comparisons. Existing approaches simplify by considering\ncoreference only within document clusters, but this fails to handle\ninter-cluster coreference, common in many applications. As a result\ncross-document coreference algorithms are rarely applied to downstream tasks.\nWe draw on an insight from discourse coherence theory: potential coreferences\nare constrained by the reader's discourse focus. We model the entities/events\nin a reader's focus as a neighborhood within a learned latent embedding space\nwhich minimizes the distance between mentions and the centroids of their gold\ncoreference clusters. We then use these neighborhoods to sample only hard\nnegatives to train a fine-grained classifier on mention pairs and their local\ndiscourse features. Our approach achieves state-of-the-art results for both\nevents and entities on the ECB+, Gun Violence, Football Coreference, and\nCross-Domain Cross-Document Coreference corpora. Furthermore, training on\nmultiple corpora improves average performance across all datasets by 17.2 F1\npoints, leading to a robust coreference resolution model for use in downstream\ntasks where link distribution is unknown.", "published": "2021-10-11 15:41:47", "link": "http://arxiv.org/abs/2110.05362v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explainable Fact-checking through Question Answering", "abstract": "Misleading or false information has been creating chaos in some places around\nthe world. To mitigate this issue, many researchers have proposed automated\nfact-checking methods to fight the spread of fake news. However, most methods\ncannot explain the reasoning behind their decisions, failing to build trust\nbetween machines and humans using such technology. Trust is essential for\nfact-checking to be applied in the real world. Here, we address fact-checking\nexplainability through question answering. In particular, we propose generating\nquestions and answers from claims and answering the same questions from\nevidence. We also propose an answer comparison model with an attention\nmechanism attached to each question. Leveraging question answering as a proxy,\nwe break down automated fact-checking into several steps -- this separation\naids models' explainability as it allows for more detailed analysis of their\ndecision-making processes. Experimental results show that the proposed model\ncan achieve state-of-the-art performance while providing reasonable explainable\ncapabilities.", "published": "2021-10-11 15:55:11", "link": "http://arxiv.org/abs/2110.05369v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Neural Machine Translation with Generative Language Models\n  Only", "abstract": "We show how to derive state-of-the-art unsupervised neural machine\ntranslation systems from generatively pre-trained language models. Our method\nconsists of three steps: few-shot amplification, distillation, and\nbacktranslation. We first use the zero-shot translation ability of large\npre-trained language models to generate translations for a small set of\nunlabeled sentences. We then amplify these zero-shot translations by using them\nas few-shot demonstrations for sampling a larger synthetic dataset. This\ndataset is distilled by discarding the few-shot demonstrations and then\nfine-tuning. During backtranslation, we repeatedly generate translations for a\nset of inputs and then fine-tune a single language model on both directions of\nthe translation task at once, ensuring cycle-consistency by swapping the roles\nof gold monotext and generated translations when fine-tuning. By using our\nmethod to leverage GPT-3's zero-shot translation capability, we achieve a new\nstate-of-the-art in unsupervised translation on the WMT14 English-French\nbenchmark, attaining a BLEU score of 42.1.", "published": "2021-10-11 17:35:34", "link": "http://arxiv.org/abs/2110.05448v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rome was built in 1776: A Case Study on Factual Correctness in\n  Knowledge-Grounded Response Generation", "abstract": "Recently neural response generation models have leveraged large pre-trained\ntransformer models and knowledge snippets to generate relevant and informative\nresponses. However, this does not guarantee that generated responses are\nfactually correct. In this paper, we examine factual correctness in\nknowledge-grounded neural response generation models. We present a human\nannotation setup to identify three different response types: responses that are\nfactually consistent with respect to the input knowledge, responses that\ncontain hallucinated knowledge, and non-verifiable chitchat style responses. We\nuse this setup to annotate responses generated using different stateof-the-art\nmodels, knowledge snippets, and decoding strategies. In addition, to facilitate\nthe development of a factual consistency detector, we automatically create a\nnew corpus called Conv-FEVER that is adapted from the Wizard of Wikipedia\ndataset and includes factually consistent and inconsistent responses. We\ndemonstrate the benefit of our Conv-FEVER dataset by showing that the models\ntrained on this data perform reasonably well to detect factually inconsistent\nresponses with respect to the provided knowledge through evaluation on our\nhuman annotated data. We will release the Conv-FEVER dataset and the human\nannotated responses.", "published": "2021-10-11 17:48:11", "link": "http://arxiv.org/abs/2110.05456v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition", "abstract": "The Transformer architecture has been well adopted as a dominant architecture\nin most sequence transduction tasks including automatic speech recognition\n(ASR), since its attention mechanism excels in capturing long-range\ndependencies. While models built solely upon attention can be better\nparallelized than regular RNN, a novel network architecture, SRU++, was\nrecently proposed. By combining the fast recurrence and attention mechanism,\nSRU++ exhibits strong capability in sequence modeling and achieves\nnear-state-of-the-art results in various language modeling and machine\ntranslation tasks with improved compute efficiency. In this work, we present\nthe advantages of applying SRU++ in ASR tasks by comparing with Conformer\nacross multiple ASR benchmarks and study how the benefits can be generalized to\nlong-form speech inputs. On the popular LibriSpeech benchmark, our SRU++ model\nachieves 2.0% / 4.7% WER on test-clean / test-other, showing competitive\nperformances compared with the state-of-the-art Conformer encoder under the\nsame set-up. Specifically, SRU++ can surpass Conformer on long-form speech\ninput with a large margin, based on our analysis.", "published": "2021-10-11 19:23:50", "link": "http://arxiv.org/abs/2110.05571v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Generalizing to New Domains by Mapping Natural Language to Lifted LTL", "abstract": "Recent work on using natural language to specify commands to robots has\ngrounded that language to LTL. However, mapping natural language task\nspecifications to LTL task specifications using language models require\nprobability distributions over finite vocabulary. Existing state-of-the-art\nmethods have extended this finite vocabulary to include unseen terms from the\ninput sequence to improve output generalization. However, novel\nout-of-vocabulary atomic propositions cannot be generated using these methods.\nTo overcome this, we introduce an intermediate contextual query representation\nwhich can be learned from single positive task specification examples,\nassociating a contextual query with an LTL template. We demonstrate that this\nintermediate representation allows for generalization over unseen object\nreferences, assuming accurate groundings are available. We compare our method\nof mapping natural language task specifications to intermediate contextual\nqueries against state-of-the-art CopyNet models capable of translating natural\nlanguage to LTL, by evaluating whether correct LTL for manipulation and\nnavigation task specifications can be output, and show that our method\noutperforms the CopyNet model on unseen object references. We demonstrate that\nthe grounded LTL our method outputs can be used for planning in a simulated\nOO-MDP environment. Finally, we discuss some common failure modes encountered\nwhen translating natural language task specifications to grounded LTL.", "published": "2021-10-11 20:49:26", "link": "http://arxiv.org/abs/2110.05603v2", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "TCube: Domain-Agnostic Neural Time-series Narration", "abstract": "The task of generating rich and fluent narratives that aptly describe the\ncharacteristics, trends, and anomalies of time-series data is invaluable to the\nsciences (geology, meteorology, epidemiology) or finance (trades, stocks, or\nsales and inventory). The efforts for time-series narration hitherto are\ndomain-specific and use predefined templates that offer consistency but lead to\nmechanical narratives. We present TCube (Time-series-to-text), a\ndomain-agnostic neural framework for time-series narration, that couples the\nrepresentation of essential time-series elements in the form of a dense\nknowledge graph and the translation of said knowledge graph into rich and\nfluent narratives through the transfer-learning capabilities of PLMs\n(Pre-trained Language Models). TCube's design primarily addresses the challenge\nthat lies in building a neural framework in the complete paucity of annotated\ntraining data for time-series. The design incorporates knowledge graphs as an\nintermediary for the representation of essential time-series elements which can\nbe linearized for textual translation. To the best of our knowledge, TCube is\nthe first investigation of the use of neural strategies for time-series\nnarration. Through extensive evaluations, we show that TCube can improve the\nlexical diversity of the generated narratives by up to 65.38% while still\nmaintaining grammatical integrity. The practicality and deployability of TCube\nis further validated through an expert review (n=21) where 76.2% of\nparticipating experts wary of auto-generated narratives favored TCube as a\ndeployable system for time-series narration due to its richer narratives. Our\ncode-base, models, and datasets, with detailed instructions for reproducibility\nis publicly hosted at https://github.com/Mandar-Sharma/TCube.", "published": "2021-10-11 22:12:46", "link": "http://arxiv.org/abs/2110.05633v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Calling to CNN-LSTM for Rumor Detection: A Deep Multi-channel Model for\n  Message Veracity Classification in Microblogs", "abstract": "Reputed by their low-cost, easy-access, real-time and valuable information,\nsocial media also wildly spread unverified or fake news. Rumors can notably\ncause severe damage on individuals and the society. Therefore, rumor detection\non social media has recently attracted tremendous attention. Most rumor\ndetection approaches focus on rumor feature analysis and social features, i.e.,\nmetadata in social media. Unfortunately, these features are data-specific and\nmay not always be available, e.g., when the rumor has just popped up and not\nyet propagated. In contrast, post contents (including images or videos) play an\nimportant role and can indicate the diffusion purpose of a rumor. Furthermore,\nrumor classification is also closely related to opinion mining and sentiment\nanalysis. Yet, to the best of our knowledge, exploiting images and sentiments\nis little investigated.Considering the available multimodal features from\nmicroblogs, notably, we propose in this paper an end-to-end model called\ndeepMONITOR that is based on deep neural networks and allows quite accurate\nautomated rumor verification, by utilizing all three characteristics: post\ntextual and image contents, as well as sentiment. deepMONITOR concatenates\nimage features with the joint text and sentiment features to produce a\nreliable, fused classification. We conduct extensive experiments on two\nlarge-scale, real-world datasets. The results show that deepMONITOR achieves a\nhigher accuracy than state-of-the-art methods.", "published": "2021-10-11 07:42:41", "link": "http://arxiv.org/abs/2110.15727v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topic Modeling, Clade-assisted Sentiment Analysis, and Vaccine Brand\n  Reputation Analysis of COVID-19 Vaccine-related Facebook Comments in the\n  Philippines", "abstract": "Vaccine hesitancy and other COVID-19-related concerns and complaints in the\nPhilippines are evident on social media. It is important to identify these\ndifferent topics and sentiments in order to gauge public opinion, use the\ninsights to develop policies, and make necessary adjustments or actions to\nimprove public image and reputation of the administering agency and the\nCOVID-19 vaccines themselves. This paper proposes a semi-supervised machine\nlearning pipeline to perform topic modeling, sentiment analysis, and an\nanalysis of vaccine brand reputation to obtain an in-depth understanding of\nnational public opinion of Filipinos on Facebook. The methodology makes use of\na multilingual version of Bidirectional Encoder Representations from\nTransformers or BERT for topic modeling, hierarchical clustering, five\ndifferent classifiers for sentiment analysis, and cosine similarity of BERT\ntopic embeddings for vaccine brand reputation analysis. Results suggest that\nany type of COVID-19 misinformation is an emergent property of COVID-19 public\nopinion, and that the detection of COVID-19 misinformation can be an\nunsupervised task. Sentiment analysis aided by hierarchical clustering reveal\nthat 21 of the 25 topics extrapolated by topic modeling are negative topics.\nSuch negative comments spike in count whenever the Department of Health in the\nPhilippines posts about the COVID-19 situation in other countries.\nAdditionally, the high numbers of laugh reactions on the Facebook posts by the\nsame agency -- without any humorous content -- suggest that the reactors of\nthese posts tend to react the way they do, not because of what the posts are\nabout but because of who posted them.", "published": "2021-10-11 11:08:38", "link": "http://arxiv.org/abs/2111.04416v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs\n  for Robust Speech Recognition", "abstract": "The goal of self-supervised learning (SSL) for automatic speech recognition\n(ASR) is to learn good speech representations from a large amount of unlabeled\nspeech for the downstream ASR task. However, most SSL frameworks do not\nconsider noise robustness which is crucial for real-world applications. In this\npaper we propose wav2vec-Switch, a method to encode noise robustness into\ncontextualized representations of speech via contrastive learning.\nSpecifically, we feed original-noisy speech pairs simultaneously into the\nwav2vec 2.0 network. In addition to the existing contrastive learning task, we\nswitch the quantized representations of the original and noisy speech as\nadditional prediction targets of each other. By doing this, it enforces the\nnetwork to have consistent predictions for the original and noisy speech, thus\nallows to learn contextualized representation with noise robustness. Our\nexperiments on synthesized and real noisy data show the effectiveness of our\nmethod: it achieves 2.9--4.9% relative word error rate (WER) reduction on the\nsynthesized noisy LibriSpeech data without deterioration on the original data,\nand 5.7% on CHiME-4 real 1-channel noisy data compared to a data augmentation\nbaseline even with a strong language model for decoding. Our results on CHiME-4\ncan match or even surpass those with well-designed speech enhancement\ncomponents.", "published": "2021-10-11 00:08:48", "link": "http://arxiv.org/abs/2110.04934v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Advances in Multi-turn Dialogue Comprehension: A Survey", "abstract": "Training machines to understand natural language and interact with humans is\nan elusive and essential task of artificial intelligence. A diversity of\ndialogue systems has been designed with the rapid development of deep learning\ntechniques, especially the recent pre-trained language models (PrLMs). Among\nthese studies, the fundamental yet challenging type of task is dialogue\ncomprehension whose role is to teach the machines to read and comprehend the\ndialogue context before responding. In this paper, we review the previous\nmethods from the technical perspective of dialogue modeling for the dialogue\ncomprehension task. We summarize the characteristics and challenges of dialogue\ncomprehension in contrast to plain-text reading comprehension. Then, we discuss\nthree typical patterns of dialogue modeling. In addition, we categorize\ndialogue-related pre-training techniques which are employed to enhance PrLMs in\ndialogue scenarios. Finally, we highlight the technical advances in recent\nyears and point out the lessons from the empirical analysis and the prospects\ntowards a new frontier of researches.", "published": "2021-10-11 03:52:37", "link": "http://arxiv.org/abs/2110.04984v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Using Personality Detection Tools for Software Engineering Research: How\n  Far Can We Go?", "abstract": "Assessing the personality of software engineers may help to match individual\ntraits with the characteristics of development activities such as code review\nand testing, as well as support managers in team composition. However,\nself-assessment questionnaires are not a practical solution for collecting\nmultiple observations on a large scale. Instead, automatic personality\ndetection, while overcoming these limitations, is based on off-the-shelf\nsolutions trained on non-technical corpora, which might not be readily\napplicable to technical domains like Software Engineering (SE). In this paper,\nwe first assess the performance of general-purpose personality detection tools\nwhen applied to a technical corpus of developers' emails retrieved from the\npublic archives of the Apache Software Foundation. We observe a general low\naccuracy of predictions and an overall disagreement among the tools. Second, we\nreplicate two previous research studies in SE by replacing the personality\ndetection tool used to infer developers' personalities from pull-request\ndiscussions and emails. We observe that the original results are not confirmed,\ni.e., changing the tool used in the original study leads to diverging\nconclusions. Our results suggest a need for personality detection tools\nspecially targeted for the software engineering domain.", "published": "2021-10-11 07:02:34", "link": "http://arxiv.org/abs/2110.05035v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "ViSeRet: A simple yet effective approach to moment retrieval via\n  fine-grained video segmentation", "abstract": "Video-text retrieval has many real-world applications such as media\nanalytics, surveillance, and robotics. This paper presents the 1st place\nsolution to the video retrieval track of the ICCV VALUE Challenge 2021. We\npresent a simple yet effective approach to jointly tackle two video-text\nretrieval tasks (video retrieval and video corpus moment retrieval) by\nleveraging the model trained only on the video retrieval task. In addition, we\ncreate an ensemble model that achieves the new state-of-the-art performance on\nall four datasets (TVr, How2r, YouCook2r, and VATEXr) presented in the VALUE\nChallenge.", "published": "2021-10-11 10:39:13", "link": "http://arxiv.org/abs/2110.05146v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Comparative Study on Non-Autoregressive Modelings for Speech-to-Text\n  Generation", "abstract": "Non-autoregressive (NAR) models simultaneously generate multiple outputs in a\nsequence, which significantly reduces the inference speed at the cost of\naccuracy drop compared to autoregressive baselines. Showing great potential for\nreal-time applications, an increasing number of NAR models have been explored\nin different fields to mitigate the performance gap against AR models. In this\nwork, we conduct a comparative study of various NAR modeling methods for\nend-to-end automatic speech recognition (ASR). Experiments are performed in the\nstate-of-the-art setting using ESPnet. The results on various tasks provide\ninteresting findings for developing an understanding of NAR ASR, such as the\naccuracy-speed trade-off and robustness against long-form utterances. We also\nshow that the techniques can be combined for further improvement and applied to\nNAR end-to-end speech translation. All the implementations are publicly\navailable to encourage further research in NAR speech processing.", "published": "2021-10-11 13:05:06", "link": "http://arxiv.org/abs/2110.05249v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Gender Fairness of Pre-Trained Language Models without\n  Catastrophic Forgetting", "abstract": "Existing studies addressing gender bias of pre-trained language models,\nusually build a small gender-neutral data set and conduct a second phase\npre-training on the model with such data. However, given the limited size and\nconcentrated focus of the gender-neutral data, catastrophic forgetting would\noccur during second-phase pre-training. Forgetting information in the original\ntraining data may damage the model's downstream performance by a large margin.\nIn this work, we empirically show that catastrophic forgetting occurs in such\nmethods by evaluating them with general NLP tasks in GLUE. Then, we propose a\nnew method, GEnder Equality Prompt (GEEP), to improve gender fairness of\npre-trained models with less forgetting. GEEP freezes the pre-trained model and\nlearns gender-related prompts with gender-neutral data. Empirical results show\nthat GEEP not only achieves SOTA performances on gender fairness tasks, but\nalso forgets less and performs better on GLUE by a large margin.", "published": "2021-10-11 15:52:16", "link": "http://arxiv.org/abs/2110.05367v3", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Calibrate your listeners! Robust communication-based training for\n  pragmatic speakers", "abstract": "To be good conversational partners, natural language processing (NLP) systems\nshould be trained to produce contextually useful utterances. Prior work has\ninvestigated training NLP systems with communication-based objectives, where a\nneural listener stands in as a communication partner. However, these systems\ncommonly suffer from semantic drift where the learned language diverges\nradically from natural language. We propose a method that uses a population of\nneural listeners to regularize speaker training. We first show that language\ndrift originates from the poor uncertainty calibration of a neural listener,\nwhich makes high-certainty predictions on novel sentences. We explore ensemble-\nand dropout-based populations of listeners and find that the former results in\nbetter uncertainty quantification. We evaluate both population-based objectives\non reference games, and show that the ensemble method with better calibration\nenables the speaker to generate pragmatic utterances while scaling to a large\nvocabulary and generalizing to new games and listeners.", "published": "2021-10-11 17:07:38", "link": "http://arxiv.org/abs/2110.05422v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "We Need to Talk About Data: The Importance of Data Readiness in Natural\n  Language Processing", "abstract": "In this paper, we identify the state of data as being an important reason for\nfailure in applied Natural Language Processing (NLP) projects. We argue that\nthere is a gap between academic research in NLP and its application to problems\noutside academia, and that this gap is rooted in poor mutual understanding\nbetween academic researchers and their non-academic peers who seek to apply\nresearch results to their operations. To foster transfer of research results\nfrom academia to non-academic settings, and the corresponding influx of\nrequirements back to academia, we propose a method for improving the\ncommunication between researchers and external stakeholders regarding the\naccessibility, validity, and utility of data based on Data Readiness Levels\n\\cite{lawrence2017data}. While still in its infancy, the method has been\niterated on and applied in multiple innovation and research projects carried\nout with stakeholders in both the private and public sectors. Finally, we\ninvite researchers and practitioners to share their experiences, and thus\ncontributing to a body of work aimed at raising awareness of the importance of\ndata readiness for NLP.", "published": "2021-10-11 17:55:07", "link": "http://arxiv.org/abs/2110.05464v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spatial Data Mining of Public Transport Incidents reported in Social\n  Media", "abstract": "Public transport agencies use social media as an essential tool for\ncommunicating mobility incidents to passengers. However, while the short term,\nday-to-day information about transport phenomena is usually posted in social\nmedia with low latency, its availability is short term as the content is rarely\nmade an aggregated form. Social media communication of transport phenomena\nusually lacks GIS annotations as most social media platforms do not allow\nattaching non-POI GPS coordinates to posts. As a result, the analysis of\ntransport phenomena information is minimal. We collected three years of social\nmedia posts of a polish public transport company with user comments. Through\nexploration, we infer a six-class transport information typology. We\nsuccessfully build an information type classifier for social media posts,\ndetect stop names in posts, and relate them to GPS coordinates, obtaining a\nspatial understanding of long-term aggregated phenomena. We show that our\napproach enables citizen science and use it to analyze the impact of three\nyears of infrastructure incidents on passenger mobility, and the sentiment and\nreaction scale towards each of the events. All these results are achieved for\nPolish, an under-resourced language when it comes to spatial language\nunderstanding, especially in social media contexts. To improve the situation,\nwe released two of our annotated data sets: social media posts with incident\ntype labels and matched stop names and social media comments with the annotated\nsentiment. We also opensource the experimental codebase.", "published": "2021-10-11 19:28:11", "link": "http://arxiv.org/abs/2110.05573v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Advancing Momentum Pseudo-Labeling with Conformer and Initialization\n  Strategy", "abstract": "Pseudo-labeling (PL), a semi-supervised learning (SSL) method where a seed\nmodel performs self-training using pseudo-labels generated from untranscribed\nspeech, has been shown to enhance the performance of end-to-end automatic\nspeech recognition (ASR). Our prior work proposed momentum pseudo-labeling\n(MPL), which performs PL-based SSL via an interaction between online and\noffline models, inspired by the mean teacher framework. MPL achieves remarkable\nresults on various semi-supervised settings, showing robustness to variations\nin the amount of data and domain mismatch severity. However, there is further\nroom for improving the seed model used to initialize the MPL training, as it is\nin general critical for a PL-based method to start training from high-quality\npseudo-labels. To this end, we propose to enhance MPL by (1) introducing the\nConformer architecture to boost the overall recognition accuracy and (2)\nexploiting iterative pseudo-labeling with a language model to improve the seed\nmodel before applying MPL. The experimental results demonstrate that the\nproposed approaches effectively improve MPL performance, outperforming other\nPL-based methods. We also present in-depth investigations to make our\nimprovements effective, e.g., with regard to batch normalization typically used\nin Conformer and LM quality.", "published": "2021-10-11 00:52:14", "link": "http://arxiv.org/abs/2110.04948v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Kernel Learning For Sound Field Estimation With L1 and L2\n  Regularizations", "abstract": "A method to estimate an acoustic field from discrete microphone measurements\nis proposed. A kernel-interpolation-based method using the kernel function\nformulated for sound field interpolation has been used in various applications.\nThe kernel function with directional weighting makes it possible to incorporate\nprior information on source directions to improve estimation accuracy. However,\nin prior studies, parameters for directional weighting have been empirically\ndetermined. We propose a method to optimize these parameters using observation\nvalues, which is particularly useful when prior information on source\ndirections is uncertain. The proposed algorithm is based on discretization of\nthe parameters and representation of the kernel function as a weighted sum of\nsub-kernels. Two types of regularization for the weights, $L_1$ and $L_2$, are\ninvestigated. Experimental results indicate that the proposed method achieves\nhigher estimation accuracy than the method without kernel learning.", "published": "2021-10-11 03:02:54", "link": "http://arxiv.org/abs/2110.04972v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-query multi-head attention pooling and Inter-topK penalty for\n  speaker verification", "abstract": "This paper describes the multi-query multi-head attention (MQMHA) pooling and\ninter-topK penalty methods which were first proposed in our submitted system\ndescription for VoxCeleb speaker recognition challenge (VoxSRC) 2021. Most\nmulti-head attention pooling mechanisms either attend to the whole feature\nthrough multiple heads or attend to several split parts of the whole feature.\nOur proposed MQMHA combines both these two mechanisms and gain more diversified\ninformation. The margin-based softmax loss functions are commonly adopted to\nobtain discriminative speaker representations. To further enhance the\ninter-class discriminability, we propose a method that adds an extra inter-topK\npenalty on some confused speakers. By adopting both the MQMHA and inter-topK\npenalty, we achieved state-of-the-art performance in all of the public VoxCeleb\ntest sets.", "published": "2021-10-11 07:20:30", "link": "http://arxiv.org/abs/2110.05042v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Amicable examples for informed source separation", "abstract": "This paper deals with the problem of informed source separation (ISS), where\nthe sources are accessible during the so-called \\textit{encoding} stage.\nPrevious works computed side-information during the encoding stage and source\nseparation models were designed to utilize the side-information to improve the\nseparation performance. In contrast, in this work, we improve the performance\nof a pretrained separation model that does not use any side-information. To\nthis end, we propose to adopt an adversarial attack for the opposite purpose,\ni.e., rather than computing the perturbation to degrade the separation, we\ncompute an imperceptible perturbation called amicable noise to improve the\nseparation. Experimental results show that the proposed approach selectively\nimproves the performance of the targeted separation model by 2.23 dB on average\nand is robust to signal compression. Moreover, we propose multi-model\nmulti-purpose learning that control the effect of the perturbation on different\nmodels individually.", "published": "2021-10-11 07:54:41", "link": "http://arxiv.org/abs/2110.05059v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multi-Resolution Front-End for End-to-End Speech Anti-Spoofing", "abstract": "The choice of an optimal time-frequency resolution is usually a difficult but\nimportant step in tasks involving speech signal classification, e.g., speech\nanti-spoofing. The variations of the performance with different choices of\ntimefrequency resolutions can be as large as those with different model\narchitectures, which makes it difficult to judge what the improvement actually\ncomes from when a new network architecture is invented and introduced as the\nclassifier. In this paper, we propose a multi-resolution front-end for feature\nextraction in an end-to-end classification framework. Optimal weighted\ncombinations of multiple time-frequency resolutions will be learned\nautomatically given the objective of a classification task. Features extracted\nwith different time-frequency resolutions are weighted and concatenated as\ninputs to the successive networks, where the weights are predicted by a\nlearnable neural network inspired by the weighting block in\nsqueeze-and-excitation networks (SENet). Furthermore, the refinement of the\nchosen timefrequency resolutions is investigated by pruning the ones with\nrelatively low importance, which reduces the complexity and size of the model.\nThe proposed method is evaluated on the tasks of speech anti-spoofing in\nASVSpoof 2019 and its superiority has been justified by comparing with similar\nbaselines.", "published": "2021-10-11 08:44:18", "link": "http://arxiv.org/abs/2110.05087v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "vocadito: A dataset of solo vocals with $f_0$, note, and lyric\n  annotations", "abstract": "To compliment the existing set of datasets, we present a small dataset\nentitled vocadito, consisting of 40 short excerpts of monophonic singing, sung\nin 7 different languages by singers with varying of levels of training, and\nrecorded on a variety of devices. We provide several types of annotations,\nincluding $f_0$, lyrics, and two different note annotations. All annotations\nwere created by musicians. We provide an analysis of the differences between\nthe two note annotations, and see that the agreement level is low, which has\nimplications for evaluating vocal note estimation algorithms. We also analyze\nthe relation between the $f_0$ and note annotations, and show that quantizing\n$f_0$ values in frequency does not provide a reasonable note estimate,\nreinforcing the difficulty of the note estimation task for singing voice.\nFinally, we provide baseline results from recent algorithms on vocadito for\nnote and $f_0$ transcription. Vocadito is made freely available for public use.", "published": "2021-10-11 19:42:55", "link": "http://arxiv.org/abs/2110.05580v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LaughNet: synthesizing laughter utterances from waveform silhouettes and\n  a single laughter example", "abstract": "Emotional and controllable speech synthesis is a topic that has received much\nattention. However, most studies focused on improving the expressiveness and\ncontrollability in the context of linguistic content, even though natural\nverbal human communication is inseparable from spontaneous non-speech\nexpressions such as laughter, crying, or grunting. We propose a model called\nLaughNet for synthesizing laughter by using waveform silhouettes as inputs. The\nmotivation is not simply synthesizing new laughter utterances, but testing a\nnovel synthesis-control paradigm that uses an abstract representation of the\nwaveform. We conducted basic listening test experiments, and the results showed\nthat LaughNet can synthesize laughter utterances with moderate quality and\nretain the characteristics of the training example. More importantly, the\ngenerated waveforms have shapes similar to the input silhouettes. For future\nwork, we will test the same method on other types of human nonverbal\nexpressions and integrate it into more elaborated synthesis systems.", "published": "2021-10-11 00:45:07", "link": "http://arxiv.org/abs/2110.04946v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MELONS: generating melody with long-term structure using transformers\n  and structure graph", "abstract": "The creation of long melody sequences requires effective expression of\ncoherent musical structure. However, there is no clear representation of\nmusical structure. Recent works on music generation have suggested various\napproaches to deal with the structural information of music, but generating a\nfull-song melody with clear long-term structure remains a challenge. In this\npaper, we propose MELONS, a melody generation framework based on a graph\nrepresentation of music structure which consists of eight types of bar-level\nrelations. MELONS adopts a multi-step generation method with transformer-based\nnetworks by factoring melody generation into two sub-problems: structure\ngeneration and structure conditional melody generation. Experimental results\nshow that MELONS can produce structured melodies with high quality and rich\ncontents.", "published": "2021-10-11 06:12:51", "link": "http://arxiv.org/abs/2110.05020v3", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pitch Preservation In Singing Voice Synthesis", "abstract": "Suffering from limited singing voice corpus, existing singing voice synthesis\n(SVS) methods that build encoder-decoder neural networks to directly generate\nspectrogram could lead to out-of-tune issues during the inference phase. To\nattenuate these issues, this paper presents a novel acoustic model with\nindependent pitch encoder and phoneme encoder, which disentangles the phoneme\nand pitch information from music score to fully utilize the corpus.\nSpecifically, according to equal temperament theory, the pitch encoder is\nconstrained by a pitch metric loss that maps distances between adjacent input\npitches into corresponding frequency multiples between the encoder outputs. For\nthe phoneme encoder, based on the analysis that same phonemes corresponding to\nvarying pitches can produce similar pronunciations, this encoder is followed by\nan adversarially trained pitch classifier to enforce the identical phonemes\nwith different pitches mapping into the same phoneme feature space. By these\nmeans, the sparse phonemes and pitches in original input spaces can be\ntransformed into more compact feature spaces respectively, where the same\nelements cluster closely and cooperate mutually to enhance synthesis quality.\nThen, the outputs of the two encoders are summed together to pass through the\nfollowing decoder in the acoustic model. Experimental results indicate that the\nproposed approaches can characterize intrinsic structure between pitch inputs\nto obtain better pitch synthesis accuracy and achieve superior singing\nsynthesis performance against the advanced baseline system.", "published": "2021-10-11 07:01:06", "link": "http://arxiv.org/abs/2110.05033v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Source Mixing and Separation Robust Audio Steganography", "abstract": "Audio steganography aims at concealing secret information in carrier audio\nwith imperceptible modification on the carrier. Although previous works\naddressed the robustness of concealed message recovery against distortions\nintroduced during transmission, they do not address the robustness against\naggressive editing such as mixing of other audio sources and source separation.\nIn this work, we propose for the first time a steganography method that can\nembed information into individual sound sources in a mixture such as\ninstrumental tracks in music. To this end, we propose a time-domain model and\ncurriculum learning essential to learn to decode the concealed message from the\nseparated sources. Experimental results show that the proposed method\nsuccessfully conceals the information in an imperceptible perturbation and that\nthe information can be correctly recovered even after mixing of other sources\nand separation by a source separation algorithm. Furthermore, we show that the\nproposed method can be applied to multiple sources simultaneously without\ninterfering with the decoder for other sources even after the sources are mixed\nand separated.", "published": "2021-10-11 07:47:41", "link": "http://arxiv.org/abs/2110.05054v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Training of Audio Transformers with Patchout", "abstract": "The great success of transformer-based models in natural language processing\n(NLP) has led to various attempts at adapting these architectures to other\ndomains such as vision and audio. Recent work has shown that transformers can\noutperform Convolutional Neural Networks (CNNs) on vision and audio tasks.\nHowever, one of the main shortcomings of transformer models, compared to the\nwell-established CNNs, is the computational complexity. In transformers, the\ncompute and memory complexity is known to grow quadratically with the input\nlength. Therefore, there has been extensive work on optimizing transformers,\nbut often at the cost of degrading predictive performance. In this work, we\npropose a novel method to optimize and regularize transformers on audio\nspectrograms. Our proposed models achieve a new state-of-the-art performance on\nAudioset and can be trained on a single consumer-grade GPU. Furthermore, we\npropose a transformer model that outperforms CNNs in terms of both performance\nand training speed. Source code: https://github.com/kkoutini/PaSST", "published": "2021-10-11 08:07:50", "link": "http://arxiv.org/abs/2110.05069v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interactive Feature Fusion for End-to-End Noise-Robust Speech\n  Recognition", "abstract": "Speech enhancement (SE) aims to suppress the additive noise from a noisy\nspeech signal to improve the speech's perceptual quality and intelligibility.\nHowever, the over-suppression phenomenon in the enhanced speech might degrade\nthe performance of downstream automatic speech recognition (ASR) task due to\nthe missing latent information. To alleviate such problem, we propose an\ninteractive feature fusion network (IFF-Net) for noise-robust speech\nrecognition to learn complementary information from the enhanced feature and\noriginal noisy feature. Experimental results show that the proposed method\nachieves absolute word error rate (WER) reduction of 4.1% over the best\nbaseline on RATS Channel-A corpus. Our further analysis indicates that the\nproposed IFF-Net can complement some missing information in the over-suppressed\nenhanced feature.", "published": "2021-10-11 13:40:07", "link": "http://arxiv.org/abs/2110.05267v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Source Separation via Bayesian Inference in the Latent\n  Domain", "abstract": "State of the art audio source separation models rely on supervised\ndata-driven approaches, which can be expensive in terms of labeling resources.\nOn the other hand, approaches for training these models without any direct\nsupervision are typically high-demanding in terms of memory and time\nrequirements, and remain impractical to be used at inference time. We aim to\ntackle these limitations by proposing a simple yet effective unsupervised\nseparation algorithm, which operates directly on a latent representation of\ntime-domain signals. Our algorithm relies on deep Bayesian priors in the form\nof pre-trained autoregressive networks to model the probability distributions\nof each source. We leverage the low cardinality of the discrete latent space,\ntrained with a novel loss term imposing a precise arithmetic structure on it,\nto perform exact Bayesian inference without relying on an approximation\nstrategy. We validate our approach on the Slakh dataset arXiv:1909.08494,\ndemonstrating results in line with state of the art supervised approaches while\nrequiring fewer resources with respect to other unsupervised methods.", "published": "2021-10-11 14:32:55", "link": "http://arxiv.org/abs/2110.05313v4", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "DeepFilterNet: A Low Complexity Speech Enhancement Framework for\n  Full-Band Audio based on Deep Filtering", "abstract": "Complex-valued processing has brought deep learning-based speech enhancement\nand signal extraction to a new level. Typically, the process is based on a\ntime-frequency (TF) mask which is applied to a noisy spectrogram, while complex\nmasks (CM) are usually preferred over real-valued masks due to their ability to\nmodify the phase. Recent work proposed to use a complex filter instead of a\npoint-wise multiplication with a mask. This allows to incorporate information\nfrom previous and future time steps exploiting local correlations within each\nfrequency band. In this work, we propose DeepFilterNet, a two stage speech\nenhancement framework utilizing deep filtering. First, we enhance the spectral\nenvelope using ERB-scaled gains modeling the human frequency perception. The\nsecond stage employs deep filtering to enhance the periodic components of\nspeech. Additionally to taking advantage of perceptual properties of speech, we\nenforce network sparsity via separable convolutions and extensive grouping in\nlinear and recurrent layers to design a low complexity architecture. We further\nshow that our two stage deep filtering approach outperforms complex masks over\na variety of frequency resolutions and latencies and demonstrate convincing\nperformance compared to other state-of-the-art models.", "published": "2021-10-11 20:03:52", "link": "http://arxiv.org/abs/2110.05588v2", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Partial Variable Training for Efficient On-Device Federated Learning", "abstract": "This paper aims to address the major challenges of Federated Learning (FL) on\nedge devices: limited memory and expensive communication. We propose a novel\nmethod, called Partial Variable Training (PVT), that only trains a small subset\nof variables on edge devices to reduce memory usage and communication cost.\nWith PVT, we show that network accuracy can be maintained by utilizing more\nlocal training steps and devices, which is favorable for FL involving a large\npopulation of devices. According to our experiments on two state-of-the-art\nneural networks for speech recognition and two different datasets, PVT can\nreduce memory usage by up to 1.9$\\times$ and communication cost by up to\n593$\\times$ while attaining comparable accuracy when compared with full network\ntraining.", "published": "2021-10-11 20:57:06", "link": "http://arxiv.org/abs/2110.05607v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Rank-based loss for learning hierarchical representations", "abstract": "Hierarchical taxonomies are common in many contexts, and they are a very\nnatural structure humans use to organise information. In machine learning, the\nfamily of methods that use the 'extra' information is called hierarchical\nclassification. However, applied to audio classification, this remains\nrelatively unexplored. Here we focus on how to integrate the hierarchical\ninformation of a problem to learn embeddings representative of the hierarchical\nrelationships. Previously, triplet loss has been proposed to address this\nproblem, however it presents some issues like requiring the careful\nconstruction of the triplets, and being limited in the extent of hierarchical\ninformation it uses at each iteration. In this work we propose a rank based\nloss function that uses hierarchical information and translates this into a\nrank ordering of target distances between the examples. We show that rank based\nloss is suitable to learn hierarchical representations of the data. By testing\non unseen fine level classes we show that this method is also capable of\nlearning hierarchically correct representations of the new classes. Rank based\nloss has two promising aspects, it is generalisable to hierarchies with any\nnumber of levels, and is capable of dealing with data with incomplete\nhierarchical labels.", "published": "2021-10-11 10:32:45", "link": "http://arxiv.org/abs/2110.05941v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multi-View Self-Attention Based Transformer for Speaker Recognition", "abstract": "Initially developed for natural language processing (NLP), Transformer model\nis now widely used for speech processing tasks such as speaker recognition, due\nto its powerful sequence modeling capabilities. However, conventional\nself-attention mechanisms are originally designed for modeling textual sequence\nwithout considering the characteristics of speech and speaker modeling.\nBesides, different Transformer variants for speaker recognition have not been\nwell studied. In this work, we propose a novel multi-view self-attention\nmechanism and present an empirical study of different Transformer variants with\nor without the proposed attention mechanism for speaker recognition.\nSpecifically, to balance the capabilities of capturing global dependencies and\nmodeling the locality, we propose a multi-view self-attention mechanism for\nspeaker Transformer, in which different attention heads can attend to different\nranges of the receptive field. Furthermore, we introduce and compare five\nTransformer variants with different network architectures, embedding locations,\nand pooling methods to learn speaker embeddings. Experimental results on the\nVoxCeleb1 and VoxCeleb2 datasets show that the proposed multi-view\nself-attention mechanism achieves improvement in the performance of speaker\nrecognition, and the proposed speaker Transformer network attains excellent\nresults compared with state-of-the-art models.", "published": "2021-10-11 07:03:23", "link": "http://arxiv.org/abs/2110.05036v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Evaluation of Latent Space Disentanglement in the Presence of\n  Interdependent Attributes", "abstract": "Controllable music generation with deep generative models has become\nincreasingly reliant on disentanglement learning techniques. However, current\ndisentanglement metrics, such as mutual information gap (MIG), are often\ninadequate and misleading when used for evaluating latent representations in\nthe presence of interdependent semantic attributes often encountered in\nreal-world music datasets. In this work, we propose a dependency-aware\ninformation metric as a drop-in replacement for MIG that accounts for the\ninherent relationship between semantic attributes.", "published": "2021-10-11 20:01:14", "link": "http://arxiv.org/abs/2110.05587v1", "categories": ["cs.SD", "cs.IR", "cs.IT", "cs.LG", "eess.AS", "math.IT"], "primary_category": "cs.SD"}
