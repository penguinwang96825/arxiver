{"title": "Cross-language Learning with Adversarial Neural Networks: Application to\n  Community Question Answering", "abstract": "We address the problem of cross-language adaptation for question-question\nsimilarity reranking in community question answering, with the objective to\nport a system trained on one input language to another input language given\nlabeled training data for the first language and only unlabeled data for the\nsecond language. In particular, we propose to use adversarial training of\nneural networks to learn high-level features that are discriminative for the\nmain learning task, and at the same time are invariant across the input\nlanguages. The evaluation results show sizable improvements for our\ncross-language adversarial neural network (CLANN) model over a strong\nnon-adversarial system.", "published": "2017-06-21 06:11:59", "link": "http://arxiv.org/abs/1706.06749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JaTeCS an open-source JAva TExt Categorization System", "abstract": "JaTeCS is an open source Java library that supports research on automatic\ntext categorization and other related problems, such as ordinal regression and\nquantification, which are of special interest in opinion mining applications.\nIt covers all the steps of an experimental activity, from reading the corpus to\nthe evaluation of the experimental results. As JaTeCS is focused on text as the\nmain input data, it provides the user with many text-dedicated tools, e.g.:\ndata readers for many formats, including the most commonly used text corpora\nand lexical resources, natural language processing tools, multi-language\nsupport, methods for feature selection and weighting, the implementation of\nmany machine learning algorithms as well as wrappers for well-known external\nsoftware (e.g., SVM_light) which enable their full control from code. JaTeCS\nsupport its expansion by abstracting through interfaces many of the typical\ntools and procedures used in text processing tasks. The library also provides a\nnumber of \"template\" implementations of typical experimental setups (e.g.,\ntrain-test, k-fold validation, grid-search optimization, randomized runs) which\nenable fast realization of experiments just by connecting the templates with\ndata readers, learning algorithms and evaluation measures.", "published": "2017-06-21 09:14:02", "link": "http://arxiv.org/abs/1706.06802v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Detection in Turkish Tweets", "abstract": "Stance detection is a classification problem in natural language processing\nwhere for a text and target pair, a class result from the set {Favor, Against,\nNeither} is expected. It is similar to the sentiment analysis problem but\ninstead of the sentiment of the text author, the stance expressed for a\nparticular target is investigated in stance detection. In this paper, we\npresent a stance detection tweet data set for Turkish comprising stance\nannotations of these tweets for two popular sports clubs as targets.\nAdditionally, we provide the evaluation results of SVM classifiers for each\ntarget on this data set, where the classifiers use unigram, bigram, and hashtag\nfeatures. This study is significant as it presents one of the initial stance\ndetection data sets proposed so far and the first one for Turkish language, to\nthe best of our knowledge. The data set and the evaluation results of the\ncorresponding SVM-based approaches will form plausible baselines for the\ncomparison of future studies on stance detection.", "published": "2017-06-21 13:32:21", "link": "http://arxiv.org/abs/1706.06894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural-based Natural Language Generation in Dialogue using RNN\n  Encoder-Decoder with Semantic Aggregation", "abstract": "Natural language generation (NLG) is an important component in spoken\ndialogue systems. This paper presents a model called Encoder-Aggregator-Decoder\nwhich is an extension of an Recurrent Neural Network based Encoder-Decoder\narchitecture. The proposed Semantic Aggregator consists of two components: an\nAligner and a Refiner. The Aligner is a conventional attention calculated over\nthe encoded input information, while the Refiner is another attention or gating\nmechanism stacked over the attentive Aligner in order to further select and\naggregate the semantic elements. The proposed model can be jointly trained both\nsentence planning and surface realization to produce natural language\nutterances. The model was extensively assessed on four different NLG domains,\nin which the experimental results showed that the proposed generator\nconsistently outperforms the previous methods on all the NLG domains.", "published": "2017-06-21 01:07:02", "link": "http://arxiv.org/abs/1706.06714v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Generative Model of Group Conversation", "abstract": "Conversations with non-player characters (NPCs) in games are typically\nconfined to dialogue between a human player and a virtual agent, where the\nconversation is initiated and controlled by the player. To create richer, more\nbelievable environments for players, we need conversational behavior to reflect\ninitiative on the part of the NPCs, including conversations that include\nmultiple NPCs who interact with one another as well as the player. We describe\na generative computational model of group conversation between agents, an\nabstract simulation of discussion in a small group setting. We define\nconversational interactions in terms of rules for turn taking and interruption,\nas well as belief change, sentiment change, and emotional response, all of\nwhich are dependent on agent personality, context, and relationships. We\nevaluate our model using a parameterized expressive range analysis, observing\ncorrelations between simulation parameters and features of the resulting\nconversations. This analysis confirms, for example, that character\npersonalities will predict how often they speak, and that heterogeneous groups\nof characters will generate more belief change.", "published": "2017-06-21 16:19:59", "link": "http://arxiv.org/abs/1706.06987v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Statistical Inferences for Polarity Identification in Natural Language", "abstract": "Information forms the basis for all human behavior, including the ubiquitous\ndecision-making that people constantly perform in their every day lives. It is\nthus the mission of researchers to understand how humans process information to\nreach decisions. In order to facilitate this task, this work proposes a novel\nmethod of studying the reception of granular expressions in natural language.\nThe approach utilizes LASSO regularization as a statistical tool to extract\ndecisive words from textual content and draw statistical inferences based on\nthe correspondence between the occurrences of words and an exogenous response\nvariable. Accordingly, the method immediately suggests significant implications\nfor social sciences and Information Systems research: everyone can now identify\ntext segments and word choices that are statistically relevant to authors or\nreaders and, based on this knowledge, test hypotheses from behavioral research.\nWe demonstrate the contribution of our method by examining how authors\ncommunicate subjective information through narrative materials. This allows us\nto answer the question of which words to choose when communicating negative\ninformation. On the other hand, we show that investors trade not only upon\nfacts in financial disclosures but are distracted by filler words and\nnon-informative language. Practitioners - for example those in the fields of\ninvestor communications or marketing - can exploit our insights to enhance\ntheir writings based on the true perception of word choice.", "published": "2017-06-21 16:37:54", "link": "http://arxiv.org/abs/1706.06996v2", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Jointly Learning Word Embeddings and Latent Topics", "abstract": "Word embedding models such as Skip-gram learn a vector-space representation\nfor each word, based on the local word collocation patterns that are observed\nin a text corpus. Latent topic models, on the other hand, take a more global\nview, looking at the word distributions across the corpus to assign a topic to\neach word occurrence. These two paradigms are complementary in how they\nrepresent the meaning of word occurrences. While some previous works have\nalready looked at using word embeddings for improving the quality of latent\ntopics, and conversely, at using latent topics for improving word embeddings,\nsuch \"two-step\" methods cannot capture the mutual interaction between the two\nparadigms. In this paper, we propose STE, a framework which can learn word\nembeddings and latent topics in a unified manner. STE naturally obtains\ntopic-specific word embeddings, and thus addresses the issue of polysemy. At\nthe same time, it also learns the term distributions of the topics, and the\ntopic distributions of the documents. Our experimental results demonstrate that\nthe STE model can indeed generate useful topic-specific word embeddings and\ncoherent latent topics in an effective and efficient way.", "published": "2017-06-21 06:19:24", "link": "http://arxiv.org/abs/1706.07276v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
