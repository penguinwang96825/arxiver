{"title": "Towards annotation of text worlds in a literary work", "abstract": "Literary texts are usually rich in meanings and their interpretation\ncomplicates corpus studies and automatic processing. There have been several\nattempts to create collections of literary texts with annotation of literary\nelements like the author's speech, characters, events, scenes etc. However,\nthey resulted in small collections and standalone rules for annotation. The\npresent article describes an experiment on lexical annotation of text worlds in\na literary work and quantitative methods of their comparison. The experiment\nshows that for a well-agreed tag assignment annotation rules should be set much\nmore strictly. However, if borders between text worlds and other elements are\nthe result of a subjective interpretation, they should be modeled as fuzzy\nentities.", "published": "2021-11-14 06:37:01", "link": "http://arxiv.org/abs/2111.07256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Will You Find These Shortcuts?\" A Protocol for Evaluating the\n  Faithfulness of Input Salience Methods for Text Classification", "abstract": "Feature attribution a.k.a. input salience methods which assign an importance\nscore to a feature are abundant but may produce surprisingly different results\nfor the same model on the same input. While differences are expected if\ndisparate definitions of importance are assumed, most methods claim to provide\nfaithful attributions and point at the features most relevant for a model's\nprediction. Existing work on faithfulness evaluation is not conclusive and does\nnot provide a clear answer as to how different methods are to be compared.\nFocusing on text classification and the model debugging scenario, our main\ncontribution is a protocol for faithfulness evaluation that makes use of\npartially synthetic data to obtain ground truth for feature importance ranking.\nFollowing the protocol, we do an in-depth analysis of four standard salience\nmethod classes on a range of datasets and shortcuts for BERT and LSTM models\nand demonstrate that some of the most popular method configurations provide\npoor results even for simplest shortcuts. We recommend following the protocol\nfor each new task and model combination to find the best method for identifying\nshortcuts.", "published": "2021-11-14 15:31:29", "link": "http://arxiv.org/abs/2111.07367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time Waits for No One! Analysis and Challenges of Temporal Misalignment", "abstract": "When an NLP model is trained on text data from one time period and tested or\ndeployed on data from another, the resulting temporal misalignment can degrade\nend-task performance. In this work, we establish a suite of eight diverse tasks\nacross different domains (social media, science papers, news, and reviews) and\nperiods of time (spanning five years or more) to quantify the effects of\ntemporal misalignment. Our study is focused on the ubiquitous setting where a\npretrained model is optionally adapted through continued domain-specific\npretraining, followed by task-specific finetuning. We establish a suite of\ntasks across multiple domains to study temporal misalignment in modern NLP\nsystems. We find stronger effects of temporal misalignment on task performance\nthan have been previously reported. We also find that, while temporal\nadaptation through continued pretraining can help, these gains are small\ncompared to task-specific finetuning on data from the target time period. Our\nfindings motivate continued research to improve temporal robustness of NLP\nmodels.", "published": "2021-11-14 18:29:19", "link": "http://arxiv.org/abs/2111.07408v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Jargon: Combining Extraction and Generation for Definition\n  Modeling", "abstract": "Can machines know what twin prime is? From the composition of this phrase,\nmachines may guess twin prime is a certain kind of prime, but it is still\ndifficult to deduce exactly what twin stands for without additional knowledge.\nHere, twin prime is a jargon - a specialized term used by experts in a\nparticular field. Explaining jargon is challenging since it usually requires\ndomain knowledge to understand. Recently, there is an increasing interest in\nextracting and generating definitions of words automatically. However, existing\napproaches, either extraction or generation, perform poorly on jargon. In this\npaper, we propose to combine extraction and generation for jargon definition\nmodeling: first extract self- and correlative definitional information of\ntarget jargon from the Web and then generate the final definitions by\nincorporating the extracted definitional information. Our framework is\nremarkably simple but effective: experiments demonstrate our method can\ngenerate high-quality definitions for jargon and outperform state-of-the-art\nmodels significantly, e.g., BLEU score from 8.76 to 22.66 and human-annotated\nscore from 2.34 to 4.04.", "published": "2021-11-14 08:03:18", "link": "http://arxiv.org/abs/2111.07267v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DEEP: DEnoising Entity Pre-training for Neural Machine Translation", "abstract": "It has been shown that machine translation models usually generate poor\ntranslations for named entities that are infrequent in the training corpus.\nEarlier named entity translation methods mainly focus on phonetic\ntransliteration, which ignores the sentence context for translation and is\nlimited in domain and language coverage. To address this limitation, we propose\nDEEP, a DEnoising Entity Pre-training method that leverages large amounts of\nmonolingual data and a knowledge base to improve named entity translation\naccuracy within sentences. Besides, we investigate a multi-task learning\nstrategy that finetunes a pre-trained neural machine translation model on both\nentity-augmented monolingual data and parallel data to further improve entity\ntranslation. Experimental results on three language pairs demonstrate that\n\\method results in significant improvements over strong denoising auto-encoding\nbaselines, with a gain of up to 1.3 BLEU and up to 9.2 entity accuracy points\nfor English-Russian translation.", "published": "2021-11-14 17:28:09", "link": "http://arxiv.org/abs/2111.07393v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Clustering: Toward Unsupervised Bias Reduction for Emotion\n  and Sentiment Classification", "abstract": "Background: When neural network emotion and sentiment classifiers are used in\npublic health informatics studies, biases present in the classifiers could\nproduce inadvertently misleading results.\n  Objective: This study assesses the impact of bias on COVID-19 topics, and\ndemonstrates an automatic algorithm for reducing bias when applied to COVID-19\nsocial media texts. This could help public health informatics studies produce\nmore timely results during crises, with a reduced risk of misleading results.\n  Methods: Emotion and sentiment classifiers were applied to COVID-19 data\nbefore and after debiasing the classifiers using unsupervised contrastive\nclustering. Contrastive clustering approximates the degree to which tokens\nexhibit a causal versus correlational relationship with emotion or sentiment,\nby contrasting the tokens' relative salience to topics versus emotions or\nsentiments.\n  Results: Contrastive clustering distinguishes correlation from causation for\ntokens with an F1 score of 0.753. Masking bias prone tokens from the classifier\ninput decreases the classifier's overall F1 score by 0.02 (anger) and 0.033\n(negative sentiment), but improves the F1 score for sentences annotated as bias\nprone by 0.155 (anger) and 0.103 (negative sentiment). Averaging across topics,\ndebiasing reduces anger estimates by 14.4% and negative sentiment estimates by\n8.0%.\n  Conclusions: Contrastive clustering reduces algorithmic bias in emotion and\nsentiment classification for social media text pertaining to the COVID-19\npandemic. Public health informatics studies should account for bias, due to its\nprevalence across a range of topics. Further research is needed to improve bias\nreduction techniques and to explore the adverse impact of bias on public health\ninformatics analyses.", "published": "2021-11-14 20:58:04", "link": "http://arxiv.org/abs/2111.07448v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Automatic evaluation of scientific abstracts through natural language\n  processing", "abstract": "This work presents a framework to classify and evaluate distinct research\nabstract texts which are focused on the description of processes and their\napplications. In this context, this paper proposes natural language processing\nalgorithms to classify, segment and evaluate the results of scientific work.\nInitially, the proposed framework categorize the abstract texts into according\nto the problems intended to be solved by employing a text classification\napproach. Then, the abstract text is segmented into problem description,\nmethodology and results. Finally, the methodology of the abstract is ranked\nbased on the sentiment analysis of its results. The proposed framework allows\nus to quickly rank the best methods to solve specific problems. To validate the\nproposed framework, oil production anomaly abstracts were experimented and\nachieved promising results.", "published": "2021-11-14 12:55:29", "link": "http://arxiv.org/abs/2112.01842v1", "categories": ["cs.CL", "cs.LG", "62M10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Meta-Voice: Fast few-shot style transfer for expressive voice cloning\n  using meta learning", "abstract": "The task of few-shot style transfer for voice cloning in text-to-speech (TTS)\nsynthesis aims at transferring speaking styles of an arbitrary source speaker\nto a target speaker's voice using very limited amount of neutral data. This is\na very challenging task since the learning algorithm needs to deal with\nfew-shot voice cloning and speaker-prosody disentanglement at the same time.\nAccelerating the adaptation process for a new target speaker is of importance\nin real-world applications, but even more challenging. In this paper, we\napproach to the hard fast few-shot style transfer for voice cloning task using\nmeta learning. We investigate the model-agnostic meta-learning (MAML) algorithm\nand meta-transfer a pre-trained multi-speaker and multi-prosody base TTS model\nto be highly sensitive for adaptation with few samples. Domain adversarial\ntraining mechanism and orthogonal constraint are adopted to disentangle speaker\nand prosody representations for effective cross-speaker style transfer.\nExperimental results show that the proposed approach is able to conduct fast\nvoice cloning using only 5 samples (around 12 second speech data) from a target\nspeaker, with only 100 adaptation steps. Audio samples are available online.", "published": "2021-11-14 01:30:37", "link": "http://arxiv.org/abs/2111.07218v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Curriculum Learning for Vision-and-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) is a task where an agent navigates in an\nembodied indoor environment under human instructions. Previous works ignore the\ndistribution of sample difficulty and we argue that this potentially degrade\ntheir agent performance. To tackle this issue, we propose a novel\ncurriculum-based training paradigm for VLN tasks that can balance human prior\nknowledge and agent learning progress about training samples. We develop the\nprinciple of curriculum design and re-arrange the benchmark Room-to-Room (R2R)\ndataset to make it suitable for curriculum training. Experiments show that our\nmethod is model-agnostic and can significantly improve the performance, the\ngeneralizability, and the training efficiency of current state-of-the-art\nnavigation agents without increasing model complexity.", "published": "2021-11-14 03:02:07", "link": "http://arxiv.org/abs/2111.07228v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Towards Interpretability of Speech Pause in Dementia Detection using\n  Adversarial Learning", "abstract": "Speech pause is an effective biomarker in dementia detection. Recent deep\nlearning models have exploited speech pauses to achieve highly accurate\ndementia detection, but have not exploited the interpretability of speech\npauses, i.e., what and how positions and lengths of speech pauses affect the\nresult of dementia detection. In this paper, we will study the positions and\nlengths of dementia-sensitive pauses using adversarial learning approaches.\nSpecifically, we first utilize an adversarial attack approach by adding the\nperturbation to the speech pauses of the testing samples, aiming to reduce the\nconfidence levels of the detection model. Then, we apply an adversarial\ntraining approach to evaluate the impact of the perturbation in training\nsamples on the detection model. We examine the interpretability from the\nperspectives of model accuracy, pause context, and pause length. We found that\nsome pauses are more sensitive to dementia than other pauses from the model's\nperspective, e.g., speech pauses near to the verb \"is\". Increasing lengths of\nsensitive pauses or adding sensitive pauses leads the model inference to\nAlzheimer's Disease, while decreasing the lengths of sensitive pauses or\ndeleting sensitive pauses leads to non-AD.", "published": "2021-11-14 21:26:18", "link": "http://arxiv.org/abs/2111.07454v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Question Answering for Complex Electronic Health Records Database using\n  Unified Encoder-Decoder Architecture", "abstract": "An intelligent machine that can answer human questions based on electronic\nhealth records (EHR-QA) has a great practical value, such as supporting\nclinical decisions, managing hospital administration, and medical chatbots.\nPrevious table-based QA studies focusing on translating natural questions into\ntable queries (NLQ2SQL), however, suffer from the unique nature of EHR data due\nto complex and specialized medical terminology, hence increased decoding\ndifficulty. In this paper, we design UniQA, a unified encoder-decoder\narchitecture for EHR-QA where natural language questions are converted to\nqueries such as SQL or SPARQL. We also propose input masking (IM), a simple and\neffective method to cope with complex medical terms and various typos and\nbetter learn the SQL/SPARQL syntax. Combining the unified architecture with an\neffective auxiliary training objective, UniQA demonstrated a significant\nperformance improvement against the previous state-of-the-art model for\nMIMICSQL* (14.2% gain), the most complex NLQ2SQL dataset in the EHR domain, and\nits typo-ridden versions (approximately 28.8% gain). In addition, we confirmed\nconsistent results for the graph-based EHR-QA dataset, MIMICSPARQL*.", "published": "2021-11-14 05:01:38", "link": "http://arxiv.org/abs/2111.14703v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Textless Speech Emotion Conversion using Discrete and Decomposed\n  Representations", "abstract": "Speech emotion conversion is the task of modifying the perceived emotion of a\nspeech utterance while preserving the lexical content and speaker identity. In\nthis study, we cast the problem of emotion conversion as a spoken language\ntranslation task. We use a decomposition of the speech signal into discrete\nlearned representations, consisting of phonetic-content units, prosodic\nfeatures, speaker, and emotion. First, we modify the speech content by\ntranslating the phonetic-content units to a target emotion, and then predict\nthe prosodic features based on these units. Finally, the speech waveform is\ngenerated by feeding the predicted representations into a neural vocoder. Such\na paradigm allows us to go beyond spectral and parametric changes of the\nsignal, and model non-verbal vocalizations, such as laughter insertion, yawning\nremoval, etc. We demonstrate objectively and subjectively that the proposed\nmethod is vastly superior to current approaches and even beats text-based\nsystems in terms of perceived emotion and audio quality. We rigorously evaluate\nall components of such a complex system and conclude with an extensive model\nanalysis and ablation study to better emphasize the architectural choices,\nstrengths and weaknesses of the proposed method. Samples are available under\nthe following link: https://speechbot.github.io/emotion.", "published": "2021-11-14 18:16:42", "link": "http://arxiv.org/abs/2111.07402v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Intelligent Trading Systems: A Sentiment-Aware Reinforcement Learning\n  Approach", "abstract": "The feasibility of making profitable trades on a single asset on stock\nexchanges based on patterns identification has long attracted researchers.\nReinforcement Learning (RL) and Natural Language Processing have gained\nnotoriety in these single-asset trading tasks, but only a few works have\nexplored their combination. Moreover, some issues are still not addressed, such\nas extracting market sentiment momentum through the explicit capture of\nsentiment features that reflect the market condition over time and assessing\nthe consistency and stability of RL results in different situations. Filling\nthis gap, we propose the Sentiment-Aware RL (SentARL) intelligent trading\nsystem that improves profit stability by leveraging market mood through an\nadaptive amount of past sentiment features drawn from textual news. We\nevaluated SentARL across twenty assets, two transaction costs, and five\ndifferent periods and initializations to show its consistent effectiveness\nagainst baselines. Subsequently, this thorough assessment allowed us to\nidentify the boundary between news coverage and market sentiment regarding the\ncorrelation of price-time series above which SentARL's effectiveness is\noutstanding.", "published": "2021-11-14 16:30:45", "link": "http://arxiv.org/abs/2112.02095v1", "categories": ["q-fin.TR", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "q-fin.TR"}
{"title": "Forecasting Crude Oil Price Using Event Extraction", "abstract": "Research on crude oil price forecasting has attracted tremendous attention\nfrom scholars and policymakers due to its significant effect on the global\neconomy. Besides supply and demand, crude oil prices are largely influenced by\nvarious factors, such as economic development, financial markets, conflicts,\nwars, and political events. Most previous research treats crude oil price\nforecasting as a time series or econometric variable prediction problem.\nAlthough recently there have been researches considering the effects of\nreal-time news events, most of these works mainly use raw news headlines or\ntopic models to extract text features without profoundly exploring the event\ninformation. In this study, a novel crude oil price forecasting framework,\nAGESL, is proposed to deal with this problem. In our approach, an open domain\nevent extraction algorithm is utilized to extract underlying related events,\nand a text sentiment analysis algorithm is used to extract sentiment from\nmassive news. Then a deep neural network integrating the news event features,\nsentimental features, and historical price features is built to predict future\ncrude oil prices. Empirical experiments are performed on West Texas\nIntermediate (WTI) crude oil price data, and the results show that our approach\nobtains superior performance compared with several benchmark methods.", "published": "2021-11-14 08:48:43", "link": "http://arxiv.org/abs/2111.09111v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "econ.GN", "q-fin.EC", "stat.AP"], "primary_category": "cs.LG"}
{"title": "Speech Emotion Recognition System by Quaternion Nonlinear Echo State\n  Network", "abstract": "The echo state network (ESN) is a powerful and efficient tool for displaying\ndynamic data. However, many existing ESNs have limitations for properly\nmodeling high-dimensional data. The most important limitation of these networks\nis the high memory consumption due to their reservoir structure, which has\nprevented the increase of reservoir units and the maximum use of special\ncapabilities of this type of network. One way to solve this problem is to use\nquaternion algebra. Because quaternions have four different dimensions,\nhigh-dimensional data are easily represented and, using Hamilton\nmultiplication, with fewer parameters than real numbers, make external\nrelations between the multidimensional features easier. In addition to the\nmemory problem in the ESN network, the linear output of the ESN network poses\nan indescribable limit to its processing capacity, as it cannot effectively\nutilize higher-order statistics of features provided by the nonlinear dynamics\nof reservoir neurons. In this research, a new structure based on ESN is\npresented, in which quaternion algebra is used to compress the network data\nwith the simple split function, and the output linear combiner is replaced by a\nmultidimensional bilinear filter. This filter will be used for nonlinear\ncalculations of the output layer of the ESN. In addition, the two-dimensional\nprincipal component analysis technique is used to reduce the number of data\ntransferred to the bilinear filter. In this study, the coefficients and the\nweights of the quaternion nonlinear ESN (QNESN) are optimized using the genetic\nalgorithm. In order to prove the effectiveness of the proposed model compared\nto the previous methods, experiments for speech emotion recognition have been\nperformed on EMODB, SAVEE, and IEMOCAP speech emotional datasets. Comparisons\nshow that the proposed QNESN network performs better than the ESN and most\ncurrently SER systems.", "published": "2021-11-14 03:45:43", "link": "http://arxiv.org/abs/2111.07234v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binary classification of spoken words with passive phononic\n  metamaterials", "abstract": "Mitigating the energy requirements of artificial intelligence requires novel\nphysical substrates for computation. Phononic metamaterials have a vanishingly\nlow power dissipation and hence are a prime candidate for green, always-on\ncomputers. However, their use in machine learning applications has not been\nexplored due to the complexity of their design process: Current phononic\nmetamaterials are restricted to simple geometries (e.g. periodic, tapered), and\nhence do not possess sufficient expressivity to encode machine learning tasks.\nWe design and fabricate a non-periodic phononic metamaterial, directly from\ndata samples, that can distinguish between pairs of spoken words in the\npresence of a simple readout nonlinearity; hence demonstrating that phononic\nmetamaterials are a viable avenue towards zero-power smart devices.", "published": "2021-11-14 16:11:26", "link": "http://arxiv.org/abs/2111.08503v2", "categories": ["eess.SP", "cond-mat.dis-nn", "cs.ET", "cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "eess.SP"}
