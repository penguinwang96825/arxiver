{"title": "A Context-based Disambiguation Model for Sentiment Concepts Using a\n  Bag-of-concepts Approach", "abstract": "With the widespread dissemination of user-generated content on different\nsocial networks, and online consumer systems such as Amazon, the quantity of\nopinionated information available on the Internet has been increased. One of\nthe main tasks of the sentiment analysis is to detect polarity within a text.\nThe existing polarity detection methods mainly focus on keywords and their\nnaive frequency counts; however, they less regard the meanings and implicit\ndimensions of the natural concepts. Although background knowledge plays a\ncritical role in determining the polarity of concepts, it has been disregarded\nin polarity detection methods. This study presents a context-based model to\nsolve ambiguous polarity concepts using commonsense knowledge. First, a model\nis presented to generate a source of ambiguous sentiment concepts based on\nSenticNet by computing the probability distribution. Then the model uses a\nbag-of-concepts approach to remove ambiguities and semantic augmentation with\nthe ConceptNet handling to overcome lost knowledge. ConceptNet is a large-scale\nsemantic network with a large number of commonsense concepts. In this paper,\nthe point mutual information (PMI) measure is used to select the contextual\nconcepts having strong relationships with ambiguous concepts. The polarity of\nthe ambiguous concepts is precisely detected using positive/negative contextual\nconcepts and the relationship of the concepts in the semantic knowledge base.\nThe text representation scheme is semantically enriched using Numberbatch,\nwhich is a word embedding model based on the concepts from the ConceptNet\nsemantic network. The proposed model is evaluated by applying a corpus of\nproduct reviews, called Semeval. The experimental results revealed an accuracy\nrate of 82.07%, representing the effectiveness of the proposed model.", "published": "2020-08-07 07:16:40", "link": "http://arxiv.org/abs/2008.03020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Privacy Guarantees for De-identifying Text Transformations", "abstract": "Machine Learning approaches to Natural Language Processing tasks benefit from\na comprehensive collection of real-life user data. At the same time, there is a\nclear need for protecting the privacy of the users whose data is collected and\nprocessed. For text collections, such as, e.g., transcripts of voice\ninteractions or patient records, replacing sensitive parts with benign\nalternatives can provide de-identification. However, how much privacy is\nactually guaranteed by such text transformations, and are the resulting texts\nstill useful for machine learning? In this paper, we derive formal privacy\nguarantees for general text transformation-based de-identification methods on\nthe basis of Differential Privacy. We also measure the effect that different\nways of masking private information in dialog transcripts have on a subsequent\nmachine learning task. To this end, we formulate different masking strategies\nand compare their privacy-utility trade-offs. In particular, we compare a\nsimple redact approach with more sophisticated word-by-word replacement using\ndeep learning models on multiple natural language understanding tasks like\nnamed entity recognition, intent detection, and dialog act classification. We\nfind that only word-by-word replacement is robust against performance drops in\nvarious tasks.", "published": "2020-08-07 12:06:42", "link": "http://arxiv.org/abs/2008.03101v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IMS at SemEval-2020 Task 1: How low can you go? Dimensionality in\n  Lexical Semantic Change Detection", "abstract": "We present the results of our system for SemEval-2020 Task 1 that exploits a\ncommonly used lexical semantic change detection model based on Skip-Gram with\nNegative Sampling. Our system focuses on Vector Initialization (VI) alignment,\ncompares VI to the currently top-ranking models for Subtask 2 and demonstrates\nthat these can be outperformed if we optimize VI dimensionality. We demonstrate\nthat differences in performance can largely be attributed to model-specific\nsources of noise, and we reveal a strong relationship between dimensionality\nand frequency-induced noise in VI alignment. Our results suggest that lexical\nsemantic change models integrating vector space alignment should pay more\nattention to the role of the dimensionality parameter.", "published": "2020-08-07 13:16:28", "link": "http://arxiv.org/abs/2008.03164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quran Intelligent Ontology Construction Approach Using Association Rules\n  Mining", "abstract": "Ontology can be seen as a formal representation of knowledge. They have been\ninvestigated in many artificial intelligence studies including semantic web,\nsoftware engineering, and information retrieval. The aim of ontology is to\ndevelop knowledge representations that can be shared and reused. This research\nproject is concerned with the use of association rules to extract the Quran\nontology. The manual acquisition of ontologies from Quran verses can be very\ncostly; therefore, we need an intelligent system for Quran ontology\nconstruction using patternbased schemes and associations rules to discover\nQuran concepts and semantics relations from Quran verses. Our system is based\non the combination of statistics and linguistics methods to extract concepts\nand conceptual relations from Quran. In particular, a linguistic pattern-based\napproach is exploited to extract specific concepts from the Quran, while the\nconceptual relations are found based on association rules technique. The Quran\nontology will offer a new and powerful representation of Quran knowledge, and\nthe association rules will help to represent the relations between all classes\nof connected concepts in the Quran ontology.", "published": "2020-08-07 15:48:58", "link": "http://arxiv.org/abs/2008.03232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning a natural-language to LTL executable semantic parser for\n  grounded robotics", "abstract": "Children acquire their native language with apparent ease by observing how\nlanguage is used in context and attempting to use it themselves. They do so\nwithout laborious annotations, negative examples, or even direct corrections.\nWe take a step toward robots that can do the same by training a grounded\nsemantic parser, which discovers latent linguistic representations that can be\nused for the execution of natural-language commands. In particular, we focus on\nthe difficult domain of commands with a temporal aspect, whose semantics we\ncapture with Linear Temporal Logic, LTL. Our parser is trained with pairs of\nsentences and executions as well as an executor. At training time, the parser\nhypothesizes a meaning representation for the input as a formula in LTL. Three\ncompeting pressures allow the parser to discover meaning from language. First,\nany hypothesized meaning for a sentence must be permissive enough to reflect\nall the annotated execution trajectories. Second, the executor -- a pretrained\nend-to-end LTL planner -- must find that the observe trajectories are likely\nexecutions of the meaning. Finally, a generator, which reconstructs the\noriginal input, encourages the model to find representations that conserve\nknowledge about the command. Together these ensure that the meaning is neither\ntoo general nor too specific. Our model generalizes well, being able to parse\nand execute both machine-generated and human-generated commands, with\nnear-equal accuracy, despite the fact that the human-generated sentences are\nmuch more varied and complex with an open lexicon. The approach presented here\nis not specific to LTL: it can be applied to any domain where sentence meanings\ncan be hypothesized and an executor can verify these meanings, thus opening the\ndoor to many applications for robotic agents.", "published": "2020-08-07 17:28:32", "link": "http://arxiv.org/abs/2008.03277v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Retrofitting Vector Representations of Adverse Event Reporting Data to\n  Structured Knowledge to Improve Pharmacovigilance Signal Detection", "abstract": "Adverse drug events (ADE) are prevalent and costly. Clinical trials are\nconstrained in their ability to identify potential ADEs, motivating the\ndevelopment of spontaneous reporting systems for post-market surveillance.\nStatistical methods provide a convenient way to detect signals from these\nreports but have limitations in leveraging relationships between drugs and ADEs\ngiven their discrete count-based nature. A previously proposed method, aer2vec,\ngenerates distributed vector representations of ADE report entities that\ncapture patterns of similarity but cannot utilize lexical knowledge. We address\nthis limitation by retrofitting aer2vec drug embeddings to knowledge from\nRxNorm and developing a novel retrofitting variant using vector rescaling to\npreserve magnitude. When evaluated in the context of a pharmacovigilance signal\ndetection task, aer2vec with retrofitting consistently outperforms\ndisproportionality metrics when trained on minimally preprocessed data.\nRetrofitting with rescaling results in further improvements in the larger and\nmore challenging of two pharmacovigilance reference sets used for evaluation.", "published": "2020-08-07 19:11:51", "link": "http://arxiv.org/abs/2008.03340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Weighted Training Strategies for Grammatical Error Correction", "abstract": "Recent progress in the task of Grammatical Error Correction (GEC) has been\ndriven by addressing data sparsity, both through new methods for generating\nlarge and noisy pretraining data and through the publication of small and\nhigher-quality finetuning data in the BEA-2019 shared task. Building upon\nrecent work in Neural Machine Translation (NMT), we make use of both kinds of\ndata by deriving example-level scores on our large pretraining data based on a\nsmaller, higher-quality dataset. In this work, we perform an empirical study to\ndiscover how to best incorporate delta-log-perplexity, a type of example\nscoring, into a training schedule for GEC. In doing so, we perform experiments\nthat shed light on the function and applicability of delta-log-perplexity.\nModels trained on scored data achieve state-of-the-art results on common GEC\ntest sets.", "published": "2020-08-07 03:30:14", "link": "http://arxiv.org/abs/2008.02976v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Perception Score, A Learned Metric for Open-ended Text Generation\n  Evaluation", "abstract": "Automatic evaluation for open-ended natural language generation tasks remains\na challenge. Existing metrics such as BLEU show a low correlation with human\njudgment. We propose a novel and powerful learning-based evaluation metric:\nPerception Score. The method measures the overall quality of the generation and\nscores holistically instead of only focusing on one evaluation criteria, such\nas word overlapping. Moreover, it also shows the amount of uncertainty about\nits evaluation result. By connecting the uncertainty, Perception Score gives a\nmore accurate evaluation for the generation system. Perception Score provides\nstate-of-the-art results on two conditional generation tasks and two\nunconditional generation tasks.", "published": "2020-08-07 10:48:40", "link": "http://arxiv.org/abs/2008.03082v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SemEval-2020 Task 10: Emphasis Selection for Written Text in Visual\n  Media", "abstract": "In this paper, we present the main findings and compare the results of\nSemEval-2020 Task 10, Emphasis Selection for Written Text in Visual Media. The\ngoal of this shared task is to design automatic methods for emphasis selection,\ni.e. choosing candidates for emphasis in textual content to enable automated\ndesign assistance in authoring. The main focus is on short text instances for\nsocial media, with a variety of examples, from social media posts to\ninspirational quotes. Participants were asked to model emphasis using plain\ntext with no additional context from the user or other design considerations.\nSemEval-2020 Emphasis Selection shared task attracted 197 participants in the\nearly phase and a total of 31 teams made submissions to this task. The\nhighest-ranked submission achieved 0.823 Matchm score. The analysis of systems\nsubmitted to the task indicates that BERT and RoBERTa were the most common\nchoice of pre-trained models used, and part of speech tag (POS) was the most\nuseful feature. Full results can be found on the task's website.", "published": "2020-08-07 17:24:53", "link": "http://arxiv.org/abs/2008.03274v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diversifying Task-oriented Dialogue Response Generation with Prototype\n  Guided Paraphrasing", "abstract": "Existing methods for Dialogue Response Generation (DRG) in Task-oriented\nDialogue Systems (TDSs) can be grouped into two categories: template-based and\ncorpus-based. The former prepare a collection of response templates in advance\nand fill the slots with system actions to produce system responses at runtime.\nThe latter generate system responses token by token by taking system actions\ninto account. While template-based DRG provides high precision and highly\npredictable responses, they usually lack in terms of generating diverse and\nnatural responses when compared to (neural) corpus-based approaches.\nConversely, while corpus-based DRG methods are able to generate natural\nresponses, we cannot guarantee their precision or predictability. Moreover, the\ndiversity of responses produced by today's corpus-based DRG methods is still\nlimited. We propose to combine the merits of template-based and corpus-based\nDRGs by introducing a prototype-based, paraphrasing neural network, called\nP2-Net, which aims to enhance quality of the responses in terms of both\nprecision and diversity. Instead of generating a response from scratch, P2-Net\ngenerates system responses by paraphrasing template-based responses. To\nguarantee the precision of responses, P2-Net learns to separate a response into\nits semantics, context influence, and paraphrasing noise, and to keep the\nsemantics unchanged during paraphrasing. To introduce diversity, P2-Net\nrandomly samples previous conversational utterances as prototypes, from which\nthe model can then extract speaking style information. We conduct extensive\nexperiments on the MultiWOZ dataset with both automatic and human evaluations.\nThe results show that P2-Net achieves a significant improvement in diversity\nwhile preserving the semantics of responses.", "published": "2020-08-07 22:25:36", "link": "http://arxiv.org/abs/2008.03391v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Location-aware Graph Convolutional Networks for Video Question Answering", "abstract": "We addressed the challenging task of video question answering, which requires\nmachines to answer questions about videos in a natural language form. Previous\nstate-of-the-art methods attempt to apply spatio-temporal attention mechanism\non video frame features without explicitly modeling the location and relations\namong object interaction occurred in videos. However, the relations between\nobject interaction and their location information are very critical for both\naction recognition and question reasoning. In this work, we propose to\nrepresent the contents in the video as a location-aware graph by incorporating\nthe location information of an object into the graph construction. Here, each\nnode is associated with an object represented by its appearance and location\nfeatures. Based on the constructed graph, we propose to use graph convolution\nto infer both the category and temporal locations of an action. As the graph is\nbuilt on objects, our method is able to focus on the foreground action contents\nfor better video question answering. Lastly, we leverage an attention mechanism\nto combine the output of graph convolution and encoded question features for\nfinal answer reasoning. Extensive experiments demonstrate the effectiveness of\nthe proposed methods. Specifically, our method significantly outperforms\nstate-of-the-art methods on TGIF-QA, Youtube2Text-QA, and MSVD-QA datasets.\nCode and pre-trained models are publicly available at:\nhttps://github.com/SunDoge/L-GCN", "published": "2020-08-07 02:12:56", "link": "http://arxiv.org/abs/2008.09105v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deep Active Learning with Crowdsourcing Data for Privacy Policy\n  Classification", "abstract": "Privacy policies are statements that notify users of the services' data\npractices. However, few users are willing to read through policy texts due to\nthe length and complexity. While automated tools based on machine learning\nexist for privacy policy analysis, to achieve high classification accuracy,\nclassifiers need to be trained on a large labeled dataset. Most existing policy\ncorpora are labeled by skilled human annotators, requiring significant amount\nof labor hours and effort. In this paper, we leverage active learning and\ncrowdsourcing techniques to develop an automated classification tool named\nCalpric (Crowdsourcing Active Learning PRIvacy Policy Classifier), which is\nable to perform annotation equivalent to those done by skilled human annotators\nwith high accuracy while minimizing the labeling cost. Specifically, active\nlearning allows classifiers to proactively select the most informative segments\nto be labeled. On average, our model is able to achieve the same F1 score using\nonly 62% of the original labeling effort. Calpric's use of active learning also\naddresses naturally occurring class imbalance in unlabeled privacy policy\ndatasets as there are many more statements stating the collection of private\ninformation than stating the absence of collection. By selecting samples from\nthe minority class for labeling, Calpric automatically creates a more balanced\ntraining set.", "published": "2020-08-07 02:13:31", "link": "http://arxiv.org/abs/2008.02954v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Which Kind Is Better in Open-domain Multi-turn Dialog,Hierarchical or\n  Non-hierarchical Models? An Empirical Study", "abstract": "Currently, open-domain generative dialog systems have attracted considerable\nattention in academia and industry. Despite the success of single-turn dialog\ngeneration, multi-turn dialog generation is still a big challenge. So far,\nthere are two kinds of models for open-domain multi-turn dialog generation:\nhierarchical and non-hierarchical models. Recently, some works have shown that\nthe hierarchical models are better than non-hierarchical models under their\nexperimental settings; meanwhile, some works also demonstrate the opposite\nconclusion. Due to the lack of adequate comparisons, it's not clear which kind\nof models are better in open-domain multi-turn dialog generation. Thus, in this\npaper, we will measure systematically nearly all representative hierarchical\nand non-hierarchical models over the same experimental settings to check which\nkind is better. Through extensive experiments, we have the following three\nimportant conclusions: (1) Nearly all hierarchical models are worse than\nnon-hierarchical models in open-domain multi-turn dialog generation, except for\nthe HRAN model. Through further analysis, the excellent performance of HRAN\nmainly depends on its word-level attention mechanism; (2) The performance of\nother hierarchical models will also obtain a great improvement if integrating\nthe word-level attention mechanism into these models. The modified hierarchical\nmodels even significantly outperform the non-hierarchical models; (3) The\nreason why the word-level attention mechanism is so powerful for hierarchical\nmodels is because it can leverage context information more effectively,\nespecially the fine-grained information. Besides, we have implemented all of\nthe models and already released the codes.", "published": "2020-08-07 02:54:55", "link": "http://arxiv.org/abs/2008.02964v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Peking Opera Synthesis via Duration Informed Attention Network", "abstract": "Peking Opera has been the most dominant form of Chinese performing art since\naround 200 years ago. A Peking Opera singer usually exhibits a very strong\npersonal style via introducing improvisation and expressiveness on stage which\nleads the actual rhythm and pitch contour to deviate significantly from the\noriginal music score. This inconsistency poses a great challenge in Peking\nOpera singing voice synthesis from a music score. In this work, we propose to\ndeal with this issue and synthesize expressive Peking Opera singing from the\nmusic score based on the Duration Informed Attention Network (DurIAN)\nframework. To tackle the rhythm mismatch, Lagrange multiplier is used to find\nthe optimal output phoneme duration sequence with the constraint of the given\nnote duration from music score. As for the pitch contour mismatch, instead of\ndirectly inferring from music score, we adopt a pseudo music score generated\nfrom the real singing and feed it as input during training. The experiments\ndemonstrate that with the proposed system we can synthesize Peking Opera\nsinging voice with high-quality timbre, pitch and expressiveness.", "published": "2020-08-07 08:04:41", "link": "http://arxiv.org/abs/2008.03029v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pretraining Techniques for Sequence-to-Sequence Voice Conversion", "abstract": "Sequence-to-sequence (seq2seq) voice conversion (VC) models are attractive\nowing to their ability to convert prosody. Nonetheless, without sufficient\ndata, seq2seq VC models can suffer from unstable training and mispronunciation\nproblems in the converted speech, thus far from practical. To tackle these\nshortcomings, we propose to transfer knowledge from other speech processing\ntasks where large-scale corpora are easily available, typically text-to-speech\n(TTS) and automatic speech recognition (ASR). We argue that VC models\ninitialized with such pretrained ASR or TTS model parameters can generate\neffective hidden representations for high-fidelity, highly intelligible\nconverted speech. We apply such techniques to recurrent neural network\n(RNN)-based and Transformer based models, and through systematical experiments,\nwe demonstrate the effectiveness of the pretraining scheme and the superiority\nof Transformer based models over RNN-based models in terms of intelligibility,\nnaturalness, and similarity.", "published": "2020-08-07 11:02:07", "link": "http://arxiv.org/abs/2008.03088v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Convolutional Complex Knowledge Graph Embeddings", "abstract": "In this paper, we study the problem of learning continuous vector\nrepresentations of knowledge graphs for predicting missing links. We present a\nnew approach called ConEx, which infers missing links by leveraging the\ncomposition of a 2D convolution with a Hermitian inner product of\ncomplex-valued embedding vectors. We evaluate ConEx against state-of-the-art\napproaches on the WN18RR, FB15K-237, KINSHIP and UMLS benchmark datasets. Our\nexperimental results show that ConEx achieves a performance superior to that of\nstate-of-the-art approaches such as RotatE, QuatE and TuckER on the link\nprediction task on all datasets while requiring at least 8 times fewer\nparameters. We ensure the reproducibility of our results by providing an\nopen-source implementation which includes the training, evaluation scripts\nalong with pre-trained models at https://github.com/conex-kge/ConEx.", "published": "2020-08-07 12:49:01", "link": "http://arxiv.org/abs/2008.03130v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Applying Speech Tempo-Derived Features, BoAW and Fisher Vectors to\n  Detect Elderly Emotion and Speech in Surgical Masks", "abstract": "The 2020 INTERSPEECH Computational Paralinguistics Challenge (ComParE)\nconsists of three Sub-Challenges, where the tasks are to identify the level of\narousal and valence of elderly speakers, determine whether the actual speaker\nwearing a surgical mask, and estimate the actual breathing of the speaker. In\nour contribution to the Challenge, we focus on the Elderly Emotion and the Mask\nsub-challenges. Besides utilizing standard or close-to-standard features such\nas ComParE functionals, Bag-of-Audio-Words and Fisher vectors, we exploit that\nemotion is related to the velocity of speech (i.e. speech rate). To utilize\nthis, we perform phone-level recognition using an ASR system, and extract\nfeatures from the output such as articulation tempo, speech tempo, and various\nattributes measuring the amount of pauses. We also hypothesize that wearing a\nsurgical mask makes the speaker feel uneasy, leading to a slower speech rate\nand more hesitations; hence, we experiment with the same features in the Mask\nsub-challenge as well. Although this theory was not justified by the\nexperimental results on the Mask Sub-Challenge, in the Elderly Emotion\nSub-Challenge we got significantly improved arousal and valence values with\nthis feature type both on the development set and in cross-validation.", "published": "2020-08-07 13:42:25", "link": "http://arxiv.org/abs/2008.03183v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A New Approach to Accent Recognition and Conversion for Mandarin Chinese", "abstract": "Two new approaches to accent classification and conversion are presented and\nexplored, respectively. The first topic is Chinese accent\nclassification/recognition. The second topic is the use of encoder-decoder\nmodels for end-to-end Chinese accent conversion, where the classifier in the\nfirst topic is used for the training of the accent converter encoder-decoder\nmodel. Experiments using different features and model are performed for accent\nrecognition. These features include MFCCs and spectrograms. The classifier\nmodels were TDNN and 1D-CNN. On the MAGICDATA dataset with 5 classes of\naccents, the TDNN classifier trained on MFCC features achieved a test accuracy\nof 54% and a test F1 score of 0.54 while the 1D-CNN classifier trained on\nspectrograms achieve a test accuracy of 62% and a test F1 score of 0.62. A\nprototype of an end-to-end accent converter model is also presented. The\nconverter model comprises of an encoder and a decoder. The encoder model\nconverts an accented input into an accent-neutral form. The decoder model\nconverts an accent-neutral form to an accented form with the specified accent\nassigned by the input accent label. The converter prototype preserves the tone\nand foregoes the details in the output audio. An encoder-decoder structure\ndemonstrates the potential of being an effective accent converter. A proposal\nfor future improvements is also presented to address the issue of lost details\nin the decoder output.", "published": "2020-08-07 20:06:33", "link": "http://arxiv.org/abs/2008.03359v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DurIAN-SC: Duration Informed Attention Network based Singing Voice\n  Conversion System", "abstract": "Singing voice conversion is converting the timbre in the source singing to\nthe target speaker's voice while keeping singing content the same. However,\nsinging data for target speaker is much more difficult to collect compared with\nnormal speech data.In this paper, we introduce a singing voice conversion\nalgorithm that is capable of generating high quality target speaker's singing\nusing only his/her normal speech data. First, we manage to integrate the\ntraining and conversion process of speech and singing into one framework by\nunifying the features used in standard speech synthesis system and singing\nsynthesis system. In this way, normal speech data can also contribute to\nsinging voice conversion training, making the singing voice conversion system\nmore robust especially when the singing database is small.Moreover, in order to\nachieve one-shot singing voice conversion, a speaker embedding module is\ndeveloped using both speech and singing data, which provides target speaker\nidentify information during conversion. Experiments indicate proposed sing\nconversion system can convert source singing to target speaker's high-quality\nsinging with only 20 seconds of target speaker's enrollment speech data.", "published": "2020-08-07 06:41:13", "link": "http://arxiv.org/abs/2008.03009v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Disentangled speaker and nuisance attribute embedding for robust speaker\n  verification", "abstract": "Over the recent years, various deep learning-based embedding methods have\nbeen proposed and have shown impressive performance in speaker verification.\nHowever, as in most of the classical embedding techniques, the deep\nlearning-based methods are known to suffer from severe performance degradation\nwhen dealing with speech samples with different conditions (e.g., recording\ndevices, emotional states). In this paper, we propose a novel fully supervised\ntraining method for extracting a speaker embedding vector disentangled from the\nvariability caused by the nuisance attributes. The proposed framework was\ncompared with the conventional deep learning-based embedding methods using the\nRSR2015 and VoxCeleb1 dataset. Experimental results show that the proposed\napproach can extract speaker embeddings robust to channel and emotional\nvariability.", "published": "2020-08-07 07:31:21", "link": "http://arxiv.org/abs/2008.03024v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CUCHILD: A Large-Scale Cantonese Corpus of Child Speech for Phonology\n  and Articulation Assessment", "abstract": "This paper describes the design and development of CUCHILD, a large-scale\nCantonese corpus of child speech. The corpus contains spoken words collected\nfrom 1,986 child speakers aged from 3 to 6 years old. The speech materials\ninclude 130 words of 1 to 4 syllables in length. The speakers cover both\ntypically developing (TD) children and children with speech disorder. The\nintended use of the corpus is to support scientific and clinical research, as\nwell as technology development related to child speech assessment. The design\nof the corpus, including selection of words, participants recruitment, data\nacquisition process, and data pre-processing are described in detail. The\nresults of acoustical analysis are presented to illustrate the properties of\nchild speech. Potential applications of the corpus in automatic speech\nrecognition, phonological error detection and speaker diarization are also\ndiscussed.", "published": "2020-08-07 13:55:55", "link": "http://arxiv.org/abs/2008.03188v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Detection of Phonological Errors in Child Speech Using Siamese\n  Recurrent Autoencoder", "abstract": "Speech sound disorder (SSD) refers to the developmental disorder in which\nchildren encounter persistent difficulties in correctly pronouncing words.\nAssessment of SSD has been relying largely on trained speech and language\npathologists (SLPs). With the increasing demand for and long-lasting shortage\nof SLPs, automated assessment of speech disorder becomes a highly desirable\napproach to assisting clinical work. This paper describes a study on automatic\ndetection of phonological errors in Cantonese speech of kindergarten children,\nbased on a newly collected large speech corpus. The proposed approach to speech\nerror detection involves the use of a Siamese recurrent autoencoder, which is\ntrained to learn the similarity and discrepancy between phone segments in the\nembedding space. Training of the model requires only speech data from typically\ndeveloping (TD) children. To distinguish disordered speech from typical one,\ncosine distance between the embeddings of the test segment and the reference\nsegment is computed. Different model architectures and training strategies are\nexperimented. Results on detecting the 6 most common consonant errors\ndemonstrate satisfactory performance of the proposed model, with the average\nprecision value from 0.82 to 0.93.", "published": "2020-08-07 14:12:15", "link": "http://arxiv.org/abs/2008.03193v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Joint Framework for Audio Tagging and Weakly Supervised Acoustic Event\n  Detection Using DenseNet with Global Average Pooling", "abstract": "This paper proposes a network architecture mainly designed for audio tagging,\nwhich can also be used for weakly supervised acoustic event detection (AED).\nThe proposed network consists of a modified DenseNet as the feature extractor,\nand a global average pooling (GAP) layer to predict frame-level labels at\ninference time. This architecture is inspired by the work proposed by Zhou et\nal., a well-known framework using GAP to localize visual objects given\nimage-level labels. While most of the previous works on weakly supervised AED\nused recurrent layers with attention-based mechanism to localize acoustic\nevents, the proposed network directly localizes events using the feature map\nextracted by DenseNet without any recurrent layers. In the audio tagging task\nof DCASE 2017, our method significantly outperforms the state-of-the-art method\nin F1 score by 5.3% on the dev set, and 6.0% on the eval set in terms of\nabsolute values. For weakly supervised AED task in DCASE 2018, our model\noutperforms the state-of-the-art method in event-based F1 by 8.1% on the dev\nset, and 0.5% on the eval set in terms of absolute values, by using data\naugmentation and tri-training to leverage unlabeled data.", "published": "2020-08-07 19:46:33", "link": "http://arxiv.org/abs/2008.03350v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-speaker Text-to-speech Synthesis Using Deep Gaussian Processes", "abstract": "Multi-speaker speech synthesis is a technique for modeling multiple speakers'\nvoices with a single model. Although many approaches using deep neural networks\n(DNNs) have been proposed, DNNs are prone to overfitting when the amount of\ntraining data is limited. We propose a framework for multi-speaker speech\nsynthesis using deep Gaussian processes (DGPs); a DGP is a deep architecture of\nBayesian kernel regressions and thus robust to overfitting. In this framework,\nspeaker information is fed to duration/acoustic models using speaker codes. We\nalso examine the use of deep Gaussian process latent variable models (DGPLVMs).\nIn this approach, the representation of each speaker is learned simultaneously\nwith other model parameters, and therefore the similarity or dissimilarity of\nspeakers is considered efficiently. We experimentally evaluated two situations\nto investigate the effectiveness of the proposed methods. In one situation, the\namount of data from each speaker is balanced (speaker-balanced), and in the\nother, the data from certain speakers are limited (speaker-imbalanced).\nSubjective and objective evaluation results showed that both the DGP and DGPLVM\nsynthesize multi-speaker speech more effective than a DNN in the\nspeaker-balanced situation. We also found that the DGPLVM outperforms the DGP\nsignificantly in the speaker-imbalanced situation.", "published": "2020-08-07 02:03:27", "link": "http://arxiv.org/abs/2008.02950v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Incremental Text to Speech for Neural Sequence-to-Sequence Models using\n  Reinforcement Learning", "abstract": "Modern approaches to text to speech require the entire input character\nsequence to be processed before any audio is synthesised. This latency limits\nthe suitability of such models for time-sensitive tasks like simultaneous\ninterpretation. Interleaving the action of reading a character with that of\nsynthesising audio reduces this latency. However, the order of this sequence of\ninterleaved actions varies across sentences, which raises the question of how\nthe actions should be chosen. We propose a reinforcement learning based\nframework to train an agent to make this decision. We compare our performance\nagainst that of deterministic, rule-based systems. Our results demonstrate that\nour agent successfully balances the trade-off between the latency of audio\ngeneration and the quality of synthesised audio. More broadly, we show that\nneural sequence-to-sequence models can be adapted to run in an incremental\nmanner.", "published": "2020-08-07 11:48:05", "link": "http://arxiv.org/abs/2008.03096v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Machine of Few Words -- Interactive Speaker Recognition with\n  Reinforcement Learning", "abstract": "Speaker recognition is a well known and studied task in the speech processing\ndomain. It has many applications, either for security or speaker adaptation of\npersonal devices. In this paper, we present a new paradigm for automatic\nspeaker recognition that we call Interactive Speaker Recognition (ISR). In this\nparadigm, the recognition system aims to incrementally build a representation\nof the speakers by requesting personalized utterances to be spoken in contrast\nto the standard text-dependent or text-independent schemes. To do so, we cast\nthe speaker recognition task into a sequential decision-making problem that we\nsolve with Reinforcement Learning. Using a standard dataset, we show that our\nmethod achieves excellent performance while using little speech signal amounts.\nThis method could also be applied as an utterance selection mechanism for\nbuilding speech synthesis systems.", "published": "2020-08-07 12:44:08", "link": "http://arxiv.org/abs/2008.03127v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigation of Speaker-adaptation methods in Transformer based ASR", "abstract": "End-to-end models are fast replacing the conventional hybrid models in\nautomatic speech recognition. Transformer, a sequence-to-sequence model, based\non self-attention popularly used in machine translation tasks, has given\npromising results when used for automatic speech recognition. This paper\nexplores different ways of incorporating speaker information at the encoder\ninput while training a transformer-based model to improve its speech\nrecognition performance. We present speaker information in the form of speaker\nembeddings for each of the speakers. We experiment using two types of speaker\nembeddings: x-vectors and novel s-vectors proposed in our previous work. We\nreport results on two datasets a) NPTEL lecture database and b) Librispeech\n500-hour split. NPTEL is an open-source e-learning portal providing lectures\nfrom top Indian universities. We obtain improvements in the word error rate\nover the baseline through our approach of integrating speaker embeddings into\nthe model.", "published": "2020-08-07 16:09:03", "link": "http://arxiv.org/abs/2008.03247v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning Based Dereverberation of Temporal Envelopesfor Robust\n  Speech Recognition", "abstract": "Automatic speech recognition in reverberant conditions is a challenging task\nas the long-term envelopes of the reverberant speech are temporally smeared. In\nthis paper, we propose a neural model for enhancement of sub-band temporal\nenvelopes for dereverberation of speech. The temporal envelopes are derived\nusing the autoregressive modeling framework of frequency domain linear\nprediction (FDLP). The neural enhancement model proposed in this paper performs\nan envelop gain based enhancement of temporal envelopes and it consists of a\nseries of convolutional and recurrent neural network layers. The enhanced\nsub-band envelopes are used to generate features for automatic speech\nrecognition (ASR). The ASR experiments are performed on the REVERB challenge\ndataset as well as the CHiME-3 dataset. In these experiments, the proposed\nneural enhancement approach provides significant improvements over a baseline\nASR system with beamformed audio (average relative improvements of 21% on the\ndevelopment set and about 11% on the evaluation set in word error rates for\nREVERB challenge dataset).", "published": "2020-08-07 19:05:18", "link": "http://arxiv.org/abs/2008.03339v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Classification of Huntington Disease using Acoustic and Lexical Features", "abstract": "Speech is a critical biomarker for Huntington Disease (HD), with changes in\nspeech increasing in severity as the disease progresses. Speech analyses are\ncurrently conducted using either transcriptions created manually by trained\nprofessionals or using global rating scales. Manual transcription is both\nexpensive and time-consuming and global rating scales may lack sufficient\nsensitivity and fidelity. Ultimately, what is needed is an unobtrusive measure\nthat can cheaply and continuously track disease progression. We present first\nsteps towards the development of such a system, demonstrating the ability to\nautomatically differentiate between healthy controls and individuals with HD\nusing speech cues. The results provide evidence that objective analyses can be\nused to support clinical diagnoses, moving towards the tracking of\nsymptomatology outside of laboratory and clinical environments.", "published": "2020-08-07 20:32:01", "link": "http://arxiv.org/abs/2008.03367v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Controllable Neural Prosody Synthesis", "abstract": "Speech synthesis has recently seen significant improvements in fidelity,\ndriven by the advent of neural vocoders and neural prosody generators. However,\nthese systems lack intuitive user controls over prosody, making them unable to\nrectify prosody errors (e.g., misplaced emphases and contextually inappropriate\nemotions) or generate prosodies with diverse speaker excitement levels and\nemotions. We address these limitations with a user-controllable, context-aware\nneural prosody generator. Given a real or synthesized speech recording, our\nmodel allows a user to input prosody constraints for certain time frames and\ngenerates the remaining time frames from input text and contextual prosody. We\nalso propose a pitch-shifting neural vocoder to modify input speech to match\nthe synthesized prosody. Through objective and subjective evaluations we show\nthat we can successfully incorporate user control into our prosody generation\nmodel without sacrificing the overall naturalness of the synthesized speech.", "published": "2020-08-07 22:11:58", "link": "http://arxiv.org/abs/2008.03388v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
