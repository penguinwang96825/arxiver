{"title": "Improving Large Language Models for Clinical Named Entity Recognition\n  via Prompt Engineering", "abstract": "Objective: This study quantifies the capabilities of GPT-3.5 and GPT-4 for\nclinical named entity recognition (NER) tasks and proposes task-specific\nprompts to improve their performance. Materials and Methods: We evaluated these\nmodels on two clinical NER tasks: (1) to extract medical problems, treatments,\nand tests from clinical notes in the MTSamples corpus, following the 2010 i2b2\nconcept extraction shared task, and (2) identifying nervous system\ndisorder-related adverse events from safety reports in the vaccine adverse\nevent reporting system (VAERS). To improve the GPT models' performance, we\ndeveloped a clinical task-specific prompt framework that includes (1) baseline\nprompts with task description and format specification, (2) annotation\nguideline-based prompts, (3) error analysis-based instructions, and (4)\nannotated samples for few-shot learning. We assessed each prompt's\neffectiveness and compared the models to BioClinicalBERT. Results: Using\nbaseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804\nfor MTSamples, and 0.301, 0.593 for VAERS. Additional prompt components\nconsistently improved model performance. When all four components were used,\nGPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and\n0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt\nframework. Although these results trail BioClinicalBERT (F1 of 0.901 for the\nMTSamples dataset and 0.802 for the VAERS), it is very promising considering\nfew training samples are needed. Conclusion: While direct application of GPT\nmodels to clinical NER tasks falls short of optimal performance, our\ntask-specific prompt framework, incorporating medical knowledge and training\nsamples, significantly enhances GPT models' feasibility for potential clinical\napplications.", "published": "2023-03-29 02:46:18", "link": "http://arxiv.org/abs/2303.16416v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of\n  Commonsense Problem in Large Language Models", "abstract": "Large language models (LLMs) have made significant progress in NLP. However,\ntheir ability to memorize, represent, and leverage commonsense knowledge has\nbeen a well-known pain point. In this paper, we specifically focus on ChatGPT,\na widely used and easily accessible LLM, and ask the following questions: (1)\nCan ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of\nthe underlying commonsense knowledge for answering a specific question? (3) Is\nChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage\ncommonsense for answering questions? We conduct a series of experiments on 11\ndatasets to evaluate ChatGPT's commonsense abilities, including answering\ncommonsense questions, identifying necessary knowledge, generating knowledge\ndescriptions, and using knowledge descriptions to answer questions again.\nExperimental results show that: (1) ChatGPT can achieve good QA accuracies in\ncommonsense tasks, while still struggling with certain domains of datasets. (2)\nChatGPT is knowledgeable, and can accurately generate most of the commonsense\nknowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an\ninexperienced commonsense problem solver, which cannot precisely identify the\nneeded commonsense for answering a specific question. These findings raise the\nneed to explore improved mechanisms for effectively incorporating commonsense\ninto LLMs like ChatGPT, such as better instruction following and commonsense\nguidance.", "published": "2023-03-29 03:05:43", "link": "http://arxiv.org/abs/2303.16421v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Larger Probes Tell a Different Story: Extending Psycholinguistic\n  Datasets Via In-Context Learning", "abstract": "Language model probing is often used to test specific capabilities of models.\nHowever, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing. The\ndatasets and code are available on\n$\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.", "published": "2023-03-29 04:00:53", "link": "http://arxiv.org/abs/2303.16445v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMExplainer: Grounding Knowledge and Explaining Language Models", "abstract": "Language models (LMs) like GPT-4 are important in AI applications, but their\nopaque decision-making process reduces user trust, especially in\nsafety-critical areas. We introduce LMExplainer, a novel knowledge-grounded\nexplainer that clarifies the reasoning process of LMs through intuitive,\nhuman-understandable explanations. By leveraging a graph attention network\n(GAT) with a large-scale knowledge graph (KG), LMExplainer not only precisely\nnarrows the reasoning space to focus on the most relevant knowledge but also\ngrounds its reasoning in structured, verifiable knowledge to reduce\nhallucinations and enhance interpretability. LMExplainer effectively generates\nhuman-understandable explanations to enhance transparency and streamline the\ndecision-making process. Additionally, by incorporating debugging into the\nexplanation, it offers expertise suggestions that improve LMs from a\ndevelopmental perspective. Thus, LMExplainer stands as an enhancement in making\nLMs more accessible and understandable to users. We evaluate LMExplainer on\nbenchmark datasets such as CommonsenseQA and OpenBookQA, demonstrating that it\noutperforms most existing methods. By comparing the explanations generated by\nLMExplainer with those of other models, we show that our approach offers more\ncomprehensive and clearer explanations of the reasoning process. LMExplainer\nprovides a deeper understanding of the inner workings of LMs, advancing towards\nmore reliable, transparent, and equitable AI.", "published": "2023-03-29 08:59:44", "link": "http://arxiv.org/abs/2303.16537v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarizing Indian Languages using Multilingual Transformers based\n  Models", "abstract": "With the advent of multilingual models like mBART, mT5, IndicBART etc.,\nsummarization in low resource Indian languages is getting a lot of attention\nnow a days. But still the number of datasets is low in number. In this work, we\n(Team HakunaMatata) study how these multilingual models perform on the datasets\nwhich have Indian languages as source and target text while performing\nsummarization. We experimented with IndicBART and mT5 models to perform the\nexperiments and report the ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-4 scores as a\nperformance metric.", "published": "2023-03-29 13:05:17", "link": "http://arxiv.org/abs/2303.16657v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text revision in Scientific Writing Assistance: An Overview", "abstract": "Writing a scientific article is a challenging task as it is a highly codified\ngenre. Good writing skills are essential to properly convey ideas and results\nof research work. Since the majority of scientific articles are currently\nwritten in English, this exercise is all the more difficult for non-native\nEnglish speakers as they additionally have to face language issues. This\narticle aims to provide an overview of text revision in writing assistance in\nthe scientific domain.\n  We will examine the specificities of scientific writing, including the format\nand conventions commonly used in research articles.\n  Additionally, this overview will explore the various types of writing\nassistance tools available for text revision. Despite the evolution of the\ntechnology behind these tools through the years, from rule-based approaches to\ndeep neural-based ones, challenges still exist (tools' accessibility, limited\nconsideration of the context, inexplicit use of discursive information, etc.)", "published": "2023-03-29 14:25:30", "link": "http://arxiv.org/abs/2303.16726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating NLG systems: A brief introduction", "abstract": "This year the International Conference on Natural Language Generation (INLG)\nwill feature an award for the paper with the best evaluation. The purpose of\nthis award is to provide an incentive for NLG researchers to pay more attention\nto the way they assess the output of their systems. This essay provides a short\nintroduction to evaluation in NLG, explaining key terms and distinctions.", "published": "2023-03-29 14:49:29", "link": "http://arxiv.org/abs/2303.16742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced\n  Annotators", "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train\nmachine learning models with high performance. However, data annotation is\ntime-consuming and expensive, especially when the task involves a large amount\nof data or requires specialized domains. Recently, GPT-3.5 series models have\ndemonstrated remarkable few-shot and zero-shot ability across various NLP\ntasks. In this paper, we first claim that large language models (LLMs), such as\nGPT-3.5, can serve as an excellent crowdsourced annotator when provided with\nsufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM,\nan annotation system powered by LLMs, which adopts a two-step approach,\nexplain-then-annotate. Concretely, we first prompt LLMs to provide explanations\nfor why the specific ground truth answer/label was assigned for a given\nexample. Then, we construct the few-shot chain-of-thought prompt with the\nself-generated explanation and employ it to annotate the unlabeled data with\nLLMs. Our experiment results on three tasks, including user input and keyword\nrelevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses or\nperforms on par with crowdsourced annotators. Furthermore, we build the first\nconversation-based information retrieval dataset employing AnnoLLM. This\ndataset is designed to facilitate the development of retrieval models capable\nof retrieving pertinent documents for conversational text. Human evaluation has\nvalidated the dataset's high quality.", "published": "2023-03-29 17:03:21", "link": "http://arxiv.org/abs/2303.16854v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Did You Mean...? Confidence-based Trade-offs in Semantic Parsing", "abstract": "We illustrate how a calibrated model can help balance common trade-offs in\ntask-oriented parsing. In a simulated annotator-in-the-loop experiment, we show\nthat well-calibrated confidence scores allow us to balance cost with annotator\nload, improving accuracy with a small number of interactions. We then examine\nhow confidence scores can help optimize the trade-off between usability and\nsafety. We show that confidence-based thresholding can substantially reduce the\nnumber of incorrect low-confidence programs executed; however, this comes at a\ncost to usability. We propose the DidYouMean system which better balances\nusability and safety.", "published": "2023-03-29 17:07:26", "link": "http://arxiv.org/abs/2303.16857v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End $n$-ary Relation Extraction for Combination Drug Therapies", "abstract": "Combination drug therapies are treatment regimens that involve two or more\ndrugs, administered more commonly for patients with cancer, HIV, malaria, or\ntuberculosis. Currently there are over 350K articles in PubMed that use the\n\"combination drug therapy\" MeSH heading with at least 10K articles published\nper year over the past two decades. Extracting combination therapies from\nscientific literature inherently constitutes an $n$-ary relation extraction\nproblem. Unlike in the general $n$-ary setting where $n$ is fixed (e.g.,\ndrug-gene-mutation relations where $n=3$), extracting combination therapies is\na special setting where $n \\geq 2$ is dynamic, depending on each instance.\nRecently, Tiktinsky et al. (NAACL 2022) introduced a first of its kind dataset,\nCombDrugExt, for extracting such therapies from literature. Here, we use a\nsequence-to-sequence style end-to-end extraction method to achieve an F1-Score\nof $66.7\\%$ on the CombDrugExt test set for positive (or effective)\ncombinations. This is an absolute $\\approx 5\\%$ F1-score improvement even over\nthe prior best relation classification score with spotted drug entities (hence,\nnot end-to-end). Thus our effort introduces a state-of-the-art first model for\nend-to-end extraction that is already superior to the best prior non end-to-end\nmodel for this task. Our model seamlessly extracts all drug entities and\nrelations in a single pass and is highly suitable for dynamic $n$-ary\nextraction scenarios.", "published": "2023-03-29 17:55:50", "link": "http://arxiv.org/abs/2303.16886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BEVERS: A General, Simple, and Performant Framework for Automatic Fact\n  Verification", "abstract": "Automatic fact verification has become an increasingly popular topic in\nrecent years and among datasets the Fact Extraction and VERification (FEVER)\ndataset is one of the most popular. In this work we present BEVERS, a tuned\nbaseline system for the FEVER dataset. Our pipeline uses standard approaches\nfor document retrieval, sentence selection, and final claim classification,\nhowever, we spend considerable effort ensuring optimal performance for each\ncomponent. The results are that BEVERS achieves the highest FEVER score and\nlabel accuracy among all systems, published or unpublished. We also apply this\npipeline to another fact verification dataset, Scifact, and achieve the highest\nlabel accuracy among all systems on that dataset as well. We also make our full\ncode available.", "published": "2023-03-29 19:16:19", "link": "http://arxiv.org/abs/2303.16974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ContraSim -- Analyzing Neural Representations Based on Contrastive\n  Learning", "abstract": "Recent work has compared neural network representations via similarity-based\nanalyses to improve model interpretation. The quality of a similarity measure\nis typically evaluated by its success in assigning a high score to\nrepresentations that are expected to be matched. However, existing similarity\nmeasures perform mediocrely on standard benchmarks. In this work, we develop a\nnew similarity measure, dubbed ContraSim, based on contrastive learning. In\ncontrast to common closed-form similarity measures, ContraSim learns a\nparameterized measure by using both similar and dissimilar examples. We perform\nan extensive experimental evaluation of our method, with both language and\nvision models, on the standard layer prediction benchmark and two new\nbenchmarks that we introduce: the multilingual benchmark and the image-caption\nbenchmark. In all cases, ContraSim achieves much higher accuracy than previous\nsimilarity measures, even when presented with challenging examples. Finally,\nContraSim is more suitable for the analysis of neural networks, revealing new\ninsights not captured by previous measures.", "published": "2023-03-29 19:43:26", "link": "http://arxiv.org/abs/2303.16992v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How do decoding algorithms distribute information in dialogue responses?", "abstract": "Humans tend to follow the Uniform Information Density (UID) principle by\ndistributing information evenly in utterances. We study if decoding algorithms\nimplicitly follow this UID principle, and under what conditions adherence to\nUID might be desirable for dialogue generation. We generate responses using\ndifferent decoding algorithms with GPT-2 on the Persona-Chat dataset and\ncollect human judgments on their quality using Amazon Mechanical Turk. We find\nthat (i) surprisingly, model-generated responses follow the UID principle to a\ngreater extent than human responses, and (ii) decoding algorithms that promote\nUID do not generate higher-quality responses. Instead, when we control for\nsurprisal, non-uniformity of information density correlates with the quality of\nresponses with very low/high surprisal. Our findings indicate that encouraging\nnon-uniform responses is a potential solution to the ``likelihood trap''\nproblem (quality degradation in very high-likelihood text). Our dataset\ncontaining multiple candidate responses per dialog history along with\nhuman-annotated quality ratings is available at\nhttps://huggingface.co/datasets/saranya132/dialog_uid_gpt2.", "published": "2023-03-29 20:21:45", "link": "http://arxiv.org/abs/2303.17006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with\n  Millions of APIs", "abstract": "Artificial Intelligence (AI) has made incredible progress recently. On the\none hand, advanced foundation models like ChatGPT can offer powerful\nconversation, in-context learning and code generation abilities on a broad\nrange of open-domain tasks. They can also generate high-level solution outlines\nfor domain-specific tasks based on the common sense knowledge they have\nacquired. However, they still face difficulties with some specialized tasks\nbecause they lack enough domain-specific data during pre-training or they often\nhave errors in their neural network computations on those tasks that need\naccurate executions. On the other hand, there are also many existing models and\nsystems (symbolic-based or neural-based) that can do some domain-specific tasks\nvery well. However, due to the different implementation or working mechanisms,\nthey are not easily accessible or compatible with foundation models. Therefore,\nthere is a clear and pressing need for a mechanism that can leverage foundation\nmodels to propose task solution outlines and then automatically match some of\nthe sub-tasks in the outlines to the off-the-shelf models and systems with\nspecial functionalities to complete them. Inspired by this, we introduce\nTaskMatrix.AI as a new AI ecosystem that connects foundation models with\nmillions of APIs for task completion. Unlike most previous work that aimed to\nimprove a single AI model, TaskMatrix.AI focuses more on using existing\nfoundation models (as a brain-like central system) and APIs of other AI models\nand systems (as sub-task solvers) to achieve diversified tasks in both digital\nand physical domains. As a position paper, we will present our vision of how to\nbuild such an ecosystem, explain each key component, and use study cases to\nillustrate both the feasibility of this vision and the main challenges we need\nto address next.", "published": "2023-03-29 03:30:38", "link": "http://arxiv.org/abs/2303.16434v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AraSpot: Arabic Spoken Command Spotting", "abstract": "Spoken keyword spotting (KWS) is the task of identifying a keyword in an\naudio stream and is widely used in smart devices at the edge in order to\nactivate voice assistants and perform hands-free tasks. The task is daunting as\nthere is a need, on the one hand, to achieve high accuracy while at the same\ntime ensuring that such systems continue to run efficiently on low power and\npossibly limited computational capabilities devices. This work presents AraSpot\nfor Arabic keyword spotting trained on 40 Arabic keywords, using different\nonline data augmentation, and introducing ConformerGRU model architecture.\nFinally, we further improve the performance of the model by training a\ntext-to-speech model for synthetic data generation. AraSpot achieved a\nState-of-the-Art SOTA 99.59% result outperforming previous approaches.", "published": "2023-03-29 12:22:17", "link": "http://arxiv.org/abs/2303.16621v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "abstract": "The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts. The code is at https://github.com/nlpyang/geval", "published": "2023-03-29 12:46:54", "link": "http://arxiv.org/abs/2303.16634v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Semantic Similarity and Text Embedding to Measure the Social Media\n  Echo of Strategic Communications", "abstract": "Online discourse covers a wide range of topics and many actors tailor their\ncontent to impact online discussions through carefully crafted messages and\ntargeted campaigns. Yet the scale and diversity of online media content make it\ndifficult to evaluate the impact of a particular message. In this paper, we\npresent a new technique that leverages semantic similarity to quantify the\nchange in the discussion after a particular message has been published. We use\na set of press releases from environmental organisations and tweets from the\nclimate change debate to show that our novel approach reveals a heavy-tailed\ndistribution of response in online discourse to strategic communications.", "published": "2023-03-29 13:46:07", "link": "http://arxiv.org/abs/2303.16694v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Adapting to the Low-Resource Double-Bind: Investigating Low-Compute\n  Methods on Low-Resource African Languages", "abstract": "Many natural language processing (NLP) tasks make use of massively\npre-trained language models, which are computationally expensive. However,\naccess to high computational resources added to the issue of data scarcity of\nAfrican languages constitutes a real barrier to research experiments on these\nlanguages. In this work, we explore the applicability of low-compute approaches\nsuch as language adapters in the context of this low-resource double-bind. We\nintend to answer the following question: do language adapters allow those who\nare doubly bound by data and compute to practically build useful models?\nThrough fine-tuning experiments on African languages, we evaluate their\neffectiveness as cost-effective approaches to low-resource African NLP. Using\nsolely free compute resources, our results show that language adapters achieve\ncomparable performances to massive pre-trained language models which are heavy\non computational resources. This opens the door to further experimentation and\nexploration on full-extent of language adapters capacities.", "published": "2023-03-29 19:25:43", "link": "http://arxiv.org/abs/2303.16985v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A semi-automatic method for document classification in the shipping\n  industry", "abstract": "In the shipping industry, document classification plays a crucial role in\nensuring that the necessary documents are properly identified and processed for\ncustoms clearance. OCR technology is being used to automate the process of\ndocument classification, which involves identifying important documents such as\nCommercial Invoices, Packing Lists, Export/Import Customs Declarations, Bills\nof Lading, Sea Waybills, Certificates, Air or Rail Waybills, Arrival Notices,\nCertificate of Origin, Importer Security Filings, and Letters of Credit. By\nusing OCR technology, the shipping industry can improve accuracy and efficiency\nin document classification and streamline the customs clearance process. The\naim of this study is to build a robust document classification system based on\nkeyword frequencies. The research is carried out by analyzing Contract-Breach\nlaw documents available with IN-D. The documents were collected by scraping the\nSingapore Government Judiciary website. The database developed has 250\nContract-Breach documents. These documents are splitted to generate 200\ntraining documents and 50 test documents. A semi-automatic approach is used to\nselect keyword vectors for document classification. The accuracy of the\nreported model is 92.00 %.", "published": "2023-03-29 10:00:43", "link": "http://arxiv.org/abs/2305.06148v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Video-Moment Retrieval and Step-Captioning", "abstract": "There is growing interest in searching for information from large video\ncorpora. Prior works have studied relevant tasks, such as text-based video\nretrieval, moment retrieval, video summarization, and video captioning in\nisolation, without an end-to-end setup that can jointly search from video\ncorpora and generate summaries. Such an end-to-end setup would allow for many\ninteresting applications, e.g., a text-based search that finds a relevant video\nfrom a video corpus, extracts the most relevant moment from that video, and\nsegments the moment into important steps with captions. To address this, we\npresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset and\npropose a new benchmark that covers hierarchical information retrieval and\nvisual/textual stepwise summarization from an instructional video corpus.\nHiREST consists of 3.4K text-video pairs from an instructional video dataset,\nwhere 1.1K videos have annotations of moment spans relevant to text query and\nbreakdown of each moment into key instruction steps with caption and timestamps\n(totaling 8.6K step captions). Our hierarchical benchmark consists of video\nretrieval, moment retrieval, and two novel moment segmentation and step\ncaptioning tasks. In moment segmentation, models break down a video moment into\ninstruction steps and identify start-end boundaries. In step captioning, models\ngenerate a textual summary for each step. We also present starting point\ntask-specific and end-to-end joint baseline models for our new benchmark. While\nthe baseline models show some promising results, there still exists large room\nfor future improvement by the community. Project website:\nhttps://hirest-cvpr2023.github.io", "published": "2023-03-29 02:33:54", "link": "http://arxiv.org/abs/2303.16406v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Building a Knowledge Graph of Distributed Ledger Technologies", "abstract": "Distributed ledger systems have become more prominent and successful in\nrecent years, with a focus on blockchains and cryptocurrency. This has led to\nvarious misunderstandings about both the technology itself and its\ncapabilities, as in many cases blockchain and cryptocurrency is used\nsynonymously and other applications are often overlooked. Therefore, as a\nwhole, the view of distributed ledger technology beyond blockchains and\ncryptocurrencies is very limited. Existing vocabularies and ontologies often\nfocus on single aspects of the technology, or in some cases even just on one\nproduct. This potentially leads to other types of distributed ledgers and their\npossible use cases being neglected. In this paper, we present a knowledge graph\nand an ontology for distributed ledger technologies, which includes security\nconsiderations to model aspects such as threats and vulnerabilities,\napplication domains, as well as relevant standards and regulations. Such a\nknowledge graph improves the overall understanding of distributed ledgers,\nreveals their strengths, and supports the work of security personnel, i.e.\nanalysts and system architects. We discuss potential uses and follow semantic\nweb best practices to evaluate and publish the ontology and knowledge graph.", "published": "2023-03-29 08:34:01", "link": "http://arxiv.org/abs/2303.16528v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Reference-less Analysis of Context Specificity in Translation with\n  Personalised Language Models", "abstract": "Sensitising language models (LMs) to external context helps them to more\neffectively capture the speaking patterns of individuals with specific\ncharacteristics or in particular environments. This work investigates to what\nextent rich character and film annotations can be leveraged to personalise LMs\nin a scalable manner. We then explore the use of such models in evaluating\ncontext specificity in machine translation. We build LMs which leverage rich\ncontextual information to reduce perplexity by up to 6.5% compared to a\nnon-contextual model, and generalise well to a scenario with no\nspeaker-specific data, relying on combinations of demographic characteristics\nexpressed via metadata. Our findings are consistent across two corpora, one of\nwhich (Cornell-rich) is also a contribution of this paper. We then use our\npersonalised LMs to measure the co-occurrence of extra-textual context and\ntranslation hypotheses in a machine translation setting. Our results suggest\nthat the degree to which professional translations in our domain are\ncontext-specific can be preserved to a better extent by a contextual machine\ntranslation model than a non-contextual model, which is also reflected in the\ncontextual model's superior reference-based scores.", "published": "2023-03-29 12:19:23", "link": "http://arxiv.org/abs/2303.16618v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-shot Entailment of Leaderboards for Empirical AI Research", "abstract": "We present a large-scale empirical investigation of the zero-shot learning\nphenomena in a specific recognizing textual entailment (RTE) task category,\ni.e. the automated mining of leaderboards for Empirical AI Research. The prior\nreported state-of-the-art models for leaderboards extraction formulated as an\nRTE task, in a non-zero-shot setting, are promising with above 90% reported\nperformances. However, a central research question remains unexamined: did the\nmodels actually learn entailment? Thus, for the experiments in this paper, two\nprior reported state-of-the-art models are tested out-of-the-box for their\nability to generalize or their capacity for entailment, given leaderboard\nlabels that were unseen during training. We hypothesize that if the models\nlearned entailment, their zero-shot performances can be expected to be\nmoderately high as well--perhaps, concretely, better than chance. As a result\nof this work, a zero-shot labeled dataset is created via distant labeling\nformulating the leaderboard extraction RTE task.", "published": "2023-03-29 16:28:43", "link": "http://arxiv.org/abs/2303.16835v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks", "abstract": "The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, we observe that the two most popular\nmultimodal tasks, the generative and contrastive tasks, are nontrivial to\naccommodate in one architecture, and further need adaptations for downstream\ntasks. We propose a novel paradigm of training with a decoder-only model for\nmultimodal tasks, which is surprisingly effective in jointly learning of these\ndisparate vision-language tasks. This is done with a simple model, called\nMaMMUT. It consists of a single vision encoder and a text decoder, and is able\nto accommodate contrastive and generative learning by a novel two-pass approach\non the text decoder. We demonstrate that joint learning of these diverse\nobjectives is simple, effective, and maximizes the weight-sharing of the model\nacross these tasks. Furthermore, the same architecture enables straightforward\nextensions to open-vocabulary object detection and video-language tasks. The\nmodel tackles a diverse range of tasks, while being modest in capacity. Our\nmodel achieves the state of the art on image-text and text-image retrieval,\nvideo question answering and open-vocabulary detection tasks, outperforming\nmuch larger and more extensively trained foundational models. It shows very\ncompetitive results on VQA and Video Captioning, especially considering its\ncapacity. Ablations confirm the flexibility and advantages of our approach.", "published": "2023-03-29 16:42:30", "link": "http://arxiv.org/abs/2303.16839v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with\n  GPT and Prototype Guidance", "abstract": "Understanding 3D scenes from multi-view inputs has been proven to alleviate\nthe view discrepancy issue in 3D visual grounding. However, existing methods\nnormally neglect the view cues embedded in the text modality and fail to weigh\nthe relative importance of different views. In this paper, we propose\nViewRefer, a multi-view framework for 3D visual grounding exploring how to\ngrasp the view knowledge from both text and 3D modalities. For the text branch,\nViewRefer leverages the diverse linguistic knowledge of large-scale language\nmodels, e.g., GPT, to expand a single grounding text to multiple\ngeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer\nfusion module with inter-view attention is introduced to boost the interaction\nof objects across views. On top of that, we further present a set of learnable\nmulti-view prototypes, which memorize scene-agnostic knowledge for different\nviews, and enhance the framework from two perspectives: a view-guided attention\nmodule for more robust text features, and a view-guided scoring strategy during\nthe final prediction. With our designed paradigm, ViewRefer achieves superior\nperformance on three benchmarks and surpasses the second-best by +2.8%, +1.5%,\nand +1.35% on Sr3D, Nr3D, and ScanRefer. Code is released at\nhttps://github.com/Ivan-Tang-3D/ViewRefer3D.", "published": "2023-03-29 17:59:10", "link": "http://arxiv.org/abs/2303.16894v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission\n  Exams", "abstract": "The present study aims to explore the capabilities of Language Models (LMs)\nin tackling high-stakes multiple-choice tests, represented here by the Exame\nNacional do Ensino M\\'edio (ENEM), a multidisciplinary entrance examination\nwidely adopted by Brazilian universities. This exam poses challenging tasks for\nLMs, since its questions may span into multiple fields of knowledge, requiring\nunderstanding of information from diverse domains. For instance, a question may\nrequire comprehension of both statistics and biology to be solved. This work\nanalyzed responses generated by GPT-3.5 and GPT-4 models for questions\npresented in the 2009-2017 exams, as well as for questions of the 2022 exam,\nwhich were made public after the training of the models was completed.\nFurthermore, different prompt strategies were tested, including the use of\nChain-of-Thought (CoT) prompts to generate explanations for answers. On the\n2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy\nof 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on\nexperiments are available at https://github.com/piresramon/gpt-4-enem.", "published": "2023-03-29 20:10:13", "link": "http://arxiv.org/abs/2303.17003v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray\n  Report Generation", "abstract": "Automated generation of clinically accurate radiology reports can improve\npatient care. Previous report generation methods that rely on image captioning\nmodels often generate incoherent and incorrect text due to their lack of\nrelevant domain knowledge, while retrieval-based attempts frequently retrieve\nreports that are irrelevant to the input image. In this work, we propose\nContrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology\nreport generation module that uses an image-text matching score to measure the\nsimilarity of a chest X-ray image and radiology report for report retrieval. We\nobserve that computing the image-text matching score with a language-image\nmodel can effectively capture the fine-grained interaction between image and\ntext that is often lost when using cosine similarity. X-REM outperforms\nmultiple prior radiology report generation modules in terms of both natural\nlanguage and clinical metrics. Human evaluation of the generated reports\nsuggests that X-REM increased the number of zero-error reports and decreased\nthe average error severity compared to the baseline retrieval approach. Our\ncode is available at: https://github.com/rajpurkarlab/X-REM", "published": "2023-03-29 04:00:47", "link": "http://arxiv.org/abs/2303.17579v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Kernel-interpolation-based spatial active noise control with exterior\n  radiation suppression", "abstract": "A spatial active noise control (ANC) method based on kernel interpolation of\na sound field with exterior radiation suppression is proposed. The aim of\nspatial ANC is to reduce incoming noise over a target region by using multiple\nsecondary sources and microphones. The method based on kernel interpolation of\na sound field allows noise attenuation in a regional space with an array of\narbitrary geometry. The cost function is defined as the acoustic potential\nenergy, i.e., the regional integral of the power distribution inside the target\nregion. However, this cost function does not take into consideration the\nexterior radiation of secondary sources. Thus, the acoustic power in the\nexterior region can be amplified by the output of the secondary sources. We\npropose two spatial ANC methods with exterior radiation suppression. The first\napproach is based on the minimization of the cost function formulated as a sum\nof the interior acoustic potential energy and exterior radiation power. The\nsecond approach is based on the minimization of the interior acoustic potential\nenergy with inequality constraints on the exterior radiation power. Adaptive\nalgorithms for minimizing the cost function are derived for the two approaches.\nNumerical experimental results indicate that the proposed methods can reduce\nthe interior regional noise while suppressing the exterior radiation.", "published": "2023-03-29 02:00:07", "link": "http://arxiv.org/abs/2303.16389v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Joint unsupervised and supervised learning for context-aware language\n  identification", "abstract": "Language identification (LID) recognizes the language of a spoken utterance\nautomatically. According to recent studies, LID models trained with an\nautomatic speech recognition (ASR) task perform better than those trained with\na LID task only. However, we need additional text labels to train the model to\nrecognize speech, and acquiring the text labels is a cost high. In order to\novercome this problem, we propose context-aware language identification using a\ncombination of unsupervised and supervised learning without any text labels.\nThe proposed method learns the context of speech through masked language\nmodeling (MLM) loss and simultaneously trains to determine the language of the\nutterance with supervised learning loss. The proposed joint learning was found\nto reduce the error rate by 15.6% compared to the same structure model trained\nby supervised-only learning on a subset of the VoxLingua107 dataset consisting\nof sub-three-second utterances in 11 languages.", "published": "2023-03-29 07:39:11", "link": "http://arxiv.org/abs/2303.16511v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot\n  AV-ASR", "abstract": "Audiovisual automatic speech recognition (AV-ASR) aims to improve the\nrobustness of a speech recognition system by incorporating visual information.\nTraining fully supervised multimodal models for this task from scratch, however\nis limited by the need for large labelled audiovisual datasets (in each\ndownstream domain of interest). We present AVFormer, a simple method for\naugmenting audio-only models with visual information, at the same time\nperforming lightweight domain adaptation. We do this by (i) injecting visual\nembeddings into a frozen ASR model using lightweight trainable adaptors. We\nshow that these can be trained on a small amount of weakly labelled video data\nwith minimum additional training time and parameters. (ii) We also introduce a\nsimple curriculum scheme during training which we show is crucial to enable the\nmodel to jointly process audio and visual information effectively; and finally\n(iii) we show that our model achieves state of the art zero-shot results on\nthree different AV-ASR benchmarks (How2, VisSpeech and Ego4D), while also\ncrucially preserving decent performance on traditional audio-only speech\nrecognition benchmarks (LibriSpeech). Qualitative results show that our model\neffectively leverages visual information for robust speech recognition.", "published": "2023-03-29 07:24:28", "link": "http://arxiv.org/abs/2303.16501v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos", "abstract": "Modeling sounds emitted from physical object interactions is critical for\nimmersive perceptual experiences in real and virtual worlds. Traditional\nmethods of impact sound synthesis use physics simulation to obtain a set of\nphysics parameters that could represent and synthesize the sound. However, they\nrequire fine details of both the object geometries and impact locations, which\nare rarely available in the real world and can not be applied to synthesize\nimpact sounds from common videos. On the other hand, existing video-driven deep\nlearning-based approaches could only capture the weak correspondence between\nvisual content and impact sounds since they lack of physics knowledge. In this\nwork, we propose a physics-driven diffusion model that can synthesize\nhigh-fidelity impact sound for a silent video clip. In addition to the video\ncontent, we propose to use additional physics priors to guide the impact sound\nsynthesis procedure. The physics priors include both physics parameters that\nare directly estimated from noisy real-world impact sound examples without\nsophisticated setup and learned residual parameters that interpret the sound\nenvironment via neural networks. We further implement a novel diffusion model\nwith specific training and inference strategies to combine physics priors and\nvisual information for impact sound synthesis. Experimental results show that\nour model outperforms several existing systems in generating realistic impact\nsounds. More importantly, the physics-based representations are fully\ninterpretable and transparent, thus enabling us to perform sound editing\nflexibly.", "published": "2023-03-29 17:59:53", "link": "http://arxiv.org/abs/2303.16897v3", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Transformer-based Self-supervised Multimodal Representation Learning for\n  Wearable Emotion Recognition", "abstract": "Recently, wearable emotion recognition based on peripheral physiological\nsignals has drawn massive attention due to its less invasive nature and its\napplicability in real-life scenarios. However, how to effectively fuse\nmultimodal data remains a challenging problem. Moreover, traditional\nfully-supervised based approaches suffer from overfitting given limited labeled\ndata. To address the above issues, we propose a novel self-supervised learning\n(SSL) framework for wearable emotion recognition, where efficient multimodal\nfusion is realized with temporal convolution-based modality-specific encoders\nand a transformer-based shared encoder, capturing both intra-modal and\ninter-modal correlations. Extensive unlabeled data is automatically assigned\nlabels by five signal transforms, and the proposed SSL model is pre-trained\nwith signal transformation recognition as a pretext task, allowing the\nextraction of generalized multimodal representations for emotion-related\ndownstream tasks. For evaluation, the proposed SSL model was first pre-trained\non a large-scale self-collected physiological dataset and the resulting encoder\nwas subsequently frozen or fine-tuned on three public supervised emotion\nrecognition datasets. Ultimately, our SSL-based method achieved\nstate-of-the-art results in various emotion classification tasks. Meanwhile,\nthe proposed model proved to be more accurate and robust compared to\nfully-supervised methods on low data regimes.", "published": "2023-03-29 19:45:55", "link": "http://arxiv.org/abs/2303.17611v1", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
