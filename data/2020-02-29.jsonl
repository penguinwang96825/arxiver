{"title": "Depth-Adaptive Graph Recurrent Network for Text Classification", "abstract": "The Sentence-State LSTM (S-LSTM) is a powerful and high efficient graph\nrecurrent network, which views words as nodes and performs layer-wise recurrent\nsteps between them simultaneously. Despite its successes on text\nrepresentations, the S-LSTM still suffers from two drawbacks. Firstly, given a\nsentence, certain words are usually more ambiguous than others, and thus more\ncomputation steps need to be taken for these difficult words and vice versa.\nHowever, the S-LSTM takes fixed computation steps for all words, irrespective\nof their hardness. The secondary one comes from the lack of sequential\ninformation (e.g., word order) that is inherently important for natural\nlanguage. In this paper, we try to address these issues and propose a\ndepth-adaptive mechanism for the S-LSTM, which allows the model to learn how\nmany computational steps to conduct for different words as required. In\naddition, we integrate an extra RNN layer to inject sequential information,\nwhich also serves as an input feature for the decision of adaptive depths.\nResults on the classic text classification task (24 datasets in various sizes\nand domains) show that our model brings significant improvements against the\nconventional S-LSTM and other high-performance models (e.g., the Transformer),\nmeanwhile achieving a good accuracy-speed trade off.", "published": "2020-02-29 03:09:55", "link": "http://arxiv.org/abs/2003.00166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Text Summarization with Syntax-Based Negation and Semantic\n  Concept Identification", "abstract": "In the era of clinical information explosion, a good strategy for clinical\ntext summarization is helpful to improve the clinical workflow. The ideal\nsummarization strategy can preserve important information in the informative\nbut less organized, ill-structured clinical narrative texts. Instead of using\npure statistical learning approaches, which are difficult to interpret and\nexplain, we utilized knowledge of computational linguistics with human\nexperts-curated biomedical knowledge base to achieve the interpretable and\nmeaningful clinical text summarization. Our research objective is to use the\nbiomedical ontology with semantic information, and take the advantage from the\nlanguage hierarchical structure, the constituency tree, in order to identify\nthe correct clinical concepts and the corresponding negation information, which\nis critical for summarizing clinical concepts from narrative text. We achieved\nthe clinically acceptable performance for both negation detection and concept\nidentification, and the clinical concepts with common negated patterns can be\nidentified and negated by the proposed method.", "published": "2020-02-29 22:15:15", "link": "http://arxiv.org/abs/2003.00353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Wins the Game of Thrones? How Sentiments Improve the Prediction of\n  Candidate Choice", "abstract": "This paper analyzes how candidate choice prediction improves by different\npsychological predictors. To investigate this question, it collected an\noriginal survey dataset featuring the popular TV series \"Game of Thrones\". The\nrespondents answered which character they anticipated to win in the final\nepisode of the series, and explained their choice of the final candidate in\nfree text from which sentiments were extracted. These sentiments were compared\nto feature sets derived from candidate likeability and candidate personality\nratings. In our benchmarking of 10-fold cross-validation in 100 repetitions,\nall feature sets except the likeability ratings yielded a 10-11% improvement in\naccuracy on the holdout set over the base model. Treating the class imbalance\nwith synthetic minority oversampling (SMOTE) increased holdout set performance\nby 20-34% but surprisingly not testing set performance. Taken together, our\nstudy provides a quantified estimation of the additional predictive value of\npsychological predictors. Likeability ratings were clearly outperformed by the\nfeature sets based on personality, emotional valence, and basic emotions.", "published": "2020-02-29 04:30:28", "link": "http://arxiv.org/abs/2003.07683v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "What Emotions Make One or Five Stars? Understanding Ratings of Online\n  Product Reviews by Sentiment Analysis and XAI", "abstract": "When people buy products online, they primarily base their decisions on the\nrecommendations of others given in online reviews. The current work analyzed\nthese online reviews by sentiment analysis and used the extracted sentiments as\nfeatures to predict the product ratings by several machine learning algorithms.\nThese predictions were disentangled by various meth-ods of explainable AI (XAI)\nto understand whether the model showed any bias during prediction. Study 1\nbenchmarked these algorithms (knn, support vector machines, random forests,\ngradient boosting machines, XGBoost) and identified random forests and XGBoost\nas best algorithms for predicting the product ratings. In Study 2, the analysis\nof global feature importance identified the sentiment joy and the emotional\nvalence negative as most predictive features. Two XAI visualization methods,\nlocal feature attributions and partial dependency plots, revealed several\nincorrect prediction mechanisms on the instance-level. Performing the\nbenchmarking as classification, Study 3 identified a high no-information rate\nof 64.4% that indicated high class imbalance as underlying reason for the\nidentified problems. In conclusion, good performance by machine learning\nalgorithms must be taken with caution because the dataset, as encountered in\nthis work, could be biased towards certain predictions. This work demonstrates\nhow XAI methods reveal such prediction bias.", "published": "2020-02-29 07:39:35", "link": "http://arxiv.org/abs/2003.00201v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Voice trigger detection from LVCSR hypothesis lattices using\n  bidirectional lattice recurrent neural networks", "abstract": "We propose a method to reduce false voice triggers of a speech-enabled\npersonal assistant by post-processing the hypothesis lattice of a server-side\nlarge-vocabulary continuous speech recognizer (LVCSR) via a neural network. We\nfirst discuss how an estimate of the posterior probability of the trigger\nphrase can be obtained from the hypothesis lattice using known techniques to\nperform detection, then investigate a statistical model that processes the\nlattice in a more explicitly data-driven, discriminative manner. We propose\nusing a Bidirectional Lattice Recurrent Neural Network (LatticeRNN) for the\ntask, and show that it can significantly improve detection accuracy over using\nthe 1-best result or the posterior.", "published": "2020-02-29 17:02:41", "link": "http://arxiv.org/abs/2003.00304v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and\n  Perspective", "abstract": "Neural-symbolic computing has now become the subject of interest of both\nacademic and industry research laboratories. Graph Neural Networks (GNN) have\nbeen widely used in relational and symbolic domains, with widespread\napplication of GNNs in combinatorial optimization, constraint satisfaction,\nrelational reasoning and other scientific domains. The need for improved\nexplainability, interpretability and trust of AI systems in general demands\nprincipled methodologies, as suggested by neural-symbolic computing. In this\npaper, we review the state-of-the-art on the use of GNNs as a model of\nneural-symbolic computing. This includes the application of GNNs in several\ndomains as well as its relationship to current developments in neural-symbolic\ncomputing.", "published": "2020-02-29 18:55:13", "link": "http://arxiv.org/abs/2003.00330v7", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Understanding the Downstream Instability of Word Embeddings", "abstract": "Many industrial machine learning (ML) systems require frequent retraining to\nkeep up-to-date with constantly changing data. This retraining exacerbates a\nlarge challenge facing ML systems today: model training is unstable, i.e.,\nsmall changes in training data can cause significant changes in the model's\npredictions. In this paper, we work on developing a deeper understanding of\nthis instability, with a focus on how a core building block of modern natural\nlanguage processing (NLP) pipelines---pre-trained word embeddings---affects the\ninstability of downstream NLP models. We first empirically reveal a tradeoff\nbetween stability and memory: increasing the embedding memory 2x can reduce the\ndisagreement in predictions due to small changes in training data by 5% to 37%\n(relative). To theoretically explain this tradeoff, we introduce a new measure\nof embedding instability---the eigenspace instability measure---which we prove\nbounds the disagreement in downstream predictions introduced by the change in\nword embeddings. Practically, we show that the eigenspace instability measure\ncan be a cost-effective way to choose embedding parameters to minimize\ninstability without training downstream models, outperforming other embedding\ndistance measures and performing competitively with a nearest neighbor-based\nmeasure. Finally, we demonstrate that the observed stability-memory tradeoffs\nextend to other types of embeddings as well, including knowledge graph and\ncontextual word embeddings.", "published": "2020-02-29 00:39:12", "link": "http://arxiv.org/abs/2003.04983v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Generating EEG features from Acoustic features", "abstract": "In this paper we demonstrate predicting electroencephalograpgy (EEG) features\nfrom acoustic features using recurrent neural network (RNN) based regression\nmodel and generative adversarial network (GAN). We predict various types of EEG\nfeatures from acoustic features. We compare our results with the previously\nstudied problem on speech synthesis using EEG and our results demonstrate that\nEEG features can be generated from acoustic features with lower root mean\nsquare error (RMSE), normalized RMSE values compared to generating acoustic\nfeatures from EEG features (ie: speech synthesis using EEG) when tested using\nthe same data sets.", "published": "2020-02-29 16:44:08", "link": "http://arxiv.org/abs/2003.00007v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Emotion Recognition System from Speech and Visual Information based on\n  Convolutional Neural Networks", "abstract": "Emotion recognition has become an important field of research in the\nhuman-computer interactions domain. The latest advancements in the field show\nthat combining visual with audio information lead to better results if compared\nto the case of using a single source of information separately. From a visual\npoint of view, a human emotion can be recognized by analyzing the facial\nexpression of the person. More precisely, the human emotion can be described\nthrough a combination of several Facial Action Units. In this paper, we propose\na system that is able to recognize emotions with a high accuracy rate and in\nreal time, based on deep Convolutional Neural Networks. In order to increase\nthe accuracy of the recognition system, we analyze also the speech data and\nfuse the information coming from both sources, i.e., visual and audio.\nExperimental results show the effectiveness of the proposed scheme for emotion\nrecognition and the importance of combining visual with audio data.", "published": "2020-02-29 22:09:46", "link": "http://arxiv.org/abs/2003.00351v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Voice Separation with an Unknown Number of Multiple Speakers", "abstract": "We present a new method for separating a mixed audio sequence, in which\nmultiple voices speak simultaneously. The new method employs gated neural\nnetworks that are trained to separate the voices at multiple processing steps,\nwhile maintaining the speaker in each output channel fixed. A different model\nis trained for every number of possible speakers, and the model with the\nlargest number of speakers is employed to select the actual number of speakers\nin a given sample. Our method greatly outperforms the current state of the art,\nwhich, as we show, is not competitive for more than two speakers.", "published": "2020-02-29 20:02:54", "link": "http://arxiv.org/abs/2003.01531v4", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Robust Robotic Pouring using Audition and Haptics", "abstract": "Robust and accurate estimation of liquid height lies as an essential part of\npouring tasks for service robots. However, vision-based methods often fail in\noccluded conditions while audio-based methods cannot work well in a noisy\nenvironment. We instead propose a multimodal pouring network (MP-Net) that is\nable to robustly predict liquid height by conditioning on both audition and\nhaptics input. MP-Net is trained on a self-collected multimodal pouring\ndataset. This dataset contains 300 robot pouring recordings with audio and\nforce/torque measurements for three types of target containers. We also augment\nthe audio data by inserting robot noise. We evaluated MP-Net on our collected\ndataset and a wide variety of robot experiments. Both network training results\nand robot experiments demonstrate that MP-Net is robust against noise and\nchanges to the task and environment. Moreover, we further combine the predicted\nheight and force data to estimate the shape of the target container.", "published": "2020-02-29 20:24:11", "link": "http://arxiv.org/abs/2003.00342v2", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
