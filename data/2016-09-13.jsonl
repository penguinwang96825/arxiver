{"title": "An Experimental Study of LSTM Encoder-Decoder Model for Text\n  Simplification", "abstract": "Text simplification (TS) aims to reduce the lexical and structural complexity\nof a text, while still retaining the semantic meaning. Current automatic TS\ntechniques are limited to either lexical-level applications or manually\ndefining a large amount of rules. Since deep neural networks are powerful\nmodels that have achieved excellent performance over many difficult tasks, in\nthis paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder\nmodel for sentence level TS, which makes minimal assumptions about word\nsequence. We conduct preliminary experiments to find that the model is able to\nlearn operation rules such as reversing, sorting and replacing from sequence\npairs, which shows that the model may potentially discover and apply rules such\nas modifying sentence structure, substituting words, and removing words for TS.", "published": "2016-09-13 03:02:32", "link": "http://arxiv.org/abs/1609.03663v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Attention for Neural Machine Translation", "abstract": "The attention mechanism is an important part of the neural machine\ntranslation (NMT) where it was reported to produce richer source representation\ncompared to fixed-length encoding sequence-to-sequence models. Recently, the\neffectiveness of attention has also been explored in the context of image\ncaptioning. In this work, we assess the feasibility of a multimodal attention\nmechanism that simultaneously focus over an image and its natural language\ndescription for generating a description in another language. We train several\nvariants of our proposed attention mechanism on the Multi30k multilingual image\ncaptioning dataset. We show that a dedicated attention for each modality\nachieves up to 1.6 points in BLEU and METEOR compared to a textual NMT\nbaseline.", "published": "2016-09-13 18:46:03", "link": "http://arxiv.org/abs/1609.03976v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Character-Level Language Modeling with Hierarchical Recurrent Neural\n  Networks", "abstract": "Recurrent neural network (RNN) based character-level language models (CLMs)\nare extremely useful for modeling out-of-vocabulary words by nature. However,\ntheir performance is generally much worse than the word-level language models\n(WLMs), since CLMs need to consider longer history of tokens to properly\npredict the next one. We address this problem by proposing hierarchical RNN\narchitectures, which consist of multiple modules with different timescales.\nDespite the multi-timescale structures, the input and output layers operate\nwith the character-level clock, which allows the existing RNN CLM training\napproaches to be directly applicable without any modifications. Our CLM models\nshow better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word\nBenchmark with only 2% of parameters. Also, we present real-time\ncharacter-level end-to-end speech recognition examples on the Wall Street\nJournal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the\nproposed models results in better recognition accuracies even though the number\nof parameters are reduced to 30%.", "published": "2016-09-13 11:41:48", "link": "http://arxiv.org/abs/1609.03777v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
