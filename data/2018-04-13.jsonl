{"title": "An Ontology-Based Dialogue Management System for Banking and Finance\n  Dialogue Systems", "abstract": "Keeping the dialogue state in dialogue systems is a notoriously difficult\ntask. We introduce an ontology-based dialogue manage(OntoDM), a dialogue\nmanager that keeps the state of the conversation, provides a basis for anaphora\nresolution and drives the conversation via domain ontologies. The banking and\nfinance area promises great potential for disambiguating the context via a rich\nset of products and specificity of proper nouns, named entities and verbs. We\nused ontologies both as a knowledge base and a basis for the dialogue manager;\nthe knowledge base component and dialogue manager components coalesce in a\nsense. Domain knowledge is used to track Entities of Interest, i.e. nodes\n(classes) of the ontology which happen to be products and services. In this way\nwe also introduced conversation memory and attention in a sense. We finely\nblended linguistic methods, domain-driven keyword ranking and domain ontologies\nto create ways of domain-driven conversation. Proposed framework is used in our\nin-house German language banking and finance chatbots. General challenges of\nGerman language processing and finance-banking domain chatbot language models\nand lexicons are also introduced. This work is still in progress, hence no\nsuccess metrics have been introduced yet.", "published": "2018-04-13 08:40:57", "link": "http://arxiv.org/abs/1804.04838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Dictionaries into Deep Neural Networks for the Chinese\n  Clinical Named Entity Recognition", "abstract": "Clinical Named Entity Recognition (CNER) aims to identify and classify\nclinical terms such as diseases, symptoms, treatments, exams, and body parts in\nelectronic health records, which is a fundamental and crucial task for clinical\nand translational research. In recent years, deep neural networks have achieved\nsignificant success in named entity recognition and many other Natural Language\nProcessing (NLP) tasks. Most of these algorithms are trained end to end, and\ncan automatically learn features from large scale labeled datasets. However,\nthese data-driven methods typically lack the capability of processing rare or\nunseen entities. Previous statistical methods and feature engineering practice\nhave demonstrated that human knowledge can provide valuable information for\nhandling rare and unseen cases. In this paper, we address the problem by\nincorporating dictionaries into deep neural networks for the Chinese CNER task.\nTwo different architectures that extend the Bi-directional Long Short-Term\nMemory (Bi-LSTM) neural network and five different feature representation\nschemes are proposed to handle the task. Computational results on the CCKS-2017\nTask 2 benchmark dataset show that the proposed method achieves the highly\ncompetitive performance compared with the state-of-the-art deep learning\nmethods.", "published": "2018-04-13 16:36:44", "link": "http://arxiv.org/abs/1804.05017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pieces of Eight: 8-bit Neural Machine Translation", "abstract": "Neural machine translation has achieved levels of fluency and adequacy that\nwould have been surprising a short time ago. Output quality is extremely\nrelevant for industry purposes, however it is equally important to produce\nresults in the shortest time possible, mainly for latency-sensitive\napplications and to control cloud hosting costs. In this paper we show the\neffectiveness of translating with 8-bit quantization for models that have been\ntrained using 32-bit floating point values. Results show that 8-bit translation\nmakes a non-negligible impact in terms of speed with no degradation in accuracy\nand adequacy.", "published": "2018-04-13 17:10:12", "link": "http://arxiv.org/abs/1804.05038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Language Identification System for Hindi and Magahi", "abstract": "Language identification has become a prerequisite for all kinds of automated\ntext processing systems. In this paper, we present a rule-based language\nidentifier tool for two closely related Indo-Aryan languages: Hindi and Magahi.\nThis system has currently achieved an accuracy of approx 86.34%. We hope to\nimprove this in the future. Automatic identification of languages will be\nsignificant in the accuracy of output of Web Crawlers.", "published": "2018-04-13 19:38:52", "link": "http://arxiv.org/abs/1804.05095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neologisms on Facebook", "abstract": "In this paper, we present a study of neologisms and loan words frequently\noccurring in Facebook user posts. We have analyzed a dataset of several million\npublically available posts written during 2006-2013 by Russian-speaking\nFacebook users. From these, we have built a vocabulary of most frequent\nlemmatized words missing from the OpenCorpora dictionary the assumption being\nthat many such words have entered common use only recently. This assumption is\ncertainly not true for all the words extracted in this way; for that reason, we\nmanually filtered the automatically obtained list in order to exclude\nnon-Russian or incorrectly lemmatized words, as well as words recorded by other\ndictionaries or those occurring in texts from the Russian National Corpus. The\nresult is a list of 168 words that can potentially be considered neologisms. We\npresent an attempt at an etymological classification of these neologisms\n(unsurprisingly, most of them have recently been borrowed from English, but\nthere are also quite a few new words composed of previously borrowed stems) and\nidentify various derivational patterns. We also classify words into several\nlarge thematic areas, \"internet\", \"marketing\", and \"multimedia\" being among\nthose with the largest number of words. We believe that, together with the word\nbase collected in the process, they can serve as a starting point in further\nstudies of neologisms and lexical processes that lead to their acceptance into\nthe mainstream language.", "published": "2018-04-13 16:57:59", "link": "http://arxiv.org/abs/1804.05831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demo of Sanskrit-Hindi SMT System", "abstract": "The demo proposal presents a Phrase-based Sanskrit-Hindi (SaHiT) Statistical\nMachine Translation system. The system has been developed on Moses. 43k\nsentences of Sanskrit-Hindi parallel corpus and 56k sentences of a monolingual\ncorpus in the target language (Hindi) have been used. This system gives 57 BLEU\nscore.", "published": "2018-04-13 19:44:56", "link": "http://arxiv.org/abs/1804.06716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Good Configurations for GitHub and Stack Overflow Topic\n  Models", "abstract": "Software repositories contain large amounts of textual data, ranging from\nsource code comments and issue descriptions to questions, answers, and comments\non Stack Overflow. To make sense of this textual data, topic modelling is\nfrequently used as a text-mining tool for the discovery of hidden semantic\nstructures in text bodies. Latent Dirichlet allocation (LDA) is a commonly used\ntopic model that aims to explain the structure of a corpus by grouping texts.\nLDA requires multiple parameters to work well, and there are only rough and\nsometimes conflicting guidelines available on how these parameters should be\nset. In this paper, we contribute (i) a broad study of parameters to arrive at\ngood local optima for GitHub and Stack Overflow text corpora, (ii) an\na-posteriori characterisation of text corpora related to eight programming\nlanguages, and (iii) an analysis of corpus feature importance via per-corpus\nLDA configuration. We find that (1) popular rules of thumb for topic modelling\nparameter configuration are not applicable to the corpora used in our\nexperiments, (2) corpora sampled from GitHub and Stack Overflow have different\ncharacteristics and require different configurations to achieve good model fit,\nand (3) we can predict good configurations for unseen corpora reliably. These\nfindings support researchers and practitioners in efficiently determining\nsuitable configurations for topic modelling when analysing textual data\ncontained in software repositories.", "published": "2018-04-13 00:09:48", "link": "http://arxiv.org/abs/1804.04749v3", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "S\u00ed o no, qu\u00e8 penses? Catalonian Independence and Linguistic Identity\n  on Social Media", "abstract": "Political identity is often manifested in language variation, but the\nrelationship between the two is still relatively unexplored from a quantitative\nperspective. This study examines the use of Catalan, a language local to the\nsemi-autonomous region of Catalonia in Spain, on Twitter in discourse related\nto the 2017 independence referendum. We corroborate prior findings that\npro-independence tweets are more likely to include the local language than\nanti-independence tweets. We also find that Catalan is used more often in\nreferendum-related discourse than in other contexts, contrary to prior findings\non language variation. This suggests a strong role for the Catalan language in\nthe expression of Catalonian political identity.", "published": "2018-04-13 18:52:14", "link": "http://arxiv.org/abs/1804.05088v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Speaker Embedding Extraction with Phonetic Information", "abstract": "Speaker embeddings achieve promising results on many speaker verification\ntasks. Phonetic information, as an important component of speech, is rarely\nconsidered in the extraction of speaker embeddings. In this paper, we introduce\nphonetic information to the speaker embedding extraction based on the x-vector\narchitecture. Two methods using phonetic vectors and multi-task learning are\nproposed. On the Fisher dataset, our best system outperforms the original\nx-vector approach by 20% in EER, and by 15%, 15% in minDCF08 and minDCF10,\nrespectively. Experiments conducted on NIST SRE10 further demonstrate the\neffectiveness of the proposed methods.", "published": "2018-04-13 09:51:14", "link": "http://arxiv.org/abs/1804.04862v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Language Recognition using Time Delay Deep Neural Network", "abstract": "This work explores the use of a monolingual Deep Neural Network (DNN) model\nas an universal background model (UBM) to address the problem of Language\nRecognition (LR) in I-vector framework. A Time Delay Deep Neural Network\n(TDDNN) architecture is used in this work, which is trained as an acoustic\nmodel in an English Automatic Speech Recognition (ASR) task. A logistic\nregression model is trained to classify the I-vectors. The proposed system is\ntested with fourteen languages with various confusion pairs and it can be\neasily extended to include a new language by just retraining the last simple\nlogistic regression model. The architectural flexibility is the major advantage\nof the proposed system compared to the single DNN classifier based approach.", "published": "2018-04-13 15:43:59", "link": "http://arxiv.org/abs/1804.05000v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voices Obscured in Complex Environmental Settings (VOICES) corpus", "abstract": "This paper introduces the Voices Obscured In Complex Environmental Settings\n(VOICES) corpus, a freely available dataset under Creative Commons BY 4.0. This\ndataset will promote speech and signal processing research of speech recorded\nby far-field microphones in noisy room conditions. Publicly available speech\ncorpora are mostly composed of isolated speech at close-range microphony. A\ntypical approach to better represent realistic scenarios, is to convolve clean\nspeech with noise and simulated room response for model training. Despite these\nefforts, model performance degrades when tested against uncurated speech in\nnatural conditions. For this corpus, audio was recorded in furnished rooms with\nbackground noise played in conjunction with foreground speech selected from the\nLibriSpeech corpus. Multiple sessions were recorded in each room to accommodate\nfor all foreground speech-background noise combinations. Audio was recorded\nusing twelve microphones placed throughout the room, resulting in 120 hours of\naudio per microphone. This work is a multi-organizational effort led by SRI\nInternational and Lab41 with the intent to push forward state-of-the-art\ndistant microphone approaches in signal processing and speech recognition.", "published": "2018-04-13 17:47:55", "link": "http://arxiv.org/abs/1804.05053v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancement of Throat Microphone Recordings Using Gaussian Mixture Model\n  Probabilistic Estimator", "abstract": "The throat microphone is a body-attached transducer that is worn against the\nneck. It captures the signals that are transmitted through the vocal folds,\nalong with the buzz tone of the larynx. Due to its skin contact, it is more\nrobust to the environmental noise compared to the acoustic microphone that\npicks up the vibrations through air pressure, and hence the all interventions.\nThe throat speech is partly intelligible, but gives unnatural and croaky sound.\nThis thesis tries to recover missing frequency bands of the throat speech and\ninvestigates envelope and excitation mapping problem with joint analysis of\nthroat- and acoustic-microphone recordings. A new phone-dependent GMM-based\nspectral envelope mapping scheme, which performs the minimum mean square error\n(MMSE) estimation of the acoustic-microphone spectral envelope, has been\nproposed. In the source-filter decomposition framework, we observed that the\nspectral envelope difference of the excitation signals of throat- and\nacoustic-microphone recordings is an important source of the degradation in the\nthroat-microphone voice quality. Thus, we also model spectral envelope\ndifference of the excitation signals as a spectral tilt vector, and propose a\nnew phone-dependent GMM-based spectral tilt mapping scheme to enhance throat\nexcitation signal. Experimental evaluations are performed to compare the\nproposed mapping scheme using both objective and subjective evaluations.\nObjective evaluations are performed with the log-spectral distortion (LSD) and\nthe wide-band perceptual evaluation of speech quality (PESQ) metrics.\nSubjective evaluations are performed with A/B pair comparison listening test.\nBoth objective and subjective evaluations yield that the proposed\nphone-dependent mapping consistently improves performances over the\nstate-of-the-art GMM estimators.", "published": "2018-04-13 13:05:01", "link": "http://arxiv.org/abs/1804.05937v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MeetSense: A Lightweight Framework for Group Identification using\n  Smartphones", "abstract": "In an organization, individuals prefer to form various formal and informal\ngroups for mutual interactions. Therefore, ubiquitous identification of such\ngroups and understanding their dynamics are important to monitor activities,\nbehaviours and well-being of the individuals. In this paper, we develop a\nlightweight, yet near-accurate, methodology, called MeetSense, to identify\nvarious interacting groups based on collective sensing through users'\nsmartphones. Group detection from sensor signals is not straightforward because\nusers in proximity may not always be under the same group. Therefore, we use\nacoustic context extracted from audio signals to infer interaction pattern\namong the subjects in proximity. We have developed an unsupervised and\nlightweight mechanism for user group detection by taking cues from network\nscience and measuring the cohesivity of the detected groups in terms of\nmodularity. Taking modularity into consideration, MeetSense can efficiently\neliminate incorrect groups, as well as adapt the mechanism depending on the\nrole played by the proximity and the acoustic context in a specific scenario.\nThe proposed method has been implemented and tested under many real-life\nscenarios in an academic institute environment, and we observe that MeetSense\ncan identify user groups with close to 90% accuracy even in a noisy\nenvironment.", "published": "2018-04-13 17:51:48", "link": "http://arxiv.org/abs/1804.05055v2", "categories": ["cs.SI", "cs.SD", "eess.AS"], "primary_category": "cs.SI"}
{"title": "Multi-Sound-Source Localization Using Machine Learning for Small\n  Autonomous Unmanned Vehicles with a Self-Rotating Bi-Microphone Array", "abstract": "Abstract While vision-based localization techniques have been widely studied\nfor small autonomous unmanned vehicles (SAUVs), sound-source localization\ncapabilities have not been fully enabled for SAUVs. This paper presents two\nnovel approaches for SAUVs to perform three-dimensional (3D)\nmulti-sound-sources localization (MSSL) using only the inter-channel time\ndifference (ICTD) signal generated by a self-rotating bi-microphone array. The\nproposed two approaches are based on two machine learning techniques viz.,\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) and Random\nSample Consensus (RANSAC) algorithms, respectively, whose performances are\ntested and compared in both simulations and experiments. The results show that\nboth approaches are capable of correctly identifying the number of sound\nsources along with their 3D orientations in a reverberant environment.", "published": "2018-04-13 20:37:46", "link": "http://arxiv.org/abs/1804.05111v2", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
