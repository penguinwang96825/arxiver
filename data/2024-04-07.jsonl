{"title": "Generating Uncontextualized and Contextualized Questions for\n  Document-Level Event Argument Extraction", "abstract": "This paper presents multiple question generation strategies for\ndocument-level event argument extraction. These strategies do not require human\ninvolvement and result in uncontextualized questions as well as contextualized\nquestions grounded on the event and document of interest. Experimental results\nshow that combining uncontextualized and contextualized questions is\nbeneficial, especially when event triggers and arguments appear in different\nsentences. Our approach does not have corpus-specific components, in\nparticular, the question generation strategies transfer across corpora. We also\npresent a qualitative analysis of the most common errors made by our best\nmodel.", "published": "2024-04-07 00:50:21", "link": "http://arxiv.org/abs/2404.04770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Machine Translation through Retrieval-Augmented LLM\n  Prompting: A Study on the Mambai Language", "abstract": "This study explores the use of large language models (LLMs) for translating\nEnglish into Mambai, a low-resource Austronesian language spoken in\nTimor-Leste, with approximately 200,000 native speakers. Leveraging a novel\ncorpus derived from a Mambai language manual and additional sentences\ntranslated by a native speaker, we examine the efficacy of few-shot LLM\nprompting for machine translation (MT) in this low-resource context. Our\nmethodology involves the strategic selection of parallel sentences and\ndictionary entries for prompting, aiming to enhance translation accuracy, using\nopen-source and proprietary LLMs (LlaMa 2 70b, Mixtral 8x7B, GPT-4). We find\nthat including dictionary entries in prompts and a mix of sentences retrieved\nthrough TF-IDF and semantic embeddings significantly improves translation\nquality. However, our findings reveal stark disparities in translation\nperformance across test sets, with BLEU scores reaching as high as 21.2 on\nmaterials from the language manual, in contrast to a maximum of 4.4 on a test\nset provided by a native speaker. These results underscore the importance of\ndiverse and representative corpora in assessing MT for low-resource languages.\nOur research provides insights into few-shot LLM prompting for low-resource MT,\nand makes available an initial corpus for the Mambai language.", "published": "2024-04-07 05:04:38", "link": "http://arxiv.org/abs/2404.04809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FRACTAL: Fine-Grained Scoring from Aggregate Text Labels", "abstract": "Large language models (LLMs) are being increasingly tuned to power complex\ngeneration tasks such as writing, fact-seeking, querying and reasoning.\nTraditionally, human or model feedback for evaluating and further tuning LLM\nperformance has been provided at the response level, enabling faster and more\ncost-effective assessments. However, recent works (Amplayo et al. [2022], Wu et\nal. [2023]) indicate that sentence-level labels may provide more accurate and\ninterpretable feedback for LLM optimization. In this work, we introduce methods\nto disaggregate response-level labels into sentence-level (pseudo-)labels. Our\napproach leverages multiple instance learning (MIL) and learning from label\nproportions (LLP) techniques in conjunction with prior information (e.g.,\ndocument-sentence cosine similarity) to train a specialized model for\nsentence-level scoring. We also employ techniques which use model predictions\nto pseudo-label the train-set at the sentence-level for model training to\nfurther improve performance.\n  We conduct extensive evaluations of our methods across six datasets and four\ntasks: retrieval, question answering, summarization, and math reasoning. Our\nresults demonstrate improved performance compared to multiple baselines across\nmost of these tasks. Our work is the first to develop response-level feedback\nto sentence-level scoring techniques, leveraging sentence-level prior\ninformation, along with comprehensive evaluations on multiple tasks as well as\nend-to-end finetuning evaluation showing performance comparable to a model\ntrained on fine-grained human annotated labels.", "published": "2024-04-07 05:54:28", "link": "http://arxiv.org/abs/2404.04817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Bias According to Bipol: Men are Naturally Right and It is the Role\n  of Women to Follow Their Lead", "abstract": "We introduce new large labeled datasets on bias in 3 languages and show in\nexperiments that bias exists in all 10 datasets of 5 languages evaluated,\nincluding benchmark datasets on the English GLUE/SuperGLUE leaderboards. The 3\nnew languages give a total of almost 6 million labeled samples and we benchmark\non these datasets using SotA multilingual pretrained models: mT5 and mBERT. The\nchallenge of social bias, based on prejudice, is ubiquitous, as recent events\nwith AI and large language models (LLMs) have shown. Motivated by this\nchallenge, we set out to estimate bias in multiple datasets. We compare some\nrecent bias metrics and use bipol, which has explainability in the metric. We\nalso confirm the unverified assumption that bias exists in toxic comments by\nrandomly sampling 200 samples from a toxic dataset population using the\nconfidence level of 95% and error margin of 7%. Thirty gold samples were\nrandomly distributed in the 200 samples to secure the quality of the\nannotation. Our findings confirm that many of the datasets have male bias\n(prejudice against women), besides other types of bias. We publicly release our\nnew datasets, lexica, models, and codes.", "published": "2024-04-07 07:24:45", "link": "http://arxiv.org/abs/2404.04838v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "F-MALLOC: Feed-forward Memory Allocation for Continual Learning in\n  Neural Machine Translation", "abstract": "In the evolving landscape of Neural Machine Translation (NMT), the\npretrain-then-finetune paradigm has yielded impressive results. However, the\npersistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While\nprevious work has introduced Continual Learning (CL) methods to address CF,\nthese approaches grapple with the delicate balance between avoiding forgetting\nand maintaining system extensibility. To address this, we propose a CL method,\nnamed $\\textbf{F-MALLOC}$ ($\\textbf{F}$eed-forward $\\textbf{M}$emory\n$\\textbf{ALLOC}ation)$. F-MALLOC is inspired by recent insights highlighting\nthat feed-forward layers emulate neural memories and encapsulate crucial\ntranslation knowledge. It decomposes feed-forward layers into discrete memory\ncells and allocates these memories to different tasks. By learning to allocate\nand safeguard these memories, our method effectively alleviates CF while\nensuring robust extendability. Besides, we propose a comprehensive assessment\nprotocol for multi-stage CL of NMT systems. Experiments conducted following\nthis new protocol showcase the superior performance of F-MALLOC, evidenced by\nhigher BLEU scores and almost zero forgetting.", "published": "2024-04-07 07:39:45", "link": "http://arxiv.org/abs/2404.04846v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Many Languages Make Good Multilingual Instruction Tuning? A Case\n  Study on BLOOM", "abstract": "Instruction tuning a large language model with multiple languages can prepare\nit for multilingual downstream tasks. Nonetheless, it is yet to be determined\nwhether having a handful of languages is sufficient, or whether the benefits\nincrease with the inclusion of more. By fine-tuning large multilingual models\non 1 to 52 languages, we present a case study on BLOOM to understand three\npertinent factors affecting performance: the number of languages, language\nexposure, and similarity between training and test languages. Overall we found\nthat 1) expanding language coverage in multilingual instruction tuning proves\nto be beneficial; 2) accuracy often significantly boots if the test language\nappears in the instruction mixture; 3) languages' genetic features correlate\nwith cross-lingual transfer more than merely the number of language but\ndifferent languages benefit to various degrees.", "published": "2024-04-07 07:44:33", "link": "http://arxiv.org/abs/2404.04850v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ethos and Pathos in Online Group Discussions: Corpora for Polarisation\n  Issues in Social Media", "abstract": "Growing polarisation in society caught the attention of the scientific\ncommunity as well as news media, which devote special issues to this\nphenomenon. At the same time, digitalisation of social interactions requires to\nrevise concepts from social science regarding establishment of trust, which is\na key feature of all human interactions, and group polarisation, as well as new\ncomputational tools to process large quantities of available data. Existing\nmethods seem insufficient to tackle the problem fully, thus, we propose to\napproach the problem by investigating rhetorical strategies employed by\nindividuals in polarising discussions online. To this end, we develop\nmulti-topic and multi-platform corpora with manual annotation of appeals to\nethos and pathos, two modes of persuasion in Aristotelian rhetoric. It can be\nemployed for training language models to advance the study of communication\nstrategies online on a large scale. With the use of computational methods, our\ncorpora allows an investigation of recurring patterns in polarising exchanges\nacross topics of discussion and media platforms, and conduct both quantitative\nand qualitative analyses of language structures leading to and engaged in\npolarisation.", "published": "2024-04-07 09:10:47", "link": "http://arxiv.org/abs/2404.04889v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Radial Networks: Dynamic Layer Routing for High-Performance Large\n  Language Models", "abstract": "Large language models (LLMs) often struggle with strict memory, latency, and\npower demands. To meet these demands, various forms of dynamic sparsity have\nbeen proposed that reduce compute on an input-by-input basis. These methods\nimprove over static methods by exploiting the variance across individual\ninputs, which has steadily grown with the exponential increase in training\ndata. Yet, the increasing depth within modern models, currently with hundreds\nof layers, has opened opportunities for dynamic layer sparsity, which skips the\ncomputation for entire layers. In this work, we explore the practicality of\nlayer sparsity by profiling residual connections and establish the relationship\nbetween model depth and layer sparsity. For example, the residual blocks in the\nOPT-66B model have a median contribution of 5% to its output. We then take\nadvantage of this dynamic sparsity and propose Radial Networks, which perform\ntoken-level routing between layers guided by a trained router module. These\nnetworks can be used in a post-training distillation from sequential networks\nor trained from scratch to co-learn the router and layer weights. They enable\nscaling to larger model sizes by decoupling the number of layers from the\ndynamic depth of the network, and their design allows for layer reuse. By\nvarying the compute token by token, they reduce the overall resources needed\nfor generating entire sequences. Overall, this leads to larger capacity\nnetworks with significantly lower compute and serving costs for large language\nmodels.", "published": "2024-04-07 09:52:31", "link": "http://arxiv.org/abs/2404.04900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Large Language Model: A Survey of Resources, Taxonomy and\n  Frontiers", "abstract": "Multilingual Large Language Models are capable of using powerful Large\nLanguage Models to handle and respond to queries in multiple languages, which\nachieves remarkable success in multilingual natural language processing tasks.\nDespite these breakthroughs, there still remains a lack of a comprehensive\nsurvey to summarize existing approaches and recent developments in this field.\nTo this end, in this paper, we present a thorough review and provide a unified\nperspective to summarize the recent progress as well as emerging trends in\nmultilingual large language models (MLLMs) literature. The contributions of\nthis paper can be summarized: (1) First survey: to our knowledge, we take the\nfirst step and present a thorough review in MLLMs research field according to\nmulti-lingual alignment; (2) New taxonomy: we offer a new and unified\nperspective to summarize the current progress of MLLMs; (3) New frontiers: we\nhighlight several emerging frontiers and discuss the corresponding challenges;\n(4) Abundant resources: we collect abundant open-source resources, including\nrelevant papers, data corpora, and leaderboards. We hope our work can provide\nthe community with quick access and spur breakthrough research in MLLMs.", "published": "2024-04-07 11:52:44", "link": "http://arxiv.org/abs/2404.04925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unleashing Large Language Models' Proficiency in Zero-shot Essay Scoring", "abstract": "Advances in automated essay scoring (AES) have traditionally relied on\nlabeled essays, requiring tremendous cost and expertise for their acquisition.\nRecently, large language models (LLMs) have achieved great success in various\ntasks, but their potential is less explored in AES. In this paper, we show that\nour zero-shot prompting framework, Multi Trait Specialization (MTS), elicits\nLLMs' ample potential for essay scoring. In particular, we automatically\ndecompose writing proficiency into distinct traits and generate scoring\ncriteria for each trait. Then, an LLM is prompted to extract trait scores from\nseveral conversational rounds, each round scoring one of the traits based on\nthe scoring criteria. Finally, we derive the overall score via trait averaging\nand min-max scaling. Experimental results on two benchmark datasets demonstrate\nthat MTS consistently outperforms straightforward prompting (Vanilla) in\naverage QWK across all LLMs and datasets, with maximum gains of 0.437 on\nTOEFL11 and 0.355 on ASAP. Additionally, with the help of MTS, the small-sized\nLlama2-13b-chat substantially outperforms ChatGPT, facilitating an effective\ndeployment in real applications.", "published": "2024-04-07 12:25:35", "link": "http://arxiv.org/abs/2404.04941v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Two Dimensional Feature Engineering Method for Relation Extraction", "abstract": "Transforming a sentence into a two-dimensional (2D) representation (e.g., the\ntable filling) has the ability to unfold a semantic plane, where an element of\nthe plane is a word-pair representation of a sentence which may denote a\npossible relation representation composed of two named entities. The 2D\nrepresentation is effective in resolving overlapped relation instances.\nHowever, in related works, the representation is directly transformed from a\nraw input. It is weak to utilize prior knowledge, which is important to support\nthe relation extraction task. In this paper, we propose a two-dimensional\nfeature engineering method in the 2D sentence representation for relation\nextraction. Our proposed method is evaluated on three public datasets (ACE05\nChinese, ACE05 English, and SanWen) and achieves the state-of-the-art\nperformance. The results indicate that two-dimensional feature engineering can\ntake advantage of a two-dimensional sentence representation and make full use\nof prior knowledge in traditional feature engineering. Our code is publicly\navailable at\nhttps://github.com/Wang-ck123/A-Two-Dimensional-Feature-Engineering-Method-for-Entity-Relation-Extraction", "published": "2024-04-07 13:37:30", "link": "http://arxiv.org/abs/2404.04959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MLaKE: Multilingual Knowledge Editing Benchmark for Large Language\n  Models", "abstract": "The extensive utilization of large language models (LLMs) underscores the\ncrucial necessity for precise and contemporary knowledge embedded within their\nintrinsic parameters. Existing research on knowledge editing primarily\nconcentrates on monolingual scenarios, neglecting the complexities presented by\nmultilingual contexts and multi-hop reasoning. To address these challenges, our\nstudy introduces MLaKE (Multilingual Language Knowledge Editing), a novel\nbenchmark comprising 4072 multi-hop and 5360 single-hop questions designed to\nevaluate the adaptability of knowledge editing methods across five languages:\nEnglish, Chinese, Japanese, French, and German. MLaKE aggregates fact chains\nfrom Wikipedia across languages and utilizes LLMs to generate questions in both\nfree-form and multiple-choice. We evaluate the multilingual knowledge editing\ngeneralization capabilities of existing methods on MLaKE. Existing knowledge\nediting methods demonstrate higher success rates in English samples compared to\nother languages. However, their generalization capabilities are limited in\nmulti-language experiments. Notably, existing knowledge editing methods often\nshow relatively high generalization for languages within the same language\nfamily compared to languages from different language families. These results\nunderscore the imperative need for advancements in multilingual knowledge\nediting and we hope MLaKE can serve as a valuable resource for benchmarking and\nsolution development.", "published": "2024-04-07 15:23:28", "link": "http://arxiv.org/abs/2404.04990v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How much reliable is ChatGPT's prediction on Information Extraction\n  under Input Perturbations?", "abstract": "In this paper, we assess the robustness (reliability) of ChatGPT under input\nperturbations for one of the most fundamental tasks of Information Extraction\n(IE) i.e. Named Entity Recognition (NER). Despite the hype, the majority of the\nresearchers have vouched for its language understanding and generation\ncapabilities; a little attention has been paid to understand its robustness:\nHow the input-perturbations affect 1) the predictions, 2) the confidence of\npredictions and 3) the quality of rationale behind its prediction. We perform a\nsystematic analysis of ChatGPT's robustness (under both zero-shot and few-shot\nsetup) on two NER datasets using both automatic and human evaluation. Based on\nautomatic evaluation metrics, we find that 1) ChatGPT is more brittle on Drug\nor Disease replacements (rare entities) compared to the perturbations on widely\nknown Person or Location entities, 2) the quality of explanations for the same\nentity considerably differ under different types of \"Entity-Specific\" and\n\"Context-Specific\" perturbations and the quality can be significantly improved\nusing in-context learning, and 3) it is overconfident for majority of the\nincorrect predictions, and hence it could lead to misguidance of the end-users.", "published": "2024-04-07 22:06:19", "link": "http://arxiv.org/abs/2404.05088v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation\n  and Fine-grained Classification", "abstract": "To advance the evaluation of multimodal math reasoning in large multimodal\nmodels (LMMs), this paper introduces a novel benchmark, MM-MATH. MM-MATH\nconsists of 5,929 open-ended middle school math problems with visual contexts,\nwith fine-grained classification across difficulty, grade level, and knowledge\npoints. Unlike existing benchmarks relying on binary answer comparison, MM-MATH\nincorporates both outcome and process evaluations. Process evaluation employs\nLMM-as-a-judge to automatically analyze solution steps, identifying and\ncategorizing errors into specific error types. Extensive evaluation of ten\nmodels on MM-MATH reveals significant challenges for existing LMMs,\nhighlighting their limited utilization of visual information and struggles with\nhigher-difficulty problems. The best-performing model achieves only 31%\naccuracy on MM-MATH, compared to 82% for humans. This highlights the\nchallenging nature of our benchmark for existing models and the significant gap\nbetween the multimodal reasoning capabilities of current models and humans. Our\nprocess evaluation reveals that diagram misinterpretation is the most common\nerror, accounting for more than half of the total error cases, underscoring the\nneed for improved image comprehension in multimodal reasoning.", "published": "2024-04-07 22:16:50", "link": "http://arxiv.org/abs/2404.05091v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget", "abstract": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.", "published": "2024-04-07 03:08:14", "link": "http://arxiv.org/abs/2404.04793v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SLPL SHROOM at SemEval2024 Task 06: A comprehensive study on models\n  ability to detect hallucination", "abstract": "Language models, particularly generative models, are susceptible to\nhallucinations, generating outputs that contradict factual knowledge or the\nsource text. This study explores methods for detecting hallucinations in three\nSemEval-2024 Task 6 tasks: Machine Translation, Definition Modeling, and\nParaphrase Generation. We evaluate two methods: semantic similarity between the\ngenerated text and factual references, and an ensemble of language models that\njudge each other's outputs. Our results show that semantic similarity achieves\nmoderate accuracy and correlation scores in trial data, while the ensemble\nmethod offers insights into the complexities of hallucination detection but\nfalls short of expectations. This work highlights the challenges of\nhallucination detection and underscores the need for further research in this\ncritical area.", "published": "2024-04-07 07:34:49", "link": "http://arxiv.org/abs/2404.04845v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Elementary fractal geometry. 5. Weak separation is strong separation", "abstract": "For self-similar sets, there are two important separation properties: the\nopen set condition and the weak separation condition introduced by Zerner,\nwhich may be replaced by the formally stronger finite type property of Ngai and\nWang. We show that any finite type self-similar set can be represented as a\ngraph-directed construction obeying the open set condition. The proof is based\non a combinatorial algorithm which performed well in computer experiments.", "published": "2024-04-07 09:21:09", "link": "http://arxiv.org/abs/2404.04892v1", "categories": ["math.DS", "cs.CL", "28A80, 11A63, 37B10, 54B15, 68Q45"], "primary_category": "math.DS"}
{"title": "Towards Understanding the Influence of Reward Margin on Preference Model\n  Performance", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a widely used framework\nfor the training of language models. However, the process of using RLHF to\ndevelop a language model that is well-aligned presents challenges, especially\nwhen it comes to optimizing the reward model. Our research has found that\nexisting reward models, when trained using the traditional ranking objective\nbased on human preference data, often struggle to effectively distinguish\nbetween responses that are more or less favorable in real-world scenarios. To\nbridge this gap, our study introduces a novel method to estimate the preference\ndifferences without the need for detailed, exhaustive labels from human\nannotators. Our experimental results provide empirical evidence that\nincorporating margin values into the training process significantly improves\nthe effectiveness of reward models. This comparative analysis not only\ndemonstrates the superiority of our approach in terms of reward prediction\naccuracy but also highlights its effectiveness in practical applications.", "published": "2024-04-07 12:10:04", "link": "http://arxiv.org/abs/2404.04932v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SilverSight: A Multi-Task Chinese Financial Large Language Model Based\n  on Adaptive Semantic Space Learning", "abstract": "Large language models (LLMs) are increasingly being applied across various\nspecialized fields, leveraging their extensive knowledge to empower a multitude\nof scenarios within these domains. However, each field encompasses a variety of\nspecific tasks that require learning, and the diverse, heterogeneous data\nacross these domains can lead to conflicts during model task transfer. In\nresponse to this challenge, our study introduces an Adaptive Semantic Space\nLearning (ASSL) framework, which utilizes the adaptive reorganization of data\ndistributions within the semantic space to enhance the performance and\nselection efficacy of multi-expert models. Utilizing this framework, we trained\na financial multi-task LLM named \"SilverSight\". Our research findings\ndemonstrate that our framework can achieve results close to those obtained with\nfull data training using only 10% of the data, while also exhibiting strong\ngeneralization capabilities.", "published": "2024-04-07 13:02:21", "link": "http://arxiv.org/abs/2404.04949v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for\n  Clinical Trials", "abstract": "Large Language Models (LLMs) are at the forefront of NLP achievements but\nfall short in dealing with shortcut learning, factual inconsistency, and\nvulnerability to adversarial inputs.These shortcomings are especially critical\nin medical contexts, where they can misrepresent actual model capabilities.\nAddressing this, we present SemEval-2024 Task 2: Safe Biomedical Natural\nLanguage Inference for ClinicalTrials. Our contributions include the refined\nNLI4CT-P dataset (i.e., Natural Language Inference for Clinical Trials -\nPerturbed), designed to challenge LLMs with interventional and causal reasoning\ntasks, along with a comprehensive evaluation of methods and results for\nparticipant submissions. A total of 106 participants registered for the task\ncontributing to over 1200 individual submissions and 25 system overview papers.\nThis initiative aims to advance the robustness and applicability of NLI models\nin healthcare, ensuring safer and more dependable AI assistance in clinical\ndecision-making. We anticipate that the dataset, models, and outcomes of this\ntask can support future research in the field of biomedical NLI. The dataset,\ncompetition leaderboard, and website are publicly available.", "published": "2024-04-07 13:58:41", "link": "http://arxiv.org/abs/2404.04963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Reliable and Empathetic Depression-Diagnosis-Oriented Chats", "abstract": "Chatbots can serve as a viable tool for preliminary depression diagnosis via\ninteractive conversations with potential patients. Nevertheless, the blend of\ntask-oriented and chit-chat in diagnosis-related dialogues necessitates\nprofessional expertise and empathy. Such unique requirements challenge\ntraditional dialogue frameworks geared towards single optimization goals. To\naddress this, we propose an innovative ontology definition and generation\nframework tailored explicitly for depression diagnosis dialogues, combining the\nreliability of task-oriented conversations with the appeal of empathy-related\nchit-chat. We further apply the framework to D$^4$, the only existing public\ndialogue dataset on depression diagnosis-oriented chats. Exhaustive\nexperimental results indicate significant improvements in task completion and\nemotional support generation in depression diagnosis, fostering a more\ncomprehensive approach to task-oriented chat dialogue system development and\nits applications in digital mental health.", "published": "2024-04-07 16:35:53", "link": "http://arxiv.org/abs/2404.05012v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI\n  Feedback", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated proficiency in\ntackling a variety of visual-language tasks. However, current LVLMs suffer from\nmisalignment between text and image modalities which causes three kinds of\nhallucination problems, i.e., object existence, object attribute, and object\nrelationship. To tackle this issue, existing methods mainly utilize\nReinforcement Learning (RL) to align modalities in LVLMs. However, they still\nsuffer from three main limitations: (1) General feedback can not indicate the\nhallucination type contained in the response; (2) Sparse rewards only give the\nsequence-level reward for the whole response; and (3)Annotation cost is\ntime-consuming and labor-intensive. To handle these limitations, we propose an\ninnovative method to align modalities in LVLMs through Fine-Grained Artificial\nIntelligence Feedback (FGAIF), which mainly consists of three steps: AI-based\nFeedback Collection, Fine-grained Reward Model Training, and Reinforcement\nLearning with Fine-grained Reward. Specifically, We first utilize AI tools to\npredict the types of hallucination for each segment in the response and obtain\na collection of fine-grained feedback. Then, based on the collected reward\ndata, three specialized reward models are trained to produce dense rewards.\nFinally, a novel fine-grained feedback module is integrated into the Proximal\nPolicy Optimization (PPO) algorithm. Extensive experiments are conducted on\nhallucination and general benchmarks, demonstrating the superior performance of\nour proposed method. Notably, compared with previous models trained with the\nRL-based aligning method, our proposed method is effective even with fewer\nparameters.", "published": "2024-04-07 19:00:45", "link": "http://arxiv.org/abs/2404.05046v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "QRscript: Embedding a Programming Language in QR codes to support\n  Decision and Management", "abstract": "Embedding a programming language in a QR code is a new and extremely\npromising opportunity, as it makes devices and objects smarter without\nnecessarily requiring an Internet connection. In this paper, all the steps\nneeded to translate a program written in a high-level programming language to\nits binary representation encoded in a QR code, and the opposite process that,\nstarting from the QR code, executes it by means of a virtual machine, have been\ncarefully detailed. The proposed programming language was named QRscript, and\ncan be easily extended so as to integrate new features. One of the main design\ngoals was to produce a very compact target binary code. In particular, in this\nwork we propose a specific sub-language (a dialect) that is aimed at encoding\ndecision trees. Besides industrial scenarios, this is useful in many other\napplication fields. The reported example, related to the configuration of an\nindustrial networked device, highlights the potential of the proposed\ntechnology, and permits to better understand all the translation steps.", "published": "2024-04-07 21:02:55", "link": "http://arxiv.org/abs/2404.05073v1", "categories": ["cs.NI", "cs.CL"], "primary_category": "cs.NI"}
{"title": "SEER-MoE: Sparse Expert Efficiency through Regularization for\n  Mixture-of-Experts", "abstract": "The advancement of deep learning has led to the emergence of\nMixture-of-Experts (MoEs) models, known for their dynamic allocation of\ncomputational resources based on input. Despite their promise, MoEs face\nchallenges, particularly in terms of memory requirements. To address this, our\nwork introduces SEER-MoE, a novel two-stage framework for reducing both the\nmemory footprint and compute requirements of pre-trained MoE models. The first\nstage involves pruning the total number of experts using a heavy-hitters\ncounting guidance, while the second stage employs a regularization-based\nfine-tuning strategy to recover accuracy loss and reduce the number of\nactivated experts during inference. Our empirical studies demonstrate the\neffectiveness of our method, resulting in a sparse MoEs model optimized for\ninference efficiency with minimal accuracy trade-offs.", "published": "2024-04-07 22:13:43", "link": "http://arxiv.org/abs/2404.05089v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multi-Level Framework for Accelerating Training Transformer Models", "abstract": "The fast growing capabilities of large-scale deep learning models, such as\nBert, GPT and ViT, are revolutionizing the landscape of NLP, CV and many other\ndomains. Training such models, however, poses an unprecedented demand for\ncomputing power, which incurs exponentially increasing energy cost and carbon\ndioxide emissions. It is thus critical to develop efficient training solutions\nto reduce the training costs. Motivated by a set of key observations of inter-\nand intra-layer similarities among feature maps and attentions that can be\nidentified from typical training processes, we propose a multi-level framework\nfor training acceleration. Specifically, the framework is based on three basic\noperators, Coalescing, De-coalescing and Interpolation, which can be\norchestrated to build a multi-level training framework. The framework consists\nof a V-cycle training process, which progressively down- and up-scales the\nmodel size and projects the parameters between adjacent levels of models via\ncoalescing and de-coalescing. The key idea is that a smaller model that can be\ntrained for fast convergence and the trained parameters provides high-qualities\nintermediate solutions for the next level larger network. The interpolation\noperator is designed to break the symmetry of neurons incurred by de-coalescing\nfor better convergence performance. Our experiments on transformer-based\nlanguage models (e.g. Bert, GPT) as well as a vision model (e.g. DeiT) prove\nthat the proposed framework reduces the computational cost by about 20% on\ntraining BERT/GPT-Base models and up to 51.6% on training the BERT-Large model\nwhile preserving the performance.", "published": "2024-04-07 03:04:34", "link": "http://arxiv.org/abs/2404.07999v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DWE+: Dual-Way Matching Enhanced Framework for Multimodal Entity Linking", "abstract": "Multimodal entity linking (MEL) aims to utilize multimodal information\n(usually textual and visual information) to link ambiguous mentions to\nunambiguous entities in knowledge base. Current methods facing main issues:\n(1)treating the entire image as input may contain redundant information. (2)the\ninsufficient utilization of entity-related information, such as attributes in\nimages. (3)semantic inconsistency between the entity in knowledge base and its\nrepresentation. To this end, we propose DWE+ for multimodal entity linking.\nDWE+ could capture finer semantics and dynamically maintain semantic\nconsistency with entities. This is achieved by three aspects: (a)we introduce a\nmethod for extracting fine-grained image features by partitioning the image\ninto multiple local objects. Then, hierarchical contrastive learning is used to\nfurther align semantics between coarse-grained information(text and image) and\nfine-grained (mention and visual objects). (b)we explore ways to extract visual\nattributes from images to enhance fusion feature such as facial features and\nidentity. (c)we leverage Wikipedia and ChatGPT to capture the entity\nrepresentation, achieving semantic enrichment from both static and dynamic\nperspectives, which better reflects the real-world entity semantics.\nExperiments on Wikimel, Richpedia, and Wikidiverse datasets demonstrate the\neffectiveness of DWE+ in improving MEL performance. Specifically, we optimize\nthese datasets and achieve state-of-the-art performance on the enhanced\ndatasets. The code and enhanced datasets are released on\nhttps://github.com/season1blue/DWET", "published": "2024-04-07 05:56:42", "link": "http://arxiv.org/abs/2404.04818v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Adapting LLMs for Efficient Context Processing through Soft Prompt\n  Compression", "abstract": "The rapid advancement of Large Language Models (LLMs) has inaugurated a\ntransformative epoch in natural language processing, fostering unprecedented\nproficiency in text generation, comprehension, and contextual scrutiny.\nNevertheless, effectively handling extensive contexts, crucial for myriad\napplications, poses a formidable obstacle owing to the intrinsic constraints of\nthe models' context window sizes and the computational burdens entailed by\ntheir operations. This investigation presents an innovative framework that\nstrategically tailors LLMs for streamlined context processing by harnessing the\nsynergies among natural language summarization, soft prompt compression, and\naugmented utility preservation mechanisms. Our methodology, dubbed\nSoftPromptComp, amalgamates natural language prompts extracted from\nsummarization methodologies with dynamically generated soft prompts to forge a\nconcise yet semantically robust depiction of protracted contexts. This\ndepiction undergoes further refinement via a weighting mechanism optimizing\ninformation retention and utility for subsequent tasks. We substantiate that\nour framework markedly diminishes computational overhead and enhances LLMs'\nefficacy across various benchmarks, while upholding or even augmenting the\ncaliber of the produced content. By amalgamating soft prompt compression with\nsophisticated summarization, SoftPromptComp confronts the dual challenges of\nmanaging lengthy contexts and ensuring model scalability. Our findings point\ntowards a propitious trajectory for augmenting LLMs' applicability and\nefficiency, rendering them more versatile and pragmatic for real-world\napplications. This research enriches the ongoing discourse on optimizing\nlanguage models, providing insights into the potency of soft prompts and\nsummarization techniques as pivotal instruments for the forthcoming generation\nof NLP solutions.", "published": "2024-04-07 15:44:20", "link": "http://arxiv.org/abs/2404.04997v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Shortcut-connected Expert Parallelism for Accelerating\n  Mixture-of-Experts", "abstract": "Expert parallelism has been introduced as a strategy to distribute the\ncomputational workload of sparsely-gated mixture-of-experts (MoE) models across\nmultiple computing devices, facilitating the execution of these increasingly\nlarge-scale models. However, the All-to-All communication intrinsic to expert\nparallelism constitutes a significant overhead, diminishing the MoE models'\nefficiency. Current optimization approaches offer some relief, yet they are\nconstrained by the sequential interdependence of communication and computation\noperations. To address this limitation, we present a novel shortcut-connected\nMoE (ScMoE) architecture with an overlapping parallel strategy, which\neffectively decouples communication from its conventional sequence, allowing\nfor a substantial overlap of 70% to 100% with computation. When compared with\nthe prevalent top-2 MoE architecture, ScMoE demonstrates training speed\nimprovements of 30% and 11%, and inference improvements of 40% and 15%, in our\ndistributed environments with PCIe and NVLink hardware, respectively, where\ncommunication constitutes 60% and 15% of the total MoE time consumption.\nBuilding on the ScMoE architecture, we further implement an expert offloading\nstrategy to facilitate memory-limited inference, optimizing latency through the\noverlap of expert migration. Additionally, extensive experiments and\ntheoretical analyses indicate that ScMoE not only achieves comparable but in\nsome instances surpasses the model quality of existing approaches.", "published": "2024-04-07 17:17:23", "link": "http://arxiv.org/abs/2404.05019v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "DREAM: Improving Video-Text Retrieval Through Relevance-Based\n  Augmentation Using Large Foundation Models", "abstract": "Recent progress in video-text retrieval has been driven largely by\nadvancements in model architectures and training strategies. However, the\nrepresentation learning capabilities of videotext retrieval models remain\nconstrained by lowquality and limited training data annotations. To address\nthis issue, we present a novel ViDeoText Retrieval Paradigm with\nRElevance-based AugMentation, namely DREAM, which enhances video and text data\nusing large foundation models to learn more generalized features. Specifically,\nwe first adopt a simple augmentation method, which generates self-similar data\nby randomly duplicating or dropping subwords and frames. In addition, inspired\nby the recent advancement in visual and language generative models, we propose\na more robust augmentation method through textual paraphrasing and video\nstylization using large language models (LLMs) and visual generative models\n(VGMs). To further enrich video and text information, we propose a\nrelevance-based augmentation method, where LLMs and VGMs generate and integrate\nnew relevant information into the original data. Leveraging this enriched data,\nextensive experiments on several video-text retrieval benchmarks demonstrate\nthe superiority of DREAM over existing methods.", "published": "2024-04-07 21:46:47", "link": "http://arxiv.org/abs/2404.05083v2", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Note on LoRA", "abstract": "LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently\nadapting Large Language Models (LLMs) with remarkable simplicity and efficacy.\nThis note extends the original LoRA paper by offering new perspectives that\nwere not initially discussed and presents a series of insights for deploying\nLoRA at scale. Without introducing new experiments, we aim to improve the\nunderstanding and application of LoRA.", "published": "2024-04-07 22:00:50", "link": "http://arxiv.org/abs/2404.05086v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Bad is Training on Synthetic Data? A Statistical Analysis of\n  Language Model Collapse", "abstract": "The phenomenon of model collapse, introduced in (Shumailov et al., 2023),\nrefers to the deterioration in performance that occurs when new models are\ntrained on synthetic data generated from previously trained models. This\nrecursive training loop makes the tails of the original distribution disappear,\nthereby making future-generation models forget about the initial (real)\ndistribution. With the aim of rigorously understanding model collapse in\nlanguage models, we consider in this paper a statistical model that allows us\nto characterize the impact of various recursive training scenarios.\nSpecifically, we demonstrate that model collapse cannot be avoided when\ntraining solely on synthetic data. However, when mixing both real and synthetic\ndata, we provide an estimate of a maximal amount of synthetic data below which\nmodel collapse can eventually be avoided. Our theoretical conclusions are\nfurther supported by empirical validations.", "published": "2024-04-07 22:15:13", "link": "http://arxiv.org/abs/2404.05090v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PMG : Personalized Multimodal Generation with Large Language Models", "abstract": "The emergence of large language models (LLMs) has revolutionized the\ncapabilities of text comprehension and generation. Multi-modal generation\nattracts great attention from both the industry and academia, but there is\nlittle work on personalized generation, which has important applications such\nas recommender systems. This paper proposes the first method for personalized\nmultimodal generation using LLMs, showcases its applications and validates its\nperformance via an extensive experimental study on two datasets. The proposed\nmethod, Personalized Multimodal Generation (PMG for short) first converts user\nbehaviors (e.g., clicks in recommender systems or conversations with a virtual\nassistant) into natural language to facilitate LLM understanding and extract\nuser preference descriptions. Such user preferences are then fed into a\ngenerator, such as a multimodal LLM or diffusion model, to produce personalized\ncontent. To capture user preferences comprehensively and accurately, we propose\nto let the LLM output a combination of explicit keywords and implicit\nembeddings to represent user preferences. Then the combination of keywords and\nembeddings are used as prompts to condition the generator. We optimize a\nweighted sum of the accuracy and preference scores so that the generated\ncontent has a good balance between them. Compared to a baseline method without\npersonalization, PMG has a significant improvement on personalization for up to\n8% in terms of LPIPS while retaining the accuracy of generation.", "published": "2024-04-07 03:05:57", "link": "http://arxiv.org/abs/2404.08677v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Your Finetuned Large Language Model is Already a Powerful\n  Out-of-distribution Detector", "abstract": "We revisit the likelihood ratio between a pretrained large language model\n(LLM) and its finetuned variant as a criterion for out-of-distribution (OOD)\ndetection. The intuition behind such a criterion is that, the pretrained LLM\nhas the prior knowledge about OOD data due to its large amount of training\ndata, and once finetuned with the in-distribution data, the LLM has sufficient\nknowledge to distinguish their difference. Leveraging the power of LLMs, we\nshow that, the likelihood ratio can serve as an effective OOD detection\ncriterion. Moreover, we apply the proposed LLM-based likelihood ratio to detect\nOOD questions in question-answering (QA) systems, which can be used to improve\nthe performance of specialized LLMs for general questions. Given that\nlikelihood can be easily obtained by the loss functions within contemporary\nneural network frameworks, it is straightforward to implement this approach in\npractice. Since both the pretrained LLMs and its various finetuned models are\nwidely available from online platforms such as Hugging Face, our proposed\ncriterion can be effortlessly incorporated for OOD detection without the need\nfor further training. We conduct comprehensive evaluation across on multiple\nsettings, including far OOD, near OOD, spam detection, and QA scenarios, to\ndemonstrate the effectiveness of the method. Code can be found at\nhttps://github.com/andiac/LLMOODratio", "published": "2024-04-07 10:32:49", "link": "http://arxiv.org/abs/2404.08679v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A geometric framework for interstellar discourse on fundamental physical\n  structures", "abstract": "This paper considers the possibility that abstract thinking and advanced\nsynthesis skills might encourage extraterrestrial civilizations to accept\ncommunication with mankind on Earth. For this purpose, a notation not relying\nupon the use of alphabet and numbers is proposed, in order to denote just some\nbasic geometric structures of current physical theories: vector fields,\none-form fields, and tensor fields of arbitrary order. An advanced civilization\nmight appreciate the way here proposed to achieve a concise description of\nelectromagnetism and general relativity, and hence it might accept the\nchallenge of responding to our signals. The abstract symbols introduced in this\npaper to describe the basic structures of physical theories are encoded into\nblack and white bitmap images that can be easily converted into short bit\nsequences and modulated on a carrier wave for radio transmission.", "published": "2024-04-07 16:03:01", "link": "http://arxiv.org/abs/2405.02314v1", "categories": ["cs.IT", "cs.CL", "math.IT"], "primary_category": "cs.IT"}
{"title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis", "abstract": "Audio deepfake detection (ADD) is essential for preventing the misuse of\nsynthetic voices that may infringe on personal rights and privacy. Recent\nzero-shot text-to-speech (TTS) models pose higher risks as they can clone\nvoices with a single utterance. However, the existing ADD datasets are\noutdated, leading to suboptimal generalization of detection models. In this\npaper, we construct a new cross-domain ADD dataset comprising over 300 hours of\nspeech data that is generated by five advanced zero-shot TTS models. To\nsimulate real-world scenarios, we employ diverse attack methods and audio\nprompts from different datasets. Experiments show that, through novel\nattack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve\nequal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate\nour models' outstanding few-shot ADD ability by fine-tuning with just one\nminute of target-domain data. Nonetheless, neural codec compressors greatly\naffect the detection accuracy, necessitating further research.", "published": "2024-04-07 10:10:15", "link": "http://arxiv.org/abs/2404.04904v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Test-Time Training for Depression Detection", "abstract": "Previous works on depression detection use datasets collected in similar\nenvironments to train and test the models. In practice, however, the train and\ntest distributions cannot be guaranteed to be identical. Distribution shifts\ncan be introduced due to variations such as recording environment (e.g.,\nbackground noise) and demographics (e.g., gender, age, etc). Such\ndistributional shifts can surprisingly lead to severe performance degradation\nof the depression detection models. In this paper, we analyze the application\nof test-time training (TTT) to improve robustness of models trained for\ndepression detection. When compared to regular testing of the models, we find\nTTT can significantly improve the robustness of the model under a variety of\ndistributional shifts introduced due to: (a) background-noise, (b) gender-bias,\nand (c) data collection and curation procedure (i.e., train and test samples\nare from separate datasets).", "published": "2024-04-07 20:50:13", "link": "http://arxiv.org/abs/2404.05071v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Gull: A Generative Multifunctional Audio Codec", "abstract": "We introduce Gull, a generative multifunctional audio codec. Gull is a\ngeneral purpose neural audio compression and decompression model which can be\napplied to a wide range of tasks and applications such as real-time\ncommunication, audio super-resolution, and codec language models. The key\ncomponents of Gull include (1) universal-sample-rate modeling via subband\nmodeling schemes motivated by recent progress in audio source separation, (2)\ngain-shape representations motivated by traditional audio codecs, (3) improved\nresidual vector quantization modules, (4) elastic decoder network that enables\nuser-defined model size and complexity during inference time, (5) built-in\nability for audio super-resolution without the increase of bitrate. We compare\nGull with existing traditional and neural audio codecs and show that Gull is\nable to achieve on par or better performance across various sample rates,\nbitrates and model complexities in both subjective and objective evaluation\nmetrics.", "published": "2024-04-07 12:57:46", "link": "http://arxiv.org/abs/2404.04947v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
