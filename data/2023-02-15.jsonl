{"title": "The Capacity for Moral Self-Correction in Large Language Models", "abstract": "We test the hypothesis that language models trained with reinforcement\nlearning from human feedback (RLHF) have the capability to \"morally\nself-correct\" -- to avoid producing harmful outputs -- if instructed to do so.\nWe find strong evidence in support of this hypothesis across three different\nexperiments, each of which reveal different facets of moral self-correction. We\nfind that the capability for moral self-correction emerges at 22B model\nparameters, and typically improves with increasing model size and RLHF\ntraining. We believe that at this level of scale, language models obtain two\ncapabilities that they can use for moral self-correction: (1) they can follow\ninstructions and (2) they can learn complex normative concepts of harm like\nstereotyping, bias, and discrimination. As such, they can follow instructions\nto avoid certain kinds of morally harmful outputs. We believe our results are\ncause for cautious optimism regarding the ability to train language models to\nabide by ethical principles.", "published": "2023-02-15 04:25:40", "link": "http://arxiv.org/abs/2302.07459v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Whats New? Identifying the Unfolding of New Events in Narratives", "abstract": "Narratives include a rich source of events unfolding over time and context.\nAutomatic understanding of these events provides a summarised comprehension of\nthe narrative for further computation (such as reasoning). In this paper, we\nstudy the Information Status (IS) of the events and propose a novel challenging\ntask: the automatic identification of new events in a narrative. We define an\nevent as a triplet of subject, predicate, and object. The event is categorized\nas new with respect to the discourse context and whether it can be inferred\nthrough commonsense reasoning. We annotated a publicly available corpus of\nnarratives with the new events at sentence level using human annotators. We\npresent the annotation protocol and study the quality of the annotation and the\ndifficulty of the task. We publish the annotated dataset, annotation materials,\nand machine learning baseline models for the task of new event extraction for\nnarrative understanding.", "published": "2023-02-15 15:54:01", "link": "http://arxiv.org/abs/2302.07748v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the Instability of Fine-Tuning", "abstract": "Fine-tuning pre-trained language models on downstream tasks with varying\nrandom seeds has been shown to be unstable, especially on small datasets. Many\nprevious studies have investigated this instability and proposed methods to\nmitigate it. However, most studies only used the standard deviation of\nperformance scores (SD) as their measure, which is a narrow characterization of\ninstability. In this paper, we analyze SD and six other measures quantifying\ninstability at different levels of granularity. Moreover, we propose a\nsystematic framework to evaluate the validity of these measures. Finally, we\nanalyze the consistency and difference between different measures by\nreassessing existing instability mitigation methods. We hope our results will\ninform the development of better measurements of fine-tuning instability.", "published": "2023-02-15 16:55:15", "link": "http://arxiv.org/abs/2302.07778v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmented Language Models: a Survey", "abstract": "This survey reviews works in which language models (LMs) are augmented with\nreasoning skills and the ability to use tools. The former is defined as\ndecomposing a potentially complex task into simpler subtasks while the latter\nconsists in calling external modules such as a code interpreter. LMs can\nleverage these augmentations separately or in combination via heuristics, or\nlearn to do so from demonstrations. While adhering to a standard missing tokens\nprediction objective, such augmented LMs can use various, possibly\nnon-parametric external modules to expand their context processing ability,\nthus departing from the pure language modeling paradigm. We therefore refer to\nthem as Augmented Language Models (ALMs). The missing token objective allows\nALMs to learn to reason, use tools, and even act, while still performing\nstandard natural language tasks and even outperforming most regular LMs on\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\nconclude that this new research direction has the potential to address common\nlimitations of traditional LMs such as interpretability, consistency, and\nscalability issues.", "published": "2023-02-15 18:25:52", "link": "http://arxiv.org/abs/2302.07842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speculative Decoding with Big Little Decoder", "abstract": "The recent emergence of Large Language Models based on the Transformer\narchitecture has enabled dramatic advancements in the field of Natural Language\nProcessing. However, these models have long inference latency, which limits\ntheir deployment and makes them prohibitively expensive for various real-time\napplications. The inference latency is further exacerbated by autoregressive\ngenerative tasks, as models need to run iteratively to generate tokens\nsequentially without leveraging token-level parallelization. To address this,\nwe propose Big Little Decoder (BiLD), a framework that can improve inference\nefficiency and latency for a wide range of text generation applications. The\nBiLD framework contains two models with different sizes that collaboratively\ngenerate text. The small model runs autoregressively to generate text with a\nlow inference cost, and the large model is only invoked occasionally to refine\nthe small model's inaccurate predictions in a non-autoregressive manner. To\ncoordinate the small and large models, BiLD introduces two simple yet effective\npolicies: (1) the fallback policy that determines when to hand control over to\nthe large model; and (2) the rollback policy that determines when the large\nmodel needs to correct the small model's inaccurate predictions. To evaluate\nour framework across different tasks and models, we apply BiLD to various text\ngeneration scenarios encompassing machine translation on IWSLT 2017 De-En and\nWMT 2014 De-En, and summarization on XSUM and CNN/DailyMail. On an NVIDIA T4\nGPU, our framework achieves a speedup of up to 2.12x speedup with minimal\ngeneration quality degradation. Furthermore, our framework is fully\nplug-and-play and can be applied without any modifications in the training\nprocess or model architecture. Our code is open-sourced", "published": "2023-02-15 18:55:29", "link": "http://arxiv.org/abs/2302.07863v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meeting the Needs of Low-Resource Languages: The Value of Automatic\n  Alignments via Pretrained Models", "abstract": "Large multilingual models have inspired a new class of word alignment\nmethods, which work well for the model's pretraining languages. However, the\nlanguages most in need of automatic alignment are low-resource and, thus, not\ntypically included in the pretraining data. In this work, we ask: How do modern\naligners perform on unseen languages, and are they better than traditional\nmethods? We contribute gold-standard alignments for Bribri--Spanish,\nGuarani--Spanish, Quechua--Spanish, and Shipibo-Konibo--Spanish. With these, we\nevaluate state-of-the-art aligners with and without model adaptation to the\ntarget language. Finally, we also evaluate the resulting alignments\nextrinsically through two downstream tasks: named entity recognition and\npart-of-speech tagging. We find that although transformer-based methods\ngenerally outperform traditional models, the two classes of approach remain\ncompetitive with each other.", "published": "2023-02-15 19:06:17", "link": "http://arxiv.org/abs/2302.07912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree-Based Representation and Generation of Natural and Mathematical\n  Language", "abstract": "Mathematical language in scientific communications and educational scenarios\nis important yet relatively understudied compared to natural languages. Recent\nworks on mathematical language focus either on representing stand-alone\nmathematical expressions, especially in their natural tree format, or\nmathematical reasoning in pre-trained natural language models. Existing works\non jointly modeling and generating natural and mathematical languages simply\ntreat mathematical expressions as text, without accounting for the rigid\nstructural properties of mathematical expressions. In this paper, we propose a\nseries of modifications to existing language models to jointly represent and\ngenerate text and math: representing mathematical expressions as sequences of\nnode tokens in their operator tree format, using math symbol and tree position\nembeddings to preserve the semantic and structural properties of mathematical\nexpressions, and using a constrained decoding method to generate mathematically\nvalid expressions. We ground our modifications in GPT-2, resulting in a model\nMathGPT, and demonstrate that it outperforms baselines on mathematical\nexpression generation tasks.", "published": "2023-02-15 22:38:34", "link": "http://arxiv.org/abs/2302.07974v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable\n  Dense Retrieval", "abstract": "Various techniques have been developed in recent years to improve dense\nretrieval (DR), such as unsupervised contrastive learning and pseudo-query\ngeneration. Existing DRs, however, often suffer from effectiveness tradeoffs\nbetween supervised and zero-shot retrieval, which some argue was due to the\nlimited model capacity. We contradict this hypothesis and show that a\ngeneralizable DR can be trained to achieve high accuracy in both supervised and\nzero-shot retrieval without increasing model size. In particular, we\nsystematically examine the contrastive learning of DRs, under the framework of\nData Augmentation (DA). Our study shows that common DA practices such as query\naugmentation with generative models and pseudo-relevance label creation using a\ncross-encoder, are often inefficient and sub-optimal. We hence propose a new DA\napproach with diverse queries and sources of supervision to progressively train\na generalizable DR. As a result, DRAGON, our dense retriever trained with\ndiverse augmentation, is the first BERT-base-sized DR to achieve\nstate-of-the-art effectiveness in both supervised and zero-shot evaluations and\neven competes with models using more complex late interaction (ColBERTv2 and\nSPLADE++).", "published": "2023-02-15 03:53:26", "link": "http://arxiv.org/abs/2302.07452v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DP-BART for Privatized Text Rewriting under Local Differential Privacy", "abstract": "Privatized text rewriting with local differential privacy (LDP) is a recent\napproach that enables sharing of sensitive textual documents while formally\nguaranteeing privacy protection to individuals. However, existing systems face\nseveral issues, such as formal mathematical flaws, unrealistic privacy\nguarantees, privatization of only individual words, as well as a lack of\ntransparency and reproducibility. In this paper, we propose a new system\n'DP-BART' that largely outperforms existing LDP systems. Our approach uses a\nnovel clipping method, iterative pruning, and further training of internal\nrepresentations which drastically reduces the amount of noise required for DP\nguarantees. We run experiments on five textual datasets of varying sizes,\nrewriting them at different privacy guarantees and evaluating the rewritten\ntexts on downstream text classification tasks. Finally, we thoroughly discuss\nthe privatized text rewriting approach and its limitations, including the\nproblem of the strict text adjacency constraint in the LDP paradigm that leads\nto the high noise requirement.", "published": "2023-02-15 13:07:34", "link": "http://arxiv.org/abs/2302.07636v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "On graph-based reentrancy-free semantic parsing", "abstract": "We propose a novel graph-based approach for semantic parsing that resolves\ntwo problems observed in the literature: (1) seq2seq models fail on\ncompositional generalization tasks; (2) previous work using phrase structure\nparsers cannot cover all the semantic parses observed in treebanks. We prove\nthat both MAP inference and latent tag anchoring (required for\nweakly-supervised learning) are NP-hard problems. We propose two optimization\nalgorithms based on constraint smoothing and conditional gradient to\napproximately solve these inference problems. Experimentally, our approach\ndelivers state-of-the-art results on Geoquery, Scan and Clevr, both for i.i.d.\nsplits and for splits that test for compositional generalization.", "published": "2023-02-15 14:14:09", "link": "http://arxiv.org/abs/2302.07679v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dictionary-based Phrase-level Prompting of Large Language Models for\n  Machine Translation", "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT)\nabilities via prompting, even though they were not explicitly trained for this\ntask. However, even given the incredible quantities of data they are trained\non, LLMs can struggle to translate inputs with rare words, which are common in\nlow resource or domain transfer scenarios. We show that LLM prompting can\nprovide an effective solution for rare words as well, by using prior knowledge\nfrom bilingual dictionaries to provide control hints in the prompts. We propose\na novel method, DiPMT, that provides a set of possible translations for a\nsubset of the input words, thereby enabling fine-grained phrase-level prompted\ncontrol of the LLM. Extensive experiments show that DiPMT outperforms the\nbaseline both in low-resource MT, as well as for out-of-domain MT. We further\nprovide a qualitative analysis of the benefits and limitations of this\napproach, including the overall level of controllability that is achieved.", "published": "2023-02-15 18:46:42", "link": "http://arxiv.org/abs/2302.07856v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Deep Neural Networks Capture Compositionality in Arithmetic\n  Reasoning?", "abstract": "Compositionality is a pivotal property of symbolic reasoning. However, how\nwell recent neural models capture compositionality remains underexplored in the\nsymbolic reasoning tasks. This study empirically addresses this question by\nsystematically examining recently published pre-trained seq2seq models with a\ncarefully controlled dataset of multi-hop arithmetic symbolic reasoning. We\nintroduce a skill tree on compositionality in arithmetic symbolic reasoning\nthat defines the hierarchical levels of complexity along with three\ncompositionality dimensions: systematicity, productivity, and substitutivity.\nOur experiments revealed that among the three types of composition, the models\nstruggled most with systematicity, performing poorly even with relatively\nsimple compositions. That difficulty was not resolved even after training the\nmodels with intermediate reasoning steps.", "published": "2023-02-15 18:59:04", "link": "http://arxiv.org/abs/2302.07866v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Commonsense Reasoning for Conversational AI: A Survey of the State of\n  the Art", "abstract": "Large, transformer-based pretrained language models like BERT, GPT, and T5\nhave demonstrated a deep understanding of contextual semantics and language\nsyntax. Their success has enabled significant advances in conversational AI,\nincluding the development of open-dialogue systems capable of coherent, salient\nconversations which can answer questions, chat casually, and complete tasks.\nHowever, state-of-the-art models still struggle with tasks that involve higher\nlevels of reasoning - including commonsense reasoning that humans find trivial.\nThis paper presents a survey of recent conversational AI research focused on\ncommonsense reasoning. The paper lists relevant training datasets and describes\nthe primary approaches to include commonsense in conversational AI. The paper\nalso discusses benchmarks used for evaluating commonsense in conversational AI\nproblems. Finally, the paper presents preliminary observations of the limited\ncommonsense capabilities of two state-of-the-art open dialogue models,\nBlenderBot3 and LaMDA, and its negative effect on natural interactions. These\nobservations further motivate research on commonsense reasoning in\nconversational AI.", "published": "2023-02-15 19:55:57", "link": "http://arxiv.org/abs/2302.07926v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Envisioning the Next-Gen Document Reader", "abstract": "People read digital documents on a daily basis to share, exchange, and\nunderstand information in electronic settings. However, current document\nreaders create a static, isolated reading experience, which does not support\nusers' goals of gaining more knowledge and performing additional tasks through\ndocument interaction. In this work, we present our vision for the next-gen\ndocument reader that strives to enhance user understanding and create a more\nconnected, trustworthy information experience. We describe 18 NLP-powered\nfeatures to add to existing document readers and propose a novel plug-in\nmarketplace that allows users to further customize their reading experience, as\ndemonstrated through 3 exploratory UI prototypes available at\nhttps://github.com/catherinesyeh/nextgen-prototypes", "published": "2023-02-15 06:43:12", "link": "http://arxiv.org/abs/2302.07492v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word class representations spontaneously emerge in a deep neural network\n  trained on next word prediction", "abstract": "How do humans learn language, and can the first language be learned at all?\nThese fundamental questions are still hotly debated. In contemporary\nlinguistics, there are two major schools of thought that give completely\nopposite answers. According to Chomsky's theory of universal grammar, language\ncannot be learned because children are not exposed to sufficient data in their\nlinguistic environment. In contrast, usage-based models of language assume a\nprofound relationship between language structure and language use. In\nparticular, contextual mental processing and mental representations are assumed\nto have the cognitive capacity to capture the complexity of actual language use\nat all levels. The prime example is syntax, i.e., the rules by which words are\nassembled into larger units such as sentences. Typically, syntactic rules are\nexpressed as sequences of word classes. However, it remains unclear whether\nword classes are innate, as implied by universal grammar, or whether they\nemerge during language acquisition, as suggested by usage-based approaches.\nHere, we address this issue from a machine learning and natural language\nprocessing perspective. In particular, we trained an artificial deep neural\nnetwork on predicting the next word, provided sequences of consecutive words as\ninput. Subsequently, we analyzed the emerging activation patterns in the hidden\nlayers of the neural network. Strikingly, we find that the internal\nrepresentations of nine-word input sequences cluster according to the word\nclass of the tenth word to be predicted as output, even though the neural\nnetwork did not receive any explicit information about syntactic rules or word\nclasses during training. This surprising result suggests, that also in the\nhuman brain, abstract representational categories such as word classes may\nnaturally emerge as a consequence of predictive coding and processing during\nlanguage acquisition.", "published": "2023-02-15 11:02:50", "link": "http://arxiv.org/abs/2302.07588v1", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "NL2CMD: An Updated Workflow for Natural Language to Bash Commands\n  Translation", "abstract": "Translating natural language into Bash Commands is an emerging research field\nthat has gained attention in recent years. Most efforts have focused on\nproducing more accurate translation models. To the best of our knowledge, only\ntwo datasets are available, with one based on the other. Both datasets involve\nscraping through known data sources (through platforms like stack overflow,\ncrowdsourcing, etc.) and hiring experts to validate and correct either the\nEnglish text or Bash Commands. This paper provides two contributions to\nresearch on synthesizing Bash Commands from scratch. First, we describe a\nstate-of-the-art translation model used to generate Bash Commands from the\ncorresponding English text. Second, we introduce a new NL2CMD dataset that is\nautomatically generated, involves minimal human intervention, and is over six\ntimes larger than prior datasets. Since the generation pipeline does not rely\non existing Bash Commands, the distribution and types of commands can be custom\nadjusted. We evaluate the performance of ChatGPT on this task and discuss the\npotential of using it as a data generator. Our empirical results show how the\nscale and diversity of our dataset can offer unique opportunities for semantic\nparsing researchers.", "published": "2023-02-15 18:31:36", "link": "http://arxiv.org/abs/2302.07845v3", "categories": ["cs.CL", "cs.AI", "cs.PF"], "primary_category": "cs.CL"}
{"title": "\u00c0-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable\n  Prompting", "abstract": "We introduce \\`A-la-carte Prompt Tuning (APT), a transformer-based scheme to\ntune prompts on distinct data so that they can be arbitrarily composed at\ninference time. The individual prompts can be trained in isolation, possibly on\ndifferent devices, at different times, and on different distributions or\ndomains. Furthermore each prompt only contains information about the subset of\ndata it was exposed to during training. During inference, models can be\nassembled based on arbitrary selections of data sources, which we call\n\"\\`a-la-carte learning\". \\`A-la-carte learning enables constructing bespoke\nmodels specific to each user's individual access rights and preferences. We can\nadd or remove information from the model by simply adding or removing the\ncorresponding prompts without retraining from scratch. We demonstrate that\n\\`a-la-carte built models achieve accuracy within $5\\%$ of models trained on\nthe union of the respective sources, with comparable cost in terms of training\nand inference time. For the continual learning benchmarks Split CIFAR-100 and\nCORe50, we achieve state-of-the-art performance.", "published": "2023-02-15 23:51:09", "link": "http://arxiv.org/abs/2302.07994v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and\n  Spatial Reasoning", "abstract": "We conduct a pilot study selectively evaluating the cognitive abilities\n(decision making and spatial reasoning) of two recently released generative\ntransformer models, ChatGPT and DALL-E 2. Input prompts were constructed\nfollowing neutral a priori guidelines, rather than adversarial intent. Post hoc\nqualitative analysis of the outputs shows that DALL-E 2 is able to generate at\nleast one correct image for each spatial reasoning prompt, but most images\ngenerated are incorrect (even though the model seems to have a clear\nunderstanding of the objects mentioned in the prompt). Similarly, in evaluating\nChatGPT on the rationality axioms developed under the classical Von\nNeumann-Morgenstern utility theorem, we find that, although it demonstrates\nsome level of rational decision-making, many of its decisions violate at least\none of the axioms even under reasonable constructions of preferences, bets, and\ndecision-making prompts. ChatGPT's outputs on such problems generally tended to\nbe unpredictable: even as it made irrational decisions (or employed an\nincorrect reasoning process) for some simpler decision-making problems, it was\nable to draw correct conclusions for more complex bet structures. We briefly\ncomment on the nuances and challenges involved in scaling up such a 'cognitive'\nevaluation or conducting it with a closed set of answer keys ('ground truth'),\ngiven that these models are inherently generative and open-ended in responding\nto prompts.", "published": "2023-02-15 05:04:49", "link": "http://arxiv.org/abs/2302.09068v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Confidence Score Based Speaker Adaptation of Conformer Speech\n  Recognition Systems", "abstract": "Speaker adaptation techniques provide a powerful solution to customise\nautomatic speech recognition (ASR) systems for individual users. Practical\napplication of unsupervised model-based speaker adaptation techniques to data\nintensive end-to-end ASR systems is hindered by the scarcity of speaker-level\ndata and performance sensitivity to transcription errors. To address these\nissues, a set of compact and data efficient speaker-dependent (SD) parameter\nrepresentations are used to facilitate both speaker adaptive training and\ntest-time unsupervised speaker adaptation of state-of-the-art Conformer ASR\nsystems. The sensitivity to supervision quality is reduced using a confidence\nscore-based selection of the less erroneous subset of speaker-level adaptation\ndata. Two lightweight confidence score estimation modules are proposed to\nproduce more reliable confidence scores. The data sparsity issue, which is\nexacerbated by data selection, is addressed by modelling the SD parameter\nuncertainty using Bayesian learning. Experiments on the benchmark 300-hour\nSwitchboard and the 233-hour AMI datasets suggest that the proposed confidence\nscore-based adaptation schemes consistently outperformed the baseline\nspeaker-independent (SI) Conformer model and conventional non-Bayesian, point\nestimate-based adaptation using no speaker data selection. Similar consistent\nperformance improvements were retained after external Transformer and LSTM\nlanguage model rescoring. In particular, on the 300-hour Switchboard corpus,\nstatistically significant WER reductions of 1.0%, 1.3%, and 1.4% absolute\n(9.5%, 10.9%, and 11.3% relative) were obtained over the baseline SI Conformer\non the NIST Hub5'00, RT02, and RT03 evaluation sets respectively. Similar WER\nreductions of 2.7% and 3.3% absolute (8.9% and 10.2% relative) were also\nobtained on the AMI development and evaluation sets.", "published": "2023-02-15 08:29:42", "link": "http://arxiv.org/abs/2302.07521v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised classification to improve the quality of a bird song\n  recording dataset", "abstract": "Open audio databases such as Xeno-Canto are widely used to build datasets to\nexplore bird song repertoire or to train models for automatic bird sound\nclassification by deep learning algorithms. However, such databases suffer from\nthe fact that bird sounds are weakly labelled: a species name is attributed to\neach audio recording without timestamps that provide the temporal localization\nof the bird song of interest. Manual annotations can solve this issue, but they\nare time consuming, expert-dependent, and cannot run on large datasets. Another\nsolution consists in using a labelling function that automatically segments\naudio recordings before assigning a label to each segmented audio sample.\nAlthough labelling functions were introduced to expedite strong label\nassignment, their classification performance remains mostly unknown. To address\nthis issue and reduce label noise (wrong label assignment) in large bird song\ndatasets, we introduce a data-centric novel labelling function composed of\nthree successive steps: 1) time-frequency sound unit segmentation, 2) feature\ncomputation for each sound unit, and 3) classification of each sound unit as\nbird song or noise with either an unsupervised DBSCAN algorithm or the\nsupervised BirdNET neural network. The labelling function was optimized,\nvalidated, and tested on the songs of 44 West-Palearctic common bird species.\nWe first showed that the segmentation of bird songs alone aggregated from 10%\nto 83% of label noise depending on the species. We also demonstrated that our\nlabelling function was able to significantly reduce the initial label noise\npresent in the dataset by up to a factor of three. Finally, we discuss\ndifferent opportunities to design suitable labelling functions to build\nhigh-quality animal vocalizations with minimum expert annotation effort.", "published": "2023-02-15 10:01:58", "link": "http://arxiv.org/abs/2302.07560v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multi-Channel Target Speaker Extraction with Refinement: The WavLab\n  Submission to the Second Clarity Enhancement Challenge", "abstract": "This paper describes our submission to the Second Clarity Enhancement\nChallenge (CEC2), which consists of target speech enhancement for hearing-aid\n(HA) devices in noisy-reverberant environments with multiple interferers such\nas music and competing speakers.\n  Our approach builds upon the powerful iterative neural/beamforming\nenhancement (iNeuBe) framework introduced in our recent work, and this paper\nextends it for target speaker extraction. We therefore name the proposed\napproach as iNeuBe-X, where the X stands for extraction. To address the\nchallenges encountered in the CEC2 setting, we introduce four major novelties:\n  (1) we extend the state-of-the-art TF-GridNet model, originally designed for\nmonaural speaker separation, for multi-channel, causal speech enhancement, and\nlarge improvements are observed by replacing the TCNDenseNet used in iNeuBe\nwith this new architecture;\n  (2) we leverage a recent dual window size approach with future-frame\nprediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic\nlatency required by CEC2;\n  (3) we introduce a novel speaker-conditioning branch for TF-GridNet to\nachieve target speaker extraction;\n  (4) we propose a fine-tuning step, where we compute an additional loss with\nrespect to the target speaker signal compensated with the listener audiogram.\n  Without using external data, on the official development set our best model\nreaches a hearing-aid speech perception index (HASPI) score of 0.942 and a\nscale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB.\nThese results are promising given the fact that the CEC2 data is extremely\nchallenging (e.g., on the development set the mixture SI-SDR is -12.3 dB). A\ndemo of our submitted system is available at WAVLab CEC2 demo.", "published": "2023-02-15 20:08:21", "link": "http://arxiv.org/abs/2302.07928v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Fast and Blind Speech Copy-Move Detection and Localization in Noise", "abstract": "Copy-move forgery on speech (CMF), coupled with post-processing techniques,\npresents a great challenge to the forensic detection and localization of\ntampered areas. Most of the existing CMF detection approaches necessitate\npre-segmentation of speech to facilitate similarity calculations among these\nsegments. However, these approaches usually suffer from the problems of\nuncontrollable computational complexity and sensitivity to the presence of a\nword that is read multiple times within a speech recording. To address these\nissues, we propose a local feature tensors-based CMF detection algorithm that\ncan transform duplicate detection and localization problems into a special\ntensor-matching procedure, accompanied by complete theoretical analysis as\nsupport. Through extensive experimentation, we have demonstrated that our\nmethod exhibits computational efficiency and robustness against post-processing\ntechniques. Notably, it can effectively and blindly detect tampered segments,\neven those as short as a fractional second. These advantages highlight the\npromising potential of our approach for practical applications.", "published": "2023-02-15 10:57:31", "link": "http://arxiv.org/abs/2302.07584v4", "categories": ["eess.AS", "cs.IT", "cs.SD", "eess.SP", "math.IT"], "primary_category": "eess.AS"}
