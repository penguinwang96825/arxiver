{"title": "Correspondence Factor Analysis of Big Data Sets: A Case Study of 30\n  Million Words; and Contrasting Analytics using Apache Solr and Correspondence\n  Analysis in R", "abstract": "We consider a large number of text data sets. These are cooking recipes. Term\ndistribution and other distributional properties of the data are investigated.\nOur aim is to look at various analytical approaches which allow for mining of\ninformation on both high and low detail scales. Metric space embedding is\nfundamental to our interest in the semantic properties of this data. We\nconsider the projection of all data into analyses of aggregated versions of the\ndata. We contrast that with projection of aggregated versions of the data into\nanalyses of all the data. Analogously for the term set, we look at analysis of\nselected terms. We also look at inherent term associations such as between\nsingular and plural. In addition to our use of Correspondence Analysis in R,\nfor latent semantic space mapping, we also use Apache Solr. Setting up the Solr\nserver and carrying out querying is described. A further novelty is that\nquerying is supported in Solr based on the principal factor plane mapping of\nall the data. This uses a bounding box query, based on factor projections.", "published": "2015-07-06 16:32:52", "link": "http://arxiv.org/abs/1507.01529v1", "categories": ["cs.CL", "62H25, 62.07", "G.3; H.2.8"], "primary_category": "cs.CL"}
{"title": "Reflections on Sentiment/Opinion Analysis", "abstract": "In this paper, we described possible directions for deeper understanding,\nhelping bridge the gap between psychology / cognitive science and computational\napproaches in sentiment/opinion analysis literature. We focus on the opinion\nholder's underlying needs and their resultant goals, which, in a utilitarian\nmodel of sentiment, provides the basis for explaining the reason a sentiment\nvalence is held. While these thoughts are still immature, scattered,\nunstructured, and even imaginary, we believe that these perspectives might\nsuggest fruitful avenues for various kinds of future work.", "published": "2015-07-06 22:25:55", "link": "http://arxiv.org/abs/1507.01636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grid Long Short-Term Memory", "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences\nor higher dimensional data such as images. The network differs from existing\ndeep LSTM architectures in that the cells are connected between network layers\nas well as along the spatiotemporal dimensions of the data. The network\nprovides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\nsequence memorization, where it is able to significantly outperform the\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\nprediction benchmark, which is state-of-the-art among neural approaches. In\naddition, we use the Grid LSTM to define a novel two-dimensional translation\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\nsystem on a Chinese-to-English translation task.", "published": "2015-07-06 16:30:05", "link": "http://arxiv.org/abs/1507.01526v3", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
