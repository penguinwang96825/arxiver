{"title": "Extracting and Validating Explanatory Word Archipelagoes using Dual\n  Entropy", "abstract": "The logical connectivity of text is represented by the connectivity of words\nthat form archipelagoes. Here, each archipelago is a sequence of islands of the\noccurrences of a certain word. An island here means the local sequence of\nsentences where the word is emphasized, and an archipelago of a length\ncomparable to the target text is extracted using the co-variation of entropy A\n(the window-based entropy) on the distribution of the word's occurrences with\nthe width of each time window. Then, the logical connectivity of text is\nevaluated on entropy B (the graph-based entropy) computed on the distribution\nof sentences to connected word-clusters obtained on the co-occurrence of words.\nThe results show the parts of the target text with words forming archipelagoes\nextracted on entropy A, without learned or prepared knowledge, form an\nexplanatory part of the text that is of smaller entropy B than the parts\nextracted by the baseline methods.", "published": "2020-02-22 00:35:26", "link": "http://arxiv.org/abs/2002.09581v2", "categories": ["cs.CL", "68W32 (Primary) 68T50, 91F20 (Secondary)"], "primary_category": "cs.CL"}
{"title": "\"Wait, I'm Still Talking!\" Predicting the Dialogue Interaction Behavior\n  Using Imagine-Then-Arbitrate Model", "abstract": "Producing natural and accurate responses like human beings is the ultimate\ngoal of intelligent dialogue agents. So far, most of the past works concentrate\non selecting or generating one pertinent and fluent response according to\ncurrent query and its context. These models work on a one-to-one environment,\nmaking one response to one utterance each round. However, in real human-human\nconversations, human often sequentially sends several short messages for\nreadability instead of a long message in one turn. Thus messages will not end\nwith an explicit ending signal, which is crucial for agents to decide when to\nreply. So the first step for an intelligent dialogue agent is not replying but\ndeciding if it should reply at the moment. To address this issue, in this\npaper, we propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to\nhelp the agent decide whether to wait or to make a response directly. Our\nmethod has two imaginator modules and an arbitrator module. The two imaginators\nwill learn the agent's and user's speaking style respectively, generate\npossible utterances as the input of the arbitrator, combining with dialogue\nhistory. And the arbitrator decides whether to wait or to make a response to\nthe user directly. To verify the performance and effectiveness of our method,\nwe prepared two dialogue datasets and compared our approach with several\npopular models. Experimental results show that our model performs well on\naddressing ending prediction issue and outperforms baseline models.", "published": "2020-02-22 04:05:41", "link": "http://arxiv.org/abs/2002.09616v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Sentence Embedding via Semantic Subspace Analysis", "abstract": "A novel sentence embedding method built upon semantic subspace analysis,\ncalled semantic subspace sentence embedding (S3E), is proposed in this work.\nGiven the fact that word embeddings can capture semantic relationship while\nsemantically similar words tend to form semantic groups in a high-dimensional\nembedding space, we develop a sentence representation scheme by analyzing\nsemantic subspaces of its constituent words. Specifically, we construct a\nsentence model from two aspects. First, we represent words that lie in the same\nsemantic group using the intra-group descriptor. Second, we characterize the\ninteraction between multiple semantic groups with the inter-group descriptor.\nThe proposed S3E method is evaluated on both textual similarity tasks and\nsupervised tasks. Experimental results show that it offers comparable or better\nperformance than the state-of-the-art. The complexity of our S3E method is also\nmuch lower than other parameterized models.", "published": "2020-02-22 04:12:37", "link": "http://arxiv.org/abs/2002.09620v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Copy-Mechanism in Dialogue State Tracking", "abstract": "While several state-of-the-art approaches to dialogue state tracking (DST)\nhave shown promising performances on several benchmarks, there is still a\nsignificant performance gap between seen slot values (i.e., values that occur\nin both training set and test set) and unseen ones (values that occur in\ntraining set but not in test set). Recently, the copy-mechanism has been widely\nused in DST models to handle unseen slot values, which copies slot values from\nuser utterance directly. In this paper, we aim to find out the factors that\ninfluence the generalization ability of a common copy-mechanism model for DST.\nOur key observations include: 1) the copy-mechanism tends to memorize values\nrather than infer them from contexts, which is the primary reason for\nunsatisfactory generalization performance; 2) greater diversity of slot values\nin the training set increase the performance on unseen values but slightly\ndecrease the performance on seen values. Moreover, we propose a simple but\neffective algorithm of data augmentation to train copy-mechanism models, which\naugments the input dataset by copying user utterances and replacing the real\nslot values with randomly generated strings. Users could use two\nhyper-parameters to realize a trade-off between the performances on seen values\nand unseen ones, as well as a trade-off between overall performance and\ncomputational cost. Experimental results on three widely used datasets (WoZ\n2.0, DSTC2, and Multi-WoZ 2.0) show the effectiveness of our approach.", "published": "2020-02-22 05:40:32", "link": "http://arxiv.org/abs/2002.09634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Markov Chain Monte-Carlo Phylogenetic Inference Construction in\n  Computational Historical Linguistics", "abstract": "More and more languages in the world are under study nowadays, as a result,\nthe traditional way of historical linguistics study is facing some challenges.\nFor example, the linguistic comparative research among languages needs manual\nannotation, which becomes more and more impossible with the increasing amount\nof language data coming out all around the world. Although it could hardly\nreplace linguists work, the automatic computational methods have been taken\ninto consideration and it can help people reduce their workload. One of the\nmost important work in historical linguistics is word comparison from different\nlanguages and find the cognate words for them, which means people try to figure\nout if the two languages are related to each other or not. In this paper, I am\ngoing to use computational method to cluster the languages and use Markov Chain\nMonte Carlo (MCMC) method to build the language typology relationship tree\nbased on the clusters.", "published": "2020-02-22 06:01:52", "link": "http://arxiv.org/abs/2002.09637v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation System Selection from Bandit Feedback", "abstract": "Adapting machine translation systems in the real world is a difficult\nproblem. In contrast to offline training, users cannot provide the type of\nfine-grained feedback (such as correct translations) typically used for\nimproving the system. Moreover, different users have different translation\nneeds, and even a single user's needs may change over time.\n  In this work we take a different approach, treating the problem of adaptation\nas one of selection. Instead of adapting a single system, we train many\ntranslation systems using different architectures, datasets, and optimization\nmethods. Using bandit learning techniques on simulated user feedback, we learn\na policy to choose which system to use for a particular translation task. We\nshow that our approach can (1) quickly adapt to address domain changes in\ntranslation tasks, (2) outperform the single best system in mixed-domain\ntranslation tasks, and (3) make effective instance-specific decisions when\nusing contextual bandit strategies.", "published": "2020-02-22 06:54:04", "link": "http://arxiv.org/abs/2002.09646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Effective Global Information via Adaptive Gate Attention\n  for Text Classification", "abstract": "The dominant text classification studies focus on training classifiers using\ntextual instances only or introducing external knowledge (e.g., hand-craft\nfeatures and domain expert knowledge). In contrast, some corpus-level\nstatistical features, like word frequency and distribution, are not well\nexploited. Our work shows that such simple statistical information can enhance\nclassification performance both efficiently and significantly compared with\nseveral baseline models. In this paper, we propose a classifier with gate\nmechanism named Adaptive Gate Attention model with Global Information (AGA+GI),\nin which the adaptive gate mechanism incorporates global statistical features\ninto latent semantic features and the attention layer captures dependency\nrelationship within the sentence. To alleviate the overfitting issue, we\npropose a novel Leaky Dropout mechanism to improve generalization ability and\nperformance stability. Our experiments show that the proposed method can\nachieve better accuracy than CNN-based and RNN-based approaches without global\ninformation on several benchmarks.", "published": "2020-02-22 10:06:37", "link": "http://arxiv.org/abs/2002.09673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Typed Syntactic Dependencies for Targeted Sentiment\n  Classification Using Graph Attention Neural Network", "abstract": "Targeted sentiment classification predicts the sentiment polarity on given\ntarget mentions in input texts. Dominant methods employ neural networks for\nencoding the input sentence and extracting relations between target mentions\nand their contexts. Recently, graph neural network has been investigated for\nintegrating dependency syntax for the task, achieving the state-of-the-art\nresults. However, existing methods do not consider dependency label\ninformation, which can be intuitively useful. To solve the problem, we\ninvestigate a novel relational graph attention network that integrates typed\nsyntactic dependency information. Results on standard benchmarks show that our\nmethod can effectively leverage label information for improving targeted\nsentiment classification performances. Our final model significantly\noutperforms state-of-the-art syntax-based approaches.", "published": "2020-02-22 11:17:16", "link": "http://arxiv.org/abs/2002.09685v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Question Answering Models From Synthetic Data", "abstract": "Question and answer generation is a data augmentation method that aims to\nimprove question answering (QA) models given the limited amount of human\nlabeled data. However, a considerable gap remains between synthetic and\nhuman-generated question-answer pairs. This work aims to narrow this gap by\ntaking advantage of large language models and explores several factors such as\nmodel size, quality of pretrained models, scale of data synthesized, and\nalgorithmic choices. On the SQuAD1.1 question answering task, we achieve higher\naccuracy using solely synthetic questions and answers than when using the\nSQuAD1.1 training set questions alone. Removing access to real Wikipedia data,\nwe synthesize questions and answers from a synthetic corpus generated by an 8.3\nbillion parameter GPT-2 model. With no access to human supervision and only\naccess to other models, we are able to train state of the art question\nanswering networks on entirely model-generated data that achieve 88.4 Exact\nMatch (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our\nmethodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to\nprior work using synthetic data.", "published": "2020-02-22 01:49:27", "link": "http://arxiv.org/abs/2002.09599v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergent Communication with World Models", "abstract": "We introduce Language World Models, a class of language-conditional\ngenerative model which interpret natural language messages by predicting latent\ncodes of future observations. This provides a visual grounding of the message,\nsimilar to an enhanced observation of the world, which may include objects\noutside of the listening agent's field-of-view. We incorporate this\n\"observation\" into a persistent memory state, and allow the listening agent's\npolicy to condition on it, akin to the relationship between memory and\ncontroller in a World Model. We show this improves effective communication and\ntask success in 2D gridworld speaker-listener navigation tasks. In addition, we\ndevelop two losses framed specifically for our model-based formulation to\npromote positive signalling and positive listening. Finally, because messages\nare interpreted in a generative model, we can visualize the model beliefs to\ngain insight into how the communication channel is utilized.", "published": "2020-02-22 02:34:51", "link": "http://arxiv.org/abs/2002.09604v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Question Decomposition for Question Answering", "abstract": "We aim to improve question answering (QA) by decomposing hard questions into\nsimpler sub-questions that existing QA systems are capable of answering. Since\nlabeling questions with decompositions is cumbersome, we take an unsupervised\napproach to produce sub-questions, also enabling us to leverage millions of\nquestions from the internet. Specifically, we propose an algorithm for One-to-N\nUnsupervised Sequence transduction (ONUS) that learns to map one hard,\nmulti-hop question to many simpler, single-hop sub-questions. We answer\nsub-questions with an off-the-shelf QA model and give the resulting answers to\na recomposition model that combines them into a final answer. We show large QA\nimprovements on HotpotQA over a strong baseline on the original, out-of-domain,\nand multi-hop dev sets. ONUS automatically learns to decompose different kinds\nof questions, while matching the utility of supervised and heuristic\ndecomposition methods for QA and exceeding those methods in fluency.\nQualitatively, we find that using sub-questions is promising for shedding light\non why a QA system makes a prediction.", "published": "2020-02-22 19:40:35", "link": "http://arxiv.org/abs/2002.09758v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Branch Learning for Weakly-Labeled Sound Event Detection", "abstract": "There are two sub-tasks implied in the weakly-supervised SED: audio tagging\nand event boundary detection. Current methods which combine multi-task learning\nwith SED requires annotations both for these two sub-tasks. Since there are\nonly annotations for audio tagging available in weakly-supervised SED, we\ndesign multiple branches with different learning purposes instead of pursuing\nmultiple tasks. Similar to multiple tasks, multiple different learning purposes\ncan also prevent the common feature which the multiple branches share from\noverfitting to any one of the learning purposes. We design these multiple\ndifferent learning purposes based on combinations of different MIL strategies\nand different pooling methods. Experiments on the DCASE 2018 Task 4 dataset and\nthe URBAN-SED dataset both show that our method achieves competitive\nperformance.", "published": "2020-02-22 08:40:04", "link": "http://arxiv.org/abs/2002.09661v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Aluminum Nitride Two-Dimensional-Resonant-Rods", "abstract": "In the last decades, Bulk-Acoustic-Wave (BAW) filters have been essential\ncomponents of 1G-to-4G radios. These devices rely on the high electromechanical\ncoupling coefficient (kt2~7%), attained by Aluminum Nitride (AlN)\nFilm-Bulk-Acoustic-Resonators (FBARs), to achieve a wideband and low-loss\nfrequency response. As the resonance frequency of FBARs is set by their\nthickness, the integration of multiple FBARs, to form filters, can only be\nattained through the adoption of frequency tuning fabrication steps, such as\nmass loading or trimming. However, as the ability to reliably control these\nsteps significantly decays for thinner FBARs, manufacturing FBARs-based\nfilters, addressing the needs of emerging IoT and 5G high-frequency\napplications, is becoming more and more challenging. Consequently, there is a\nquest for new acoustic resonant components, simultaneously exhibiting high kt2\nand a lithographic frequency tunability. In this work, a novel class of AlN\nresonators is presented. These radio-frequency devices, labelled as\nTwo-Dimensional-Resonant-Rods (2DRRs), exploit, for the first time, the\nunconventional acoustic behavior exhibited by a forest of locally resonant\nrods, built in the body of a profiled AlN layer that is sandwiched between a\nbottom un-patterned metal plate and a top metallic grating. 2DRRs exhibit\nunexplored modal features that make them able to achieve high kt2, a\nsignificant lithographic frequency tunability and a relaxed lithographic\nresolution, while relying on an optimal AlN crystalline orientation. The\noperation of 2DRRs is discussed, in this work, by means of analytical and\nfinite-element (FE) investigations. The measured performance of the first\nfabricated 2DRR, showing a kt2 in excess of 7.4%, are also reported.", "published": "2020-02-22 13:32:53", "link": "http://arxiv.org/abs/2105.11232v1", "categories": ["eess.AS", "physics.app-ph"], "primary_category": "eess.AS"}
{"title": "Multi-Representation Knowledge Distillation For Audio Classification", "abstract": "As an important component of multimedia analysis tasks, audio classification\naims to discriminate between different audio signal types and has received\nintensive attention due to its wide applications. Generally speaking, the raw\nsignal can be transformed into various representations (such as Short Time\nFourier Transform and Mel Frequency Cepstral Coefficients), and information\nimplied in different representations can be complementary. Ensembling the\nmodels trained on different representations can greatly boost the\nclassification performance, however, making inference using a large number of\nmodels is cumbersome and computationally expensive. In this paper, we propose a\nnovel end-to-end collaborative learning framework for the audio classification\ntask. The framework takes multiple representations as the input to train the\nmodels in parallel. The complementary information provided by different\nrepresentations is shared by knowledge distillation. Consequently, the\nperformance of each model can be significantly promoted without increasing the\ncomputational overhead in the inference stage. Extensive experimental results\ndemonstrate that the proposed approach can improve the classification\nperformance and achieve state-of-the-art results on both acoustic scene\nclassification tasks and general audio tagging tasks.", "published": "2020-02-22 02:53:39", "link": "http://arxiv.org/abs/2002.09607v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "DECIBEL: Improving Audio Chord Estimation for Popular Music by Alignment\n  and Integration of Crowd-Sourced Symbolic Representations", "abstract": "Automatic Chord Estimation (ACE) is a fundamental task in Music Information\nRetrieval (MIR) and has applications in both music performance and MIR\nresearch. The task consists of segmenting a music recording or score and\nassigning a chord label to each segment. Although it has been a task in the\nannual benchmarking evaluation MIREX for over 10 years, ACE is not yet a solved\nproblem, since performance has stagnated and modern systems have started to\ntune themselves to subjective training data. We propose DECIBEL, a new ACE\nsystem that exploits widely available MIDI and tab representations to improve\nACE from audio only. From an audio file and a set of MIDI and tab files\ncorresponding to the same popular music song, DECIBEL first estimates chord\nsequences. For audio, state-of-the-art audio ACE methods are used. MIDI files\nare aligned to the audio, followed by a MIDI chord estimation step. Tab files\nare transformed into untimed chord sequences and then aligned to the audio.\nNext, DECIBEL uses data fusion to integrate all estimated chord sequences into\none final output sequence. DECIBEL improves all tested state-of-the-art ACE\nmethods by over 3 percent on average. This result shows that the integration of\nmusical knowledge from heterogeneous symbolic music representations is a\nsuitable strategy for addressing challenging MIR tasks such as ACE.", "published": "2020-02-22 18:42:00", "link": "http://arxiv.org/abs/2002.09748v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Synthesis using EEG", "abstract": "In this paper we demonstrate speech synthesis using different\nelectroencephalography (EEG) feature sets recently introduced in [1]. We make\nuse of a recurrent neural network (RNN) regression model to predict acoustic\nfeatures directly from EEG features. We demonstrate our results using EEG\nfeatures recorded in parallel with spoken speech as well as using EEG recorded\nin parallel with listening utterances. We provide EEG based speech synthesis\nresults for four subjects in this paper and our results demonstrate the\nfeasibility of synthesizing speech directly from EEG features.", "published": "2020-02-22 03:53:45", "link": "http://arxiv.org/abs/2002.12756v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "q-bio.QM", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Novel Decision Tree for Depression Recognition in Speech", "abstract": "Depression is a common mental disorder worldwide which causes a range of\nserious outcomes. The diagnosis of depression relies on patient-reported scales\nand psychiatrist interview which may lead to subjective bias. In recent years,\nmore and more researchers are devoted to depression recognition in speech ,\nwhich may be an effective and objective indicator. This study proposes a new\nspeech segment fusion method based on decision tree to improve the depression\nrecognition accuracy and conducts a validation on a sample of 52 subjects (23\ndepressed patients and 29 healthy controls). The recognition accuracy are 75.8%\nand 68.5% for male and female respectively on gender-dependent models. It can\nbe concluded from the data that the proposed decision tree model can improve\nthe depression classification performance.", "published": "2020-02-22 10:46:38", "link": "http://arxiv.org/abs/2002.12759v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "q-bio.QM", "stat.ML"], "primary_category": "eess.AS"}
