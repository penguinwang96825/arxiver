{"title": "Introducing Syntactic Structures into Target Opinion Word Extraction\n  with Deep Learning", "abstract": "Targeted opinion word extraction (TOWE) is a sub-task of aspect based\nsentiment analysis (ABSA) which aims to find the opinion words for a given\naspect-term in a sentence. Despite their success for TOWE, the current deep\nlearning models fail to exploit the syntactic information of the sentences that\nhave been proved to be useful for TOWE in the prior research. In this work, we\npropose to incorporate the syntactic structures of the sentences into the deep\nlearning models for TOWE, leveraging the syntax-based opinion possibility\nscores and the syntactic connections between the words. We also introduce a\nnovel regularization technique to improve the performance of the deep learning\nmodels based on the representation distinctions between the words in TOWE. The\nproposed model is extensively analyzed and achieves the state-of-the-art\nperformance on four benchmark datasets.", "published": "2020-10-26 07:13:17", "link": "http://arxiv.org/abs/2010.13378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastFormers: Highly Efficient Transformer Models for Natural Language\n  Understanding", "abstract": "Transformer-based models are the state-of-the-art for Natural Language\nUnderstanding (NLU) applications. Models are getting bigger and better on\nvarious tasks. However, Transformer models remain computationally challenging\nsince they are not efficient at inference-time compared to traditional\napproaches. In this paper, we present FastFormers, a set of recipes to achieve\nefficient inference-time performance for Transformer-based models on various\nNLU tasks. We show how carefully utilizing knowledge distillation, structured\npruning and numerical optimization can lead to drastic improvements on\ninference efficiency. We provide effective recipes that can guide practitioners\nto choose the best settings for various NLU tasks and pretrained models.\nApplying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x\nup to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also\nachieve up to 12.4x speed-up with the presented methods. We show that\nFastFormers can drastically reduce cost of serving 100 million requests from\n4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a\nsustainable runtime by reducing energy consumption 6.9x - 125.8x according to\nthe metrics used in the SustaiNLP 2020 shared task.", "published": "2020-10-26 07:25:15", "link": "http://arxiv.org/abs/2010.13382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Aspect-based Sentiment Analysis with Gated Graph Convolutional\n  Networks and Syntax-based Regulation", "abstract": "Aspect-based Sentiment Analysis (ABSA) seeks to predict the sentiment\npolarity of a sentence toward a specific aspect. Recently, it has been shown\nthat dependency trees can be integrated into deep learning models to produce\nthe state-of-the-art performance for ABSA. However, these models tend to\ncompute the hidden/representation vectors without considering the aspect terms\nand fail to benefit from the overall contextual importance scores of the words\nthat can be obtained from the dependency tree for ABSA. In this work, we\npropose a novel graph-based deep learning model to overcome these two issues of\nthe prior work on ABSA. In our model, gate vectors are generated from the\nrepresentation vectors of the aspect terms to customize the hidden vectors of\nthe graph-based models toward the aspect terms. In addition, we propose a\nmechanism to obtain the importance scores for each word in the sentences based\non the dependency trees that are then injected into the model to improve the\nrepresentation vectors for ABSA. The proposed model achieves the\nstate-of-the-art performance on three benchmark datasets.", "published": "2020-10-26 07:36:24", "link": "http://arxiv.org/abs/2010.13389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Transformer Networks with Syntactic and Semantic Structures for\n  Event Argument Extraction", "abstract": "The goal of Event Argument Extraction (EAE) is to find the role of each\nentity mention for a given event trigger word. It has been shown in the\nprevious works that the syntactic structures of the sentences are helpful for\nthe deep learning models for EAE. However, a major problem in such prior works\nis that they fail to exploit the semantic structures of the sentences to induce\neffective representations for EAE. Consequently, in this work, we propose a\nnovel model for EAE that exploits both syntactic and semantic structures of the\nsentences with the Graph Transformer Networks (GTNs) to learn more effective\nsentence structures for EAE. In addition, we introduce a novel inductive bias\nbased on information bottleneck to improve generalization of the EAE models.\nExtensive experiments are performed to demonstrate the benefits of the proposed\nmodel, leading to state-of-the-art performance for EAE on standard datasets.", "published": "2020-10-26 07:41:40", "link": "http://arxiv.org/abs/2010.13391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TPLinker: Single-stage Joint Extraction of Entities and Relations\n  Through Token Pair Linking", "abstract": "Extracting entities and relations from unstructured text has attracted\nincreasing attention in recent years but remains challenging, due to the\nintrinsic difficulty in identifying overlapping relations with shared entities.\nPrior works show that joint learning can result in a noticeable performance\ngain. However, they usually involve sequential interrelated steps and suffer\nfrom the problem of exposure bias. At training time, they predict with the\nground truth conditions while at inference it has to make extraction from\nscratch. This discrepancy leads to error accumulation. To mitigate the issue,\nwe propose in this paper a one-stage joint extraction model, namely, TPLinker,\nwhich is capable of discovering overlapping relations sharing one or both\nentities while immune from the exposure bias. TPLinker formulates joint\nextraction as a token pair linking problem and introduces a novel handshaking\ntagging scheme that aligns the boundary tokens of entity pairs under each\nrelation type. Experiment results show that TPLinker performs significantly\nbetter on overlapping and multiple relation extraction, and achieves\nstate-of-the-art performance on two public datasets.", "published": "2020-10-26 08:35:06", "link": "http://arxiv.org/abs/2010.13415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syllabification of the Divine Comedy", "abstract": "We provide a syllabification algorithm for the Divine Comedy using techniques\nfrom probabilistic and constraint programming. We particularly focus on the\nsynalephe, addressed in terms of the \"propensity\" of a word to take part in a\nsynalephe with adjacent words. We jointly provide an online vocabulary\ncontaining, for each word, information about its syllabification, the location\nof the tonic accent, and the aforementioned synalephe propensity, on the left\nand right sides. The algorithm is intrinsically nondeterministic, producing\ndifferent possible syllabifications for each verse, with different likelihoods;\nmetric constraints relative to accents on the 10th, 4th and 6th syllables are\nused to further reduce the solution space. The most likely syllabification is\nhence returned as output. We believe that this work could be a major milestone\nfor a lot of different investigations. From the point of view of digital\nhumanities it opens new perspectives on computer assisted analysis of digital\nsources, comprising automated detection of anomalous and problematic cases,\nmetric clustering of verses and their categorization, or more foundational\ninvestigations addressing e.g. the phonetic roles of consonants and vowels.\nFrom the point of view of text processing and deep learning, information about\nsyllabification and the location of accents opens a wide range of exciting\nperspectives, from the possibility of automatic learning syllabification of\nwords and verses, to the improvement of generative models, aware of metric\nissues, and more respectful of the expected musicality.", "published": "2020-10-26 12:14:14", "link": "http://arxiv.org/abs/2010.13515v1", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Meta-Learning for Neural Relation Classification with Distant\n  Supervision", "abstract": "Distant supervision provides a means to create a large number of weakly\nlabeled data at low cost for relation classification. However, the resulting\nlabeled instances are very noisy, containing data with wrong labels. Many\napproaches have been proposed to select a subset of reliable instances for\nneural model training, but they still suffer from noisy labeling problem or\nunderutilization of the weakly-labeled data. To better select more reliable\ntraining instances, we introduce a small amount of manually labeled data as\nreference to guide the selection process. In this paper, we propose a\nmeta-learning based approach, which learns to reweight noisy training data\nunder the guidance of reference data. As the clean reference data is usually\nvery small, we propose to augment it by dynamically distilling the most\nreliable elite instances from the noisy data. Experiments on several datasets\ndemonstrate that the reference data can effectively guide the selection of\ntraining data, and our augmented approach consistently improves the performance\nof relation classification comparing to the existing state-of-the-art methods.", "published": "2020-10-26 12:52:28", "link": "http://arxiv.org/abs/2010.13544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curious Case of Language Generation Evaluation Metrics: A Cautionary\n  Tale", "abstract": "Automatic evaluation of language generation systems is a well-studied problem\nin Natural Language Processing. While novel metrics are proposed every year, a\nfew popular metrics remain as the de facto metrics to evaluate tasks such as\nimage captioning and machine translation, despite their known limitations. This\nis partly due to ease of use, and partly because researchers expect to see them\nand know how to interpret them. In this paper, we urge the community for more\ncareful consideration of how they automatically evaluate their models by\ndemonstrating important failure cases on multiple datasets, language pairs and\ntasks. Our experiments show that metrics (i) usually prefer system outputs to\nhuman-authored texts, (ii) can be insensitive to correct translations of rare\nwords, (iii) can yield surprisingly high scores when given a single sentence as\nsystem output for the entire test set.", "published": "2020-10-26 13:57:20", "link": "http://arxiv.org/abs/2010.13588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is it Great or Terrible? Preserving Sentiment in Neural Machine\n  Translation of Arabic Reviews", "abstract": "Since the advent of Neural Machine Translation (NMT) approaches there has\nbeen a tremendous improvement in the quality of automatic translation. However,\nNMT output still lacks accuracy in some low-resource languages and sometimes\nmakes major errors that need extensive post-editing. This is particularly\nnoticeable with texts that do not follow common lexico-grammatical standards,\nsuch as user generated content (UGC). In this paper we investigate the\nchallenges involved in translating book reviews from Arabic into English, with\nparticular focus on the errors that lead to incorrect translation of sentiment\npolarity. Our study points to the special characteristics of Arabic UGC,\nexamines the sentiment transfer errors made by Google Translate of Arabic UGC\nto English, analyzes why the problem occurs, and proposes an error typology\nspecific of the translation of Arabic UGC. Our analysis shows that the output\nof online translation tools of Arabic UGC can either fail to transfer the\nsentiment at all by producing a neutral target text, or completely flips the\nsentiment polarity of the target word or phrase and hence delivers a wrong\naffect message. We address this problem by fine-tuning an NMT model with\nrespect to sentiment polarity showing that this approach can significantly help\nwith correcting sentiment errors detected in the online translation of Arabic\nUGC.", "published": "2020-10-26 18:01:52", "link": "http://arxiv.org/abs/2010.13814v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Spoken Language Understanding via Self-Supervised Speech\n  and Language Model Pretraining", "abstract": "Much recent work on Spoken Language Understanding (SLU) is limited in at\nleast one of three ways: models were trained on oracle text input and neglected\nASR errors, models were trained to predict only intents without the slot\nvalues, or models were trained on a large amount of in-house data. In this\npaper, we propose a clean and general framework to learn semantics directly\nfrom speech with semi-supervision from transcribed or untranscribed speech to\naddress these issues. Our framework is built upon pretrained end-to-end (E2E)\nASR and self-supervised language models, such as BERT, and fine-tuned on a\nlimited amount of target SLU data. We study two semi-supervised settings for\nthe ASR component: supervised pretraining on transcribed speech, and\nunsupervised pretraining by replacing the ASR encoder with self-supervised\nspeech representations, such as wav2vec. In parallel, we identify two essential\ncriteria for evaluating SLU models: environmental noise-robustness and E2E\nsemantics evaluation. Experiments on ATIS show that our SLU framework with\nspeech as input can perform on par with those using oracle text as input in\nsemantics understanding, even though environmental noise is present and a\nlimited amount of labeled semantics data is available for training.", "published": "2020-10-26 18:21:27", "link": "http://arxiv.org/abs/2010.13826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Frequency Does Not Predict Grammatical Knowledge in Language Models", "abstract": "Neural language models learn, to varying degrees of accuracy, the grammatical\nproperties of natural languages. In this work, we investigate whether there are\nsystematic sources of variation in the language models' accuracy. Focusing on\nsubject-verb agreement and reflexive anaphora, we find that certain nouns are\nsystematically understood better than others, an effect which is robust across\ngrammatical tasks and different language models. Surprisingly, we find that\nacross four orders of magnitude, corpus frequency is unrelated to a noun's\nperformance on grammatical tasks. Finally, we find that a novel noun's\ngrammatical properties can be few-shot learned from various types of training\ndata. The results present a paradox: there should be less variation in\ngrammatical performance than is actually observed.", "published": "2020-10-26 19:51:36", "link": "http://arxiv.org/abs/2010.13870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Neural Language Model Fusion for Streaming Recurrent Neural\n  Network Transducer", "abstract": "Recurrent Neural Network Transducer (RNN-T), like most end-to-end speech\nrecognition model architectures, has an implicit neural network language model\n(NNLM) and cannot easily leverage unpaired text data during training. Previous\nwork has proposed various fusion methods to incorporate external NNLMs into\nend-to-end ASR to address this weakness. In this paper, we propose extensions\nto these techniques that allow RNN-T to exploit external NNLMs during both\ntraining and inference time, resulting in 13-18% relative Word Error Rate\nimprovement on Librispeech compared to strong baselines. Furthermore, our\nmethods do not incur extra algorithmic latency and allow for flexible\nplug-and-play of different NNLMs without re-training. We also share in-depth\nanalysis to better understand the benefits of the different NNLM fusion\nmethods. Our work provides a reliable technique for leveraging unpaired text\ndata to significantly improve RNN-T while keeping the system streamable,\nflexible, and lightweight.", "published": "2020-10-26 20:10:12", "link": "http://arxiv.org/abs/2010.13878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reading Between the Lines: Exploring Infilling in Visual Narratives", "abstract": "Generating long form narratives such as stories and procedures from multiple\nmodalities has been a long standing dream for artificial intelligence. In this\nregard, there is often crucial subtext that is derived from the surrounding\ncontexts. The general seq2seq training methods render the models shorthanded\nwhile attempting to bridge the gap between these neighbouring contexts. In this\npaper, we tackle this problem by using \\textit{infilling} techniques involving\nprediction of missing steps in a narrative while generating textual\ndescriptions from a sequence of images. We also present a new large scale\n\\textit{visual procedure telling} (ViPT) dataset with a total of 46,200\nprocedures and around 340k pairwise images and textual descriptions that is\nrich in such contextual dependencies. Generating steps using infilling\ntechnique demonstrates the effectiveness in visual procedures with more\ncoherent texts. We conclusively show a METEOR score of 27.51 on procedures\nwhich is higher than the state-of-the-art on visual storytelling. We also\ndemonstrate the effects of interposing new text with missing images during\ninference. The code and the dataset will be publicly available at\nhttps://visual-narratives.github.io/Visual-Narratives/.", "published": "2020-10-26 23:09:09", "link": "http://arxiv.org/abs/2010.13944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Information Status Classification Using Discourse\n  Context-Aware BERT", "abstract": "Previous work on bridging anaphora recognition (Hou et al., 2013a) casts the\nproblem as a subtask of learning fine-grained information status (IS). However,\nthese systems heavily depend on many hand-crafted linguistic features. In this\npaper, we propose a simple discourse context-aware BERT model for fine-grained\nIS classification. On the ISNotes corpus (Markert et al., 2012), our model\nachieves new state-of-the-art performance on fine-grained IS classification,\nobtaining a 4.8 absolute overall accuracy improvement compared to Hou et al.\n(2013a). More importantly, we also show an improvement of 10.5 F1 points for\nbridging anaphora recognition without using any complex hand-crafted semantic\nfeatures designed for capturing the bridging phenomenon. We further analyze the\ntrained model and find that the most attended signals for each IS category\ncorrespond well to linguistic notions of information status.", "published": "2020-10-26 22:30:17", "link": "http://arxiv.org/abs/2010.14759v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LXPER Index 2.0: Improving Text Readability Assessment Model for L2\n  English Students in Korea", "abstract": "Developing a text readability assessment model specifically for texts in a\nforeign English Language Training (ELT) curriculum has never had much attention\nin the field of Natural Language Processing. Hence, most developed models show\nextremely low accuracy for L2 English texts, up to the point where not many\neven serve as a fair comparison. In this paper, we investigate a text\nreadability assessment model for L2 English learners in Korea. In accordance,\nwe improve and expand the Text Corpus of the Korean ELT curriculum\n(CoKEC-text). Each text is labeled with its target grade level. We train our\nmodel with CoKEC-text and significantly improve the accuracy of readability\nassessment for texts in the Korean ELT curriculum.", "published": "2020-10-26 07:03:14", "link": "http://arxiv.org/abs/2010.13374v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model", "abstract": "Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.", "published": "2020-10-26 08:00:48", "link": "http://arxiv.org/abs/2010.13404v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UPB at SemEval-2020 Task 12: Multilingual Offensive Language Detection\n  on Social Media by Fine-tuning a Variety of BERT-based Models", "abstract": "Offensive language detection is one of the most challenging problem in the\nnatural language processing field, being imposed by the rising presence of this\nphenomenon in online social media. This paper describes our Transformer-based\nsolutions for identifying offensive language on Twitter in five languages\n(i.e., English, Arabic, Danish, Greek, and Turkish), which was employed in\nSubtask A of the Offenseval 2020 shared task. Several neural architectures\n(i.e., BERT, mBERT, Roberta, XLM-Roberta, and ALBERT), pre-trained using both\nsingle-language and multilingual corpora, were fine-tuned and compared using\nmultiple combinations of datasets. Finally, the highest-scoring models were\nused for our submissions in the competition, which ranked our team 21st of 85,\n28th of 53, 19th of 39, 16th of 37, and 10th of 46 for English, Arabic, Danish,\nGreek, and Turkish, respectively.", "published": "2020-10-26 14:28:29", "link": "http://arxiv.org/abs/2010.13609v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dutch Humor Detection by Generating Negative Examples", "abstract": "Detecting if a text is humorous is a hard task to do computationally, as it\nusually requires linguistic and common sense insights. In machine learning,\nhumor detection is usually modeled as a binary classification task, trained to\npredict if the given text is a joke or another type of text. Rather than using\ncompletely different non-humorous texts, we propose using text generation\nalgorithms for imitating the original joke dataset to increase the difficulty\nfor the learning algorithm. We constructed several different joke and non-joke\ndatasets to test the humor detection abilities of different language\ntechnologies. In particular, we compare the humor detection capabilities of\nclassic neural network approaches with the state-of-the-art Dutch language\nmodel RobBERT. In doing so, we create and compare the first Dutch humor\ndetection systems. We found that while other language models perform well when\nthe non-jokes came from completely different domains, RobBERT was the only one\nthat was able to distinguish jokes from generated negative examples. This\nperformance illustrates the usefulness of using text generation to create\nnegative datasets for humor recognition, and also shows that transformer models\nare a large step forward in humor detection.", "published": "2020-10-26 15:15:10", "link": "http://arxiv.org/abs/2010.13652v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Constraint Translation Candidates: A Bridge between Neural Query\n  Translation and Cross-lingual Information Retrieval", "abstract": "Query translation (QT) is a key component in cross-lingual information\nretrieval system (CLIR). With the help of deep learning, neural machine\ntranslation (NMT) has shown promising results on various tasks. However, NMT is\ngenerally trained with large-scale out-of-domain data rather than in-domain\nquery translation pairs. Besides, the translation model lacks a mechanism at\nthe inference time to guarantee the generated words to match the search index.\nThe two shortages of QT result in readable texts for human but inadequate\ncandidates for the downstream retrieval task. In this paper, we propose a novel\napproach to alleviate these problems by limiting the open target vocabulary\nsearch space of QT to a set of important words mined from search index\ndatabase. The constraint translation candidates are employed at both of\ntraining and inference time, thus guiding the translation model to learn and\ngenerate well performing target queries. The proposed methods are exploited and\nexamined in a real-word CLIR system--Aliexpress e-Commerce search engine.\nExperimental results demonstrate that our approach yields better performance on\nboth translation quality and retrieval accuracy than the strong NMT baseline.", "published": "2020-10-26 15:27:51", "link": "http://arxiv.org/abs/2010.13658v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploiting Neural Query Translation into Cross Lingual Information\n  Retrieval", "abstract": "As a crucial role in cross-language information retrieval (CLIR), query\ntranslation has three main challenges: 1) the adequacy of translation; 2) the\nlack of in-domain parallel training data; and 3) the requisite of low latency.\nTo this end, existing CLIR systems mainly exploit statistical-based machine\ntranslation (SMT) rather than the advanced neural machine translation (NMT),\nlimiting the further improvements on both translation and retrieval quality. In\nthis paper, we investigate how to exploit neural query translation model into\nCLIR system. Specifically, we propose a novel data augmentation method that\nextracts query translation pairs according to user clickthrough data, thus to\nalleviate the problem of domain-adaptation in NMT. Then, we introduce an\nasynchronous strategy which is able to leverage the advantages of the real-time\nin SMT and the veracity in NMT. Experimental results reveal that the proposed\napproach yields better retrieval quality than strong baselines and can be well\napplied into a real-world CLIR system, i.e. Aliexpress e-Commerce search\nengine. Readers can examine and test their cases on our website:\nhttps://aliexpress.com .", "published": "2020-10-26 15:28:19", "link": "http://arxiv.org/abs/2010.13659v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Corpus for Argumentative Writing Support in German", "abstract": "In this paper, we present a novel annotation approach to capture claims and\npremises of arguments and their relations in student-written persuasive peer\nreviews on business models in German language. We propose an annotation scheme\nbased on annotation guidelines that allows to model claims and premises as well\nas support and attack relations for capturing the structure of argumentative\ndiscourse in student-written peer reviews. We conduct an annotation study with\nthree annotators on 50 persuasive essays to evaluate our annotation scheme. The\nobtained inter-rater agreement of $\\alpha=0.57$ for argument components and\n$\\alpha=0.49$ for argumentative relations indicates that the proposed\nannotation scheme successfully guides annotators to moderate agreement.\nFinally, we present our freely available corpus of 1,000 persuasive\nstudent-written peer reviews on business models and our annotation guidelines\nto encourage future research on the design and development of argumentative\nwriting support systems for students.", "published": "2020-10-26 15:52:12", "link": "http://arxiv.org/abs/2010.13674v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Embedding Space Alignment Methods for Language and Knowledge\n  Graphs", "abstract": "Neural embedding approaches have become a staple in the fields of computer\nvision, natural language processing, and more recently, graph analytics. Given\nthe pervasive nature of these algorithms, the natural question becomes how to\nexploit the embedding spaces to map, or align, embeddings of different data\nsources. To this end, we survey the current research landscape on word,\nsentence and knowledge graph embedding algorithms. We provide a classification\nof the relevant alignment techniques and discuss benchmark datasets used in\nthis field of research. By gathering these diverse approaches into a singular\nsurvey, we hope to further motivate research into alignment of embedding spaces\nof varied data types and sources.", "published": "2020-10-26 16:08:13", "link": "http://arxiv.org/abs/2010.13688v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PowerTransformer: Unsupervised Controllable Revision for Biased Language\n  Correction", "abstract": "Unconscious biases continue to be prevalent in modern text and media, calling\nfor algorithms that can assist writers with bias correction. For example, a\nfemale character in a story is often portrayed as passive and powerless (\"She\ndaydreams about being a doctor\") while a man is portrayed as more proactive and\npowerful (\"He pursues his dream of being a doctor\").\n  We formulate *Controllable Debiasing*, a new revision task that aims to\nrewrite a given text to correct the implicit and potentially undesirable bias\nin character portrayals. We then introduce PowerTransformer as an approach that\ndebiases text through the lens of connotation frames (Sap et al., 2017), which\nencode pragmatic knowledge of implied power dynamics with respect to verb\npredicates. One key challenge of our task is the lack of parallel corpora. To\naddress this challenge, we adopt an unsupervised approach using auxiliary\nsupervision with related tasks such as paraphrasing and self-supervision based\non a reconstruction loss, building on pretrained language models.\n  Through comprehensive experiments based on automatic and human evaluations,\nwe demonstrate that our approach outperforms ablations and existing methods\nfrom related tasks. Furthermore, we demonstrate the use of PowerTransformer as\na step toward mitigating the well-documented gender bias in character portrayal\nin movie scripts.", "published": "2020-10-26 18:05:48", "link": "http://arxiv.org/abs/2010.13816v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement\n  Learning", "abstract": "We present VisualHints, a novel environment for multimodal reinforcement\nlearning (RL) involving text-based interactions along with visual hints\n(obtained from the environment). Real-life problems often demand that agents\ninteract with the environment using both natural language information and\nvisual perception towards solving a goal. However, most traditional RL\nenvironments either solve pure vision-based tasks like Atari games or\nvideo-based robotic manipulation; or entirely use natural language as a mode of\ninteraction, like Text-based games and dialog systems. In this work, we aim to\nbridge this gap and unify these two approaches in a single environment for\nmultimodal RL. We introduce an extension of the TextWorld cooking environment\nwith the addition of visual clues interspersed throughout the environment. The\ngoal is to force an RL agent to use both text and visual features to predict\nnatural language action commands for solving the final task of cooking a meal.\nWe enable variations and difficulties in our environment to emulate various\ninteractive real-world scenarios. We present a baseline multimodal agent for\nsolving such problems using CNN-based feature extraction from visual hints and\nLSTMs for textual feature extraction. We believe that our proposed\nvisual-lingual environment will facilitate novel problem settings for the RL\ncommunity.", "published": "2020-10-26 18:51:02", "link": "http://arxiv.org/abs/2010.13839v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Data Troubles in Sentence Level Confidence Estimation for Machine\n  Translation", "abstract": "The paper investigates the feasibility of confidence estimation for neural\nmachine translation models operating at the high end of the performance\nspectrum. As a side product of the data annotation process necessary for\nbuilding such models we propose sentence level accuracy $SACC$ as a simple,\nself-explanatory evaluation metric for quality of translation.\n  Experiments on two different annotator pools, one comprised of non-expert\n(crowd-sourced) and one of expert (professional) translators show that $SACC$\ncan vary greatly depending on the translation proficiency of the annotators,\ndespite the fact that both pools are about equally reliable according to\nKrippendorff's alpha metric; the relatively low values of inter-annotator\nagreement confirm the expectation that sentence-level binary labeling $good$ /\n$needs\\ work$ for translation out of context is very hard.\n  For an English-Spanish translation model operating at $SACC = 0.89$ according\nto a non-expert annotator pool we can derive a confidence estimate that labels\n0.5-0.6 of the $good$ translations in an \"in-domain\" test set with 0.95\nPrecision. Switching to an expert annotator pool decreases $SACC$ dramatically:\n$0.61$ for English-Spanish, measured on the exact same data as above. This\nforces us to lower the CE model operating point to 0.9 Precision while labeling\ncorrectly about 0.20-0.25 of the $good$ translations in the data.\n  We find surprising the extent to which CE depends on the level of proficiency\nof the annotator pool used for labeling the data. This leads to an important\nrecommendation we wish to make when tackling CE modeling in practice: it is\ncritical to match the end-user expectation for translation quality in the\ndesired domain with the demands of annotators assigning binary quality labels\nto CE training data.", "published": "2020-10-26 19:20:29", "link": "http://arxiv.org/abs/2010.13856v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing Task-Oriented Dialogue Representation from Language Models", "abstract": "This paper investigates pre-trained language models to find out which model\nintrinsically carries the most informative representation for task-oriented\ndialogue tasks. We approach the problem from two aspects: supervised classifier\nprobe and unsupervised mutual information probe. We fine-tune a feed-forward\nlayer as the classifier probe on top of a fixed pre-trained language model with\nannotated labels in a supervised way. Meanwhile, we propose an unsupervised\nmutual information probe to evaluate the mutual dependence between a real\nclustering and a representation clustering. The goals of this empirical paper\nare to 1) investigate probing techniques, especially from the unsupervised\nmutual information aspect, 2) provide guidelines of pre-trained language model\nselection for the dialogue research community, 3) find insights of pre-training\nfactors for dialogue application that may be the key to success.", "published": "2020-10-26 21:34:39", "link": "http://arxiv.org/abs/2010.13912v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Limited Labeled Dialogue State Tracking with Self-Supervision", "abstract": "Existing dialogue state tracking (DST) models require plenty of labeled data.\nHowever, collecting high-quality labels is costly, especially when the number\nof domains increases. In this paper, we address a practical DST problem that is\nrarely discussed, i.e., learning efficiently with limited labeled data. We\npresent and investigate two self-supervised objectives: preserving latent\nconsistency and modeling conversational behavior. We encourage a DST model to\nhave consistent latent distributions given a perturbed input, making it more\nrobust to an unseen scenario. We also add an auxiliary utterance generation\ntask, modeling a potential correlation between conversational behavior and\ndialogue states. The experimental results show that our proposed\nself-supervised signals can improve joint goal accuracy by 8.95\\% when only 1\\%\nlabeled data is used on the MultiWOZ dataset. We can achieve an additional\n1.76\\% improvement if some unlabeled data is jointly trained as semi-supervised\nlearning. We analyze and visualize how our proposed self-supervised signals\nhelp the DST task and hope to stimulate future data-efficient DST research.", "published": "2020-10-26 21:57:42", "link": "http://arxiv.org/abs/2010.13920v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Designing learning experiences for online teaching and learning", "abstract": "Teaching is about constantly innovating strategies, ways and means to engage\ndiverse students in active and meaningful learning. In line with this, SUTD\nadopts various student-centric teaching and learning teaching methods and\napproaches. This means that our graduate/undergraduate instructors have to be\nready to teach using these student student-centric teaching and learning\npedagogies. In this article, I share my experiences of redesigning this\nteaching course that is typically conducted face-to-face to a synchronous\nonline course and also invite one of the participant in this course to reflect\non his experience as a student.", "published": "2020-10-26 07:03:49", "link": "http://arxiv.org/abs/2010.15602v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Improved Mask-CTC for Non-Autoregressive End-to-End ASR", "abstract": "For real-world deployment of automatic speech recognition (ASR), the system\nis desired to be capable of fast inference while relieving the requirement of\ncomputational resources. The recently proposed end-to-end ASR system based on\nmask-predict with connectionist temporal classification (CTC), Mask-CTC,\nfulfills this demand by generating tokens in a non-autoregressive fashion.\nWhile Mask-CTC achieves remarkably fast inference speed, its recognition\nperformance falls behind that of conventional autoregressive (AR) systems. To\nboost the performance of Mask-CTC, we first propose to enhance the encoder\nnetwork architecture by employing a recently proposed architecture called\nConformer. Next, we propose new training and decoding methods by introducing\nauxiliary objective to predict the length of a partial target sequence, which\nallows the model to delete or insert tokens during inference. Experimental\nresults on different ASR tasks show that the proposed approaches improve\nMask-CTC significantly, outperforming a standard CTC model (15.5% $\\rightarrow$\n9.1% WER on WSJ). Moreover, Mask-CTC now achieves competitive results to AR\nmodels with no degradation of inference speed ($<$ 0.1 RTF using CPU). We also\nshow a potential application of Mask-CTC to end-to-end speech translation.", "published": "2020-10-26 01:22:35", "link": "http://arxiv.org/abs/2010.13270v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Anonymization with Distribution-Preserving X-Vector Generation\n  for the VoicePrivacy Challenge 2020", "abstract": "In this paper, we present a Distribution-Preserving Voice Anonymization\ntechnique, as our submission to the VoicePrivacy Challenge 2020. We observe\nthat the challenge baseline system generates fake X-vectors which are very\nsimilar to each other, significantly more so than those extracted from organic\nspeakers. This difference arises from averaging many X-vectors from a pool of\nspeakers in the anonymization process, causing a loss of information. We\npropose a new method to generate fake X-vectors which overcomes these\nlimitations by preserving the distributional properties of X-vectors and their\nintra-similarity. We use population data to learn the properties of the\nX-vector space, before fitting a generative model which we use to sample fake\nX-vectors. We show how this approach generates X-vectors that more closely\nfollow the expected intra-similarity distribution of organic speaker X-vectors.\nOur method can be easily integrated with others as the anonymization component\nof the system and removes the need to distribute a pool of speakers to use\nduring the anonymization. Our approach leads to an increase in EER of up to\n$19.4\\%$ in males and $11.1\\%$ in females in scenarios where enrollment and\ntrial utterances are anonymized versus the baseline solution, demonstrating the\ndiversity of our generated voices.", "published": "2020-10-26 09:53:56", "link": "http://arxiv.org/abs/2010.13457v2", "categories": ["cs.SD", "cs.CL", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hierarchical Metadata-Aware Document Categorization under Weak\n  Supervision", "abstract": "Categorizing documents into a given label hierarchy is intuitively appealing\ndue to the ubiquity of hierarchical topic structures in massive text corpora.\nAlthough related studies have achieved satisfying performance in fully\nsupervised hierarchical document classification, they usually require massive\nhuman-annotated training data and only utilize text information. However, in\nmany domains, (1) annotations are quite expensive where very few training\nsamples can be acquired; (2) documents are accompanied by metadata information.\nHence, this paper studies how to integrate the label hierarchy, metadata, and\ntext signals for document categorization under weak supervision. We develop\nHiMeCat, an embedding-based generative framework for our task. Specifically, we\npropose a novel joint representation learning module that allows simultaneous\nmodeling of category dependencies, metadata information and textual semantics,\nand we introduce a data augmentation module that hierarchically synthesizes\ntraining documents to complement the original, small-scale training set. Our\nexperiments demonstrate a consistent improvement of HiMeCat over competitive\nbaselines and validate the contribution of our representation learning and data\naugmentation modules.", "published": "2020-10-26 13:07:56", "link": "http://arxiv.org/abs/2010.13556v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enabling Efficient Cyber Threat Hunting With Cyber Threat Intelligence", "abstract": "Log-based cyber threat hunting has emerged as an important solution to\ncounter sophisticated attacks. However, existing approaches require non-trivial\nefforts of manual query construction and have overlooked the rich external\nthreat knowledge provided by open-source Cyber Threat Intelligence (OSCTI). To\nbridge the gap, we propose ThreatRaptor, a system that facilitates threat\nhunting in computer systems using OSCTI. Built upon system auditing frameworks,\nThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP\npipeline that extracts structured threat behaviors from unstructured OSCTI\ntext, (2) a concise and expressive domain-specific query language, TBQL, to\nhunt for malicious system activities, (3) a query synthesis mechanism that\nautomatically synthesizes a TBQL query for hunting, and (4) an efficient query\nexecution engine to search the big audit logging data. Evaluations on a broad\nset of attack cases demonstrate the accuracy and efficiency of ThreatRaptor in\npractical threat hunting.", "published": "2020-10-26 14:54:01", "link": "http://arxiv.org/abs/2010.13637v2", "categories": ["cs.CR", "cs.CL", "cs.DB"], "primary_category": "cs.CR"}
{"title": "Automatically Identifying Words That Can Serve as Labels for Few-Shot\n  Text Classification", "abstract": "A recent approach for few-shot text classification is to convert textual\ninputs to cloze questions that contain some form of task description, process\nthem with a pretrained language model and map the predicted words to labels.\nManually defining this mapping between words and labels requires both domain\nexpertise and an understanding of the language model's abilities. To mitigate\nthis issue, we devise an approach that automatically finds such a mapping given\nsmall amounts of training data. For a number of tasks, the mapping found by our\napproach performs almost as well as hand-crafted label-to-word mappings.", "published": "2020-10-26 14:56:22", "link": "http://arxiv.org/abs/2010.13641v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Three computational models and its equivalence", "abstract": "The study of computability has its origin in Hilbert's conference of 1900,\nwhere an adjacent question, to the ones he asked, is to give a precise\ndescription of the notion of algorithm. In the search for a good definition\narose three independent theories: Turing and the Turing machines, G\\\"odel and\nthe recursive functions, Church and the Lambda Calculus.\n  Later there were established by Kleene that the classic models of computation\nare equivalent. This fact is widely accepted by many textbooks and the proof is\nomitted since the proof is tedious and unreadable. We intend to fill this gap\npresenting the proof in a modern way, without forgetting the mathematical\ndetails.", "published": "2020-10-26 05:55:19", "link": "http://arxiv.org/abs/2010.15600v1", "categories": ["cs.LO", "cs.CC", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Distributed Node-Specific Block-Diagonal LCMV Beamforming in Wireless\n  Acoustic Sensor Networks", "abstract": "This paper derives the analytical solution of a novel distributed\nnode-specific block-diagonal linearly constrained minimum variance beamformer\nfrom the centralized linearly constrained minimum variance (LCMV) beamformer\nwhen considering that the noise covariance matrix is block-diagonal. To further\nreduce the computational complexity of the proposed beamformer, the\nShermanMorrison-Woodbury formula is introduced to compute the inversion of\nnoise sample covariance matrix. By doing so, the exchanged signals can be\ncomputed with lower dimensions between nodes, where the optimal LCMV beamformer\nis still available at each node as if each node is to transmit its all raw\nsensor signal observations. The proposed beamformer is fully distributable\nwithout imposing restrictions on the underlying network topology or scaling\ncomputational complexity, i.e., there is no increase in the per-node complexity\nwhen new nodes are added to the networks. Compared with state-of-the-art\ndistributed node-specific algorithms that are often time-recursive, the\nproposed beamformer exactly solves the LCMV beamformer optimally frame by\nframe, which has much lower computational complexity and is more robust to\nacoustic transfer function estimation error and voice activity detector error.\nNumerous experimental results are presented to validate the effectiveness of\nthe proposed beamformer.", "published": "2020-10-26 04:28:06", "link": "http://arxiv.org/abs/2010.13334v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emotion controllable speech synthesis using emotion-unlabeled dataset\n  with the assistance of cross-domain speech emotion recognition", "abstract": "Neural text-to-speech (TTS) approaches generally require a huge number of\nhigh quality speech data, which makes it difficult to obtain such a dataset\nwith extra emotion labels. In this paper, we propose a novel approach for\nemotional TTS synthesis on a TTS dataset without emotion labels. Specifically,\nour proposed method consists of a cross-domain speech emotion recognition (SER)\nmodel and an emotional TTS model. Firstly, we train the cross-domain SER model\non both SER and TTS datasets. Then, we use emotion labels on the TTS dataset\npredicted by the trained SER model to build an auxiliary SER task and jointly\ntrain it with the TTS model. Experimental results show that our proposed method\ncan generate speech with the specified emotional expressiveness and nearly no\nhindering on the speech quality.", "published": "2020-10-26 05:31:55", "link": "http://arxiv.org/abs/2010.13350v2", "categories": ["eess.AS", "cs.SD", "I.2"], "primary_category": "eess.AS"}
{"title": "TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-Quality\n  Speech Synthesis", "abstract": "In this paper, we propose a text-to-speech (TTS)-driven data augmentation\nmethod for improving the quality of a non-autoregressive (AR) TTS system.\nRecently proposed non-AR models, such as FastSpeech 2, have successfully\nachieved fast speech synthesis system. However, their quality is not\nsatisfactory, especially when the amount of training data is insufficient. To\naddress this problem, we propose an effective data augmentation method using a\nwell-designed AR TTS system. In this method, large-scale synthetic corpora\nincluding text-waveform pairs with phoneme duration are generated by the AR TTS\nsystem and then used to train the target non-AR model. Perceptual listening\ntest results showed that the proposed method significantly improved the quality\nof the non-AR TTS system. In particular, we augmented five hours of a training\ndatabase to 179 hours of a synthetic one. Using these databases, our TTS system\nconsisting of a FastSpeech 2 acoustic model with a Parallel WaveGAN vocoder\nachieved a mean opinion score of 3.74, which is 40% higher than that achieved\nby the conventional method.", "published": "2020-10-26 08:44:27", "link": "http://arxiv.org/abs/2010.13421v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Sound Event Detection Metrics: Insights from DCASE 2020", "abstract": "The ranking of sound event detection (SED) systems may be biased by\nassumptions inherent to evaluation criteria and to the choice of an operating\npoint. This paper compares conventional event-based and segment-based criteria\nagainst the Polyphonic Sound Detection Score (PSDS)'s intersection-based\ncriterion, over a selection of systems from DCASE 2020 Challenge Task 4. It\nshows that, by relying on collars , the conventional event-based criterion\nintroduces different strictness levels depending on the length of the sound\nevents, and that the segment-based criterion may lack precision and be\napplication dependent. Alternatively, PSDS's intersection-based criterion\novercomes the dependency of the evaluation on sound event duration and provides\nrobustness to labelling subjectivity, by allowing valid detections of\ninterrupted events. Furthermore, PSDS enhances the comparison of SED systems by\nmeasuring sound event modelling performance independently from the systems'\noperating points.", "published": "2020-10-26 15:11:23", "link": "http://arxiv.org/abs/2010.13648v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network\n  for Voice Activity Detection", "abstract": "We present MarbleNet, an end-to-end neural network for Voice Activity\nDetection (VAD). MarbleNet is a deep residual network composed from blocks of\n1D time-channel separable convolution, batch-normalization, ReLU and dropout\nlayers. When compared to a state-of-the-art VAD model, MarbleNet is able to\nachieve similar performance with roughly 1/10-th the parameter cost. We further\nconduct extensive ablation studies on different training methods and choices of\nparameters in order to study the robustness of MarbleNet in real-world VAD\ntasks.", "published": "2020-10-26 20:26:05", "link": "http://arxiv.org/abs/2010.13886v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recent Developments on ESPnet Toolkit Boosted by Conformer", "abstract": "In this study, we present recent developments on ESPnet: End-to-End Speech\nProcessing toolkit, which mainly involves a recently proposed architecture\ncalled Conformer, Convolution-augmented Transformer. This paper shows the\nresults for a wide range of end-to-end speech processing applications, such as\nautomatic speech recognition (ASR), speech translations (ST), speech separation\n(SS) and text-to-speech (TTS). Our experiments reveal various training tips and\nsignificant performance benefits obtained with the Conformer on different\ntasks. These results are competitive or even outperform the current\nstate-of-art Transformer models. We are preparing to release all-in-one recipes\nusing open source and publicly available corpora for all the above tasks with\npre-trained models. Our aim for this work is to contribute to our research\ncommunity by reducing the burden of preparing state-of-the-art research\nenvironments usually requiring high resources.", "published": "2020-10-26 23:49:23", "link": "http://arxiv.org/abs/2010.13956v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Shimon the Robot Film Composer and DeepScore: An LSTM for Generation of\n  Film Scores based on Visual Analysis", "abstract": "Composing for a film requires developing an understanding of the film, its\ncharacters and the film aesthetic choices made by the director. We propose\nusing existing visual analysis systems as a core technology for film music\ngeneration. We extract film features including main characters and their\nemotions to develop a computer understanding of the film's narrative arc. This\narc is combined with visually analyzed director aesthetic choices including\npacing and levels of movement. Two systems are presented, the first using a\nrobotic film composer and marimbist to generate film scores in real-time\nperformance. The second software-based system builds on the results from the\nrobot film composer to create narrative driven film scores.", "published": "2020-10-26 19:41:47", "link": "http://arxiv.org/abs/2011.07953v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Effect of Language Proficiency on Subjective Evaluation of Noise\n  Suppression Algorithms", "abstract": "Speech communication systems based on Voice-over-IP technology are frequently\nused by native as well as non-native speakers of a target language, e.g. in\ninternational phone calls or telemeetings. Frequently, such calls also occur in\na noisy environment, making noise suppression modules necessary to increase\nperceived quality of experience. Whereas standard tests for assessing perceived\nquality make use of native listeners, we assume that noise-reduced speech and\nresidual noise may affect native and non-native listeners of a target language\nin different ways. To test this assumption, we report results of two subjective\ntests conducted with English and German native listeners who judge the quality\nof speech samples recorded by native English, German, and Mandarin speakers,\nwhich are degraded with different background noise levels and noise suppression\neffects. The experiments were conducted following the standardized ITU-T Rec.\nP.835 approach, however implemented in a crowdsourcing setting according to\nITU-T Rec. P.808. Our results show a significant influence of language on\nspeech signal ratings and, consequently, on the overall perceived quality in\nspecific conditions.", "published": "2020-10-26 00:31:42", "link": "http://arxiv.org/abs/2010.13260v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Improving pronunciation assessment via ordinal regression with anchored\n  reference samples", "abstract": "Sentence level pronunciation assessment is important for Computer Assisted\nLanguage Learning (CALL). Traditional speech pronunciation assessment, based on\nthe Goodness of Pronunciation (GOP) algorithm, has some weakness in assessing a\nspeech utterance: 1) Phoneme GOP scores cannot be easily translated into a\nsentence score with a simple average for effective assessment; 2) The rank\nordering information has not been well exploited in GOP scoring for delivering\na robust assessment and correlate well with a human rater's evaluations. In\nthis paper, we propose two new statistical features, average GOP (aGOP) and\nconfusion GOP (cGOP) and use them to train a binary classifier in Ordinal\nRegression with Anchored Reference Samples (ORARS). When the proposed approach\nis tested on Microsoft mTutor ESL Dataset, a relative improvement of Pearson\ncorrelation coefficient of 26.9% is obtained over the conventional GOP-based\none. The performance is at a human-parity level or better than human raters.", "published": "2020-10-26 04:53:17", "link": "http://arxiv.org/abs/2010.13339v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Integrating end-to-end neural and clustering-based diarization: Getting\n  the best of both worlds", "abstract": "Recent diarization technologies can be categorized into two approaches, i.e.,\nclustering and end-to-end neural approaches, which have different pros and\ncons. The clustering-based approaches assign speaker labels to speech regions\nby clustering speaker embeddings such as x-vectors. While it can be seen as a\ncurrent state-of-the-art approach that works for various challenging data with\nreasonable robustness and accuracy, it has a critical disadvantage that it\ncannot handle overlapped speech that is inevitable in natural conversational\ndata. In contrast, the end-to-end neural diarization (EEND), which directly\npredicts diarization labels using a neural network, was devised to handle the\noverlapped speech. While the EEND, which can easily incorporate emerging\ndeep-learning technologies, has started outperforming the x-vector clustering\napproach in some realistic database, it is difficult to make it work for `long'\nrecordings (e.g., recordings longer than 10 minutes) because of, e.g., its huge\nmemory consumption. Block-wise independent processing is also difficult because\nit poses an inter-block label permutation problem, i.e., an ambiguity of the\nspeaker label assignments between blocks. In this paper, we propose a simple\nbut effective hybrid diarization framework that works with overlapped speech\nand for long recordings containing an arbitrary number of speakers. It modifies\nthe conventional EEND framework to simultaneously output global speaker\nembeddings so that speaker clustering can be performed across blocks to solve\nthe permutation problem. With experiments based on simulated noisy reverberant\n2-speaker meeting-like data, we show that the proposed framework works\nsignificantly better than the original EEND especially when the input data is\nlong.", "published": "2020-10-26 06:33:02", "link": "http://arxiv.org/abs/2010.13366v2", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Melody Harmonization Using Orderless NADE, Chord Balancing, and Blocked\n  Gibbs Sampling", "abstract": "Coherence and interestingness are two criteria for evaluating the performance\nof melody harmonization, which aims to generate a chord progression from a\nsymbolic melody. In this study, we apply the concept of orderless NADE, which\ntakes the melody and its partially masked chord sequence as the input of the\nBiLSTM-based networks to learn the masked ground truth, to the training\nprocess. In addition, the class weights are used to compensate for some\nreasonable chord labels that are rarely seen in the training set. Consistent\nwith the stochasticity in training, blocked Gibbs sampling with proper numbers\nof masking/generating loops is used in the inference phase to progressively\ntrade the coherence of the generated chord sequence off against its\ninterestingness. The experiments were conducted on a dataset of 18,005\nmelody/chord pairs. Our proposed model outperforms the state-of-the-art system\nMTHarmonizer in five of six different objective metrics based on chord/melody\nharmonicity and chord progression. The subjective test results with more than\n100 participants also show the superiority of our model.", "published": "2020-10-26 10:18:46", "link": "http://arxiv.org/abs/2010.13468v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contrastive Unsupervised Learning for Audio Fingerprinting", "abstract": "The rise of video-sharing platforms has attracted more and more people to\nshoot videos and upload them to the Internet. These videos mostly contain a\ncarefully-edited background audio track, where serious speech change, pitch\nshifting and various types of audio effects may involve, and existing audio\nidentification systems may fail to recognize the audio. To solve this problem,\nin this paper, we introduce the idea of contrastive learning to the task of\naudio fingerprinting (AFP). Contrastive learning is an unsupervised approach to\nlearn representations that can effectively group similar samples and\ndiscriminate dissimilar ones. In our work, we consider an audio track and its\ndifferently distorted versions as similar while considering different audio\ntracks as dissimilar. Based on the momentum contrast (MoCo) framework, we\ndevise a contrastive learning method for AFP, which can generate fingerprints\nthat are both discriminative and robust. A set of experiments showed that our\nAFP method is effective for audio identification, with robustness to serious\naudio distortions, including the challenging speed change and pitch shifting.", "published": "2020-10-26 12:49:39", "link": "http://arxiv.org/abs/2010.13540v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Frequency Spectrum and Geometry of the Hal Saflieni Hypogeum Appear\n  Tuned", "abstract": "The Hal Saflieni Hypogeum is a unique subterranean Maltese Neolithic\nsanctuary with a well-documented history of interest in its acoustics. Previous\nstudies have noted its unusual strongly-defined frequency spectrum, but it is\nunknown if this was coincidental. In this paper, we present evidence that the\nHypogeum's creators shaped the site's geometry to create or amplify its\nfrequency spectrum, or another property closely correlated with the spectrum.\nSpecifically, we show that the observed spectrum required jointly fine-tuning\nthe dimensions of multiple non-contiguous cave walls across multiple\nindependent chambers, to a degree that seems unlikely to be coincidental. We\nalso note that the peak frequencies are evenly spaced and resemble a whole-tone\nscale in music, which is also unlikely to be coincidental and suggests the\nspectrum itself might have held some cultural significance. Taken together, it\nsuggests acoustic or spectral properties may have played a motivational or\ncultural role for the site's Neolithic creators. This work identifies one of\nthe earliest known examples of a manmade structure with a significant musical\nelement to its interior architecture.", "published": "2020-10-26 16:28:49", "link": "http://arxiv.org/abs/2010.13697v1", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "cs.SD"}
{"title": "Decentralizing Feature Extraction with Quantum Convolutional Neural\n  Network for Automatic Speech Recognition", "abstract": "We propose a novel decentralized feature extraction approach in federated\nlearning to address privacy-preservation issues for speech recognition. It is\nbuilt upon a quantum convolutional neural network (QCNN) composed of a quantum\ncircuit encoder for feature extraction, and a recurrent neural network (RNN)\nbased end-to-end acoustic model (AM). To enhance model parameter protection in\na decentralized architecture, an input speech is first up-streamed to a quantum\ncomputing server to extract Mel-spectrogram, and the corresponding\nconvolutional features are encoded using a quantum circuit algorithm with\nrandom parameters. The encoded features are then down-streamed to the local RNN\nmodel for the final recognition. The proposed decentralized framework takes\nadvantage of the quantum learning progress to secure models and to avoid\nprivacy leakage attacks. Testing on the Google Speech Commands Dataset, the\nproposed QCNN encoder attains a competitive accuracy of 95.12% in a\ndecentralized model, which is better than the previous architectures using\ncentralized RNN models with convolutional features. We also conduct an in-depth\nstudy of different quantum circuit encoder architectures to provide insights\ninto designing QCNN-based feature extractors. Neural saliency analyses\ndemonstrate a correlation between the proposed QCNN features, class activation\nmaps, and input spectrograms. We provide an implementation for future studies.", "published": "2020-10-26 03:36:01", "link": "http://arxiv.org/abs/2010.13309v2", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS", "quant-ph"], "primary_category": "cs.SD"}
