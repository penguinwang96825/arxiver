{"title": "A Dual-Stream Recurrence-Attention Network With Global-Local Awareness\n  for Emotion Recognition in Textual Dialog", "abstract": "In real-world dialog systems, the ability to understand the user's emotions\nand interact anthropomorphically is of great significance. Emotion Recognition\nin Conversation (ERC) is one of the key ways to accomplish this goal and has\nattracted growing attention. How to model the context in a conversation is a\ncentral aspect and a major challenge of ERC tasks. Most existing approaches\nstruggle to adequately incorporate both global and local contextual\ninformation, and their network structures are overly sophisticated. For this\nreason, we propose a simple and effective Dual-stream Recurrence-Attention\nNetwork (DualRAN), which is based on Recurrent Neural Network (RNN) and\nMulti-head ATtention network (MAT). DualRAN eschews the complex components of\ncurrent methods and focuses on combining recurrence-based methods with\nattention-based ones. DualRAN is a dual-stream structure mainly consisting of\nlocal- and global-aware modules, modeling a conversation simultaneously from\ndistinct perspectives. In addition, we develop two single-stream network\nvariants for DualRAN, i.e., SingleRANv1 and SingleRANv2. According to the\nexperimental findings, DualRAN boosts the weighted F1 scores by 1.43% and 0.64%\non the IEMOCAP and MELD datasets, respectively, in comparison to the strongest\nbaseline. On two other datasets (i.e., EmoryNLP and DailyDialog), our method\nalso attains competitive results.", "published": "2023-07-02 01:25:47", "link": "http://arxiv.org/abs/2307.00449v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal\n  Data", "abstract": "This paper addresses the ethical concerns arising from the use of\nunauthorized public data in deep learning models and proposes a novel solution.\nSpecifically, building on the work of Huang et al. (2021), we extend their\nbi-level optimization approach to generate unlearnable text using a\ngradient-based search technique. However, although effective, this approach\nfaces practical limitations, including the requirement of batches of instances\nand model architecture knowledge that is not readily accessible to ordinary\nusers with limited access to their own data. Furthermore, even with\nsemantic-preserving constraints, unlearnable noise can alter the text's\nsemantics. To address these challenges, we extract simple patterns from\nunlearnable text produced by bi-level optimization and demonstrate that the\ndata remains unlearnable for unknown models. Additionally, these patterns are\nnot instance- or dataset-specific, allowing users to readily apply them to text\nclassification and question-answering tasks, even if only a small proportion of\nusers implement them on their public content. We also open-source codes to\ngenerate unlearnable text and assess unlearnable noise to benefit the public\nand future studies.", "published": "2023-07-02 02:34:57", "link": "http://arxiv.org/abs/2307.00456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Enable Few-Shot Clustering", "abstract": "Unlike traditional unsupervised clustering, semi-supervised clustering allows\nusers to provide meaningful structure to the data, which helps the clustering\nalgorithm to match the user's intent. Existing approaches to semi-supervised\nclustering require a significant amount of feedback from an expert to improve\nthe clusters. In this paper, we ask whether a large language model can amplify\nan expert's guidance to enable query-efficient, few-shot semi-supervised text\nclustering. We show that LLMs are surprisingly effective at improving\nclustering. We explore three stages where LLMs can be incorporated into\nclustering: before clustering (improving input features), during clustering (by\nproviding constraints to the clusterer), and after clustering (using LLMs\npost-correction). We find incorporating LLMs in the first two stages can\nroutinely provide significant improvements in cluster quality, and that LLMs\nenable a user to make trade-offs between cost and accuracy to produce desired\nclusters. We release our code and LLM prompts for the public to use.", "published": "2023-07-02 09:17:11", "link": "http://arxiv.org/abs/2307.00524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SSP: Self-Supervised Post-training for Conversational Search", "abstract": "Conversational search has been regarded as the next-generation search\nparadigm. Constrained by data scarcity, most existing methods distill the\nwell-trained ad-hoc retriever to the conversational retriever. However, these\nmethods, which usually initialize parameters by query reformulation to discover\ncontextualized dependency, have trouble in understanding the dialogue structure\ninformation and struggle with contextual semantic vanishing. In this paper, we\npropose \\fullmodel (\\model) which is a new post-training paradigm with three\nself-supervised tasks to efficiently initialize the conversational search model\nto enhance the dialogue structure and contextual semantic understanding.\nFurthermore, the \\model can be plugged into most of the existing conversational\nmodels to boost their performance. To verify the effectiveness of our proposed\nmethod, we apply the conversational encoder post-trained by \\model on the\nconversational search task using two benchmark datasets: CAsT-19 and CAsT-20.\nExtensive experiments that our \\model can boost the performance of several\nexisting conversational search methods. Our source code is available at\n\\url{https://github.com/morecry/SSP}.", "published": "2023-07-02 13:36:36", "link": "http://arxiv.org/abs/2307.00569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PatternGPT :A Pattern-Driven Framework for Large Language Model Text\n  Generation", "abstract": "Large language models(LLMS)have shown excellent text generation capabilities,\ncapable of generating fluent human-like responses for many downstream tasks.\nHowever, applying large language models to real-world critical tasks remains\nchallenging due to their susceptibility to hallucinations and inability to\ndirectly use external knowledge. To cope with the above challenges, this paper\nproposes PatternGPT, a pattern-driven text generation framework for Large\nLanguage Models. Firstly, the framework utilizes the extraction capability of\nLarge Language Models to generate rich and diversified structured and\nformalized patterns, which facilitates the introduction of external knowledge\nto do the computation, and then draws on the idea of federated learning to use\nmultiple agents to achieve the sharing in order to obtain more diversified\npatterns, and finally uses judgment criteria and optimization algorithm to\nsearch for high-quality patterns to guide the generation of models. Finally,\nexternal knowledge such as judgment criteria and optimization algorithms are\nused to search for high-quality patterns, and the searched patterns are used to\nguide model generation. This framework has the advantages of generating\ndiversified patterns, protecting data privacy, combining external knowledge,\nand improving the quality of generation, which provides an effective method to\noptimize the text generation capability of large language models, and make it\nbetter applied to the field of intelligent dialogue and content generation.", "published": "2023-07-02 04:32:41", "link": "http://arxiv.org/abs/2307.00470v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations\n  via Residual Adapters", "abstract": "Speech representations learned in a self-supervised fashion from massive\nunlabeled speech corpora have been adapted successfully toward several\ndownstream tasks. However, such representations may be skewed toward canonical\ndata characteristics of such corpora and perform poorly on atypical, non-native\naccented speaker populations. With the state-of-the-art HuBERT model as a\nbaseline, we propose and investigate self-supervised adaptation of speech\nrepresentations to such populations in a parameter-efficient way via training\naccent-specific residual adapters. We experiment with 4 accents and choose\nautomatic speech recognition (ASR) as the downstream task of interest. We\nobtain strong word error rate reductions (WERR) over HuBERT-large for all 4\naccents, with a mean WERR of 22.7% with accent-specific adapters and a mean\nWERR of 25.1% if the entire encoder is accent-adapted. While our experiments\nutilize HuBERT and ASR as the downstream task, our proposed approach is both\nmodel and task-agnostic.", "published": "2023-07-02 02:21:29", "link": "http://arxiv.org/abs/2307.00453v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "GenRec: Large Language Model for Generative Recommendation", "abstract": "In recent years, large language models (LLM) have emerged as powerful tools\nfor diverse natural language processing tasks. However, their potential for\nrecommender systems under the generative recommendation paradigm remains\nrelatively unexplored. This paper presents an innovative approach to\nrecommendation systems using large language models (LLMs) based on text data.\nIn this paper, we present a novel LLM for generative recommendation (GenRec)\nthat utilized the expressive power of LLM to directly generate the target item\nto recommend, rather than calculating ranking score for each candidate item one\nby one as in traditional discriminative recommendation. GenRec uses LLM's\nunderstanding ability to interpret context, learn user preferences, and\ngenerate relevant recommendation. Our proposed approach leverages the vast\nknowledge encoded in large language models to accomplish recommendation tasks.\nWe first we formulate specialized prompts to enhance the ability of LLM to\ncomprehend recommendation tasks. Subsequently, we use these prompts to\nfine-tune the LLaMA backbone LLM on a dataset of user-item interactions,\nrepresented by textual data, to capture user preferences and item\ncharacteristics. Our research underscores the potential of LLM-based generative\nrecommendation in revolutionizing the domain of recommendation systems and\noffers a foundational framework for future explorations in this field. We\nconduct extensive experiments on benchmark datasets, and the experiments shows\nthat our GenRec has significant better results on large dataset.", "published": "2023-07-02 02:37:07", "link": "http://arxiv.org/abs/2307.00457v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text", "abstract": "The task of textual geolocation - retrieving the coordinates of a place based\non a free-form language description - calls for not only grounding but also\nnatural language understanding and geospatial reasoning. Even though there are\nquite a few datasets in English used for geolocation, they are currently based\non open-source data (Wikipedia and Twitter), where the location of the\ndescribed place is mostly implicit, such that the location retrieval resolution\nis limited. Furthermore, there are no datasets available for addressing the\nproblem of textual geolocation in morphologically rich and resource-poor\nlanguages, such as Hebrew. In this paper, we present the Hebrew Geo-Location\n(HeGeL) corpus, designed to collect literal place descriptions and analyze\nlingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place\ndescriptions of various place types in three cities in Israel. Qualitative and\nempirical analysis show that the data exhibits abundant use of geospatial\nreasoning and requires a novel environmental representation.", "published": "2023-07-02 08:09:10", "link": "http://arxiv.org/abs/2307.00509v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed\n  Search Logs for Zero-shot Biomedical Information Retrieval", "abstract": "Information retrieval (IR) is essential in biomedical knowledge acquisition\nand clinical decision support. While recent progress has shown that language\nmodel encoders perform better semantic retrieval, training such models requires\nabundant query-article annotations that are difficult to obtain in biomedicine.\nAs a result, most biomedical IR systems only conduct lexical matching. In\nresponse, we introduce MedCPT, a first-of-its-kind Contrastively Pre-trained\nTransformer model for zero-shot semantic IR in biomedicine. To train MedCPT, we\ncollected an unprecedented scale of 255 million user click logs from PubMed.\nWith such data, we use contrastive learning to train a pair of\nclosely-integrated retriever and re-ranker. Experimental results show that\nMedCPT sets new state-of-the-art performance on six biomedical IR tasks,\noutperforming various baselines including much larger models such as\nGPT-3-sized cpt-text-XL. In addition, MedCPT also generates better biomedical\narticle and sentence representations for semantic evaluations. As such, MedCPT\ncan be readily applied to various real-world biomedical IR tasks.", "published": "2023-07-02 15:11:59", "link": "http://arxiv.org/abs/2307.00589v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "q-bio.QM"], "primary_category": "cs.IR"}
{"title": "Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to\n  Estimate the Check-Worthiness of Multi-Modal Tweets", "abstract": "The option of sharing images, videos and audio files on social media opens up\nnew possibilities for distinguishing between false information and fake news on\nthe Internet. Due to the vast amount of data shared every second on social\nmedia, not all data can be verified by a computer or a human expert. Here, a\ncheck-worthiness analysis can be used as a first step in the fact-checking\npipeline and as a filtering mechanism to improve efficiency. This paper\nproposes a novel way of detecting the check-worthiness in multi-modal tweets.\nIt takes advantage of two classifiers, each trained on a single modality. For\nimage data, extracting the embedded text with an OCR analysis has shown to\nperform best. By combining the two classifiers, the proposed solution was able\nto place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297\nachieved on the private test set.", "published": "2023-07-02 16:35:54", "link": "http://arxiv.org/abs/2307.00610v2", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Conformer LLMs -- Convolution Augmented Large Language Models", "abstract": "This work builds together two popular blocks of neural architecture, namely\nconvolutional layers and Transformers, for large language models (LLMs).\nNon-causal conformers are used ubiquitously in automatic speech recognition.\nThis work aims to adapt these architectures in a causal setup for training\nLLMs. Transformers decoders effectively capture long-range dependencies over\nseveral modalities and form a core backbone of modern advancements in machine\nlearning. Convolutional architectures have been popular in extracting features\nin domains such as raw 1-D signals, speech, and images, to name a few. In this\npaper, by combining local and global dependencies over latent representations\nusing causal convolutional filters and Transformer, we achieve significant\ngains in performance. This work showcases a robust speech architecture that can\nbe integrated and adapted in a causal setup beyond speech applications for\nlarge-scale language modeling.", "published": "2023-07-02 03:05:41", "link": "http://arxiv.org/abs/2307.00461v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "cs.CL"}
{"title": "TensorGPT: Efficient Compression of Large Language Models based on\n  Tensor-Train Decomposition", "abstract": "High-dimensional token embeddings underpin Large Language Models (LLMs), as\nthey can capture subtle semantic information and significantly enhance the\nmodelling of complex language patterns. However, this high dimensionality also\nintroduces considerable model parameters and prohibitively high model storage\nand memory requirements, which is particularly unaffordable for low-end\ndevices. Targeting no extra training data and insufficient computation cases,\nwe propose a training-free model compression approach based on the Tensor-Train\nDecomposition (TTD), whereby each pre-trained token embedding is converted into\na lower-dimensional Matrix Product State (MPS). We then comprehensively\ninvestigate the low-rank structures extracted by this approach, in terms of the\ncompression ratio, the language task performance, and latency on a typical\nlow-end device (i.e. Raspberry Pi). Taking GPT family models (i.e. GPT-2 and\nCerebrasGPT) as case studies, our approach theoretically results in $46.89\\%$\nfewer parameters of the entire model, with a compression ratio $39.38\\times$ -\n$65.64\\times$ for the embedding layers. With different hyperparameter choices,\nthe model compressed with our approach can achieve a comparable language task\nperformance to the original model with around $2.0\\times$ embedding layer\ncompression. This empirically proves the existence of low-rank structure in GPT\nfamily models, and demonstrates that about half of the parameters in the\nembedding layers are redundant.", "published": "2023-07-02 09:33:09", "link": "http://arxiv.org/abs/2307.00526v2", "categories": ["cs.CL", "cs.LG", "cs.NA", "cs.NE", "math.NA"], "primary_category": "cs.CL"}
