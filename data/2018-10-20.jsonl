{"title": "Improving Multilingual Semantic Textual Similarity with Shared Sentence\n  Encoder for Low-resource Languages", "abstract": "Measuring the semantic similarity between two sentences (or Semantic Textual\nSimilarity - STS) is fundamental in many NLP applications. Despite the\nremarkable results in supervised settings with adequate labeling, little\nattention has been paid to this task in low-resource languages with\ninsufficient labeling. Existing approaches mostly leverage machine translation\ntechniques to translate sentences into rich-resource language. These approaches\neither beget language biases, or be impractical in industrial applications\nwhere spoken language scenario is more often and rigorous efficiency is\nrequired. In this work, we propose a multilingual framework to tackle the STS\ntask in a low-resource language e.g. Spanish, Arabic , Indonesian and Thai, by\nutilizing the rich annotation data in a rich resource language, e.g. English.\nOur approach is extended from a basic monolingual STS framework to a shared\nmultilingual encoder pretrained with translation task to incorporate\nrich-resource language data. By exploiting the nature of a shared multilingual\nencoder, one sentence can have multiple representations for different target\ntranslation language, which are used in an ensemble model to improve similarity\nevaluation. We demonstrate the superiority of our method over other state of\nthe art approaches on SemEval STS task by its significant improvement on non-MT\nmethod, as well as an online industrial product where MT method fails to beat\nbaseline while our approach still has consistently improvements.", "published": "2018-10-20 03:00:53", "link": "http://arxiv.org/abs/1810.08740v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Composite Labels for Neural Morphological Tagging", "abstract": "Neural morphological tagging has been regarded as an extension to POS tagging\ntask, treating each morphological tag as a monolithic label and ignoring its\ninternal structure. We propose to view morphological tags as composite labels\nand explicitly model their internal structure in a neural sequence tagger. For\nthis, we explore three different neural architectures and compare their\nperformance with both CRF and simple neural multiclass baselines. We evaluate\nour models on 49 languages and show that the neural architecture that models\nthe morphological labels as sequences of morphological category values performs\nsignificantly better than both baselines establishing state-of-the-art results\nin morphological tagging for most languages.", "published": "2018-10-20 15:00:23", "link": "http://arxiv.org/abs/1810.08815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstractive Summarization Using Attentive Neural Techniques", "abstract": "In a world of proliferating data, the ability to rapidly summarize text is\ngrowing in importance. Automatic summarization of text can be thought of as a\nsequence to sequence problem. Another area of natural language processing that\nsolves a sequence to sequence problem is machine translation, which is rapidly\nevolving due to the development of attention-based encoder-decoder networks.\nThis work applies these modern techniques to abstractive summarization. We\nperform analysis on various attention mechanisms for summarization with the\ngoal of developing an approach and architecture aimed at improving the state of\nthe art. In particular, we modify and optimize a translation model with\nself-attention for generating abstractive sentence summaries. The effectiveness\nof this base model along with attention variants is compared and analyzed in\nthe context of standardized evaluation sets and test metrics. However, we show\nthat these metrics are limited in their ability to effectively score\nabstractive summaries, and propose a new approach based on the intuition that\nan abstractive model requires an abstractive evaluation.", "published": "2018-10-20 18:24:48", "link": "http://arxiv.org/abs/1810.08838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence\n  Inference", "abstract": "Reasoning about implied relationships (e.g., paraphrastic, common sense,\nencyclopedic) between pairs of words is crucial for many cross-sentence\ninference problems. This paper proposes new methods for learning and using\nembeddings of word pairs that implicitly represent background knowledge about\nsuch relationships. Our pairwise embeddings are computed as a compositional\nfunction on word representations, which is learned by maximizing the pointwise\nmutual information (PMI) with the contexts in which the two words co-occur. We\nadd these representations to the cross-sentence attention layer of existing\ninference models (e.g. BiDAF for QA, ESIM for NLI), instead of extending or\nreplacing existing word embeddings. Experiments show a gain of 2.7% on the\nrecently released SQuAD2.0 and 1.3% on MultiNLI. Our representations also aid\nin better generalization with gains of around 6-7% on adversarial SQuAD\ndatasets, and 8.8% on the adversarial entailment test set by Glockner et al.\n(2018).", "published": "2018-10-20 21:37:08", "link": "http://arxiv.org/abs/1810.08854v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Context Modelling in Multimodal Dialogue Generation", "abstract": "In this work, we investigate the task of textual response generation in a\nmultimodal task-oriented dialogue system. Our work is based on the recently\nreleased Multimodal Dialogue (MMD) dataset (Saha et al., 2017) in the fashion\ndomain. We introduce a multimodal extension to the Hierarchical Recurrent\nEncoder-Decoder (HRED) model and show that this extension outperforms strong\nbaselines in terms of text-based similarity metrics. We also showcase the\nshortcomings of current vision and language models by performing an error\nanalysis on our system's output.", "published": "2018-10-20 17:07:42", "link": "http://arxiv.org/abs/1810.11955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collective Learning From Diverse Datasets for Entity Typing in the Wild", "abstract": "Entity typing (ET) is the problem of assigning labels to given entity\nmentions in a sentence. Existing works for ET require knowledge about the\ndomain and target label set for a given test instance. ET in the absence of\nsuch knowledge is a novel problem that we address as ET in the wild. We\nhypothesize that the solution to this problem is to build supervised models\nthat generalize better on the ET task as a whole, rather than a specific\ndataset. In this direction, we propose a Collective Learning Framework (CLF),\nwhich enables learning from diverse datasets in a unified way. The CLF first\ncreates a unified hierarchical label set (UHLS) and a label mapping by\naggregating label information from all available datasets. Then it builds a\nsingle neural network classifier using UHLS, label mapping, and a partial loss\nfunction. The single classifier predicts the finest possible label across all\navailable domains even though these labels may not be present in any\ndomain-specific dataset. We also propose a set of evaluation schemes and\nmetrics to evaluate the performance of models in this novel problem. Extensive\nexperimentation on seven diverse real-world datasets demonstrates the efficacy\nof our CLF.", "published": "2018-10-20 09:59:31", "link": "http://arxiv.org/abs/1810.08782v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Knowledge-Grounded Multimodal Search-Based Conversational Agent", "abstract": "Multimodal search-based dialogue is a challenging new task: It extends\nvisually grounded question answering systems into multi-turn conversations with\naccess to an external database. We address this new challenge by learning a\nneural response generation system from the recently released Multimodal\nDialogue (MMD) dataset (Saha et al., 2017). We introduce a knowledge-grounded\nmultimodal conversational model where an encoded knowledge base (KB)\nrepresentation is appended to the decoder input. Our model substantially\noutperforms strong baselines in terms of text-based similarity measures (over 9\nBLEU points, 3 of which are solely due to the use of additional information\nfrom the KB.", "published": "2018-10-20 16:58:54", "link": "http://arxiv.org/abs/1810.11954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition on Twitter for Turkish using Semi-supervised\n  Learning with Word Embeddings", "abstract": "Recently, due to the increasing popularity of social media, the necessity for\nextracting information from informal text types, such as microblog texts, has\ngained significant attention. In this study, we focused on the Named Entity\nRecognition (NER) problem on informal text types for Turkish. We utilized a\nsemi-supervised learning approach based on neural networks. We applied a fast\nunsupervised method for learning continuous representations of words in vector\nspace. We made use of these obtained word embeddings, together with language\nindependent features that are engineered to work better on informal text types,\nfor generating a Turkish NER system on microblog texts. We evaluated our\nTurkish NER system on Twitter messages and achieved better F-score performances\nthan the published results of previously proposed NER systems on Turkish\ntweets. Since we did not employ any language dependent features, we believe\nthat our method can be easily adapted to microblog texts in other\nmorphologically rich languages.", "published": "2018-10-20 02:00:35", "link": "http://arxiv.org/abs/1810.08732v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Hierarchical Text Generation using an Outline", "abstract": "Many challenges in natural language processing require generating text,\nincluding language translation, dialogue generation, and speech recognition.\nFor all of these problems, text generation becomes more difficult as the text\nbecomes longer. Current language models often struggle to keep track of\ncoherence for long pieces of text. Here, we attempt to have the model construct\nand use an outline of the text it generates to keep it focused. We find that\nthe usage of an outline improves perplexity. We do not find that using the\noutline improves human evaluation over a simpler baseline, revealing a\ndiscrepancy in perplexity and human perception. Similarly, hierarchical\ngeneration is not found to improve human evaluation scores.", "published": "2018-10-20 13:27:04", "link": "http://arxiv.org/abs/1810.08802v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
