{"title": "Relation extraction between the clinical entities based on the shortest\n  dependency path based LSTM", "abstract": "Owing to the exponential rise in the electronic medical records, information\nextraction in this domain is becoming an important area of research in recent\nyears. Relation extraction between the medical concepts such as medical\nproblem, treatment, and test etc. is also one of the most important tasks in\nthis area. In this paper, we present an efficient relation extraction system\nbased on the shortest dependency path (SDP) generated from the dependency\nparsed tree of the sentence. Instead of relying on many handcrafted features\nand the whole sequence of tokens present in a sentence, our system relies only\non the SDP between the target entities. For every pair of entities, the system\ntakes only the words in the SDP, their dependency labels, Part-of-Speech\ninformation and the types of the entities as the input. We develop a dependency\nparser for extracting dependency information. We perform our experiments on the\nbenchmark i2b2 dataset for clinical relation extraction challenge 2010.\nExperimental results show that our system outperforms the existing systems.", "published": "2019-03-24 07:54:57", "link": "http://arxiv.org/abs/1903.09941v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep recommender engine based on efficient product embeddings neural\n  pipeline", "abstract": "Predictive analytics systems are currently one of the most important areas of\nresearch and development within the Artificial Intelligence domain and\nparticularly in Machine Learning. One of the \"holy grails\" of predictive\nanalytics is the research and development of the \"perfect\" recommendation\nsystem. In our paper, we propose an advanced pipeline model for the multi-task\nobjective of determining product complementarity, similarity and sales\nprediction using deep neural models applied to big-data sequential transaction\nsystems. Our highly parallelized hybrid model pipeline consists of both\nunsupervised and supervised models, used for the objectives of generating\nsemantic product embeddings and predicting sales, respectively. Our\nexperimentation and benchmarking processes have been done using pharma industry\nretail real-life transactional Big-Data streams.", "published": "2019-03-24 08:11:58", "link": "http://arxiv.org/abs/1903.09942v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Optimization of Speaker Extraction Neural Network with Magnitude and\n  Temporal Spectrum Approximation Loss", "abstract": "The SpeakerBeam-FE (SBF) method is proposed for speaker extraction. It\nattempts to overcome the problem of unknown number of speakers in an audio\nrecording during source separation. The mask approximation loss of SBF is\nsub-optimal, which doesn't calculate direct signal reconstruction error and\nconsider the speech context. To address these problems, this paper proposes a\nmagnitude and temporal spectrum approximation loss to estimate a phase\nsensitive mask for the target speaker with the speaker characteristics.\nMoreover, this paper explores a concatenation framework instead of the context\nadaptive deep neural network in the SBF method to encode a speaker embedding\ninto the mask estimation network. Experimental results under open evaluation\ncondition show that the proposed method achieves 70.4% and 17.7% relative\nimprovement over the SBF baseline on signal-to-distortion ratio (SDR) and\nperceptual evaluation of speech quality (PESQ), respectively. A further\nanalysis demonstrates 69.1% and 72.3% relative SDR improvements obtained by the\nproposed method for different and same gender mixtures.", "published": "2019-03-24 09:25:26", "link": "http://arxiv.org/abs/1903.09952v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Abstractive Text Summarization and Fake News Detection", "abstract": "In this work, we study abstractive text summarization by exploring different\nmodels such as LSTM-encoder-decoder with attention, pointer-generator networks,\ncoverage mechanisms, and transformers. Upon extensive and careful\nhyperparameter tuning we compare the proposed architectures against each other\nfor the abstractive text summarization task. Finally, as an extension of our\nwork, we apply our text summarization model as a feature extractor for a fake\nnews detection task where the news articles prior to classification will be\nsummarized and the results are compared against the classification using only\nthe original news text.\n  keywords: LSTM, encoder-deconder, abstractive text summarization,\npointer-generator, coverage mechanism, transformers, fake news detection", "published": "2019-03-24 07:27:51", "link": "http://arxiv.org/abs/1904.00788v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
