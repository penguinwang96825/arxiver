{"title": "The Analysis of Synonymy and Antonymy in Discourse Relations: An\n  interpretable Modeling Approach", "abstract": "The idea that discourse relations are construed through explicit content and\nshared, or implicit, knowledge between producer and interpreter is ubiquitous\nin discourse research and linguistics. However, the actual contribution of the\nlexical semantics of arguments is unclear. We propose a computational approach\nto the analysis of contrast and concession relations in the PDTB corpus. Our\nwork sheds light on the extent to which lexical semantics contributes to\nsignaling explicit and implicit discourse relations and clarifies the\ncontribution of different parts of speech in both. This study contributes to\nbridging the gap between corpus linguistics and computational linguistics by\nproposing transparent and explainable models of discourse relations based on\nthe synonymy and antonymy of their arguments.", "published": "2022-08-09 00:56:53", "link": "http://arxiv.org/abs/2208.04479v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring Hate Speech Detection with HateXplain and BERT", "abstract": "Hate Speech takes many forms to target communities with derogatory comments,\nand takes humanity a step back in societal progress. HateXplain is a recently\npublished and first dataset to use annotated spans in the form of rationales,\nalong with speech classification categories and targeted communities to make\nthe classification more humanlike, explainable, accurate and less biased. We\ntune BERT to perform this task in the form of rationales and class prediction,\nand compare our performance on different metrics spanning across accuracy,\nexplainability and bias. Our novelty is threefold. Firstly, we experiment with\nthe amalgamated rationale class loss with different importance values.\nSecondly, we experiment extensively with the ground truth attention values for\nthe rationales. With the introduction of conservative and lenient attentions,\nwe compare performance of the model on HateXplain and test our hypothesis.\nThirdly, in order to improve the unintended bias in our models, we use masking\nof the target community words and note the improvement in bias and\nexplainability metrics. Overall, we are successful in achieving model\nexplanability, bias removal and several incremental improvements on the\noriginal BERT implementation.", "published": "2022-08-09 01:32:44", "link": "http://arxiv.org/abs/2208.04489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Embarrassingly Easy but Strong Baseline for Nested Named Entity\n  Recognition", "abstract": "Named entity recognition (NER) is the task to detect and classify the entity\nspans in the text. When entity spans overlap between each other, this problem\nis named as nested NER. Span-based methods have been widely used to tackle the\nnested NER. Most of these methods will get a score $n \\times n$ matrix, where\n$n$ means the length of sentence, and each entry corresponds to a span.\nHowever, previous work ignores spatial relations in the score matrix. In this\npaper, we propose using Convolutional Neural Network (CNN) to model these\nspatial relations in the score matrix. Despite being simple, experiments in\nthree commonly used nested NER datasets show that our model surpasses several\nrecently proposed methods with the same pre-trained encoders. Further analysis\nshows that using CNN can help the model find more nested entities. Besides, we\nfound that different papers used different sentence tokenizations for the three\nnested NER datasets, which will influence the comparison. Thus, we release a\npre-processing script to facilitate future comparison.", "published": "2022-08-09 04:33:46", "link": "http://arxiv.org/abs/2208.04534v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Detection From Tweets Using a BERT and SVM Ensemble Model", "abstract": "Automatic identification of emotions expressed in Twitter data has a wide\nrange of applications. We create a well-balanced dataset by adding a neutral\nclass to a benchmark dataset consisting of four emotions: fear, sadness, joy,\nand anger. On this extended dataset, we investigate the use of Support Vector\nMachine (SVM) and Bidirectional Encoder Representations from Transformers\n(BERT) for emotion recognition. We propose a novel ensemble model by combining\nthe two BERT and SVM models. Experiments show that the proposed model achieves\na state-of-the-art accuracy of 0.91 on emotion recognition in tweets.", "published": "2022-08-09 05:32:29", "link": "http://arxiv.org/abs/2208.04547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Positively transitioned sentiment dialogue corpus for developing\n  emotion-affective open-domain chatbots", "abstract": "In this paper, we describe a data enhancement method for developing Emily, an\nemotion-affective open-domain chatbot. The proposed method is based on\nexplicitly modeling positively transitioned (PT) sentiment data from multi-turn\ndialogues. We construct a dialogue corpus with PT sentiment data and will\nrelease it for public use. By fine-tuning a pretrained dialogue model using the\nproduced PT-enhanced dialogues, we are able to develop an emotion-affective\nopen-domain chatbot exhibiting close-to-human performance in various\nemotion-affective metrics. We evaluate Emily against a few state-of-the-art\n(SOTA) open-domain chatbots and show the effectiveness of the proposed\napproach. The corpus is made publicly available.", "published": "2022-08-09 07:05:14", "link": "http://arxiv.org/abs/2208.04565v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Where's the Learning in Representation Learning for Compositional\n  Semantics and the Case of Thematic Fit", "abstract": "Observing that for certain NLP tasks, such as semantic role prediction or\nthematic fit estimation, random embeddings perform as well as pretrained\nembeddings, we explore what settings allow for this and examine where most of\nthe learning is encoded: the word embeddings, the semantic role embeddings, or\n``the network''. We find nuanced answers, depending on the task and its\nrelation to the training objective. We examine these representation learning\naspects in multi-task learning, where role prediction and role-filling are\nsupervised tasks, while several thematic fit tasks are outside the models'\ndirect supervision. We observe a non-monotonous relation between some tasks'\nquality score and the training data size. In order to better understand this\nobservation, we analyze these results using easier, per-verb versions of these\ntasks.", "published": "2022-08-09 12:37:46", "link": "http://arxiv.org/abs/2208.04749v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Evaluation on Japanese Textual Entailment and Similarity", "abstract": "Natural Language Inference (NLI) and Semantic Textual Similarity (STS) are\nwidely used benchmark tasks for compositional evaluation of pre-trained\nlanguage models. Despite growing interest in linguistic universals, most\nNLI/STS studies have focused almost exclusively on English. In particular,\nthere are no available multilingual NLI/STS datasets in Japanese, which is\ntypologically different from English and can shed light on the currently\ncontroversial behavior of language models in matters such as sensitivity to\nword order and case particles. Against this background, we introduce JSICK, a\nJapanese NLI/STS dataset that was manually translated from the English dataset\nSICK. We also present a stress-test dataset for compositional inference,\ncreated by transforming syntactic structures of sentences in JSICK to\ninvestigate whether language models are sensitive to word order and case\nparticles. We conduct baseline experiments on different pre-trained language\nmodels and compare the performance of multilingual models when applied to\nJapanese and other languages. The results of the stress-test experiments\nsuggest that the current pre-trained language models are insensitive to word\norder and case marking.", "published": "2022-08-09 15:10:56", "link": "http://arxiv.org/abs/2208.04826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for Systems Engineering: Automatic\n  Generation of Systems Modelling Language Diagrams", "abstract": "The design of complex engineering systems is an often long and articulated\nprocess that highly relies on engineers' expertise and professional judgment.\nAs such, the typical pitfalls of activities involving the human factor often\nmanifest themselves in terms of lack of completeness or exhaustiveness of the\nanalysis, inconsistencies across design choices or documentation, as well as an\nimplicit degree of subjectivity. An approach is proposed to assist systems\nengineers in the automatic generation of systems diagrams from unstructured\nnatural language text. Natural Language Processing (NLP) techniques are used to\nextract entities and their relationships from textual resources (e.g.,\nspecifications, manuals, technical reports, maintenance reports) available\nwithin an organisation, and convert them into Systems Modelling Language\n(SysML) diagrams, with particular focus on structure and requirement diagrams.\nThe intention is to provide the users with a more standardised, comprehensive\nand automated starting point onto which subsequently refine and adapt the\ndiagrams according to their needs. The proposed approach is flexible and\nopen-domain. It consists of six steps which leverage open-access tools, and it\nleads to an automatic generation of SysML diagrams without intermediate\nmodelling requirement, but through the specification of a set of parameters by\nthe user. The applicability and benefits of the proposed approach are shown\nthrough six case studies having different textual sources as inputs, and\nbenchmarked against manually defined diagram elements.", "published": "2022-08-09 19:20:33", "link": "http://arxiv.org/abs/2208.05008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Limitations of Language Models in Arithmetic and Symbolic Induction", "abstract": "Recent work has shown that large pretrained Language Models (LMs) can not\nonly perform remarkably well on a range of Natural Language Processing (NLP)\ntasks but also start improving on reasoning tasks such as arithmetic induction,\nsymbolic manipulation, and commonsense reasoning with increasing size of\nmodels. However, it is still unclear what the underlying capabilities of these\nLMs are. Surprisingly, we find that these models have limitations on certain\nbasic symbolic manipulation tasks such as copy, reverse, and addition. When the\ntotal number of symbols or repeating symbols increases, the model performance\ndrops quickly. We investigate the potential causes behind this phenomenon and\nexamine a set of possible methods, including explicit positional markers,\nfine-grained computation steps, and LMs with callable programs. Experimental\nresults show that none of these techniques can solve the simplest addition\ninduction problem completely. In the end, we introduce LMs with tutor, which\ndemonstrates every single step of teaching. LMs with tutor is able to deliver\n100% accuracy in situations of OOD and repeating symbols, shedding new insights\non the boundary of large LMs in induction.", "published": "2022-08-09 21:47:01", "link": "http://arxiv.org/abs/2208.05051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multimodal Transformer: Fusing Clinical Notes with Structured EHR Data\n  for Interpretable In-Hospital Mortality Prediction", "abstract": "Deep-learning-based clinical decision support using structured electronic\nhealth records (EHR) has been an active research area for predicting risks of\nmortality and diseases. Meanwhile, large amounts of narrative clinical notes\nprovide complementary information, but are often not integrated into predictive\nmodels. In this paper, we provide a novel multimodal transformer to fuse\nclinical notes and structured EHR data for better prediction of in-hospital\nmortality. To improve interpretability, we propose an integrated gradients (IG)\nmethod to select important words in clinical notes and discover the critical\nstructured EHR features with Shapley values. These important words and clinical\nfeatures are visualized to assist with interpretation of the prediction\noutcomes. We also investigate the significance of domain adaptive pretraining\nand task adaptive fine-tuning on the Clinical BERT, which is used to learn the\nrepresentations of clinical notes. Experiments demonstrated that our model\noutperforms other methods (AUCPR: 0.538, AUCROC: 0.877, F1:0.490).", "published": "2022-08-09 03:49:52", "link": "http://arxiv.org/abs/2208.10240v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "High Recall Data-to-text Generation with Progressive Edit", "abstract": "Data-to-text (D2T) generation is the task of generating texts from structured\ninputs. We observed that when the same target sentence was repeated twice,\nTransformer (T5) based model generates an output made up of asymmetric\nsentences from structured inputs. In other words, these sentences were\ndifferent in length and quality. We call this phenomenon \"Asymmetric\nGeneration\" and we exploit this in D2T generation. Once asymmetric sentences\nare generated, we add the first part of the output with a no-repeated-target.\nAs this goes through progressive edit (ProEdit), the recall increases. Hence,\nthis method better covers structured inputs than before editing. ProEdit is a\nsimple but effective way to improve performance in D2T generation and it\nachieves the new stateof-the-art result on the ToTTo dataset", "published": "2022-08-09 06:22:05", "link": "http://arxiv.org/abs/2208.04558v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Impact of Data Corruption on Named Entity Recognition for\n  Low-resourced Languages", "abstract": "Data availability and quality are major challenges in natural language\nprocessing for low-resourced languages. In particular, there is significantly\nless data available than for higher-resourced languages. This data is also\noften of low quality, rife with errors, invalid text or incorrect annotations.\nMany prior works focus on dealing with these problems, either by generating\nsynthetic data, or filtering out low-quality parts of datasets. We instead\ninvestigate these factors more deeply, by systematically measuring the effect\nof data quantity and quality on the performance of pre-trained language models\nin a low-resourced setting. Our results show that having fewer\ncompletely-labelled sentences is significantly better than having more\nsentences with missing labels; and that models can perform remarkably well with\nonly 10% of the training data. Importantly, these results are consistent across\nten low-resource languages, English, and four pre-trained models.", "published": "2022-08-09 07:15:20", "link": "http://arxiv.org/abs/2208.04568v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ASR Error Correction with Constrained Decoding on Operation Prediction", "abstract": "Error correction techniques remain effective to refine outputs from automatic\nspeech recognition (ASR) models. Existing end-to-end error correction methods\nbased on an encoder-decoder architecture process all tokens in the decoding\nphase, creating undesirable latency. In this paper, we propose an ASR error\ncorrection method utilizing the predictions of correction operations. More\nspecifically, we construct a predictor between the encoder and the decoder to\nlearn if a token should be kept (\"K\"), deleted (\"D\"), or changed (\"C\") to\nrestrict decoding to only part of the input sequence embeddings (the \"C\"\ntokens) for fast inference. Experiments on three public datasets demonstrate\nthe effectiveness of the proposed approach in reducing the latency of the\ndecoding process in ASR correction. It enhances the inference speed by at least\nthree times (3.4 and 5.7 times) while maintaining the same level of accuracy\n(with WER reductions of 0.53% and 1.69% respectively) for our two proposed\nmodels compared to a solid encoder-decoder baseline. In the meantime, we\nproduce and release a benchmark dataset contributing to the ASR error\ncorrection community to foster research along this line.", "published": "2022-08-09 09:59:30", "link": "http://arxiv.org/abs/2208.04641v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Thai Wav2Vec2.0 with CommonVoice V8", "abstract": "Recently, Automatic Speech Recognition (ASR), a system that converts audio\ninto text, has caught a lot of attention in the machine learning community.\nThus, a lot of publicly available models were released in HuggingFace. However,\nmost of these ASR models are available in English; only a minority of the\nmodels are available in Thai. Additionally, most of the Thai ASR models are\nclosed-sourced, and the performance of existing open-sourced models lacks\nrobustness. To address this problem, we train a new ASR model on a pre-trained\nXLSR-Wav2Vec model with the Thai CommonVoice corpus V8 and train a trigram\nlanguage model to boost the performance of our ASR model. We hope that our\nmodels will be beneficial to individuals and the ASR community in Thailand.", "published": "2022-08-09 14:21:48", "link": "http://arxiv.org/abs/2208.04799v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Anchor-Free Detector for Continuous Speech Keyword Spotting", "abstract": "Continuous Speech Keyword Spotting (CSKWS) is a task to detect predefined\nkeywords in a continuous speech. In this paper, we regard CSKWS as a\none-dimensional object detection task and propose a novel anchor-free detector,\nnamed AF-KWS, to solve the problem. AF-KWS directly regresses the center\nlocations and lengths of the keywords through a single-stage deep neural\nnetwork. In particular, AF-KWS is tailored for this speech task as we introduce\nan auxiliary unknown class to exclude other words from non-speech or silent\nbackground. We have built two benchmark datasets named LibriTop-20 and\ncontinuous meeting analysis keywords (CMAK) dataset for CSKWS. Evaluations on\nthese two datasets show that our proposed AF-KWS outperforms reference schemes\nby a large margin, and therefore provides a decent baseline for future\nresearch.", "published": "2022-08-09 09:37:49", "link": "http://arxiv.org/abs/2208.04622v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recycling an anechoic pre-trained speech separation deep neural network\n  for binaural dereverberation of a single source", "abstract": "Reverberation results in reduced intelligibility for both normal and\nhearing-impaired listeners. This paper presents a novel psychoacoustic approach\nof dereverberation of a single speech source by recycling a pre-trained\nbinaural anechoic speech separation neural network. As training the deep neural\nnetwork (DNN) is a lengthy and computationally expensive process, the advantage\nof using a pre-trained separation network for dereverberation is that the\nnetwork does not need to be retrained, saving both time and computational\nresources. The interaural cues of a reverberant source are given to this\npretrained neural network to discriminate between the direct path signal and\nthe reverberant speech. The results show an average improvement of 1.3% in\nsignal intelligibility, 0.83 dB in SRMR (signal to reverberation energy ratio)\nand 0.16 points in perceptual evaluation of speech quality (PESQ) over other\nstate-of-the-art signal processing dereverberation algorithms and 14% in\nintelligibility and 0.35 points in quality over orthogonal matching pursuit\nwith spectral subtraction (OSS), a machine learning based dereverberation\nalgorithm.", "published": "2022-08-09 09:43:43", "link": "http://arxiv.org/abs/2208.04626v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DDSP-based Singing Vocoders: A New Subtractive-based Synthesizer and A\n  Comprehensive Evaluation", "abstract": "A vocoder is a conditional audio generation model that converts acoustic\nfeatures such as mel-spectrograms into waveforms. Taking inspiration from\nDifferentiable Digital Signal Processing (DDSP), we propose a new vocoder named\nSawSing for singing voices. SawSing synthesizes the harmonic part of singing\nvoices by filtering a sawtooth source signal with a linear time-variant finite\nimpulse response filter whose coefficients are estimated from the input\nmel-spectrogram by a neural network. As this approach enforces phase\ncontinuity, SawSing can generate singing voices without the phase-discontinuity\nglitch of many existing vocoders. Moreover, the source-filter assumption\nprovides an inductive bias that allows SawSing to be trained on a small amount\nof data. Our experiments show that SawSing converges much faster and\noutperforms state-of-the-art generative adversarial network and diffusion-based\nvocoders in a resource-limited scenario with only 3 training recordings and a\n3-hour training time.", "published": "2022-08-09 13:06:08", "link": "http://arxiv.org/abs/2208.04756v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generative Data Augmentation Guided by Triplet Loss for Speech Emotion\n  Recognition", "abstract": "Speech Emotion Recognition (SER) is crucial for human-computer interaction\nbut still remains a challenging problem because of two major obstacles: data\nscarcity and imbalance. Many datasets for SER are substantially imbalanced,\nwhere data utterances of one class (most often Neutral) are much more frequent\nthan those of other classes. Furthermore, only a few data resources are\navailable for many existing spoken languages. To address these problems, we\nexploit a GAN-based augmentation model guided by a triplet network, to improve\nSER performance given imbalanced and insufficient training data. We conduct\nexperiments and demonstrate: 1) With a highly imbalanced dataset, our\naugmentation strategy significantly improves the SER performance (+8% recall\nscore compared with the baseline). 2) Moreover, in a cross-lingual benchmark,\nwhere we train a model with enough source language utterances but very few\ntarget language utterances (around 50 in our experiments), our augmentation\nstrategy brings benefits for the SER performance of all three target languages.", "published": "2022-08-09 18:39:42", "link": "http://arxiv.org/abs/2208.04994v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker-adaptive Lip Reading with User-dependent Padding", "abstract": "Lip reading aims to predict speech based on lip movements alone. As it\nfocuses on visual information to model the speech, its performance is\ninherently sensitive to personal lip appearances and movements. This makes the\nlip reading models show degraded performance when they are applied to unseen\nspeakers due to the mismatch between training and testing conditions. Speaker\nadaptation technique aims to reduce this mismatch between train and test\nspeakers, thus guiding a trained model to focus on modeling the speech content\nwithout being intervened by the speaker variations. In contrast to the efforts\nmade in audio-based speech recognition for decades, the speaker adaptation\nmethods have not well been studied in lip reading. In this paper, to remedy the\nperformance degradation of lip reading model on unseen speakers, we propose a\nspeaker-adaptive lip reading method, namely user-dependent padding. The\nuser-dependent padding is a speaker-specific input that can participate in the\nvisual feature extraction stage of a pre-trained lip reading model. Therefore,\nthe lip appearances and movements information of different speakers can be\nconsidered during the visual feature encoding, adaptively for individual\nspeakers. Moreover, the proposed method does not need 1) any additional layers,\n2) to modify the learned weights of the pre-trained model, and 3) the speaker\nlabel of train data used during pre-train. It can directly adapt to unseen\nspeakers by learning the user-dependent padding only, in a supervised or\nunsupervised manner. Finally, to alleviate the speaker information\ninsufficiency in public lip reading databases, we label the speaker of a\nwell-known audio-visual database, LRW, and design an unseen-speaker lip reading\nscenario named LRW-ID.", "published": "2022-08-09 01:59:30", "link": "http://arxiv.org/abs/2208.04498v1", "categories": ["cs.CV", "cs.AI", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Extending GCC-PHAT using Shift Equivariant Neural Networks", "abstract": "Speaker localization using microphone arrays depends on accurate time delay\nestimation techniques. For decades, methods based on the generalized cross\ncorrelation with phase transform (GCC-PHAT) have been widely adopted for this\npurpose. Recently, the GCC-PHAT has also been used to provide input features to\nneural networks in order to remove the effects of noise and reverberation, but\nat the cost of losing theoretical guarantees in noise-free conditions. We\npropose a novel approach to extending the GCC-PHAT, where the received signals\nare filtered using a shift equivariant neural network that preserves the timing\ninformation contained in the signals. By extensive experiments we show that our\nmodel consistently reduces the error of the GCC-PHAT in adverse environments,\nwith guarantees of exact time delay recovery in ideal conditions.", "published": "2022-08-09 10:31:10", "link": "http://arxiv.org/abs/2208.04654v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pure Data and INScore: Animated notation for new music", "abstract": "New music is made with computers, taking advantage of its graphics displays\nrather than its audio algorithms. Pure Data can be used to compose them. This\nessay will show a case study that uses Pure Data, in connection with INScore,\nfor making a new type of score that uses animated notation or dynamic\nmusicography for making music with performers. This sample was made by the\nauthor of the text, and it will show a number of notation possibilities that\ncan be done using the combination of software. This will be accompanied by a\nsimple prediction of what a musician could perform with it.", "published": "2022-08-09 16:11:45", "link": "http://arxiv.org/abs/2208.04877v1", "categories": ["cs.SD", "cs.GR", "eess.AS", "I.3"], "primary_category": "cs.SD"}
{"title": "Mathematical Foundations of Complex Tonality", "abstract": "Equal temperament, in which semitones are tuned in the irrational ratio of\n$2^{1/12} : 1$, is best seen as a serviceable compromise, sacrificing purity\nfor flexibility. Just intonation, in which intervals are given by products of\npowers of $2$, $3$, and $5$, is more natural, but of limited flexibility. We\npropose a new scheme in which ratios of Gaussian integers form the basis of an\nabstract tonal system. The tritone, so problematic in just temperament, given\nambiguously by the ratios $\\tfrac{45}{32}$, $\\tfrac{64}{45}$, $\\tfrac{36}{25}$,\n$\\tfrac{25}{18}$, none satisfactory, is in our scheme represented by the\ncomplex ratio $1 + \\rm{i} : 1$. The major and minor whole tones, given by\nintervals of $\\tfrac{9}{8}$ and $\\tfrac{10}{9}$, can each be factorized into\nproducts of complex semitones, giving us a major complex semitone\n$\\tfrac{3}{4}(1 + \\rm{i})$ and a minor complex semitone $\\tfrac{1}{3}(3 +\n\\rm{i})$. The perfect third, given by the interval $\\tfrac{5}{4}$, factorizes\ninto the product of a complex whole tone $\\tfrac{1}{2}(1 + 2\\rm{i})$ and its\ncomplex conjugate. Augmented with these supplementary tones, the resulting\nscheme of complex intervals based on products of low powers of Gaussian primes\nleads to the construction of a complete system of major and minor scales in all\nkeys.", "published": "2022-08-09 18:01:50", "link": "http://arxiv.org/abs/2208.04974v4", "categories": ["cs.SD", "eess.AS", "math.NT"], "primary_category": "cs.SD"}
{"title": "Subjective Evaluation of Deep Neural Network Based Speech Enhancement\n  Systems in Real-World Conditions", "abstract": "Subjective evaluation results for two low-latency deep neural networks (DNN)\nare compared to a matured version of a traditional Wiener-filter based noise\nsuppressor. The target use-case is real-world single-channel speech enhancement\napplications, e.g., communications. Real-world recordings consisting of\nadditive stationary and non-stationary noise types are included. The evaluation\nis divided into four outcomes: speech quality, noise transparency, speech\nintelligibility or listening effort, and noise level w.r.t. speech. It is shown\nthat DNNs improve noise suppression in all conditions in comparison to the\ntraditional Wiener-filter baseline without major degradation in speech quality\nand noise transparency while maintaining speech intelligibility better than the\nbaseline.", "published": "2022-08-09 22:12:51", "link": "http://arxiv.org/abs/2208.05057v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Few-shot Bioacoustic Event Detection with Machine Learning Methods", "abstract": "Few-shot learning is a type of classification through which predictions are\nmade based on a limited number of samples for each class. This type of\nclassification is sometimes referred to as a meta-learning problem, in which\nthe model learns how to learn to identify rare cases. We seek to extract\ninformation from five exemplar vocalisations of mammals or birds and detect and\nclassify these sounds in field recordings [2]. This task was provided in the\nDetection and Classification of Acoustic Scenes and Events (DCASE) Challenge of\n2021. Rather than utilize deep learning, as is most commonly done, we\nformulated a novel solution using only machine learning methods. Various models\nwere tested, and it was found that logistic regression outperformed both linear\nregression and template matching. However, all of these methods over-predicted\nthe number of events in the field recordings.", "published": "2022-08-09 23:40:18", "link": "http://arxiv.org/abs/2211.00569v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
