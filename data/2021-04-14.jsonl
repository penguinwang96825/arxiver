{"title": "Zero-Resource Multi-Dialectal Arabic Natural Language Understanding", "abstract": "A reasonable amount of annotated data is required for fine-tuning pre-trained\nlanguage models (PLM) on downstream tasks. However, obtaining labeled examples\nfor different language varieties can be costly. In this paper, we investigate\nthe zero-shot performance on Dialectal Arabic (DA) when fine-tuning a PLM on\nmodern standard Arabic (MSA) data only -- identifying a significant performance\ndrop when evaluating such models on DA. To remedy such performance drop, we\npropose self-training with unlabeled DA data and apply it in the context of\nnamed entity recognition (NER), part-of-speech (POS) tagging, and sarcasm\ndetection (SRD) on several DA varieties. Our results demonstrate the\neffectiveness of self-training with unlabeled DA data: improving zero-shot\nMSA-to-DA transfer by as large as $\\sim$10\\% F$_1$ (NER), 2\\% accuracy (POS\ntagging), and 4.5\\% F$_1$ (SRD). We conduct an ablation experiment and show\nthat the performance boost observed directly results from the unlabeled DA\nexamples used for self-training. Our work opens up opportunities for leveraging\nthe relatively abundant labeled MSA datasets to develop DA models for zero and\nlow-resource dialects. We also report new state-of-the-art performance on all\nthree tasks and open-source our fine-tuned models for the research community.", "published": "2021-04-14 02:29:27", "link": "http://arxiv.org/abs/2104.06591v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AR-LSAT: Investigating Analytical Reasoning of Text", "abstract": "Analytical reasoning is an essential and challenging task that requires a\nsystem to analyze a scenario involving a set of particular circumstances and\nperform reasoning over it to make conclusions. In this paper, we study the\nchallenge of analytical reasoning of text and introduce a new dataset\nconsisting of questions from the Law School Admission Test from 1991 to 2016.\nWe analyze what knowledge understanding and reasoning abilities are required to\ndo well on this task. Furthermore, to address this reasoning challenge, we\ndesign two different baselines: (1) a Transformer-based method which leverages\nthe state-of-the-art pre-trained language models and (2) Analytical Reasoning\nMachine (ARM), a logical-level reasoning framework extracting symbolic\nknowledge (e.g, participants, facts, logical functions) to deduce legitimate\nsolutions. In our experiments, we find that the Transformer-based models\nstruggle to solve this task as their performance is close to random guess and\nARM achieves better performance by leveraging symbolic knowledge and\ninterpretable reasoning steps. Results show that both methods still lag far\nbehind human performance, which leave further space for future research.", "published": "2021-04-14 02:53:32", "link": "http://arxiv.org/abs/2104.06598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Learning Truth-Conditional Denotations and Groundings using\n  Parallel Attention", "abstract": "We present a model that jointly learns the denotations of words together with\ntheir groundings using a truth-conditional semantics. Our model builds on the\nneurosymbolic approach of Mao et al. (2019), learning to ground objects in the\nCLEVR dataset (Johnson et al., 2017) using a novel parallel attention\nmechanism. The model achieves state of the art performance on visual question\nanswering, learning to detect and ground objects with question performance as\nthe only training signal. We also show that the model is able to learn flexible\nnon-canonical groundings just by adjusting answers to questions in the training\nset.", "published": "2021-04-14 06:33:27", "link": "http://arxiv.org/abs/2104.06645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-Scale Self- and Semi-Supervised Learning for Speech Translation", "abstract": "In this paper, we improve speech translation (ST) through effectively\nleveraging large quantities of unlabeled speech and text data in different and\ncomplementary ways. We explore both pretraining and self-training by using the\nlarge Libri-Light speech audio corpus and language modeling with CommonCrawl.\nOur experiments improve over the previous state of the art by 2.6 BLEU on\naverage on all four considered CoVoST 2 language pairs via a simple recipe of\ncombining wav2vec 2.0 pretraining, a single iteration of self-training and\ndecoding with a language model. Different to existing work, our approach does\nnot leverage any other supervision than ST data. Code and models will be\npublicly released.", "published": "2021-04-14 07:44:52", "link": "http://arxiv.org/abs/2104.06678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ask what's missing and what's useful: Improving Clarification Question\n  Generation using Global Knowledge", "abstract": "The ability to generate clarification questions i.e., questions that identify\nuseful missing information in a given context, is important in reducing\nambiguity. Humans use previous experience with similar contexts to form a\nglobal view and compare it to the given context to ascertain what is missing\nand what is useful in the context. Inspired by this, we propose a model for\nclarification question generation where we first identify what is missing by\ntaking a difference between the global and the local view and then train a\nmodel to identify what is useful and generate a question about it. Our model\noutperforms several baselines as judged by both automatic metrics and humans.", "published": "2021-04-14 12:59:08", "link": "http://arxiv.org/abs/2104.06828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Unsupervised Entity and Event Salience Estimation", "abstract": "Salience Estimation aims to predict term importance in documents. Due to few\nexisting human-annotated datasets and the subjective notion of salience,\nprevious studies typically generate pseudo-ground truth for evaluation.\nHowever, our investigation reveals that the evaluation protocol proposed by\nprior work is difficult to replicate, thus leading to few follow-up studies\nexisting. Moreover, the evaluation process is problematic: the entity linking\ntool used for entity matching is very noisy, while the ignorance of event\nargument for event evaluation leads to boosted performance. In this work, we\npropose a light yet practical entity and event salience estimation evaluation\nprotocol, which incorporates the more reliable syntactic dependency parser.\nFurthermore, we conduct a comprehensive analysis among popular entity and event\ndefinition standards, and present our own definition for the Salience\nEstimation task to reduce noise during the pseudo-ground truth generation\nprocess. Furthermore, we construct dependency-based heterogeneous graphs to\ncapture the interactions of entities and events. The empirical results show\nthat both baseline methods and the novel GNN method utilizing the heterogeneous\ngraph consistently outperform the previous SOTA model in all proposed metrics.", "published": "2021-04-14 15:23:08", "link": "http://arxiv.org/abs/2104.06924v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation and Multi-Domain Adaptation for Neural Machine\n  Translation: A Survey", "abstract": "The development of deep learning techniques has allowed Neural Machine\nTranslation (NMT) models to become extremely powerful, given sufficient\ntraining data and training time. However, systems struggle when translating\ntext from a new domain with a distinct style or vocabulary. Fine-tuning on\nin-domain data allows good domain adaptation, but requires sufficient relevant\nbilingual data. Even if this is available, simple fine-tuning can cause\noverfitting to new data and `catastrophic forgetting' of previously learned\nbehaviour.\n  We concentrate on robust approaches to domain adaptation for NMT,\nparticularly where a system may need to translate across multiple domains. We\ndivide techniques into those revolving around data selection or generation,\nmodel architecture, parameter adaptation procedure, and inference procedure. We\nfinally highlight the benefits of domain adaptation and multi-domain adaptation\ntechniques to other lines of NMT research.", "published": "2021-04-14 16:21:37", "link": "http://arxiv.org/abs/2104.06951v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "K-PLUG: Knowledge-injected Pre-trained Language Model for Natural\n  Language Understanding and Generation in E-Commerce", "abstract": "Existing pre-trained language models (PLMs) have demonstrated the\neffectiveness of self-supervised learning for a broad range of natural language\nprocessing (NLP) tasks. However, most of them are not explicitly aware of\ndomain-specific knowledge, which is essential for downstream tasks in many\ndomains, such as tasks in e-commerce scenarios. In this paper, we propose\nK-PLUG, a knowledge-injected pre-trained language model based on the\nencoder-decoder transformer that can be transferred to both natural language\nunderstanding and generation tasks. We verify our method in a diverse range of\ne-commerce scenarios that require domain-specific knowledge. Specifically, we\npropose five knowledge-aware self-supervised pre-training objectives to\nformulate the learning of domain-specific knowledge, including e-commerce\ndomain-specific knowledge-bases, aspects of product entities, categories of\nproduct entities, and unique selling propositions of product entities. K-PLUG\nachieves new state-of-the-art results on a suite of domain-specific NLP tasks,\nincluding product knowledge base completion, abstractive product summarization,\nand multi-turn dialogue, significantly outperforms baselines across the board,\nwhich demonstrates that the proposed method effectively learns a diverse set of\ndomain-specific knowledge for both language understanding and generation tasks.", "published": "2021-04-14 16:37:31", "link": "http://arxiv.org/abs/2104.06960v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for\n  Unsupervised Sentence Embedding Learning", "abstract": "Learning sentence embeddings often requires a large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong domain adaptation and\npre-training method for sentence embeddings, significantly outperforming other\napproaches like Masked Language Model.\n  A crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.", "published": "2021-04-14 17:02:18", "link": "http://arxiv.org/abs/2104.06979v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UPB at SemEval-2021 Task 1: Combining Deep Learning and Hand-Crafted\n  Features for Lexical Complexity Prediction", "abstract": "Reading is a complex process which requires proper understanding of texts in\norder to create coherent mental representations. However, comprehension\nproblems may arise due to hard-to-understand sections, which can prove\ntroublesome for readers, while accounting for their specific language skills.\nAs such, steps towards simplifying these sections can be performed, by\naccurately identifying and evaluating difficult structures. In this paper, we\ndescribe our approach for the SemEval-2021 Task 1: Lexical Complexity\nPrediction competition that consists of a mixture of advanced NLP techniques,\nnamely Transformer-based language models, pre-trained word embeddings, Graph\nConvolutional Networks, Capsule Networks, as well as a series of hand-crafted\ntextual complexity features. Our models are applicable on both subtasks and\nachieve good performance results, with a MAE below 0.07 and a Person\ncorrelation of .73 for single word identification, as well as a MAE below 0.08\nand a Person correlation of .79 for multiple word targets. Our results are just\n5.46% and 6.5% lower than the top scores obtained in the competition on the\nfirst and the second subtasks, respectively.", "published": "2021-04-14 17:05:46", "link": "http://arxiv.org/abs/2104.06983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Update to the Minho Quotation Resource", "abstract": "The Minho Quotation Resource was originally released in 2012. It provided\napproximately 500,000 quotes from business leaders, analysts and politicians\nthat spanned the period from 2008 to 2012. The original resource had several\nfailings which include a large number of missing job titles and affiliations as\nwell as unnormalised job titles which produced a large variation in spellings\nand formats of the same employment position. Also, there were numerous\nduplicate posts. This update has standardised the job title text as well as the\nimputation of missing job titles and affiliations. Duplicate quotes have been\ndeleted. This update also provides some metaphor and simile extraction as well\nas an emotion distribution of the quotes. This update has also replaced an\nantiquated version of Lucene index with a JSONL format as well as a rudimentary\ninterface that can query the data supplied with the resource. It is hoped that\nthis update will encourage the study of business communication in a time of a\nfinancial crisis.", "published": "2021-04-14 17:14:39", "link": "http://arxiv.org/abs/2104.06987v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media", "abstract": "Online social media platforms increasingly rely on Natural Language\nProcessing (NLP) techniques to detect abusive content at scale in order to\nmitigate the harms it causes to their users. However, these techniques suffer\nfrom various sampling and association biases present in training data, often\nresulting in sub-par performance on content relevant to marginalized groups,\npotentially furthering disproportionate harms towards them. Studies on such\nbiases so far have focused on only a handful of axes of disparities and\nsubgroups that have annotations/lexicons available. Consequently, biases\nconcerning non-Western contexts are largely ignored in the literature. In this\npaper, we introduce a weakly supervised method to robustly detect lexical\nbiases in broader geocultural contexts. Through a case study on a publicly\navailable toxicity detection model, we demonstrate that our method identifies\nsalient groups of cross-geographic errors, and, in a follow up, demonstrate\nthat these groupings reflect human judgments of offensive and inoffensive\nlanguage in those geographic contexts. We also conduct analysis of a model\ntrained on a dataset with ground truth labels to better understand these\nbiases, and present preliminary mitigation experiments.", "published": "2021-04-14 17:32:05", "link": "http://arxiv.org/abs/2104.06999v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IGA : An Intent-Guided Authoring Assistant", "abstract": "While large-scale pretrained language models have significantly improved\nwriting assistance functionalities such as autocomplete, more complex and\ncontrollable writing assistants have yet to be explored. We leverage advances\nin language modeling to build an interactive writing assistant that generates\nand rephrases text according to fine-grained author specifications. Users\nprovide input to our Intent-Guided Assistant (IGA) in the form of text\ninterspersed with tags that correspond to specific rhetorical directives (e.g.,\nadding description or contrast, or rephrasing a particular sentence). We\nfine-tune a language model on a dataset heuristically-labeled with author\nintent, which allows IGA to fill in these tags with generated text that users\ncan subsequently edit to their liking. A series of automatic and crowdsourced\nevaluations confirm the quality of IGA's generated outputs, while a small-scale\nuser study demonstrates author preference for IGA over baseline methods in a\ncreative writing task. We release our dataset, code, and demo to spur further\nresearch into AI-assisted writing.", "published": "2021-04-14 17:32:21", "link": "http://arxiv.org/abs/2104.07000v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Discourse Trees from Transformer-based Neural Summarizers", "abstract": "Previous work indicates that discourse information benefits summarization. In\nthis paper, we explore whether this synergy between discourse and summarization\nis bidirectional, by inferring document-level discourse trees from pre-trained\nneural summarizers. In particular, we generate unlabeled RST-style discourse\ntrees from the self-attention matrices of the transformer model. Experiments\nacross models and datasets reveal that the summarizer learns both, dependency-\nand constituency-style discourse information, which is typically encoded in a\nsingle head, covering long- and short-distance discourse dependencies. Overall,\nthe experimental results suggest that the learned discourse information is\ngeneral and transferable inter-domain.", "published": "2021-04-14 18:10:05", "link": "http://arxiv.org/abs/2104.07058v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Everything in Order? A Simple Way to Order Sentences", "abstract": "The task of organizing a shuffled set of sentences into a coherent text has\nbeen used to evaluate a machine's understanding of causal and temporal\nrelations. We formulate the sentence ordering task as a conditional\ntext-to-marker generation problem. We present Reorder-BART (Re-BART) that\nleverages a pre-trained Transformer-based model to identify a coherent order\nfor a given set of shuffled sentences. The model takes a set of shuffled\nsentences with sentence-specific markers as input and generates a sequence of\nposition markers of the sentences in the ordered text. Re-BART achieves the\nstate-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and\nKendall's tau ($\\tau$). We perform evaluations in a zero-shot setting,\nshowcasing that our model is able to generalize well across other datasets. We\nadditionally perform several experiments to understand the functioning and\nlimitations of our framework.", "published": "2021-04-14 18:16:47", "link": "http://arxiv.org/abs/2104.07064v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UDALM: Unsupervised Domain Adaptation through Language Modeling", "abstract": "In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained\nlanguage models for downstream tasks. We introduce UDALM, a fine-tuning\nprocedure, using a mixed classification and Masked Language Model loss, that\ncan adapt to the target domain distribution in a robust and sample efficient\nmanner. Our experiments show that performance of models trained with the mixed\nloss scales with the amount of available target data and the mixed loss can be\neffectively used as a stopping criterion during UDA training. Furthermore, we\ndiscuss the relationship between A-distance and the target error and explore\nsome limitations of the Domain Adversarial Training approach. Our method is\nevaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset,\nyielding $91.74\\%$ accuracy, which is an $1.11\\%$ absolute improvement over the\nstate-of-the-art.", "published": "2021-04-14 19:05:01", "link": "http://arxiv.org/abs/2104.07078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TWEAC: Transformer with Extendable QA Agent Classifiers", "abstract": "Question answering systems should help users to access knowledge on a broad\nrange of topics and to answer a wide array of different questions. Most systems\nfall short of this expectation as they are only specialized in one particular\nsetting, e.g., answering factual questions with Wikipedia data. To overcome\nthis limitation, we propose composing multiple QA agents within a meta-QA\nsystem. We argue that there exist a wide range of specialized QA agents in\nliterature. Thus, we address the central research question of how to\neffectively and efficiently identify suitable QA agents for any given question.\nWe study both supervised and unsupervised approaches to address this challenge,\nshowing that TWEAC -- Transformer with Extendable Agent Classifiers -- achieves\nthe best performance overall with 94% accuracy. We provide extensive insights\non the scalability of TWEAC, demonstrating that it scales robustly to over 100\nQA agents with each providing just 1000 examples of questions they can answer.\nOur code and data is available:\nhttps://github.com/UKPLab/TWEAC-qa-agent-selection", "published": "2021-04-14 19:06:11", "link": "http://arxiv.org/abs/2104.07081v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SummScreen: A Dataset for Abstractive Screenplay Summarization", "abstract": "We introduce SummScreen, a summarization dataset comprised of pairs of TV\nseries transcripts and human written recaps. The dataset provides a challenging\ntestbed for abstractive summarization for several reasons. Plot details are\noften expressed indirectly in character dialogues and may be scattered across\nthe entirety of the transcript. These details must be found and integrated to\nform the succinct plot descriptions in the recaps. Also, TV scripts contain\ncontent that does not directly pertain to the central plot but rather serves to\ndevelop characters or provide comic relief. This information is rarely\ncontained in recaps. Since characters are fundamental to TV series, we also\npropose two entity-centric evaluation metrics. Empirically, we characterize the\ndataset by evaluating several methods, including neural models and those based\non nearest neighbors. An oracle extractive approach outperforms all benchmarked\nmodels according to automatic metrics, showing that the neural models are\nunable to fully exploit the input transcripts. Human evaluation and qualitative\nanalysis reveal that our non-oracle models are competitive with their oracle\ncounterparts in terms of generating faithful plot events and can benefit from\nbetter content selectors. Both oracle and non-oracle models generate unfaithful\nfacts, suggesting future research directions.", "published": "2021-04-14 19:37:40", "link": "http://arxiv.org/abs/2104.07091v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Static Embeddings as Efficient Knowledge Bases?", "abstract": "Recent research investigates factual knowledge stored in large pretrained\nlanguage models (PLMs). Instead of structural knowledge base (KB) queries,\nmasked sentences such as \"Paris is the capital of [MASK]\" are used as probes.\nThe good performance on this analysis task has been interpreted as PLMs\nbecoming potential repositories of factual knowledge. In experiments across ten\nlinguistically diverse languages, we study knowledge contained in static\nembeddings. We show that, when restricting the output space to a candidate set,\nsimple nearest neighbor matching using static embeddings performs better than\nPLMs. E.g., static embeddings perform 1.6% points better than BERT while just\nusing 0.3% of energy for training. One important factor in their good\ncomparative performance is that static embeddings are standardly learned for a\nlarge vocabulary. In contrast, BERT exploits its more sophisticated, but\nexpensive ability to compose meaningful representations from a much smaller\nsubword vocabulary.", "published": "2021-04-14 19:42:20", "link": "http://arxiv.org/abs/2104.07094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes a Scientific Paper be Accepted for Publication?", "abstract": "Despite peer-reviewing being an essential component of academia since the\n1600s, it has repeatedly received criticisms for lack of transparency and\nconsistency. We posit that recent work in machine learning and explainable AI\nprovide tools that enable insights into the decisions from a given peer review\nprocess. We start by extracting global explanations in the form of linguistic\nfeatures that affect the acceptance of a scientific paper for publication on an\nopen peer-review dataset. Second, since such global explanations do not justify\ncausal interpretations, we provide a methodology for detecting confounding\neffects in natural language in order to generate causal explanations, under\nassumptions, in the form of lexicons. Our proposed linguistic explanation\nmethodology indicates the following on a case dataset of ICLR submissions: a)\nthe organising committee follows, for the most part, the recommendations of\nreviewers, and, b) the paper's main characteristics that led to reviewers\nrecommending acceptance for publication are originality, clarity and substance.", "published": "2021-04-14 20:26:51", "link": "http://arxiv.org/abs/2104.07112v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts", "abstract": "Natural-language prompts have recently been used to coax pretrained language\nmodels into performing other AI tasks, using a fill-in-the-blank paradigm\n(Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al.,\n2020). For example, language models retain factual knowledge from their\ntraining corpora that can be extracted by asking them to \"fill in the blank\" in\na sentential prompt. However, where does this prompt come from? We explore the\nidea of learning prompts by gradient descent -- either fine-tuning prompts\ntaken from previous work, or starting from random initialization. Our prompts\nconsist of \"soft words,\" i.e., continuous vectors that are not necessarily word\ntype embeddings from the language model. Furthermore, for each task, we\noptimize a mixture of prompts, learning which prompts are most effective and\nhow to ensemble them. Across multiple English LMs and tasks, our approach\nhugely outperforms previous methods, showing that the implicit factual\nknowledge in language models was previously underestimated. Moreover, this\nknowledge is cheap to elicit: random initialization is nearly as good as\ninformed initialization.", "published": "2021-04-14 02:56:14", "link": "http://arxiv.org/abs/2104.06599v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Masked Language Modeling and the Distributional Hypothesis: Order Word\n  Matters Pre-training for Little", "abstract": "A possible explanation for the impressive performance of masked language\nmodel (MLM) pre-training is that such models have learned to represent the\nsyntactic structures prevalent in classical NLP pipelines. In this paper, we\npropose a different explanation: MLMs succeed on downstream tasks almost\nentirely due to their ability to model higher-order word co-occurrence\nstatistics. To demonstrate this, we pre-train MLMs on sentences with randomly\nshuffled word order, and show that these models still achieve high accuracy\nafter fine-tuning on many downstream tasks -- including on tasks specifically\ndesigned to be challenging for models that ignore word order. Our models\nperform surprisingly well according to some parametric syntactic probes,\nindicating possible deficiencies in how we test representations for syntactic\ninformation. Overall, our results show that purely distributional information\nlargely explains the success of pre-training, and underscore the importance of\ncurating challenging evaluation datasets that require deeper linguistic\nknowledge.", "published": "2021-04-14 06:30:36", "link": "http://arxiv.org/abs/2104.06644v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NAREOR: The Narrative Reordering Problem", "abstract": "Many implicit inferences exist in text depending on how it is structured that\ncan critically impact the text's interpretation and meaning. One such\nstructural aspect present in text with chronology is the order of its\npresentation. For narratives or stories, this is known as the narrative order.\nReordering a narrative can impact the temporal, causal, event-based, and other\ninferences readers draw from it, which in turn can have strong effects both on\nits interpretation and interestingness. In this paper, we propose and\ninvestigate the task of Narrative Reordering (NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot. We\npresent a dataset, NAREORC, with human rewritings of stories within ROCStories\nin non-linear orders, and conduct a detailed analysis of it. Further, we\npropose novel task-specific training methods with suitable evaluation metrics.\nWe perform experiments on NAREORC using state-of-the-art models such as BART\nand T5 and conduct extensive automatic and human evaluations. We demonstrate\nthat although our models can perform decently, NAREOR is a challenging task\nwith potential for further exploration. We also investigate two applications of\nNAREOR: generation of more interesting variations of stories and serving as\nadversarial sets for temporal/event-related tasks, besides discussing other\nprospective ones, such as for pedagogical setups related to language skills\nlike essay writing and applications to medicine involving clinical narratives.", "published": "2021-04-14 07:33:02", "link": "http://arxiv.org/abs/2104.06669v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards BERT-based Automatic ICD Coding: Limitations and Opportunities", "abstract": "Automatic ICD coding is the task of assigning codes from the International\nClassification of Diseases (ICD) to medical notes. These codes describe the\nstate of the patient and have multiple applications, e.g., computer-assisted\ndiagnosis or epidemiological studies. ICD coding is a challenging task due to\nthe complexity and length of medical notes. Unlike the general trend in\nlanguage processing, no transformer model has been reported to reach high\nperformance on this task. Here, we investigate in detail ICD coding using\nPubMedBERT, a state-of-the-art transformer model for biomedical language\nunderstanding. We find that the difficulty of fine-tuning the model on long\npieces of text is the main limitation for BERT-based models on ICD coding. We\nrun extensive experiments and show that despite the gap with current\nstate-of-the-art, pretrained transformers can reach competitive performance\nusing relatively small portions of text. We point at better methods to\naggregate information from long texts as the main need for improving BERT-based\nICD coding.", "published": "2021-04-14 09:12:53", "link": "http://arxiv.org/abs/2104.06709v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentence Embeddings by Ensemble Distillation", "abstract": "This paper contributes a new State Of The Art (SOTA) for Semantic Textual\nSimilarity (STS). We compare and combine a number of recently proposed sentence\nembedding methods for STS, and propose a novel and simple ensemble knowledge\ndistillation scheme that improves on previous approaches. Our experiments\ndemonstrate that a model trained to learn the average embedding space from\nmultiple ensemble students outperforms all the other individual models with\nhigh robustness. Utilizing our distillation method in combination with previous\nmethods, we significantly improve on the SOTA unsupervised STS, and by proper\nhyperparameter tuning of previous methods we improve the supervised SOTA\nscores.", "published": "2021-04-14 09:23:27", "link": "http://arxiv.org/abs/2104.06719v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WARM: A Weakly (+Semi) Supervised Model for Solving Math word Problems", "abstract": "Solving math word problems (MWPs) is an important and challenging problem in\nnatural language processing. Existing approaches to solve MWPs require full\nsupervision in the form of intermediate equations. However, labeling every MWP\nwith its corresponding equations is a time-consuming and expensive task. In\norder to address this challenge of equation annotation, we propose a weakly\nsupervised model for solving MWPs by requiring only the final answer as\nsupervision. We approach this problem by first learning to generate the\nequation using the problem description and the final answer, which we\nsubsequently use to train a supervised MWP solver. We propose and compare\nvarious weakly supervised techniques to learn to generate equations directly\nfrom the problem description and answer. Through extensive experiments, we\ndemonstrate that without using equations for supervision, our approach achieves\naccuracy gains of 4.5% and 32% over the state-of-the-art weakly supervised\napproach, on the standard Math23K and AllArith datasets respectively.\nAdditionally, we curate and release new datasets of roughly 10k MWPs each in\nEnglish and in Hindi (a low resource language).These datasets are suitable for\ntraining weakly supervised models. We also present an extension of WARMM to\nsemi-supervised learning and present further improvements on results, along\nwith insights.", "published": "2021-04-14 09:25:38", "link": "http://arxiv.org/abs/2104.06722v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge-driven Answer Generation for Conversational Search", "abstract": "The conversational search paradigm introduces a step change over the\ntraditional search paradigm by allowing users to interact with search agents in\na multi-turn and natural fashion. The conversation flows naturally and is\nusually centered around a target field of knowledge. In this work, we propose a\nknowledge-driven answer generation approach for open-domain conversational\nsearch, where a conversation-wide entities' knowledge graph is used to bias\nsearch-answer generation. First, a conversation-specific knowledge graph is\nextracted from the top passages retrieved with a Transformer-based re-ranker.\nThe entities knowledge-graph is then used to bias a search-answer generator\nTransformer towards information rich and concise answers. This conversation\nspecific bias is computed by identifying the most relevant passages according\nto the most salient entities of that particular conversation. Experiments show\nthat the proposed approach successfully exploits entities knowledge along the\nconversation, and outperforms a set of baselines on the search-answer\ngeneration task.", "published": "2021-04-14 14:35:54", "link": "http://arxiv.org/abs/2104.06892v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Surprising Performance of Simple Baselines for Misinformation\n  Detection", "abstract": "As social media becomes increasingly prominent in our day to day lives, it is\nincreasingly important to detect informative content and prevent the spread of\ndisinformation and unverified rumours. While many sophisticated and successful\nmodels have been proposed in the literature, they are often compared with older\nNLP baselines such as SVMs, CNNs, and LSTMs. In this paper, we examine the\nperformance of a broad set of modern transformer-based language models and show\nthat with basic fine-tuning, these models are competitive with and can even\nsignificantly outperform recently proposed state-of-the-art methods. We present\nour framework as a baseline for creating and evaluating new methods for\nmisinformation detection. We further study a comprehensive set of benchmark\ndatasets, and discuss potential data leakage and the need for careful design of\nthe experiments and understanding of datasets to account for confounding\nvariables. As an extreme case example, we show that classifying only based on\nthe first three digits of tweet ids, which contain information on the date,\ngives state-of-the-art performance on a commonly used benchmark dataset for\nfake news detection --Twitter16. We provide a simple tool to detect this\nproblem and suggest steps to mitigate it in future datasets.", "published": "2021-04-14 16:25:22", "link": "http://arxiv.org/abs/2104.06952v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic\n  Aware Sampling", "abstract": "A vital step towards the widespread adoption of neural retrieval models is\ntheir resource efficiency throughout the training, indexing and query\nworkflows. The neural IR community made great advancements in training\neffective dual-encoder dense retrieval (DR) models recently. A dense text\nretrieval model uses a single vector representation per query and passage to\nscore a match, which enables low-latency first stage retrieval with a nearest\nneighbor search. Increasingly common, training approaches require enormous\ncompute power, as they either conduct negative passage sampling out of a\ncontinuously updating refreshing index or require very large batch sizes for\nin-batch negative sampling. Instead of relying on more compute capability, we\nintroduce an efficient topic-aware query and balanced margin sampling\ntechnique, called TAS-Balanced. We cluster queries once before training and\nsample queries out of a cluster per batch. We train our lightweight 6-layer DR\nmodel with a novel dual-teacher supervision that combines pairwise and in-batch\nnegative teachers. Our method is trainable on a single consumer-grade GPU in\nunder 48 hours (as opposed to a common configuration of 8x V100s). We show that\nour TAS-Balanced training method achieves state-of-the-art low-latency (64ms\nper query) results on two TREC Deep Learning Track query sets. Evaluated on\nNDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by\n11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces\nthe first dense retriever that outperforms every other method on recall at any\ncutoff on TREC-DL and allows more resource intensive re-ranking models to\noperate on fewer passages to improve results further.", "published": "2021-04-14 16:49:18", "link": "http://arxiv.org/abs/2104.06967v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Event Detection as Question Answering with Entity Information", "abstract": "In this paper, we propose a recent and under-researched paradigm for the task\nof event detection (ED) by casting it as a question-answering (QA) problem with\nthe possibility of multiple answers and the support of entities. The extraction\nof event triggers is, thus, transformed into the task of identifying answer\nspans from a context, while also focusing on the surrounding entities. The\narchitecture is based on a pre-trained and fine-tuned language model, where the\ninput context is augmented with entities marked at different levels, their\npositions, their types, and, finally, the argument roles. Experiments on the\nACE~2005 corpus demonstrate that the proposed paradigm is a viable solution for\nthe ED task and it significantly outperforms the state-of-the-art models.\nMoreover, we prove that our methods are also able to extract unseen event\ntypes.", "published": "2021-04-14 16:53:11", "link": "http://arxiv.org/abs/2104.06969v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "[RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias\n  Mitigation", "abstract": "Despite widespread use in natural language processing (NLP) tasks, word\nembeddings have been criticized for inheriting unintended gender bias from\ntraining corpora. programmer is more closely associated with man and homemaker\nis more closely associated with woman. Such gender bias has also been shown to\npropagate in downstream tasks.", "published": "2021-04-14 16:56:14", "link": "http://arxiv.org/abs/2104.06973v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sparse Attention with Linear Units", "abstract": "Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.", "published": "2021-04-14 17:52:38", "link": "http://arxiv.org/abs/2104.07012v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment,\n  Emotion, Physiological-Emotion, and Stress", "abstract": "Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the\ntasks of sentiment and emotion, as well as physiological-emotion and\nemotion-based stress recognition through more comprehensively integrating the\naudio-visual, language, and biological signal modalities. The purpose of MuSe\n2021 is to bring together communities from different disciplines; mainly, the\naudio-visual emotion recognition community (signal-based), the sentiment\nanalysis community (symbol-based), and the health informatics community. We\npresent four distinct sub-challenges: MuSe-Wilder and MuSe-Stress which focus\non continuous emotion (valence and arousal) prediction; MuSe-Sent, in which\nparticipants recognise five classes each for valence and arousal; and\nMuSe-Physio, in which the novel aspect of `physiological-emotion' is to be\npredicted. For this years' challenge, we utilise the MuSe-CaR dataset focusing\non user-generated reviews and introduce the Ulm-TSST dataset, which displays\npeople in stressful depositions. This paper also provides detail on the\nstate-of-the-art feature sets extracted from these datasets for utilisation by\nour baseline model, a Long Short-Term Memory-Recurrent Neural Network. For each\nsub-challenge, a competitive baseline for participants is set; namely, on test,\nwe report a Concordance Correlation Coefficient (CCC) of .4616 CCC for\nMuSe-Wilder; .4717 CCC for MuSe-Stress, and .4606 CCC for MuSe-Physio. For\nMuSe-Sent an F1 score of 32.82 % is obtained.", "published": "2021-04-14 20:56:04", "link": "http://arxiv.org/abs/2104.07123v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Interpretability Illusion for BERT", "abstract": "We describe an \"interpretability illusion\" that arises when analyzing the\nBERT model. Activations of individual neurons in the network may spuriously\nappear to encode a single, simple concept, when in fact they are encoding\nsomething far more complex. The same effect holds for linear combinations of\nactivations. We trace the source of this illusion to geometric properties of\nBERT's embedding space as well as the fact that common text corpora represent\nonly narrow slices of possible English sentences. We provide a taxonomy of\nmodel-learned concepts and discuss methodological implications for\ninterpretability research, especially the importance of testing hypotheses on\nmultiple data sets.", "published": "2021-04-14 22:04:48", "link": "http://arxiv.org/abs/2104.07143v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Intent Classification and Slot Labeling in\n  Goal-oriented Dialog Systems to Real-world Noise", "abstract": "Intent Classification (IC) and Slot Labeling (SL) models, which form the\nbasis of dialogue systems, often encounter noisy data in real-word\nenvironments. In this work, we investigate how robust IC/SL models are to noisy\ndata. We collect and publicly release a test-suite for seven common noise types\nfound in production human-to-bot conversations (abbreviations, casing,\nmisspellings, morphological variants, paraphrases, punctuation and synonyms).\nOn this test-suite, we show that common noise types substantially degrade the\nIC accuracy and SL F1 performance of state-of-the-art BERT-based IC/SL models.\nBy leveraging cross-noise robustness transfer -- training on one noise type to\nimprove robustness on another noise type -- we design aggregate\ndata-augmentation approaches that increase the model performance across all\nseven noise types by +10.8% for IC accuracy and +15 points for SL F1 on\naverage. To the best of our knowledge, this is the first work to present a\nsingle IC/SL model that is robust to a wide range of noise phenomena.", "published": "2021-04-14 22:14:41", "link": "http://arxiv.org/abs/2104.07149v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Disentangling Representations of Text by Masking Transformers", "abstract": "Representations from large pretrained models such as BERT encode a range of\nfeatures into monolithic vectors, affording strong predictive accuracy across a\nmultitude of downstream tasks. In this paper we explore whether it is possible\nto learn disentangled representations by identifying existing subnetworks\nwithin pretrained models that encode distinct, complementary aspect\nrepresentations. Concretely, we learn binary masks over transformer weights or\nhidden units to uncover subsets of features that correlate with a specific\nfactor of variation; this eliminates the need to train a disentangled model\nfrom scratch for a particular task. We evaluate this method with respect to its\nability to disentangle representations of sentiment from genre in movie\nreviews, \"toxicity\" from dialect in Tweets, and syntax from semantics.\n  By combining masking with magnitude pruning we find that we can identify\nsparse subnetworks within BERT that strongly encode particular aspects (e.g.,\ntoxicity) while only weakly encoding others (e.g., race). Moreover, despite\nonly learning masks, we find that disentanglement-via-masking performs as well\nas -- and often better than -- previously proposed methods based on variational\nautoencoders and adversarial training.", "published": "2021-04-14 22:45:34", "link": "http://arxiv.org/abs/2104.07155v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Annealing Knowledge Distillation", "abstract": "Significant memory and computational requirements of large deep neural\nnetworks restrict their application on edge devices. Knowledge distillation\n(KD) is a prominent model compression technique for deep neural networks in\nwhich the knowledge of a trained large teacher model is transferred to a\nsmaller student model. The success of knowledge distillation is mainly\nattributed to its training objective function, which exploits the soft-target\ninformation (also known as \"dark knowledge\") besides the given regular hard\nlabels in a training set. However, it is shown in the literature that the\nlarger the gap between the teacher and the student networks, the more difficult\nis their training using knowledge distillation. To address this shortcoming, we\npropose an improved knowledge distillation method (called Annealing-KD) by\nfeeding the rich information provided by the teacher's soft-targets\nincrementally and more efficiently. Our Annealing-KD technique is based on a\ngradual transition over annealed soft-targets generated by the teacher at\ndifferent temperatures in an iterative process, and therefore, the student is\ntrained to follow the annealed teacher output in a step-by-step manner. This\npaper includes theoretical and empirical evidence as well as practical\nexperiments to support the effectiveness of our Annealing-KD method. We did a\ncomprehensive set of experiments on different tasks such as image\nclassification (CIFAR-10 and 100) and NLP language inference with BERT-based\nmodels on the GLUE benchmark and consistently got superior results.", "published": "2021-04-14 23:45:03", "link": "http://arxiv.org/abs/2104.07163v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring diachronic sense change: new models and Monte Carlo methods\n  for Bayesian inference", "abstract": "In a bag-of-words model, the senses of a word with multiple meanings, e.g.\n\"bank\" (used either in a river-bank or an institution sense), are represented\nas probability distributions over context words, and sense prevalence is\nrepresented as a probability distribution over senses. Both of these may change\nwith time. Modelling and measuring this kind of sense change is challenging due\nto the typically high-dimensional parameter space and sparse datasets. A\nrecently published corpus of ancient Greek texts contains expert-annotated\nsense labels for selected target words. Automatic sense-annotation for the word\n\"kosmos\" (meaning decoration, order or world) has been used as a test case in\nrecent work with related generative models and Monte Carlo methods. We adapt an\nexisting generative sense change model to develop a simpler model for the main\neffects of sense and time, and give MCMC methods for Bayesian inference on all\nthese models that are more efficient than existing methods. We carry out\nautomatic sense-annotation of snippets containing \"kosmos\" using our model, and\nmeasure the time-evolution of its three senses and their prevalence. As far as\nwe are aware, ours is the first analysis of this data, within the class of\ngenerative models we consider, that quantifies uncertainty and returns credible\nsets for evolving sense prevalence in good agreement with those given by expert\nannotation.", "published": "2021-04-14 11:40:21", "link": "http://arxiv.org/abs/2105.00819v2", "categories": ["cs.CL", "stat.ME"], "primary_category": "cs.CL"}
{"title": "The Curious Case of Hallucinations in Neural Machine Translation", "abstract": "In this work, we study hallucinations in Neural Machine Translation (NMT),\nwhich lie at an extreme end on the spectrum of NMT pathologies. Firstly, we\nconnect the phenomenon of hallucinations under source perturbation to the\nLong-Tail theory of Feldman (2020), and present an empirically validated\nhypothesis that explains hallucinations under source perturbation. Secondly, we\nconsider hallucinations under corpus-level noise (without any source\nperturbation) and demonstrate that two prominent types of natural\nhallucinations (detached and oscillatory outputs) could be generated and\nexplained through specific corpus-level noise patterns. Finally, we elucidate\nthe phenomenon of hallucination amplification in popular data-generation\nprocesses such as Backtranslation and sequence-level Knowledge Distillation.", "published": "2021-04-14 08:09:57", "link": "http://arxiv.org/abs/2104.06683v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive sequence-to-sequence voice conversion", "abstract": "This paper proposes a novel voice conversion (VC) method based on\nnon-autoregressive sequence-to-sequence (NAR-S2S) models. Inspired by the great\nsuccess of NAR-S2S models such as FastSpeech in text-to-speech (TTS), we extend\nthe FastSpeech2 model for the VC problem. We introduce the\nconvolution-augmented Transformer (Conformer) instead of the Transformer,\nmaking it possible to capture both local and global context information from\nthe input sequence. Furthermore, we extend variance predictors to variance\nconverters to explicitly convert the source speaker's prosody components such\nas pitch and energy into the target speaker. The experimental evaluation with\nthe Japanese speaker dataset, which consists of male and female speakers of\n1,000 utterances, demonstrates that the proposed model enables us to perform\nmore stable, faster, and better conversion than autoregressive S2S (AR-S2S)\nmodels such as Tacotron2 and Transformer.", "published": "2021-04-14 11:53:51", "link": "http://arxiv.org/abs/2104.06793v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Word-Level Semantic Representation via Dependency Structure\n  for Expressive Text-to-Speech Synthesis", "abstract": "Exploiting rich linguistic information in raw text is crucial for expressive\ntext-to-speech (TTS). As large scale pre-trained text representation develops,\nbidirectional encoder representations from Transformers (BERT) has been proven\nto embody semantic information and employed to TTS recently. However, original\nor simply fine-tuned BERT embeddings still cannot provide sufficient semantic\nknowledge that expressive TTS models should take into account. In this paper,\nwe propose a word-level semantic representation enhancing method based on\ndependency structure and pre-trained BERT embedding. The BERT embedding of each\nword is reprocessed considering its specific dependencies and related words in\nthe sentence, to generate more effective semantic representation for TTS. To\nbetter utilize the dependency structure, relational gated graph network (RGGN)\nis introduced to make semantic information flow and aggregate through the\ndependency structure. The experimental results show that the proposed method\ncan further improve the naturalness and expressiveness of synthesized speeches\non both Mandarin and English datasets.", "published": "2021-04-14 13:09:51", "link": "http://arxiv.org/abs/2104.06835v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "I Wish I Would Have Loved This One, But I Didn't -- A Multilingual\n  Dataset for Counterfactual Detection in Product Reviews", "abstract": "Counterfactual statements describe events that did not or cannot take place.\nWe consider the problem of counterfactual detection (CFD) in product reviews.\nFor this purpose, we annotate a multilingual CFD dataset from Amazon product\nreviews covering counterfactual statements written in English, German, and\nJapanese languages. The dataset is unique as it contains counterfactuals in\nmultiple languages, covers a new application area of e-commerce reviews, and\nprovides high quality professional annotations. We train CFD models using\ndifferent text representation methods and classifiers. We find that these\nmodels are robust against the selectional biases introduced due to cue\nphrase-based sentence selection. Moreover, our CFD dataset is compatible with\nprior datasets and can be merged to learn accurate CFD models. Applying machine\ntranslation on English counterfactual examples to create multilingual data\nperforms poorly, demonstrating the language-specificity of this problem, which\nhas been ignored so far.", "published": "2021-04-14 14:38:36", "link": "http://arxiv.org/abs/2104.06893v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Interpretable Clauses Semantically using Pretrained Word\n  Representation", "abstract": "Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based\non propositional logic, which has demonstrated competitive performance in many\nNatural Language Processing (NLP) tasks, including sentiment analysis, text\nclassification, and Word Sense Disambiguation. To obtain human-level\ninterpretability, legacy TM employs Boolean input features such as bag-of-words\n(BOW). However, the BOW representation makes it difficult to use any\npre-trained information, for instance, word2vec and GloVe word representations.\nThis restriction has constrained the performance of TM compared to deep neural\nnetworks (DNNs) in NLP. To reduce the performance gap, in this paper, we\npropose a novel way of using pre-trained word representations for TM. The\napproach significantly enhances the performance and interpretability of TM. We\nachieve this by extracting semantically related words from pre-trained word\nrepresentations as input features to the TM. Our experiments show that the\naccuracy of the proposed approach is significantly higher than the previous\nBOW-based TM, reaching the level of DNN-based models.", "published": "2021-04-14 14:48:41", "link": "http://arxiv.org/abs/2104.06901v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating synthetic natural language to database queries: a polyglot\n  deep learning framework", "abstract": "The number of databases as well as their size and complexity is increasing.\nThis creates a barrier to use especially for non-experts, who have to come to\ngrips with the nature of the data, the way it has been represented in the\ndatabase, and the specific query languages or user interfaces by which data are\naccessed. These difficulties worsen in research settings, where it is common to\nwork with many different databases. One approach to improving this situation is\nto allow users to pose their queries in natural language.\n  In this work we describe a machine learning framework, Polyglotter, that in a\ngeneral way supports the mapping of natural language searches to database\nqueries. Importantly, it does not require the creation of manually annotated\ndata for training and therefore can be applied easily to multiple domains. The\nframework is polyglot in the sense that it supports multiple different database\nengines that are accessed with a variety of query languages, including SQL and\nCypher. Furthermore Polyglotter also supports multi-class queries.\n  Our results indicate that our framework performs well on both synthetic and\nreal databases, and may provide opportunities for database maintainers to\nimprove accessibility to their resources.", "published": "2021-04-14 17:43:51", "link": "http://arxiv.org/abs/2104.07010v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Human Mental States with an Entity-based Narrative Graph", "abstract": "Understanding narrative text requires capturing characters' motivations,\ngoals, and mental states. This paper proposes an Entity-based Narrative Graph\n(ENG) to model the internal-states of characters in a story. We explicitly\nmodel entities, their interactions and the context in which they appear, and\nlearn rich representations for them. We experiment with different task-adaptive\npre-training objectives, in-domain training, and symbolic inference to capture\ndependencies between different decisions in the output space. We evaluate our\nmodel on two narrative understanding tasks: predicting character mental states,\nand desire fulfillment, and conduct a qualitative analysis.", "published": "2021-04-14 19:05:19", "link": "http://arxiv.org/abs/2104.07079v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural-Language Multi-Agent Simulations of Argumentative Opinion\n  Dynamics", "abstract": "This paper develops a natural-language agent-based model of argumentation\n(ABMA). Its artificial deliberative agents (ADAs) are constructed with the help\nof so-called neural language models recently developed in AI and computational\nlinguistics. ADAs are equipped with a minimalist belief system and may generate\nand submit novel contributions to a conversation. The natural-language ABMA\nallows us to simulate collective deliberation in English, i.e. with arguments,\nreasons, and claims themselves -- rather than with their mathematical\nrepresentations (as in formal models). This paper uses the natural-language\nABMA to test the robustness of formal reason-balancing models of argumentation\n[Maes & Flache 2013, Singer et al. 2019]: First of all, as long as ADAs remain\npassive, confirmation bias and homophily updating trigger polarization, which\nis consistent with results from formal models. However, once ADAs start to\nactively generate new contributions, the evolution of a conservation is\ndominated by properties of the agents *as authors*. This suggests that the\ncreation of new arguments, reasons, and claims critically affects a\nconversation and is of pivotal importance for understanding the dynamics of\ncollective deliberation. The paper closes by pointing out further fruitful\napplications of the model and challenges for future research.", "published": "2021-04-14 09:45:22", "link": "http://arxiv.org/abs/2104.06737v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning Metrics from Mean Teacher: A Supervised Learning Method for\n  Improving the Generalization of Speaker Verification System", "abstract": "Most speaker verification tasks are studied as an open-set evaluation\nscenario considering the real-world condition. Thus, the generalization power\nto unseen speakers is of paramount important to the performance of the speaker\nverification system. We propose to apply \\textit {Mean Teacher}, a temporal\naveraging model, to extract speaker embeddings with small intra-class variance\nand large inter-class variance. The mean teacher network refers to the temporal\naveraging of deep neural network parameters; it can produces more accurate and\nstable representations than using weights after the training finished. By\nlearning the reliable intermediate representation of the mean teacher network,\nwe expect that the proposed method can explore more discriminatory embedding\nspaces and improve the generalization performance of the speaker verification\nsystem. Experimental results on the VoxCeleb1 test set demonstrate that the\nproposed method relatively improves performance by 11.61\\%, compared to a\nbaseline system.", "published": "2021-04-14 03:06:38", "link": "http://arxiv.org/abs/2104.06604v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Revisiting the Onsets and Frames Model with Additive Attention", "abstract": "Recent advances in automatic music transcription (AMT) have achieved highly\naccurate polyphonic piano transcription results by incorporating onset and\noffset detection. The existing literature, however, focuses mainly on the\nleverage of deep and complex models to achieve state-of-the-art (SOTA)\naccuracy, without understanding model behaviour. In this paper, we conduct a\ncomprehensive examination of the Onsets-and-Frames AMT model, and pinpoint the\nessential components contributing to a strong AMT performance. This is achieved\nthrough exploitation of a modified additive attention mechanism. The\nexperimental results suggest that the attention mechanism beyond a moderate\ntemporal context does not benefit the model, and that rule-based\npost-processing is largely responsible for the SOTA performance. We also\ndemonstrate that the onsets are the most significant attentive feature\nregardless of model complexity. The findings encourage AMT research to weigh\nmore on both a robust onset detector and an effective post-processor.", "published": "2021-04-14 03:14:58", "link": "http://arxiv.org/abs/2104.06607v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-based cough counting using independent subspace analysis", "abstract": "In this paper, an algorithm designed to detect characteristic cough events in\naudio recordings is presented, significantly reducing the time required for\nmanual counting. Using time-frequency representations and independent subspace\nanalysis (ISA), sound events that exhibit characteristics of coughs are\nautomatically detected, producing a summary of the events detected. Using a\ndataset created from publicly available audio recordings, this algorithm has\nbeen tested on a variety of synthesized audio scenarios representative of those\nlikely to be encountered by subjects undergoing an ambulatory cough recording,\nachieving a true positive rate of 76% with an average of 2.85 false positives\nper minute.", "published": "2021-04-14 12:03:28", "link": "http://arxiv.org/abs/2104.06798v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient conformer-based speech recognition with linear attention", "abstract": "Recently, conformer-based end-to-end automatic speech recognition, which\noutperforms recurrent neural network based ones, has received much attention.\nAlthough the parallel computing of conformer is more efficient than recurrent\nneural networks, the computational complexity of its dot-product self-attention\nis quadratic with respect to the length of the input feature. To reduce the\ncomputational complexity of the self-attention layer, we propose multi-head\nlinear self-attention for the self-attention layer, which reduces its\ncomputational complexity to linear order. In addition, we propose to factorize\nthe feed forward module of the conformer by low-rank matrix factorization,\nwhich successfully reduces the number of the parameters by approximate 50% with\nlittle performance loss. The proposed model, named linear attention based\nconformer (LAC), can be trained and inferenced jointly with the connectionist\ntemporal classification objective, which further improves the performance of\nLAC. To evaluate the effectiveness of LAC, we conduct experiments on the\nAISHELL-1 and LibriSpeech corpora. Results show that the proposed LAC achieves\nbetter performance than 7 recently proposed speech recognition models, and is\ncompetitive with the state-of-the-art conformer. Meanwhile, the proposed LAC\nhas a number of parameters of only 50% over the conformer with faster training\nspeed than the latter.", "published": "2021-04-14 13:56:32", "link": "http://arxiv.org/abs/2104.06865v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FastS2S-VC: Streaming Non-Autoregressive Sequence-to-Sequence Voice\n  Conversion", "abstract": "This paper proposes a non-autoregressive extension of our previously proposed\nsequence-to-sequence (S2S) model-based voice conversion (VC) methods. S2S\nmodel-based VC methods have attracted particular attention in recent years for\ntheir flexibility in converting not only the voice identity but also the pitch\ncontour and local duration of input speech, thanks to the ability of the\nencoder-decoder architecture with the attention mechanism. However, one of the\nobstacles to making these methods work in real-time is the autoregressive (AR)\nstructure. To overcome this obstacle, we develop a method to obtain a model\nthat is free from an AR structure and behaves similarly to the original S2S\nmodels, based on a teacher-student learning framework. In our method, called\n\"FastS2S-VC\", the student model consists of encoder, decoder, and attention\npredictor. The attention predictor learns to predict attention distributions\nsolely from source speech along with a target class index with the guidance of\nthose predicted by the teacher model from both source and target speech. Thanks\nto this structure, the model is freed from an AR structure and allows for\nparallelization. Furthermore, we show that FastS2S-VC is suitable for real-time\nimplementation based on a sliding-window approach, and describe how to make it\nrun in real-time. Through speaker-identity and emotional-expression conversion\nexperiments, we confirmed that FastS2S-VC was able to speed up the conversion\nprocess by 70 to 100 times compared to the original AR-type S2S-VC methods,\nwithout significantly degrading the audio quality and similarity to target\nspeech. We also confirmed that the real-time version of FastS2S-VC can be run\nwith a latency of 32 ms when run on a GPU.", "published": "2021-04-14 14:48:22", "link": "http://arxiv.org/abs/2104.06900v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-end Keyword Spotting using Neural Architecture Search and\n  Quantization", "abstract": "This paper introduces neural architecture search (NAS) for the automatic\ndiscovery of end-to-end keyword spotting (KWS) models in limited resource\nenvironments. We employ a differentiable NAS approach to optimize the structure\nof convolutional neural networks (CNNs) operating on raw audio waveforms. After\na suitable KWS model is found with NAS, we conduct quantization of weights and\nactivations to reduce the memory footprint. We conduct extensive experiments on\nthe Google speech commands dataset. In particular, we compare our end-to-end\napproach to mel-frequency cepstral coefficient (MFCC) based systems. For\nquantization, we compare fixed bit-width quantization and trained bit-width\nquantization. Using NAS only, we were able to obtain a highly efficient model\nwith an accuracy of 95.55% using 75.7k parameters and 13.6M operations. Using\ntrained bit-width quantization, the same model achieves a test accuracy of\n93.76% while using on average only 2.91 bits per activation and 2.51 bits per\nweight.", "published": "2021-04-14 07:22:22", "link": "http://arxiv.org/abs/2104.06666v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Audio feature ranking for sound-based COVID-19 patient detection", "abstract": "Audio classification using breath and cough samples has recently emerged as a\nlow-cost, non-invasive, and accessible COVID-19 screening method. However, a\ncomprehensive survey shows that no application has been approved for official\nuse at the time of writing, due to the stringent reliability and accuracy\nrequirements of the critical healthcare setting. To support the development of\nMachine Learning classification models, we performed an extensive comparative\ninvestigation and ranking of 15 audio features, including less well-known ones.\nThe results were verified on two independent COVID-19 sound datasets. By using\nthe identified top-performing features, we have increased COVID-19\nclassification accuracy by up to 17% on the Cambridge dataset and up to 10% on\nthe Coswara dataset compared to the original baseline accuracies without our\nfeature ranking.", "published": "2021-04-14 21:06:20", "link": "http://arxiv.org/abs/2104.07128v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Design of Deep Priors for Unsupervised Audio Restoration", "abstract": "Unsupervised deep learning methods for solving audio restoration problems\nextensively rely on carefully tailored neural architectures that carry strong\ninductive biases for defining priors in the time or spectral domain. In this\ncontext, lot of recent success has been achieved with sophisticated\nconvolutional network constructions that recover audio signals in the spectral\ndomain. However, in practice, audio priors require careful engineering of the\nconvolutional kernels to be effective at solving ill-posed restoration tasks,\nwhile also being easy to train. To this end, in this paper, we propose a new\nU-Net based prior that does not impact either the network complexity or\nconvergence behavior of existing convolutional architectures, yet leads to\nsignificantly improved restoration. In particular, we advocate the use of\ncarefully designed dilation schedules and dense connections in the U-Net\narchitecture to obtain powerful audio priors. Using empirical studies on\nstandard benchmarks and a variety of ill-posed restoration tasks, such as audio\ndenoising, in-painting and source separation, we demonstrate that our proposed\napproach consistently outperforms widely adopted audio prior architectures.", "published": "2021-04-14 23:16:25", "link": "http://arxiv.org/abs/2104.07161v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
