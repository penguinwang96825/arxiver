{"title": "MoCE: Adaptive Mixture of Contextualization Experts for Byte-based\n  Neural Machine Translation", "abstract": "Byte-based machine translation systems have shown significant potential in\nmassively multilingual settings. Unicode encoding, which maps each character to\nspecific byte(s), eliminates the emergence of unknown words, even in new\nlanguages. This avoids out-of-vocabulary risk in multilingual translation and\nenables broad language scalability. However, byte-level tokenization results in\nsequences that are hard to interpret due to limited semantic information per\nbyte. Local contextualization has proven effective in assigning initial\nsemantics to tokens, improving sentence comprehension. Nevertheless, variations\nin encoding rules across languages necessitate an adaptive approach for\neffective contextualization. To this end, we propose Mixture of\nContextualization Experts (MoCE), adaptively selecting and mixing attention\nheads, which are treated as contextualization experts. This enhances the\nflexibility of contextualization scales and allows models to search for better\ncontextualization combinations. Experiment results show that our method\noutperforms existing methods without extensive manual adjustment of\nhyper-parameters and surpasses subword-based models with fewer parameters in\nTed-59 dataset. Our code is available at https://github.com/ictnlp/MoCE.", "published": "2024-11-03 08:15:43", "link": "http://arxiv.org/abs/2411.01474v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching Models to Improve on Tape", "abstract": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such \"corrective feedback\". Here we claim that this\nskill of LLMs can be significantly enhanced via training. We introduce an RL\nframework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.", "published": "2024-11-03 08:49:55", "link": "http://arxiv.org/abs/2411.01483v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-specific Guided Summarization for Mental Health Posts", "abstract": "In domain-specific contexts, particularly mental health, abstractive\nsummarization requires advanced techniques adept at handling specialized\ncontent to generate domain-relevant and faithful summaries. In response to\nthis, we introduce a guided summarizer equipped with a dual-encoder and an\nadapted decoder that utilizes novel domain-specific guidance signals, i.e.,\nmental health terminologies and contextually rich sentences from the source\ndocument, to enhance its capacity to align closely with the content and context\nof guidance, thereby generating a domain-relevant summary. Additionally, we\npresent a post-editing correction model to rectify errors in the generated\nsummary, thus enhancing its consistency with the original content in detail.\nEvaluation on the MentSum dataset reveals that our model outperforms existing\nbaseline models in terms of both ROUGE and FactCC scores. Although the\nexperiments are specifically designed for mental health posts, the methodology\nwe've developed offers broad applicability, highlighting its versatility and\neffectiveness in producing high-quality domain-specific summaries.", "published": "2024-11-03 08:57:41", "link": "http://arxiv.org/abs/2411.01485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in\n  Endangered Uralic Languages using ChatGPT", "abstract": "We showcase that ChatGPT can be used to disambiguate lemmas in two endangered\nlanguages ChatGPT is not proficient in, namely Erzya and Skolt Sami. We augment\nour prompt by providing dictionary translations of the candidate lemmas to a\nmajority language - Finnish in our case. This dictionary augmented generation\napproach results in 50\\% accuracy for Skolt Sami and 41\\% accuracy for Erzya.\nOn a closer inspection, many of the error types were of the kind even an\nuntrained human annotator would make.", "published": "2024-11-03 11:25:39", "link": "http://arxiv.org/abs/2411.01531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining and Improving Contrastive Decoding by Extrapolating the\n  Probabilities of a Huge and Hypothetical LM", "abstract": "Contrastive decoding (CD) (Li et al., 2023) improves the next-token\ndistribution of a large expert language model (LM) using a small amateur LM.\nAlthough CD is applied to various LMs and domains to enhance open-ended text\ngeneration, it is still unclear why CD often works well, when it could fail,\nand how we can make it better. To deepen our understanding of CD, we first\ntheoretically prove that CD could be viewed as linearly extrapolating the\nnext-token logits from a huge and hypothetical LM. We also highlight that the\nlinear extrapolation could make CD unable to output the most obvious answers\nthat have already been assigned high probabilities by the amateur LM.\n  To overcome CD's limitation, we propose a new unsupervised decoding method\ncalled $\\mathbf{A}$symptotic $\\mathbf{P}$robability $\\mathbf{D}$ecoding (APD).\nAPD explicitly extrapolates the probability curves from the LMs of different\nsizes to infer the asymptotic probabilities from an infinitely large LM without\ninducing more inference costs than CD. In FactualityPrompts, an open-ended text\ngeneration benchmark, sampling using APD significantly boosts factuality in\ncomparison to the CD sampling and its variants, and achieves state-of-the-art\nresults for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA\ndatasets, APD is often significantly better than CD and achieves a similar\neffect of using a larger LLM. For example, the perplexity of APD on top of\nPythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA\nand LAMBADA.", "published": "2024-11-03 15:31:44", "link": "http://arxiv.org/abs/2411.01610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Large Language Models for Complex Word Identification in\n  Multilingual and Multidomain Setups", "abstract": "Complex Word Identification (CWI) is an essential step in the lexical\nsimplification task and has recently become a task on its own. Some variations\nof this binary classification task have emerged, such as lexical complexity\nprediction (LCP) and complexity evaluation of multi-word expressions (MWE).\nLarge language models (LLMs) recently became popular in the Natural Language\nProcessing community because of their versatility and capability to solve\nunseen tasks in zero/few-shot settings. Our work investigates LLM usage,\nspecifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and\nclosed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE\nsettings. We evaluate zero-shot, few-shot, and fine-tuning settings and show\nthat LLMs struggle in certain conditions or achieve comparable results against\nexisting methods. In addition, we provide some views on meta-learning combined\nwith prompt learning. In the end, we conclude that the current state of LLMs\ncannot or barely outperform existing methods, which are usually much smaller.", "published": "2024-11-03 22:31:02", "link": "http://arxiv.org/abs/2411.01706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DPCL-Diff: The Temporal Knowledge Graph Reasoning Based on Graph Node\n  Diffusion Model with Dual-Domain Periodic Contrastive Learning", "abstract": "Temporal knowledge graph (TKG) reasoning that infers future missing facts is\nan essential and challenging task. Predicting future events typically relies on\nclosely related historical facts, yielding more accurate results for repetitive\nor periodic events. However, for future events with sparse historical\ninteractions, the effectiveness of this method, which focuses on leveraging\nhigh-frequency historical information, diminishes. Recently, the capabilities\nof diffusion models in image generation have opened new opportunities for TKG\nreasoning. Therefore, we propose a graph node diffusion model with dual-domain\nperiodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)\nintroduces noise into sparsely related events to simulate new events,\ngenerating high-quality data that better conforms to the actual distribution.\nThis generative mechanism significantly enhances the model's ability to reason\nabout new events. Additionally, the dual-domain periodic contrastive learning\n(DPCL) maps periodic and non-periodic event entities to Poincar\\'e and\nEuclidean spaces, leveraging their characteristics to distinguish similar\nperiodic events effectively. Experimental results on four public datasets\ndemonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG\nmodels in event prediction, demonstrating our approach's effectiveness. This\nstudy also investigates the combined effectiveness of GNDiff and DPCL in TKG\ntasks.", "published": "2024-11-03 08:30:29", "link": "http://arxiv.org/abs/2411.01477v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Integration of Large Vision Language Models for Efficient Post-disaster\n  Damage Assessment and Reporting", "abstract": "Traditional natural disaster response involves significant coordinated\nteamwork where speed and efficiency are key. Nonetheless, human limitations can\ndelay critical actions and inadvertently increase human and economic losses.\nAgentic Large Vision Language Models (LVLMs) offer a new avenue to address this\nchallenge, with the potential for substantial socio-economic impact,\nparticularly by improving resilience and resource access in underdeveloped\nregions. We introduce DisasTeller, the first multi-LVLM-powered framework\ndesigned to automate tasks in post-disaster management, including on-site\nassessment, emergency alerts, resource allocation, and recovery planning. By\ncoordinating four specialised LVLM agents with GPT-4 as the core model,\nDisasTeller autonomously implements disaster response activities, reducing\nhuman execution time and optimising resource distribution. Our evaluations\nthrough both LVLMs and humans demonstrate DisasTeller's effectiveness in\nstreamlining disaster response. This framework not only supports expert teams\nbut also simplifies access to disaster management processes for non-experts,\nbridging the gap between traditional response methods and LVLM-driven\nefficiency.", "published": "2024-11-03 10:25:55", "link": "http://arxiv.org/abs/2411.01511v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA"}
{"title": "SinaTools: Open Source Toolkit for Arabic Natural Language Processing", "abstract": "We introduce SinaTools, an open-source Python package for Arabic natural\nlanguage processing and understanding. SinaTools is a unified package allowing\npeople to integrate it into their system workflow, offering solutions for\nvarious tasks such as flat and nested Named Entity Recognition (NER),\nfully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy\nExtractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root\nTagging, and additional helper utilities such as corpus processing, text\nstripping methods, and diacritic-aware word matching. This paper presents\nSinaTools and its benchmarking results, demonstrating that SinaTools\noutperforms all similar tools on the aforementioned tasks, such as Flat NER\n(87.33%), Nested NER (89.42%), WSD (82.63%), Semantic Relatedness (0.49\nSpearman rank), Lemmatization (90.5%), POS tagging (97.5%), among others.\nSinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).", "published": "2024-11-03 11:03:52", "link": "http://arxiv.org/abs/2411.01523v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs and the Madness of Crowds", "abstract": "We investigate the patterns of incorrect answers produced by large language\nmodels (LLMs) during evaluation. These errors exhibit highly non-intuitive\nbehaviors unique to each model. By analyzing these patterns, we measure the\nsimilarities between LLMs and construct a taxonomy that categorizes them based\non their error correlations. Our findings reveal that the incorrect responses\nare not randomly distributed but systematically correlated across models,\nproviding new insights into the underlying structures and relationships among\nLLMs.", "published": "2024-11-03 12:03:12", "link": "http://arxiv.org/abs/2411.01539v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are LLMs good pragmatic speakers?", "abstract": "Large language models (LLMs) are trained on data assumed to include natural\nlanguage pragmatics, but do they actually behave like pragmatic speakers? We\nattempt to answer this question using the Rational Speech Act (RSA) framework,\nwhich models pragmatic reasoning in human communication. Using the paradigm of\na reference game constructed from the TUNA corpus, we score candidate\nreferential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct) and\nin the RSA model, comparing and contrasting these scores. Given that RSA\nrequires defining alternative utterances and a truth-conditional meaning\nfunction, we explore such comparison for different choices of each of these\nrequirements. We find that while scores from the LLM have some positive\ncorrelation with those from RSA, there isn't sufficient evidence to claim that\nit behaves like a pragmatic speaker. This initial study paves way for further\ntargeted efforts exploring different models and settings, including\nhuman-subject evaluation, to see if LLMs truly can, or be made to, behave like\npragmatic speakers.", "published": "2024-11-03 13:23:18", "link": "http://arxiv.org/abs/2411.01562v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ontology Population using LLMs", "abstract": "Knowledge graphs (KGs) are increasingly utilized for data integration,\nrepresentation, and visualization. While KG population is critical, it is often\ncostly, especially when data must be extracted from unstructured text in\nnatural language, which presents challenges, such as ambiguity and complex\ninterpretations. Large Language Models (LLMs) offer promising capabilities for\nsuch tasks, excelling in natural language understanding and content generation.\nHowever, their tendency to ``hallucinate'' can produce inaccurate outputs.\nDespite these limitations, LLMs offer rapid and scalable processing of natural\nlanguage data, and with prompt engineering and fine-tuning, they can\napproximate human-level performance in extracting and structuring data for KGs.\nThis study investigates LLM effectiveness for the KG population, focusing on\nthe Enslaved.org Hub Ontology. In this paper, we report that compared to the\nground truth, LLM's can extract ~90% of triples, when provided a modular\nontology as guidance in the prompts.", "published": "2024-11-03 15:39:20", "link": "http://arxiv.org/abs/2411.01612v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "EcoAct: Economic Agent Determines When to Register What Action", "abstract": "Recent advancements have enabled Large Language Models (LLMs) to function as\nagents that can perform actions using external tools. This requires\nregistering, i.e., integrating tool information into the LLM context prior to\ntaking actions. Current methods indiscriminately incorporate all candidate\ntools into the agent's context and retain them across multiple reasoning steps.\nThis process remains opaque to LLM agents and is not integrated into their\nreasoning procedures, leading to inefficiencies due to increased context length\nfrom irrelevant tools. To address this, we introduce EcoAct, a tool using\nalgorithm that allows LLMs to selectively register tools as needed, optimizing\ncontext use. By integrating the tool registration process into the reasoning\nprocedure, EcoAct reduces computational costs by over 50% in multiple steps\nreasoning tasks while maintaining performance, as demonstrated through\nextensive experiments. Moreover, it can be plugged into any reasoning pipeline\nwith only minor modifications to the prompt, making it applicable to LLM agents\nnow and future.", "published": "2024-11-03 17:37:06", "link": "http://arxiv.org/abs/2411.01643v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors", "abstract": "Despite significant advancements, large language models (LLMs) still struggle\nwith providing accurate answers when lacking domain-specific or up-to-date\nknowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by\nincorporating external knowledge bases, but it also introduces new attack\nsurfaces. In this paper, we investigate data extraction attacks targeting RAG's\nknowledge databases. We show that previous prompt injection-based extraction\nattacks largely rely on the instruction-following capabilities of LLMs. As a\nresult, they fail on models that are less responsive to such malicious prompts\n-- for example, our experiments show that state-of-the-art attacks achieve\nnear-zero success on Gemma-2B-IT. Moreover, even for models that can follow\nthese instructions, we found fine-tuning may significantly reduce attack\nperformance. To further reveal the vulnerability, we propose to backdoor RAG,\nwhere a small portion of poisoned data is injected during the fine-tuning phase\nto create a backdoor within the LLM. When this compromised LLM is integrated\ninto a RAG system, attackers can exploit specific triggers in prompts to\nmanipulate the LLM to leak documents from the retrieval database. By carefully\ndesigning the poisoned data, we achieve both verbatim and paraphrased document\nextraction. For example, on Gemma-2B-IT, we show that with only 5\\% poisoned\ndata, our method achieves an average success rate of 94.1\\% for verbatim\nextraction (ROUGE-L score: 82.1) and 63.6\\% for paraphrased extraction (average\nROUGE score: 66.4) across four datasets. These results underscore the privacy\nrisks associated with the supply chain when deploying RAG systems.", "published": "2024-11-03 22:27:40", "link": "http://arxiv.org/abs/2411.01705v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in\n  Automatic Evaluation by Large Language Models", "abstract": "LLMs have demonstrated impressive proficiency in generating coherent and\nhigh-quality text, making them valuable across a range of text-generation\ntasks. However, rigorous evaluation of this generated content is crucial, as\nensuring its quality remains a significant challenge due to persistent issues\nsuch as factual inaccuracies and hallucination. This paper introduces three\nfine-tuned general-purpose LLM autoevaluators, REC-8B, REC-12B and REC-70B,\nspecifically designed to evaluate generated text across several dimensions:\nfaithfulness, instruction following, coherence, and completeness. These models\nnot only provide ratings for these metrics but also offer detailed explanation\nand verifiable citation, thereby enhancing trust in the content. Moreover, the\nmodels support various citation modes, accommodating different requirements for\nlatency and granularity. Extensive evaluations on diverse benchmarks\ndemonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms\nstate-of-the-art LLMs, excelling in content evaluation by delivering better\nquality explanation and citation with minimal bias. It achieves Rank #1 as of\nFeb 15th, 2025 as a generative model on the RewardBench leaderboard under the\nmodel name TextEval-Llama3.1-70B. Our REC dataset and models are available at\nhttps://github.com/adelaidehsu/REC.", "published": "2024-11-03 02:36:33", "link": "http://arxiv.org/abs/2411.02448v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Sentiment Analysis Framework for Hate Speech Detection:\n  Implementing Binary and Multiclass Classification Strategy", "abstract": "A significant challenge in automating hate speech detection on social media\nis distinguishing hate speech from regular and offensive language. These\nidentify an essential category of content that web filters seek to remove. Only\nautomated methods can manage this volume of daily data. To solve this problem,\nthe community of Natural Language Processing is currently investigating\ndifferent ways of hate speech detection. In addition to those, previous\napproaches (e.g., Convolutional Neural Networks, multi-channel BERT models, and\nlexical detection) have always achieved low precision without carefully\ntreating other related tasks like sentiment analysis and emotion\nclassification. They still like to group all messages with specific words in\nthem as hate speech simply because those terms often appear alongside hateful\nrhetoric. In this research, our paper presented the hate speech text\nclassification system model drawn upon deep learning and machine learning. In\nthis paper, we propose a new multitask model integrated with shared emotional\nrepresentations to detect hate speech across the English language. The\nTransformer-based model we used from Hugging Face and sentiment analysis helped\nus prevent false positives. Conclusion. We conclude that utilizing sentiment\nanalysis and a Transformer-based trained model considerably improves hate\nspeech detection across multiple datasets.", "published": "2024-11-03 04:11:33", "link": "http://arxiv.org/abs/2411.05819v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Classifier-guided Gradient Modulation for Enhanced Multimodal Learning", "abstract": "Multimodal learning has developed very fast in recent years. However, during\nthe multimodal training process, the model tends to rely on only one modality\nbased on which it could learn faster, thus leading to inadequate use of other\nmodalities. Existing methods to balance the training process always have some\nlimitations on the loss functions, optimizers and the number of modalities and\nonly consider modulating the magnitude of the gradients while ignoring the\ndirections of the gradients. To solve these problems, in this paper, we present\na novel method to balance multimodal learning with Classifier-Guided Gradient\nModulation (CGGM), considering both the magnitude and directions of the\ngradients. We conduct extensive experiments on four multimodal datasets:\nUPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification,\nregression and segmentation tasks. The results show that CGGM outperforms all\nthe baselines and other state-of-the-art methods consistently, demonstrating\nits effectiveness and versatility. Our code is available at\nhttps://github.com/zrguo/CGGM.", "published": "2024-11-03 02:38:43", "link": "http://arxiv.org/abs/2411.01409v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Sample-Efficient Alignment for LLMs", "abstract": "We study methods for efficiently aligning large language models (LLMs) with\nhuman preferences given budgeted online feedback. We first formulate the LLM\nalignment problem in the frame of contextual dueling bandits. This formulation,\nsubsuming recent paradigms such as online RLHF and online DPO, inherently\nquests for sample-efficient algorithms that incorporate online active\nexploration. Leveraging insights from bandit theory, we introduce a unified\nalgorithm based on Thompson sampling and highlight its applications in two\ndistinct LLM alignment scenarios. The practical agent that efficiently\nimplements this algorithm, named SEA (Sample-Efficient Alignment), is\nempirically validated through extensive experiments across three model scales\n(1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The\nresults demonstrate that SEA achieves highly sample-efficient alignment with\noracle's preferences, outperforming recent active exploration methods for LLMs.\nAdditionally, we release the implementation of SEA together with an efficient\ncodebase designed for online alignment of LLMs, aiming to accelerate future\nresearch in this field.", "published": "2024-11-03 09:18:28", "link": "http://arxiv.org/abs/2411.01493v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhancing LLM Evaluations: The Garbling Trick", "abstract": "As large language models (LLMs) become increasingly powerful, traditional\nevaluation metrics tend to saturate, making it challenging to distinguish\nbetween models based on their performance. We propose a general method to\ntransform existing LLM evaluations into a series of progressively more\ndifficult tasks. These enhanced evaluations emphasize reasoning capabilities\nand can reveal relative performance differences that are not apparent in the\noriginal assessments.\n  To demonstrate the effectiveness of our approach, we create a new\nmultiple-choice test corpus, extend it into a family of evaluations, and assess\na collection of LLMs. Our results offer insights into the comparative reasoning\nabilities of these models, particularly highlighting distinctions between\nOpenAI's o1-preview and Google's gemini-pro-1.5-002.", "published": "2024-11-03 11:39:50", "link": "http://arxiv.org/abs/2411.01533v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Microservices Architecture for Dynamic Pricing in the Travel\n  Industry: Algorithms, Scalability, and Impact on Revenue and Customer\n  Satisfaction", "abstract": "This research investigates the implementation of a real-time,\nmicroservices-oriented dynamic pricing system for the travel sector. The system\nis designed to address factors such as demand, competitor pricing, and other\nexternal circumstances in real-time. Both controlled simulation and real-life\napplication showed a respectable gain of 22% in revenue generation and a 17%\nimprovement in pricing response time which concern the issues of scaling and\nflexibility of classical pricing mechanisms. Demand forecasting, competitor\npricing strategies, and event-based pricing were implemented as separate\nmicroservices to enhance their scalability and reduce resource consumption by\n30% during peak loads. Customers were also more content as depicted by a 15%\nincrease in satisfaction score post-implementation given the appreciation of\nmore appropriate pricing. This research enhances the existing literature with\npractical illustrations of the possible application of microservices technology\nin developing dynamic pricing solutions in a complex and data-driven context.\nThere exist however areas for improvement for instance inter-service latency\nand the need for extensive real-time data pipelines. The present research goes\non to suggest combining these with direct data capture from customer behavior\nat the same time as machine learning capacity developments in pricing\nalgorithms to assist in more accurate real time pricing. It is determined that\nthe use of microservices is a reasonable and efficient model for dynamic\npricing, allowing the tourism sector to employ evidence-based and customer\ncentric pricing techniques, which ensures that their profits are not\njeopardized because of the need for customers.", "published": "2024-11-03 17:24:02", "link": "http://arxiv.org/abs/2411.01636v1", "categories": ["cs.CE", "cs.CL", "cs.DC", "cs.GT"], "primary_category": "cs.CE"}
{"title": "Diagnosing Medical Datasets with Training Dynamics", "abstract": "This study explores the potential of using training dynamics as an automated\nalternative to human annotation for evaluating the quality of training data.\nThe framework used is Data Maps, which classifies data points into categories\nsuch as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020).\nSwayamdipta et al. (2020) highlight that difficult-to-learn examples often\ncontain errors, and ambiguous cases significantly impact model training. To\nconfirm the reliability of these findings, we replicated the experiments using\na challenging dataset, with a focus on medical question answering. In addition\nto text comprehension, this field requires the acquisition of detailed medical\nknowledge, which further complicates the task. A comprehensive evaluation was\nconducted to assess the feasibility and transferability of the Data Maps\nframework to the medical domain. The evaluation indicates that the framework is\nunsuitable for addressing datasets' unique challenges in answering medical\nquestions.", "published": "2024-11-03 18:37:35", "link": "http://arxiv.org/abs/2411.01653v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unlocking the Theory Behind Scaling 1-Bit Neural Networks", "abstract": "Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an\nimpressive combination of efficiency and performance that rivals traditional\nLLMs. Research by Wang et al. (2023); Ma et al. (2024) indicates that the\nperformance of these 1-bit LLMs progressively improves as the number of\nparameters increases, hinting at the potential existence of a Scaling Law for\n1-bit Neural Networks. In this paper, we present the first theoretical result\nthat rigorously establishes this scaling law for 1-bit models. We prove that,\ndespite the constraint of weights restricted to $\\{-1, +1\\}$, the dynamics of\nmodel training inevitably align with kernel behavior as the network width\ngrows. This theoretical breakthrough guarantees convergence of the 1-bit model\nto an arbitrarily small loss as width increases. Furthermore, we introduce the\nconcept of the generalization difference, defined as the gap between the\noutputs of 1-bit networks and their full-precision counterparts, and\ndemonstrate that this difference maintains a negligible level as network width\nscales. Building on the work of Kaplan et al. (2020), we conclude by examining\nhow the training loss scales as a power-law function of the model size, dataset\nsize, and computational resources utilized for training. Our findings\nunderscore the promising potential of scaling 1-bit neural networks, suggesting\nthat int1 could become the standard in future neural network precision.", "published": "2024-11-03 19:18:57", "link": "http://arxiv.org/abs/2411.01663v1", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.CL"], "primary_category": "cs.LG"}
{"title": "UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on\n  Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) have revolutionized vision-language\nunderstanding but remain vulnerable to multimodal jailbreak attacks, where\nadversarial inputs are meticulously crafted to elicit harmful or inappropriate\nresponses. We propose UniGuard, a novel multimodal safety guardrail that\njointly considers the unimodal and cross-modal harmful signals. UniGuard trains\na multimodal guardrail to minimize the likelihood of generating harmful\nresponses in a toxic corpus. The guardrail can be seamlessly applied to any\ninput prompt during inference with minimal computational costs. Extensive\nexperiments demonstrate the generalizability of UniGuard across multiple\nmodalities, attack strategies, and multiple state-of-the-art MLLMs, including\nLLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust\ndefense mechanism maintains the models' overall vision-language understanding\ncapabilities.", "published": "2024-11-03 22:19:20", "link": "http://arxiv.org/abs/2411.01703v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation", "abstract": "Spurred by the demand for interpretable models, research on eXplainable AI\nfor language technologies has experienced significant growth, with feature\nattribution methods emerging as a cornerstone of this progress. While prior\nwork in NLP explored such methods for classification tasks and textual\napplications, explainability intersecting generation and speech is lagging,\nwith existing techniques failing to account for the autoregressive nature of\nstate-of-the-art models and to provide fine-grained, phonetically meaningful\nexplanations. We address this gap by introducing Spectrogram Perturbation for\nExplainable Speech-to-text Generation (SPES), a feature attribution technique\napplicable to sequence generation tasks with autoregressive models. SPES\nprovides explanations for each predicted token based on both the input\nspectrogram and the previously generated tokens. Extensive evaluation on speech\nrecognition and translation demonstrates that SPES generates explanations that\nare faithful and plausible to humans.", "published": "2024-11-03 23:02:30", "link": "http://arxiv.org/abs/2411.01710v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models", "abstract": "Modern optimizers such as AdamW, equipped with momentum and adaptive learning\nrate, are designed to escape local minima and explore the vast parameter space.\nThis exploration is beneficial for finding good loss basins when training from\nscratch. It is not necessarily ideal when resuming from a powerful foundation\nmodel because it can lead to large deviations from the pre-trained\ninitialization and, consequently, worse robustness and generalization. At the\nsame time, strong regularization on all parameters can lead to under-fitting.\nWe hypothesize that selectively regularizing the parameter space is the key to\nfitting and retraining the pre-trained knowledge. This paper proposes a new\nweight decay technique, Selective Projection Decay (SPD), that selectively\nimposes a strong penalty on certain layers while allowing others to change\nfreely. Intuitively, SPD expands and contracts the parameter search space for\nlayers with consistent and inconsistent loss reduction, respectively.\nExperimentally, when equipped with SPD, Adam consistently provides better\nin-distribution generalization and out-of-distribution robustness performance\non multiple popular vision and language benchmarks. Code available\nat~\\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}", "published": "2024-11-03 23:36:53", "link": "http://arxiv.org/abs/2411.01713v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "High-performance automated abstract screening with large language model\n  ensembles", "abstract": "Large language models (LLMs) excel in tasks requiring processing and\ninterpretation of input text. Abstract screening is a labour-intensive\ncomponent of systematic review involving repetitive application of inclusion\nand exclusion criteria on a large volume of studies identified by a literature\nsearch. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5\nPro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue\nof the Cochrane Library to evaluate their accuracy in zero-shot binary\nclassification for abstract screening. Trials over a subset of 800 records\nidentified optimal prompting strategies and demonstrated superior performance\nof LLMs to human researchers in terms of sensitivity (LLM-max = 1.000,\nhuman-max = 0.775), precision (LLM-max = 0.927, human-max = 0.911), and\nbalanced accuracy (LLM-max = 0.904, human-max = 0.865). The best performing\nLLM-prompt combinations were trialled across every replicated search result (n\n= 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but\ndiminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles\nexhibited perfect sensitivity with a maximal precision of 0.458, with less\nobserved performance drop in larger trials. Significant variation in\nperformance was observed between reviews, highlighting the importance of\ndomain-specific validation before deployment. LLMs may reduce the human labour\ncost of systematic review with maintained or improved accuracy and sensitivity.\nSystematic review is the foundation of evidence synthesis across academic\ndisciplines, including evidence-based medicine, and LLMs may increase the\nefficiency and quality of this mode of research.", "published": "2024-11-03 10:06:14", "link": "http://arxiv.org/abs/2411.02451v2", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Graph-based Confidence Calibration for Large Language Models", "abstract": "One important approach to improving the reliability of large language models\n(LLMs) is to provide accurate confidence estimations regarding the correctness\nof their answers. However, developing a well-calibrated confidence estimation\nmodel is challenging, as mistakes made by LLMs can be difficult to detect. We\npropose a novel method combining the LLM's self-consistency with labeled data\nand training an auxiliary model to estimate the correctness of its responses to\nquestions. This auxiliary model predicts the correctness of responses based\nsolely on their consistent information. To set up the learning problem, we use\na weighted graph to represent the consistency among the LLM's multiple\nresponses to a question. Correctness labels are assigned to these responses\nbased on their similarity to the correct answer. We then train a graph neural\nnetwork to estimate the probability of correct responses. Experiments\ndemonstrate that the proposed approach substantially outperforms several of the\nmost recent methods in confidence calibration across multiple widely adopted\nbenchmark datasets. Furthermore, the proposed approach significantly improves\nthe generalization capability of confidence calibration on out-of-domain (OOD)\ndata.", "published": "2024-11-03 20:36:44", "link": "http://arxiv.org/abs/2411.02454v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Exploration of Higher Education Course Evaluation by Large Language\n  Models", "abstract": "Course evaluation is a critical component in higher education pedagogy. It\nnot only serves to identify limitations in existing course designs and provide\na basis for curricular innovation, but also to offer quantitative insights for\nuniversity administrative decision-making. Traditional evaluation methods,\nprimarily comprising student surveys, instructor self-assessments, and expert\nreviews, often encounter challenges, including inherent subjectivity, feedback\ndelays, inefficiencies, and limitations in addressing innovative teaching\napproaches. Recent advancements in large language models (LLMs) within\nartificial intelligence (AI) present promising new avenues for enhancing course\nevaluation processes. This study explores the application of LLMs in automated\ncourse evaluation from multiple perspectives and conducts rigorous experiments\nacross 100 courses at a major university in China. The findings indicate that:\n(1) LLMs can be an effective tool for course evaluation; (2) their\neffectiveness is contingent upon appropriate fine-tuning and prompt\nengineering; and (3) LLM-generated evaluation results demonstrate a notable\nlevel of rationality and interpretability.", "published": "2024-11-03 20:43:52", "link": "http://arxiv.org/abs/2411.02455v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Sing-On-Your-Beat: Simple Text-Controllable Accompaniment Generations", "abstract": "Singing is one of the most cherished forms of human entertainment. However,\ncreating a beautiful song requires an accompaniment that complements the vocals\nand aligns well with the song instruments and genre. With advancements in deep\nlearning, previous research has focused on generating suitable accompaniments\nbut often lacks precise alignment with the desired instrumentation and genre.\nTo address this, we propose a straightforward method that enables control over\nthe accompaniment through text prompts, allowing the generation of music that\ncomplements the vocals and aligns with the song instrumental and genre\nrequirements. Through extensive experiments, we successfully generate 10-second\naccompaniments using vocal input and text control.", "published": "2024-11-03 19:17:20", "link": "http://arxiv.org/abs/2411.01661v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
