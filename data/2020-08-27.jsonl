{"title": "Relation/Entity-Centric Reading Comprehension", "abstract": "Constructing a machine that understands human language is one of the most\nelusive and long-standing challenges in artificial intelligence. This thesis\naddresses this challenge through studies of reading comprehension with a focus\non understanding entities and their relationships. More specifically, we focus\non question answering tasks designed to measure reading comprehension. We focus\non entities and relations because they are typically used to represent the\nsemantics of natural language.", "published": "2020-08-27 06:42:18", "link": "http://arxiv.org/abs/2008.11940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improvement of a dedicated model for open domain persona-aware dialogue\n  generation", "abstract": "This paper analyzes some speed and performance improvement methods of\nTransformer architecture in recent years, mainly its application in dedicated\nmodel training. The dedicated model studied here refers to the open domain\npersona-aware dialogue generation model, and the dataset is multi turn short\ndialogue, The total length of a single input sequence is no more than 105\ntokens. Therefore, many improvements in the architecture and attention\nmechanism of transformer architecture for long sequence processing are not\ndiscussed in this paper. The source code of the experiments has been open\nsourced: https://github.com/ghosthamlet/persona", "published": "2020-08-27 07:53:47", "link": "http://arxiv.org/abs/2008.11970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query Focused Multi-document Summarisation of Biomedical Texts", "abstract": "This paper presents the participation of Macquarie University and the\nAustralian National University for Task B Phase B of the 2020 BioASQ Challenge\n(BioASQ8b). Our overall framework implements Query focused multi-document\nextractive summarisation by applying either a classification or a regression\nlayer to the candidate sentence embeddings and to the comparison between the\nquestion and sentence embeddings. We experiment with variants using BERT and\nBioBERT, Siamese architectures, and reinforcement learning. We observe the best\nresults when BERT is used to obtain the word embeddings, followed by an LSTM\nlayer to obtain sentence embeddings. Variants using Siamese architectures or\nBioBERT did not improve the results.", "published": "2020-08-27 08:31:13", "link": "http://arxiv.org/abs/2008.11986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Evaluation Metrics Used for NLG Systems", "abstract": "The success of Deep Learning has created a surge in interest in a wide a\nrange of Natural Language Generation (NLG) tasks. Deep Learning has not only\npushed the state of the art in several existing NLG tasks but has also\nfacilitated researchers to explore various newer NLG tasks such as image\ncaptioning. Such rapid progress in NLG has necessitated the development of\naccurate automatic evaluation metrics that would allow us to track the progress\nin the field of NLG. However, unlike classification tasks, automatically\nevaluating NLG systems in itself is a huge challenge. Several works have shown\nthat early heuristic-based metrics such as BLEU, ROUGE are inadequate for\ncapturing the nuances in the different NLG tasks. The expanding number of NLG\nmodels and the shortcomings of the current metrics has led to a rapid surge in\nthe number of evaluation metrics proposed since 2014. Moreover, various\nevaluation metrics have shifted from using pre-determined heuristic-based\nformulae to trained transformer models. This rapid change in a relatively short\ntime has led to the need for a survey of the existing NLG metrics to help\nexisting and new researchers to quickly come up to speed with the developments\nthat have happened in NLG evaluation in the last few years. Through this\nsurvey, we first wish to highlight the challenges and difficulties in\nautomatically evaluating NLG systems. Then, we provide a coherent taxonomy of\nthe evaluation metrics to organize the existing metrics and to better\nunderstand the developments in the field. We also describe the different\nmetrics in detail and highlight their key contributions. Later, we discuss the\nmain shortcomings identified in the existing metrics and describe the\nmethodology used to evaluate evaluation metrics. Finally, we discuss our\nsuggestions and recommendations on the next steps forward to improve the\nautomatic evaluation metrics.", "published": "2020-08-27 09:25:05", "link": "http://arxiv.org/abs/2008.12009v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GREEK-BERT: The Greeks visiting Sesame Street", "abstract": "Transformer-based language models, such as BERT and its variants, have\nachieved state-of-the-art performance in several downstream natural language\nprocessing (NLP) tasks on generic benchmark datasets (e.g., GLUE, SQUAD, RACE).\nHowever, these models have mostly been applied to the resource-rich English\nlanguage. In this paper, we present GREEK-BERT, a monolingual BERT-based\nlanguage model for modern Greek. We evaluate its performance in three NLP\ntasks, i.e., part-of-speech tagging, named entity recognition, and natural\nlanguage inference, obtaining state-of-the-art performance. Interestingly, in\ntwo of the benchmarks GREEK-BERT outperforms two multilingual Transformer-based\nmodels (M-BERT, XLM-R), as well as shallower neural baselines operating on\npre-trained word embeddings, by a large margin (5%-10%). Most importantly, we\nmake both GREEK-BERT and our training code publicly available, along with code\nillustrating how GREEK-BERT can be fine-tuned for downstream NLP tasks. We\nexpect these resources to boost NLP research and applications for modern Greek.", "published": "2020-08-27 09:36:14", "link": "http://arxiv.org/abs/2008.12014v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uralic Language Identification (ULI) 2020 shared task dataset and the\n  Wanca 2017 corpus", "abstract": "This article introduces the Wanca 2017 corpus of texts crawled from the\ninternet from which the sentences in rare Uralic languages for the use of the\nUralic Language Identification (ULI) 2020 shared task were collected. We\ndescribe the ULI dataset and how it was constructed using the Wanca 2017 corpus\nand texts in different languages from the Leipzig corpora collection. We also\nprovide baseline language identification experiments conducted using the ULI\n2020 dataset.", "published": "2020-08-27 14:57:01", "link": "http://arxiv.org/abs/2008.12169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-shift Conditioning using Adaptable Filtering via Hierarchical\n  Embeddings for Robust Chinese Spell Check", "abstract": "Spell check is a useful application which processes noisy human-generated\ntext. Spell check for Chinese poses unresolved problems due to the large number\nof characters, the sparse distribution of errors, and the dearth of resources\nwith sufficient coverage of heterogeneous and shifting error domains. For\nChinese spell check, filtering using confusion sets narrows the search space\nand makes finding corrections easier. However, most, if not all, confusion sets\nused to date are fixed and thus do not include new, shifting error domains. We\npropose a scalable adaptable filter that exploits hierarchical character\nembeddings to (1) obviate the need to handcraft confusion sets, and (2) resolve\nsparsity problems related to infrequent errors. Our approach compares favorably\nwith competitive baselines and obtains SOTA results on the 2014 and 2015\nChinese Spelling Check Bake-off datasets.", "published": "2020-08-27 17:34:40", "link": "http://arxiv.org/abs/2008.12281v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity and Evidence Guided Relation Extraction for DocRED", "abstract": "Document-level relation extraction is a challenging task which requires\nreasoning over multiple sentences in order to predict relations in a document.\nIn this paper, we pro-pose a joint training frameworkE2GRE(Entity and Evidence\nGuided Relation Extraction)for this task. First, we introduce entity-guided\nsequences as inputs to a pre-trained language model (e.g. BERT, RoBERTa). These\nentity-guided sequences help a pre-trained language model (LM) to focus on\nareas of the document related to the entity. Secondly, we guide the fine-tuning\nof the pre-trained language model by using its internal attention probabilities\nas additional features for evidence prediction.Our new approach encourages the\npre-trained language model to focus on the entities and supporting/evidence\nsentences. We evaluate our E2GRE approach on DocRED, a recently released\nlarge-scale dataset for relation extraction. Our approach is able to achieve\nstate-of-the-art results on the public leaderboard across all metrics, showing\nthat our E2GRE is both effective and synergistic on relation extraction and\nevidence prediction.", "published": "2020-08-27 17:41:23", "link": "http://arxiv.org/abs/2008.12283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models as Emotional Classifiers for Textual Conversations", "abstract": "Emotions play a critical role in our everyday lives by altering how we\nperceive, process and respond to our environment. Affective computing aims to\ninstill in computers the ability to detect and act on the emotions of human\nactors. A core aspect of any affective computing system is the classification\nof a user's emotion. In this study we present a novel methodology for\nclassifying emotion in a conversation. At the backbone of our proposed\nmethodology is a pre-trained Language Model (LM), which is supplemented by a\nGraph Convolutional Network (GCN) that propagates information over the\npredicate-argument structure identified in an utterance. We apply our proposed\nmethodology on the IEMOCAP and Friends data sets, achieving state-of-the-art\nperformance on the former and a higher accuracy on certain emotional labels on\nthe latter. Furthermore, we examine the role context plays in our methodology\nby altering how much of the preceding conversation the model has access to when\nmaking a classification.", "published": "2020-08-27 20:04:30", "link": "http://arxiv.org/abs/2008.12360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization", "abstract": "Pre-trained language models such as BERT have exhibited remarkable\nperformances in many tasks in natural language understanding (NLU). The tokens\nin the models are usually fine-grained in the sense that for languages like\nEnglish they are words or sub-words and for languages like Chinese they are\ncharacters. In English, for example, there are multi-word expressions which\nform natural lexical units and thus the use of coarse-grained tokenization also\nappears to be reasonable. In fact, both fine-grained and coarse-grained\ntokenizations have advantages and disadvantages for learning of pre-trained\nlanguage models. In this paper, we propose a novel pre-trained language model,\nreferred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained\nand coarse-grained tokenizations. For English, AMBERT takes both the sequence\nof words (fine-grained tokens) and the sequence of phrases (coarse-grained\ntokens) as input after tokenization, employs one encoder for processing the\nsequence of words and the other encoder for processing the sequence of the\nphrases, utilizes shared parameters between the two encoders, and finally\ncreates a sequence of contextualized representations of the words and a\nsequence of contextualized representations of the phrases. Experiments have\nbeen conducted on benchmark datasets for Chinese and English, including CLUE,\nGLUE, SQuAD and RACE. The results show that AMBERT can outperform BERT in all\ncases, particularly the improvements are significant for Chinese. We also\ndevelop a method to improve the efficiency of AMBERT in inference, which still\nperforms better than BERT with the same computational cost as BERT.", "published": "2020-08-27 00:23:48", "link": "http://arxiv.org/abs/2008.11869v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Summarisation: A Scoping Review", "abstract": "Speech summarisation techniques take human speech as input and then output an\nabridged version as text or speech. Speech summarisation has applications in\nmany domains from information technology to health care, for example improving\nspeech archives or reducing clinical documentation burden. This scoping review\nmaps the speech summarisation literature, with no restrictions on time frame,\nlanguage summarised, research method, or paper type. We reviewed a total of 110\npapers out of a set of 153 found through a literature search and extracted\nspeech features used, methods, scope, and training corpora. Most studies employ\none of four speech summarisation architectures: (1) Sentence extraction and\ncompaction; (2) Feature extraction and classification or rank-based sentence\nselection; (3) Sentence compression and compression summarisation; and (4)\nLanguage modelling. We also discuss the strengths and weaknesses of these\ndifferent methods and speech features. Overall, supervised methods (e.g. Hidden\nMarkov support vector machines, Ranking support vector machines, Conditional\nrandom fields) performed better than unsupervised methods. As supervised\nmethods require manually annotated training data which can be costly, there was\nmore interest in unsupervised methods. Recent research into unsupervised\nmethods focusses on extending language modelling, for example by combining\nUni-gram modelling with deep neural networks. Protocol registration: The\nprotocol for this scoping review is registered at https://osf.io.", "published": "2020-08-27 03:15:40", "link": "http://arxiv.org/abs/2008.11897v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Opinion-aware Answer Generation for Review-driven Question Answering in\n  E-Commerce", "abstract": "Product-related question answering (QA) is an important but challenging task\nin E-Commerce. It leads to a great demand on automatic review-driven QA, which\naims at providing instant responses towards user-posted questions based on\ndiverse product reviews. Nevertheless, the rich information about personal\nopinions in product reviews, which is essential to answer those\nproduct-specific questions, is underutilized in current generation-based\nreview-driven QA studies. There are two main challenges when exploiting the\nopinion information from the reviews to facilitate the opinion-aware answer\ngeneration: (i) jointly modeling opinionated and interrelated information\nbetween the question and reviews to capture important information for answer\ngeneration, (ii) aggregating diverse opinion information to uncover the common\nopinion towards the given question. In this paper, we tackle opinion-aware\nanswer generation by jointly learning answer generation and opinion mining\ntasks with a unified model. Two kinds of opinion fusion strategies, namely,\nstatic and dynamic fusion, are proposed to distill and aggregate important\nopinion information learned from the opinion mining task into the answer\ngeneration process. Then a multi-view pointer-generator network is employed to\ngenerate opinion-aware answers for a given product-related question.\nExperimental results show that our method achieves superior performance in\nreal-world E-Commerce QA datasets, and effectively generate opinionated and\ninformative answers.", "published": "2020-08-27 07:54:45", "link": "http://arxiv.org/abs/2008.11972v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Neural Generation Meets Real People: Towards Emotionally Engaging\n  Mixed-Initiative Conversations", "abstract": "We present Chirpy Cardinal, an open-domain dialogue agent, as a research\nplatform for the 2019 Alexa Prize competition. Building an open-domain\nsocialbot that talks to real people is challenging - such a system must meet\nmultiple user expectations such as broad world knowledge, conversational style,\nand emotional connection. Our socialbot engages users on their terms -\nprioritizing their interests, feelings and autonomy. As a result, our socialbot\nprovides a responsive, personalized user experience, capable of talking\nknowledgeably about a wide variety of topics, as well as chatting\nempathetically about ordinary life. Neural generation plays a key role in\nachieving these goals, providing the backbone for our conversational and\nemotional tone. At the end of the competition, Chirpy Cardinal progressed to\nthe finals with an average rating of 3.6/5.0, a median conversation duration of\n2 minutes 16 seconds, and a 90th percentile duration of over 12 minutes.", "published": "2020-08-27 19:37:27", "link": "http://arxiv.org/abs/2008.12348v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DAVE: Deriving Automatically Verilog from English", "abstract": "While specifications for digital systems are provided in natural language,\nengineers undertake significant efforts to translate them into the programming\nlanguages understood by compilers for digital systems. Automating this process\nallows designers to work with the language in which they are most comfortable\n--the original natural language -- and focus instead on other downstream design\nchallenges. We explore the use of state-of-the-art machine learning (ML) to\nautomatically derive Verilog snippets from English via fine-tuning GPT-2, a\nnatural language ML system. We describe our approach for producing a suitable\ndataset of novice-level digital design tasks and provide a detailed exploration\nof GPT-2, finding encouraging translation performance across our task sets\n(94.8% correct), with the ability to handle both simple and abstract design\ntasks.", "published": "2020-08-27 15:25:03", "link": "http://arxiv.org/abs/2009.01026v1", "categories": ["cs.SE", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.SE"}
{"title": "Repurposing TREC-COVID Annotations to Answer the Key Questions of\n  CORD-19", "abstract": "The novel coronavirus disease 2019 (COVID-19) began in Wuhan, China in late\n2019 and to date has infected over 14M people worldwide, resulting in over\n750,000 deaths. On March 10, 2020 the World Health Organization (WHO) declared\nthe outbreak a global pandemic. Many academics and researchers, not restricted\nto the medical domain, began publishing papers describing new discoveries.\nHowever, with the large influx of publications, it was hard for these\nindividuals to sift through the large amount of data and make sense of the\nfindings. The White House and a group of industry research labs, lead by the\nAllen Institute for AI, aggregated over 200,000 journal articles related to a\nvariety of coronaviruses and tasked the community with answering key questions\nrelated to the corpus, releasing the dataset as CORD-19. The information\nretrieval (IR) community repurposed the journal articles within CORD-19 to more\nclosely resemble a classic TREC-style competition, dubbed TREC-COVID, with\nhuman annotators providing relevancy judgements at the end of each round of\ncompetition. Seeing the related endeavors, we set out to repurpose the\nrelevancy annotations for TREC-COVID tasks to identify journal articles in\nCORD-19 which are relevant to the key questions posed by CORD-19. A BioBERT\nmodel trained on this repurposed dataset prescribes relevancy annotations for\nCORD-19 tasks that have an overall agreement of 0.4430 with majority human\nannotations in terms of Cohen's kappa. We present the methodology used to\nconstruct the new dataset and describe the decision process used throughout.", "published": "2020-08-27 19:51:07", "link": "http://arxiv.org/abs/2008.12353v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Noise Embedding: Noise Aware Training and Adaptation for Speech\n  Enhancement", "abstract": "Estimating noise information exactly is crucial for noise aware training in\nspeech applications including speech enhancement (SE) which is our focus in\nthis paper. To estimate noise-only frames, we employ voice activity detection\n(VAD) to detect non-speech frames by applying optimal threshold on speech\nposterior. Here, the non-speech frames can be regarded as noise-only frames in\nnoisy signal. These estimated frames are used to extract noise embedding, named\ndynamic noise embedding (DNE), which is useful for an SE module to capture the\ncharacteristic of background noise. The DNE is extracted by a simple neural\nnetwork, and the SE module with the DNE can be jointly trained to be adaptive\nto the environment. Experiments are conducted on TIMIT dataset for\nsingle-channel denoising task and U-Net is used as a backbone SE module.\nExperimental results show that the DNE plays an important role in the SE module\nby increasing the quality and the intelligibility of corrupted signal even if\nthe noise is non-stationary and unseen in training. In addition, we demonstrate\nthat the DNE can be flexibly applied to other neural network-based SE modules.", "published": "2020-08-27 05:45:46", "link": "http://arxiv.org/abs/2008.11920v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-end Music-mixed Speech Recognition", "abstract": "Automatic speech recognition (ASR) in multimedia content is one of the\npromising applications, but speech data in this kind of content are frequently\nmixed with background music, which is harmful for the performance of ASR. In\nthis study, we propose a method for improving ASR with background music based\non time-domain source separation. We utilize Conv-TasNet as a separation\nnetwork, which has achieved state-of-the-art performance for multi-speaker\nsource separation, to extract the speech signal from a speech-music mixture in\nthe waveform domain. We also propose joint fine-tuning of a pre-trained\nConv-TasNet front-end with an attention-based ASR back-end using both\nseparation and ASR objectives. We evaluated our method through ASR experiments\nusing speech data mixed with background music from a wide variety of Japanese\nanimations. We show that time-domain speech-music separation drastically\nimproves ASR performance of the back-end model trained with mixture data, and\nthe joint optimization yielded a further significant WER reduction. The\ntime-domain separation method outperformed a frequency-domain separation\nmethod, which reuses the phase information of the input mixture signal, both in\nsimple cascading and joint training settings. We also demonstrate that our\nmethod works robustly for music interference from classical, jazz and popular\ngenres.", "published": "2020-08-27 10:51:26", "link": "http://arxiv.org/abs/2008.12048v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Length- and Noise-aware Training Techniques for Short-utterance Speaker\n  Recognition", "abstract": "Speaker recognition performance has been greatly improved with the emergence\nof deep learning. Deep neural networks show the capacity to effectively deal\nwith impacts of noise and reverberation, making them attractive to far-field\nspeaker recognition systems. The x-vector framework is a popular choice for\ngenerating speaker embeddings in recent literature due to its robust training\nmechanism and excellent performance in various test sets. In this paper, we\nstart with early work on including invariant representation learning (IRL) to\nthe loss function and modify the approach with centroid alignment (CA) and\nlength variability cost (LVC) techniques to further improve robustness in\nnoisy, far-field applications. This work mainly focuses on improvements for\nshort-duration test utterances (1-8s). We also present improved results on\nlong-duration tasks. In addition, this work discusses a novel self-attention\nmechanism. On the VOiCES far-field corpus, the combination of the proposed\ntechniques achieves relative improvements of 7.0% for extremely short and 8.2%\nfor full-duration test utterances on equal error rate (EER) over our baseline\nsystem.", "published": "2020-08-27 16:15:52", "link": "http://arxiv.org/abs/2008.12218v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech Sentiment and Customer Satisfaction Estimation in Socialbot\n  Conversations", "abstract": "For an interactive agent, such as task-oriented spoken dialog systems or\nchatbots, measuring and adapting to Customer Satisfaction (CSAT) is critical in\norder to understand user perception of an agent's behavior and increase user\nengagement and retention. However, an agent often relies on explicit customer\nfeedback for measuring CSAT. Such explicit feedback may result in potential\ndistraction to users and it can be challenging to capture continuously changing\nuser's satisfaction. To address this challenge, we present a new approach to\nautomatically estimate CSAT using acoustic and lexical information in the Alexa\nPrize Socialbot data. We first explore the relationship between CSAT and\nsentiment scores at both the utterance and conversation level. We then\ninvestigate static and temporal modeling methods that use estimated sentiment\nscores as a mid-level representation. The results show that the sentiment\nscores, particularly valence and satisfaction, are correlated with CSAT. We\nalso demonstrate that our proposed temporal modeling approach for estimating\nCSAT achieves competitive performance, relative to static baselines as well as\nhuman performance. This work provides insights into open domain social\nconversations between real users and socialbots, and the use of both acoustic\nand lexical information for understanding the relationship between CSAT and\nsentiment scores.", "published": "2020-08-27 21:34:53", "link": "http://arxiv.org/abs/2008.12376v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Estimating Uniqueness of I-Vector Representation of Human Voice", "abstract": "We study the individuality of the human voice with respect to a widely used\nfeature representation of speech utterances, namely, the i-vector model. As a\nfirst step toward this goal, we compare and contrast uniqueness measures\nproposed for different biometric modalities. Then, we introduce a new\nuniqueness measure that evaluates the entropy of i-vectors while taking into\naccount speaker level variations. Our measure operates in the discrete feature\nspace and relies on accurate estimation of the distribution of i-vectors.\nTherefore, i-vectors are quantized while ensuring that both the quantized and\noriginal representations yield similar speaker verification performance.\nUniqueness estimates are obtained from two newly generated datasets and the\npublic VoxCeleb dataset. The first custom dataset contains more than one and a\nhalf million speech samples of 20,741 speakers obtained from TEDx Talks videos.\nThe second one includes over twenty one thousand speech samples from 1,595\nactors that are extracted from movie dialogues. Using this data, we analyzed\nhow several factors, such as the number of speakers, number of samples per\nspeaker, sample durations, and diversity of utterances affect uniqueness\nestimates. Most notably, we determine that the discretization of i-vectors does\nnot cause a reduction in speaker recognition performance. Our results show that\nthe degree of distinctiveness offered by i-vector-based representation may\nreach 43-70 bits considering 5-second long speech samples; however, under less\nconstrained variations in speech, uniqueness estimates are found to reduce by\naround 30 bits. We also find that doubling the sample duration increases the\ndistinctiveness of the i-vector representation by around 20 bits.", "published": "2020-08-27 08:28:42", "link": "http://arxiv.org/abs/2008.11985v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DrumGAN: Synthesis of Drum Sounds With Timbral Feature Conditioning\n  Using Generative Adversarial Networks", "abstract": "Synthetic creation of drum sounds (e.g., in drum machines) is commonly\nperformed using analog or digital synthesis, allowing a musician to sculpt the\ndesired timbre modifying various parameters. Typically, such parameters control\nlow-level features of the sound and often have no musical meaning or perceptual\ncorrespondence. With the rise of Deep Learning, data-driven processing of audio\nemerges as an alternative to traditional signal processing. This new paradigm\nallows controlling the synthesis process through learned high-level features or\nby conditioning a model on musically relevant information. In this paper, we\napply a Generative Adversarial Network to the task of audio synthesis of drum\nsounds. By conditioning the model on perceptual features computed with a\npublicly available feature-extractor, intuitive control is gained over the\ngeneration process. The experiments are carried out on a large collection of\nkick, snare, and cymbal sounds. We show that, compared to a specific prior work\nbased on a U-Net architecture, our approach considerably improves the quality\nof the generated drum samples, and that the conditional input indeed shapes the\nperceptual characteristics of the sounds. Also, we provide audio examples and\nrelease the code used in our experiments.", "published": "2020-08-27 12:06:30", "link": "http://arxiv.org/abs/2008.12073v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Listener-Position and Orientation Dependency of Auditory Perception in\n  an Enclosed Space: Elicitation of Salient Attributes", "abstract": "This paper presents a subjective study conducted on the perception of salient\nauditory attributes depending on the listener's position and head orientations\nin an enclosed space. Two elicitation experiments were carried out using the\nRepertory Grid Technique; in-situ and laboratory experiments aimed to identify\nperceptual attributes among ten different combinations of the listener's\npositions and head orientations in a concert hall. Results revealed that,\nbetween the in-situ and laboratory experiments, the listening positions and\nhead orientations were clustered identically. Ten salient perceptual attributes\nwere identified from the data obtained from the laboratory experiment. Whilst\nthese included conventional attributes such as ASW (apparent source width) and\nLEV (listener envelopment), new attributes such as PRL (perceived reverb\nloudness), ARW (apparent reverb width) and Reverb Direction were identified,\nand they are hypothesised to be sub-attributes of LEV (listener envelopment).\nTimbral characteristics such as Reverb Brightness and Echo Brightness were also\nidentified as salient attributes, which are considered to potentially\ncontribute to the overall sound clarity.", "published": "2020-08-27 16:50:54", "link": "http://arxiv.org/abs/2008.12255v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Nonparallel Voice Conversion with Augmented Classifier Star Generative\n  Adversarial Networks", "abstract": "We previously proposed a method that allows for nonparallel voice conversion\n(VC) by using a variant of generative adversarial networks (GANs) called\nStarGAN. The main features of our method, called StarGAN-VC, are as follows:\nFirst, it requires no parallel utterances, transcriptions, or time alignment\nprocedures for speech generator training. Second, it can simultaneously learn\nmappings across multiple domains using a single generator network and thus\nfully exploit available training data collected from multiple domains to\ncapture latent features that are common to all the domains. Third, it can\ngenerate converted speech signals quickly enough to allow real-time\nimplementations and requires only several minutes of training examples to\ngenerate reasonably realistic-sounding speech. In this paper, we describe three\nformulations of StarGAN, including a newly introduced novel StarGAN variant\ncalled \"Augmented classifier StarGAN (A-StarGAN)\", and compare them in a\nnonparallel VC task. We also compare them with several baseline methods.", "published": "2020-08-27 10:30:05", "link": "http://arxiv.org/abs/2008.12604v7", "categories": ["eess.AS", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Exploring British Accents: Modelling the Trap-Bath Split with Functional\n  Data Analysis", "abstract": "The sound of our speech is influenced by the places we come from. Great\nBritain contains a wide variety of distinctive accents which are of interest to\nlinguistics. In particular, the \"a\" vowel in words like \"class\" is pronounced\ndifferently in the North and the South. Speech recordings of this vowel can be\nrepresented as formant curves or as Mel-frequency cepstral coefficient curves.\nFunctional data analysis and generalized additive models offer techniques to\nmodel the variation in these curves. Our first aim is to model the difference\nbetween typical Northern and Southern vowels /ae/ and /a/, by training two\nclassifiers on the North-South Class Vowels dataset collected for this paper\n(Koshy 2020). Our second aim is to visualize geographical variation of accents\nin Great Britain. For this we use speech recordings from a second dataset, the\nBritish National Corpus (BNC) audio edition (Coleman et al. 2012). The trained\nmodels are used to predict the accent of speakers in the BNC, and then we model\nthe geographical patterns in these predictions using a soap film smoother. This\nwork demonstrates a flexible and interpretable approach to modeling phonetic\naccent variation in speech recordings.", "published": "2020-08-27 16:29:50", "link": "http://arxiv.org/abs/2008.12233v2", "categories": ["cs.SD", "eess.AS", "stat.AP", "62P25, 62R10, 62G08, 62J12, 62H25"], "primary_category": "cs.SD"}
