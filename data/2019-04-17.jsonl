{"title": "End-to-End Speech Translation with Knowledge Distillation", "abstract": "End-to-end speech translation (ST), which directly translates from source\nlanguage speech into target language text, has attracted intensive attentions\nin recent years. Compared to conventional pipeline systems, end-to-end ST\nmodels have advantages of lower latency, smaller model size and less error\npropagation. However, the combination of speech recognition and text\ntranslation in one model is more difficult than each of these two tasks. In\nthis paper, we propose a knowledge distillation approach to improve ST model by\ntransferring the knowledge from text translation model. Specifically, we first\ntrain a text translation model, regarded as a teacher model, and then ST model\nis trained to learn output probabilities from teacher model through knowledge\ndistillation. Experiments on English- French Augmented LibriSpeech and\nEnglish-Chinese TED corpus show that end-to-end ST is possible to implement on\nboth similar and dissimilar language pairs. In addition, with the instruction\nof teacher model, end-to-end ST model can gain significant improvements by over\n3.5 BLEU points.", "published": "2019-04-17 04:00:52", "link": "http://arxiv.org/abs/1904.08075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Amobee at SemEval-2019 Tasks 5 and 6: Multiple Choice CNN Over\n  Contextual Embedding", "abstract": "This article describes Amobee's participation in \"HatEval: Multilingual\ndetection of hate speech against immigrants and women in Twitter\" (task 5) and\n\"OffensEval: Identifying and Categorizing Offensive Language in Social Media\"\n(task 6). The goal of task 5 was to detect hate speech targeted to women and\nimmigrants. The goal of task 6 was to identify and categorized offensive\nlanguage in social media, and identify offense target. We present a novel type\nof convolutional neural network called \"Multiple Choice CNN\" (MC-CNN) that we\nused over our newly developed contextual embedding, Rozental et al. (2019). For\nboth tasks we used this architecture and achieved 4th place out of 69\nparticipants with an F1 score of 0.53 in task 5, in task 6 achieved 2nd place\n(out of 75) in Sub-task B - automatic categorization of offense types (our\nmodel reached places 18/2/7 out of 103/75/65 for sub-tasks A, B and C\nrespectively in task 6).", "published": "2019-04-17 14:34:12", "link": "http://arxiv.org/abs/1904.08292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Accuracy Prediction for AMR Parsing", "abstract": "Abstract Meaning Representation (AMR) represents sentences as directed,\nacyclic and rooted graphs, aiming at capturing their meaning in a machine\nreadable format. AMR parsing converts natural language sentences into such\ngraphs. However, evaluating a parser on new data by means of comparison to\nmanually created AMR graphs is very costly. Also, we would like to be able to\ndetect parses of questionable quality, or preferring results of alternative\nsystems by selecting the ones for which we can assess good quality. We propose\nAMR accuracy prediction as the task of predicting several metrics of\ncorrectness for an automatically generated AMR parse - in absence of the\ncorresponding gold parse. We develop a neural end-to-end multi-output\nregression model and perform three case studies: firstly, we evaluate the\nmodel's capacity of predicting AMR parse accuracies and test whether it can\nreliably assign high scores to gold parses. Secondly, we perform parse\nselection based on predicted parse accuracies of candidate parses from\nalternative systems, with the aim of improving overall results. Finally, we\npredict system ranks for submissions from two AMR shared tasks on the basis of\ntheir predicted parse accuracy averages. All experiments are carried out across\ntwo different domains and show that our method is effective.", "published": "2019-04-17 14:59:45", "link": "http://arxiv.org/abs/1904.08301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocBERT: BERT for Document Classification", "abstract": "We present, to our knowledge, the first application of BERT to document\nclassification. A few characteristics of the task might lead one to think that\nBERT is not the most appropriate model: syntactic structures matter less for\ncontent categories, documents can often be longer than typical BERT input, and\ndocuments often have multiple labels. Nevertheless, we show that a\nstraightforward classification model using BERT is able to achieve the state of\nthe art across four popular datasets. To address the computational expense\nassociated with BERT inference, we distill knowledge from BERT-large to small\nbidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x\nfewer parameters. The primary contribution of our paper is improved baselines\nthat can provide the foundation for future work.", "published": "2019-04-17 17:55:18", "link": "http://arxiv.org/abs/1904.08398v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Headline Generation: Learning from Decomposable Document Titles", "abstract": "We propose a novel method for generating titles for unstructured text\ndocuments. We reframe the problem as a sequential question-answering task. A\ndeep neural network is trained on document-title pairs with decomposable\ntitles, meaning that the vocabulary of the title is a subset of the vocabulary\nof the document. To train the model we use a corpus of millions of publicly\navailable document-title pairs: news articles and headlines. We present the\nresults of a randomized double-blind trial in which subjects were unaware of\nwhich titles were human or machine-generated. When trained on approximately 1.5\nmillion news articles, the model generates headlines that humans judge to be as\ngood or better than the original human-written headlines in the majority of\ncases.", "published": "2019-04-17 19:03:07", "link": "http://arxiv.org/abs/1904.08455v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Homonym per Translation", "abstract": "The study of homonymy is vital to resolving fundamental problems in lexical\nsemantics. In this paper, we propose four hypotheses that characterize the\nunique behavior of homonyms in the context of translations, discourses,\ncollocations, and sense clusters. We present a new annotated homonym resource\nthat allows us to test our hypotheses on existing WSD resources. The results of\nthe experiments provide strong empirical evidence for the hypotheses. This\nstudy represents a step towards a computational method for distinguishing\nbetween homonymy and polysemy, and constructing a definitive inventory of\ncoarse-grained senses.", "published": "2019-04-17 23:19:25", "link": "http://arxiv.org/abs/1904.08533v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Constituency Parsing of Speech Transcripts", "abstract": "This paper studies the performance of a neural self-attentive parser on\ntranscribed speech. Speech presents parsing challenges that do not appear in\nwritten text, such as the lack of punctuation and the presence of speech\ndisfluencies (including filled pauses, repetitions, corrections, etc.).\nDisfluencies are especially problematic for conventional syntactic parsers,\nwhich typically fail to find any EDITED disfluency nodes at all. This motivated\nthe development of special disfluency detection systems, and special mechanisms\nadded to parsers specifically to handle disfluencies. However, we show here\nthat neural parsers can find EDITED disfluency nodes, and the best neural\nparsers find them with an accuracy surpassing that of specialized disfluency\ndetection systems, thus making these specialized mechanisms unnecessary. This\npaper also investigates a modified loss function that puts more weight on\nEDITED nodes. It also describes tree-transformations that simplify the\ndisfluency detection task by providing alternative encodings of disfluencies\nand syntactic information.", "published": "2019-04-17 23:30:17", "link": "http://arxiv.org/abs/1904.08535v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CraftAssist Instruction Parsing: Semantic Parsing for a Minecraft\n  Assistant", "abstract": "We propose a large scale semantic parsing dataset focused on\ninstruction-driven communication with an agent in Minecraft. We describe the\ndata collection process which yields additional 35K human generated\ninstructions with their semantic annotations. We report the performance of\nthree baseline models and find that while a dataset of this size helps us train\na usable instruction parser, it still poses interesting generalization\nchallenges which we hope will help develop better and more robust models.", "published": "2019-04-17 19:55:20", "link": "http://arxiv.org/abs/1905.01978v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Posterior-regularized REINFORCE for Instance Selection in Distant\n  Supervision", "abstract": "This paper provides a new way to improve the efficiency of the REINFORCE\ntraining process. We apply it to the task of instance selection in distant\nsupervision. Modeling the instance selection in one bag as a sequential\ndecision process, a reinforcement learning agent is trained to determine\nwhether an instance is valuable or not and construct a new bag with less noisy\ninstances. However unbiased methods, such as REINFORCE, could usually take much\ntime to train. This paper adopts posterior regularization (PR) to integrate\nsome domain-specific rules in instance selection using REINFORCE. As the\nexperiment results show, this method remarkably improves the performance of the\nrelation classifier trained on cleaned distant supervision dataset as well as\nthe efficiency of the REINFORCE training.", "published": "2019-04-17 02:21:51", "link": "http://arxiv.org/abs/1904.08051v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning Based Emotional Editing Constraint Conversation\n  Generation", "abstract": "In recent years, the generation of conversation content based on deep neural\nnetworks has attracted many researchers. However, traditional neural language\nmodels tend to generate general replies, lacking logical and emotional factors.\nThis paper proposes a conversation content generation model that combines\nreinforcement learning with emotional editing constraints to generate more\nmeaningful and customizable emotional replies. The model divides the replies\ninto three clauses based on pre-generated keywords and uses the emotional\neditor to further optimize the final reply. The model combines multi-task\nlearning with multiple indicator rewards to comprehensively optimize the\nquality of replies. Experiments shows that our model can not only improve the\nfluency of the replies, but also significantly enhance the logical relevance\nand emotional relevance of the replies.", "published": "2019-04-17 03:01:16", "link": "http://arxiv.org/abs/1904.08061v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Patent Analytics Based on Feature Vector Space Model: A Case of IoT", "abstract": "The number of approved patents worldwide increases rapidly each year, which\nrequires new patent analytics to efficiently mine the valuable information\nattached to these patents. Vector space model (VSM) represents documents as\nhigh-dimensional vectors, where each dimension corresponds to a unique term.\nWhile originally proposed for information retrieval systems, VSM has also seen\nwide applications in patent analytics, and used as a fundamental tool to map\npatent documents to structured data. However, VSM method suffers from several\nlimitations when applied to patent analysis tasks, such as loss of\nsentence-level semantics and curse-of-dimensionality problems. In order to\naddress the above limitations, we propose a patent analytics based on feature\nvector space model (FVSM), where the FVSM is constructed by mapping patent\ndocuments to feature vectors extracted by convolutional neural networks (CNN).\nThe applications of FVSM for three typical patent analysis tasks, i.e., patents\nsimilarity comparison, patent clustering, and patent map generation are\ndiscussed. A case study using patents related to Internet of Things (IoT)\ntechnology is illustrated to demonstrate the performance and effectiveness of\nFVSM. The proposed FVSM can be adopted by other patent analysis studies to\nreplace VSM, based on which various big data learning tasks can be performed.", "published": "2019-04-17 06:20:53", "link": "http://arxiv.org/abs/1904.08100v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Contextual Aware Joint Probability Model Towards Question Answering\n  System", "abstract": "In this paper, we address the question answering challenge with the SQuAD 2.0\ndataset. We design a model architecture which leverages BERT's capability of\ncontext-aware word embeddings and BiDAF's context interactive exploration\nmechanism. By integrating these two state-of-the-art architectures, our system\ntries to extract the contextual word representation at word and character\nlevels, for better comprehension of both question and context and their\ncorrelations. We also propose our original joint posterior probability\npredictor module and its associated loss functions. Our best model so far\nobtains F1 score of 75.842% and EM score of 72.24% on the test PCE leaderboad.", "published": "2019-04-17 07:16:10", "link": "http://arxiv.org/abs/1904.08109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effective Estimation of Deep Generative Language Models", "abstract": "Advances in variational inference enable parameterisation of probabilistic\nmodels by deep neural networks. This combines the statistical transparency of\nthe probabilistic modelling framework with the representational power of deep\nlearning. Yet, due to a problem known as posterior collapse, it is difficult to\nestimate such models in the context of language modelling effectively. We\nconcentrate on one such model, the variational auto-encoder, which we argue is\nan important building block in hierarchical probabilistic models of language.\nThis paper contributes a sober view of the problem, a survey of techniques to\naddress it, novel techniques, and extensions to the model. To establish a\nranking of techniques, we perform a systematic comparison using Bayesian\noptimisation and find that many techniques perform reasonably similar, given\nenough resources. Still, a favourite can be named based on convenience. We also\nmake several empirical observations and recommendations of best practices that\nshould help researchers interested in this exciting field.", "published": "2019-04-17 11:24:58", "link": "http://arxiv.org/abs/1904.08194v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and\n  Knowledge Distillation", "abstract": "Conventional automatic speech recognition (ASR) systems trained from\nframe-level alignments can easily leverage posterior fusion to improve ASR\naccuracy and build a better single model with knowledge distillation.\nEnd-to-end ASR systems trained using the Connectionist Temporal Classification\n(CTC) loss do not require frame-level alignment and hence simplify model\ntraining. However, sparse and arbitrary posterior spike timings from CTC models\npose a new set of challenges in posterior fusion from multiple models and\nknowledge distillation between CTC models. We propose a method to train a CTC\nmodel so that its spike timings are guided to align with those of a pre-trained\nguiding CTC model. As a result, all models that share the same guiding model\nhave aligned spike timings. We show the advantage of our method in various\nscenarios including posterior fusion of CTC models and knowledge distillation\nbetween CTC models with different architectures. With the 300-hour Switchboard\ntraining data, the single word CTC model distilled from multiple models\nimproved the word error rates to 13.7%/23.1% from 14.9%/24.1% on the Hub5 2000\nSwitchboard/CallHome test sets without using any data augmentation, language\nmodel, or complex decoder.", "published": "2019-04-17 15:18:23", "link": "http://arxiv.org/abs/1904.08311v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoralStrength: Exploiting a Moral Lexicon and Embedding Similarity for\n  Moral Foundations Prediction", "abstract": "Moral rhetoric plays a fundamental role in how we perceive and interpret the\ninformation we receive, greatly influencing our decision-making process.\nEspecially when it comes to controversial social and political issues, our\nopinions and attitudes are hardly ever based on evidence alone. The Moral\nFoundations Dictionary (MFD) was developed to operationalize moral values in\nthe text. In this study, we present MoralStrength, a lexicon of approximately\n1,000 lemmas, obtained as an extension of the Moral Foundations Dictionary,\nbased on WordNet synsets. Moreover, for each lemma it provides with a\ncrowdsourced numeric assessment of Moral Valence, indicating the strength with\nwhich a lemma is expressing the specific value. We evaluated the predictive\npotentials of this moral lexicon, defining three utilization approaches of\nincreased complexity, ranging from lemmas' statistical properties to a deep\nlearning approach of word embeddings based on semantic similarity. Logistic\nregression models trained on the features extracted from MoralStrength,\nsignificantly outperformed the current state-of-the-art, reaching an F1-score\nof 87.6% over the previous 62.4% (p-value<0.01), and an average F1-Score of\n86.25% over six different datasets. Such findings pave the way for further\nresearch, allowing for an in-depth understanding of moral narratives in text\nfor a wide range of social issues.", "published": "2019-04-17 15:21:33", "link": "http://arxiv.org/abs/1904.08314v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Towards Open Intent Discovery for Conversational Text", "abstract": "Detecting and identifying user intent from text, both written and spoken,\nplays an important role in modelling and understand dialogs. Existing research\nfor intent discovery model it as a classification task with a predefined set of\nknown categories. To generailze beyond these preexisting classes, we define a\nnew task of \\textit{open intent discovery}. We investigate how intent can be\ngeneralized to those not seen during training. To this end, we propose a\ntwo-stage approach to this task - predicting whether an utterance contains an\nintent, and then tagging the intent in the input utterance. Our model consists\nof a bidirectional LSTM with a CRF on top to capture contextual semantics,\nsubject to some constraints. Self-attention is used to learn long distance\ndependencies. Further, we adapt an adversarial training approach to improve\nrobustness and perforamce across domains. We also present a dataset of 25k\nreal-life utterances that have been labelled via crowd sourcing. Our\nexperiments across different domains and real-world datasets show the\neffectiveness of our approach, with less than 100 annotated examples needed per\nunique domain to recognize diverse intents. The approach outperforms\nstate-of-the-art baselines by 5-15% F1 score points.", "published": "2019-04-17 22:40:01", "link": "http://arxiv.org/abs/1904.08524v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "True Parallel Graph Transformations: an Algebraic Approach Based on Weak\n  Spans", "abstract": "We address the problem of defining graph transformations by the simultaneous\napplication of direct transformations even when these cannot be applied\nindependently of each other. An algebraic approach is adopted, with production\nrules of the form $L\\xleftarrow{l}K \\xleftarrow{i} I \\xrightarrow{r} R$, called\nweak spans. A parallel coherent transformation is introduced and shown to be a\nconservative extension of the interleaving semantics of parallel independent\ndirect transformations. A categorical construction of finitely attributed\nstructures is proposed, in which parallel coherent transformations can be built\nin a natural way. These notions are introduced and illustrated on detailed\nexamples.", "published": "2019-04-17 14:21:43", "link": "http://arxiv.org/abs/1904.08850v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment\n  Analysis", "abstract": "Sentiment analysis, mostly based on text, has been rapidly developing in the\nlast decade and has attracted widespread attention in both academia and\nindustry. However, the information in the real world usually comes from\nmultiple modalities, such as audio and text. Therefore, in this paper, based on\naudio and text, we consider the task of multimodal sentiment analysis and\npropose a novel fusion strategy including both multi-feature fusion and\nmulti-modality fusion to improve the accuracy of audio-text sentiment analysis.\nWe call it the DFF-ATMF (Deep Feature Fusion - Audio and Text Modality Fusion)\nmodel, which consists of two parallel branches, the audio modality based branch\nand the text modality based branch. Its core mechanisms are the fusion of\nmultiple feature vectors and multiple modality attention. Experiments on the\nCMU-MOSI dataset and the recently released CMU-MOSEI dataset, both collected\nfrom YouTube for sentiment analysis, show the very competitive results of our\nDFF-ATMF model. Furthermore, by virtue of attention weight distribution\nheatmaps, we also demonstrate the deep features learned by using DFF-ATMF are\ncomplementary to each other and robust. Surprisingly, DFF-ATMF also achieves\nnew state-of-the-art results on the IEMOCAP dataset, indicating that the\nproposed fusion strategy also has a good generalization ability for multimodal\nemotion recognition.", "published": "2019-04-17 08:46:53", "link": "http://arxiv.org/abs/1904.08138v5", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Casting Light on Invisible Cities: Computationally Engaging with\n  Literary Criticism", "abstract": "Literary critics often attempt to uncover meaning in a single work of\nliterature through careful reading and analysis. Applying natural language\nprocessing methods to aid in such literary analyses remains a challenge in\ndigital humanities. While most previous work focuses on \"distant reading\" by\nalgorithmically discovering high-level patterns from large collections of\nliterary works, here we sharpen the focus of our methods to a single literary\ntheory about Italo Calvino's postmodern novel Invisible Cities, which consists\nof 55 short descriptions of imaginary cities. Calvino has provided a\nclassification of these cities into eleven thematic groups, but literary\nscholars disagree as to how trustworthy his categorization is. Due to the\nunique structure of this novel, we can computationally weigh in on this debate:\nwe leverage pretrained contextualized representations to embed each city's\ndescription and use unsupervised methods to cluster these embeddings.\nAdditionally, we compare results of our computational approach to similarity\njudgments generated by human readers. Our work is a first step towards\nincorporating natural language processing into literary criticism.", "published": "2019-04-17 17:37:33", "link": "http://arxiv.org/abs/1904.08386v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Text Classification Algorithms: A Survey", "abstract": "In recent years, there has been an exponential growth in the number of\ncomplex documents and texts that require a deeper understanding of machine\nlearning methods to be able to accurately classify texts in many applications.\nMany machine learning approaches have achieved surpassing results in natural\nlanguage processing. The success of these learning algorithms relies on their\ncapacity to understand complex models and non-linear relationships within data.\nHowever, finding suitable structures, architectures, and techniques for text\nclassification is a challenge for researchers. In this paper, a brief overview\nof text classification algorithms is discussed. This overview covers different\ntext feature extractions, dimensionality reduction methods, existing algorithms\nand techniques, and evaluations methods. Finally, the limitations of each\ntechnique and their application in the real-world problem are discussed.", "published": "2019-04-17 03:29:05", "link": "http://arxiv.org/abs/1904.08067v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Filtering: Signal Extraction and Reconstruction Using Complex\n  Time-Frequency Filters", "abstract": "Signal extraction from a single-channel mixture with additional undesired\nsignals is most commonly performed using time-frequency (TF) masks. Typically,\nthe mask is estimated with a deep neural network (DNN), and element-wise\napplied to the complex mixture short-time Fourier transform (STFT)\nrepresentation to perform the extraction. Ideal mask magnitudes are zero for\nsolely undesired signals in a TF bin and undefined for total destructive\ninterference. Usually, masks have an upper bound to provide well-defined DNN\noutputs at the cost of limited extraction capabilities. We propose to estimate\nwith a DNN a complex TF filter for each mixture TF bin which maps an STFT area\nin the respective mixture to the desired TF bin to address destructive\ninterference in mixture TF bins. The DNN is optimized by minimizing the error\nbetween the extracted and the ground-truth desired signal allowing to learn the\nTF filters without having to specify ground-truth TF filters. We compare our\napproach with complex and real-valued TF masks by separating speech from a\nvariety of different sound and noise classes from the Google AudioSet corpus.\nWe also process the mixture STFT with notch-filters and zero whole time-frames,\nto simulate packet-loss during transmission, to demonstrate the reconstruction\ncapabilities of our approach. The proposed method outperformed the baselines,\nespecially when notch-filters and time-frame zeroing were applied.", "published": "2019-04-17 17:10:10", "link": "http://arxiv.org/abs/1904.08369v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hard Sample Mining for the Improved Retraining of Automatic Speech\n  Recognition", "abstract": "It is an effective way that improves the performance of the existing\nAutomatic Speech Recognition (ASR) systems by retraining with more and more new\ntraining data in the target domain. Recently, Deep Neural Network (DNN) has\nbecome a successful model in the ASR field. In the training process of the DNN\nbased methods, a back propagation of error between the transcription and the\ncorresponding annotated text is used to update and optimize the parameters.\nThus, the parameters are more influenced by the training samples with a big\npropagation error than the samples with a small one. In this paper, we define\nthe samples with significant error as the hard samples and try to improve the\nperformance of the ASR system by adding many of them. Unfortunately, the hard\nsamples are sparse in the training data of the target domain, and manually\nlabel them is expensive. Therefore, we propose a hard samples mining method\nbased on an enhanced deep multiple instance learning, which can find the hard\nsamples from unlabeled training data by using a small subset of the dataset\nwith manual labeling in the target domain. We applied our method to an End2End\nASR task and obtained the best performance.", "published": "2019-04-17 00:39:35", "link": "http://arxiv.org/abs/1904.08031v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multi-Task Learning Framework for Overcoming the Catastrophic\n  Forgetting in Automatic Speech Recognition", "abstract": "Recently, data-driven based Automatic Speech Recognition (ASR) systems have\nachieved state-of-the-art results. And transfer learning is often used when\nthose existing systems are adapted to the target domain, e.g., fine-tuning,\nretraining. However, in the processes, the system parameters may well deviate\ntoo much from the previously learned parameters. Thus, it is difficult for the\nsystem training process to learn knowledge from target domains meanwhile not\nforgetting knowledge from the previous learning process, which is called as\ncatastrophic forgetting (CF). In this paper, we attempt to solve the CF problem\nwith the lifelong learning and propose a novel multi-task learning (MTL)\ntraining framework for ASR. It considers reserving original knowledge and\nlearning new knowledge as two independent tasks, respectively. On the one hand,\nwe constrain the new parameters not to deviate too far from the original\nparameters and punish the new system when forgetting original knowledge. On the\nother hand, we force the new system to solve new knowledge quickly. Then, a MTL\nmechanism is employed to get the balance between the two tasks. We applied our\nmethod to an End2End ASR task and obtained the best performance in both target\nand original datasets.", "published": "2019-04-17 00:55:04", "link": "http://arxiv.org/abs/1904.08039v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RawNet: Advanced end-to-end deep neural network using raw waveforms for\n  text-independent speaker verification", "abstract": "Recently, direct modeling of raw waveforms using deep neural networks has\nbeen widely studied for a number of tasks in audio domains. In speaker\nverification, however, utilization of raw waveforms is in its preliminary\nphase, requiring further investigation. In this study, we explore end-to-end\ndeep neural networks that input raw waveforms to improve various aspects:\nfront-end speaker embedding extraction including model architecture,\npre-training scheme, additional objective functions, and back-end\nclassification. Adjustment of model architecture using a pre-training scheme\ncan extract speaker embeddings, giving a significant improvement in\nperformance. Additional objective functions simplify the process of extracting\nspeaker embeddings by merging conventional two-phase processes: extracting\nutterance-level features such as i-vectors or x-vectors and the feature\nenhancement phase, e.g., linear discriminant analysis. Effective back-end\nclassification models that suit the proposed speaker embedding are also\nexplored. We propose an end-to-end system that comprises two deep neural\nnetworks, one front-end for utterance-level speaker embedding extraction and\nthe other for back-end classification. Experiments conducted on the VoxCeleb1\ndataset demonstrate that the proposed model achieves state-of-the-art\nperformance among systems without data augmentation. The proposed system is\nalso comparable to the state-of-the-art x-vector system that adopts data\naugmentation.", "published": "2019-04-17 06:37:22", "link": "http://arxiv.org/abs/1904.08104v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MOSNet: Deep Learning based Objective Assessment for Voice Conversion", "abstract": "Existing objective evaluation metrics for voice conversion (VC) are not\nalways correlated with human perception. Therefore, training VC models with\nsuch criteria may not effectively improve naturalness and similarity of\nconverted speech. In this paper, we propose deep learning-based assessment\nmodels to predict human ratings of converted speech. We adopt the convolutional\nand recurrent neural network models to build a mean opinion score (MOS)\npredictor, termed as MOSNet. The proposed models are tested on large-scale\nlistening test results of the Voice Conversion Challenge (VCC) 2018.\nExperimental results show that the predicted scores of the proposed MOSNet are\nhighly correlated with human MOS ratings at the system level while being fairly\ncorrelated with human MOS ratings at the utterance level. Meanwhile, we have\nmodified MOSNet to predict the similarity scores, and the preliminary results\nshow that the predicted scores are also fairly correlated with human ratings.\nThese results confirm that the proposed models could be used as a computational\nevaluator to measure the MOS of VC systems to reduce the need for expensive\nhuman rating.", "published": "2019-04-17 16:38:31", "link": "http://arxiv.org/abs/1904.08352v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Regression and Classification for Direction-of-Arrival Estimation with\n  Convolutional Recurrent Neural Networks", "abstract": "We present a novel learning-based approach to estimate the\ndirection-of-arrival (DOA) of a sound source using a convolutional recurrent\nneural network (CRNN) trained via regression on synthetic data and Cartesian\nlabels. We also describe an improved method to generate synthetic data to train\nthe neural network using state-of-the-art sound propagation algorithms that\nmodel specular as well as diffuse reflections of sound. We compare our model\nagainst three other CRNNs trained using different formulations of the same\nproblem: classification on categorical labels, and regression on spherical\ncoordinate labels. In practice, our model achieves up to 43% decrease in\nangular error over prior methods. The use of diffuse reflection results in 34%\nand 41% reduction in angular prediction errors on LOCATA and SOFA datasets,\nrespectively, over prior methods based on image-source methods. Our method\nresults in an additional 3% error reduction over prior schemes that use\nclassification based networks, and we use 36% fewer network parameters.", "published": "2019-04-17 18:49:12", "link": "http://arxiv.org/abs/1904.08452v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Few Shot Speaker Recognition using Deep Neural Networks", "abstract": "The recent advances in deep learning are mostly driven by availability of\nlarge amount of training data. However, availability of such data is not always\npossible for specific tasks such as speaker recognition where collection of\nlarge amount of data is not possible in practical scenarios. Therefore, in this\npaper, we propose to identify speakers by learning from only a few training\nexamples. To achieve this, we use a deep neural network with prototypical loss\nwhere the input to the network is a spectrogram. For output, we project the\nclass feature vectors into a common embedding space, followed by\nclassification. Further, we show the effectiveness of capsule net in a few shot\nlearning setting. To this end, we utilize an auto-encoder to learn generalized\nfeature embeddings from class-specific embeddings obtained from capsule\nnetwork. We provide exhaustive experiments on publicly available datasets and\ncompetitive baselines, demonstrating the superiority and generalization ability\nof the proposed few shot learning pipelines.", "published": "2019-04-17 09:25:02", "link": "http://arxiv.org/abs/1904.08775v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
