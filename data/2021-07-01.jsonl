{"title": "Controllable Open-ended Question Generation with A New Question Type\n  Ontology", "abstract": "We investigate the less-explored task of generating open-ended questions that\nare typically answered by multiple sentences. We first define a new question\ntype ontology which differentiates the nuanced nature of questions better than\nwidely used question words. A new dataset with 4,959 questions is labeled based\non the new ontology. We then propose a novel question type-aware question\ngeneration framework, augmented by a semantic graph representation, to jointly\npredict question focuses and produce the question. Based on this framework, we\nfurther use both exemplars and automatically generated templates to improve\ncontrollability and diversity. Experiments on two newly collected large-scale\ndatasets show that our model improves question quality over competitive\ncomparisons based on automatic metrics. Human judges also rate our model\noutputs highly in answerability, coverage of scope, and overall quality.\nFinally, our model variants with templates can produce questions with enhanced\ncontrollability and diversity.", "published": "2021-07-01 00:02:03", "link": "http://arxiv.org/abs/2107.00152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Elbert: Fast Albert with Confidence-Window Based Early Exit", "abstract": "Despite the great success in Natural Language Processing (NLP) area, large\npre-trained language models like BERT are not well-suited for\nresource-constrained or real-time applications owing to the large number of\nparameters and slow inference speed. Recently, compressing and accelerating\nBERT have become important topics. By incorporating a parameter-sharing\nstrategy, ALBERT greatly reduces the number of parameters while achieving\ncompetitive performance. Nevertheless, ALBERT still suffers from a long\ninference time. In this work, we propose the ELBERT, which significantly\nimproves the average inference speed compared to ALBERT due to the proposed\nconfidence-window based early exit mechanism, without introducing additional\nparameters or extra training overhead. Experimental results show that ELBERT\nachieves an adaptive inference speedup varying from 2$\\times$ to 10$\\times$\nwith negligible accuracy degradation compared to ALBERT on various datasets.\nBesides, ELBERT achieves higher accuracy than existing early exit methods used\nfor accelerating BERT under the same computation cost. Furthermore, to\nunderstand the principle of the early exit mechanism, we also visualize the\ndecision-making process of it in ELBERT.", "published": "2021-07-01 02:02:39", "link": "http://arxiv.org/abs/2107.00175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning for Abstractive Question Summarization with\n  Question-aware Semantic Rewards", "abstract": "The growth of online consumer health questions has led to the necessity for\nreliable and accurate question answering systems. A recent study showed that\nmanual summarization of consumer health questions brings significant\nimprovement in retrieving relevant answers. However, the automatic\nsummarization of long questions is a challenging task due to the lack of\ntraining data and the complexity of the related subtasks, such as the question\nfocus and type recognition. In this paper, we introduce a reinforcement\nlearning-based framework for abstractive question summarization. We propose two\nnovel rewards obtained from the downstream tasks of (i) question-type\nidentification and (ii) question-focus recognition to regularize the question\ngeneration model. These rewards ensure the generation of semantically valid\nquestions and encourage the inclusion of key medical entities/foci in the\nquestion summary. We evaluated our proposed method on two benchmark datasets\nand achieved higher performance over state-of-the-art models. The manual\nevaluation of the summaries reveals that the generated questions are more\ndiverse and have fewer factual inconsistencies than the baseline summaries", "published": "2021-07-01 02:06:46", "link": "http://arxiv.org/abs/2107.00176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing Event Argument Interaction via A Bi-Directional Entity-Level\n  Recurrent Decoder", "abstract": "Capturing interactions among event arguments is an essential step towards\nrobust event argument extraction (EAE). However, existing efforts in this\ndirection suffer from two limitations: 1) The argument role type information of\ncontextual entities is mainly utilized as training signals, ignoring the\npotential merits of directly adopting it as semantically rich input features;\n2) The argument-level sequential semantics, which implies the overall\ndistribution pattern of argument roles over an event mention, is not well\ncharacterized. To tackle the above two bottlenecks, we formalize EAE as a\nSeq2Seq-like learning problem for the first time, where a sentence with a\nspecific event trigger is mapped to a sequence of event argument roles. A\nneural architecture with a novel Bi-directional Entity-level Recurrent Decoder\n(BERD) is proposed to generate argument roles by incorporating contextual\nentities' argument role predictions, like a word-by-word text generation\nprocess, thereby distinguishing implicit argument distribution patterns within\nan event more accurately.", "published": "2021-07-01 02:55:12", "link": "http://arxiv.org/abs/2107.00189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientia Potentia Est -- On the Role of Knowledge in Computational\n  Argumentation", "abstract": "Despite extensive research efforts in recent years, computational\nargumentation (CA) remains one of the most challenging areas of natural\nlanguage processing. The reason for this is the inherent complexity of the\ncognitive processes behind human argumentation, which integrate a plethora of\ndifferent types of knowledge, ranging from topic-specific facts and common\nsense to rhetorical knowledge. The integration of knowledge from such a wide\nrange in CA requires modeling capabilities far beyond many other natural\nlanguage understanding tasks. Existing research on mining, assessing, reasoning\nover, and generating arguments largely acknowledges that much more knowledge is\nneeded to accurately model argumentation computationally. However, a systematic\noverview of the types of knowledge introduced in existing CA models is missing,\nhindering targeted progress in the field. Adopting the operational definition\nof knowledge as any task-relevant normative information not provided as input,\nthe survey paper at hand fills this gap by (1) proposing a taxonomy of types of\nknowledge required in CA tasks, (2) systematizing the large body of CA work\naccording to the reliance on and exploitation of these knowledge types for the\nfour main research areas in CA, and (3) outlining and discussing directions for\nfuture research efforts in CA.", "published": "2021-07-01 08:12:41", "link": "http://arxiv.org/abs/2107.00281v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-pronoun Data Augmentation for Japanese-to-English Translation", "abstract": "For Japanese-to-English translation, zero pronouns in Japanese pose a\nchallenge, since the model needs to infer and produce the corresponding pronoun\nin the target side of the English sentence. However, although fully resolving\nzero pronouns often needs discourse context, in some cases, the local context\nwithin a sentence gives clues to the inference of the zero pronoun. In this\nstudy, we propose a data augmentation method that provides additional training\nsignals for the translation model to learn correlations between local context\nand zero pronouns. We show that the proposed method significantly improves the\naccuracy of zero pronoun translation with machine translation experiments in\nthe conversational domain.", "published": "2021-07-01 09:17:59", "link": "http://arxiv.org/abs/2107.00318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Central Repository: a Cross-lingual Framework for\n  Developing Wordnets", "abstract": "Language resources are necessary for language processing,but building them is\ncostly, involves many researches from different areas and needs constant\nupdating. In this paper, we describe the crosslingual framework used for\ndeveloping the Multilingual Central Repository (MCR), a multilingual knowledge\nbase that includes wordnets of Basque, Catalan, English, Galician, Portuguese,\nSpanish and the following ontologies: Base Concepts, Top Ontology, WordNet\nDomains and Suggested Upper Merged Ontology. We present the story of MCR, its\nstate in 2017 and the developed tools.", "published": "2021-07-01 09:50:55", "link": "http://arxiv.org/abs/2107.00333v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Target-side Inflection in Placeholder Translation", "abstract": "Placeholder translation systems enable the users to specify how a specific\nphrase is translated in the output sentence. The system is trained to output\nspecial placeholder tokens, and the user-specified term is injected into the\noutput through the context-free replacement of the placeholder token. However,\nthis approach could result in ungrammatical sentences because it is often the\ncase that the specified term needs to be inflected according to the context of\nthe output, which is unknown before the translation. To address this problem,\nwe propose a novel method of placeholder translation that can inflect specified\nterms according to the grammatical construction of the output sentence. We\nextend the sequence-to-sequence architecture with a character-level decoder\nthat takes the lemma of a user-specified term and the words generated from the\nword-level decoder to output the correct inflected form of the lemma. We\nevaluate our approach with a Japanese-to-English translation task in the\nscientific writing domain, and show that our model can incorporate specified\nterms in the correct form more successfully than other comparable models.", "published": "2021-07-01 09:54:22", "link": "http://arxiv.org/abs/2107.00334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Learning-Based Approach for Improving Generalization Capability\n  of Machine Reading Comprehension Systems", "abstract": "Machine Reading Comprehension (MRC) is an active field in natural language\nprocessing with many successful developed models in recent years. Despite their\nhigh in-distribution accuracy, these models suffer from two issues: high\ntraining cost and low out-of-distribution accuracy. Even though some approaches\nhave been presented to tackle the generalization problem, they have high,\nintolerable training costs. In this paper, we investigate the effect of\nensemble learning approach to improve generalization of MRC systems without\nretraining a big model. After separately training the base models with\ndifferent structures on different datasets, they are ensembled using weighting\nand stacking approaches in probabilistic and non-probabilistic settings. Three\nconfigurations are investigated including heterogeneous, homogeneous, and\nhybrid on eight datasets and six state-of-the-art models. We identify the\nimportant factors in the effectiveness of ensemble methods. Also, we compare\nthe robustness of ensemble and fine-tuned models against data distribution\nshifts. The experimental results show the effectiveness and robustness of the\nensemble approach in improving the out-of-distribution accuracy of MRC systems,\nespecially when the base models are similar in accuracies.", "published": "2021-07-01 11:11:17", "link": "http://arxiv.org/abs/2107.00368v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation for Quality Estimation", "abstract": "Quality Estimation (QE) is the task of automatically predicting Machine\nTranslation quality in the absence of reference translations, making it\napplicable in real-time settings, such as translating online social media\nconversations. Recent success in QE stems from the use of multilingual\npre-trained representations, where very large models lead to impressive\nresults. However, the inference time, disk and memory requirements of such\nmodels do not allow for wide usage in the real world. Models trained on\ndistilled pre-trained representations remain prohibitively large for many usage\nscenarios. We instead propose to directly transfer knowledge from a strong QE\nteacher model to a much smaller model with a different, shallower architecture.\nWe show that this approach, in combination with data augmentation, leads to\nlight-weight QE models that perform competitively with distilled pre-trained\nrepresentations with 8x fewer parameters.", "published": "2021-07-01 12:36:21", "link": "http://arxiv.org/abs/2107.00411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiCite: Modeling realistic citations requires moving beyond the\n  single-sentence single-label setting", "abstract": "Citation context analysis (CCA) is an important task in natural language\nprocessing that studies how and why scholars discuss each others' work. Despite\ndecades of study, traditional frameworks for CCA have largely relied on\noverly-simplistic assumptions of how authors cite, which ignore several\nimportant phenomena. For instance, scholarly papers often contain rich\ndiscussions of cited work that span multiple sentences and express multiple\nintents concurrently. Yet, CCA is typically approached as a single-sentence,\nsingle-label classification task, and thus existing datasets fail to capture\nthis interesting discourse. In our work, we address this research gap by\nproposing a novel framework for CCA as a document-level context extraction and\nlabeling task. We release MultiCite, a new dataset of 12,653 citation contexts\nfrom over 1,200 computational linguistics papers. Not only is it the largest\ncollection of expert-annotated citation contexts to-date, MultiCite contains\nmulti-sentence, multi-label citation contexts within full paper texts. Finally,\nwe demonstrate how our dataset, while still usable for training classic CCA\nmodels, also supports the development of new types of models for CCA beyond\nfixed-width text classification. We release our code and dataset at\nhttps://github.com/allenai/multicite.", "published": "2021-07-01 12:54:23", "link": "http://arxiv.org/abs/2107.00414v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Graph-based Transformer Framework for Biomedical Relation\n  Extraction", "abstract": "The recent advancement of pre-trained Transformer models has propelled the\ndevelopment of effective text mining models across various biomedical tasks.\nHowever, these models are primarily learned on the textual data and often lack\nthe domain knowledge of the entities to capture the context beyond the\nsentence. In this study, we introduced a novel framework that enables the model\nto learn multi-omnics biological information about entities (proteins) with the\nhelp of additional multi-modal cues like molecular structure. Towards this,\nrather developing modality-specific architectures, we devise a generalized and\noptimized graph based multi-modal learning mechanism that utilizes the\nGraphBERT model to encode the textual and molecular structure information and\nexploit the underlying features of various modalities to enable end-to-end\nlearning. We evaluated our proposed method on ProteinProtein Interaction task\nfrom the biomedical corpus, where our proposed generalized approach is observed\nto be benefited by the additional domain-specific modality.", "published": "2021-07-01 16:37:17", "link": "http://arxiv.org/abs/2107.00596v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Primer on Pretrained Multilingual Language Models", "abstract": "Multilingual Language Models (\\MLLMs) such as mBERT, XLM, XLM-R,\n\\textit{etc.} have emerged as a viable option for bringing the power of\npretraining to a large number of languages. Given their success in zero-shot\ntransfer learning, there has emerged a large body of work in (i) building\nbigger \\MLLMs~covering a large number of languages (ii) creating exhaustive\nbenchmarks covering a wider variety of tasks and languages for evaluating\n\\MLLMs~ (iii) analysing the performance of \\MLLMs~on monolingual, zero-shot\ncross-lingual and bilingual tasks (iv) understanding the universal language\npatterns (if any) learnt by \\MLLMs~ and (v) augmenting the (often) limited\ncapacity of \\MLLMs~ to improve their performance on seen or even unseen\nlanguages. In this survey, we review the existing literature covering the above\nbroad areas of research pertaining to \\MLLMs. Based on our survey, we recommend\nsome promising directions of future research.", "published": "2021-07-01 18:01:46", "link": "http://arxiv.org/abs/2107.00676v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive decoding of words from visual speech recognition models", "abstract": "This work describes an interactive decoding method to improve the performance\nof visual speech recognition systems using user input to compensate for the\ninherent ambiguity of the task. Unlike most phoneme-to-word decoding pipelines,\nwhich produce phonemes and feed these through a finite state transducer, our\nmethod instead expands words in lockstep, facilitating the insertion of\ninteraction points at each word position. Interaction points enable us to\nsolicit input during decoding, allowing users to interactively direct the\ndecoding process. We simulate the behavior of user input using an oracle to\ngive an automated evaluation, and show promise for the use of this method for\ntext input.", "published": "2021-07-01 18:38:01", "link": "http://arxiv.org/abs/2107.00692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Feature and Instance Attribution to Detect Artifacts", "abstract": "Training the deep neural networks that dominate NLP requires large datasets.\nThese are often collected automatically or via crowdsourcing, and may exhibit\nsystematic biases or annotation artifacts. By the latter we mean spurious\ncorrelations between inputs and outputs that do not represent a generally held\ncausal relationship between features and classes; models that exploit such\ncorrelations may appear to perform a given task well, but fail on out of sample\ndata. In this paper we evaluate use of different attribution methods for aiding\nidentification of training data artifacts. We propose new hybrid approaches\nthat combine saliency maps (which highlight important input features) with\ninstance attribution methods (which retrieve training samples influential to a\ngiven prediction). We show that this proposed training-feature attribution can\nbe used to efficiently uncover artifacts in training data when a challenging\nvalidation set is available. We also carry out a small user study to evaluate\nwhether these methods are useful to NLP researchers in practice, with promising\nresults. We make code for all methods and experiments in this paper available.", "published": "2021-07-01 09:26:13", "link": "http://arxiv.org/abs/2107.00323v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural\n  Language Understanding", "abstract": "Despite pre-trained language models have proven useful for learning\nhigh-quality semantic representations, these models are still vulnerable to\nsimple perturbations. Recent works aimed to improve the robustness of\npre-trained models mainly focus on adversarial training from perturbed examples\nwith similar semantics, neglecting the utilization of different or even\nopposite semantics. Different from the image processing field, the text is\ndiscrete and few word substitutions can cause significant semantic changes. To\nstudy the impact of semantics caused by small perturbations, we conduct a\nseries of pilot experiments and surprisingly find that adversarial training is\nuseless or even harmful for the model to detect these semantic changes. To\naddress this problem, we propose Contrastive Learning with semantIc Negative\nExamples (CLINE), which constructs semantic negative examples unsupervised to\nimprove the robustness under semantically adversarial attacking. By comparing\nwith similar and opposite semantic examples, the model can effectively perceive\nthe semantic changes caused by small perturbations. Empirical results show that\nour approach yields substantial improvements on a range of sentiment analysis,\nreasoning, and reading comprehension tasks. And CLINE also ensures the\ncompactness within the same semantics and separability across different\nsemantics in sentence-level.", "published": "2021-07-01 13:34:12", "link": "http://arxiv.org/abs/2107.00440v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Investigation of the (In)effectiveness of Counterfactually Augmented\n  Data", "abstract": "While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features; and (b) CAD may exacerbate existing spurious correlations in\nthe data. On two crowdsourced CAD datasets, our results show that the lack of\nperturbation diversity limits their effectiveness on OOD generalization,\ncalling for innovative crowdsourcing procedures to elicit diverse perturbation\nof examples.", "published": "2021-07-01 21:46:43", "link": "http://arxiv.org/abs/2107.00753v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tackling COVID-19 Infodemic using Deep Learning", "abstract": "Humanity is battling one of the most deleterious virus in modern history, the\nCOVID-19 pandemic, but along with the pandemic there's an infodemic permeating\nthe pupil and society with misinformation which exacerbates the current malady.\nWe try to detect and classify fake news on online media to detect fake\ninformation relating to COVID-19 and coronavirus. The dataset contained fake\nposts, articles and news gathered from fact checking websites like politifact\nwhereas real tweets were taken from verified twitter handles. We incorporated\nmultiple conventional classification techniques like Naive Bayes, KNN, Gradient\nBoost and Random Forest along with Deep learning approaches, specifically CNN,\nRNN, DNN and the ensemble model RMDL. We analyzed these approaches with two\nfeature extraction techniques, TF-IDF and GloVe Word Embeddings which would\nprovide deeper insights into the dataset containing COVID-19 info on online\nmedia.", "published": "2021-07-01 11:07:47", "link": "http://arxiv.org/abs/2107.02012v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word-Free Spoken Language Understanding for Mandarin-Chinese", "abstract": "Spoken dialogue systems such as Siri and Alexa provide great convenience to\npeople's everyday life. However, current spoken language understanding (SLU)\npipelines largely depend on automatic speech recognition (ASR) modules, which\nrequire a large amount of language-specific training data. In this paper, we\npropose a Transformer-based SLU system that works directly on phones. This\nacoustic-based SLU system consists of only two blocks and does not require the\npresence of ASR module. The first block is a universal phone recognition\nsystem, and the second block is a Transformer-based language model for phones.\nWe verify the effectiveness of the system on an intent classification dataset\nin Mandarin Chinese.", "published": "2021-07-01 02:31:22", "link": "http://arxiv.org/abs/2107.00186v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at\n  IWSLT 2021", "abstract": "This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous\nSpeech Translation task. We proposed a novel simultaneous translation model,\nCross Attention Augmented Transducer (CAAT), which extends conventional RNN-T\nto sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous\ntranslation. Experiments on speech-to-text (S2T) and text-to-text (T2T)\nsimultaneous translation tasks shows CAAT achieves better quality-latency\ntrade-offs compared to \\textit{wait-k}, one of the previous state-of-the-art\napproaches. Based on CAAT architecture and data augmentation, we build S2T and\nT2T simultaneous translation systems in this evaluation campaign. Compared to\nlast year's optimal systems, our S2T simultaneous translation system improves\nby an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous\ntranslation system improves by an average of 4.6 BLEU.", "published": "2021-07-01 08:09:00", "link": "http://arxiv.org/abs/2107.00279v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Objective Evaluation Framework for Pathological Speech Synthesis", "abstract": "The development of pathological speech systems is currently hindered by the\nlack of a standardised objective evaluation framework. In this work, (1) we\nutilise existing detection and analysis techniques to propose a general\nframework for the consistent evaluation of synthetic pathological speech. This\nframework evaluates the voice quality and the intelligibility aspects of speech\nand is shown to be complementary using our experiments. (2) Using our proposed\nevaluation framework, we develop and test a dysarthric voice conversion system\n(VC) using CycleGAN-VC and a PSOLA-based speech rate modification technique. We\nshow that the developed system is able to synthesise dysarthric speech with\ndifferent levels of speech intelligibility.", "published": "2021-07-01 08:55:57", "link": "http://arxiv.org/abs/2107.00308v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP\n  Systems", "abstract": "Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.", "published": "2021-07-01 09:08:43", "link": "http://arxiv.org/abs/2107.00315v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GlyphCRM: Bidirectional Encoder Representation for Chinese Character\n  with its Glyph", "abstract": "Previous works indicate that the glyph of Chinese characters contains rich\nsemantic information and has the potential to enhance the representation of\nChinese characters. The typical method to utilize the glyph features is by\nincorporating them into the character embedding space. Inspired by previous\nmethods, we innovatively propose a Chinese pre-trained representation model\nnamed as GlyphCRM, which abandons the ID-based character embedding method yet\nsolely based on sequential character images. We render each character into a\nbinary grayscale image and design two-channel position feature maps for it.\nFormally, we first design a two-layer residual convolutional neural network,\nnamely HanGlyph to generate the initial glyph representation of Chinese\ncharacters, and subsequently adopt multiple bidirectional encoder Transformer\nblocks as the superstructure to capture the context-sensitive information.\nMeanwhile, we feed the glyph features extracted from each layer of the HanGlyph\nmodule into the underlying Transformer blocks by skip-connection method to\nfully exploit the glyph features of Chinese characters. As the HanGlyph module\ncan obtain a sufficient glyph representation of any Chinese character, the\nlong-standing out-of-vocabulary problem could be effectively solved. Extensive\nexperimental results indicate that GlyphCRM substantially outperforms the\nprevious BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has\nstrong transferability and generalization on specialized fields and\nlow-resource tasks. We hope this work could spark further research beyond the\nrealms of well-established representation of Chinese texts.", "published": "2021-07-01 12:14:05", "link": "http://arxiv.org/abs/2107.00395v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "What do End-to-End Speech Models Learn about Speaker, Language and\n  Channel Information? A Layer-wise and Neuron-level Analysis", "abstract": "Deep neural networks are inherently opaque and challenging to interpret.\nUnlike hand-crafted feature-based models, we struggle to comprehend the\nconcepts learned and how they interact within these models. This understanding\nis crucial not only for debugging purposes but also for ensuring fairness in\nethical decision-making. In our study, we conduct a post-hoc functional\ninterpretability analysis of pretrained speech models using the probing\nframework [1]. Specifically, we analyze utterance-level representations of\nspeech models trained for various tasks such as speaker recognition and dialect\nidentification. We conduct layer and neuron-wise analyses, probing for speaker,\nlanguage, and channel properties. Our study aims to answer the following\nquestions: i) what information is captured within the representations? ii) how\nis it represented and distributed? and iii) can we identify a minimal subset of\nthe network that possesses this information?\n  Our results reveal several novel findings, including: i) channel and gender\ninformation are distributed across the network, ii) the information is\nredundantly available in neurons with respect to a task, iii) complex\nproperties such as dialectal information are encoded only in the task-oriented\npretrained network, iv) and is localised in the upper layers, v) we can extract\na minimal subset of neurons encoding the pre-defined property, vi) salient\nneurons are sometimes shared between properties, vii) our analysis highlights\nthe presence of biases (for example gender) in the network. Our\ncross-architectural comparison indicates that: i) the pretrained models capture\nspeaker-invariant information, and ii) CNN models are competitive with\nTransformer models in encoding various understudied properties.", "published": "2021-07-01 13:32:55", "link": "http://arxiv.org/abs/2107.00439v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "StableEmit: Selection Probability Discount for Reducing Emission Latency\n  of Streaming Monotonic Attention ASR", "abstract": "While attention-based encoder-decoder (AED) models have been successfully\nextended to the online variants for streaming automatic speech recognition\n(ASR), such as monotonic chunkwise attention (MoChA), the models still have a\nlarge label emission latency because of the unconstrained end-to-end training\nobjective. Previous works tackled this problem by leveraging alignment\ninformation to control the timing to emit tokens during training. In this work,\nwe propose a simple alignment-free regularization method, StableEmit, to\nencourage MoChA to emit tokens earlier. StableEmit discounts the selection\nprobabilities in hard monotonic attention for token boundary detection by a\nconstant factor and regularizes them to recover the total attention mass during\ntraining. As a result, the scale of the selection probabilities is increased,\nand the values can reach a threshold for token emission earlier, leading to a\nreduction of emission latency and deletion errors. Moreover, StableEmit can be\ncombined with methods that constraint alignments to further improve the\naccuracy and latency. Experimental evaluations with LSTM and Conformer encoders\ndemonstrate that StableEmit significantly reduces the recognition errors and\nthe emission latency simultaneously. We also show that the use of alignment\ninformation is complementary in both metrics.", "published": "2021-07-01 17:49:31", "link": "http://arxiv.org/abs/2107.00635v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ESPnet-ST IWSLT 2021 Offline Speech Translation System", "abstract": "This paper describes the ESPnet-ST group's IWSLT 2021 submission in the\noffline speech translation track. This year we made various efforts on training\ndata, architecture, and audio segmentation. On the data side, we investigated\nsequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech\ntranslation. Specifically, we used multi-referenced SeqKD from multiple\nteachers trained on different amounts of bitext. On the architecture side, we\nadopted the Conformer encoder and the Multi-Decoder architecture, which equips\ndedicated decoders for speech recognition and translation tasks in a unified\nencoder-decoder model and enables search in both source and target language\nspaces during inference. We also significantly improved audio segmentation by\nusing the pyannote.audio toolkit and merging multiple short segments for long\ncontext modeling. Experimental evaluations showed that each of them contributed\nto large improvements in translation performance. Our best E2E system combined\nall the above techniques with model ensembling and achieved 31.4 BLEU on the\n2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of\ntst2021.", "published": "2021-07-01 17:49:43", "link": "http://arxiv.org/abs/2107.00636v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Normalizing Flow based Hidden Markov Models for Classification of Speech\n  Phones with Explainability", "abstract": "In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.", "published": "2021-07-01 20:10:55", "link": "http://arxiv.org/abs/2107.00730v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Combining Frame-Synchronous and Label-Synchronous Systems for Speech\n  Recognition", "abstract": "Commonly used automatic speech recognition (ASR) systems can be classified\ninto frame-synchronous and label-synchronous categories, based on whether the\nspeech is decoded on a per-frame or per-label basis. Frame-synchronous systems,\nsuch as traditional hidden Markov model systems, can easily incorporate\nexisting knowledge and can support streaming ASR applications.\nLabel-synchronous systems, based on attention-based encoder-decoder models, can\njointly learn the acoustic and language information with a single model, which\ncan be regarded as audio-grounded language models. In this paper, we propose\nrescoring the N-best hypotheses or lattices produced by a first-pass\nframe-synchronous system with a label-synchronous system in a second-pass. By\nexploiting the complementary modelling of the different approaches, the\ncombined two-pass systems achieve competitive performance without using any\nextra speech or text data on two standard ASR tasks. For the 80-hour AMI IHM\ndataset, the combined system has a 13.7% word error rate (WER) on the\nevaluation set, which is up to a 29% relative WER reduction over the individual\nsystems. For the 300-hour Switchboard dataset, the WERs of the combined system\nare 5.7% and 12.1% on Switchboard and CallHome subsets of Hub5'00, and 13.2%\nand 7.6% on Switchboard Cellular and Fisher subsets of RT03, up to a 33%\nrelative reduction in WER over the individual systems.", "published": "2021-07-01 22:27:53", "link": "http://arxiv.org/abs/2107.00764v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Attention-based multi-channel speaker verification with ad-hoc\n  microphone arrays", "abstract": "Recently, ad-hoc microphone array has been widely studied. Unlike traditional\nmicrophone array settings, the spatial arrangement and number of microphones of\nad-hoc microphone arrays are not known in advance, which hinders the adaptation\nof traditional speaker verification technologies to ad-hoc microphone arrays.\nTo overcome this weakness, in this paper, we propose attention-based\nmulti-channel speaker verification with ad-hoc microphone arrays. Specifically,\nwe add an inter-channel processing layer and a global fusion layer after the\npooling layer of a single-channel speaker verification system. The\ninter-channel processing layer applies a so-called residual self-attention\nalong the channel dimension for allocating weights to different microphones.\nThe global fusion layer integrates all channels in a way that is independent to\nthe number of the input channels. We further replace the softmax operator in\nthe residual self-attention with sparsemax, which forces the channel weights of\nvery noisy channels to zero. Experimental results with ad-hoc microphone arrays\nof over 30 channels demonstrate the effectiveness of the proposed methods. For\nexample, the multi-channel speaker verification with sparsemax achieves an\nequal error rate (EER) of over 20% lower than oracle one-best system on\nsemi-real data sets, and over 30% lower on simulation data sets, in test\nscenarios with both matched and mismatched channel numbers.", "published": "2021-07-01 02:12:33", "link": "http://arxiv.org/abs/2107.00178v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sonority Measurement Using System, Source, and Suprasegmental\n  Information", "abstract": "Sonorant sounds are characterized by regions with prominent formant\nstructure, high energy and high degree of periodicity. In this work, the\nvocal-tract system, excitation source and suprasegmental features derived from\nthe speech signal are analyzed to measure the sonority information present in\neach of them. Vocal-tract system information is extracted from the Hilbert\nenvelope of numerator of group delay function. It is derived from zero time\nwindowed speech signal that provides better resolution of the formants. A\nfive-dimensional feature set is computed from the estimated formants to measure\nthe prominence of the spectral peaks. A feature representing strength of\nexcitation is derived from the Hilbert envelope of linear prediction residual,\nwhich represents the source information. Correlation of speech over ten\nconsecutive pitch periods is used as the suprasegmental feature representing\nperiodicity information. The combination of evidences from the three different\naspects of speech provides better discrimination among different sonorant\nclasses, compared to the baseline MFCC features. The usefulness of the proposed\nsonority feature is demonstrated in the tasks of phoneme recognition and\nsonorant classification.", "published": "2021-07-01 08:31:09", "link": "http://arxiv.org/abs/2107.00297v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prediction of tone detection thresholds in interaurally delayed noise\n  based on interaural phase difference fluctuations", "abstract": "Differences between the interaural phase of a noise and a target tone improve\ndetection thresholds. The maximum masking release is obtained for detecting an\nantiphasic tone (S$\\pi$) in diotic noise (N0). It has been shown in several\nstudies that this benefit gradually declines as an interaural delay is applied\nto the N0S$\\pi$ complex. This decline has been attributed to the reduced\ninteraural coherence of the noise. Here, we report detection thresholds for a\n500 Hz tone in masking noise with up to 8 ms interaural delay and bandwidths\nfrom 25 to 1000 Hz. When reducing the noise bandwidth from 100 to 50 and 25 Hz,\nthe masking release at 8 ms delay increases, as expected for increasing\ntemporal coherence with decreasing bandwidth. For bandwidths of 100 to 1000 Hz,\nno significant difference was observed and detection thresholds with these\nnoises have a delay dependence that is fully described by the temporal\ncoherence imposed by the typical monaurally determined auditory filter\nbandwidth. A minimalistic binaural model is suggested based on interaural phase\ndifference fluctuations without the assumption of delay lines.", "published": "2021-07-01 09:21:14", "link": "http://arxiv.org/abs/2107.00320v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audiovisual Singing Voice Separation", "abstract": "Separating a song into vocal and accompaniment components is an active\nresearch topic, and recent years witnessed an increased performance from\nsupervised training using deep learning techniques. We propose to apply the\nvisual information corresponding to the singers' vocal activities to further\nimprove the quality of the separated vocal signals. The video frontend model\ntakes the input of mouth movement and fuses it into the feature embeddings of\nan audio-based separation framework. To facilitate the network to learn\naudiovisual correlation of singing activities, we add extra vocal signals\nirrelevant to the mouth movement to the audio mixture during training. We\ncreate two audiovisual singing performance datasets for training and\nevaluation, respectively, one curated from audition recordings on the Internet,\nand the other recorded in house. The proposed method outperforms audio-based\nmethods in terms of separation quality on most test recordings. This advantage\nis especially pronounced when there are backing vocals in the accompaniment,\nwhich poses a great challenge for audio-only methods.", "published": "2021-07-01 06:04:53", "link": "http://arxiv.org/abs/2107.00231v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Sample Detection for Speaker Verification by Neural Vocoders", "abstract": "Automatic speaker verification (ASV), one of the most important technology\nfor biometric identification, has been widely adopted in security-critical\napplications. However, ASV is seriously vulnerable to recently emerged\nadversarial attacks, yet effective countermeasures against them are limited. In\nthis paper, we adopt neural vocoders to spot adversarial samples for ASV. We\nuse the neural vocoder to re-synthesize audio and find that the difference\nbetween the ASV scores for the original and re-synthesized audio is a good\nindicator for discrimination between genuine and adversarial samples. This\neffort is, to the best of our knowledge, among the first to pursue such a\ntechnical direction for detecting time-domain adversarial samples for ASV, and\nhence there is a lack of established baselines for comparison. Consequently, we\nimplement the Griffin-Lim algorithm as the detection baseline. The proposed\napproach achieves effective detection performance that outperforms the\nbaselines in all the settings. We also show that the neural vocoder adopted in\nthe detection framework is dataset-independent. Our codes will be made\nopen-source for future works to do fair comparison.", "published": "2021-07-01 08:58:16", "link": "http://arxiv.org/abs/2107.00309v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pretext Tasks selection for multitask self-supervised speech\n  representation learning", "abstract": "Through solving pretext tasks, self-supervised learning leverages unlabeled\ndata to extract useful latent representations replacing traditional input\nfeatures in the downstream task. In audio/speech signal processing, a wide\nrange of features where engineered through decades of research efforts. As it\nturns out, learning to predict such features (a.k.a pseudo-labels) has proven\nto be a particularly relevant pretext task, leading to useful self-supervised\nrepresentations which prove to be effective for downstream tasks. However,\nmethods and common practices for combining such pretext tasks for better\nperformance on the downstream task have not been explored and understood\nproperly. In fact, the process relies almost exclusively on a computationally\nheavy experimental procedure, which becomes intractable with the increase of\nthe number of pretext tasks. This paper introduces a method to select a group\nof pretext tasks among a set of candidates. The method we propose estimates\ncalibrated weights for the partial losses corresponding to the considered\npretext tasks during the self-supervised training process. The experiments\nconducted on automatic speech recognition, speaker and emotion recognition\nvalidate our approach, as the groups selected and weighted with our method\nperform better than classic baselines, thus facilitating the selection and\ncombination of relevant pseudo-labels for self-supervised representation\nlearning.", "published": "2021-07-01 16:36:29", "link": "http://arxiv.org/abs/2107.00594v5", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Improving Sound Event Classification by Increasing Shift Invariance in\n  Convolutional Neural Networks", "abstract": "Recent studies have put into question the commonly assumed shift invariance\nproperty of convolutional networks, showing that small shifts in the input can\naffect the output predictions substantially. In this paper, we analyze the\nbenefits of addressing lack of shift invariance in CNN-based sound event\nclassification. Specifically, we evaluate two pooling methods to improve shift\ninvariance in CNNs, based on low-pass filtering and adaptive sampling of\nincoming feature maps. These methods are implemented via small architectural\nmodifications inserted into the pooling layers of CNNs. We evaluate the effect\nof these architectural changes on the FSD50K dataset using models of different\ncapacity and in presence of strong regularization. We show that these\nmodifications consistently improve sound event classification in all cases\nconsidered. We also demonstrate empirically that the proposed pooling methods\nincrease shift invariance in the network, making it more robust against\ntime/frequency shifts in input spectrograms. This is achieved by adding a\nnegligible amount of trainable parameters, which makes these methods an\nappealing alternative to conventional pooling layers. The outcome is a new\nstate-of-the-art mAP of 0.541 on the FSD50K classification benchmark.", "published": "2021-07-01 17:21:02", "link": "http://arxiv.org/abs/2107.00623v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
