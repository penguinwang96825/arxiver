{"title": "Automatically Creating a Large Number of New Bilingual Dictionaries", "abstract": "This paper proposes approaches to automatically create a large number of new\nbilingual dictionaries for low-resource languages, especially resource-poor and\nendangered languages, from a single input bilingual dictionary. Our algorithms\nproduce translations of words in a source language to plentiful target\nlanguages using available Wordnets and a machine translator (MT). Since our\napproaches rely on just one input dictionary, available Wordnets and an MT,\nthey are applicable to any bilingual dictionary as long as one of the two\nlanguages is English or has a Wordnet linked to the Princeton Wordnet. Starting\nwith 5 available bilingual dictionaries, we create 48 new bilingual\ndictionaries. Of these, 30 pairs of languages are not supported by the popular\nMTs: Google and Bing.", "published": "2022-08-12 04:25:23", "link": "http://arxiv.org/abs/2208.06110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Probability of Agreement", "abstract": "Measuring inter-annotator agreement is important for annotation tasks, but\nmany metrics require a fully-annotated set of data, where all annotators\nannotate all samples. We define Sparse Probability of Agreement, SPA, which\nestimates the probability of agreement when not all annotator-item-pairs are\navailable. We show that under certain conditions, SPA is an unbiased estimator,\nand we provide multiple weighing schemes for handling data with various degrees\nof annotation.", "published": "2022-08-12 08:15:34", "link": "http://arxiv.org/abs/2208.06161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Legal Arguments in Court Decisions", "abstract": "Identifying, classifying, and analyzing arguments in legal discourse has been\na prominent area of research since the inception of the argument mining field.\nHowever, there has been a major discrepancy between the way natural language\nprocessing (NLP) researchers model and annotate arguments in court decisions\nand the way legal experts understand and analyze legal argumentation. While\ncomputational approaches typically simplify arguments into generic premises and\nclaims, arguments in legal research usually exhibit a rich typology that is\nimportant for gaining insights into the particular case and applications of law\nin general. We address this problem and make several substantial contributions\nto move the field forward. First, we design a new annotation scheme for legal\narguments in proceedings of the European Court of Human Rights (ECHR) that is\ndeeply rooted in the theory and practice of legal argumentation research.\nSecond, we compile and annotate a large corpus of 373 court decisions (2.3M\ntokens and 15k annotated argument spans). Finally, we train an argument mining\nmodel that outperforms state-of-the-art models in the legal NLP domain and\nprovide a thorough expert-based evaluation. All datasets and source codes are\navailable under open lincenses at\nhttps://github.com/trusthlt/mining-legal-arguments.", "published": "2022-08-12 08:59:55", "link": "http://arxiv.org/abs/2208.06178v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Utterance Labeling of Conversations Using Natural Language\n  Processing", "abstract": "Conversational data is essential in psychology because it can help\nresearchers understand individuals cognitive processes, emotions, and\nbehaviors. Utterance labelling is a common strategy for analyzing this type of\ndata. The development of NLP algorithms allows researchers to automate this\ntask. However, psychological conversational data present some challenges to NLP\nresearchers, including multilabel classification, a large number of classes,\nand limited available data. This study explored how automated labels generated\nby NLP methods are comparable to human labels in the context of conversations\non adulthood transition. We proposed strategies to handle three common\nchallenges raised in psychological studies. Our findings showed that the deep\nlearning method with domain adaptation (RoBERTa-CON) outperformed all other\nmachine learning methods; and the hierarchical labelling system that we\nproposed was shown to help researchers strategically analyze conversational\ndata. Our Python code and NLP model are available at\nhttps://github.com/mlaricheva/automated_labeling.", "published": "2022-08-12 23:03:45", "link": "http://arxiv.org/abs/2208.06525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Sign Language Production via Knowledge Distillation", "abstract": "Sign Language Production (SLP) aims to translate expressions in spoken\nlanguage into corresponding ones in sign language, such as skeleton-based sign\nposes or videos. Existing SLP models are either AutoRegressive (AR) or\nNon-Autoregressive (NAR). However, AR-SLP models suffer from regression to the\nmean and error propagation during decoding. NSLP-G, a NAR-based model, resolves\nthese issues to some extent but engenders other problems. For example, it does\nnot consider target sign lengths and suffers from false decoding initiation. We\npropose a novel NAR-SLP model via Knowledge Distillation (KD) to address these\nproblems. First, we devise a length regulator to predict the end of the\ngenerated sign pose sequence. We then adopt KD, which distills\nspatial-linguistic features from a pre-trained pose encoder to alleviate false\ndecoding initiation. Extensive experiments show that the proposed approach\nsignificantly outperforms existing SLP models in both Frechet Gesture Distance\nand Back-Translation evaluation.", "published": "2022-08-12 09:17:11", "link": "http://arxiv.org/abs/2208.06183v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Is Your Model Sensitive? SPeDaC: A New Benchmark for Detecting and\n  Classifying Sensitive Personal Data", "abstract": "In recent years, there has been an exponential growth of applications,\nincluding dialogue systems, that handle sensitive personal information. This\nhas brought to light the extremely important issue of personal data protection\nin virtual environments. Sensitive Information Detection (SID) approaches\ndifferent domains and languages in literature. However, if we refer to the\npersonal data domain, a shared benchmark or the absence of an available labeled\nresource makes comparison with the state-of-the-art difficult. We introduce and\nrelease SPeDaC , a new annotated resource for the identification of sensitive\npersonal data categories in the English language. SPeDaC enables the evaluation\nof computational models for three different SID subtasks with increasing levels\nof complexity. SPeDaC 1 regards binary classification, a model has to detect if\na sentence contains sensitive information or not; whereas, in SPeDaC 2 we\ncollected labeled sentences using 5 categories that relate to macro-domains of\npersonal information; in SPeDaC 3, the labeling is fine-grained (61 personal\ndata categories). We conduct an extensive evaluation of the resource using\ndifferent state-of-the-art-classifiers. The results show that SPeDaC is\nchallenging, particularly with regard to fine-grained classification. The\ntransformer models achieve the best results (acc. RoBERTa on SPeDaC 1 = 98.20%,\nDeBERTa on SPeDaC 2 = 95.81% and SPeDaC 3 = 77.63%).", "published": "2022-08-12 10:57:48", "link": "http://arxiv.org/abs/2208.06216v3", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "RealityTalk: Real-Time Speech-Driven Augmented Presentation for AR Live\n  Storytelling", "abstract": "We present RealityTalk, a system that augments real-time live presentations\nwith speech-driven interactive virtual elements. Augmented presentations\nleverage embedded visuals and animation for engaging and expressive\nstorytelling. However, existing tools for live presentations often lack\ninteractivity and improvisation, while creating such effects in video editing\ntools require significant time and expertise. RealityTalk enables users to\ncreate live augmented presentations with real-time speech-driven interactions.\nThe user can interactively prompt, move, and manipulate graphical elements\nthrough real-time speech and supporting modalities. Based on our analysis of\n177 existing video-edited augmented presentations, we propose a novel set of\ninteraction techniques and then incorporated them into RealityTalk. We evaluate\nour tool from a presenter's perspective to demonstrate the effectiveness of our\nsystem.", "published": "2022-08-12 16:12:00", "link": "http://arxiv.org/abs/2208.06350v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "LM-CORE: Language Models with Contextually Relevant External Knowledge", "abstract": "Large transformer-based pre-trained language models have achieved impressive\nperformance on a variety of knowledge-intensive tasks and can capture factual\nknowledge in their parameters. We argue that storing large amounts of knowledge\nin the model parameters is sub-optimal given the ever-growing amounts of\nknowledge and resource requirements. We posit that a more efficient alternative\nis to provide explicit access to contextually relevant structured knowledge to\nthe model and train it to use that knowledge. We present LM-CORE -- a general\nframework to achieve this -- that allows \\textit{decoupling} of the language\nmodel training from the external knowledge source and allows the latter to be\nupdated without affecting the already trained model. Experimental results show\nthat LM-CORE, having access to external knowledge, achieves significant and\nrobust outperformance over state-of-the-art knowledge-enhanced language models\non knowledge probing tasks; can effectively handle knowledge updates; and\nperforms well on two downstream tasks. We also present a thorough error\nanalysis highlighting the successes and failures of LM-CORE.", "published": "2022-08-12 18:59:37", "link": "http://arxiv.org/abs/2208.06458v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Gentle Introduction and Survey on Computing with Words (CWW)\n  Methodologies", "abstract": "Human beings have an inherent capability to use linguistic information (LI)\nseamlessly even though it is vague and imprecise. Computing with Words (CWW)\nwas proposed to impart computing systems with this capability of human beings.\nThe interest in the field of CWW is evident from a number of publications on\nvarious CWW methodologies. These methodologies use different ways to model the\nsemantics of the LI. However, to the best of our knowledge, the literature on\nthese methodologies is mostly scattered and does not give an interested\nresearcher a comprehensive but gentle guide about the notion and utility of\nthese methodologies. Hence, to introduce the foundations and state-of-the-art\nCWW methodologies, we provide a concise but a wide-ranging coverage of them in\na simple and easy to understand manner. We feel that the simplicity with which\nwe give a high-quality review and introduction to the CWW methodologies is very\nuseful for investigators, especially those embarking on the use of CWW for the\nfirst time. We also provide future research directions to build upon for the\ninterested and motivated researchers.", "published": "2022-08-12 23:58:57", "link": "http://arxiv.org/abs/2208.06532v1", "categories": ["cs.AI", "cs.CL", "68T27, 03B52, 94D05", "I.2.4; I.1.1"], "primary_category": "cs.AI"}
{"title": "ForecastTKGQuestions: A Benchmark for Temporal Question Answering and\n  Forecasting over Temporal Knowledge Graphs", "abstract": "Question answering over temporal knowledge graphs (TKGQA) has recently found\nincreasing interest. TKGQA requires temporal reasoning techniques to extract\nthe relevant information from temporal knowledge bases. The only existing TKGQA\ndataset, i.e., CronQuestions, consists of temporal questions based on the facts\nfrom a fixed time period, where a temporal knowledge graph (TKG) spanning the\nsame period can be fully used for answer inference, allowing the TKGQA models\nto use even the future knowledge to answer the questions based on the past\nfacts. In real-world scenarios, however, it is also common that given the\nknowledge until now, we wish the TKGQA systems to answer the questions asking\nabout the future. As humans constantly seek plans for the future, building\nTKGQA systems for answering such forecasting questions is important.\nNevertheless, this has still been unexplored in previous research. In this\npaper, we propose a novel task: forecasting question answering over temporal\nknowledge graphs. We also propose a large-scale TKGQA benchmark dataset, i.e.,\nForecastTKGQuestions, for this task. It includes three types of questions,\ni.e., entity prediction, yes-no, and fact reasoning questions. For every\nforecasting question in our dataset, QA models can only have access to the TKG\ninformation before the timestamp annotated in the given question for answer\ninference. We find that the state-of-the-art TKGQA methods perform poorly on\nforecasting questions, and they are unable to answer yes-no questions and fact\nreasoning questions. To this end, we propose ForecastTKGQA, a TKGQA model that\nemploys a TKG forecasting module for future inference, to answer all three\ntypes of questions. Experimental results show that ForecastTKGQA outperforms\nrecent TKGQA methods on the entity prediction questions, and it also shows\ngreat effectiveness in answering the other two types of questions.", "published": "2022-08-12 21:02:35", "link": "http://arxiv.org/abs/2208.06501v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "An investigation on selecting audio pre-trained models for audio\n  captioning", "abstract": "Audio captioning is a task that generates description of audio based on\ncontent. Pre-trained models are widely used in audio captioning due to high\ncomplexity. Unless a comprehensive system is re-trained, it is hard to\ndetermine how well pre-trained models contribute to audio captioning system. To\nprevent the time consuming and energy consuming process of retraining, it is\nnecessary to propose a preditor of performance for the pre-trained model in\naudio captioning. In this paper, a series of pre-trained models are\ninvestigated for the correlation between extracted audio features and the\nperformance of audio captioning. A couple of predictor is proposed based on the\nexperiment results.The result demonstrates that the kurtosis and skewness of\naudio features extracted may act as an indicator of the performance of audio\ncaptioning systems for pre-trained audio due to the high correlation between\nkurtosis and skewness of audio features and the performance of audio captioning\nsystems.", "published": "2022-08-12 06:14:20", "link": "http://arxiv.org/abs/2208.06127v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DDX7: Differentiable FM Synthesis of Musical Instrument Sounds", "abstract": "FM Synthesis is a well-known algorithm used to generate complex timbre from a\ncompact set of design primitives. Typically featuring a MIDI interface, it is\nusually impractical to control it from an audio source. On the other hand,\nDifferentiable Digital Signal Processing (DDSP) has enabled nuanced audio\nrendering by Deep Neural Networks (DNNs) that learn to control differentiable\nsynthesis layers from arbitrary sound inputs. The training process involves a\ncorpus of audio for supervision, and spectral reconstruction loss functions.\nSuch functions, while being great to match spectral amplitudes, present a lack\nof pitch direction which can hinder the joint optimization of the parameters of\nFM synthesizers. In this paper, we take steps towards enabling continuous\ncontrol of a well-established FM synthesis architecture from an audio input.\nFirstly, we discuss a set of design constraints that ease spectral optimization\nof a differentiable FM synthesizer via a standard reconstruction loss. Next, we\npresent Differentiable DX7 (DDX7), a lightweight architecture for neural FM\nresynthesis of musical instrument sounds in terms of a compact set of\nparameters. We train the model on instrument samples extracted from the URMP\ndataset, and quantitatively demonstrate its comparable audio quality against\nselected benchmarks.", "published": "2022-08-12 08:39:45", "link": "http://arxiv.org/abs/2208.06169v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "H.5.5; I.2.6"], "primary_category": "cs.SD"}
