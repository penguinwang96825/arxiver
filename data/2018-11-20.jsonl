{"title": "Unsupervised Pseudo-Labeling for Extractive Summarization on Electronic\n  Health Records", "abstract": "Extractive summarization is very useful for physicians to better manage and\ndigest Electronic Health Records (EHRs). However, the training of a supervised\nmodel requires disease-specific medical background and is thus very expensive.\nWe studied how to utilize the intrinsic correlation between multiple EHRs to\ngenerate pseudo-labels and train a supervised model with no external\nannotation. Experiments on real-patient data validate that our model is\neffective in summarizing crucial disease-specific information for patients.", "published": "2018-11-20 01:08:09", "link": "http://arxiv.org/abs/1811.08040v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QuaRel: A Dataset and Models for Answering Questions about Qualitative\n  Relationships", "abstract": "Many natural language questions require recognizing and reasoning with\nqualitative relationships (e.g., in science, economics, and medicine), but are\nchallenging to answer with corpus-based methods. Qualitative modeling provides\ntools that support such reasoning, but the semantic parsing task of mapping\nquestions into those models has formidable challenges. We present QuaRel, a\ndataset of diverse story questions involving qualitative relationships that\ncharacterize these challenges, and techniques that begin to address them. The\ndataset has 2771 questions relating 19 different types of quantities. For\nexample, \"Jenny observes that the robot vacuum cleaner moves slower on the\nliving room carpet than on the bedroom carpet. Which carpet has more friction?\"\nWe contribute (1) a simple and flexible conceptual framework for representing\nthese kinds of questions; (2) the QuaRel dataset, including logical forms,\nexemplifying the parsing challenges; and (3) two novel models for this task,\nbuilt as extensions of type-constrained semantic parsing. The first of these\nmodels (called QuaSP+) significantly outperforms off-the-shelf tools on QuaRel.\nThe second (QuaSP+Zero) demonstrates zero-shot capability, i.e., the ability to\nhandle new qualitative relationships without requiring additional training\ndata, something not possible with previous models. This work thus makes inroads\ninto answering complex, qualitative questions that require reasoning, and\nscaling to new relationships at low cost. The dataset and models are available\nat http://data.allenai.org/quarel.", "published": "2018-11-20 02:59:30", "link": "http://arxiv.org/abs/1811.08048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An empirical evaluation of AMR parsing for legal documents", "abstract": "Many approaches have been proposed to tackle the problem of Abstract Meaning\nRepresentation (AMR) parsing, helps solving various natural language processing\nissues recently. In our paper, we provide an overview of different methods in\nAMR parsing and their performances when analyzing legal documents. We conduct\nexperiments of different AMR parsers on our annotated dataset extracted from\nthe English version of Japanese Civil Code. Our results show the limitations as\nwell as open a room for improvements of current parsing techniques when\napplying in this complicated domain.", "published": "2018-11-20 05:12:31", "link": "http://arxiv.org/abs/1811.08078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Another Diversity-Promoting Objective Function for Neural Dialogue\n  Generation", "abstract": "Although generation-based dialogue systems have been widely researched, the\nresponse generations by most existing systems have very low diversities. The\nmost likely reason for this problem is Maximum Likelihood Estimation (MLE) with\nSoftmax Cross-Entropy (SCE) loss. MLE trains models to generate the most\nfrequent responses from enormous generation candidates, although in actual\ndialogues there are various responses based on the context. In this paper, we\npropose a new objective function called Inverse Token Frequency (ITF) loss,\nwhich individually scales smaller loss for frequent token classes and larger\nloss for rare token classes. This function encourages the model to generate\nrare tokens rather than frequent tokens. It does not complicate the model and\nits training is stable because we only replace the objective function. On the\nOpenSubtitles dialogue dataset, our loss model establishes a state-of-the-art\nDIST-1 of 7.56, which is the unigram diversity score, while maintaining a good\nBLEU-1 score. On a Japanese Twitter replies dataset, our loss model achieves a\nDIST-1 score comparable to the ground truth.", "published": "2018-11-20 07:15:31", "link": "http://arxiv.org/abs/1811.08100v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alignment Analysis of Sequential Segmentation of Lexicons to Improve\n  Automatic Cognate Detection", "abstract": "Ranking functions in information retrieval are often used in search engines\nto recommend the relevant answers to the query. This paper makes use of this\nnotion of information retrieval and applies onto the problem domain of cognate\ndetection. The main contributions of this paper are: (1) positional\nsegmentation, which incorporates the sequential notion; (2) graphical error\nmodelling, which deduces the transformations. The current research work focuses\non classification problem; which is distinguishing whether a pair of words are\ncognates. This paper focuses on a harder problem, whether we could predict a\npossible cognate from the given input. Our study shows that when language\nmodelling smoothing methods are applied as the retrieval functions and used in\nconjunction with positional segmentation and error modelling gives better\nresults than competing baselines, in both classification and prediction of\ncognates.\n  Source code is at: https://github.com/pranav-ust/cognates", "published": "2018-11-20 08:59:53", "link": "http://arxiv.org/abs/1811.08129v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Learning Robust Heterogeneous Signal Features from Parallel Neural\n  Network for Audio Sentiment Analysis", "abstract": "Audio Sentiment Analysis is a popular research area which extends the\nconventional text-based sentiment analysis to depend on the effectiveness of\nacoustic features extracted from speech. However, current progress on audio\nsentiment analysis mainly focuses on extracting homogeneous acoustic features\nor doesn't fuse heterogeneous features effectively. In this paper, we propose\nan utterance-based deep neural network model, which has a parallel combination\nof Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based\nnetwork, to obtain representative features termed Audio Sentiment Vector (ASV),\nthat can maximally reflect sentiment information in an audio. Specifically, our\nmodel is trained by utterance-level labels and ASV can be extracted and fused\ncreatively from two branches. In the CNN model branch, spectrum graphs produced\nby signals are fed as inputs while in the LSTM model branch, inputs include\nspectral features and cepstrum coefficient extracted from dependent utterances\nin audio. Besides, Bidirectional Long Short-Term Memory (BiLSTM) with attention\nmechanism is used for feature fusion. Extensive experiments have been conducted\nto show our model can recognize audio sentiment precisely and quickly, and\ndemonstrate our ASV is better than traditional acoustic features or vectors\nextracted from other deep learning models. Furthermore, experimental results\nindicate that the proposed model outperforms the state-of-the-art approach by\n9.33\\% on Multimodal Opinion-level Sentiment Intensity dataset (MOSI) dataset.", "published": "2018-11-20 04:10:23", "link": "http://arxiv.org/abs/1811.08065v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DeepZip: Lossless Data Compression using Recurrent Neural Networks", "abstract": "Sequential data is being generated at an unprecedented pace in various forms,\nincluding text and genomic data. This creates the need for efficient\ncompression mechanisms to enable better storage, transmission and processing of\nsuch data. To solve this problem, many of the existing compressors attempt to\nlearn models for the data and perform prediction-based compression. Since\nneural networks are known as universal function approximators with the\ncapability to learn arbitrarily complex mappings, and in practice show\nexcellent performance in prediction tasks, we explore and devise methods to\ncompress sequential data using neural network predictors. We combine recurrent\nneural network predictors with an arithmetic coder and losslessly compress a\nvariety of synthetic, text and genomic datasets. The proposed compressor\noutperforms Gzip on the real datasets and achieves near-optimal compression for\nthe synthetic datasets. The results also help understand why and where neural\nnetworks are good alternatives for traditional finite context models", "published": "2018-11-20 10:12:55", "link": "http://arxiv.org/abs/1811.08162v1", "categories": ["cs.CL", "eess.SP", "q-bio.GN"], "primary_category": "cs.CL"}
{"title": "WEST: Word Encoded Sequence Transducers", "abstract": "Most of the parameters in large vocabulary models are used in embedding layer\nto map categorical features to vectors and in softmax layer for classification\nweights. This is a bottle-neck in memory constraint on-device training\napplications like federated learning and on-device inference applications like\nautomatic speech recognition (ASR). One way of compressing the embedding and\nsoftmax layers is to substitute larger units such as words with smaller\nsub-units such as characters. However, often the sub-unit models perform poorly\ncompared to the larger unit models. We propose WEST, an algorithm for encoding\ncategorical features and output classes with a sequence of random or domain\ndependent sub-units and demonstrate that this transduction can lead to\nsignificant compression without compromising performance. WEST bridges the gap\nbetween larger unit and sub-unit models and can be interpreted as a MaxEnt\nmodel over sub-unit features, which can be of independent interest.", "published": "2018-11-20 18:47:50", "link": "http://arxiv.org/abs/1811.08417v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Fading of collective attention shapes the evolution of linguistic\n  variants", "abstract": "Language change involves the competition between alternative linguistic forms\n(1). The spontaneous evolution of these forms typically results in monotonic\ngrowths or decays (2, 3) like in winner-take-all attractor behaviors. In the\ncase of the Spanish past subjunctive, the spontaneous evolution of its two\ncompeting forms (ended in -ra and -se) was perturbed by the appearance of the\nRoyal Spanish Academy in 1713, which enforced the spelling of both forms as\nperfectly interchangeable variants (4), at a moment in which the -ra form was\ndominant (5). Time series extracted from a massive corpus of books (6) reveal\nthat this regulation in fact produced a transient renewed interest for the old\nform -se which, once faded, left the -ra again as the dominant form up to the\npresent day. We show that time series are successfully explained by a\ntwo-dimensional linear model that integrates an imitative and a novelty\ncomponent. The model reveals that the temporal scale over which collective\nattention fades is in inverse proportion to the verb frequency. The integration\nof the two basic mechanisms of imitation and attention to novelty allows to\nunderstand diverse competing objects, with lifetimes that range from hours for\nmemes and news (7, 8) to decades for verbs, suggesting the existence of a\ngeneral mechanism underlying cultural evolution.", "published": "2018-11-20 19:54:41", "link": "http://arxiv.org/abs/1811.08465v4", "categories": ["cs.CL", "physics.soc-ph", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Sound-Stream II: Towards Real-Time Gesture Controlled Articulatory Sound\n  Synthesis", "abstract": "We present an interface involving four degrees-of-freedom (DOF) mechanical\ncontrol of a two dimensional, mid-sagittal tongue through a biomechanical\ntoolkit called ArtiSynth and a sound synthesis engine called JASS towards\narticulatory sound synthesis. As a demonstration of the project, the user will\nlearn to produce a range of JASS vocal sounds, by varying the shape and\nposition of the ArtiSynth tongue in 2D space through a set of four force-based\nsensors. In other words, the user will be able to physically play around with\nthese four sensors, thereby virtually controlling the magnitude of four\nselected muscle excitations of the tongue to vary articulatory structure. This\nvariation is computed in terms of Area Functions in ArtiSynth environment and\ncommunicated to the JASS based audio-synthesizer coupled with two-mass glottal\nexcitation model to complete this end-to-end gesture-to-sound mapping.", "published": "2018-11-20 00:23:28", "link": "http://arxiv.org/abs/1811.08029v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Sequence-to-Sequence Acoustic Modeling by Adding\n  Text-Supervision", "abstract": "This paper presents methods of making using of text supervision to improve\nthe performance of sequence-to-sequence (seq2seq) voice conversion. Compared\nwith conventional frame-to-frame voice conversion approaches, the seq2seq\nacoustic modeling method proposed in our previous work achieved higher\nnaturalness and similarity. In this paper, we further improve its performance\nby utilizing the text transcriptions of parallel training data. First, a\nmulti-task learning structure is designed which adds auxiliary classifiers to\nthe middle layers of the seq2seq model and predicts linguistic labels as a\nsecondary task. Second, a data-augmentation method is proposed which utilizes\ntext alignment to produce extra parallel sequences for model training.\nExperiments are conducted to evaluate our proposed method with training sets at\ndifferent sizes. Experimental results show that the multi-task learning with\nlinguistic labels is effective at reducing the errors of seq2seq voice\nconversion. The data-augmentation method can further improve the performance of\nseq2seq voice conversion when only 50 or 100 training utterances are available.", "published": "2018-11-20 08:02:54", "link": "http://arxiv.org/abs/1811.08111v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Proceedings of the LOCATA Challenge Workshop -- a satellite event of\n  IWAENC 2018", "abstract": "Algorithms for acoustic source localization and tracking provide estimates of\nthe positional information about active sound sources in acoustic environments\nand are essential for a wide range of applications such as personal assistants,\nsmart homes, tele-conferencing systems, hearing aids, or autonomous systems.\nThe aim of the IEEE-AASP Challenge on sound source localization and tracking\n(LOCATA) was to objectively benchmark state-of-the-art localization and\ntracking algorithms using an open-access data corpus of recordings for\nscenarios typically encountered in audio and acoustic signal processing\napplications. The challenge tasks ranged from the localization of a single\nsource with a static microphone array to the tracking of multiple moving\nsources with a moving microphone array.", "published": "2018-11-20 20:54:09", "link": "http://arxiv.org/abs/1811.08482v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Differentiable Consistency Constraints for Improved Deep Speech\n  Enhancement", "abstract": "In recent years, deep networks have led to dramatic improvements in speech\nenhancement by framing it as a data-driven pattern recognition problem. In many\nmodern enhancement systems, large amounts of data are used to train a deep\nnetwork to estimate masks for complex-valued short-time Fourier transforms\n(STFTs) to suppress noise and preserve speech. However, current masking\napproaches often neglect two important constraints: STFT consistency and\nmixture consistency. Without STFT consistency, the system's output is not\nnecessarily the STFT of a time-domain signal, and without mixture consistency,\nthe sum of the estimated sources does not necessarily equal the input mixture.\nFurthermore, the only previous approaches that apply mixture consistency use\nreal-valued masks; mixture consistency has been ignored for complex-valued\nmasks. In this paper, we show that STFT consistency and mixture consistency can\nbe jointly imposed by adding simple differentiable projection layers to the\nenhancement network. These layers are compatible with real or complex-valued\nmasks. Using both of these constraints with complex-valued masks provides a 0.7\ndB increase in scale-invariant signal-to-distortion ratio (SI-SDR) on a large\ndataset of speech corrupted by a wide variety of nonstationary noise across a\nrange of input SNRs.", "published": "2018-11-20 22:44:12", "link": "http://arxiv.org/abs/1811.08521v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Coupled Recurrent Models for Polyphonic Music Composition", "abstract": "This paper introduces a novel recurrent model for music composition that is\ntailored to the structure of polyphonic music. We propose an efficient new\nconditional probabilistic factorization of musical scores, viewing a score as a\ncollection of concurrent, coupled sequences: i.e. voices. To model the\nconditional distributions, we borrow ideas from both convolutional and\nrecurrent neural models; we argue that these ideas are natural for capturing\nmusic's pitch invariances, temporal structure, and polyphony. We train models\nfor single-voice and multi-voice composition on 2,300 scores from the\nKernScores dataset.", "published": "2018-11-20 02:45:44", "link": "http://arxiv.org/abs/1811.08045v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "A Gray Box Interpretable Visual Debugging Approach for Deep Sequence\n  Learning Model", "abstract": "Deep Learning algorithms are often used as black box type learning and they\nare too complex to understand. The widespread usability of Deep Learning\nalgorithms to solve various machine learning problems demands deep and\ntransparent understanding of the internal representation as well as decision\nmaking. Moreover, the learning models, trained on sequential data, such as\naudio and video data, have intricate internal reasoning process due to their\ncomplex distribution of features. Thus, a visual simulator might be helpful to\ntrace the internal decision making mechanisms in response to adversarial input\ndata, and it would help to debug and design appropriate deep learning models.\nHowever, interpreting the internal reasoning of deep learning model is not well\nstudied in the literature. In this work, we have developed a visual interactive\nweb application, namely d-DeVIS, which helps to visualize the internal\nreasoning of the learning model which is trained on the audio data. The\nproposed system allows to perceive the behavior as well as to debug the model\nby interactively generating adversarial audio data point. The web application\nof d-DeVIS is available at ddevis.herokuapp.com.", "published": "2018-11-20 17:13:49", "link": "http://arxiv.org/abs/1811.08374v1", "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for\n  Symbolic Music Generation", "abstract": "With recent breakthroughs in artificial neural networks, deep generative\nmodels have become one of the leading techniques for computational creativity.\nDespite very promising progress on image and short sequence generation,\nsymbolic music generation remains a challenging problem since the structure of\ncompositions are usually complicated. In this study, we attempt to solve the\nmelody generation problem constrained by the given chord progression. This\nmusic meta-creation problem can also be incorporated into a plan recognition\nsystem with user inputs and predictive structural outputs. In particular, we\nexplore the effect of explicit architectural encoding of musical structure via\ncomparing two sequential generative models: LSTM (a type of RNN) and WaveNet\n(dilated temporal-CNN). As far as we know, this is the first study of applying\nWaveNet to symbolic music generation, as well as the first systematic\ncomparison between temporal-CNN and RNN for music generation. We conduct a\nsurvey for evaluation in our generations and implemented Variable Markov Oracle\nin music pattern discovery. Experimental results show that to encode structure\nmore explicitly using a stack of dilated convolution layers improved the\nperformance significantly, and a global encoding of underlying chord\nprogression into the generation procedure gains even more.", "published": "2018-11-20 17:35:43", "link": "http://arxiv.org/abs/1811.08380v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-scale aggregation of phase information for reducing computational\n  cost of CNN based DOA estimation", "abstract": "In a recent work on direction-of-arrival (DOA) estimation of multiple\nspeakers with convolutional neural networks (CNNs), the phase component of\nshort-time Fourier transform (STFT) coefficients of the microphone signal is\ngiven as input and small filters are used to learn the phase relations between\nneighboring microphones. Due to this chosen filter size, $M-1$ convolution\nlayers are required to achieve the best performance for a microphone array with\nM microphones. For arrays with large number of microphones, this requirement\nleads to a high computational cost making the method practically infeasible. In\nthis work, we propose to use systematic dilations of the convolution filters in\neach of the convolution layers of the previously proposed CNN for expansion of\nthe receptive field of the filters to reduce the computational cost of the\nmethod. Different strategies for expansion of the receptive field of the\nfilters for a specific microphone array are explored. With experimental\nanalysis of the different strategies, it is shown that an aggressive expansion\nstrategy results in a considerable reduction in computational cost while a\nrelatively gradual expansion of the receptive field exhibits the best DOA\nestimation performance along with reduction in the computational cost.", "published": "2018-11-20 12:29:51", "link": "http://arxiv.org/abs/1811.08552v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
