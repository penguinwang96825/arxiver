{"title": "Razmecheno: Named Entity Recognition from Digital Archive of Diaries\n  \"Prozhito\"", "abstract": "The vast majority of existing datasets for Named Entity Recognition (NER) are\nbuilt primarily on news, research papers and Wikipedia with a few exceptions,\ncreated from historical and literary texts. What is more, English is the main\nsource for data for further labelling. This paper aims to fill in multiple gaps\nby creating a novel dataset \"Razmecheno\", gathered from the diary texts of the\nproject \"Prozhito\" in Russian. Our dataset is of interest for multiple research\nlines: literary studies of diary texts, transfer learning from other domains,\nlow-resource or cross-lingual named entity recognition. Razmecheno comprises\n1331 sentences and 14119 tokens, sampled from diaries, written during the\nPerestroika. The annotation schema consists of five commonly used entity tags:\nperson, characteristics, location, organisation, and facility. The labelling is\ncarried out on the crowdsourcing platfrom Yandex.Toloka in two stages. First,\nworkers selected sentences, which contain an entity of particular type. Second,\nthey marked up entity spans. As a result 1113 entities were obtained. Empirical\nevaluation of Razmecheno is carried out with off-the-shelf NER tools and by\nfine-tuning pre-trained contextualized encoders. We release the annotated\ndataset for open access.", "published": "2022-01-24 23:06:01", "link": "http://arxiv.org/abs/2201.09997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion-based Modeling of Mental Disorders on Social Media", "abstract": "According to the World Health Organization (WHO), one in four people will be\naffected by mental disorders at some point in their lives. However, in many\nparts of the world, patients do not actively seek professional diagnosis\nbecause of stigma attached to mental illness, ignorance of mental health and\nits associated symptoms. In this paper, we propose a model for passively\ndetecting mental disorders using conversations on Reddit. Specifically, we\nfocus on a subset of mental disorders that are characterized by distinct\nemotional patterns (henceforth called emotional disorders): major depressive,\nanxiety, and bipolar disorders. Through passive (i.e., unprompted) detection,\nwe can encourage patients to seek diagnosis and treatment for mental disorders.\nOur proposed model is different from other work in this area in that our model\nis based entirely on the emotional states, and the transition between these\nstates of users on Reddit, whereas prior work is typically based on\ncontent-based representations (e.g., n-grams, language model embeddings, etc).\nWe show that content-based representation is affected by domain and topic bias\nand thus does not generalize, while our model, on the other hand, suppresses\ntopic-specific information and thus generalizes well across different topics\nand times. We conduct experiments on our model's ability to detect different\nemotional disorders and on the generalizability of our model. Our experiments\nshow that while our model performs comparably to content-based models, such as\nBERT, it generalizes much better across time and topic.", "published": "2022-01-24 04:41:02", "link": "http://arxiv.org/abs/2201.09451v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Synthetic Books", "abstract": "The article explores new ways of written language aided by AI technologies,\nlike GPT-2 and GPT-3. The question that is stated in the paper is not about\nwhether these novel technologies will eventually replace authored books, but\nhow to relate to and contextualize such publications and what kind of new\ntools, processes, and ideas are behind them. For that purpose, a new concept of\nsynthetic books is introduced in the article. It stands for the publications\ncreated by deploying AI technology, more precisely autoregressive language\nmodels that are able to generate human-like text. Supported by the case\nstudies, the value and reasoning of the synthetic books are discussed. The\npaper emphasizes that artistic quality is an issue when it comes to\nAI-generated content. The article introduces projects that demonstrate an\ninteractive input by an artist and/or audience combined with the\ndeep-learning-based language models. In the end, the paper focuses on\nunderstanding the neural aesthetics of written language in the art context.", "published": "2022-01-24 08:26:28", "link": "http://arxiv.org/abs/2201.09518v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BTPK-based interpretable method for NER tasks based on Talmudic Public\n  Announcement Logic", "abstract": "As one of the basic tasks in natural language processing (NLP), named entity\nrecognition (NER) is an important basic tool for downstream tasks of NLP, such\nas information extraction, syntactic analysis, machine translation and so on.\nThe internal operation logic of current name entity recognition model is\nblack-box to the user, so the user has no basis to determine which name entity\nmakes more sense. Therefore, a user-friendly explainable recognition process\nwould be very useful for many people. In this paper, we propose a novel\ninterpretable method, BTPK (Binary Talmudic Public Announcement Logic model),\nto help users understand the internal recognition logic of the name entity\nrecognition tasks based on Talmudic Public Announcement Logic. BTPK model can\nalso capture the semantic information in the input sentences, that is, the\ncontext dependency of the sentence. We observed the public announcement of BTPK\npresents the inner decision logic of BRNNs, and the explanations obtained from\na BTPK model show us how BRNNs essentially handle NER tasks.", "published": "2022-01-24 08:34:41", "link": "http://arxiv.org/abs/2201.09523v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Artefact Retrieval: Overview of NLP Models with Knowledge Base Access", "abstract": "Many NLP models gain performance by having access to a knowledge base. A lot\nof research has been devoted to devising and improving the way the knowledge\nbase is accessed and incorporated into the model, resulting in a number of\nmechanisms and pipelines. Despite the diversity of proposed mechanisms, there\nare patterns in the designs of such systems. In this paper, we systematically\ndescribe the typology of artefacts (items retrieved from a knowledge base),\nretrieval mechanisms and the way these artefacts are fused into the model. This\nfurther allows us to uncover combinations of design decisions that had not yet\nbeen tried. Most of the focus is given to language models, though we also show\nhow question answering, fact-checking and knowledgable dialogue models fit into\nthis system as well. Having an abstract model which can describe the\narchitecture of specific models also helps with transferring these\narchitectures between multiple NLP tasks.", "published": "2022-01-24 13:15:33", "link": "http://arxiv.org/abs/2201.09651v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Relational Memory Augmented Language Models", "abstract": "We present a memory-augmented approach to condition an autoregressive\nlanguage model on a knowledge graph. We represent the graph as a collection of\nrelation triples and retrieve relevant relations for a given context to improve\ntext generation. Experiments on WikiText-103, WMT19, and enwik8 English\ndatasets demonstrate that our approach produces a better language model in\nterms of perplexity and bits per character. We also show that relational memory\nimproves coherence, is complementary to token-based memory, and enables causal\ninterventions. Our model provides a simple yet effective way to combine an\nautoregressive language model with a knowledge graph for a more coherent and\nlogical generation.", "published": "2022-01-24 13:25:41", "link": "http://arxiv.org/abs/2201.09680v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unified Question Generation with Continual Lifelong Learning", "abstract": "Question Generation (QG), as a challenging Natural Language Processing task,\naims at generating questions based on given answers and context. Existing QG\nmethods mainly focus on building or training models for specific QG datasets.\nThese works are subject to two major limitations: (1) They are dedicated to\nspecific QG formats (e.g., answer-extraction or multi-choice QG), therefore, if\nwe want to address a new format of QG, a re-design of the QG model is required.\n(2) Optimal performance is only achieved on the dataset they were just trained\non. As a result, we have to train and keep various QG models for different QG\ndatasets, which is resource-intensive and ungeneralizable.\n  To solve the problems, we propose a model named Unified-QG based on lifelong\nlearning techniques, which can continually learn QG tasks across different\ndatasets and formats. Specifically, we first build a format-convert encoding to\ntransform different kinds of QG formats into a unified representation. Then, a\nmethod named \\emph{STRIDER} (\\emph{S}imilari\\emph{T}y \\emph{R}egular\\emph{I}zed\n\\emph{D}ifficult \\emph{E}xample \\emph{R}eplay) is built to alleviate\ncatastrophic forgetting in continual QG learning. Extensive experiments were\nconducted on $8$ QG datasets across $4$ QG formats (answer-extraction,\nanswer-abstraction, multi-choice, and boolean QG) to demonstrate the\neffectiveness of our approach. Experimental results demonstrate that our\nUnified-QG can effectively and continually adapt to QG tasks when datasets and\nformats vary. In addition, we verify the ability of a single trained Unified-QG\nmodel in improving $8$ Question Answering (QA) systems' performance through\ngenerating synthetic QA data.", "published": "2022-01-24 14:05:18", "link": "http://arxiv.org/abs/2201.09696v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Collaborative Question Answering: A Preliminary Study", "abstract": "Knowledge and expertise in the real-world can be disjointedly owned. To solve\na complex question, collaboration among experts is often called for. In this\npaper, we propose CollabQA, a novel QA task in which several expert agents\ncoordinated by a moderator work together to answer questions that cannot be\nanswered with any single agent alone. We make a synthetic dataset of a large\nknowledge graph that can be distributed to experts. We define the process to\nform a complex question from ground truth reasoning path, neural network agent\nmodels that can learn to solve the task, and evaluation metrics to check the\nperformance. We show that the problem can be challenging without introducing\nprior of the collaboration structure, unless experts are perfect and uniform.\nBased on this experience, we elaborate extensions needed to approach\ncollaboration tasks in real-world settings.", "published": "2022-01-24 14:27:00", "link": "http://arxiv.org/abs/2201.09708v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Table Pre-training: A Survey on Model Architectures, Pre-training\n  Objectives, and Downstream Tasks", "abstract": "Since a vast number of tables can be easily collected from web pages,\nspreadsheets, PDFs, and various other document types, a flurry of table\npre-training frameworks have been proposed following the success of text and\nimages, and they have achieved new state-of-the-arts on various tasks such as\ntable question answering, table type recognition, column relation\nclassification, table search, formula prediction, etc. To fully use the\nsupervision signals in unlabeled tables, a variety of pre-training objectives\nhave been designed and evaluated, for example, denoising cell values,\npredicting numerical relationships, and implicitly executing SQLs. And to best\nleverage the characteristics of (semi-)structured tables, various tabular\nlanguage models, particularly with specially-designed attention mechanisms,\nhave been explored. Since tables usually appear and interact with free-form\ntext, table pre-training usually takes the form of table-text joint\npre-training, which attracts significant research interests from multiple\ndomains. This survey aims to provide a comprehensive review of different model\ndesigns, pre-training objectives, and downstream tasks for table pre-training,\nand we further share our thoughts and vision on existing challenges and future\nopportunities.", "published": "2022-01-24 15:22:24", "link": "http://arxiv.org/abs/2201.09745v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Classification Of Fake News Headline Based On Neural Networks", "abstract": "Over the last few years, Text classification is one of the fundamental tasks\nin natural language processing (NLP) in which the objective is to categorize\ntext documents into one of the predefined classes. The news is full of our\nlife. Therefore, news headlines classification is a crucial task to connect\nusers with the right news. The news headline classification is a kind of text\nclassification, which can be generally divided into three mainly parts: feature\nextraction, classifier selection, and evaluations. In this article, we use the\ndataset, containing news over a period of eighteen years provided by Kaggle\nplatform to classify news headlines. We choose TF-IDF to extract features and\nneural network as the classifier, while the evaluation metrics is accuracy.\nFrom the experiment result, it is obvious that our NN model has the best\nperformance among these models in the metrics of accuracy. The higher the\naccuracy is, the better performance the model will gain. Our NN model owns the\naccuracy 0.8622, which is highest accuracy among these four models. And it is\n0.0134, 0.033, 0.080 higher than its of other models.", "published": "2022-01-24 21:37:39", "link": "http://arxiv.org/abs/2201.09966v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HC4: A New Suite of Test Collections for Ad Hoc CLIR", "abstract": "HC4 is a new suite of test collections for ad hoc Cross-Language Information\nRetrieval (CLIR), with Common Crawl News documents in Chinese, Persian, and\nRussian, topics in English and in the document languages, and graded relevance\njudgments. New test collections are needed because existing CLIR test\ncollections built using pooling of traditional CLIR runs have systematic gaps\nin their relevance judgments when used to evaluate neural CLIR methods. The HC4\ncollections contain 60 topics and about half a million documents for each of\nChinese and Persian, and 54 topics and five million documents for Russian.\nActive learning was used to determine which documents to annotate after being\nseeded using interactive search and judgment. Documents were judged on a\nthree-grade relevance scale. This paper describes the design and construction\nof the new test collections and provides baseline results for demonstrating\ntheir utility for evaluating systems.", "published": "2022-01-24 22:52:11", "link": "http://arxiv.org/abs/2201.09992v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Text and Code Embeddings by Contrastive Pre-Training", "abstract": "Text embeddings are useful features in many applications such as semantic\nsearch and computing text similarity. Previous work typically trains models\ncustomized for different use cases, varying in dataset choice, training\nobjective and model architecture. In this work, we show that contrastive\npre-training on unsupervised data at scale leads to high quality vector\nrepresentations of text and code. The same unsupervised text embeddings that\nachieve new state-of-the-art results in linear-probe classification also\ndisplay impressive semantic search capabilities and sometimes even perform\ncompetitively with fine-tuned models. On linear-probe classification accuracy\naveraging over 7 tasks, our best unsupervised model achieves a relative\nimprovement of 4% and 1.8% over previous best unsupervised and supervised text\nembedding models respectively. The same text embeddings when evaluated on\nlarge-scale semantic search attains a relative improvement of 23.4%, 14.7%, and\n10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and\nTriviaQA benchmarks, respectively. Similarly to text embeddings, we train code\nembedding models on (text, code) pairs, obtaining a 20.8% relative improvement\nover prior best work on code search.", "published": "2022-01-24 23:36:20", "link": "http://arxiv.org/abs/2201.10005v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Multimodal Punctuation Restoration Framework for Mixed-Modality\n  Corpus", "abstract": "The punctuation restoration task aims to correctly punctuate the output\ntranscriptions of automatic speech recognition systems. Previous punctuation\nmodels, either using text only or demanding the corresponding audio, tend to be\nconstrained by real scenes, where unpunctuated sentences are a mixture of those\nwith and without audio. This paper proposes a unified multimodal punctuation\nrestoration framework, named UniPunc, to punctuate the mixed sentences with a\nsingle model. UniPunc jointly represents audio and non-audio samples in a\nshared latent space, based on which the model learns a hybrid representation\nand punctuates both kinds of samples. We validate the effectiveness of the\nUniPunc on real-world datasets, which outperforms various strong baselines\n(e.g. BERT, MuSe) by at least 0.8 overall F1 scores, making a new\nstate-of-the-art. Extensive experiments show that UniPunc's design is a\npervasive solution: by grafting onto previous models, UniPunc enables them to\npunctuate on the mixed corpus. Our code is available at\ngithub.com/Yaoming95/UniPunc", "published": "2022-01-24 10:15:53", "link": "http://arxiv.org/abs/2202.00468v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data and knowledge-driven approaches for multilingual training to\n  improve the performance of speech recognition systems of Indian languages", "abstract": "We propose data and knowledge-driven approaches for multilingual training of\nthe automated speech recognition (ASR) system for a target language by pooling\nspeech data from multiple source languages. Exploiting the acoustic\nsimilarities between Indian languages, we implement two approaches. In\nphone/senone mapping, deep neural network (DNN) learns to map senones or phones\nfrom one language to the others, and the transcriptions of the source languages\nare modified such that they can be used along with the target language data to\ntrain and fine-tune the target language ASR system. In the other approach, we\nmodel the acoustic information for all the languages simultaneously by training\na multitask DNN (MTDNN) to predict the senones of each language in different\noutput layers. The cross-entropy loss and the weight update procedure are\nmodified such that only the shared layers and the output layer responsible for\npredicting the senone classes of a language are updated during training, if the\nfeature vector belongs to that particular language. In the low-resource setting\n(LRS), 40 hours of transcribed data each for Tamil, Telugu and Gujarati\nlanguages are used for training. The DNN based senone mapping technique gives\nrelative improvements in word error rates (WER) of 9.66%, 7.2% and 15.21% over\nthe baseline system for Tamil, Gujarati and Telugu languages, respectively. In\nmedium-resourced setting (MRS), 160, 275 and 135 hours of data for Tamil,\nKannada and Hindi languages are used, where, the same technique gives better\nrelative improvements of 13.94%, 10.28% and 27.24% for Tamil, Kannada and\nHindi, respectively. The MTDNN with senone mapping based training in LRS, gives\nhigher relative WER improvements of 15.0%, 17.54% and 16.06%, respectively for\nTamil, Gujarati and Telugu, whereas in MRS, we see improvements of 21.24%\n21.05% and 30.17% for Tamil, Kannada and Hindi languages, respectively.", "published": "2022-01-24 07:17:17", "link": "http://arxiv.org/abs/2201.09494v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Knowledge Graph Embeddings based Approach for Author Name\n  Disambiguation using Literals", "abstract": "Scholarly data is growing continuously containing information about the\narticles from a plethora of venues including conferences, journals, etc. Many\ninitiatives have been taken to make scholarly data available as Knowledge\nGraphs (KGs). These efforts to standardize these data and make them accessible\nhave also led to many challenges such as exploration of scholarly articles,\nambiguous authors, etc. This study more specifically targets the problem of\nAuthor Name Disambiguation (AND) on Scholarly KGs and presents a novel\nframework, Literally Author Name Disambiguation (LAND), which utilizes\nKnowledge Graph Embeddings (KGEs) using multimodal literal information\ngenerated from these KGs. This framework is based on three components: 1)\nMultimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical\nAgglomerative Clustering. Extensive experiments have been conducted on two\nnewly created KGs: (i) KG containing information from Scientometrics Journal\nfrom 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known\nbenchmark for AND provided by AMiner (AMiner-534K). The results show that our\nproposed architecture outperforms our baselines of 8-14% in terms of the F1\nscore and shows competitive performances on a challenging benchmark such as\nAMiner. The code and the datasets are publicly available through Github:\nhttps://github.com/sntcristian/and-kge and\nZenodo:https://doi.org/10.5281/zenodo.6309855 respectively.", "published": "2022-01-24 09:57:24", "link": "http://arxiv.org/abs/2201.09555v3", "categories": ["cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.AI"}
{"title": "PickNet: Real-Time Channel Selection for Ad Hoc Microphone Arrays", "abstract": "This paper proposes PickNet, a neural network model for real-time channel\nselection for an ad hoc microphone array consisting of multiple recording\ndevices like cell phones. Assuming at most one person to be vocally active at\neach time point, PickNet identifies the device that is spatially closest to the\nactive person for each time frame by using a short spectral patch of just\nhundreds of milliseconds. The model is applied to every time frame, and the\nshort time frame signals from the selected microphones are concatenated across\nthe frames to produce an output signal. As the personal devices are usually\nheld close to their owners, the output signal is expected to have higher\nsignal-to-noise and direct-to-reverberation ratios on average than the input\nsignals. Since PickNet utilizes only limited acoustic context at each time\nframe, the system using the proposed model works in real time and is robust to\nchanges in acoustic conditions. Speech recognition-based evaluation was carried\nout by using real conversational recordings obtained with various smartphones.\nThe proposed model yielded significant gains in word error rate with limited\ncomputational cost over systems using a block-online beamformer and a single\ndistant microphone.", "published": "2022-01-24 10:52:43", "link": "http://arxiv.org/abs/2201.09586v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bias in Automated Speaker Recognition", "abstract": "Automated speaker recognition uses data processing to identify speakers by\ntheir voice. Today, automated speaker recognition is deployed on billions of\nsmart devices and in services such as call centres. Despite their wide-scale\ndeployment and known sources of bias in related domains like face recognition\nand natural language processing, bias in automated speaker recognition has not\nbeen studied systematically. We present an in-depth empirical and analytical\nstudy of bias in the machine learning development workflow of speaker\nverification, a voice biometric and core task in automated speaker recognition.\nDrawing on an established framework for understanding sources of harm in\nmachine learning, we show that bias exists at every development stage in the\nwell-known VoxCeleb Speaker Recognition Challenge, including data generation,\nmodel building, and implementation. Most affected are female speakers and\nnon-US nationalities, who experience significant performance degradation.\nLeveraging the insights from our findings, we make practical recommendations\nfor mitigating bias in automated speaker recognition, and outline future\nresearch directions.", "published": "2022-01-24 06:48:57", "link": "http://arxiv.org/abs/2201.09486v2", "categories": ["cs.SD", "cs.CL", "cs.CY", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Variational Auto-Encoder Based Variability Encoding for Dysarthric\n  Speech Recognition", "abstract": "Dysarthric speech recognition is a challenging task due to acoustic\nvariability and limited amount of available data. Diverse conditions of\ndysarthric speakers account for the acoustic variability, which make the\nvariability difficult to be modeled precisely. This paper presents a\nvariational auto-encoder based variability encoder (VAEVE) to explicitly encode\nsuch variability for dysarthric speech. The VAEVE makes use of both phoneme\ninformation and low-dimensional latent variable to reconstruct the input\nacoustic features, thereby the latent variable is forced to encode the\nphoneme-independent variability. Stochastic gradient variational Bayes\nalgorithm is applied to model the distribution for generating variability\nencodings, which are further used as auxiliary features for DNN acoustic\nmodeling. Experiment results conducted on the UASpeech corpus show that the\nVAEVE based variability encodings have complementary effect to the learning\nhidden unit contributions (LHUC) speaker adaptation. The systems using\nvariability encodings consistently outperform the comparable baseline systems\nwithout using them, and\" obtain absolute word error rate (WER) reduction by up\nto 2.2% on dysarthric speech with \"Very lowintelligibility level, and up to 2%\non the \"Mixed\" type of dysarthric speech with diverse or uncertain conditions.", "published": "2022-01-24 02:35:42", "link": "http://arxiv.org/abs/2201.09422v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Polyphone disambiguation and accent prediction using pre-trained\n  language models in Japanese TTS front-end", "abstract": "Although end-to-end text-to-speech (TTS) models can generate natural speech,\nchallenges still remain when it comes to estimating sentence-level phonetic and\nprosodic information from raw text in Japanese TTS systems. In this paper, we\npropose a method for polyphone disambiguation (PD) and accent prediction (AP).\nThe proposed method incorporates explicit features extracted from morphological\nanalysis and implicit features extracted from pre-trained language models\n(PLMs). We use BERT and Flair embeddings as implicit features and examine how\nto combine them with explicit features. Our objective evaluation results showed\nthat the proposed method improved the accuracy by 5.7 points in PD and 6.0\npoints in AP. Moreover, the perceptual listening test results confirmed that a\nTTS system employing our proposed model as a front-end achieved a mean opinion\nscore close to that of synthesized speech with ground-truth pronunciation and\naccent in terms of naturalness.", "published": "2022-01-24 02:49:35", "link": "http://arxiv.org/abs/2201.09427v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigation of Deep Neural Network Acoustic Modelling Approaches for\n  Low Resource Accented Mandarin Speech Recognition", "abstract": "The Mandarin Chinese language is known to be strongly influenced by a rich\nset of regional accents, while Mandarin speech with each accent is quite low\nresource. Hence, an important task in Mandarin speech recognition is to\nappropriately model the acoustic variabilities imposed by accents. In this\npaper, an investigation of implicit and explicit use of accent information on a\nrange of deep neural network (DNN) based acoustic modelling techniques is\nconducted. Meanwhile, approaches of multi-accent modelling including\nmulti-style training, multi-accent decision tree state tying, DNN tandem and\nmulti-level adaptive network (MLAN) tandem hidden Markov model (HMM) modelling\nare combined and compared in this paper. On a low resource accented Mandarin\nspeech recognition task consisting of four regional accents, an improved MLAN\ntandem HMM systems explicitly leveraging the accent information was proposed\nand significantly outperformed the baseline accent independent DNN tandem\nsystems by 0.8%-1.5% absolute (6%-9% relative) in character error rate after\nsequence level discriminative training and adaptation.", "published": "2022-01-24 03:18:38", "link": "http://arxiv.org/abs/2201.09432v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Disentangling Style and Speaker Attributes for TTS Style Transfer", "abstract": "End-to-end neural TTS has shown improved performance in speech style\ntransfer. However, the improvement is still limited by the available training\ndata in both target styles and speakers. Additionally, degenerated performance\nis observed when the trained TTS tries to transfer the speech to a target style\nfrom a new speaker with an unknown, arbitrary style. In this paper, we propose\na new approach to seen and unseen style transfer training on disjoint,\nmulti-style datasets, i.e., datasets of different styles are recorded, one\nindividual style by one speaker in multiple utterances. An inverse\nautoregressive flow (IAF) technique is first introduced to improve the\nvariational inference for learning an expressive style representation. A\nspeaker encoder network is then developed for learning a discriminative speaker\nembedding, which is jointly trained with the rest neural TTS modules. The\nproposed approach of seen and unseen style transfer is effectively trained with\nsix specifically-designed objectives: reconstruction loss, adversarial loss,\nstyle distortion loss, cycle consistency loss, style classification loss, and\nspeaker classification loss. Experiments demonstrate, both objectively and\nsubjectively, the effectiveness of the proposed approach for seen and unseen\nstyle transfer tasks. The performance of our approach is superior to and more\nrobust than those of four other reference systems of prior art.", "published": "2022-01-24 06:05:20", "link": "http://arxiv.org/abs/2201.09472v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Factored Hybrid HMM Acoustic Modeling without State Tying", "abstract": "In this work, we show that a factored hybrid hidden Markov model (FH-HMM)\nwhich is defined without any phonetic state-tying outperforms a\nstate-of-the-art hybrid HMM. The factored hybrid HMM provides a link to\ntransducer models in the way it models phonetic (label) context while\npreserving the strict separation of acoustic and language model of the hybrid\nHMM approach. Furthermore, we show that the factored hybrid model can be\ntrained from scratch without using phonetic state-tying in any of the training\nsteps. Our modeling approach enables triphone context while avoiding phonetic\nstate-tying by a decomposition into locally normalized factored posteriors for\nmonophones/HMM states in phoneme context. Experimental results are provided for\nSwitchboard 300h and LibriSpeech. On the former task we also show that by\navoiding the phonetic state-tying step, the factored hybrid can take better\nadvantage of regularization techniques during training, compared to the\nstandard hybrid HMM with phonetic state-tying based on classification and\nregression trees (CART).", "published": "2022-01-24 13:55:06", "link": "http://arxiv.org/abs/2201.09692v1", "categories": ["cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.SD"}
{"title": "A Bayesian Permutation training deep representation learning method for\n  speech enhancement with variational autoencoder", "abstract": "Recently, variational autoencoder (VAE), a deep representation learning (DRL)\nmodel, has been used to perform speech enhancement (SE). However, to the best\nof our knowledge, current VAE-based SE methods only apply VAE to the model\nspeech signal, while noise is modeled using the traditional non-negative matrix\nfactorization (NMF) model. One of the most important reasons for using NMF is\nthat these VAE-based methods cannot disentangle the speech and noise latent\nvariables from the observed signal. Based on Bayesian theory, this paper\nderives a novel variational lower bound for VAE, which ensures that VAE can be\ntrained in supervision, and can disentangle speech and noise latent variables\nfrom the observed signal. This means that the proposed method can apply the VAE\nto model both speech and noise signals, which is totally different from the\nprevious VAE-based SE works. More specifically, the proposed DRL method can\nlearn to impose speech and noise signal priors to different sets of latent\nvariables for SE. The experimental results show that the proposed method can\nnot only disentangle speech and noise latent variables from the observed signal\nbut also obtain a higher scale-invariant signal-to-distortion ratio and speech\nquality score than the similar deep neural network-based (DNN) SE method.", "published": "2022-01-24 18:51:04", "link": "http://arxiv.org/abs/2201.09875v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Novel Temporal Attentive-Pooling based Convolutional Recurrent\n  Architecture for Acoustic Signal Enhancement", "abstract": "In acoustic signal processing, the target signals usually carry semantic\ninformation, which is encoded in a hierarchal structure of short and long-term\ncontexts. However, the background noise distorts these structures in a\nnonuniform way. The existing deep acoustic signal enhancement (ASE)\narchitectures ignore this kind of local and global effect. To address this\nproblem, we propose to integrate a novel temporal attentive-pooling (TAP)\nmechanism into a conventional convolutional recurrent neural network, termed as\nTAP-CRNN. The proposed approach considers both global and local attention for\nASE tasks. Specifically, we first utilize a convolutional layer to extract\nlocal information of the acoustic signals and then a recurrent neural network\n(RNN) architecture is used to characterize temporal contextual information.\nSecond, we exploit a novelattention mechanism to contextually process salient\nregions of the noisy signals. The proposed ASE system is evaluated using a\nbenchmark infant cry dataset and compared with several well-known methods. It\nis shown that the TAPCRNN can more effectively reduce noise components from\ninfant cry signals in unseen background noises at challenging signal-to-noise\nlevels.", "published": "2022-01-24 19:13:44", "link": "http://arxiv.org/abs/2201.09913v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Microphone Utility Estimation in Acoustic Sensor Networks using\n  Single-Channel Signal Features", "abstract": "In multichannel signal processing with distributed sensors, choosing the\noptimal subset of observed sensor signals to be exploited is crucial in order\nto maximize algorithmic performance and reduce computational load, ideally both\nat the same time. In the acoustic domain, signal cross-correlation is a natural\nchoice to quantify the usefulness of microphone signals, i.e., microphone\nutility, for array processing, but its estimation requires that the uncoded\nsignals are synchronized and transmitted between nodes. In resource-constrained\nenvironments like acoustic sensor networks, low data transmission rates often\nmake transmission of all observed signals to the centralized location\ninfeasible, thus discouraging direct estimation of signal cross-correlation.\nInstead, we employ characteristic features of the recorded signals to estimate\nthe usefulness of individual microphone signals. In this contribution, we\nprovide a comprehensive analysis of model-based microphone utility estimation\napproaches that use signal features and, as an alternative, also propose\nmachine learning-based estimation methods that identify optimal sensor signal\nutility features. The performance of both approaches is validated\nexperimentally using both simulated and recorded acoustic data, comprising a\nvariety of realistic and practically relevant acoustic scenarios including\nmoving and static sources.", "published": "2022-01-24 20:51:04", "link": "http://arxiv.org/abs/2201.09946v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Endpoint Detection for Streaming End-to-End Multi-talker ASR", "abstract": "Streaming end-to-end multi-talker speech recognition aims at transcribing the\noverlapped speech from conversations or meetings with an all-neural model in a\nstreaming fashion, which is fundamentally different from a modular-based\napproach that usually cascades the speech separation and the speech recognition\nmodels trained independently. Previously, we proposed the Streaming Unmixing\nand Recognition Transducer (SURT) model based on recurrent neural network\ntransducer (RNN-T) for this problem and presented promising results. However,\nfor real applications, the speech recognition system is also required to\ndetermine the timestamp when a speaker finishes speaking for prompt system\nresponse. This problem, known as endpoint (EP) detection, has not been studied\npreviously for multi-talker end-to-end models. In this work, we address the EP\ndetection problem in the SURT framework by introducing an end-of-sentence token\nas an output unit, following the practice of single-talker end-to-end models.\nFurthermore, we also present a latency penalty approach that can significantly\ncut down the EP detection latency. Our experimental results based on the\n2-speaker LibrispeechMix dataset show that the SURT model can achieve promising\nEP detection without significantly degradation of the recognition accuracy.", "published": "2022-01-24 22:17:20", "link": "http://arxiv.org/abs/2201.09979v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Neural Speech Coding for Real-Time Communications", "abstract": "Deep-learning based methods have shown their advantages in audio coding over\ntraditional ones but limited attention has been paid on real-time\ncommunications (RTC). This paper proposes the TFNet, an end-to-end neural\nspeech codec with low latency for RTC. It takes an encoder-temporal\nfiltering-decoder paradigm that has seldom been investigated in audio coding.\nAn interleaved structure is proposed for temporal filtering to capture both\nshort-term and long-term temporal dependencies. Furthermore, with end-to-end\noptimization, the TFNet is jointly optimized with speech enhancement and packet\nloss concealment, yielding a one-for-all network for three tasks. Both\nsubjective and objective results demonstrate the efficiency of the proposed\nTFNet.", "published": "2022-01-24 03:06:30", "link": "http://arxiv.org/abs/2201.09429v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Synthetic speech detection using meta-learning with prototypical loss", "abstract": "Recent works on speech spoofing countermeasures still lack generalization\nability to unseen spoofing attacks. This is one of the key issues of ASVspoof\nchallenges especially with the rapid development of diverse and high-quality\nspoofing algorithms. In this work, we address the generalizability of spoofing\ndetection by proposing prototypical loss under the meta-learning paradigm to\nmimic the unseen test scenario during training. Prototypical loss with\nmetric-learning objectives can learn the embedding space directly and emerges\nas a strong alternative to prevailing classification loss functions. We propose\nan anti-spoofing system based on squeeze-excitation Residual network\n(SE-ResNet) architecture with prototypical loss. We demonstrate that the\nproposed single system without any data augmentation can achieve competitive\nperformance to the recent best anti-spoofing systems on ASVspoof 2019 logical\naccess (LA) task. Furthermore, the proposed system with data augmentation\noutperforms the ASVspoof 2021 challenge best baseline both in the progress and\nevaluation phase of the LA task. On ASVspoof 2019 and 2021 evaluation set LA\nscenario, we attain a relative 68.4% and 3.6% improvement in min-tDCF compared\nto the challenge best baselines, respectively.", "published": "2022-01-24 06:01:06", "link": "http://arxiv.org/abs/2201.09470v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Music Source Separation Using Differentiable Parametric\n  Source Models", "abstract": "Supervised deep learning approaches to underdetermined audio source\nseparation achieve state-of-the-art performance but require a dataset of\nmixtures along with their corresponding isolated source signals. Such datasets\ncan be extremely costly to obtain for musical mixtures. This raises a need for\nunsupervised methods. We propose a novel unsupervised model-based deep learning\napproach to musical source separation. Each source is modelled with a\ndifferentiable parametric source-filter model. A neural network is trained to\nreconstruct the observed mixture as a sum of the sources by estimating the\nsource models' parameters given their fundamental frequencies. At test time,\nsoft masks are obtained from the synthesized source signals. The experimental\nevaluation on a vocal ensemble separation task shows that the proposed method\noutperforms learning-free methods based on nonnegative matrix factorization and\na supervised deep learning baseline. Integrating domain knowledge in the form\nof source models into a data-driven method leads to high data efficiency: the\nproposed approach achieves good separation quality even when trained on less\nthan three minutes of audio. This work makes powerful deep learning based\nseparation usable in scenarios where training data with ground truth is\nexpensive or nonexistent.", "published": "2022-01-24 11:05:30", "link": "http://arxiv.org/abs/2201.09592v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimizing Tandem Speaker Verification and Anti-Spoofing Systems", "abstract": "As automatic speaker verification (ASV) systems are vulnerable to spoofing\nattacks, they are typically used in conjunction with spoofing countermeasure\n(CM) systems to improve security. For example, the CM can first determine\nwhether the input is human speech, then the ASV can determine whether this\nspeech matches the speaker's identity. The performance of such a tandem system\ncan be measured with a tandem detection cost function (t-DCF). However, ASV and\nCM systems are usually trained separately, using different metrics and data,\nwhich does not optimize their combined performance. In this work, we propose to\noptimize the tandem system directly by creating a differentiable version of\nt-DCF and employing techniques from reinforcement learning. The results\nindicate that these approaches offer better outcomes than finetuning, with our\nmethod providing a 20% relative improvement in the t-DCF in the ASVSpoof19\ndataset in a constrained setting.", "published": "2022-01-24 14:27:28", "link": "http://arxiv.org/abs/2201.09709v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards a Real-time Measure of the Perception of Anthropomorphism in\n  Human-robot Interaction", "abstract": "How human-like do conversational robots need to look to enable long-term\nhuman-robot conversation? One essential aspect of long-term interaction is a\nhuman's ability to adapt to the varying degrees of a conversational partner's\nengagement and emotions. Prosodically, this can be achieved through\n(dis)entrainment. While speech-synthesis has been a limiting factor for many\nyears, restrictions in this regard are increasingly mitigated. These\nadvancements now emphasise the importance of studying the effect of robot\nembodiment on human entrainment. In this study, we conducted a between-subjects\nonline human-robot interaction experiment in an educational use-case scenario\nwhere a tutor was either embodied through a human or a robot face. 43\nEnglish-speaking participants took part in the study for whom we analysed the\ndegree of acoustic-prosodic entrainment to the human or robot face,\nrespectively. We found that the degree of subjective and objective perception\nof anthropomorphism positively correlates with acoustic-prosodic entrainment.", "published": "2022-01-24 11:10:37", "link": "http://arxiv.org/abs/2201.09595v1", "categories": ["cs.HC", "cs.AI", "cs.RO", "eess.AS", "eess.SP"], "primary_category": "cs.HC"}
