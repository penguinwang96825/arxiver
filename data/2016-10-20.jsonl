{"title": "Lexicon Integrated CNN Models with Attention for Sentiment Analysis", "abstract": "With the advent of word embeddings, lexicons are no longer fully utilized for\nsentiment analysis although they still provide important features in the\ntraditional setting. This paper introduces a novel approach to sentiment\nanalysis that integrates lexicon embeddings and an attention mechanism into\nConvolutional Neural Networks. Our approach performs separate convolutions for\nword and lexicon embeddings and provides a global view of the document using\nattention. Our models are experimented on both the SemEval'16 Task 4 dataset\nand the Stanford Sentiment Treebank, and show comparative or better results\nagainst the existing state-of-the-art systems. Our analysis shows that lexicon\nembeddings allow to build high-performing models with much smaller word\nembeddings, and the attention mechanism effectively dims out noisy words for\nsentiment analysis.", "published": "2016-10-20 03:10:57", "link": "http://arxiv.org/abs/1610.06272v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Authorship Attribution Based on Life-Like Network Automata", "abstract": "The authorship attribution is a problem of considerable practical and\ntechnical interest. Several methods have been designed to infer the authorship\nof disputed documents in multiple contexts. While traditional statistical\nmethods based solely on word counts and related measurements have provided a\nsimple, yet effective solution in particular cases; they are prone to\nmanipulation. Recently, texts have been successfully modeled as networks, where\nwords are represented by nodes linked according to textual similarity\nmeasurements. Such models are useful to identify informative topological\npatterns for the authorship recognition task. However, there is no consensus on\nwhich measurements should be used. Thus, we proposed a novel method to\ncharacterize text networks, by considering both topological and dynamical\naspects of networks. Using concepts and methods from cellular automata theory,\nwe devised a strategy to grasp informative spatio-temporal patterns from this\nmodel. Our experiments revealed an outperformance over traditional analysis\nrelying only on topological measurements. Remarkably, we have found a\ndependence of pre-processing steps (such as the lemmatization) on the obtained\nresults, a feature that has mostly been disregarded in related works. The\noptimized results obtained here pave the way for a better characterization of\ntextual networks.", "published": "2016-10-20 17:00:42", "link": "http://arxiv.org/abs/1610.06498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning variable length units for SMT between related languages via\n  Byte Pair Encoding", "abstract": "We explore the use of segments learnt using Byte Pair Encoding (referred to\nas BPE units) as basic units for statistical machine translation between\nrelated languages and compare it with orthographic syllables, which are\ncurrently the best performing basic units for this translation task. BPE\nidentifies the most frequent character sequences as basic units, while\northographic syllables are linguistically motivated pseudo-syllables. We show\nthat BPE units modestly outperform orthographic syllables as units of\ntranslation, showing up to 11% increase in BLEU score. While orthographic\nsyllables can be used only for languages whose writing systems use vowel\nrepresentations, BPE is writing system independent and we show that BPE\noutperforms other units for non-vowel writing systems too. Our results are\nsupported by extensive experimentation spanning multiple language families and\nwriting systems.", "published": "2016-10-20 17:32:32", "link": "http://arxiv.org/abs/1610.06510v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexicons and Minimum Risk Training for Neural Machine Translation:\n  NAIST-CMU at WAT2016", "abstract": "This year, the Nara Institute of Science and Technology (NAIST)/Carnegie\nMellon University (CMU) submission to the Japanese-English translation track of\nthe 2016 Workshop on Asian Translation was based on attentional neural machine\ntranslation (NMT) models. In addition to the standard NMT model, we make a\nnumber of improvements, most notably the use of discrete translation lexicons\nto improve probability estimates, and the use of minimum risk training to\noptimize the MT system for BLEU score. As a result, our system achieved the\nhighest translation evaluation scores for the task.", "published": "2016-10-20 19:10:09", "link": "http://arxiv.org/abs/1610.06542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Characters and Hierarchical Encoding", "abstract": "Most existing Neural Machine Translation models use groups of characters or\nwhole words as their unit of input and output. We propose a model with a\nhierarchical char2word encoder, that takes individual characters both as input\nand output. We first argue that this hierarchical representation of the\ncharacter encoder reduces computational complexity, and show that it improves\ntranslation performance. Secondly, by qualitatively studying attention plots\nfrom the decoder we find that the model learns to compress common words into a\nsingle embedding whereas rare words, such as names and places, are represented\ncharacter by character.", "published": "2016-10-20 19:33:02", "link": "http://arxiv.org/abs/1610.06550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Refinement for Machine Translation", "abstract": "Existing machine translation decoding algorithms generate translations in a\nstrictly monotonic fashion and never revisit previous decisions. As a result,\nearlier mistakes cannot be corrected at a later stage. In this paper, we\npresent a translation scheme that starts from an initial guess and then makes\niterative improvements that may revisit previous decisions. We parameterize our\nmodel as a convolutional neural network that predicts discrete substitutions to\nan existing translation based on an attention mechanism over both the source\nsentence as well as the current translation output. By making less than one\nmodification per sentence, we improve the output of a phrase-based translation\nsystem by up to 0.4 BLEU on WMT15 German-English translation.", "published": "2016-10-20 20:54:07", "link": "http://arxiv.org/abs/1610.06602v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Learning to Align and Convert Graphemes to Phonemes with Neural\n  Attention Models", "abstract": "We propose an attention-enabled encoder-decoder model for the problem of\ngrapheme-to-phoneme conversion. Most previous work has tackled the problem via\njoint sequence models that require explicit alignments for training. In\ncontrast, the attention-enabled encoder-decoder model allows for jointly\nlearning to align and convert characters to phonemes. We explore different\ntypes of attention models, including global and local attention, and our best\nmodels achieve state-of-the-art results on three standard data sets (CMUDict,\nPronlex, and NetTalk).", "published": "2016-10-20 19:00:48", "link": "http://arxiv.org/abs/1610.06540v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Clinical Text Prediction with Numerically Grounded Conditional Language\n  Models", "abstract": "Assisted text input techniques can save time and effort and improve text\nquality. In this paper, we investigate how grounded and conditional extensions\nto standard neural language models can bring improvements in the tasks of word\nprediction and completion. These extensions incorporate a structured knowledge\nbase and numerical values from the text into the context used to predict the\nnext word. Our automated evaluation on a clinical dataset shows extended models\nsignificantly outperform standard models. Our best system uses both\nconditioning and grounding, because of their orthogonal benefits. For word\nprediction with a list of 5 suggestions, it improves recall from 25.03% to\n71.28% and for word completion it improves keystroke savings from 34.35% to\n44.81%, where theoretical bound for this dataset is 58.78%. We also perform a\nqualitative investigation of how models with lower perplexity occasionally fare\nbetter at the tasks. We found that at test time numbers have more influence on\nthe document level than on individual word probabilities.", "published": "2016-10-20 11:48:30", "link": "http://arxiv.org/abs/1610.06370v1", "categories": ["cs.CL", "cs.HC", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Reasoning with Memory Augmented Neural Networks for Language\n  Comprehension", "abstract": "Hypothesis testing is an important cognitive process that supports human\nreasoning. In this paper, we introduce a computational hypothesis testing\napproach based on memory augmented neural networks. Our approach involves a\nhypothesis testing loop that reconsiders and progressively refines a previously\nformed hypothesis in order to generate new hypotheses to test. We apply the\nproposed approach to language comprehension task by using Neural Semantic\nEncoders (NSE). Our NSE models achieve the state-of-the-art results showing an\nabsolute improvement of 1.2% to 2.6% accuracy over previous results obtained by\nsingle and ensemble systems on standard machine comprehension benchmarks such\nas the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.", "published": "2016-10-20 15:17:04", "link": "http://arxiv.org/abs/1610.06454v2", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Proposing Plausible Answers for Open-ended Visual Question Answering", "abstract": "Answering open-ended questions is an essential capability for any intelligent\nagent. One of the most interesting recent open-ended question answering\nchallenges is Visual Question Answering (VQA) which attempts to evaluate a\nsystem's visual understanding through its answers to natural language questions\nabout images. There exist many approaches to VQA, the majority of which do not\nexhibit deeper semantic understanding of the candidate answers they produce. We\nstudy the importance of generating plausible answers to a given question by\nintroducing the novel task of `Answer Proposal': for a given open-ended\nquestion, a system should generate a ranked list of candidate answers informed\nby the semantics of the question. We experiment with various models including a\nneural generative model as well as a semantic graph matching one. We provide\nboth intrinsic and extrinsic evaluations for the task of Answer Proposal,\nshowing that our best model learns to propose plausible answers with a high\nrecall and performs competitively with some other solutions to VQA.", "published": "2016-10-20 22:01:36", "link": "http://arxiv.org/abs/1610.06620v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
