{"title": "Formalized Lambek Calculus in Higher Order Logic (HOL4)", "abstract": "In this project, a rather complete proof-theoretical formalization of Lambek\nCalculus (non-associative with arbitrary extensions) has been ported from Coq\nproof assistent to HOL4 theorem prover, with some improvements and new\ntheorems.\n  Three deduction systems (Syntactic Calculus, Natural Deduction and Sequent\nCalculus) of Lambek Calculus are defined with many related theorems proved. The\nequivalance between these systems are formally proved. Finally, a formalization\nof Sequent Calculus proofs (where Coq has built-in supports) has been designed\nand implemented in HOL4. Some basic results including the sub-formula\nproperties of the so-called \"cut-free\" proofs are formally proved.\n  This work can be considered as the preliminary work towards a language parser\nbased on category grammars which is not multimodal but still has ability to\nsupport context-sensitive languages through customized extensions.", "published": "2017-05-20 15:04:34", "link": "http://arxiv.org/abs/1705.07318v1", "categories": ["cs.CL", "cs.LO", "D.2.4, I.2.7"], "primary_category": "cs.CL"}
{"title": "Search Engine Guided Non-Parametric Neural Machine Translation", "abstract": "In this paper, we extend an attention-based neural machine translation (NMT)\nmodel by allowing it to access an entire training set of parallel sentence\npairs even after training. The proposed approach consists of two stages. In the\nfirst stage--retrieval stage--, an off-the-shelf, black-box search engine is\nused to retrieve a small subset of sentence pairs from a training set given a\nsource sentence. These pairs are further filtered based on a fuzzy matching\nscore based on edit distance. In the second stage--translation stage--, a novel\ntranslation model, called translation memory enhanced NMT (TM-NMT), seamlessly\nuses both the source sentence and a set of retrieved sentence pairs to perform\nthe translation. Empirical evaluation on three language pairs (En-Fr, En-De,\nand En-Es) shows that the proposed approach significantly outperforms the\nbaseline approach and the improvement is more significant when more relevant\nsentence pairs were retrieved.", "published": "2017-05-20 06:53:09", "link": "http://arxiv.org/abs/1705.07267v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mixed Membership Word Embeddings for Computational Social Science", "abstract": "Word embeddings improve the performance of NLP systems by revealing the\nhidden structural relationships between words. Despite their success in many\napplications, word embeddings have seen very little use in computational social\nscience NLP tasks, presumably due to their reliance on big data, and to a lack\nof interpretability. I propose a probabilistic model-based word embedding\nmethod which can recover interpretable embeddings, without big data. The key\ninsight is to leverage mixed membership modeling, in which global\nrepresentations are shared, but individual entities (i.e. dictionary words) are\nfree to use these representations to uniquely differing degrees. I show how to\ntrain the model using a combination of state-of-the-art training techniques for\nword embeddings and topic models. The experimental results show an improvement\nin predictive language modeling of up to 63% in MRR over the skip-gram, and\ndemonstrate that the representations are beneficial for supervised learning. I\nillustrate the interpretability of the models with computational social science\ncase studies on State of the Union addresses and NIPS articles.", "published": "2017-05-20 23:45:54", "link": "http://arxiv.org/abs/1705.07368v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
