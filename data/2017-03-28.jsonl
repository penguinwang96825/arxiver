{"title": "Learning Similarity Functions for Pronunciation Variations", "abstract": "A significant source of errors in Automatic Speech Recognition (ASR) systems\nis due to pronunciation variations which occur in spontaneous and\nconversational speech. Usually ASR systems use a finite lexicon that provides\none or more pronunciations for each word. In this paper, we focus on learning a\nsimilarity function between two pronunciations. The pronunciations can be the\ncanonical and the surface pronunciations of the same word or they can be two\nsurface pronunciations of different words. This task generalizes problems such\nas lexical access (the problem of learning the mapping between words and their\npossible pronunciations), and defining word neighborhoods. It can also be used\nto dynamically increase the size of the pronunciation lexicon, or in predicting\nASR errors. We propose two methods, which are based on recurrent neural\nnetworks, to learn the similarity function. The first is based on binary\nclassification, and the second is based on learning the ranking of the\npronunciations. We demonstrate the efficiency of our approach on the task of\nlexical access using a subset of the Switchboard conversational speech corpus.\nResults suggest that on this task our methods are superior to previous methods\nwhich are based on graphical Bayesian methods.", "published": "2017-03-28 21:47:16", "link": "http://arxiv.org/abs/1703.09817v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Affective Meaning Lexicon Expansion Using Semantic and\n  Distributed Word Representations", "abstract": "In this paper, we propose an extension to graph-based sentiment lexicon\ninduction methods by incorporating distributed and semantic word\nrepresentations in building the similarity graph to expand a three-dimensional\nsentiment lexicon. We also implemented and evaluated the label propagation\nusing four different word representations and similarity metrics. Our\ncomprehensive evaluation of the four approaches was performed on a single data\nset, demonstrating that all four methods can generate a significant number of\nnew sentiment assignments with high accuracy. The highest correlations\n(tau=0.51) and the lowest error (mean absolute error < 1.1%), obtained by\ncombining both the semantic and the distributional features, outperformed the\ndistributional-based and semantic-based label-propagation models and approached\na supervised algorithm.", "published": "2017-03-28 22:05:20", "link": "http://arxiv.org/abs/1703.09825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive\n  Content in Text Body, More Similar to Satire than Real News", "abstract": "The problem of fake news has gained a lot of attention as it is claimed to\nhave had a significant impact on 2016 US Presidential Elections. Fake news is\nnot a new problem and its spread in social networks is well-studied. Often an\nunderlying assumption in fake news discussion is that it is written to look\nlike real news, fooling the reader who does not check for reliability of the\nsources or the arguments in its content. Through a unique study of three data\nsets and features that capture the style and the language of articles, we show\nthat this assumption is not true. Fake news in most cases is more similar to\nsatire than to real news, leading us to conclude that persuasion in fake news\nis achieved through heuristics rather than the strength of arguments. We show\noverall title structure and the use of proper nouns in titles are very\nsignificant in differentiating fake from real. This leads us to conclude that\nfake news is targeted for audiences who are not likely to read beyond titles\nand is aimed at creating mental associations between entities and claims.", "published": "2017-03-28 04:47:11", "link": "http://arxiv.org/abs/1703.09398v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Diving Deep into Clickbaits: Who Use Them to What Extents in Which\n  Topics with What Effects?", "abstract": "The use of alluring headlines (clickbait) to tempt the readers has become a\ngrowing practice nowadays. For the sake of existence in the highly competitive\nmedia industry, most of the on-line media including the mainstream ones, have\nstarted following this practice. Although the wide-spread practice of clickbait\nmakes the reader's reliability on media vulnerable, a large scale analysis to\nreveal this fact is still absent. In this paper, we analyze 1.67 million\nFacebook posts created by 153 media organizations to understand the extent of\nclickbait practice, its impact and user engagement by using our own developed\nclickbait detection model. The model uses distributed sub-word embeddings\nlearned from a large corpus. The accuracy of the model is 98.3%. Powered with\nthis model, we further study the distribution of topics in clickbait and\nnon-clickbait contents.", "published": "2017-03-28 05:07:38", "link": "http://arxiv.org/abs/1703.09400v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A practical approach to dialogue response generation in closed domains", "abstract": "We describe a prototype dialogue response generation model for the customer\nservice domain at Amazon. The model, which is trained in a weakly supervised\nfashion, measures the similarity between customer questions and agent answers\nusing a dual encoder network, a Siamese-like neural network architecture.\nAnswer templates are extracted from embeddings derived from past agent answers,\nwithout turn-by-turn annotations. Responses to customer inquiries are generated\nby selecting the best template from the final set of templates. We show that,\nin a closed domain like customer service, the selected templates cover $>$70\\%\nof past customer inquiries. Furthermore, the relevance of the model-selected\ntemplates is significantly higher than templates selected by a standard tf-idf\nbaseline.", "published": "2017-03-28 07:47:27", "link": "http://arxiv.org/abs/1703.09439v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Is This a Joke? Detecting Humor in Spanish Tweets", "abstract": "While humor has been historically studied from a psychological, cognitive and\nlinguistic standpoint, its study from a computational perspective is an area\nyet to be explored in Computational Linguistics. There exist some previous\nworks, but a characterization of humor that allows its automatic recognition\nand generation is far from being specified. In this work we build a\ncrowdsourced corpus of labeled tweets, annotated according to its humor value,\nletting the annotators subjectively decide which are humorous. A humor\nclassifier for Spanish tweets is assembled based on supervised learning,\nreaching a precision of 84% and a recall of 69%.", "published": "2017-03-28 12:08:46", "link": "http://arxiv.org/abs/1703.09527v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Deep Compositional Framework for Human-like Language Acquisition in\n  Virtual Environment", "abstract": "We tackle a task where an agent learns to navigate in a 2D maze-like\nenvironment called XWORLD. In each session, the agent perceives a sequence of\nraw-pixel frames, a natural language command issued by a teacher, and a set of\nrewards. The agent learns the teacher's language from scratch in a grounded and\ncompositional manner, such that after training it is able to correctly execute\nzero-shot commands: 1) the combination of words in the command never appeared\nbefore, and/or 2) the command contains new object concepts that are learned\nfrom another task but never learned from navigation. Our deep framework for the\nagent is trained end to end: it learns simultaneously the visual\nrepresentations of the environment, the syntax and semantics of the language,\nand the action module that outputs actions. The zero-shot learning capability\nof our framework results from its compositionality and modularity with\nparameter tying. We visualize the intermediate outputs of the framework,\ndemonstrating that the agent truly understands how to solve the problem. We\nbelieve that our results provide some preliminary insights on how to train an\nagent with similar abilities in a 3D environment.", "published": "2017-03-28 22:29:53", "link": "http://arxiv.org/abs/1703.09831v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Analysis of Visual Question Answering Algorithms", "abstract": "In visual question answering (VQA), an algorithm must answer text-based\nquestions about images. While multiple datasets for VQA have been created since\nlate 2014, they all have flaws in both their content and the way algorithms are\nevaluated on them. As a result, evaluation scores are inflated and\npredominantly determined by answering easier questions, making it difficult to\ncompare different methods. In this paper, we analyze existing VQA algorithms\nusing a new dataset. It contains over 1.6 million questions organized into 12\ndifferent categories. We also introduce questions that are meaningless for a\ngiven image to force a VQA system to reason about image content. We propose new\nevaluation schemes that compensate for over-represented question-types and make\nit easier to study the strengths and weaknesses of algorithms. We analyze the\nperformance of both baseline and state-of-the-art VQA models, including\nmulti-modal compact bilinear pooling (MCB), neural module networks, and\nrecurrent answering units. Our experiments establish how attention helps\ncertain categories more than others, determine which models work better than\nothers, and explain how simple models (e.g. MLP) can surpass more complex\nmodels (MCB) by simply learning to answer large, easy question categories.", "published": "2017-03-28 17:48:07", "link": "http://arxiv.org/abs/1703.09684v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Linguistic Matrix Theory", "abstract": "Recent research in computational linguistics has developed algorithms which\nassociate matrices with adjectives and verbs, based on the distribution of\nwords in a corpus of text. These matrices are linear operators on a vector\nspace of context words. They are used to construct the meaning of composite\nexpressions from that of the elementary constituents, forming part of a\ncompositional distributional approach to semantics. We propose a Matrix Theory\napproach to this data, based on permutation symmetry along with Gaussian\nweights and their perturbations. A simple Gaussian model is tested against word\nmatrices created from a large corpus of text. We characterize the cubic and\nquartic departures from the model, which we propose, alongside the Gaussian\nparameters, as signatures for comparison of linguistic corpora. We propose that\nperturbed Gaussian models with permutation symmetry provide a promising\nframework for characterizing the nature of universality in the statistical\nproperties of word matrices. The matrix theory framework developed here\nexploits the view of statistics as zero dimensional perturbative quantum field\ntheory. It perceives language as a physical system realizing a universality\nclass of matrix statistics characterized by permutation symmetry.", "published": "2017-03-28 15:20:52", "link": "http://arxiv.org/abs/1703.10252v1", "categories": ["cs.CL", "hep-th", "math.CO"], "primary_category": "cs.CL"}
