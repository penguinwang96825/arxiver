{"title": "UniDrop: A Simple yet Effective Technique to Improve Transformer without\n  Extra Cost", "abstract": "Transformer architecture achieves great success in abundant natural language\nprocessing tasks. The over-parameterization of the Transformer model has\nmotivated plenty of works to alleviate its overfitting for superior\nperformances. With some explorations, we find simple techniques such as\ndropout, can greatly boost model performance with a careful design. Therefore,\nin this paper, we integrate different dropout techniques into the training of\nTransformer models. Specifically, we propose an approach named UniDrop to\nunites three different dropout techniques from fine-grain to coarse-grain,\ni.e., feature dropout, structure dropout, and data dropout. Theoretically, we\ndemonstrate that these three dropouts play different roles from regularization\nperspectives. Empirically, we conduct experiments on both neural machine\ntranslation and text classification benchmark datasets. Extensive results\nindicate that Transformer with UniDrop can achieve around 1.5 BLEU improvement\non IWSLT14 translation tasks, and better accuracy for the classification even\nusing strong pre-trained RoBERTa as backbone.", "published": "2021-04-11 07:43:19", "link": "http://arxiv.org/abs/2104.04946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does syntax matter? A strong baseline for Aspect-based Sentiment\n  Analysis with RoBERTa", "abstract": "Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities\nfor aspects, is a fine-grained task in the field of sentiment analysis.\nPrevious work showed syntactic information, e.g. dependency trees, can\neffectively improve the ABSA performance. Recently, pre-trained models (PTMs)\nalso have shown their effectiveness on ABSA. Therefore, the question naturally\narises whether PTMs contain sufficient syntactic information for ABSA so that\nwe can obtain a good ABSA model only based on PTMs. In this paper, we firstly\ncompare the induced trees from PTMs and the dependency parsing trees on several\npopular models for the ABSA task, showing that the induced tree from fine-tuned\nRoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis\nexperiments reveal that the FT-RoBERTa Induced Tree is more\nsentiment-word-oriented and could benefit the ABSA task. The experiments also\nshow that the pure RoBERTa-based model can outperform or approximate to the\nprevious SOTA performances on six datasets across four languages since it\nimplicitly incorporates the task-oriented syntactic information.", "published": "2021-04-11 10:45:17", "link": "http://arxiv.org/abs/2104.04986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NorDial: A Preliminary Corpus of Written Norwegian Dialect Use", "abstract": "Norway has a large amount of dialectal variation, as well as a general\ntolerance to its use in the public sphere. There are, however, few available\nresources to study this variation and its change over time and in more informal\nareas, \\eg on social media. In this paper, we propose a first step to creating\na corpus of dialectal variation of written Norwegian. We collect a small corpus\nof tweets and manually annotate them as Bokm{\\aa}l, Nynorsk, any dialect, or a\nmix. We further perform preliminary experiments with state-of-the-art models,\nas well as an analysis of the data to expand this corpus in the future.\nFinally, we make the annotations and models available for future work.", "published": "2021-04-11 10:56:53", "link": "http://arxiv.org/abs/2104.04989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WEC: Deriving a Large-scale Cross-document Event Coreference dataset\n  from Wikipedia", "abstract": "Cross-document event coreference resolution is a foundational task for NLP\napplications involving multi-text processing. However, existing corpora for\nthis task are scarce and relatively small, while annotating only modest-size\nclusters of documents belonging to the same topic. To complement these\nresources and enhance future research, we present Wikipedia Event Coreference\n(WEC), an efficient methodology for gathering a large-scale dataset for\ncross-document event coreference from Wikipedia, where coreference links are\nnot restricted within predefined topics. We apply this methodology to the\nEnglish Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our\ndataset creation method is generic and can be applied with relatively little\neffort to other Wikipedia languages. To set baseline results, we develop an\nalgorithm that adapts components of state-of-the-art models for within-document\ncoreference resolution to the cross-document setting. Our model is suitably\nefficient and outperforms previously published state-of-the-art results for the\ntask.", "published": "2021-04-11 14:54:35", "link": "http://arxiv.org/abs/2104.05022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual\n  Neural Topic Modeling", "abstract": "Neural topic models can augment or replace bag-of-words inputs with the\nlearned representations of deep pre-trained transformer-based word prediction\nmodels. One added benefit when using representations from multilingual models\nis that they facilitate zero-shot polylingual topic modeling. However, while it\nhas been widely observed that pre-trained embeddings should be fine-tuned to a\ngiven task, it is not immediately clear what supervision should look like for\nan unsupervised task such as topic modeling. Thus, we propose several methods\nfor fine-tuning encoders to improve both monolingual and zero-shot polylingual\nneural topic modeling. We consider fine-tuning on auxiliary tasks, constructing\na new topic classification task, integrating the topic classification objective\ndirectly into topic model training, and continued pre-training. We find that\nfine-tuning encoder representations on topic classification and integrating the\ntopic classification task directly into topic modeling improves topic quality,\nand that fine-tuning encoder representations on any task is the most important\nfactor for facilitating cross-lingual transfer.", "published": "2021-04-11 18:03:57", "link": "http://arxiv.org/abs/2104.05064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Semantics and Syntax in Sentence Embeddings with\n  Pre-trained Language Models", "abstract": "Pre-trained language models have achieved huge success on a wide range of NLP\ntasks. However, contextual representations from pre-trained models contain\nentangled semantic and syntactic information, and therefore cannot be directly\nused to derive useful semantic sentence embeddings for some tasks. Paraphrase\npairs offer an effective way of learning the distinction between semantics and\nsyntax, as they naturally share semantics and often vary in syntax. In this\nwork, we present ParaBART, a semantic sentence embedding model that learns to\ndisentangle semantics and syntax in sentence embeddings obtained by pre-trained\nlanguage models. ParaBART is trained to perform syntax-guided paraphrasing,\nbased on a source sentence that shares semantics with the target paraphrase,\nand a parse tree that specifies the target syntax. In this way, ParaBART learns\ndisentangled semantic and syntactic representations from their respective\ninputs with separate encoders. Experiments in English show that ParaBART\noutperforms state-of-the-art sentence embedding models on unsupervised semantic\nsimilarity tasks. Additionally, we show that our approach can effectively\nremove syntactic information from semantic sentence embeddings, leading to\nbetter robustness against syntactic variation on downstream semantic tasks.", "published": "2021-04-11 21:34:46", "link": "http://arxiv.org/abs/2104.05115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Regularization as Stackelberg Game: An Unrolled Optimization\n  Approach", "abstract": "Adversarial regularization has been shown to improve the generalization\nperformance of deep learning models in various natural language processing\ntasks. Existing works usually formulate the method as a zero-sum game, which is\nsolved by alternating gradient descent/ascent algorithms. Such a formulation\ntreats the adversarial and the defending players equally, which is undesirable\nbecause only the defending player contributes to the generalization\nperformance. To address this issue, we propose Stackelberg Adversarial\nRegularization (SALT), which formulates adversarial regularization as a\nStackelberg game. This formulation induces a competition between a leader and a\nfollower, where the follower generates perturbations, and the leader trains the\nmodel subject to the perturbations. Different from conventional approaches, in\nSALT, the leader is in an advantageous position. When the leader moves, it\nrecognizes the strategy of the follower and takes the anticipated follower's\noutcomes into consideration. Such a leader's advantage enables us to improve\nthe model fitting to the unperturbed data. The leader's strategic information\nis captured by the Stackelberg gradient, which is obtained using an unrolling\nalgorithm. Our experimental results on a set of machine translation and natural\nlanguage understanding tasks show that SALT outperforms existing adversarial\nregularization baselines across all tasks. Our code is available at\nhttps://github.com/SimiaoZuo/Stackelberg-Adv.", "published": "2021-04-11 00:44:57", "link": "http://arxiv.org/abs/2104.04886v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Edge: Enriching Knowledge Graph Embeddings with External Text", "abstract": "Knowledge graphs suffer from sparsity which degrades the quality of\nrepresentations generated by various methods. While there is an abundance of\ntextual information throughout the web and many existing knowledge bases,\naligning information across these diverse data sources remains a challenge in\nthe literature. Previous work has partially addressed this issue by enriching\nknowledge graph entities based on \"hard\" co-occurrence of words present in the\nentities of the knowledge graphs and external text, while we achieve \"soft\"\naugmentation by proposing a knowledge graph enrichment and embedding framework\nnamed Edge. Given an original knowledge graph, we first generate a rich but\nnoisy augmented graph using external texts in semantic and structural level. To\ndistill the relevant knowledge and suppress the introduced noise, we design a\ngraph alignment term in a shared embedding space between the original graph and\naugmented graph. To enhance the embedding learning on the augmented graph, we\nfurther regularize the locality relationship of target entity based on negative\nsampling. Experimental results on four benchmark datasets demonstrate the\nrobustness and effectiveness of Edge in link prediction and node\nclassification.", "published": "2021-04-11 03:47:06", "link": "http://arxiv.org/abs/2104.04909v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Semantic Parsing for Compositional Task-Oriented\n  Dialog", "abstract": "Semantic parsing using sequence-to-sequence models allows parsing of deeper\nrepresentations compared to traditional word tagging based models. In spite of\nthese advantages, widespread adoption of these models for real-time\nconversational use cases has been stymied by higher compute requirements and\nthus higher latency. In this work, we propose a non-autoregressive approach to\npredict semantic parse trees with an efficient seq2seq model architecture. By\ncombining non-autoregressive prediction with convolutional neural networks, we\nachieve significant latency gains and parameter size reduction compared to\ntraditional RNN models. Our novel architecture achieves up to an 81% reduction\nin latency on TOP dataset and retains competitive performance to non-pretrained\nmodels on three different semantic parsing datasets. Our code is available at\nhttps://github.com/facebookresearch/pytext", "published": "2021-04-11 05:44:35", "link": "http://arxiv.org/abs/2104.04923v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conversational Semantic Role Labeling", "abstract": "Semantic role labeling (SRL) aims to extract the arguments for each predicate\nin an input sentence. Traditional SRL can fail to analyze dialogues because it\nonly works on every single sentence, while ellipsis and anaphora frequently\noccur in dialogues. To address this problem, we propose the conversational SRL\ntask, where an argument can be the dialogue participants, a phrase in the\ndialogue history or the current sentence. As the existing SRL datasets are in\nthe sentence level, we manually annotate semantic roles for 3,000 chit-chat\ndialogues (27,198 sentences) to boost the research in this direction.\nExperiments show that while traditional SRL systems (even with the help of\ncoreference resolution or rewriting) perform poorly for analyzing dialogues,\nmodeling dialogue histories and participants greatly helps the performance,\nindicating that adapting SRL to conversations is very promising for universal\ndialogue understanding. Our initial study by applying CSRL to two mainstream\nconversational tasks, dialogue response generation and dialogue context\nrewriting, also confirms the usefulness of CSRL.", "published": "2021-04-11 07:45:04", "link": "http://arxiv.org/abs/2104.04947v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Learning of Explainable Parse Trees for Improved\n  Generalisation", "abstract": "Recursive neural networks (RvNN) have been shown useful for learning sentence\nrepresentations and helped achieve competitive performance on several natural\nlanguage inference tasks. However, recent RvNN-based models fail to learn\nsimple grammar and meaningful semantics in their intermediate tree\nrepresentation. In this work, we propose an attention mechanism over Tree-LSTMs\nto learn more meaningful and explainable parse tree structures. We also\ndemonstrate the superior performance of our proposed model on natural language\ninference, semantic relatedness, and sentiment analysis tasks and compare them\nwith other state-of-the-art RvNN based methods. Further, we present a detailed\nqualitative and quantitative analysis of the learned parse trees and show that\nthe discovered linguistic structures are more explainable, semantically\nmeaningful, and grammatically correct than recent approaches. The source code\nof the paper is available at\nhttps://github.com/atul04/Explainable-Latent-Structures-Using-Attention.", "published": "2021-04-11 12:10:03", "link": "http://arxiv.org/abs/2104.04998v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The structure of online social networks modulates the rate of lexical\n  change", "abstract": "New words are regularly introduced to communities, yet not all of these words\npersist in a community's lexicon. Among the many factors contributing to\nlexical change, we focus on the understudied effect of social networks. We\nconduct a large-scale analysis of over 80k neologisms in 4420 online\ncommunities across a decade. Using Poisson regression and survival analysis,\nour study demonstrates that the community's network structure plays a\nsignificant role in lexical change. Apart from overall size, properties\nincluding dense connections, the lack of local clusters and more external\ncontacts promote lexical innovation and retention. Unlike offline communities,\nthese topic-based communities do not experience strong lexical levelling\ndespite increased contact but accommodate more niche words. Our work provides\nsupport for the sociolinguistic hypothesis that lexical change is partially\nshaped by the structure of the underlying network but also uncovers findings\nspecific to online communities.", "published": "2021-04-11 13:06:28", "link": "http://arxiv.org/abs/2104.05010v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Achieving Model Robustness through Discrete Adversarial Training", "abstract": "Discrete adversarial attacks are symbolic perturbations to a language input\nthat preserve the output label but lead to a prediction error. While such\nattacks have been extensively explored for the purpose of evaluating model\nrobustness, their utility for improving robustness has been limited to offline\naugmentation only. Concretely, given a trained model, attacks are used to\ngenerate perturbed (adversarial) examples, and the model is re-trained exactly\nonce. In this work, we address this gap and leverage discrete attacks for\nonline augmentation, where adversarial examples are generated at every training\nstep, adapting to the changing nature of the model. We propose (i) a new\ndiscrete attack, based on best-first search, and (ii) random sampling attacks\nthat unlike prior work are not based on expensive search-based procedures.\nSurprisingly, we find that random sampling leads to impressive gains in\nrobustness, outperforming the commonly-used offline augmentation, while leading\nto a speedup at training time of ~10x. Furthermore, online augmentation with\nsearch-based attacks justifies the higher training cost, significantly\nimproving robustness on three datasets. Last, we show that our new attack\nsubstantially improves robustness compared to prior methods.", "published": "2021-04-11 17:49:21", "link": "http://arxiv.org/abs/2104.05062v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Constructing Contrastive samples via Summarization for Text\n  Classification with limited annotations", "abstract": "Contrastive Learning has emerged as a powerful representation learning method\nand facilitates various downstream tasks especially when supervised data is\nlimited. How to construct efficient contrastive samples through data\naugmentation is key to its success. Unlike vision tasks, the data augmentation\nmethod for contrastive learning has not been investigated sufficiently in\nlanguage tasks. In this paper, we propose a novel approach to construct\ncontrastive samples for language tasks using text summarization. We use these\nsamples for supervised contrastive learning to gain better text representations\nwhich greatly benefit text classification tasks with limited annotations. To\nfurther improve the method, we mix up samples from different classes and add an\nextra regularization, named Mixsum, in addition to the cross-entropy-loss.\nExperiments on real-world text classification datasets (Amazon-5, Yelp-5, AG\nNews, and IMDb) demonstrate the effectiveness of the proposed contrastive\nlearning framework with summarization-based data augmentation and Mixsum\nregularization.", "published": "2021-04-11 20:13:24", "link": "http://arxiv.org/abs/2104.05094v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Toolbox for Construction and Analysis of Speech Datasets", "abstract": "Automatic Speech Recognition and Text-to-Speech systems are primarily trained\nin a supervised fashion and require high-quality, accurately labeled speech\ndatasets. In this work, we examine common problems with speech data and\nintroduce a toolbox for the construction and interactive error analysis of\nspeech datasets. The construction tool is based on K\\\"urzinger et al. work,\nand, to the best of our knowledge, the dataset exploration tool is the world's\nfirst open-source tool of this kind. We demonstrate how to apply these tools to\ncreate a Russian speech dataset and analyze existing speech datasets\n(Multilingual LibriSpeech, Mozilla Common Voice). The tools are open sourced as\na part of the NeMo framework.", "published": "2021-04-11 01:57:55", "link": "http://arxiv.org/abs/2104.04896v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Disentangled Contrastive Learning for Learning Robust Textual\n  Representations", "abstract": "Although the self-supervised pre-training of transformer models has resulted\nin the revolutionizing of natural language processing (NLP) applications and\nthe achievement of state-of-the-art results with regard to various benchmarks,\nthis process is still vulnerable to small and imperceptible permutations\noriginating from legitimate inputs. Intuitively, the representations should be\nsimilar in the feature space with subtle input permutations, while large\nvariations occur with different meanings. This motivates us to investigate the\nlearning of robust textual representation in a contrastive manner. However, it\nis non-trivial to obtain opposing semantic instances for textual samples. In\nthis study, we propose a disentangled contrastive learning method that\nseparately optimizes the uniformity and alignment of representations without\nnegative sampling. Specifically, we introduce the concept of momentum\nrepresentation consistency to align features and leverage power normalization\nwhile conforming the uniformity. Our experimental results for the NLP\nbenchmarks demonstrate that our approach can obtain better results compared\nwith the baselines, as well as achieve promising improvements with invariance\ntests and adversarial attacks. The code is available in\nhttps://github.com/zxlzr/DCL.", "published": "2021-04-11 03:32:49", "link": "http://arxiv.org/abs/2104.04907v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Word Embedding Refinement by $\\ell_{1}$ Norm Optimisation", "abstract": "Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages\nin a shared high-dimensional space in which vectors representing words with\nsimilar meaning (regardless of language) are closely located. Existing methods\nfor building high-quality CLWEs learn mappings that minimise the $\\ell_{2}$\nnorm loss function. However, this optimisation objective has been demonstrated\nto be sensitive to outliers. Based on the more robust Manhattan norm (aka.\n$\\ell_{1}$ norm) goodness-of-fit criterion, this paper proposes a simple\npost-processing step to improve CLWEs. An advantage of this approach is that it\nis fully agnostic to the training process of the original CLWEs and can\ntherefore be applied widely. Extensive experiments are performed involving ten\ndiverse languages and embeddings trained on different corpora. Evaluation\nresults based on bilingual lexicon induction and cross-lingual transfer for\nnatural language inference tasks show that the $\\ell_{1}$ refinement\nsubstantially outperforms four state-of-the-art baselines in both supervised\nand unsupervised settings. It is therefore recommended that this strategy be\nadopted as a standard for CLWE methods.", "published": "2021-04-11 04:37:54", "link": "http://arxiv.org/abs/2104.04916v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Innovative Bert-based Reranking Language Models for Speech Recognition", "abstract": "More recently, Bidirectional Encoder Representations from Transformers (BERT)\nwas proposed and has achieved impressive success on many natural language\nprocessing (NLP) tasks such as question answering and language understanding,\ndue mainly to its effective pre-training then fine-tuning paradigm as well as\nstrong local contextual modeling ability. In view of the above, this paper\npresents a novel instantiation of the BERT-based contextualized language models\n(LMs) for use in reranking of N-best hypotheses produced by automatic speech\nrecognition (ASR). To this end, we frame N-best hypothesis reranking with BERT\nas a prediction problem, which aims to predict the oracle hypothesis that has\nthe lowest word error rate (WER) given the N-best hypotheses (denoted by\nPBERT). In particular, we also explore to capitalize on task-specific global\ntopic information in an unsupervised manner to assist PBERT in N-best\nhypothesis reranking (denoted by TPBERT). Extensive experiments conducted on\nthe AMI benchmark corpus demonstrate the effectiveness and feasibility of our\nmethods in comparison to the conventional autoregressive models like the\nrecurrent neural network (RNN) and a recently proposed method that employed\nBERT to compute pseudo-log-likelihood (PLL) scores for N-best hypothesis\nreranking.", "published": "2021-04-11 07:55:41", "link": "http://arxiv.org/abs/2104.04950v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "NeMo Inverse Text Normalization: From Development To Production", "abstract": "Inverse text normalization (ITN) converts spoken-domain automatic speech\nrecognition (ASR) output into written-domain text to improve the readability of\nthe ASR output. Many state-of-the-art ITN systems use hand-written weighted\nfinite-state transducer(WFST) grammars since this task has extremely low\ntolerance to unrecoverable errors. We introduce an open-source Python\nWFST-based library for ITN which enables a seamless path from development to\nproduction. We describe the specification of ITN grammar rules for English, but\nthe library can be adapted for other languages. It can also be used for\nwritten-to-spoken text normalization. We evaluate the NeMo ITN library using a\nmodified version of the Google Text normalization dataset.", "published": "2021-04-11 17:09:49", "link": "http://arxiv.org/abs/2104.05055v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The DKU System Description for The Interspeech 2021 Auto-KWS Challenge", "abstract": "This paper introduces the system submitted by the DKU-SMIIP team for the\nAuto-KWS 2021 Challenge. Our implementation consists of a two-stage keyword\nspotting system based on query-by-example spoken term detection and a speaker\nverification system. We employ two different detection algorithms in our\nproposed keyword spotting system. The first stage adopts subsequence dynamic\ntime warping for template matching based on frame-level language-independent\nbottleneck feature and phoneme posterior probability. We use a sliding window\ntemplate matching algorithm based on acoustic word embeddings to further verify\nthe detection from the first stage. As a result, our KWS system achieves an\naverage score of 0.61 on the feedback dataset, which outperforms the baseline1\nsystem by 0.25.", "published": "2021-04-11 11:05:28", "link": "http://arxiv.org/abs/2104.04993v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Estimating articulatory movements in speech production with transformer\n  networks", "abstract": "We estimate articulatory movements in speech production from different\nmodalities - acoustics and phonemes. Acoustic-to articulatory inversion (AAI)\nis a sequence-to-sequence task. On the other hand, phoneme to articulatory\n(PTA) motion estimation faces a key challenge in reliably aligning the text and\nthe articulatory movements. To address this challenge, we explore the use of a\ntransformer architecture - FastSpeech, with explicit duration modelling to\nlearn hard alignments between the phonemes and articulatory movements. We also\ntrain a transformer model on AAI. We use correlation coefficient (CC) and root\nmean squared error (rMSE) to assess the estimation performance in comparison to\nexisting methods on both tasks. We observe 154%, 11.8% & 4.8% relative\nimprovement in CC with subject-dependent, pooled and fine-tuning strategies,\nrespectively, for PTA estimation. Additionally, on the AAI task, we obtain\n1.5%, 3% and 3.1% relative gain in CC on the same setups compared to the\nstate-of-the-art baseline. We further present the computational benefits of\nhaving transformer architecture as representation blocks.", "published": "2021-04-11 13:56:10", "link": "http://arxiv.org/abs/2104.05017v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comparison of Binaural RTF-Vector-Based Direction of Arrival Estimation\n  Methods Exploiting an External Microphone", "abstract": "In this paper we consider a binaural hearing aid setup, where in addition to\nthe head-mounted microphones an external microphone is available. For this\nsetup, we investigate the performance of several relative transfer function\n(RTF) vector estimation methods to estimate the direction of arrival (DOA) of\nthe target speaker in a noisy and reverberant acoustic environment. More in\nparticular, we consider the state-of-the-art covariance whitening (CW) and\ncovariance subtraction (CS) methods, either incorporating the external\nmicrophone or not, and the recently proposed spatial coherence (SC) method,\nrequiring the external microphone. To estimate the DOA from the estimated RTF\nvector, we propose to minimize the frequency-averaged Hermitian angle between\nthe estimated head-mounted RTF vector and a database of prototype head-mounted\nRTF vectors. Experimental results with stationary and moving speech sources in\na reverberant environment with diffuse-like noise show that the SC method\noutperforms the CS method and yields a similar DOA estimation accuracy as the\nCW method at a lower computational complexity.", "published": "2021-04-11 19:06:26", "link": "http://arxiv.org/abs/2104.05079v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
