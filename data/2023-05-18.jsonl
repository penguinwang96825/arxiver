{"title": "Are Large Language Models Fit For Guided Reading?", "abstract": "This paper looks at the ability of large language models to participate in\neducational guided reading. We specifically, evaluate their ability to generate\nmeaningful questions from the input text, generate diverse questions both in\nterms of content coverage and difficulty of the questions and evaluate their\nability to recommend part of the text that a student should re-read based on\nthe student's responses to the questions. Based on our evaluation of ChatGPT\nand Bard, we report that,\n  1) Large language models are able to generate high quality meaningful\nquestions that have high correlation with the input text, 2) They generate\ndiverse question that cover most topics in the input text even though this\nability is significantly degraded as the input text increases, 3)The large\nlanguage models are able to generate both low and high cognitive questions even\nthough they are significantly biased toward low cognitive question, 4) They are\nable to effectively summarize responses and extract a portion of text that\nshould be re-read.", "published": "2023-05-18 02:03:55", "link": "http://arxiv.org/abs/2305.10645v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training", "abstract": "Generative pre-trained Transformer (GPT) has demonstrates its great success\nin natural language processing and related techniques have been adapted into\nmolecular modeling. Considering that text is the most important record for\nscientific discovery, in this paper, we propose MolXPT, a unified language\nmodel of text and molecules pre-trained on SMILES (a sequence representation of\nmolecules) wrapped by text. Briefly, we detect the molecule names in each\nsequence and replace them to the corresponding SMILES. In this way, the SMILES\ncould leverage the information from surrounding text, and vice versa. The above\nwrapped sequences, text sequences from PubMed and SMILES sequences from PubChem\nare all fed into a language model for pre-training. Experimental results\ndemonstrate that MolXPT outperforms strong baselines of molecular property\nprediction on MoleculeNet, performs comparably to the best model in\ntext-molecule translation while using less than half of its parameters, and\nenables zero-shot molecular generation without finetuning.", "published": "2023-05-18 03:58:19", "link": "http://arxiv.org/abs/2305.10688v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Norm Violations in Live-Stream Chat", "abstract": "Toxic language, such as hate speech, can deter users from participating in\nonline communities and enjoying popular platforms. Previous approaches to\ndetecting toxic language and norm violations have been primarily concerned with\nconversations from online forums and social media, such as Reddit and Twitter.\nThese approaches are less effective when applied to conversations on\nlive-streaming platforms, such as Twitch and YouTube Live, as each comment is\nonly visible for a limited time and lacks a thread structure that establishes\nits relationship with other comments. In this work, we share the first NLP\nstudy dedicated to detecting norm violations in conversations on live-streaming\nplatforms. We define norm violation categories in live-stream chats and\nannotate 4,583 moderated comments from Twitch. We articulate several facets of\nlive-stream data that differ from other forums, and demonstrate that existing\nmodels perform poorly in this setting. By conducting a user study, we identify\nthe informational context humans use in live-stream moderation, and train\nmodels leveraging context to identify norm violations. Our results show that\nappropriate contextual information can boost moderation performance by 35\\%.", "published": "2023-05-18 05:58:27", "link": "http://arxiv.org/abs/2305.10731v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings", "abstract": "Prior studies diagnose the anisotropy problem in sentence representations\nfrom pre-trained language models, e.g., BERT, without fine-tuning. Our analysis\nreveals that the sentence embeddings from BERT suffer from a bias towards\nuninformative words, limiting the performance in semantic textual similarity\n(STS) tasks. To address this bias, we propose a simple and efficient\nunsupervised approach, Diagonal Attention Pooling (Ditto), which weights words\nwith model-based importance estimations and computes the weighted average of\nword representations from pre-trained models as sentence embeddings. Ditto can\nbe easily applied to any pre-trained language model as a postprocessing\noperation. Compared to prior sentence embedding approaches, Ditto does not add\nparameters nor requires any learning. Empirical evaluations demonstrate that\nour proposed Ditto can alleviate the anisotropy problem and improve various\npre-trained models on STS tasks.", "published": "2023-05-18 07:56:40", "link": "http://arxiv.org/abs/2305.10786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLEME: Debiasing Multi-reference Evaluation for Grammatical Error\n  Correction", "abstract": "Evaluating the performance of Grammatical Error Correction (GEC) systems is a\nchallenging task due to its subjectivity. Designing an evaluation metric that\nis as objective as possible is crucial to the development of GEC task. However,\nmainstream evaluation metrics, i.e., reference-based metrics, introduce bias\ninto the multi-reference evaluation by extracting edits without considering the\npresence of multiple references. To overcome this issue, we propose Chunk-LEvel\nMulti-reference Evaluation (CLEME), designed to evaluate GEC systems in the\nmulti-reference evaluation setting. CLEME builds chunk sequences with\nconsistent boundaries for the source, the hypothesis and references, thus\neliminating the bias caused by inconsistent edit boundaries. Furthermore, we\nobserve the consistent boundary could also act as the boundary of grammatical\nerrors, based on which the F$_{0.5}$ score is then computed following the\ncorrection independence assumption. We conduct experiments on six English\nreference sets based on the CoNLL-2014 shared task. Extensive experiments and\ndetailed analyses demonstrate the correctness of our discovery and the\neffectiveness of CLEME. Further analysis reveals that CLEME is robust to\nevaluate GEC systems across reference sets with varying numbers of references\nand annotation style.", "published": "2023-05-18 08:57:17", "link": "http://arxiv.org/abs/2305.10819v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning Methods for Extracting Metaphorical Names of Flowers and\n  Plants", "abstract": "The domain of Botany is rich with metaphorical terms. Those terms play an\nimportant role in the description and identification of flowers and plants.\nHowever, the identification of such terms in discourse is an arduous task. This\nleads in some cases to committing errors during translation processes and\nlexicographic tasks. The process is even more challenging when it comes to\nmachine translation, both in the cases of single-word terms and multi-word\nterms. One of the recent concerns of Natural Language Processing (NLP)\napplications and Machine Translation (MT) technologies is the automatic\nidentification of metaphor-based words in discourse through Deep Learning (DL).\nIn this study, we seek to fill this gap through the use of thirteen popular\ntransformer based models, as well as ChatGPT, and we show that discriminative\nmodels perform better than GPT-3.5 model with our best performer reporting\n92.2349% F1 score in metaphoric flower and plant names identification task.", "published": "2023-05-18 09:22:29", "link": "http://arxiv.org/abs/2305.10833v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAPIR: Learning Adaptive Revision for Incremental Natural Language\n  Understanding with a Two-Pass Model", "abstract": "Language is by its very nature incremental in how it is produced and\nprocessed. This property can be exploited by NLP systems to produce fast\nresponses, which has been shown to be beneficial for real-time interactive\napplications. Recent neural network-based approaches for incremental processing\nmainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct\nearlier output, which can be necessary in incremental processing).\nTransformers, on the other hand, consume whole sequences, and hence are by\nnature non-incremental. A restart-incremental interface that repeatedly passes\nlonger input prefixes can be used to obtain partial outputs, while providing\nthe ability to revise. However, this method becomes costly as the sentence\ngrows longer. In this work, we propose the Two-pass model for AdaPtIve Revision\n(TAPIR) and introduce a method to obtain an incremental supervision signal for\nlearning an adaptive revision policy. Experimental results on sequence\nlabelling show that our model has better incremental performance and faster\ninference speed compared to restart-incremental Transformers, while showing\nlittle degradation on full sequences.", "published": "2023-05-18 09:58:19", "link": "http://arxiv.org/abs/2305.10845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Full-Text Search Lemmatization Techniques with Paradigm\n  Retrieval from OpenCorpora", "abstract": "In this paper, we unveil a groundbreaking method to amplify full-text search\nlemmatization, utilizing the OpenCorpora dataset and a bespoke paradigm\nretrieval algorithm. Our primary aim is to streamline the extraction of a\nword's primary form or lemma - a crucial factor in full-text search.\nAdditionally, we propose a compact dictionary storage strategy, significantly\nboosting the speed and precision of lemma retrieval.", "published": "2023-05-18 10:07:50", "link": "http://arxiv.org/abs/2305.10848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse\n  Relation Recognition", "abstract": "Implicit Discourse Relation Recognition (IDRR) aims at classifying the\nrelation sense between two arguments without an explicit connective. Recently,\nthe ConnPrompt~\\cite{Wei.X:et.al:2022:COLING} has leveraged the powerful prompt\nlearning for IDRR based on the fusion of multi-prompt decisions from three\ndifferent yet much similar connective prediction templates. Instead of\nmulti-prompt ensembling, we propose to design auxiliary tasks with enlightened\nprompt learning for the IDRR task. Although an auxiliary task is not used to\ndirectly output final prediction, we argue that during the joint training some\nof its learned features can be useful to boost the main task. In light of such\nmotivations, we propose a task enlightenment prompt learning model, called\nTEPrompt, to fuse learned features from three related tasks for IDRR. In\nparticular, the TEPrompt contains three tasks, viz., Discourse Relation\nRecognition (DRR), Sense Semantics Classification (SSC) and Annotated\nConnective Prediction (ACP), each with a unique prompt template and an answer\nspace. In the training phase, we jointly train three prompt learning tasks with\nshared argument representation. In the testing phase, we only take the DRR\noutput with fused features as the final IDRR decision. Experiments with the\nsame conditions have shown that the proposed TEPrompt outperforms the\nConnPrompt. This can be attributed to the promoted decision features and\nlanguage models benefited from joint-training of auxiliary tasks.", "published": "2023-05-18 10:38:06", "link": "http://arxiv.org/abs/2305.10866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EventNet-ITA: Italian Frame Parsing for Events", "abstract": "This paper introduces EventNet-ITA, a large, multi-domain corpus annotated\nfull-text with event frames for Italian. Moreover, we present and thoroughly\nevaluate an efficient multi-label sequence labeling approach for Frame Parsing.\nCovering a wide range of individual, social and historical phenomena, with more\nthan 53,000 annotated sentences and over 200 modeled frames, EventNet-ITA\nconstitutes the first systematic attempt to provide the Italian language with a\npublicly available resource for Frame Parsing of events, useful for a broad\nspectrum of research and application tasks. Our approach achieves a promising\n0.9 strict F1-score for frame classification and 0.72 for frame element\nclassification, on top of minimizing computational requirements. The annotated\ncorpus and the frame parsing model are released under open license.", "published": "2023-05-18 11:41:56", "link": "http://arxiv.org/abs/2305.10892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Take a Break in the Middle: Investigating Subgoals towards Hierarchical\n  Script Generation", "abstract": "Goal-oriented Script Generation is a new task of generating a list of steps\nthat can fulfill the given goal. In this paper, we propose to extend the task\nfrom the perspective of cognitive theory. Instead of a simple flat structure,\nthe steps are typically organized hierarchically - Human often decompose a\ncomplex task into subgoals, where each subgoal can be further decomposed into\nsteps. To establish the benchmark, we contribute a new dataset, propose several\nbaseline methods, and set up evaluation metrics. Both automatic and human\nevaluation verify the high-quality of dataset, as well as the effectiveness of\nincorporating subgoals into hierarchical script generation. Furthermore, We\nalso design and evaluate the model to discover subgoal, and find that it is a\nbit more difficult to decompose the goals than summarizing from segmented\nsteps.", "published": "2023-05-18 12:10:06", "link": "http://arxiv.org/abs/2305.10907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Document-Grounded Dialogue Pre-training", "abstract": "The goal of document-grounded dialogue (DocGD) is to generate a response by\ngrounding the evidence in a supporting document in accordance with the dialogue\ncontext. This process involves four variables that are causally connected.\nRecently, task-specific pre-training has greatly boosted performances on many\ndownstream tasks. Existing DocGD methods, however, continue to rely on general\npre-trained language models without a specifically tailored pre-training\napproach that explicitly captures the causal relationships. To tackle this\nissue, we are the first to present a causally-complete dataset construction\nstrategy for building million-level DocGD pre-training corpora. To better\ncapture causality, we further propose a causally-perturbed pre-training\nstrategy, which introduces causal perturbations on the variables and optimizes\nthe overall causal effect. Experiments on three benchmark datasets demonstrate\nthat our causal pre-training achieves considerable and consistent improvements\nunder fully-supervised, low-resource, few-shot, and zero-shot settings.", "published": "2023-05-18 12:39:25", "link": "http://arxiv.org/abs/2305.10927v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NollySenti: Leveraging Transfer Learning and Machine Translation for\n  Nigerian Movie Sentiment Classification", "abstract": "Africa has over 2000 indigenous languages but they are under-represented in\nNLP research due to lack of datasets. In recent years, there have been progress\nin developing labeled corpora for African languages. However, they are often\navailable in a single domain and may not generalize to other domains. In this\npaper, we focus on the task of sentiment classification for cross domain\nadaptation. We create a new dataset, NollySenti - based on the Nollywood movie\nreviews for five languages widely spoken in Nigeria (English, Hausa, Igbo,\nNigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using\nclassical machine learning methods and pre-trained language models. Leveraging\ntransfer learning, we compare the performance of cross-domain adaptation from\nTwitter domain, and cross-lingual adaptation from English language. Our\nevaluation shows that transfer from English in the same target domain leads to\nmore than 5% improvement in accuracy compared to transfer from Twitter in the\nsame language. To further mitigate the domain difference, we leverage machine\ntranslation (MT) from English to other Nigerian languages, which leads to a\nfurther improvement of 7% over cross-lingual evaluation. While MT to\nlow-resource languages are often of low quality, through human evaluation, we\nshow that most of the translated sentences preserve the sentiment of the\noriginal English reviews.", "published": "2023-05-18 13:38:36", "link": "http://arxiv.org/abs/2305.10971v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-CrossRE A Multi-Lingual Multi-Domain Dataset for Relation\n  Extraction", "abstract": "Most research in Relation Extraction (RE) involves the English language,\nmainly due to the lack of multi-lingual resources. We propose Multi-CrossRE,\nthe broadest multi-lingual dataset for RE, including 26 languages in addition\nto English, and covering six text domains. Multi-CrossRE is a machine\ntranslated version of CrossRE (Bassignana and Plank, 2022), with a sub-portion\nincluding more than 200 sentences in seven diverse languages checked by native\nspeakers. We run a baseline model over the 26 new datasets and--as sanity\ncheck--over the 26 back-translations to English. Results on the back-translated\ndata are consistent with the ones on the original English CrossRE, indicating\nhigh quality of the translation and the resulting dataset.", "published": "2023-05-18 14:01:33", "link": "http://arxiv.org/abs/2305.10985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Web Can Be Your Oyster for Improving Large Language Models", "abstract": "Large language models (LLMs) encode a large amount of world knowledge.\nHowever, as such knowledge is frozen at the time of model training, the models\nbecome static and limited by the training data at that time. In order to\nfurther improve the capacity of LLMs for knowledge-intensive tasks, we consider\naugmenting LLMs with the large-scale web using search engine. Unlike previous\naugmentation sources (e.g., Wikipedia data dump), the web provides broader,\nmore comprehensive and constantly updated information. In this paper, we\npresent a web-augmented LLM UNIWEB, which is trained over 16\nknowledge-intensive tasks in a unified text-to-text format. Instead of simply\nusing the retrieved contents from web, our approach has made two major\nimprovements. Firstly, we propose an adaptive search engine assisted learning\nmethod that can self-evaluate the confidence level of LLM's predictions, and\nadaptively determine when to refer to the web for more data, which can avoid\nuseless or noisy augmentation from web. Secondly, we design a pretraining task,\ni.e., continual knowledge learning, based on salient spans prediction, to\nreduce the discrepancy between the encoded and retrieved knowledge. Experiments\non a wide range of knowledge-intensive tasks show that our model significantly\noutperforms previous retrieval-augmented methods.", "published": "2023-05-18 14:20:32", "link": "http://arxiv.org/abs/2305.10998v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal\n  Conversational Abilities", "abstract": "Multi-modal large language models are regarded as a crucial step towards\nArtificial General Intelligence (AGI) and have garnered significant interest\nwith the emergence of ChatGPT. However, current speech-language models\ntypically adopt the cascade paradigm, preventing inter-modal knowledge\ntransfer. In this paper, we propose SpeechGPT, a large language model with\nintrinsic cross-modal conversational abilities, capable of perceiving and\ngenerating multi-model content. With discrete speech representations, we first\nconstruct SpeechInstruct, a large-scale cross-modal speech instruction dataset.\nAdditionally, we employ a three-stage training strategy that includes\nmodality-adaptation pre-training, cross-modal instruction fine-tuning, and\nchain-of-modality instruction fine-tuning. The experimental results demonstrate\nthat SpeechGPT has an impressive capacity to follow multi-modal human\ninstructions and highlight the potential of handling multiple modalities with\none model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.", "published": "2023-05-18 14:23:25", "link": "http://arxiv.org/abs/2305.11000v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Insert or Attach: Taxonomy Completion via Box Embedding", "abstract": "Taxonomy completion, enriching existing taxonomies by inserting new concepts\nas parents or attaching them as children, has gained significant interest.\nPrevious approaches embed concepts as vectors in Euclidean space, which makes\nit difficult to model asymmetric relations in taxonomy. In addition, they\nintroduce pseudo-leaves to convert attachment cases into insertion cases,\nleading to an incorrect bias in network learning dominated by numerous\npseudo-leaves. Addressing these, our framework, TaxBox, leverages box\ncontainment and center closeness to design two specialized geometric scorers\nwithin the box embedding space. These scorers are tailored for insertion and\nattachment operations and can effectively capture intrinsic relationships\nbetween concepts by optimizing on a granular box constraint loss. We employ a\ndynamic ranking loss mechanism to balance the scores from these scorers,\nallowing adaptive adjustments of insertion and attachment scores. Experiments\non four real-world datasets show that TaxBox significantly outperforms previous\nmethods, yielding substantial improvements over prior methods in real-world\ndatasets, with average performance boosts of 6.7%, 34.9%, and 51.4% in MRR,\nHit@1, and Prec@1, respectively.", "published": "2023-05-18 14:34:58", "link": "http://arxiv.org/abs/2305.11004v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Silver Syntax Pre-training for Cross-Domain Relation Extraction", "abstract": "Relation Extraction (RE) remains a challenging task, especially when\nconsidering realistic out-of-domain evaluations. One of the main reasons for\nthis is the limited training size of current RE datasets: obtaining\nhigh-quality (manually annotated) data is extremely expensive and cannot\nrealistically be repeated for each new domain. An intermediate training step on\ndata from related tasks has shown to be beneficial across many NLP\ntasks.However, this setup still requires supplementary annotated data, which is\noften not available. In this paper, we investigate intermediate pre-training\nspecifically for RE. We exploit the affinity between syntactic structure and\nsemantic RE, and identify the syntactic relations which are closely related to\nRE by being on the shortest dependency path between two entities. We then take\nadvantage of the high accuracy of current syntactic parsers in order to\nautomatically obtain large amounts of low-cost pre-training data. By\npre-training our RE model on the relevant syntactic relations, we are able to\noutperform the baseline in five out of six cross-domain setups, without any\nadditional annotated data.", "published": "2023-05-18 14:49:19", "link": "http://arxiv.org/abs/2305.11016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalized Multiple Intent Conditioned Slot Filling", "abstract": "Natural language understanding includes the tasks of intent detection\n(identifying a user's objectives) and slot filling (extracting the entities\nrelevant to those objectives). Prior slot filling methods assume that each\nintent type cannot occur more than once within a message, however this is often\nnot a valid assumption for real-world settings. In this work, we generalize\nslot filling by removing the constraint of unique intents in a message. We cast\nthis as a JSON generation task and approach it using a language model. We\ncreate a pre-training dataset by combining DBpedia and existing slot filling\ndatasets that we convert for JSON generation. We also generate an in-domain\ndataset using GPT-3. We train T5 models for this task (with and without\nexemplars in the prompt) and find that both training datasets improve\nperformance, and that the model is able to generalize to intent types not seen\nduring training.", "published": "2023-05-18 15:04:52", "link": "http://arxiv.org/abs/2305.11023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words\n  Extraction with Wordpieces and Aspect Enhancement", "abstract": "State-of-the-art target-oriented opinion word extraction (TOWE) models\ntypically use BERT-based text encoders that operate on the word level, along\nwith graph convolutional networks (GCNs) that incorporate syntactic information\nextracted from syntax trees. These methods achieve limited gains with GCNs and\nhave difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to\nbe effective at representing rare words or words with insufficient context\ninformation. To address this issue, this work trades syntax trees for BERT\nwordpieces by entirely removing the GCN component from the methods'\narchitectures. To enhance TOWE performance, we tackle the issue of aspect\nrepresentation loss during encoding. Instead of solely utilizing a sentence as\nthe input, we use a sentence-aspect pair. Our relatively simple approach\nachieves state-of-the-art results on benchmark datasets and should serve as a\nstrong baseline for further research.", "published": "2023-05-18 15:22:00", "link": "http://arxiv.org/abs/2305.11034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning In-context Learning for Named Entity Recognition", "abstract": "Named entity recognition in real-world applications suffers from the\ndiversity of entity types, the emergence of new entity types, and the lack of\nhigh-quality annotations. To address the above problems, this paper proposes an\nin-context learning-based NER approach, which can effectively inject in-context\nNER ability into PLMs and recognize entities of novel types on-the-fly using\nonly a few demonstrative instances. Specifically, we model PLMs as a\nmeta-function $\\mathcal{ \\lambda_ {\\text{instruction, demonstrations, text}}.\nM}$, and a new entity extractor can be implicitly constructed by applying new\ninstruction and demonstrations to PLMs, i.e., $\\mathcal{ (\\lambda . M)\n}$(instruction, demonstrations) $\\to$ $\\mathcal{F}$ where $\\mathcal{F}$ will be\na new entity extractor, i.e., $\\mathcal{F}$: text $\\to$ entities. To inject the\nabove in-context NER ability into PLMs, we propose a meta-function pre-training\nalgorithm, which pre-trains PLMs by comparing the (instruction,\ndemonstration)-initialized extractor with a surrogate golden extractor.\nExperimental results on 4 few-shot NER datasets show that our method can\neffectively inject in-context NER ability into PLMs and significantly\noutperforms the PLMs+fine-tuning counterparts.", "published": "2023-05-18 15:31:34", "link": "http://arxiv.org/abs/2305.11038v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-modality Data Augmentation for End-to-End Sign Language\n  Translation", "abstract": "End-to-end sign language translation (SLT) aims to convert sign language\nvideos into spoken language texts directly without intermediate\nrepresentations. It has been a challenging task due to the modality gap between\nsign videos and texts and the data scarcity of labeled data. Due to these\nchallenges, the input and output distributions of end-to-end sign language\ntranslation (i.e., video-to-text) are less effective compared to the\ngloss-to-text approach (i.e., text-to-text). To tackle these challenges, we\npropose a novel Cross-modality Data Augmentation (XmDA) framework to transfer\nthe powerful gloss-to-text translation capabilities to end-to-end sign language\ntranslation (i.e. video-to-text) by exploiting pseudo gloss-text pairs from the\nsign gloss translation model. Specifically, XmDA consists of two key\ncomponents, namely, cross-modality mix-up and cross-modality knowledge\ndistillation. The former explicitly encourages the alignment between sign video\nfeatures and gloss embeddings to bridge the modality gap. The latter utilizes\nthe generation knowledge from gloss-to-text teacher models to guide the spoken\nlanguage text generation. Experimental results on two widely used SLT datasets,\ni.e., PHOENIX-2014T and CSL-Daily, demonstrate that the proposed XmDA framework\nsignificantly and consistently outperforms the baseline models. Extensive\nanalyses confirm our claim that XmDA enhances spoken language text generation\nby reducing the representation distance between videos and texts, as well as\nimproving the processing of low-frequency words and long sentences.", "published": "2023-05-18 16:34:18", "link": "http://arxiv.org/abs/2305.11096v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mLongT5: A Multilingual and Efficient Text-To-Text Transformer for\n  Longer Sequences", "abstract": "We present our work on developing a multilingual, efficient text-to-text\ntransformer that is suitable for handling long inputs. This model, called\nmLongT5, builds upon the architecture of LongT5, while leveraging the\nmultilingual datasets used for pretraining mT5 and the pretraining tasks of\nUL2. We evaluate this model on a variety of multilingual summarization and\nquestion-answering tasks, and the results show stronger performance for mLongT5\nwhen compared to existing multilingual models such as mBART or M-BERT.", "published": "2023-05-18 17:22:53", "link": "http://arxiv.org/abs/2305.11129v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model", "abstract": "Natural language generation models reproduce and often amplify the biases\npresent in their training data. Previous research explored using\nsequence-to-sequence rewriting models to transform biased model outputs (or\noriginal texts) into more gender-fair language by creating pseudo training data\nthrough linguistic rules. However, this approach is not practical for languages\nwith more complex morphology than English. We hypothesise that creating\ntraining data in the reverse direction, i.e. starting from gender-fair text, is\neasier for morphologically complex languages and show that it matches the\nperformance of state-of-the-art rewriting models for English. To eliminate the\nrule-based nature of data creation, we instead propose using machine\ntranslation models to create gender-biased text from real gender-fair text via\nround-trip translation. Our approach allows us to train a rewriting model for\nGerman without the need for elaborate handcrafted rules. The outputs of this\nmodel increased gender-fairness as shown in a human evaluation study.", "published": "2023-05-18 17:35:28", "link": "http://arxiv.org/abs/2305.11140v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Discourse Centric Evaluation of Machine Translation with a Densely\n  Annotated Parallel Corpus", "abstract": "Several recent papers claim human parity at sentence-level Machine\nTranslation (MT), especially in high-resource languages. Thus, in response, the\nMT community has, in part, shifted its focus to document-level translation.\nTranslating documents requires a deeper understanding of the structure and\nmeaning of text, which is often captured by various kinds of discourse\nphenomena such as consistency, coherence, and cohesion. However, this renders\nconventional sentence-level MT evaluation benchmarks inadequate for evaluating\nthe performance of context-aware MT systems. This paper presents a new dataset\nwith rich discourse annotations, built upon the large-scale parallel corpus BWB\nintroduced in Jiang et al. (2022). The new BWB annotation introduces four extra\nevaluation aspects, i.e., entity, terminology, coreference, and quotation,\ncovering 15,095 entity mentions in both languages. Using these annotations, we\nsystematically investigate the similarities and differences between the\ndiscourse structures of source and target languages, and the challenges they\npose to MT. We discover that MT outputs differ fundamentally from human\ntranslations in terms of their latent discourse structures. This gives us a new\nperspective on the challenges and opportunities in document-level MT. We make\nour resource publicly available to spur future research in document-level MT\nand the generalization to other language translation tasks.", "published": "2023-05-18 17:36:41", "link": "http://arxiv.org/abs/2305.11142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot\n  Relation Extractors", "abstract": "Recent work has shown that fine-tuning large language models (LLMs) on\nlarge-scale instruction-following datasets substantially improves their\nperformance on a wide range of NLP tasks, especially in the zero-shot setting.\nHowever, even advanced instruction-tuned LLMs still fail to outperform small\nLMs on relation extraction (RE), a fundamental information extraction task. We\nhypothesize that instruction-tuning has been unable to elicit strong RE\ncapabilities in LLMs due to RE's low incidence in instruction-tuning datasets,\nmaking up less than 1% of all tasks (Wang et al., 2022). To address this\nlimitation, we propose QA4RE, a framework that aligns RE with question\nanswering (QA), a predominant task in instruction-tuning datasets.\nComprehensive zero-shot RE experiments over four datasets with two series of\ninstruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework\nconsistently improves LLM performance, strongly verifying our hypothesis and\nenabling LLMs to outperform strong zero-shot baselines by a large margin.\nAdditionally, we provide thorough experiments and discussions to show the\nrobustness, few-shot effectiveness, and strong transferability of our QA4RE\nframework. This work illustrates a promising way of adapting LLMs to\nchallenging and underrepresented tasks by aligning these tasks with more common\ninstruction-tuning tasks like QA.", "published": "2023-05-18 17:48:03", "link": "http://arxiv.org/abs/2305.11159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language\n  Models", "abstract": "Factual consistency evaluation is often conducted using Natural Language\nInference (NLI) models, yet these models exhibit limited success in evaluating\nsummaries. Previous work improved such models with synthetic training data.\nHowever, the data is typically based on perturbed human-written summaries,\nwhich often differ in their characteristics from real model-generated summaries\nand have limited coverage of possible factual errors. Alternatively, large\nlanguage models (LLMs) have recently shown promising results in directly\nevaluating generative tasks, but are too computationally expensive for\npractical use. Motivated by these limitations, we introduce TrueTeacher, a\nmethod for generating synthetic data by annotating diverse model-generated\nsummaries using a LLM. Unlike prior work, TrueTeacher does not rely on\nhuman-written summaries, and is multilingual by nature. Experiments on the TRUE\nbenchmark show that a student model trained using our data, substantially\noutperforms both the state-of-the-art model with similar capacity, and the LLM\nteacher. In a systematic study, we compare TrueTeacher to existing synthetic\ndata generation methods and demonstrate its superiority and robustness to\ndomain-shift. We also show that our method generalizes to multilingual\nscenarios. Lastly, we release our large scale synthetic dataset (1.4M\nexamples), generated using TrueTeacher, and a checkpoint trained on this data.", "published": "2023-05-18 17:58:35", "link": "http://arxiv.org/abs/2305.11171v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Trends in Unsupervised Summarization", "abstract": "Unsupervised summarization is a powerful technique that enables training\nsummarizing models without requiring labeled datasets. This survey covers\ndifferent recent techniques and models used for unsupervised summarization. We\ncover extractive, abstractive, and hybrid models and strategies used to achieve\nunsupervised summarization. While the main focus of this survey is on recent\nresearch, we also cover some of the important previous research. We\nadditionally introduce a taxonomy, classifying different research based on\ntheir approach to unsupervised training. Finally, we discuss the current\napproaches and mention some datasets and evaluation methods.", "published": "2023-05-18 18:00:44", "link": "http://arxiv.org/abs/2305.11231v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Biases and the Impact of Multilingual Training across Multiple\n  Languages", "abstract": "Studies in bias and fairness in natural language processing have primarily\nexamined social biases within a single language and/or across few attributes\n(e.g. gender, race). However, biases can manifest differently across various\nlanguages for individual attributes. As a result, it is critical to examine\nbiases within each language and attribute. Of equal importance is to study how\nthese biases compare across languages and how the biases are affected when\ntraining a model on multilingual data versus monolingual data. We present a\nbias analysis across Italian, Chinese, English, Hebrew, and Spanish on the\ndownstream sentiment analysis task to observe whether specific demographics are\nviewed more positively. We study bias similarities and differences across these\nlanguages and investigate the impact of multilingual vs. monolingual training\ndata. We adapt existing sentiment bias templates in English to Italian,\nChinese, Hebrew, and Spanish for four attributes: race, religion, nationality,\nand gender. Our results reveal similarities in bias expression such as\nfavoritism of groups that are dominant in each language's culture (e.g.\nmajority religions and nationalities). Additionally, we find an increased\nvariation in predictions across protected groups, indicating bias\namplification, after multilingual finetuning in comparison to multilingual\npretraining.", "published": "2023-05-18 18:15:07", "link": "http://arxiv.org/abs/2305.11242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational thematics: Comparing algorithms for clustering the genres\n  of literary fiction", "abstract": "What are the best methods of capturing thematic similarity between literary\ntexts? Knowing the answer to this question would be useful for automatic\nclustering of book genres, or any other thematic grouping. This paper compares\na variety of algorithms for unsupervised learning of thematic similarities\nbetween texts, which we call \"computational thematics\". These algorithms belong\nto three steps of analysis: text preprocessing, extraction of text features,\nand measuring distances between the lists of features. Each of these steps\nincludes a variety of options. We test all the possible combinations of these\noptions: every combination of algorithms is given a task to cluster a corpus of\nbooks belonging to four pre-tagged genres of fiction. This clustering is then\nvalidated against the \"ground truth\" genre labels. Such comparison of\nalgorithms allows us to learn the best and the worst combinations for\ncomputational thematic analysis. To illustrate the sharp difference between the\nbest and the worst methods, we then cluster 5000 random novels from the\nHathiTrust corpus of fiction.", "published": "2023-05-18 18:32:03", "link": "http://arxiv.org/abs/2305.11251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning Implicit Sentiment with Chain-of-Thought Prompting", "abstract": "While sentiment analysis systems try to determine the sentiment polarities of\ngiven targets based on the key opinion expressions in input texts, in implicit\nsentiment analysis (ISA) the opinion cues come in an implicit and obscure\nmanner. Thus detecting implicit sentiment requires the common-sense and\nmulti-hop reasoning ability to infer the latent intent of opinion. Inspired by\nthe recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop\nReasoning (THOR) CoT framework to mimic the human-like reasoning process for\nISA. We design a three-step prompting principle for THOR to step-by-step induce\nthe implicit aspect, opinion, and finally the sentiment polarity. Our\nTHOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on\nsupervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%\nF1 on zero-shot setting. Our code is open at\nhttps://github.com/scofield7419/THOR-ISA.", "published": "2023-05-18 18:38:32", "link": "http://arxiv.org/abs/2305.11255v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational\n  Language Models", "abstract": "\\textit{\\textbf{\\textcolor{red}{Warning}:} This paper contains content that\nmay be offensive or upsetting.} Pretrained conversational agents have been\nexposed to safety issues, exhibiting a range of stereotypical human biases such\nas gender bias. However, there are still limited bias categories in current\nresearch, and most of them only focus on English. In this paper, we introduce a\nnew Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese\nconversational language models. Apart from those previous well-explored bias\ncategories, CHBias includes under-explored bias categories, such as ageism and\nappearance biases, which received less attention. We evaluate two popular\npretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias.\nFurthermore, to mitigate different biases, we apply several debiasing methods\nto the Chinese pretrained models. Experimental results show that these Chinese\npretrained models are potentially risky for generating texts that contain\nsocial biases, and debiasing methods using the proposed dataset can make\nresponse generation less biased while preserving the models' conversational\ncapabilities.", "published": "2023-05-18 18:58:30", "link": "http://arxiv.org/abs/2305.11262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Toponym Resolution with Better Candidate Generation,\n  Transformer-based Reranking, and Two-Stage Resolution", "abstract": "Geocoding is the task of converting location mentions in text into structured\ndata that encodes the geospatial semantics. We propose a new architecture for\ngeocoding, GeoNorm. GeoNorm first uses information retrieval techniques to\ngenerate a list of candidate entries from the geospatial ontology. Then it\nreranks the candidate entries using a transformer-based neural network that\nincorporates information from the ontology such as the entry's population. This\ngenerate-and-rerank process is applied twice: first to resolve the less\nambiguous countries, states, and counties, and second to resolve the remaining\nlocation mentions, using the identified countries, states, and counties as\ncontext. Our proposed toponym resolution framework achieves state-of-the-art\nperformance on multiple datasets. Code and models are available at\n\\url{https://github.com/clulab/geonorm}.", "published": "2023-05-18 21:52:48", "link": "http://arxiv.org/abs/2305.11315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in\n  Text-to-Image Generation", "abstract": "The field of text-to-image (T2I) generation has garnered significant\nattention both within the research community and among everyday users. Despite\nthe advancements of T2I models, a common issue encountered by users is the need\nfor repetitive editing of input prompts in order to receive a satisfactory\nimage, which is time-consuming and labor-intensive. Given the demonstrated text\ngeneration power of large-scale language models, such as GPT-k, we investigate\nthe potential of utilizing such models to improve the prompt editing process\nfor T2I generation. We conduct a series of experiments to compare the common\nedits made by humans and GPT-k, evaluate the performance of GPT-k in prompting\nT2I, and examine factors that may influence this process. We found that GPT-k\nmodels focus more on inserting modifiers while humans tend to replace words and\nphrases, which includes changes to the subject matter. Experimental results\nshow that GPT-k are more effective in adjusting modifiers rather than\npredicting spontaneous changes in the primary subject matters. Adopting the\nedit suggested by GPT-k models may reduce the percentage of remaining edits by\n20-30%.", "published": "2023-05-18 21:53:58", "link": "http://arxiv.org/abs/2305.11317v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Conversational Interfaces for Tabular Data\n  Analysis", "abstract": "Tabular data is the most common format to publish and exchange structured\ndata online. A clear example is the growing number of open data portals\npublished by public administrations. However, exploitation of these data\nsources is currently limited to technical people able to programmatically\nmanipulate and digest such data. As an alternative, we propose the use of\nchatbots to offer a conversational interface to facilitate the exploration of\ntabular data sources, including support for data analytics questions that are\nresponded via charts rendered by the chatbot. Moreover, our chatbots are\nautomatically generated from the data source itself thanks to the instantiation\nof a configurable collection of conversation patterns matched to the chatbot\nintents and entities.", "published": "2023-05-18 22:23:40", "link": "http://arxiv.org/abs/2305.11326v3", "categories": ["cs.CL", "I.2.7; D.2"], "primary_category": "cs.CL"}
{"title": "Paxion: Patching Action Knowledge in Video-Language Foundation Models", "abstract": "Action knowledge involves the understanding of textual, visual, and temporal\naspects of actions. We introduce the Action Dynamics Benchmark (ActionBench)\ncontaining two carefully designed probing tasks: Action Antonym and Video\nReversal, which targets multimodal alignment capabilities and temporal\nunderstanding skills of the model, respectively. Despite recent video-language\nmodels' (VidLM) impressive performance on various benchmark tasks, our\ndiagnostic tasks reveal their surprising deficiency (near-random performance)\nin action knowledge, suggesting that current models rely on object recognition\nabilities as a shortcut for action understanding. To remedy this, we propose a\nnovel framework, Paxion, along with a new Discriminative Video Dynamics\nModeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher\nnetwork to encode new action knowledge and a Knowledge Fuser component to\nintegrate the Patcher into frozen VidLMs without compromising their existing\ncapabilities. Due to limitations of the widely-used Video-Text Contrastive\n(VTC) loss for learning action knowledge, we introduce the DVDM objective to\ntrain the Knowledge Patcher. DVDM forces the model to encode the correlation\nbetween the action text and the correct ordering of video frames. Our extensive\nanalyses show that Paxion and DVDM together effectively fill the gap in action\nknowledge understanding (~50% to 80%), while maintaining or improving\nperformance on a wide spectrum of both object- and action-centric downstream\ntasks. The code and data will be made publicly available for research purposes\nat https://github.com/MikeWangWZHL/Paxion.git.", "published": "2023-05-18 03:53:59", "link": "http://arxiv.org/abs/2305.10683v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in\n  Natural Language Processing", "abstract": "Large-scale datasets in the real world inevitably involve label noise. Deep\nmodels can gradually overfit noisy labels and thus degrade model\ngeneralization. To mitigate the effects of label noise, learning with noisy\nlabels (LNL) methods are designed to achieve better generalization performance.\nDue to the lack of suitable datasets, previous studies have frequently employed\nsynthetic label noise to mimic real-world label noise. However, synthetic noise\nis not instance-dependent, making this approximation not always effective in\npractice. Recent research has proposed benchmarks for learning with real-world\nnoisy labels. However, the noise sources within may be single or fuzzy, making\nbenchmarks different from data with heterogeneous label noises in the real\nworld. To tackle these issues, we contribute NoisywikiHow, the largest NLP\nbenchmark built with minimal supervision. Specifically, inspired by human\ncognition, we explicitly construct multiple sources of label noise to imitate\nhuman errors throughout the annotation, replicating real-world noise, whose\ncorruption is affected by both ground-truth labels and instances. Moreover, we\nprovide a variety of noise levels to support controlled experiments on noisy\ndata, enabling us to evaluate LNL methods systematically and comprehensively.\nAfter that, we conduct extensive multi-dimensional experiments on a broad range\nof LNL methods, obtaining new and intriguing findings.", "published": "2023-05-18 05:01:04", "link": "http://arxiv.org/abs/2305.10709v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency", "abstract": "With growing capabilities of large language models, prompting them has become\nthe dominant way to access them. This has motivated the development of\nstrategies for automatically selecting effective language prompts. In this\npaper, we introduce prompt flatness, a new metric to quantify the expected\nutility of a language prompt. This metric is inspired by flatness\nregularization in statistical learning that quantifies the robustness of the\nmodel towards its parameter perturbations. We provide theoretical foundations\nfor this metric and its relationship with other prompt selection metrics,\nproviding a comprehensive understanding of existing methods. Empirically, we\nshow that combining prompt flatness with existing metrics improves both\nperformance and sample efficiency. Our metric outperforms the previous prompt\nselection metrics with an average increase of 5% in accuracy and 10% in Pearson\ncorrelation across 6 classification benchmarks.", "published": "2023-05-18 05:17:57", "link": "http://arxiv.org/abs/2305.10713v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Counterfactual Debiasing for Generating Factually Consistent Text\n  Summaries", "abstract": "Despite substantial progress in abstractive text summarization to generate\nfluent and informative texts, the factual inconsistency in the generated\nsummaries remains an important yet challenging problem to be solved. In this\npaper, we construct causal graphs for abstractive text summarization and\nidentify the intrinsic causes of the factual inconsistency, i.e., the language\nbias and irrelevancy bias, and further propose a debiasing framework, named\nCoFactSum, to alleviate the causal effects of these biases by counterfactual\nestimation. Specifically, the proposed CoFactSum provides two counterfactual\nestimation strategies, i.e., Explicit Counterfactual Masking with an explicit\ndynamic masking strategy, and Implicit Counterfactual Training with an implicit\ndiscriminative cross-attention mechanism. Meanwhile, we design a Debiasing\nDegree Adjustment mechanism to dynamically adapt the debiasing degree at each\ndecoding step. Extensive experiments on two widely-used summarization datasets\ndemonstrate the effectiveness of CoFactSum in enhancing the factual consistency\nof generated summaries compared with several baselines.", "published": "2023-05-18 06:15:45", "link": "http://arxiv.org/abs/2305.10736v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diffusion Language Models Generation Can Be Halted Early", "abstract": "Diffusion Language models (DLMs) are a promising avenue for text generation\ndue to their practical properties on tractable controllable generation. They\nalso have the advantage of not having to predict text autoregressively.\nHowever, despite these notable features, DLMs have not yet reached the\nperformance levels of their autoregressive counterparts. One of the ways to\nreduce the performance gap between these two types of language models is to\nspeed up the generation of DLMs. Therefore, we propose a novel methodology to\naddress this issue in this work. It enables the execution of more generation\nsteps within a given time frame, leading to higher-quality outputs.\nSpecifically, our methods estimate DLMs completeness of text generation and\nallow adaptive halting of the generation process. We evaluate our methods on\nPlaid, SSD, and CDCD DLMs and create a cohesive perspective on their generation\nworkflows. Finally, we confirm that our methods allow halting these models and\ndecrease the generation time by $10$-$40$\\% without a drop in the quality of\nmodel samples.", "published": "2023-05-18 08:56:05", "link": "http://arxiv.org/abs/2305.10818v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Ahead-of-Time P-Tuning", "abstract": "In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel\nparameter-efficient fine-tuning method for pre-trained Language Models (LMs)\nthat adds input-dependent bias before each Transformer layer. We evaluate AoT\nP-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa\nmodels, showing that it outperforms BitFit and is comparable or better than\nother baseline methods for efficient fine-tuning. Additionally, we assess the\ninference overhead of AoT P-Tuning and demonstrate that it introduces\nnegligible overhead compared to established baseline methods. Our method\nenables multi-task inference with a single backbone LM, making it a practical\nsolution for real-world applications.", "published": "2023-05-18 09:24:53", "link": "http://arxiv.org/abs/2305.10835v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models can be Guided to Evade AI-Generated Text Detection", "abstract": "Large language models (LLMs) have shown remarkable performance in various\ntasks and have been extensively utilized by the public. However, the increasing\nconcerns regarding the misuse of LLMs, such as plagiarism and spamming, have\nled to the development of multiple detectors, including fine-tuned classifiers\nand statistical methods. In this study, we equip LLMs with prompts, rather than\nrelying on an external paraphraser, to evaluate the vulnerability of these\ndetectors. We propose a novel Substitution-based In-Context example\nOptimization method (SICO) to automatically construct prompts for evading the\ndetectors. SICO is cost-efficient as it requires only 40 human-written examples\nand a limited number of LLM inferences to generate a prompt. Moreover, once a\ntask-specific prompt has been constructed, it can be universally used against a\nwide range of detectors. Extensive experiments across three real-world tasks\ndemonstrate that SICO significantly outperforms the paraphraser baselines and\nenables GPT-3.5 to successfully evade six detectors, decreasing their AUC by\n0.5 on average. Furthermore, a comprehensive human evaluation show that the\nSICO-generated text achieves human-level readability and task completion rates,\nwhile preserving high imperceptibility. Finally, we propose an ensemble\napproach to enhance the robustness of detectors against SICO attack. The code\nis publicly available at https://github.com/ColinLu50/Evade-GPT-Detector.", "published": "2023-05-18 10:03:25", "link": "http://arxiv.org/abs/2305.10847v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergent Communication with Attention", "abstract": "To develop computational agents that better communicate using their own\nemergent language, we endow the agents with an ability to focus their attention\non particular concepts in the environment. Humans often understand an object or\nscene as a composite of concepts and those concepts are further mapped onto\nwords. We implement this intuition as cross-modal attention mechanisms in\nSpeaker and Listener agents in a referential game and show attention leads to\nmore compositional and interpretable emergent language. We also demonstrate how\nattention aids in understanding the learned communication protocol by\ninvestigating the attention weights associated with each message symbol and the\nalignment of attention weights between Speaker and Listener agents. Overall,\nour results suggest that attention is a promising mechanism for developing more\nhuman-like emergent language.", "published": "2023-05-18 12:31:45", "link": "http://arxiv.org/abs/2305.10920v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Event Extraction from Historical Newspaper Adverts", "abstract": "NLP methods can aid historians in analyzing textual materials in greater\nvolumes than manually feasible. Developing such methods poses substantial\nchallenges though. First, acquiring large, annotated historical datasets is\ndifficult, as only domain experts can reliably label them. Second, most\navailable off-the-shelf NLP models are trained on modern language texts,\nrendering them significantly less effective when applied to historical corpora.\nThis is particularly problematic for less well studied tasks, and for languages\nother than English. This paper addresses these challenges while focusing on the\nunder-explored task of event extraction from a novel domain of historical\ntexts. We introduce a new multilingual dataset in English, French, and Dutch\ncomposed of newspaper ads from the early modern colonial period reporting on\nenslaved people who liberated themselves from enslavement. We find that: 1)\neven with scarce annotated data, it is possible to achieve surprisingly good\nresults by formulating the problem as an extractive QA task and leveraging\nexisting datasets and models for modern languages; and 2) cross-lingual\nlow-resource learning for historical languages is highly challenging, and\nmachine translation of the historical datasets to the considered target\nlanguages is, in practice, often the best-performing solution.", "published": "2023-05-18 12:40:41", "link": "http://arxiv.org/abs/2305.10928v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Off-Target Problem of Zero-Shot Multilingual Neural Machine\n  Translation", "abstract": "While multilingual neural machine translation has achieved great success, it\nsuffers from the off-target issue, where the translation is in the wrong\nlanguage. This problem is more pronounced on zero-shot translation tasks. In\nthis work, we find that failing in encoding discriminative target language\nsignal will lead to off-target and a closer lexical distance (i.e.,\nKL-divergence) between two languages' vocabularies is related with a higher\noff-target rate. We also find that solely isolating the vocab of different\nlanguages in the decoder can alleviate the problem. Motivated by the findings,\nwe propose Language Aware Vocabulary Sharing (LAVS), a simple and effective\nalgorithm to construct the multilingual vocabulary, that greatly alleviates the\noff-target problem of the translation model by increasing the KL-divergence\nbetween languages. We conduct experiments on a multilingual machine translation\nbenchmark in 11 languages. Experiments show that the off-target rate for 90\ntranslation tasks is reduced from 29\\% to 8\\%, while the overall BLEU score is\nimproved by an average of 1.9 points without extra training cost or sacrificing\nthe supervised directions' performance. We release the code at\nhttps://github.com/PKUnlp-icler/Off-Target-MNMT for reproduction.", "published": "2023-05-18 12:43:31", "link": "http://arxiv.org/abs/2305.10930v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making More of Little Data: Improving Low-Resource Automatic Speech\n  Recognition Using Data Augmentation", "abstract": "The performance of automatic speech recognition (ASR) systems has advanced\nsubstantially in recent years, particularly for languages for which a large\namount of transcribed speech is available. Unfortunately, for low-resource\nlanguages, such as minority languages, regional languages or dialects, ASR\nperformance generally remains much lower. In this study, we investigate whether\ndata augmentation techniques could help improve low-resource ASR performance,\nfocusing on four typologically diverse minority languages or language variants\n(West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For\nall four languages, we examine the use of self-training, where an ASR system\ntrained with the available human-transcribed data is used to generate\ntranscriptions, which are then combined with the original data to train a new\nASR system. For Gronings, for which there was a pre-existing text-to-speech\n(TTS) system available, we also examined the use of TTS to generate ASR\ntraining data from text-only sources. We find that using a self-training\napproach consistently yields improved performance (a relative WER reduction up\nto 20.5% compared to using an ASR system trained on 24 minutes of manually\ntranscribed speech). The performance gain from TTS augmentation for Gronings\nwas even stronger (up to 25.5% relative reduction in WER compared to a system\nbased on 24 minutes of manually transcribed speech). In sum, our results show\nthe benefit of using self-training or (if possible) TTS-generated data as an\nefficient solution to overcome the limitations of data availability for\nresource-scarce languages in order to improve ASR performance.", "published": "2023-05-18 13:20:38", "link": "http://arxiv.org/abs/2305.10951v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Less is More! A slim architecture for optimal language translation", "abstract": "The softmax attention mechanism has emerged as a noteworthy development in\nthe field of Artificial Intelligence research, building on the successes of\nTransformer-based architectures. However, their ever increasing sizes\nnecessitate ever increasing computational memory, that limits their usage. We\npropose KgV, a sigmoid gating mechanism that, in conjunction with softmax\nattention, significantly boosts performance without increasing architecture\nsize. To amend the size requirements, we leverage Tensor Chains to identify and\nprune the excess parameters. We find that such excess resides primarily within\nthe embedding layer, and not in the output linear layer. To further improve\nembedding and significantly reduce parameters, we introduce H-SoftPOS, a\nhierarchical embedding layer which simultaneously enhances performance.\nRemarkably, on the WMT14 English-German validation set, our approach yields a\nthreefold reduction in perplexity, surpassing the current state-of-the-art,\nwhile reducing parameter counts also by a factor of 3. When we further reduce\nthe number of parameters up to sevenfold, we can still achieve a 21\\% decrease\nin perplexity with respect to the baseline Transformer. To understand\ngeneralization capabilities, we conduct experiments on the 7 language pairs of\nthe WMT17 dataset. Our method outperforms existing techniques in terms of test\nloss while simultaneously halving the number of parameters. Moreover, we\nobserve a 70 times reduction in variance with respect to the prior\nstate-of-the-art. In conclusion, our proposed method yields significant\nimprovements in performance and much lower memory cost. We call the resulting\narchitecture Anthe.", "published": "2023-05-18 14:09:52", "link": "http://arxiv.org/abs/2305.10991v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How does the task complexity of masked pretraining objectives affect\n  downstream performance?", "abstract": "Masked language modeling (MLM) is a widely used self-supervised pretraining\nobjective, where a model needs to predict an original token that is replaced\nwith a mask given contexts. Although simpler and computationally efficient\npretraining objectives, e.g., predicting the first character of a masked token,\nhave recently shown comparable results to MLM, no objectives with a masking\nscheme actually outperform it in downstream tasks. Motivated by the assumption\nthat their lack of complexity plays a vital role in the degradation, we\nvalidate whether more complex masked objectives can achieve better results and\ninvestigate how much complexity they should have to perform comparably to MLM.\nOur results using GLUE, SQuAD, and Universal Dependencies benchmarks\ndemonstrate that more complicated objectives tend to show better downstream\nresults with at least half of the MLM complexity needed to perform comparably\nto MLM. Finally, we discuss how we should pretrain a model using a masked\nobjective from the task complexity perspective.", "published": "2023-05-18 14:11:57", "link": "http://arxiv.org/abs/2305.10992v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty Guided Label Denoising for Document-level Distant Relation\n  Extraction", "abstract": "Document-level relation extraction (DocRE) aims to infer complex semantic\nrelations among entities in a document. Distant supervision (DS) is able to\ngenerate massive auto-labeled data, which can improve DocRE performance. Recent\nworks leverage pseudo labels generated by the pre-denoising model to reduce\nnoise in DS data. However, unreliable pseudo labels bring new noise, e.g.,\nadding false pseudo labels and losing correct DS labels. Therefore, how to\nselect effective pseudo labels to denoise DS data is still a challenge in\ndocument-level distant relation extraction. To tackle this issue, we introduce\nuncertainty estimation technology to determine whether pseudo labels can be\ntrusted. In this work, we propose a Document-level distant Relation Extraction\nframework with Uncertainty Guided label denoising, UGDRE. Specifically, we\npropose a novel instance-level uncertainty estimation method, which measures\nthe reliability of the pseudo labels with overlapping relations. By further\nconsidering the long-tail problem, we design dynamic uncertainty thresholds for\ndifferent types of relations to filter high-uncertainty pseudo labels. We\nconduct experiments on two public datasets. Our framework outperforms strong\nbaselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset.", "published": "2023-05-18 15:15:56", "link": "http://arxiv.org/abs/2305.11029v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval", "abstract": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.", "published": "2023-05-18 15:43:09", "link": "http://arxiv.org/abs/2305.11052v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Self-supervised Fine-tuning for Improved Content Representations by\n  Speaker-invariant Clustering", "abstract": "Self-supervised speech representation models have succeeded in various tasks,\nbut improving them for content-related problems using unlabeled data is\nchallenging. We propose speaker-invariant clustering (Spin), a novel\nself-supervised learning method that clusters speech representations and\nperforms swapped prediction between the original and speaker-perturbed\nutterances. Spin disentangles speaker information and preserves content\nrepresentations with just 45 minutes of fine-tuning on a single GPU. Spin\nimproves pre-trained networks and outperforms prior methods in speech\nrecognition and acoustic unit discovery.", "published": "2023-05-18 15:59:36", "link": "http://arxiv.org/abs/2305.11072v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Inspecting the Geographical Representativeness of Images from\n  Text-to-Image Models", "abstract": "Recent progress in generative models has resulted in models that produce both\nrealistic as well as relevant images for most textual inputs. These models are\nbeing used to generate millions of images everyday, and hold the potential to\ndrastically impact areas such as generative art, digital marketing and data\naugmentation. Given their outsized impact, it is important to ensure that the\ngenerated content reflects the artifacts and surroundings across the globe,\nrather than over-representing certain parts of the world. In this paper, we\nmeasure the geographical representativeness of common nouns (e.g., a house)\ngenerated through DALL.E 2 and Stable Diffusion models using a crowdsourced\nstudy comprising 540 participants across 27 countries. For deliberately\nunderspecified inputs without country names, the generated images most reflect\nthe surroundings of the United States followed by India, and the top\ngenerations rarely reflect surroundings from all other countries (average score\nless than 3 out of 5). Specifying the country names in the input increases the\nrepresentativeness by 1.44 points on average for DALL.E 2 and 0.75 for Stable\nDiffusion, however, the overall scores for many countries still remain low,\nhighlighting the need for future models to be more geographically inclusive.\nLastly, we examine the feasibility of quantifying the geographical\nrepresentativeness of generated images without conducting user studies.", "published": "2023-05-18 16:08:11", "link": "http://arxiv.org/abs/2305.11080v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LLMScore: Unveiling the Power of Large Language Models in Text-to-Image\n  Synthesis Evaluation", "abstract": "Existing automatic evaluation on text-to-image synthesis can only provide an\nimage-text matching score, without considering the object-level\ncompositionality, which results in poor correlation with human judgments. In\nthis work, we propose LLMScore, a new framework that offers evaluation scores\nwith multi-granularity compositionality. LLMScore leverages the large language\nmodels (LLMs) to evaluate text-to-image models. Initially, it transforms the\nimage into image-level and object-level visual descriptions. Then an evaluation\ninstruction is fed into the LLMs to measure the alignment between the\nsynthesized image and the text, ultimately generating a score accompanied by a\nrationale. Our substantial analysis reveals the highest correlation of LLMScore\nwith human judgments on a wide range of datasets (Attribute Binding Contrast,\nConcept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScore\nachieves Kendall's tau correlation with human evaluations that is 58.8% and\n31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP,\nrespectively.", "published": "2023-05-18 16:57:57", "link": "http://arxiv.org/abs/2305.11116v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Comparing Machines and Children: Using Developmental Psychology\n  Experiments to Assess the Strengths and Weaknesses of LaMDA Responses", "abstract": "Developmental psychologists have spent decades devising experiments to test\nthe intelligence and knowledge of infants and children, tracing the origin of\ncrucial concepts and capacities. Moreover, experimental techniques in\ndevelopmental psychology have been carefully designed to discriminate the\ncognitive capacities that underlie particular behaviors. We propose that using\nclassical experiments from child development is a particularly effective way to\nprobe the computational abilities of AI models, in general, and LLMs in\nparticular. First, the methodological techniques of developmental psychology,\nsuch as the use of novel stimuli to control for past experience or control\nconditions to determine whether children are using simple associations, can be\nequally helpful for assessing the capacities of LLMs. In parallel, testing LLMs\nin this way can tell us whether the information that is encoded in text is\nsufficient to enable particular responses, or whether those responses depend on\nother kinds of information, such as information from exploration of the\nphysical world. In this work we adapt classical developmental experiments to\nevaluate the capabilities of LaMDA, a large language model from Google. We\npropose a novel LLM Response Score (LRS) metric which can be used to evaluate\nother language models, such as GPT. We find that LaMDA generates appropriate\nresponses that are similar to those of children in experiments involving social\nunderstanding, perhaps providing evidence that knowledge of these domains is\ndiscovered through language. On the other hand, LaMDA's responses in early\nobject and action understanding, theory of mind, and especially causal\nreasoning tasks are very different from those of young children, perhaps\nshowing that these domains require more real-world, self-initiated exploration\nand cannot simply be learned from patterns in language input.", "published": "2023-05-18 18:15:43", "link": "http://arxiv.org/abs/2305.11243v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Writing your own book: A method for going from closed to open book QA to\n  improve robustness and performance of smaller LLMs", "abstract": "We introduce two novel methods, Tree-Search and Self-contextualizing QA,\ndesigned to enhance the performance of large language models (LLMs) in\nquestion-answering tasks. Tree-Search is a sampling technique specifically\ncreated to extract diverse information from an LLM for a given prompt.\nSelf-contextualizing QA leverages Tree-Search to enable the model to create its\nown context using a wide range of information relevant to the prompt, evaluate\nit explicitly and return a open book answer to the initial prompt . We\ndemonstrate that the quality of generated answers improves according to various\nmetrics, including accuracy, informativeness, coherence, and consistency, as\nevaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods\nresult in increased robustness and that performance is positively correlated\nwith tree size, benefiting both answer quality and robustness. Finally, we\ndiscuss other promising applications of Tree-Search, highlighting its potential\nto enhance a broad range of tasks beyond question-answering.\n  \\noindent We also discuss several areas for future work, including refining\nthe Tree-Search and Self-Contextualizing QA methods, improving the coherence of\nthe generated context, and investigating the impact of bootstrapping on model\nrobustness", "published": "2023-05-18 22:47:06", "link": "http://arxiv.org/abs/2305.11334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak\n  Signals", "abstract": "The emergence of social media as one of the main platforms for people to\naccess news has enabled the wide dissemination of fake news. This has motivated\nnumerous studies on automating fake news detection. Although there have been\nlimited attempts at unsupervised fake news detection, their performance suffers\ndue to not exploiting the knowledge from various modalities related to news\nrecords and due to the presence of various latent biases in the existing news\ndatasets. To address these limitations, this work proposes an effective\nframework for unsupervised fake news detection, which first embeds the\nknowledge available in four modalities in news records and then proposes a\nnovel noise-robust self-supervised learning technique to identify the veracity\nof news records from the multi-modal embeddings. Also, we propose a novel\ntechnique to construct news datasets minimizing the latent biases in existing\nnews datasets. Following the proposed approach for dataset construction, we\nproduce a Large-scale Unlabelled News Dataset consisting 419,351 news articles\nrelated to COVID-19, acronymed as LUND-COVID. We trained the proposed\nunsupervised framework using LUND-COVID to exploit the potential of large\ndatasets, and evaluate it using a set of existing labelled datasets. Our\nresults show that the proposed unsupervised framework largely outperforms\nexisting unsupervised baselines for different tasks such as multi-modal fake\nnews detection, fake news early detection and few-shot fake news detection,\nwhile yielding notable improvements for unseen domains during training.", "published": "2023-05-18 23:49:31", "link": "http://arxiv.org/abs/2305.11349v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark", "abstract": "Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard\nto benchmark the performance of Self-Supervised Learning (SSL) models on\nvarious speech processing tasks. However, SUPERB largely considers English\nspeech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),\ncovering 143 languages (ranging from high-resource to endangered), and\nconsidering both automatic speech recognition and language identification.\nFollowing the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and\nemploys a simple framework for multilingual tasks by learning a shallow\ndownstream model. Similar to the SUPERB benchmark, we find speech SSL models\ncan significantly improve performance compared to FBANK features. Furthermore,\nwe find that multilingual models do not always perform better than their\nmonolingual counterparts. We will release ML-SUPERB as a challenge with\norganized datasets and reproducible training scripts for future multilingual\nrepresentation research.", "published": "2023-05-18 00:01:27", "link": "http://arxiv.org/abs/2305.10615v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Language Models Meet World Models: Embodied Experiences Enhance Language\n  Models", "abstract": "While large language models (LMs) have shown remarkable capabilities across\nnumerous tasks, they often struggle with simple reasoning and planning in\nphysical environments, such as understanding object permanence or planning\nhousehold activities. The limitation arises from the fact that LMs are trained\nonly on written text and miss essential embodied knowledge and skills. In this\npaper, we propose a new paradigm of enhancing LMs by finetuning them with world\nmodels, to gain diverse embodied knowledge while retaining their general\nlanguage capabilities. Our approach deploys an embodied agent in a world model,\nparticularly a simulator of the physical world (VirtualHome), and acquires a\ndiverse set of embodied experiences through both goal-oriented planning and\nrandom exploration. These experiences are then used to finetune LMs to teach\ndiverse abilities of reasoning and acting in the physical world, e.g., planning\nand completing goals, object permanence and tracking, etc. Moreover, it is\ndesirable to preserve the generality of LMs during finetuning, which\nfacilitates generalizing the embodied knowledge across tasks rather than being\ntied to specific simulations. We thus further introduce the classical (EWC) for\nselective weight updates, combined with low-rank adapters (LoRA) for training\nefficiency. Extensive experiments show our approach substantially improves base\nLMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs\n(1.3B, 6B, and 13B) enhanced by our approach match or even outperform much\nlarger LMs (e.g., ChatGPT).", "published": "2023-05-18 00:35:38", "link": "http://arxiv.org/abs/2305.10626v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BioAug: Conditional Generation based Data Augmentation for Low-Resource\n  Biomedical NER", "abstract": "Biomedical Named Entity Recognition (BioNER) is the fundamental task of\nidentifying named entities from biomedical text. However, BioNER suffers from\nsevere data scarcity and lacks high-quality labeled data due to the highly\nspecialized and expert knowledge required for annotation. Though data\naugmentation has shown to be highly effective for low-resource NER in general,\nexisting data augmentation techniques fail to produce factual and diverse\naugmentations for BioNER. In this paper, we present BioAug, a novel data\naugmentation framework for low-resource BioNER. BioAug, built on BART, is\ntrained to solve a novel text reconstruction task based on selective masking\nand knowledge augmentation. Post training, we perform conditional generation\nand generate diverse augmentations conditioning BioAug on selectively corrupted\ntext similar to the training stage. We demonstrate the effectiveness of BioAug\non 5 benchmark BioNER datasets and show that BioAug outperforms all our\nbaselines by a significant margin (1.5%-21.5% absolute improvement) and is able\nto generate augmentations that are both more factual and diverse. Code:\nhttps://github.com/Sreyan88/BioAug.", "published": "2023-05-18 02:04:38", "link": "http://arxiv.org/abs/2305.10647v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs", "abstract": "In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding\nPrompt-and-Refine strategy (Figure 3), two simple but effective\n\\textbf{training-free} methods to decrease the Token Display Time (TDT) of\nstreaming ASR models \\textbf{without any accuracy loss}. The core idea of\nZeroPrompt is to append zeroed content to each chunk during inference, which\nacts like a prompt to encourage the model to predict future tokens even before\nthey were spoken. We argue that streaming acoustic encoders naturally have the\nmodeling ability of Masked Language Models and our experiments demonstrate that\nZeroPrompt is engineering cheap and can be applied to streaming acoustic\nencoders on any dataset without any accuracy loss. Specifically, compared with\nour baseline models, we achieve 350 $\\sim$ 700ms reduction on First Token\nDisplay Time (TDT-F) and 100 $\\sim$ 400ms reduction on Last Token Display Time\n(TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and\nLibrispeech datasets.", "published": "2023-05-18 02:08:33", "link": "http://arxiv.org/abs/2305.10649v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Speech Separation based on Contrastive Learning and Deep Modularization", "abstract": "The current monaural state of the art tools for speech separation relies on\nsupervised learning. This means that they must deal with permutation problem,\nthey are impacted by the mismatch on the number of speakers used in training\nand inference. Moreover, their performance heavily relies on the presence of\nhigh-quality labelled data. These problems can be effectively addressed by\nemploying a fully unsupervised technique for speech separation. In this paper,\nwe use contrastive learning to establish the representations of frames then use\nthe learned representations in the downstream deep modularization task.\nConcretely, we demonstrate experimentally that in speech separation, different\nframes of a speaker can be viewed as augmentations of a given hidden standard\nframe of that speaker. The frames of a speaker contain enough prosodic\ninformation overlap which is key in speech separation. Based on this, we\nimplement a self-supervised learning to learn to minimize the distance between\nframes belonging to a given speaker. The learned representations are used in a\ndownstream deep modularization task to cluster frames based on speaker\nidentity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix\nshows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively\nin WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7\nrespectively in WSJ0-2mix. Its greatest strength being that as the number of\nspeakers increase, its performance does not degrade significantly.", "published": "2023-05-18 02:19:05", "link": "http://arxiv.org/abs/2305.10652v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A unified front-end framework for English text-to-speech synthesis", "abstract": "The front-end is a critical component of English text-to-speech (TTS)\nsystems, responsible for extracting linguistic features that are essential for\na text-to-speech model to synthesize speech, such as prosodies and phonemes.\nThe English TTS front-end typically consists of a text normalization (TN)\nmodule, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme\n(G2P) module. However, current research on the English TTS front-end focuses\nsolely on individual modules, neglecting the interdependence between them and\nresulting in sub-optimal performance for each module. Therefore, this paper\nproposes a unified front-end framework that captures the dependencies among the\nEnglish TTS front-end modules. Extensive experiments have demonstrated that the\nproposed method achieves state-of-the-art (SOTA) performance in all modules.", "published": "2023-05-18 02:57:54", "link": "http://arxiv.org/abs/2305.10666v3", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Think Outside the Code: Brainstorming Boosts Large Language Models in\n  Code Generation", "abstract": "Code generation aims to automatically generate source code from high-level\ntask specifications, which can significantly increase productivity of software\nengineering. Recently, approaches based on large language models (LLMs) have\nshown remarkable code generation abilities on simple tasks. However, generate\ncode for more complex tasks, such as competition-level problems, remains\nchallenging. In this paper, we introduce Brainstorm framework for code\ngeneration. It leverages a brainstorming step that generates and selects\ndiverse thoughts on the problem to facilitate algorithmic reasoning, where the\nthoughts are possible blueprint of solving the problem. We demonstrate that\nBrainstorm significantly enhances the ability of LLMs to solve\ncompetition-level programming problems, resulting in a more than 50% increase\nin the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving\nstate-of-the-art performance. Furthermore, our experiments conducted on\nLeetCode contests show that our framework boosts the ability of ChatGPT to a\nlevel comparable to that of human programmers.", "published": "2023-05-18 03:32:54", "link": "http://arxiv.org/abs/2305.10679v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Accurate and Reliable Confidence Estimation Based on Non-Autoregressive\n  End-to-End Speech Recognition System", "abstract": "Estimating confidence scores for recognition results is a classic task in ASR\nfield and of vital importance for kinds of downstream tasks and training\nstrategies. Previous end-to-end~(E2E) based confidence estimation models (CEM)\npredict score sequences of equal length with input transcriptions, leading to\nunreliable estimation when deletion and insertion errors occur. In this paper\nwe proposed CIF-Aligned confidence estimation model (CA-CEM) to achieve\naccurate and reliable confidence estimation based on novel non-autoregressive\nE2E ASR model - Paraformer. CA-CEM utilizes the modeling character of\ncontinuous integrate-and-fire (CIF) mechanism to generate token-synchronous\nacoustic embedding, which solves the estimation failure issue above. We measure\nthe quality of estimation with AUC and RMSE in token level and ECE-U - a\nproposed metrics in utterance level. CA-CEM gains 24% and 19% relative\nreduction on ECE-U and also better AUC and RMSE on two test sets. Furthermore,\nwe conduct analysis to explore the potential of CEM for different ASR related\nusage.", "published": "2023-05-18 03:34:50", "link": "http://arxiv.org/abs/2305.10680v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RMSSinger: Realistic-Music-Score based Singing Voice Synthesis", "abstract": "We are interested in a challenging task, Realistic-Music-Score based Singing\nVoice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices\ngiven realistic music scores with different note types (grace, slur, rest,\netc.). Though significant progress has been achieved, recent singing voice\nsynthesis (SVS) methods are limited to fine-grained music scores, which require\na complicated data collection pipeline with time-consuming manual annotation to\nalign music notes with phonemes. Furthermore, these manual annotation destroys\nthe regularity of note durations in music scores, making fine-grained music\nscores inconvenient for composing. To tackle these challenges, we propose\nRMSSinger, the first RMS-SVS method, which takes realistic music scores as\ninput, eliminating most of the tedious manual annotation and avoiding the\naforementioned inconvenience. Note that music scores are based on words rather\nthan phonemes, in RMSSinger, we introduce word-level modeling to avoid the\ntime-consuming phoneme duration annotation and the complicated phoneme-level\nmel-note alignment. Furthermore, we propose the first diffusion-based pitch\nmodeling method, which ameliorates the naturalness of existing pitch-modeling\nmethods. To achieve these, we collect a new dataset containing realistic music\nscores and singing voices according to these realistic music scores from\nprofessional singers. Extensive experiments on the dataset demonstrate the\neffectiveness of our methods. Audio samples are available at\nhttps://rmssinger.github.io/.", "published": "2023-05-18 03:57:51", "link": "http://arxiv.org/abs/2305.10686v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ReGen: Zero-Shot Text Classification via Training Data Generation with\n  Progressive Dense Retrieval", "abstract": "With the development of large language models (LLMs), zero-shot learning has\nattracted much attention for various NLP tasks. Different from prior works that\ngenerate training data with billion-scale natural language generation (NLG)\nmodels, we propose a retrieval-enhanced framework to create training data from\na general-domain unlabeled corpus. To realize this, we first conduct\ncontrastive pretraining to learn an unsupervised dense retriever for extracting\nthe most relevant documents using class-descriptive verbalizers. We then\nfurther propose two simple strategies, namely Verbalizer Augmentation with\nDemonstrations and Self-consistency Guided Filtering to improve the topic\ncoverage of the dataset while removing noisy examples. Experiments on nine\ndatasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines\nand saves around 70% of the time compared to baselines using large NLG models.\nBesides, REGEN can be naturally integrated with recently proposed large\nlanguage models to boost performance.", "published": "2023-05-18 04:30:09", "link": "http://arxiv.org/abs/2305.10703v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diffusion-Based Speech Enhancement with Joint Generative and Predictive\n  Decoders", "abstract": "Diffusion-based generative speech enhancement (SE) has recently received\nattention, but reverse diffusion remains time-consuming. One solution is to\ninitialize the reverse diffusion process with enhanced features estimated by a\npredictive SE system. However, the pipeline structure currently does not\nconsider for a combined use of generative and predictive decoders. The\npredictive decoder allows us to use the further complementarity between\npredictive and diffusion-based generative SE. In this paper, we propose a\nunified system that use jointly generative and predictive decoders across two\nlevels. The encoder encodes both generative and predictive information at the\nshared encoding level. At the decoded feature level, we fuse the two decoded\nfeatures by generative and predictive decoders. Specifically, the two SE\nmodules are fused in the initial and final diffusion steps: the initial fusion\ninitializes the diffusion process with the predictive SE to improve\nconvergence, and the final fusion combines the two complementary SE outputs to\nenhance SE performance. Experiments conducted on the Voice-Bank dataset\ndemonstrate that incorporating predictive information leads to faster decoding\nand higher PESQ scores compared with other score-based diffusion SE (StoRM and\nSGMSE+).", "published": "2023-05-18 06:10:49", "link": "http://arxiv.org/abs/2305.10734v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DQ-Whisper: Joint Distillation and Quantization for Efficient\n  Multilingual Speech Recognition", "abstract": "As a popular multilingual and multitask pre-trained speech model, Whisper has\nthe problem of curse of multilinguality. To enhance multilingual capabilities\nin small Whisper models, we propose DQ-Whisper, a novel joint distillation and\nquantization framework to compress Whisper for efficient inference. Firstly, we\npropose a novel dynamic matching distillation strategy. Then, a\nquantization-aware distillation framework is introduced to integrate\nquantization with distillation. Experimental results on various multilingual\ndatasets show that our suggested distillation approach can effectively enhance\nthe multilingual capabilities of small Whisper models without increasing\ncomputational costs. Up to 5.18x reduction in model size is achieved with\nmarginal performance degradation. In addition, quantization is compatible with\ndistillation, which can result in a higher compression rate.", "published": "2023-05-18 08:00:09", "link": "http://arxiv.org/abs/2305.10788v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AIwriting: Relations Between Image Generation and Digital Writing", "abstract": "During 2022, both transformer-based AI text generation sys-tems such as GPT-3\nand AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion\nmade exponential leaps forward and are unquestionably altering the fields of\ndigital art and electronic literature. In this panel a group of electronic\nliterature authors and theorists consider new oppor-tunities for human\ncreativity presented by these systems and present new works have produced\nduring the past year that specifically address these systems as environments\nfor literary expressions that are translated through iterative interlocutive\nprocesses into visual representations. The premise that binds these\npresentations is that these systems and the works gener-ated must be considered\nfrom a literary perspective, as they originate in human writing. In works\nranging from a visual memoir of the personal experience of a health crisis, to\ninterac-tive web comics, to architectures based on abstract poetic language, to\npolitical satire, four artists explore the capabili-ties of these writing\nenvironments for new genres of literary artist practice, while a digital\nculture theorist considers the origins and effects of the particular training\ndatasets of human language and images on which these new hybrid forms are\nbased.", "published": "2023-05-18 09:23:05", "link": "http://arxiv.org/abs/2305.10834v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM", "J.5"], "primary_category": "cs.AI"}
{"title": "A Lexical-aware Non-autoregressive Transformer-based ASR Model", "abstract": "Non-autoregressive automatic speech recognition (ASR) has become a mainstream\nof ASR modeling because of its fast decoding speed and satisfactory result. To\nfurther boost the performance, relaxing the conditional independence assumption\nand cascading large-scaled pre-trained models are two active research\ndirections. In addition to these strategies, we propose a lexical-aware\nnon-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of\nan acoustic encoder, a speech-text shared encoder, and a speech-text shared\ndecoder. The acoustic encoder is used to process the input speech features as\nusual, and the speech-text shared encoder and decoder are designed to train\nspeech and text data simultaneously. By doing so, LA-NAT aims to make the ASR\nmodel aware of lexical information, so the resulting model is expected to\nachieve better results by leveraging the learned linguistic knowledge. A series\nof experiments are conducted on the AISHELL-1, CSJ, and TEDLIUM 2 datasets.\nAccording to the experiments, the proposed LA-NAT can provide superior results\nthan other recently proposed non-autoregressive ASR models. In addition, LA-NAT\nis a relatively compact model than most non-autoregressive ASR models, and it\nis about 58 times faster than the classic autoregressive model.", "published": "2023-05-18 09:50:47", "link": "http://arxiv.org/abs/2305.10839v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Semantically Aligned Task Decomposition in Multi-Agent Reinforcement\n  Learning", "abstract": "The difficulty of appropriately assigning credit is particularly heightened\nin cooperative MARL with sparse reward, due to the concurrent time and\nstructural scales involved. Automatic subgoal generation (ASG) has recently\nemerged as a viable MARL approach inspired by utilizing subgoals in\nintrinsically motivated reinforcement learning. However, end-to-end learning of\ncomplex task planning from sparse rewards without prior knowledge, undoubtedly\nrequires massive training samples. Moreover, the diversity-promoting nature of\nexisting ASG methods can lead to the \"over-representation\" of subgoals,\ngenerating numerous spurious subgoals of limited relevance to the actual task\nreward and thus decreasing the sample efficiency of the algorithm. To address\nthis problem and inspired by the disentangled representation learning, we\npropose a novel \"disentangled\" decision-making method, Semantically Aligned\ntask decomposition in MARL (SAMA), that prompts pretrained language models with\nchain-of-thought that can suggest potential goals, provide suitable goal\ndecomposition and subgoal allocation as well as self-reflection-based\nreplanning. Additionally, SAMA incorporates language-grounded RL to train each\nagent's subgoal-conditioned policy. SAMA demonstrates considerable advantages\nin sample efficiency compared to state-of-the-art ASG methods, as evidenced by\nits performance on two challenging sparse-reward tasks, Overcooked and MiniRTS.", "published": "2023-05-18 10:37:54", "link": "http://arxiv.org/abs/2305.10865v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Query Performance Prediction: From Ad-hoc to Conversational Search", "abstract": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.", "published": "2023-05-18 12:37:01", "link": "http://arxiv.org/abs/2305.10923v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "H.3.3"], "primary_category": "cs.IR"}
{"title": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit", "abstract": "This paper introduces FunASR, an open-source speech recognition toolkit\ndesigned to bridge the gap between academic research and industrial\napplications. FunASR offers models trained on large-scale industrial corpora\nand the ability to deploy them in applications. The toolkit's flagship model,\nParaformer, is a non-autoregressive end-to-end speech recognition model that\nhas been trained on a manually annotated Mandarin speech recognition dataset\nthat contains 60,000 hours of speech. To improve the performance of Paraformer,\nwe have added timestamp prediction and hotword customization capabilities to\nthe standard Paraformer backbone. In addition, to facilitate model deployment,\nwe have open-sourced a voice activity detection model based on the Feedforward\nSequential Memory Network (FSMN-VAD) and a text post-processing punctuation\nmodel based on the controllable time-delay Transformer (CT-Transformer), both\nof which were trained on industrial corpora. These functional modules provide a\nsolid foundation for building high-precision long audio speech recognition\nservices. Compared to other models trained on open datasets, Paraformer\ndemonstrates superior performance.", "published": "2023-05-18 14:45:09", "link": "http://arxiv.org/abs/2305.11013v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparative Study on E-Branchformer vs Conformer in Speech\n  Recognition, Translation, and Understanding Tasks", "abstract": "Conformer, a convolution-augmented Transformer variant, has become the de\nfacto encoder architecture for speech processing due to its superior\nperformance in various tasks, including automatic speech recognition (ASR),\nspeech translation (ST) and spoken language understanding (SLU). Recently, a\nnew encoder called E-Branchformer has outperformed Conformer in the LibriSpeech\nASR benchmark, making it promising for more general speech applications. This\nwork compares E-Branchformer and Conformer through extensive experiments using\ndifferent types of end-to-end sequence-to-sequence models. Results demonstrate\nthat E-Branchformer achieves comparable or better performance than Conformer in\nalmost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while\nbeing more stable during training. We will release our training configurations\nand pre-trained models for reproducibility, which can benefit the speech\ncommunity.", "published": "2023-05-18 16:00:48", "link": "http://arxiv.org/abs/2305.11073v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emergent Representations of Program Semantics in Language Models Trained\n  on Programs", "abstract": "We present evidence that language models (LMs) of code can learn to represent\nthe formal semantics of programs, despite being trained only to perform\nnext-token prediction. Specifically, we train a Transformer model on a\nsynthetic corpus of programs written in a domain-specific language for\nnavigating 2D grid world environments. Each program in the corpus is preceded\nby a (partial) specification in the form of several input-output grid world\nstates. Despite providing no further inductive biases, we find that a probing\nclassifier is able to extract increasingly accurate representations of the\nunobserved, intermediate grid world states from the LM hidden states over the\ncourse of training, suggesting the LM acquires an emergent ability to interpret\nprograms in the formal sense. We also develop a novel interventional baseline\nthat enables us to disambiguate what is represented by the LM as opposed to\nlearned by the probe. We anticipate that this technique may be generally\napplicable to a broad range of semantic probing experiments. In summary, this\npaper does not propose any new techniques for training LMs of code, but\ndevelops an experimental framework for and provides insights into the\nacquisition and representation of formal semantics in statistical models of\ncode. Our code is available at\nhttps://github.com/charlesjin/emergent-semantics.", "published": "2023-05-18 17:58:08", "link": "http://arxiv.org/abs/2305.11169v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "Efficient Prompting via Dynamic In-Context Learning", "abstract": "The primary way of building AI applications is shifting from training\nspecialist models to prompting generalist models. A common practice for\nprompting generalist models, often referred to as in-context learning, is to\nappend a few examples (demonstrations) to the prompt to help the model better\nunderstand the task. While effective, in-context learning can be inefficient\nbecause it makes the input prompt much longer, consuming valuable space in the\ncontext window and leading to larger computational costs. In this paper, we\npropose DynaICL, a recipe for efficient prompting with black-box generalist\nmodels that dynamically allocate in-context examples according to the input\ncomplexity and the computational budget. To achieve this, we train a meta\ncontroller that predicts the number of in-context examples suitable for the\ngeneralist model to make a good prediction based on the performance-efficiency\ntrade-off for a specific input. We then dynamically allocate the number of\ndemonstrations for an input according to predictions from the meta controller\nand the given computation budget. Experimental results show that dynamic\nexample allocation helps achieve a better performance-efficiency trade-off in\ntwo practical settings where computational resources or the required\nperformance is constrained. Specifically, DynaICL saves up to 46% token budget\ncompared to the common practice that allocates the same number of in-context\nexamples to each input. We also find that a meta controller trained on a\ncertain backbone model and tasks can successfully generalize to unseen models\nand tasks.", "published": "2023-05-18 17:58:31", "link": "http://arxiv.org/abs/2305.11170v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited\n  Modalities", "abstract": "In this work, we explore a scalable way for building a general representation\nmodel toward unlimited modalities. We release ONE-PEACE, a highly extensible\nmodel with 4B parameters that can seamlessly align and integrate\nrepresentations across vision, audio, and language modalities. The architecture\nof ONE-PEACE comprises modality adapters, shared self-attention layers, and\nmodality FFNs. This design allows for the easy extension of new modalities by\nadding adapters and FFNs, while also enabling multi-modal fusion through\nself-attention layers. To pretrain ONE-PEACE, we develop two modality-agnostic\npretraining tasks, cross-modal aligning contrast and intra-modal denoising\ncontrast, which align the semantic space of different modalities and capture\nfine-grained details within modalities concurrently. With the scaling-friendly\narchitecture and pretraining tasks, ONE-PEACE has the potential to expand to\nunlimited modalities. Without using any vision or language pretrained model for\ninitialization, ONE-PEACE achieves leading results on a wide range of uni-modal\nand multi-modal tasks, including image classification (ImageNet), semantic\nsegmentation (ADE20K), audio-text retrieval (AudioCaps, Clotho), audio\nclassification (ESC-50, FSD50K, VGGSound), audio question answering (AVQA),\nimage-text retrieval (MSCOCO, Flickr30K), and visual grounding (RefCOCO/+/g).\nCode is available at https://github.com/OFA-Sys/ONE-PEACE.", "published": "2023-05-18 17:59:06", "link": "http://arxiv.org/abs/2305.11172v1", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "LIMA: Less Is More for Alignment", "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining\nfrom raw text, to learn general-purpose representations, and (2) large scale\ninstruction tuning and reinforcement learning, to better align to end tasks and\nuser preferences. We measure the relative importance of these two stages by\ntraining LIMA, a 65B parameter LLaMa language model fine-tuned with the\nstandard supervised loss on only 1,000 carefully curated prompts and responses,\nwithout any reinforcement learning or human preference modeling. LIMA\ndemonstrates remarkably strong performance, learning to follow specific\nresponse formats from only a handful of examples in the training data,\nincluding complex queries that range from planning trip itineraries to\nspeculating about alternate history. Moreover, the model tends to generalize\nwell to unseen tasks that did not appear in the training data. In a controlled\nhuman study, responses from LIMA are either equivalent or strictly preferred to\nGPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard\nand 65% versus DaVinci003, which was trained with human feedback. Taken\ntogether, these results strongly suggest that almost all knowledge in large\nlanguage models is learned during pretraining, and only limited instruction\ntuning data is necessary to teach models to produce high quality output.", "published": "2023-05-18 17:45:22", "link": "http://arxiv.org/abs/2305.11206v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Collaborative Plan Acquisition through Theory of Mind Modeling\n  in Situated Dialogue", "abstract": "Collaborative tasks often begin with partial task knowledge and incomplete\ninitial plans from each partner. To complete these tasks, agents need to engage\nin situated communication with their partners and coordinate their partial\nplans towards a complete plan to achieve a joint task goal. While such\ncollaboration seems effortless in a human-human team, it is highly challenging\nfor human-AI collaboration. To address this limitation, this paper takes a step\ntowards collaborative plan acquisition, where humans and agents strive to learn\nand communicate with each other to acquire a complete plan for joint tasks.\nSpecifically, we formulate a novel problem for agents to predict the missing\ntask knowledge for themselves and for their partners based on rich perceptual\nand dialogue history. We extend a situated dialogue benchmark for symmetric\ncollaborative tasks in a 3D blocks world and investigate computational\nstrategies for plan acquisition. Our empirical results suggest that predicting\nthe partner's missing knowledge is a more viable approach than predicting one's\nown. We show that explicit modeling of the partner's dialogue moves and mental\nstates produces improved and more stable results than without. These results\nprovide insight for future AI agents that can predict what knowledge their\npartner is missing and, therefore, can proactively communicate such information\nto help their partner acquire such missing knowledge toward a common\nunderstanding of joint tasks.", "published": "2023-05-18 19:42:04", "link": "http://arxiv.org/abs/2305.11271v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "In the Name of Fairness: Assessing the Bias in Clinical Record\n  De-identification", "abstract": "Data sharing is crucial for open science and reproducible research, but the\nlegal sharing of clinical data requires the removal of protected health\ninformation from electronic health records. This process, known as\nde-identification, is often achieved through the use of machine learning\nalgorithms by many commercial and open-source systems. While these systems have\nshown compelling results on average, the variation in their performance across\ndifferent demographic groups has not been thoroughly examined. In this work, we\ninvestigate the bias of de-identification systems on names in clinical notes\nvia a large-scale empirical analysis. To achieve this, we create 16 name sets\nthat vary along four demographic dimensions: gender, race, name popularity, and\nthe decade of popularity. We insert these names into 100 manually curated\nclinical templates and evaluate the performance of nine public and private\nde-identification methods. Our findings reveal that there are statistically\nsignificant performance gaps along a majority of the demographic dimensions in\nmost methods. We further illustrate that de-identification quality is affected\nby polysemy in names, gender context, and clinical note characteristics. To\nmitigate the identified gaps, we propose a simple and method-agnostic solution\nby fine-tuning de-identification methods with clinical context and diverse\nnames. Overall, it is imperative to address the bias in existing methods\nimmediately so that downstream stakeholders can build high-quality systems to\nserve all demographic parties fairly.", "published": "2023-05-18 23:47:00", "link": "http://arxiv.org/abs/2305.11348v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Data Redaction from Conditional Generative Models", "abstract": "Deep generative models are known to produce undesirable samples such as\nharmful content. Traditional mitigation methods include re-training from\nscratch, filtering, or editing; however, these are either computationally\nexpensive or can be circumvented by third parties. In this paper, we take a\ndifferent approach and study how to post-edit an already-trained conditional\ngenerative model so that it redacts certain conditionals that will, with high\nprobability, lead to undesirable content. This is done by distilling the\nconditioning network in the models, giving a solution that is effective,\nefficient, controllable, and universal for a class of deep generative models.\nWe conduct experiments on redacting prompts in text-to-image models and\nredacting voices in text-to-speech models. Our method is computationally light,\nleads to better redaction quality and robustness than baseline methods while\nstill retaining high generation quality.", "published": "2023-05-18 23:58:53", "link": "http://arxiv.org/abs/2305.11351v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Weakly-Supervised Visual-Textual Grounding with Semantic Prior\n  Refinement", "abstract": "Using only image-sentence pairs, weakly-supervised visual-textual grounding\naims to learn region-phrase correspondences of the respective entity mentions.\nCompared to the supervised approach, learning is more difficult since bounding\nboxes and textual phrases correspondences are unavailable. In light of this, we\npropose the Semantic Prior Refinement Model (SPRM), whose predictions are\nobtained by combining the output of two main modules. The first untrained\nmodule aims to return a rough alignment between textual phrases and bounding\nboxes. The second trained module is composed of two sub-components that refine\nthe rough alignment to improve the accuracy of the final phrase-bounding box\nalignments. The model is trained to maximize the multimodal similarity between\nan image and a sentence, while minimizing the multimodal similarity of the same\nsentence and a new unrelated image, carefully selected to help the most during\ntraining. Our approach shows state-of-the-art results on two popular datasets,\nFlickr30k Entities and ReferIt, shining especially on ReferIt with a 9.6%\nabsolute improvement. Moreover, thanks to the untrained component, it reaches\ncompetitive performances just using a small fraction of training examples.", "published": "2023-05-18 12:25:07", "link": "http://arxiv.org/abs/2305.10913v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot\n  Task Generalization", "abstract": "We investigate the emergent abilities of the recently proposed web-scale\nspeech model Whisper, by adapting it to unseen tasks with prompt engineering.\nWe selected three tasks: audio-visual speech recognition (AVSR), code-switched\nspeech recognition (CS-ASR), and speech translation (ST) on unseen language\npairs. We design task-specific prompts, by either leveraging another\nlarge-scale model, or simply manipulating the special tokens in the default\nprompts. Experiments show that compared to the default prompts, our proposed\nprompts improve performance by 10% to 45% on the three zero-shot tasks, and\neven outperform SotA supervised models on some datasets. In addition, our\nexperiments reveal many interesting properties of Whisper, including its\nrobustness to prompts, bias on accents, and the multilingual understanding in\nits latent space. Code is available at\nhttps://github.com/jasonppy/PromptingWhisper", "published": "2023-05-18 16:32:58", "link": "http://arxiv.org/abs/2305.11095v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Parameter-Efficient Learning Approach to Arabic Dialect Identification\n  with Pre-Trained General-Purpose Speech Model", "abstract": "In this work, we explore Parameter-Efficient-Learning (PEL) techniques to\nrepurpose a General-Purpose-Speech (GSM) model for Arabic dialect\nidentification (ADI). Specifically, we investigate different setups to\nincorporate trainable features into a multi-layer encoder-decoder GSM\nformulation under frozen pre-trained settings. Our architecture includes\nresidual adapter and model reprogramming (input-prompting). We design a\ntoken-level label mapping to condition the GSM for Arabic Dialect\nIdentification (ADI). This is challenging due to the high variation in\nvocabulary and pronunciation among the numerous regional dialects. We achieve\nnew state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We\nfurther reduce the training budgets with the PEL method, which performs within\n1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable\nparameters. Our study demonstrates how to identify Arabic dialects using a\nsmall dataset and limited computation with open source code and pre-trained\nmodels.", "published": "2023-05-18 18:15:53", "link": "http://arxiv.org/abs/2305.11244v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Multi-Task Learning Framework for Sound Event Detection using\n  High-level Acoustic Characteristics of Sounds", "abstract": "Sound event detection (SED) entails identifying the type of sound and\nestimating its temporal boundaries from acoustic signals. These events are\nuniquely characterized by their spatio-temporal features, which are determined\nby the way they are produced. In this study, we leverage some distinctive\nhigh-level acoustic characteristics of various sound events to assist the SED\nmodel training, without requiring additional labeled data. Specifically, we use\nthe DCASE Task 4 2022 dataset and categorize the 10 classes into four\nsubcategories based on their high-level acoustic characteristics. We then\nintroduce a novel multi-task learning framework that jointly trains the SED and\nhigh-level acoustic characteristics classification tasks, using shared layers\nand weighted loss. Our method significantly improves the performance of the SED\nsystem, achieving a 36.3% improvement in terms of the polyphonic sound event\ndetection score compared to the baseline on the DCASE 2022 Task 4 validation\nset.", "published": "2023-05-18 05:57:48", "link": "http://arxiv.org/abs/2305.10729v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Locate and Beamform: Two-dimensional Locating All-neural Beamformer for\n  Multi-channel Speech Separation", "abstract": "Recently, stunning improvements on multi-channel speech separation have been\nachieved by neural beamformers when direction information is available.\nHowever, most of them neglect to utilize speaker's 2-dimensional (2D) location\ncues contained in mixture signal, which limits the performance when two sources\ncome from close directions. In this paper, we propose an end-to-end beamforming\nnetwork for 2D location guided speech separation merely given mixture signal.\nIt first estimates discriminable direction and 2D location cues, which imply\ndirections the sources come from in multi views of microphones and their 2D\ncoordinates. These cues are then integrated into location-aware neural\nbeamformer, thus allowing accurate reconstruction of two sources' speech\nsignals. Experiments show that our proposed model not only achieves a\ncomprehensive decent improvement compared to baseline systems, but avoids\ninferior performance on spatial overlapping cases.", "published": "2023-05-18 09:02:08", "link": "http://arxiv.org/abs/2305.10821v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Diffusion-Based Mel-Spectrogram Enhancement for Personalized Speech\n  Synthesis with Found Data", "abstract": "Creating synthetic voices with found data is challenging, as real-world\nrecordings often contain various types of audio degradation. One way to address\nthis problem is to pre-enhance the speech with an enhancement model and then\nuse the enhanced data for text-to-speech (TTS) model training. This paper\ninvestigates the use of conditional diffusion models for generalized speech\nenhancement, which aims at addressing multiple types of audio degradation\nsimultaneously. The enhancement is performed on the log Mel-spectrogram domain\nto align with the TTS training objective. Text information is introduced as an\nadditional condition to improve the model robustness. Experiments on real-world\nrecordings demonstrate that the synthetic voice built on data enhanced by the\nproposed model produces higher-quality synthetic speech, compared to those\ntrained on data enhanced by strong baselines. Code and pre-trained parameters\nof the proposed enhancement model are available at\n\\url{https://github.com/dmse4tts/DMSE4TTS}", "published": "2023-05-18 11:41:26", "link": "http://arxiv.org/abs/2305.10891v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving Generalization Ability of Countermeasures for New Mismatch\n  Scenario by Combining Multiple Advanced Regularization Terms", "abstract": "The ability of countermeasure models to generalize from seen speech synthesis\nmethods to unseen ones has been investigated in the ASVspoof challenge.\nHowever, a new mismatch scenario in which fake audio may be generated from real\naudio with unseen genres has not been studied thoroughly. To this end, we first\nuse five different vocoders to create a new dataset called CN-Spoof based on\nthe CN-Celeb1\\&2 datasets. Then, we design two auxiliary objectives for\nregularization via meta-optimization and a genre alignment module,\nrespectively, and combine them with the main anti-spoofing objective using\nlearnable weights for multiple loss terms. The results on our cross-genre\nevaluation dataset for anti-spoofing show that the proposed method\nsignificantly improved the generalization ability of the countermeasures\ncompared with the baseline system in the genre mismatch scenario.", "published": "2023-05-18 12:58:29", "link": "http://arxiv.org/abs/2305.10940v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments", "abstract": "Voice conversion (VC) models have demonstrated impressive few-shot conversion\nquality on the clean, native speech populations they're trained on. However,\nwhen source or target speech accents, background noise conditions, or\nmicrophone characteristics differ from training, quality voice conversion is\nnot guaranteed. These problems are often left unexamined in VC research, giving\nrise to frustration in users trying to use pretrained VC models on their own\ndata. We are interested in accent-preserving voice conversion for name\npronunciation from self-recorded examples, a domain in which all three of the\naforementioned conditions are present, and posit that demonstrating higher\nperformance in this domain correlates with creating VC models that are more\nusable by otherwise frustrated users. We demonstrate that existing SOTA\nencoder-decoder VC models can be made robust to these variations and endowed\nwith natural denoising capabilities using more diverse data and simple data\naugmentation techniques in pretraining.", "published": "2023-05-18 03:54:10", "link": "http://arxiv.org/abs/2305.10684v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attention-based Encoder-Decoder Network for End-to-End Neural Speaker\n  Diarization with Target Speaker Attractor", "abstract": "This paper proposes a novel Attention-based Encoder-Decoder network for\nEnd-to-End Neural speaker Diarization (AED-EEND). In AED-EEND system, we\nincorporate the target speaker enrollment information used in target speaker\nvoice activity detection (TS-VAD) to calculate the attractor, which can\nmitigate the speaker permutation problem and facilitate easier model\nconvergence. In the training process, we propose a teacher-forcing strategy to\nobtain the enrollment information using the ground-truth label. Furthermore, we\npropose three heuristic decoding methods to identify the enrollment area for\neach speaker during the evaluation process. Additionally, we enhance the\nattractor calculation network LSTM used in the end-to-end encoder-decoder based\nattractor calculation (EEND-EDA) system by incorporating an attention-based\nmodel. By utilizing such an attention-based attractor decoder, our proposed\nAED-EEND system outperforms both the EEND-EDA and TS-VAD systems with only 0.5s\nof enrollment data.", "published": "2023-05-18 04:35:02", "link": "http://arxiv.org/abs/2305.10704v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise-Aware Speech Separation with Contrastive Learning", "abstract": "Recently, speech separation (SS) task has achieved remarkable progress driven\nby deep learning technique. However, it is still challenging to separate target\nspeech from noisy mixture, as the neural model is vulnerable to assign\nbackground noise to each speaker. In this paper, we propose a noise-aware SS\n(NASS) method, which aims to improve the speech quality for separated signals\nunder noisy conditions. Specifically, NASS views background noise as an\nadditional output and predicts it along with other speakers in a mask-based\nmanner. To effectively denoise, we introduce patch-wise contrastive learning\n(PCL) between noise and speaker representations from the decoder input and\nencoder output. PCL loss aims to minimize the mutual information between\npredicted noise and other speakers at multiple-patch level to suppress the\nnoise information in separated signals. Experimental results show that NASS\nachieves 1 to 2dB SI-SNRi or SDRi over DPRNN and Sepformer on WHAM! and\nLibriMix noisy datasets, with less than 0.1M parameter increase.", "published": "2023-05-18 07:06:15", "link": "http://arxiv.org/abs/2305.10761v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CLAPSpeech: Learning Prosody from Text Context with Contrastive\n  Language-Audio Pre-training", "abstract": "Improving text representation has attracted much attention to achieve\nexpressive text-to-speech (TTS). However, existing works only implicitly learn\nthe prosody with masked token reconstruction tasks, which leads to low training\nefficiency and difficulty in prosody modeling. We propose CLAPSpeech, a\ncross-modal contrastive pre-training framework that explicitly learns the\nprosody variance of the same text token under different contexts. Specifically,\n1) We encourage the model to connect the text context with its corresponding\nprosody pattern in the joint multi-modal space with the elaborate design of the\nencoder inputs and contrastive loss; 2) We introduce a multi-scale pre-training\npipeline to capture prosody patterns in multiple levels. We show how to\nincorporate CLAPSpeech into existing TTS models for better prosody. Experiments\non three datasets not only show that CLAPSpeech could improve the prosody\nprediction for existing TTS methods, but also demonstrate its generalization\nability to adapt to multiple languages and multi-speaker TTS. We also deeply\nanalyze the principle behind the performance of CLAPSpeech. Ablation studies\ndemonstrate the necessity of each component in our method. Source code and\naudio samples are available at https://clapspeech.github.io.", "published": "2023-05-18 07:07:04", "link": "http://arxiv.org/abs/2305.10763v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Speech Articulation Analysis using a Geometric Transformation\n  of the X-ray Microbeam Dataset", "abstract": "Accurate analysis of speech articulation is crucial for speech analysis.\nHowever, X-Y coordinates of articulators strongly depend on the anatomy of the\nspeakers and the variability of pellet placements, and existing methods for\nmapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail to\ncapture the entire anatomy of the vocal tract. In this paper, we propose a new\ngeometric transformation that improves the accuracy of these measurements. Our\ntransformation maps anatomical landmarks' X-Y coordinates along the midsagittal\nplane onto six relative measures: Lip Aperture (LA), Lip Protusion (LP), Tongue\nBody Constriction Location (TTCL), Degree (TBCD), Tongue Tip Constriction\nLocation (TTCL) and Degree (TTCD). Our novel contribution is the extension of\nthe palate trace towards the inferred anterior pharyngeal line, which improves\nmeasurements of tongue body constriction.", "published": "2023-05-18 07:34:17", "link": "http://arxiv.org/abs/2305.10775v3", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Listen, Think, and Understand", "abstract": "The ability of artificial intelligence (AI) systems to perceive and\ncomprehend audio signals is crucial for many applications. Although significant\nprogress has been made in this area since the development of AudioSet, most\nexisting models are designed to map audio inputs to pre-defined, discrete sound\nlabel sets. In contrast, humans possess the ability to not only classify sounds\ninto general categories, but also to listen to the finer details of the sounds,\nexplain the reason for the predictions, think about what the sound infers, and\nunderstand the scene and what action needs to be taken, if any. Such\ncapabilities beyond perception are not yet present in existing audio models. On\nthe other hand, modern large language models (LLMs) exhibit emerging reasoning\nability but they lack audio perception capabilities. Therefore, we ask the\nquestion: can we build a model that has both audio perception and a reasoning\nability?\n  In this paper, we propose a new audio foundation model, called LTU (Listen,\nThink, and Understand). To train LTU, we created a new OpenAQA-5M dataset\nconsisting of 1.9 million closed-ended and 3.7 million open-ended, diverse\n(audio, question, answer) tuples, and have used an autoregressive training\nframework with a perception-to-understanding curriculum. LTU demonstrates\nstrong performance and generalization ability on conventional audio tasks such\nas classification and captioning. More importantly, it exhibits emerging audio\nreasoning and comprehension abilities that are absent in existing audio models.\nTo the best of our knowledge, LTU is one of the first multimodal large language\nmodels that focus on general audio (rather than just speech) understanding.", "published": "2023-05-18 08:03:37", "link": "http://arxiv.org/abs/2305.10790v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Validation of an ECAPA-TDNN system for Forensic Automatic Speaker\n  Recognition under case work conditions", "abstract": "Different variants of a Forensic Automatic Speaker Recognition (FASR) system\nbased on Emphasized Channel Attention, Propagation and Aggregation in Time\nDelay Neural Network (ECAPA-TDNN) are tested under conditions reflecting those\nof a real forensic voice comparison case, according to the forensic_eval_01\nevaluation campaign settings. Using this recent neural model as an embedding\nextraction block, various normalization strategies at the level of embeddings\nand scores allow us to observe the variations in system performance, in terms\nof discriminating power, accuracy and precision metrics. From the achieved\nresults it is possible to state that ECAPA-TDNN can be very successfully used\nas a base component of a FASR system, managing to surpass the previous state of\nthe art, at least in the context of the considered operating conditions.", "published": "2023-05-18 08:37:14", "link": "http://arxiv.org/abs/2305.10805v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TACos: Learning Temporally Structured Embeddings for Few-Shot Keyword\n  Spotting with Dynamic Time Warping", "abstract": "To segment a signal into blocks to be analyzed, few-shot keyword spotting\n(KWS) systems often utilize a sliding window of fixed size. Because of the\nvarying lengths of different keywords or their spoken instances, choosing the\nright window size is a problem: A window should be long enough to contain all\nnecessary information needed to recognize a keyword but a longer window may\ncontain irrelevant information such as multiple words or noise and thus makes\nit difficult to reliably detect on- and offsets of keywords. We propose TACos,\na novel angular margin loss for deriving two-dimensional embeddings that retain\ntemporal properties of the underlying speech signal. In experiments conducted\non KWS-DailyTalk, a few-shot KWS dataset presented in this work, using these\nembeddings as templates for dynamic time warping is shown to outperform using\nother representations or a sliding window and that using time-reversed segments\nof the keywords during training improves the performance.", "published": "2023-05-18 08:52:18", "link": "http://arxiv.org/abs/2305.10816v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "mdctGAN: Taming transformer-based GAN for speech super-resolution with\n  Modified DCT spectra", "abstract": "Speech super-resolution (SSR) aims to recover a high resolution (HR) speech\nfrom its corresponding low resolution (LR) counterpart. Recent SSR methods\nfocus more on the reconstruction of the magnitude spectrogram, ignoring the\nimportance of phase reconstruction, thereby limiting the recovery quality. To\naddress this issue, we propose mdctGAN, a novel SSR framework based on modified\ndiscrete cosine transform (MDCT). By adversarial learning in the MDCT domain,\nour method reconstructs HR speeches in a phase-aware manner without vocoders or\nadditional post-processing. Furthermore, by learning frequency consistent\nfeatures with self-attentive mechanism, mdctGAN guarantees a high quality\nspeech reconstruction. For VCTK corpus dataset, the experiment results show\nthat our model produces natural auditory quality with high MOS and PESQ scores.\nIt also achieves the state-of-the-art log-spectral-distance (LSD) performance\non 48 kHz target resolution from various input rates. Code is available from\nhttps://github.com/neoncloud/mdctGAN", "published": "2023-05-18 16:49:46", "link": "http://arxiv.org/abs/2305.11104v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Unsupervised Multi-channel Separation and Adaptation", "abstract": "A key challenge in machine learning is to generalize from training data to an\napplication domain of interest. This work generalizes the recently-proposed\nmixture invariant training (MixIT) algorithm to perform unsupervised learning\nin the multi-channel setting. We use MixIT to train a model on far-field\nmicrophone array recordings of overlapping reverberant and noisy speech from\nthe AMI Corpus. The models are trained on both supervised and unsupervised\ntraining data, and are tested on real AMI recordings containing overlapping\nspeech. To objectively evaluate our models, we also use a synthetic\nmulti-channel AMI test set. Holding network architectures constant, we find\nthat a fine-tuned semi-supervised model yields the largest improvement to\nSI-SNR and to human listening ratings across synthetic and real datasets,\noutperforming supervised models trained on well-matched synthetic data. Our\nresults demonstrate that unsupervised learning through MixIT enables model\nadaptation on both single- and multi-channel real-world speech recordings.", "published": "2023-05-18 17:43:03", "link": "http://arxiv.org/abs/2305.11151v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TrustSER: On the Trustworthiness of Fine-tuning Pre-trained Speech\n  Embeddings For Speech Emotion Recognition", "abstract": "Recent studies have explored the use of pre-trained embeddings for speech\nemotion recognition (SER), achieving comparable performance to conventional\nmethods that rely on low-level knowledge-inspired acoustic features. These\nembeddings are often generated from models trained on large-scale speech\ndatasets using self-supervised or weakly-supervised learning objectives.\nDespite the significant advancements made in SER through the use of pre-trained\nembeddings, there is a limited understanding of the trustworthiness of these\nmethods, including privacy breaches, unfair performance, vulnerability to\nadversarial attacks, and computational cost, all of which may hinder the\nreal-world deployment of these systems. In response, we introduce TrustSER, a\ngeneral framework designed to evaluate the trustworthiness of SER systems using\ndeep learning methods, with a focus on privacy, safety, fairness, and\nsustainability, offering unique insights into future research in the field of\nSER. Our code is publicly available under:\nhttps://github.com/usc-sail/trust-ser.", "published": "2023-05-18 18:00:36", "link": "http://arxiv.org/abs/2305.11229v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Use of Speech Impairment Severity for Dysarthric Speech Recognition", "abstract": "A key challenge in dysarthric speech recognition is the speaker-level\ndiversity attributed to both speaker-identity associated factors such as\ngender, and speech impairment severity. Most prior researches on addressing\nthis issue focused on using speaker-identity only. To this end, this paper\nproposes a novel set of techniques to use both severity and speaker-identity in\ndysarthric speech recognition: a) multitask training incorporating severity\nprediction error; b) speaker-severity aware auxiliary feature adaptation; and\nc) structured LHUC transforms separately conditioned on speaker-identity and\nseverity. Experiments conducted on UASpeech suggest incorporating additional\nspeech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and\npre-trained Wav2vec 2.0 ASR systems produced statistically significant WER\nreductions up to 4.78% (14.03% relative). Using the best system the lowest\npublished WER of 17.82% (51.25% on very low intelligibility) was obtained on\nUASpeech.", "published": "2023-05-18 02:42:59", "link": "http://arxiv.org/abs/2305.10659v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net\n  Encoder With Multiple STFTs", "abstract": "This paper presents FastFit, a novel neural vocoder architecture that\nreplaces the U-Net encoder with multiple short-time Fourier transforms (STFTs)\nto achieve faster generation rates without sacrificing sample quality. We\nreplaced each encoder block with an STFT, with parameters equal to the temporal\nresolution of each decoder block, leading to the skip connection. FastFit\nreduces the number of parameters and the generation time of the model by almost\nhalf while maintaining high fidelity. Through objective and subjective\nevaluations, we demonstrated that the proposed model achieves nearly twice the\ngeneration speed of baseline iteration-based vocoders while maintaining high\nsound quality. We further showed that FastFit produces sound qualities similar\nto those of other baselines in text-to-speech evaluation scenarios, including\nmulti-speaker and zero-shot text-to-speech.", "published": "2023-05-18 09:05:17", "link": "http://arxiv.org/abs/2305.10823v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GETMusic: Generating Any Music Tracks with a Unified Representation and\n  Diffusion Framework", "abstract": "Symbolic music generation aims to create musical notes, which can help users\ncompose music, such as generating target instrument tracks based on provided\nsource tracks. In practical scenarios where there's a predefined ensemble of\ntracks and various composition needs, an efficient and effective generative\nmodel that can generate any target tracks based on the other tracks becomes\ncrucial. However, previous efforts have fallen short in addressing this\nnecessity due to limitations in their music representations and models. In this\npaper, we introduce a framework known as GETMusic, with ``GET'' standing for\n``GEnerate music Tracks.'' This framework encompasses a novel music\nrepresentation ``GETScore'' and a diffusion model ``GETDiff.'' GETScore\nrepresents musical notes as tokens and organizes tokens in a 2D structure, with\ntracks stacked vertically and progressing horizontally over time. At a training\nstep, each track of a music piece is randomly selected as either the target or\nsource. The training involves two processes: In the forward process, target\ntracks are corrupted by masking their tokens, while source tracks remain as the\nground truth; in the denoising process, GETDiff is trained to predict the\nmasked target tokens conditioning on the source tracks. Our proposed\nrepresentation, coupled with the non-autoregressive generative model, empowers\nGETMusic to generate music with any arbitrary source-target track combinations.\nOur experiments demonstrate that the versatile GETMusic outperforms prior works\nproposed for certain specific composition tasks.", "published": "2023-05-18 09:53:23", "link": "http://arxiv.org/abs/2305.10841v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Federated learning for secure development of AI models for Parkinson's\n  disease detection using speech from different languages", "abstract": "Parkinson's disease (PD) is a neurological disorder impacting a person's\nspeech. Among automatic PD assessment methods, deep learning models have gained\nparticular interest. Recently, the community has explored cross-pathology and\ncross-language models which can improve diagnostic accuracy even further.\nHowever, strict patient data privacy regulations largely prevent institutions\nfrom sharing patient speech data with each other. In this paper, we employ\nfederated learning (FL) for PD detection using speech signals from 3 real-world\nlanguage corpora of German, Spanish, and Czech, each from a separate\ninstitution. Our results indicate that the FL model outperforms all the local\nmodels in terms of diagnostic accuracy, while not performing very differently\nfrom the model based on centrally combined training sets, with the advantage of\nnot requiring any data sharing among collaborators. This will simplify\ninter-institutional collaborations, resulting in enhancement of patient\noutcomes.", "published": "2023-05-18 20:04:55", "link": "http://arxiv.org/abs/2305.11284v2", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for\n  Adapted Behavior Synthesis", "abstract": "Socially Interactive Agents (SIAs) are physical or virtual embodied agents\nthat display similar behavior as human multimodal behavior. Modeling SIAs'\nnon-verbal behavior, such as speech and facial gestures, has always been a\nchallenging task, given that a SIA can take the role of a speaker or a\nlistener. A SIA must emit appropriate behavior adapted to its own speech, its\nprevious behaviors (intra-personal), and the User's behaviors (inter-personal)\nfor both roles. We propose AMII, a novel approach to synthesize adaptive facial\ngestures for SIAs while interacting with Users and acting interchangeably as a\nspeaker or as a listener. AMII is characterized by modality memory encoding\nschema - where modality corresponds to either speech or facial gestures - and\nmakes use of attention mechanisms to capture the intra-personal and\ninter-personal relationships. We validate our approach by conducting objective\nevaluations and comparing it with the state-of-the-art approaches.", "published": "2023-05-18 21:22:07", "link": "http://arxiv.org/abs/2305.11310v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS", "68T07", "I.2.11"], "primary_category": "cs.HC"}
{"title": "QPGesture: Quantization-Based and Phase-Guided Motion Matching for\n  Natural Speech-Driven Gesture Generation", "abstract": "Speech-driven gesture generation is highly challenging due to the random\njitters of human motion. In addition, there is an inherent asynchronous\nrelationship between human speech and gestures. To tackle these challenges, we\nintroduce a novel quantization-based and phase-guided motion-matching\nframework. Specifically, we first present a gesture VQ-VAE module to learn a\ncodebook to summarize meaningful gesture units. With each code representing a\nunique gesture, random jittering problems are alleviated effectively. We then\nuse Levenshtein distance to align diverse gestures with different speech.\nLevenshtein distance based on audio quantization as a similarity metric of\ncorresponding speech of gestures helps match more appropriate gestures with\nspeech, and solves the alignment problem of speech and gestures well. Moreover,\nwe introduce phase to guide the optimal gesture matching based on the semantics\nof context or rhythm of audio. Phase guides when text-based or speech-based\ngestures should be performed to make the generated gestures more natural.\nExtensive experiments show that our method outperforms recent approaches on\nspeech-driven gesture generation. Our code, database, pre-trained models, and\ndemos are available at https://github.com/YoungSeng/QPGesture.", "published": "2023-05-18 16:31:25", "link": "http://arxiv.org/abs/2305.11094v1", "categories": ["cs.HC", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Parameter-Efficient Learning for Text-to-Speech Accent Adaptation", "abstract": "This paper presents a parameter-efficient learning (PEL) to develop a\nlow-resource accent adaptation for text-to-speech (TTS). A resource-efficient\nadaptation from a frozen pre-trained TTS model is developed by using only 1.2\\%\nto 0.8\\% of original trainable parameters to achieve competitive performance in\nvoice synthesis. Motivated by a theoretical foundation of optimal transport\n(OT), this study carries out PEL for TTS where an auxiliary unsupervised loss\nbased on OT is introduced to maximize a difference between the pre-trained\nsource domain and the (unseen) target domain, in addition to its supervised\ntraining loss. Further, we leverage upon this unsupervised loss refinement to\nboost system performance via either sliced Wasserstein distance or maximum mean\ndiscrepancy. The merit of this work is demonstrated by fulfilling PEL solutions\nbased on residual adapter learning, and model reprogramming when evaluating the\nMandarin accent adaptation. Experiment results show that the proposed methods\ncan achieve competitive naturalness with parameter-efficient decoder\nfine-tuning, and the auxiliary unsupervised loss improves model performance\nempirically.", "published": "2023-05-18 22:02:59", "link": "http://arxiv.org/abs/2305.11320v1", "categories": ["cs.SD", "cs.AI", "cs.NE", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
