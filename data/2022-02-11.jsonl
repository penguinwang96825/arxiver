{"title": "Hindi/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual\n  Input Learning with Self Attention", "abstract": "Sentiment Analysis typically refers to using natural language processing,\ntext analysis and computational linguistics to extract affect and emotion based\ninformation from text data. Our work explores how we can effectively use deep\nneural networks in transfer learning and joint dual input learning settings to\neffectively classify sentiments and detect hate speech in Hindi and Bengali\ndata. We start by training Word2Vec word embeddings for Hindi \\textbf{HASOC\ndataset} and Bengali hate speech and then train LSTM and subsequently, employ\nparameter sharing based transfer learning to Bengali sentiment classifiers by\nreusing and fine-tuning the trained weights of Hindi classifiers with both\nclassifier being used as baseline in our study. Finally, we use BiLSTM with\nself attention in joint dual input learning setting where we train a single\nneural network on Hindi and Bengali dataset simultaneously using their\nrespective embeddings.", "published": "2022-02-11 05:36:11", "link": "http://arxiv.org/abs/2202.05457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HaT5: Hate Language Identification using Text-to-Text Transfer\n  Transformer", "abstract": "We investigate the performance of a state-of-the art (SoTA) architecture T5\n(available on the SuperGLUE) and compare with it 3 other previous SoTA\narchitectures across 5 different tasks from 2 relatively diverse datasets. The\ndatasets are diverse in terms of the number and types of tasks they have. To\nimprove performance, we augment the training data by using an autoregressive\nmodel. We achieve near-SoTA results on a couple of the tasks - macro F1 scores\nof 81.66% for task A of the OLID 2019 dataset and 82.54% for task A of the hate\nspeech and offensive content (HASOC) 2021 dataset, where SoTA are 82.9% and\n83.05%, respectively. We perform error analysis and explain why one of the\nmodels (Bi-LSTM) makes the predictions it does by using a publicly available\nalgorithm: Integrated Gradient (IG). This is because explainable artificial\nintelligence (XAI) is essential for earning the trust of users. The main\ncontributions of this work are the implementation method of T5, which is\ndiscussed; the data augmentation using a new conversational AI model\ncheckpoint, which brought performance improvements; and the revelation on the\nshortcomings of HASOC 2021 dataset. It reveals the difficulties of poor data\nannotation by using a small set of examples where the T5 model made the correct\npredictions, even when the ground truth of the test set were incorrect (in our\nopinion). We also provide our model checkpoints on the HuggingFace hub1 to\nfoster transparency.", "published": "2022-02-11 15:21:27", "link": "http://arxiv.org/abs/2202.05690v1", "categories": ["cs.CL", "68"], "primary_category": "cs.CL"}
{"title": "White-Box Attacks on Hate-speech BERT Classifiers in German with\n  Explicit and Implicit Character Level Defense", "abstract": "In this work, we evaluate the adversarial robustness of BERT models trained\non German Hate Speech datasets. We also complement our evaluation with two\nnovel white-box character and word level attacks thereby contributing to the\nrange of attacks available. Furthermore, we also perform a comparison of two\nnovel character-level defense strategies and evaluate their robustness with one\nanother.", "published": "2022-02-11 17:20:50", "link": "http://arxiv.org/abs/2202.05778v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating MT Systems: A Theoretical Framework", "abstract": "This paper outlines a theoretical framework using which different automatic\nmetrics can be designed for evaluation of Machine Translation systems. It\nintroduces the concept of {\\em cognitive ease} which depends on {\\em adequacy}\nand {\\em lack of fluency}. Thus, cognitive ease becomes the main parameter to\nbe measured rather than comprehensibility. The framework allows the components\nof cognitive ease to be broken up and computed based on different linguistic\nlevels etc. Independence of dimensions and linearly combining them provides for\na highly modular approach.\n  The paper places the existing automatic methods in an overall framework, to\nunderstand them better and to improve upon them in future. It can also be used\nto evaluate the newer types of MT systems, such as speech to speech translation\nand discourse translation.", "published": "2022-02-11 18:05:17", "link": "http://arxiv.org/abs/2202.05806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization", "abstract": "We present ClidSum, a benchmark dataset for building cross-lingual\nsummarization systems on dialogue documents. It consists of 67k+ dialogue\ndocuments from two subsets (i.e., SAMSum and MediaSum) and 112k+ annotated\nsummaries in different target languages. Based on the proposed ClidSum, we\nintroduce two benchmark settings for supervised and semi-supervised scenarios,\nrespectively. We then build various baseline systems in different paradigms\n(pipeline and end-to-end) and conduct extensive experiments on ClidSum to\nprovide deeper analyses. Furthermore, we propose mDialBART which extends\nmBART-50 (a multi-lingual BART) via further pre-training. The multiple\nobjectives used in the further pre-training stage help the pre-trained model\ncapture the structural characteristics as well as important content in\ndialogues and the transformation from source to the target language.\nExperimental results show the superiority of mDialBART, as an end-to-end model,\noutperforms strong pipeline models on ClidSum. Finally, we discuss specific\nchallenges that current approaches faced with this task and give multiple\npromising directions for future research. We have released the dataset and code\nat https://github.com/krystalan/ClidSum.", "published": "2022-02-11 13:32:14", "link": "http://arxiv.org/abs/2202.05599v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GenderedNews: Une approche computationnelle des \u00e9carts de\n  repr\u00e9sentation des genres dans la presse fran\u00e7aise", "abstract": "In this article, we present {\\it GenderedNews}\n(\\url{https://gendered-news.imag.fr}), an online dashboard which gives weekly\nmeasures of gender imbalance in French online press. We use Natural Language\nProcessing (NLP) methods to quantify gender inequalities in the media, in the\nwake of global projects like the Global Media Monitoring Project. Such projects\nare instrumental in highlighting gender imbalance in the media and its very\nslow evolution. However, their generalisation is limited by their sampling and\ncost in terms of time, data and staff. Automation allows us to offer\ncomplementary measures to quantify inequalities in gender representation. We\nunderstand representation as the presence and distribution of men and women\nmentioned and quoted in the news -- as opposed to representation as\nstereotypification. In this paper, we first review different means adopted by\nprevious studies on gender inequality in the media : qualitative content\nanalysis, quantitative content analysis and computational methods. We then\ndetail the methods adopted by {\\it GenderedNews} and the two metrics\nimplemented: the masculinity rate of mentions and the proportion of men quoted\nin online news. We describe the data collected daily (seven main titles of\nFrench online news media) and the methodology behind our metrics, as well as a\nfew visualisations. We finally propose to illustrate possible analysis of our\ndata by conducting an in-depth observation of a sample of two months of our\ndatabase.", "published": "2022-02-11 15:16:49", "link": "http://arxiv.org/abs/2202.05682v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Constrained Optimization with Dynamic Bound-scaling for Effective\n  NLPBackdoor Defense", "abstract": "We develop a novel optimization method for NLPbackdoor inversion. We leverage\na dynamically reducing temperature coefficient in the softmax function to\nprovide changing loss landscapes to the optimizer such that the process\ngradually focuses on the ground truth trigger, which is denoted as a one-hot\nvalue in a convex hull. Our method also features a temperature rollback\nmechanism to step away from local optimals, exploiting the observation that\nlocal optimals can be easily deter-mined in NLP trigger inversion (while not in\ngeneral optimization). We evaluate the technique on over 1600 models (with\nroughly half of them having injected backdoors) on 3 prevailing NLP tasks, with\n4 different backdoor attacks and 7 architectures. Our results show that the\ntechnique is able to effectively and efficiently detect and remove backdoors,\noutperforming 4 baseline methods.", "published": "2022-02-11 16:40:25", "link": "http://arxiv.org/abs/2202.05749v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Random Perturbations to Mitigate Adversarial Attacks on Sentiment\n  Analysis Models", "abstract": "Attacks on deep learning models are often difficult to identify and therefore\nare difficult to protect against. This problem is exacerbated by the use of\npublic datasets that typically are not manually inspected before use. In this\npaper, we offer a solution to this vulnerability by using, during testing,\nrandom perturbations such as spelling correction if necessary, substitution by\nrandom synonym, or simply dropping the word. These perturbations are applied to\nrandom words in random sentences to defend NLP models against adversarial\nattacks. Our Random Perturbations Defense and Increased Randomness Defense\nmethods are successful in returning attacked models to similar accuracy of\nmodels before attacks. The original accuracy of the model used in this work is\n80% for sentiment classification. After undergoing attacks, the accuracy drops\nto accuracy between 0% and 44%. After applying our defense methods, the\naccuracy of the model is returned to the original accuracy within statistical\nsignificance.", "published": "2022-02-11 16:50:17", "link": "http://arxiv.org/abs/2202.05758v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text\n  Classification", "abstract": "Large-scale multi-label text classification (LMTC) aims to associate a\ndocument with its relevant labels from a large candidate set. Most existing\nLMTC approaches rely on massive human-annotated training data, which are often\ncostly to obtain and suffer from a long-tailed label distribution (i.e., many\nlabels occur only a few times in the training set). In this paper, we study\nLMTC under the zero-shot setting, which does not require any annotated\ndocuments with labels and only relies on label surface names and descriptions.\nTo train a classifier that calculates the similarity score between a document\nand a label, we propose a novel metadata-induced contrastive learning (MICoL)\nmethod. Different from previous text-based contrastive learning techniques,\nMICoL exploits document metadata (e.g., authors, venues, and references of\nresearch papers), which are widely available on the Web, to derive similar\ndocument-document pairs. Experimental results on two large-scale datasets show\nthat: (1) MICoL significantly outperforms strong zero-shot text classification\nand contrastive learning baselines; (2) MICoL is on par with the\nstate-of-the-art supervised metadata-aware LMTC method trained on 10K-200K\nlabeled documents; and (3) MICoL tends to predict more infrequent labels than\nsupervised methods, thus alleviates the deteriorated performance on long-tailed\nlabels.", "published": "2022-02-11 23:22:17", "link": "http://arxiv.org/abs/2202.05932v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Complementarity of Images and Text for the Expression of Emotions\n  in Social Media", "abstract": "Authors of posts in social media communicate their emotions and what causes\nthem with text and images. While there is work on emotion and stimulus\ndetection for each modality separately, it is yet unknown if the modalities\ncontain complementary emotion information in social media. We aim at filling\nthis research gap and contribute a novel, annotated corpus of English\nmultimodal Reddit posts. On this resource, we develop models to automatically\ndetect the relation between image and text, an emotion stimulus category and\nthe emotion class. We evaluate if these tasks require both modalities and find\nfor the image-text relations, that text alone is sufficient for most categories\n(complementary, illustrative, opposing): the information in the text allows to\npredict if an image is required for emotion understanding. The emotions of\nanger and sadness are best predicted with a multimodal model, while text alone\nis sufficient for disgust, joy, and surprise. Stimuli depicted by objects,\nanimals, food, or a person are best predicted by image-only models, while\nmultimodal models are most effective on art, events, memes, places, or\nscreenshots.", "published": "2022-02-11 12:33:53", "link": "http://arxiv.org/abs/2202.07427v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Including Facial Expressions in Contextual Embeddings for Sign Language\n  Generation", "abstract": "State-of-the-art sign language generation frameworks lack expressivity and\nnaturalness which is the result of only focusing manual signs, neglecting the\naffective, grammatical and semantic functions of facial expressions. The\npurpose of this work is to augment semantic representation of sign language\nthrough grounding facial expressions. We study the effect of modeling the\nrelationship between text, gloss, and facial expressions on the performance of\nthe sign generation systems. In particular, we propose a Dual Encoder\nTransformer able to generate manual signs as well as facial expressions by\ncapturing the similarities and differences found in text and sign gloss\nannotation. We take into consideration the role of facial muscle activity to\nexpress intensities of manual signs by being the first to employ facial action\nunits in sign language generation. We perform a series of experiments showing\nthat our proposed model improves the quality of automatically generated sign\nlanguage.", "published": "2022-02-11 00:47:22", "link": "http://arxiv.org/abs/2202.05383v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dual Task Framework for Improving Persona-grounded Dialogue Dataset", "abstract": "This paper introduces a simple yet effective data-centric approach for the\ntask of improving persona-conditioned dialogue agents. Prior model-centric\napproaches unquestioningly depend on the raw crowdsourced benchmark datasets\nsuch as Persona-Chat. In contrast, we aim to fix annotation artifacts in\nbenchmarking, which is orthogonally applicable to any dialogue model.\nSpecifically, we augment relevant personas to improve dialogue dataset/agent,\nby leveraging the primal-dual structure of the two tasks, predicting dialogue\nresponses and personas based on each other. Experiments on Persona-Chat show\nthat our approach outperforms pre-trained LMs by an 11.7 point gain in terms of\naccuracy.", "published": "2022-02-11 04:08:46", "link": "http://arxiv.org/abs/2202.05435v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ACORT: A Compact Object Relation Transformer for Parameter Efficient\n  Image Captioning", "abstract": "Recent research that applies Transformer-based architectures to image\ncaptioning has resulted in state-of-the-art image captioning performance,\ncapitalising on the success of Transformers on natural language tasks.\nUnfortunately, though these models work well, one major flaw is their large\nmodel sizes. To this end, we present three parameter reduction methods for\nimage captioning Transformers: Radix Encoding, cross-layer parameter sharing,\nand attention parameter sharing. By combining these methods, our proposed ACORT\nmodels have 3.7x to 21.6x fewer parameters than the baseline model without\ncompromising test performance. Results on the MS-COCO dataset demonstrate that\nour ACORT models are competitive against baselines and SOTA approaches, with\nCIDEr score >=126. Finally, we present qualitative results and ablation studies\nto demonstrate the efficacy of the proposed changes further. Code and\npre-trained models are publicly available at\nhttps://github.com/jiahuei/sparse-image-captioning.", "published": "2022-02-11 05:10:28", "link": "http://arxiv.org/abs/2202.05451v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer", "abstract": "Text spotting end-to-end methods have recently gained attention in the\nliterature due to the benefits of jointly optimizing the text detection and\nrecognition components. Existing methods usually have a distinct separation\nbetween the detection and recognition branches, requiring exact annotations for\nthe two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach\nfor text spotting and the first text spotting framework which may be trained\nwith both fully- and weakly-supervised settings. By learning a single latent\nrepresentation per word detection, and using a novel loss function based on the\nHungarian loss, our method alleviates the need for expensive localization\nannotations. Trained with only text transcription annotations on real data, our\nweakly-supervised method achieves competitive performance with previous\nstate-of-the-art fully-supervised methods. When trained in a fully-supervised\nmanner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.", "published": "2022-02-11 08:50:09", "link": "http://arxiv.org/abs/2202.05508v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "What Does it Mean for a Language Model to Preserve Privacy?", "abstract": "Natural language reflects our private lives and identities, making its\nprivacy concerns as broad as those of real life. Language models lack the\nability to understand the context and sensitivity of text, and tend to memorize\nphrases present in their training sets. An adversary can exploit this tendency\nto extract training data. Depending on the nature of the content and the\ncontext in which this data was collected, this could violate expectations of\nprivacy. Thus there is a growing interest in techniques for training language\nmodels that preserve privacy. In this paper, we discuss the mismatch between\nthe narrow assumptions made by popular data protection techniques (data\nsanitization and differential privacy), and the broadness of natural language\nand of privacy as a social norm. We argue that existing protection methods\ncannot guarantee a generic and meaningful notion of privacy for language\nmodels. We conclude that language models should be trained on text data which\nwas explicitly produced for public use.", "published": "2022-02-11 09:18:27", "link": "http://arxiv.org/abs/2202.05520v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Multi-Modal Knowledge Graph Construction and Application: A Survey", "abstract": "Recent years have witnessed the resurgence of knowledge engineering which is\nfeatured by the fast growth of knowledge graphs. However, most of existing\nknowledge graphs are represented with pure symbols, which hurts the machine's\ncapability to understand the real world. The multi-modalization of knowledge\ngraphs is an inevitable key step towards the realization of human-level machine\nintelligence. The results of this endeavor are Multi-modal Knowledge Graphs\n(MMKGs). In this survey on MMKGs constructed by texts and images, we first give\ndefinitions of MMKGs, followed with the preliminaries on multi-modal tasks and\ntechniques. We then systematically review the challenges, progresses and\nopportunities on the construction and application of MMKGs respectively, with\ndetailed analyses of the strength and weakness of different solutions. We\nfinalize this survey with open research problems relevant to MMKGs.", "published": "2022-02-11 17:31:12", "link": "http://arxiv.org/abs/2202.05786v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.4; E.0; E.2"], "primary_category": "cs.AI"}
{"title": "The xmuspeech system for multi-channel multi-party meeting transcription\n  challenge", "abstract": "This paper describes the system developed by the XMUSPEECH team for the\nMulti-channel Multi-party Meeting Transcription Challenge (M2MeT). For the\nspeaker diarization task, we propose a multi-channel speaker diarization system\nthat obtains spatial information of speaker by Difference of Arrival (DOA)\ntechnology. Speaker-spatial embedding is generated by x-vector and s-vector\nderived from Filter-and-Sum Beamforming (FSB) which makes the embedding more\nrobust. Specifically, we propose a novel multi-channel sequence-to-sequence\nneural network architecture named Discriminative Multi-stream Neural Network\n(DMSNet) which consists of Attention Filter-and-Sum block (AFSB) and Conformer\nencoder. We explore DMSNet to address overlapped speech problem on\nmulti-channel audio. Compared with LSTM based OSD module, we achieve a\ndecreases of 10.1% in Detection Error Rate(DetER). By performing DMSNet based\nOSD module, the DER of cluster-based diarization system decrease significantly\nform 13.44% to 7.63%. Our best fusion system achieves 7.09% and 9.80% of the\ndiarization error rate (DER) on evaluation set and test set.", "published": "2022-02-11 16:32:52", "link": "http://arxiv.org/abs/2202.05744v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FrAUG: A Frame Rate Based Data Augmentation Method for Depression\n  Detection from Speech Signals", "abstract": "In this paper, a data augmentation method is proposed for depression\ndetection from speech signals. Samples for data augmentation were created by\nchanging the frame-width and the frame-shift parameters during the feature\nextraction process. Unlike other data augmentation methods (such as VTLP, pitch\nperturbation, or speed perturbation), the proposed method does not explicitly\nchange acoustic parameters but rather the time-frequency resolution of\nframe-level features. The proposed method was evaluated using two different\ndatasets, models, and input acoustic features. For the DAIC-WOZ (English)\ndataset when using the DepAudioNet model and mel-Spectrograms as input, the\nproposed method resulted in an improvement of 5.97% (validation) and 25.13%\n(test) when compared to the baseline. The improvements for the CONVERGE\n(Mandarin) dataset when using the x-vector embeddings with CNN as the backend\nand MFCCs as input features were 9.32% (validation) and 12.99% (test). Baseline\nsystems do not incorporate any data augmentation. Further, the proposed method\noutperformed commonly used data-augmentation methods such as noise\naugmentation, VTLP, Speed, and Pitch Perturbation. All improvements were\nstatistically significant.", "published": "2022-02-11 21:37:22", "link": "http://arxiv.org/abs/2202.05912v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "FAAG: Fast Adversarial Audio Generation through Interactive Attack\n  Optimisation", "abstract": "Automatic Speech Recognition services (ASRs) inherit deep neural networks'\nvulnerabilities like crafted adversarial examples. Existing methods often\nsuffer from low efficiency because the target phases are added to the entire\naudio sample, resulting in high demand for computational resources. This paper\nproposes a novel scheme named FAAG as an iterative optimization-based method to\ngenerate targeted adversarial examples quickly. By injecting the noise over the\nbeginning part of the audio, FAAG generates adversarial audio in high quality\nwith a high success rate timely. Specifically, we use audio's logits output to\nmap each character in the transcription to an approximate position of the\naudio's frame. Thus, an adversarial example can be generated by FAAG in\napproximately two minutes using CPUs only and around ten seconds with one GPU\nwhile maintaining an average success rate over 85%. Specifically, the FAAG\nmethod can speed up around 60% compared with the baseline method during the\nadversarial example generation process. Furthermore, we found that appending\nbenign audio to any suspicious examples can effectively defend against the\ntargeted adversarial attack. We hope that this work paves the way for inventing\nnew adversarial attacks against speech recognition with computational\nconstraints.", "published": "2022-02-11 02:46:42", "link": "http://arxiv.org/abs/2202.05416v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Novel Chaos-based Light-weight Image Encryption Scheme for Multi-modal\n  Hearing Aids", "abstract": "Multimodal hearing aids (HAs) aim to deliver more intelligible audio in noisy\nenvironments by contextually sensing and processing data in the form of not\nonly audio but also visual information (e.g. lip reading). Machine learning\ntechniques can play a pivotal role for the contextually processing of\nmultimodal data. However, since the computational power of HA devices is low,\ntherefore this data must be processed either on the edge or cloud which, in\nturn, poses privacy concerns for sensitive user data. Existing literature\nproposes several techniques for data encryption but their computational\ncomplexity is a major bottleneck to meet strict latency requirements for\ndevelopment of future multi-modal hearing aids. To overcome this problem, this\npaper proposes a novel real-time audio/visual data encryption scheme based on\nchaos-based encryption using the Tangent-Delay Ellipse Reflecting Cavity-Map\nSystem (TD-ERCS) map and Non-linear Chaotic (NCA) Algorithm. The results\nachieved against different security parameters, including Correlation\nCoefficient, Unified Averaged Changed Intensity (UACI), Key Sensitivity\nAnalysis, Number of Changing Pixel Rate (NPCR), Mean-Square Error (MSE), Peak\nSignal to Noise Ratio (PSNR), Entropy test, and Chi-test, indicate that the\nnewly proposed scheme is more lightweight due to its lower execution time as\ncompared to existing schemes and more secure due to increased key-space against\nmodern brute-force attacks.", "published": "2022-02-11 15:01:43", "link": "http://arxiv.org/abs/2202.05662v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Audio Defect Detection in Music with Deep Networks", "abstract": "With increasing amounts of music being digitally transferred from production\nto distribution, automatic means of determining media quality are needed.\nProtection mechanisms in digital audio processing tools have not eliminated the\nneed of production entities located downstream the distribution chain to assess\naudio quality and detect defects inserted further upstream. Such analysis often\nrelies on the received audio and scarce meta-data alone. Deliberate use of\nartefacts such as clicks in popular music as well as more recent defects\nstemming from corruption in modern audio encodings call for data-centric and\ncontext sensitive solutions for detection. We present a convolutional network\narchitecture following end-to-end encoder decoder configuration to develop\ndetectors for two exemplary audio defects. A click detector is trained and\ncompared to a traditional signal processing method, with a discussion on\ncontext sensitivity. Additional post-processing is used for data augmentation\nand workflow simulation. The ability of our models to capture variance is\nexplored in a detector for artefacts from decompression of corrupted MP3\ncompressed audio. For both tasks we describe the synthetic generation of\nartefacts for controlled detector training and evaluation. We evaluate our\ndetectors on the large open-source Free Music Archive (FMA) and genre-specific\ndatasets.", "published": "2022-02-11 15:56:14", "link": "http://arxiv.org/abs/2202.05718v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Detection of Adaptive Adversarial Attacks in Speaker Verification\n  Systems", "abstract": "Speaker verification systems have been widely used in smart phones and\nInternet of things devices to identify legitimate users. In recent work, it has\nbeen shown that adversarial attacks, such as FAKEBOB, can work effectively\nagainst speaker verification systems. The goal of this paper is to design a\ndetector that can distinguish an original audio from an audio contaminated by\nadversarial attacks. Specifically, our designed detector, called MEH-FEST,\ncalculates the minimum energy in high frequencies from the short-time Fourier\ntransform of an audio and uses it as a detection metric. Through both analysis\nand experiments, we show that our proposed detector is easy to implement, fast\nto process an input audio, and effective in determining whether an audio is\ncorrupted by FAKEBOB attacks. The experimental results indicate that the\ndetector is extremely effective: with near zero false positive and false\nnegative rates for detecting FAKEBOB attacks in Gaussian mixture model (GMM)\nand i-vector speaker verification systems. Moreover, adaptive adversarial\nattacks against our proposed detector and their countermeasures are discussed\nand studied, showing the game between attackers and defenders.", "published": "2022-02-11 16:02:06", "link": "http://arxiv.org/abs/2202.05725v2", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "A Novel Speech Intelligibility Enhancement Model based on\n  CanonicalCorrelation and Deep Learning", "abstract": "Current deep learning (DL) based approaches to speech intelligibility\nenhancement in noisy environments are often trained to minimise the feature\ndistance between noise-free speech and enhanced speech signals. Despite\nimproving the speech quality, such approaches do not deliver required levels of\nspeech intelligibility in everyday noisy environments .\nIntelligibility-oriented (I-O) loss functions have recently been developed to\ntrain DL approaches for robust speech enhancement. Here, we formulate, for the\nfirst time, a novel canonical correlation based I-O loss function to more\neffectively train DL algorithms. Specifically, we present a\ncanonical-correlation based short-time objective intelligibility (CC-STOI) cost\nfunction to train a fully convolutional neural network (FCN) model. We carry\nout comparative simulation experiments to show that our CC-STOI based speech\nenhancement framework outperforms state-of-the-art DL models trained with\nconventional distance-based and STOI-based loss functions, using objective and\nsubjective evaluation measures for case of both unseen speakers and noises.\nOngoing future work is evaluating the proposed approach for design of robust\nhearing-assistive technology.", "published": "2022-02-11 16:48:41", "link": "http://arxiv.org/abs/2202.05756v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The HaMSE Ontology: Using Semantic Technologies to support Music\n  Representation Interoperability and Musicological Analysis", "abstract": "The use of Semantic Technologies - in particular the Semantic Web - has\nrevealed to be a great tool for describing the cultural heritage domain and\nartistic practices. However, the panorama of ontologies for musicological\napplications seems to be limited and restricted to specific applications. In\nthis research, we propose HaMSE, an ontology capable of describing musical\nfeatures that can assist musicological research. More specifically, HaMSE\nproposes to address sues that have been affecting musicological research for\ndecades: the representation of music and the relationship between quantitative\nand qualitative data. To do this, HaMSE allows the alignment between different\nmusic representation systems and describes a set of musicological features that\ncan allow the music analysis at different granularity levels.", "published": "2022-02-11 18:26:24", "link": "http://arxiv.org/abs/2202.05817v1", "categories": ["cs.SD", "cs.AI", "cs.DL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Sonification of the zCOSMOS Galaxy Dataset", "abstract": "Sonification is the transformation of data into acoustic signals, achievable\nthrough different techniques. Sonification can be defined as a way to represent\ndata values and relations as perceivable sounds, aiming at facilitating their\ncommunication and interpretation. Like data visualization provides meaning via\nimages, sonification conveys meaning via sound. Sonification approaches are\nuseful in a number of scenario. A first case is the possibility to receive\ninformation while keeping other sensory channels free, like in medical\nenvironment, in driving experience, etc. Another scenario addresses an easier\nrecognition of patterns when data present high dimensionality and cardinality.\nFinally, sonification can be applied to presentation and dissemination\ninitiatives, also with artistic goals. The zCOSMOS dataset contains detailed\ndata about almost 20000 galaxies, describing the evolution of a relatively\nsmall portion of the universe in the last 10 million years in terms of galaxy\nmass, absolute luminosity, redshift, distance, age, and star formation rate.\nThe present paper proposes a sonification for the mentioned dataset, with the\nfollowing goals: i) providing a general description of the dataset, accessible\nvia sound, which could also make unnoticed patterns emerge; ii) realizing an\nartistic but scientifically accurate sonic portrait of a portion of the\nuniverse, thus filling the gap between art and science in the context of\nscientific dissemination and so-called \"edutainment\"; iii) adding value to the\ndataset, since also scientific data and achievements must be considered as a\ncultural heritage that needs to be preserved and enhanced. Both scientific and\ntechnological aspects of the sonification are addressed.", "published": "2022-02-11 10:36:05", "link": "http://arxiv.org/abs/2202.05539v1", "categories": ["cs.SD", "astro-ph.IM", "cs.CY", "eess.AS", "physics.ed-ph", "physics.soc-ph"], "primary_category": "cs.SD"}
