{"title": "Boosting Biomedical Concept Extraction by Rule-Based Data Augmentation", "abstract": "Document-level biomedical concept extraction is the task of identifying\nbiomedical concepts mentioned in a given document. Recent advancements have\nadapted pre-trained language models for this task. However, the scarcity of\ndomain-specific data and the deviation of concepts from their canonical names\noften hinder these models' effectiveness. To tackle this issue, we employ\nMetaMapLite, an existing rule-based concept mapping system, to generate\nadditional pseudo-annotated data from PubMed and PMC. The annotated data are\nused to augment the limited training data. Through extensive experiments, this\nstudy demonstrates the utility of a manually crafted concept mapping tool for\ntraining a better concept extraction model.", "published": "2024-07-03 00:00:21", "link": "http://arxiv.org/abs/2407.02719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "e-Health CSIRO at \"Discharge Me!\" 2024: Generating Discharge Summary\n  Sections with Fine-tuned Language Models", "abstract": "Clinical documentation is an important aspect of clinicians' daily work and\noften demands a significant amount of time. The BioNLP 2024 Shared Task on\nStreamlining Discharge Documentation (Discharge Me!) aims to alleviate this\ndocumentation burden by automatically generating discharge summary sections,\nincluding brief hospital course and discharge instruction, which are often\ntime-consuming to synthesize and write manually. We approach the generation\ntask by fine-tuning multiple open-sourced language models (LMs), including both\ndecoder-only and encoder-decoder LMs, with various configurations on input\ncontext. We also examine different setups for decoding algorithms, model\nensembling or merging, and model specialization. Our results show that\nconditioning on the content of discharge summary prior to the target sections\nis effective for the generation task. Furthermore, we find that smaller\nencoder-decoder LMs can work as well or even slightly better than larger\ndecoder based LMs fine-tuned through LoRA. The model checkpoints from our team\n(aehrc) are openly available.", "published": "2024-07-03 00:32:28", "link": "http://arxiv.org/abs/2407.02723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MentalAgora: A Gateway to Advanced Personalized Care in Mental Health\n  through Multi-Agent Debating and Attribute Control", "abstract": "As mental health issues globally escalate, there is a tremendous need for\nadvanced digital support systems. We introduce MentalAgora, a novel framework\nemploying large language models enhanced by interaction between multiple agents\nfor tailored mental health support. This framework operates through three\nstages: strategic debating, tailored counselor creation, and response\ngeneration, enabling the dynamic customization of responses based on individual\nuser preferences and therapeutic needs. We conduct experiments utilizing a\nhigh-quality evaluation dataset TherapyTalk crafted with mental health\nprofessionals, shwoing that MentalAgora generates expert-aligned and user\npreference-enhanced responses. Our evaluations, including experiments and user\nstudies, demonstrate that MentalAgora aligns with professional standards and\neffectively meets user preferences, setting a new benchmark for digital mental\nhealth interventions.", "published": "2024-07-03 01:19:38", "link": "http://arxiv.org/abs/2407.02736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Reduce: Towards Improving Performance of Large Language\n  Models on Structured Data", "abstract": "Large Language Models (LLMs) have been achieving competent performance on a\nwide range of downstream tasks, yet existing work shows that inference on\nstructured data is challenging for LLMs. This is because LLMs need to either\nunderstand long structured data or select the most relevant evidence before\ninference, and both approaches are not trivial. This paper proposes a\nframework, Learning to Reduce, that fine-tunes a language model with On-Policy\nLearning to generate a reduced version of an input structured data. When\ncompared to state-of-the-art LLMs like GPT-4, Learning to Reduce not only\nachieves outstanding performance in reducing the input, but shows\ngeneralizability on different datasets. We further show that the model\nfine-tuned with our framework helps LLMs better perform on table QA tasks\nespecially when the context is longer.", "published": "2024-07-03 01:51:50", "link": "http://arxiv.org/abs/2407.02750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Contextualised Word Embedding Dimensions Specified for\n  Contextual and Temporal Semantic Changes", "abstract": "The sense-aware contextualised word embeddings (SCWEs) encode semantic\nchanges of words within the contextualised word embedding (CWE) spaces. Despite\nthe superior performance of SCWEs in contextual/temporal semantic change\ndetection (SCD) benchmarks, it remains unclear as to how the meaning changes\nare encoded in the embedding space. To study this, we compare pre-trained CWEs\nand their fine-tuned versions on contextual and temporal semantic change\nbenchmarks under Principal Component Analysis (PCA) and Independent Component\nAnalysis (ICA) transformations. Our experimental results reveal (a) although\nthere exist a smaller number of axes that are specific to semantic changes of\nwords in the pre-trained CWE space, this information gets distributed across\nall dimensions when fine-tuned, and (b) in contrast to prior work studying the\ngeometry of CWEs, we find that PCA to better represent semantic changes than\nICA within the top 10% of axes. These findings encourage the development of\nmore efficient SCD methods with a small number of SCD-aware dimensions. Source\ncode is available at https://github.com/LivNLP/svp-dims .", "published": "2024-07-03 05:42:20", "link": "http://arxiv.org/abs/2407.02820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-Based Sentiment Analysis Techniques: A Comparative Study", "abstract": "Since the dawn of the digitalisation era, customer feedback and online\nreviews are unequivocally major sources of insights for businesses.\nConsequently, conducting comparative analyses of such sources has become the de\nfacto modus operandi of any business that wishes to give itself a competitive\nedge over its peers and improve customer loyalty. Sentiment analysis is one\nsuch method instrumental in gauging public interest, exposing market trends,\nand analysing competitors. While traditional sentiment analysis focuses on\noverall sentiment, as the needs advance with time, it has become important to\nexplore public opinions and sentiments on various specific subjects, products\nand services mentioned in the reviews on a finer-granular level. To this end,\nAspect-based Sentiment Analysis (ABSA), supported by advances in Artificial\nIntelligence (AI) techniques which have contributed to a paradigm shift from\nsimple word-level analysis to tone and context-aware analyses, focuses on\nidentifying specific aspects within the text and determining the sentiment\nassociated with each aspect. In this study, we compare several deep-NN methods\nfor ABSA on two benchmark datasets (Restaurant14 and Laptop-14) and found that\nFAST LSA obtains the best overall results of 87.6% and 82.6% accuracy but does\nnot pass LSA+DeBERTa which reports 90.33% and 86.21% accuracy respectively.", "published": "2024-07-03 06:21:07", "link": "http://arxiv.org/abs/2407.02834v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Feature-based and Context-aware Approaches to PII\n  Generalization Level Prediction", "abstract": "Protecting Personal Identifiable Information (PII) in text data is crucial\nfor privacy, but current PII generalization methods face challenges such as\nuneven data distributions and limited context awareness. To address these\nissues, we propose two approaches: a feature-based method using machine\nlearning to improve performance on structured inputs, and a novel context-aware\nframework that considers the broader context and semantic relationships between\nthe original text and generalized candidates. The context-aware approach\nemploys Multilingual-BERT for text representation, functional transformations,\nand mean squared error scoring to evaluate candidates. Experiments on the\nWikiReplace dataset demonstrate the effectiveness of both methods, with the\ncontext-aware approach outperforming the feature-based one across different\nscales. This work contributes to advancing PII generalization techniques by\nhighlighting the importance of feature selection, ensemble learning, and\nincorporating contextual information for better privacy protection in text\nanonymization.", "published": "2024-07-03 06:32:03", "link": "http://arxiv.org/abs/2407.02837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for\n  Multi-Hop Question Answering", "abstract": "Large Language Models (LLMs) with chain-of-thought (COT) prompting have\ndemonstrated impressive abilities on simple nature language inference tasks.\nHowever, they tend to perform poorly on Multi-hop Question Answering (MHQA)\ntasks due to several challenges, including hallucination, error propagation and\nlimited context length. We propose a prompting method, Finite State Machine\n(FSM) to enhance the reasoning capabilities of LLM for complex tasks in\naddition to improved effectiveness and trustworthiness. Different from COT\nmethods, FSM addresses MHQA by iteratively decomposing a question into\nmulti-turn sub-questions, and self-correcting in time, improving the accuracy\nof answers in each step. Specifically, FSM addresses one sub-question at a time\nand decides on the next step based on its current result and state, in an\nautomaton-like format. Experiments on benchmarks show the effectiveness of our\nmethod. Although our method performs on par with the baseline on relatively\nsimpler datasets, it excels on challenging datasets like Musique. Moreover,\nthis approach mitigates the hallucination phenomenon, wherein the correct final\nanswer can be recovered despite errors in intermediate reasoning. Furthermore,\nour method improves LLMs' ability to follow specified output format\nrequirements, significantly reducing the difficulty of answer interpretation\nand the need for reformatting.", "published": "2024-07-03 10:01:01", "link": "http://arxiv.org/abs/2407.02964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Dialect Identification in Automatic Dialectal Text\n  Normalization", "abstract": "Dialectal Arabic is the primary spoken language used by native Arabic\nspeakers in daily communication. The rise of social media platforms has notably\nexpanded its use as a written language. However, Arabic dialects do not have\nstandard orthographies. This, combined with the inherent noise in\nuser-generated content on social media, presents a major challenge to NLP\napplications dealing with Dialectal Arabic. In this paper, we explore and\nreport on the task of CODAfication, which aims to normalize Dialectal Arabic\ninto the Conventional Orthography for Dialectal Arabic (CODA). We work with a\nunique parallel corpus of multiple Arabic dialects focusing on five major city\ndialects. We benchmark newly developed pretrained sequence-to-sequence models\non the task of CODAfication. We further show that using dialect identification\ninformation improves the performance across all dialects. We make our code,\ndata, and pretrained models publicly available.", "published": "2024-07-03 11:30:03", "link": "http://arxiv.org/abs/2407.03020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strategies for Arabic Readability Modeling", "abstract": "Automatic readability assessment is relevant to building NLP applications for\neducation, content analysis, and accessibility. However, Arabic readability\nassessment is a challenging task due to Arabic's morphological richness and\nlimited readability resources. In this paper, we present a set of experimental\nresults on Arabic readability assessment using a diverse range of approaches,\nfrom rule-based methods to Arabic pretrained language models. We report our\nresults on a newly created corpus at different textual granularity levels\n(words and sentence fragments). Our results show that combining different\ntechniques yields the best results, achieving an overall macro F1 score of 86.7\nat the word level and 87.9 at the fragment level on a blind test set. We make\nour code, data, and pretrained models publicly available.", "published": "2024-07-03 11:54:11", "link": "http://arxiv.org/abs/2407.03032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Conversational Abilities of Quantized Large Language Models\n  via Direct Preference Alignment", "abstract": "The rapid advancement of large language models (LLMs) has facilitated their\ntransformation into conversational chatbots that can grasp contextual nuances\nand generate pertinent sentences, closely mirroring human values through\nadvanced techniques such as instruction tuning and reinforcement learning from\nhuman feedback (RLHF). However, the computational efficiency required for LLMs,\nachieved through techniques like post-training quantization (PTQ), presents\nchallenges such as token-flipping that can impair chatbot performance. In\nresponse, we propose a novel preference alignment approach, quantization-aware\ndirect preference optimization (QDPO), that aligns quantized LLMs with their\nfull-precision counterparts, improving conversational abilities. Evaluated on\ntwo instruction-tuned LLMs in various languages, QDPO demonstrated superior\nperformance in improving conversational abilities compared to established PTQ\nand knowledge-distillation fine-tuning techniques, marking a significant step\nforward in the development of efficient and effective conversational LLMs.", "published": "2024-07-03 12:19:06", "link": "http://arxiv.org/abs/2407.03051v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALTER: Augmentation for Large-Table-Based Reasoning", "abstract": "While extensive research has explored the use of large language models (LLMs)\nfor table-based reasoning, most approaches struggle with scalability when\napplied to large tables. To maintain the superior comprehension abilities of\nLLMs in these scenarios, we introduce ALTER(Augmentation for Large-Table-Based\nReasoning)-a framework designed to harness the latent augmentation potential in\nboth free-form natural language (NL) questions, via the query augmentor, and\nsemi-structured tabular data, through the table augmentor. By utilizing only a\nsmall subset of relevant data from the table and supplementing it with\npre-augmented schema, semantic, and literal information, ALTER achieves\noutstanding performance on table-based reasoning benchmarks. We also provide a\ndetailed analysis of large-table scenarios, comparing different methods and\nvarious partitioning principles. In these scenarios, our method outperforms all\nother approaches and exhibits robustness and efficiency against perturbations.", "published": "2024-07-03 12:34:45", "link": "http://arxiv.org/abs/2407.03061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Case Study on Context-Aware Neural Machine Translation with Multi-Task\n  Learning", "abstract": "In document-level neural machine translation (DocNMT), multi-encoder\napproaches are common in encoding context and source sentences. Recent studies\n\\cite{li-etal-2020-multi-encoder} have shown that the context encoder generates\nnoise and makes the model robust to the choice of context. This paper further\ninvestigates this observation by explicitly modelling context encoding through\nmulti-task learning (MTL) to make the model sensitive to the choice of context.\nWe conduct experiments on cascade MTL architecture, which consists of one\nencoder and two decoders. Generation of the source from the context is\nconsidered an auxiliary task, and generation of the target from the source is\nthe main task. We experimented with German--English language pairs on News,\nTED, and Europarl corpora. Evaluation results show that the proposed MTL\napproach performs better than concatenation-based and multi-encoder DocNMT\nmodels in low-resource settings and is sensitive to the choice of context.\nHowever, we observe that the MTL models are failing to generate the source from\nthe context. These observations align with the previous studies, and this might\nsuggest that the available document-level parallel corpora are not\ncontext-aware, and a robust sentence-level model can outperform the\ncontext-aware models.", "published": "2024-07-03 12:50:49", "link": "http://arxiv.org/abs/2407.03076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cactus: Towards Psychological Counseling Conversations using Cognitive\n  Behavioral Theory", "abstract": "Recently, the demand for psychological counseling has significantly increased\nas more individuals express concerns about their mental health. This surge has\naccelerated efforts to improve the accessibility of counseling by using large\nlanguage models (LLMs) as counselors. To ensure client privacy, training\nopen-source LLMs faces a key challenge: the absence of realistic counseling\ndatasets. To address this, we introduce Cactus, a multi-turn dialogue dataset\nthat emulates real-life interactions using the goal-oriented and structured\napproach of Cognitive Behavioral Therapy (CBT). We create a diverse and\nrealistic dataset by designing clients with varied, specific personas, and\nhaving counselors systematically apply CBT techniques in their interactions. To\nassess the quality of our data, we benchmark against established psychological\ncriteria used to evaluate real counseling sessions, ensuring alignment with\nexpert evaluations. Experimental results demonstrate that Camel, a model\ntrained with Cactus, outperforms other models in counseling skills,\nhighlighting its effectiveness and potential as a counseling agent. We make our\ndata, model, and code publicly available.", "published": "2024-07-03 13:41:31", "link": "http://arxiv.org/abs/2407.03103v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Bias Evaluation for Large Language Models Requires Prompt\n  Variations", "abstract": "Warning: This paper contains examples of stereotypes and biases. Large\nLanguage Models (LLMs) exhibit considerable social biases, and various studies\nhave tried to evaluate and mitigate these biases accurately. Previous studies\nuse downstream tasks as prompts to examine the degree of social biases for\nevaluation and mitigation. While LLMs' output highly depends on prompts,\nprevious studies evaluating and mitigating bias have often relied on a limited\nvariety of prompts. In this paper, we investigate the sensitivity of LLMs when\nchanging prompt variations (task instruction and prompt, few-shot examples,\ndebias-prompt) by analyzing task performance and social bias of LLMs. Our\nexperimental results reveal that LLMs are highly sensitive to prompts to the\nextent that the ranking of LLMs fluctuates when comparing models for task\nperformance and social bias. Additionally, we show that LLMs have tradeoffs\nbetween performance and social bias caused by the prompts. Less bias from\nprompt setting may result in reduced performance. Moreover, the ambiguity of\ninstances is one of the reasons for this sensitivity to prompts in advanced\nLLMs, leading to various outputs. We recommend using diverse prompts, as in\nthis study, to compare the effects of prompts on social bias in LLMs.", "published": "2024-07-03 14:12:04", "link": "http://arxiv.org/abs/2407.03129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Translation Accuracy of Large Language Models through\n  Continual Pre-Training on Parallel Data", "abstract": "In this paper, we propose a two-phase training approach where pre-trained\nlarge language models are continually pre-trained on parallel data and then\nsupervised fine-tuned with a small amount of high-quality parallel data. To\ninvestigate the effectiveness of our proposed approach, we conducted continual\npre-training with a 3.8B-parameter model and parallel data across eight\ndifferent formats. We evaluate these methods on thirteen test sets for\nJapanese-to-English and English-to-Japanese translation. The results\ndemonstrate that when utilizing parallel data in continual pre-training, it is\nessential to alternate between source and target sentences. Additionally, we\ndemonstrated that the translation accuracy improves only for translation\ndirections where the order of source and target sentences aligns between\ncontinual pre-training data and inference. In addition, we demonstrate that the\nLLM-based translation model is more robust in translating spoken language and\nachieves higher accuracy with less training data compared to supervised\nencoder-decoder models. We also show that the highest accuracy is achieved when\nthe data for continual pre-training consists of interleaved source and target\nsentences and when tags are added to the source sentences.", "published": "2024-07-03 14:23:36", "link": "http://arxiv.org/abs/2407.03145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through\n  Self-Correction in Language Models", "abstract": "Requiring a Large Language Model to generate intermediary reasoning steps has\nbeen shown to be an effective way of boosting performance. In fact, it has been\nfound that instruction tuning on these intermediary reasoning steps improves\nmodel performance. In this work, we present a novel method of further improving\nperformance by requiring models to compare multiple reasoning chains before\ngenerating a solution in a single inference step. We call this method Divergent\nCoT (DCoT). We find that instruction tuning on DCoT datasets boosts the\nperformance of even smaller, and therefore more accessible, LLMs. Through a\nrigorous set of experiments spanning a wide range of tasks that require various\nreasoning types, we show that fine-tuning on DCoT consistently improves\nperformance over the CoT baseline across model families and scales (1.3B to\n70B). Through a combination of empirical and manual evaluation, we additionally\nshow that these performance gains stem from models generating multiple\ndivergent reasoning chains in a single inference step, indicative of the\nenabling of self-correction in language models. Our code and data are publicly\navailable at https://github.com/UKPLab/arxiv2024-divergent-cot.", "published": "2024-07-03 15:01:18", "link": "http://arxiv.org/abs/2407.03181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CATT: Character-based Arabic Tashkeel Transformer", "abstract": "Tashkeel, or Arabic Text Diacritization (ATD), greatly enhances the\ncomprehension of Arabic text by removing ambiguity and minimizing the risk of\nmisinterpretations caused by its absence. It plays a crucial role in improving\nArabic text processing, particularly in applications such as text-to-speech and\nmachine translation. This paper introduces a new approach to training ATD\nmodels. First, we finetuned two transformers, encoder-only and encoder-decoder,\nthat were initialized from a pretrained character-based BERT. Then, we applied\nthe Noisy-Student approach to boost the performance of the best model. We\nevaluated our models alongside 11 commercial and open-source models using two\nmanually labeled benchmark datasets: WikiNews and our CATT dataset. Our\nfindings show that our top model surpasses all evaluated models by relative\nDiacritic Error Rates (DERs) of 30.83\\% and 35.21\\% on WikiNews and CATT,\nrespectively, achieving state-of-the-art in ATD. In addition, we show that our\nmodel outperforms GPT-4-turbo on CATT dataset by a relative DER of 9.36\\%. We\nopen-source our CATT models and benchmark dataset for the research\ncommunity\\footnote{https://github.com/abjadai/catt}.", "published": "2024-07-03 16:05:20", "link": "http://arxiv.org/abs/2407.03236v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STF: Sentence Transformer Fine-Tuning For Topic Categorization With\n  Limited Data", "abstract": "Nowadays, topic classification from tweets attracts considerable research\nattention. Different classification systems have been suggested thanks to these\nresearch efforts. Nevertheless, they face major challenges owing to low\nperformance metrics due to the limited amount of labeled data. We propose\nSentence Transformers Fine-tuning (STF), a topic detection system that\nleverages pretrained Sentence Transformers models and fine-tuning to classify\ntopics from tweets accurately. Moreover, extensive parameter sensitivity\nanalyses were conducted to finetune STF parameters for our topic classification\ntask to achieve the best performance results. Experiments on two benchmark\ndatasets demonstrated that (1) the proposed STF can be effectively used for\nclassifying tweet topics and outperforms the latest state-of-the-art\napproaches, and (2) the proposed STF does not require a huge amount of labeled\ntweets to achieve good accuracy, which is a limitation of many state-of-the-art\napproaches. Our main contribution is the achievement of promising results in\ntweet topic classification by applying pretrained sentence transformers\nlanguage models.", "published": "2024-07-03 16:34:56", "link": "http://arxiv.org/abs/2407.03253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Automatic Metrics with Incremental Machine Translation\n  Systems", "abstract": "We introduce a dataset comprising commercial machine translations, gathered\nweekly over six years across 12 translation directions. Since human A/B testing\nis commonly used, we assume commercial systems improve over time, which enables\nus to evaluate machine translation (MT) metrics based on their preference for\nmore recent translations. Our study not only confirms several prior findings,\nsuch as the advantage of neural metrics over non-neural ones, but also explores\nthe debated issue of how MT quality affects metric reliability--an\ninvestigation that smaller datasets in previous research could not sufficiently\nexplore. Overall, our research demonstrates the dataset's value as a testbed\nfor metric evaluation. We release our code at https://github.com/gjwubyron/Evo", "published": "2024-07-03 17:04:17", "link": "http://arxiv.org/abs/2407.03277v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Internal States Reveal Hallucination Risk Faced With a Query", "abstract": "The hallucination problem of Large Language Models (LLMs) significantly\nlimits their reliability and trustworthiness. Humans have a self-awareness\nprocess that allows us to recognize what we don't know when faced with queries.\nInspired by this, our paper investigates whether LLMs can estimate their own\nhallucination risk before response generation. We analyze the internal\nmechanisms of LLMs broadly both in terms of training data sources and across 15\ndiverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.\nOur empirical analysis reveals two key insights: (1) LLM internal states\nindicate whether they have seen the query in training data or not; and (2) LLM\ninternal states show they are likely to hallucinate or not regarding the query.\nOur study explores particular neurons, activation layers, and tokens that play\na crucial role in the LLM perception of uncertainty and hallucination risk. By\na probing estimator, we leverage LLM self-assessment, achieving an average\nhallucination estimation accuracy of 84.32\\% at run time.", "published": "2024-07-03 17:08:52", "link": "http://arxiv.org/abs/2407.03282v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Review of the Applications of Deep Learning-Based Emergent\n  Communication", "abstract": "Emergent communication, or emergent language, is the field of research which\nstudies how human language-like communication systems emerge de novo in deep\nmulti-agent reinforcement learning environments. The possibilities of\nreplicating the emergence of a complex behavior like language have strong\nintuitive appeal, yet it is necessary to complement this with clear notions of\nhow such research can be applicable to other fields of science, technology, and\nengineering. This paper comprehensively reviews the applications of emergent\ncommunication research across machine learning, natural language processing,\nlinguistics, and cognitive science. Each application is illustrated with a\ndescription of its scope, an explication of emergent communication's unique\nrole in addressing it, a summary of the extant literature working towards the\napplication, and brief recommendations for near-term research directions.", "published": "2024-07-03 17:43:54", "link": "http://arxiv.org/abs/2407.03302v1", "categories": ["cs.CL", "I.2.7; I.6.m"], "primary_category": "cs.CL"}
{"title": "XferBench: a Data-Driven Benchmark for Emergent Language", "abstract": "In this paper, we introduce a benchmark for evaluating the overall quality of\nemergent languages using data-driven methods. Specifically, we interpret the\nnotion of the \"quality\" of an emergent language as its similarity to human\nlanguage within a deep learning framework. We measure this by using the\nemergent language as pretraining data for a downstream NLP tasks in human\nlanguage -- the better the downstream performance, the better the emergent\nlanguage. We implement this benchmark as an easy-to-use Python package that\nonly requires a text file of utterances from the emergent language to be\nevaluated. Finally, we empirically test the benchmark's validity using human,\nsynthetic, and emergent language baselines.", "published": "2024-07-03 19:02:26", "link": "http://arxiv.org/abs/2407.03456v1", "categories": ["cs.CL", "I.2.7; I.6.m"], "primary_category": "cs.CL"}
{"title": "UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs'\n  Memorization", "abstract": "This paper introduces UnSeenTimeQA, a novel data contamination-free\ntime-sensitive question-answering (TSQA) benchmark. It differs from existing\nTSQA benchmarks by avoiding web-searchable queries grounded in the real-world.\nWe present a series of time-sensitive event scenarios based on synthetically\ngenerated facts. It requires large language models (LLMs) to engage in genuine\ntemporal reasoning without depending on the factual knowledge acquired during\nthe pre-training phase. We designed three types of time-sensitive questions to\ntest LLMs' temporal reasoning abilities over sequential and parallel event\noccurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals\nmixed results: while they perform well on simpler subsets, their overall\nperformance remains inferior as compared to real-world fact-based TSQA. Error\nanalysis of LLM-generated reasoning chains indicates that LLMs face\ndifficulties in reasoning over long-range event dependencies and parallel event\ntimelines that unfold concurrently.", "published": "2024-07-03 22:02:07", "link": "http://arxiv.org/abs/2407.03525v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias", "abstract": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla, (2) a curated dataset for bias measurement\nbenchmarking and (3) testing two different probing techniques for bias\ndetection in the context of Bangla. This is the first work of such kind\ninvolving bias assessment of LLMs for Bangla to the best of our knowledge. All\nour code and resources are publicly available for the progress of bias related\nresearch in Bangla NLP.", "published": "2024-07-03 22:45:36", "link": "http://arxiv.org/abs/2407.03536v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-level Aggregation of Lexical Metrics Correlates Stronger with\n  Human Judgements than Corpus-level Aggregation", "abstract": "In this paper we show that corpus-level aggregation hinders considerably the\ncapability of lexical metrics to accurately evaluate machine translation (MT)\nsystems. With empirical experiments we demonstrate that averaging individual\nsegment-level scores can make metrics such as BLEU and chrF correlate much\nstronger with human judgements and make them behave considerably more similar\nto neural metrics such as COMET and BLEURT. We show that this difference exists\nbecause corpus- and segment-level aggregation differs considerably owing to the\nclassical average of ratio versus ratio of averages Mathematical problem.\nMoreover, as we also show, such difference affects considerably the statistical\nrobustness of corpus-level aggregation. Considering that neural metrics\ncurrently only cover a small set of sufficiently-resourced languages, the\nresults in this paper can help make the evaluation of MT systems for\nlow-resource languages more trustworthy.", "published": "2024-07-03 13:46:24", "link": "http://arxiv.org/abs/2407.12832v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion and Intent Joint Understanding in Multimodal Conversation: A\n  Benchmarking Dataset", "abstract": "Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU)\naims to decode the semantic information manifested in a multimodal\nconversational history, while inferring the emotions and intents simultaneously\nfor the current utterance. MC-EIU is enabling technology for many\nhuman-computer interfaces. However, there is a lack of available datasets in\nterms of annotation, modality, language diversity, and accessibility. In this\nwork, we propose an MC-EIU dataset, which features 7 emotion categories, 9\nintent categories, 3 modalities, i.e., textual, acoustic, and visual content,\nand two languages, i.e., English and Mandarin. Furthermore, it is completely\nopen-source for free access. To our knowledge, MC-EIU is the first\ncomprehensive and rich emotion and intent joint understanding dataset for\nmultimodal conversation. Together with the release of the dataset, we also\ndevelop an Emotion and Intent Interaction (EI$^2$) network as a reference\nsystem by modeling the deep correlation between emotion and intent in the\nmultimodal conversation. With comparative experiments and ablation studies, we\ndemonstrate the effectiveness of the proposed EI$^2$ method on the MC-EIU\ndataset. The dataset and codes will be made available at:\nhttps://github.com/MC-EIU/MC-EIU.", "published": "2024-07-03 01:56:00", "link": "http://arxiv.org/abs/2407.02751v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language\n  Models", "abstract": "Knowledge distillation is an effective technique for pre-trained language\nmodel compression. Although existing knowledge distillation methods perform\nwell for the most typical model BERT, they could be further improved in two\naspects: the relation-level knowledge could be further explored to improve\nmodel performance; and the setting of student attention head number could be\nmore flexible to decrease inference time. Therefore, we are motivated to\npropose a novel knowledge distillation method MLKD-BERT to distill multi-level\nknowledge in teacher-student framework. Extensive experiments on GLUE benchmark\nand extractive question answering tasks demonstrate that our method outperforms\nstate-of-the-art knowledge distillation methods on BERT. In addition, MLKD-BERT\ncan flexibly set student attention head number, allowing for substantial\ninference time decrease with little performance drop.", "published": "2024-07-03 03:03:30", "link": "http://arxiv.org/abs/2407.02775v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Framework for Quantum Finite-State Languages with Density Mapping", "abstract": "A quantum finite-state automaton (QFA) is a theoretical model designed to\nsimulate the evolution of a quantum system with finite memory in response to\nsequential input strings. We define the language of a QFA as the set of strings\nthat lead the QFA to an accepting state when processed from its initial state.\nQFAs exemplify how quantum computing can achieve greater efficiency compared to\nclassical computing. While being one of the simplest quantum models, QFAs are\nstill notably challenging to construct from scratch due to the preliminary\nknowledge of quantum mechanics required for superimposing unitary constraints\non the automata. Furthermore, even when QFAs are correctly assembled, the\nlimitations of a current quantum computer may cause fluctuations in the\nsimulation results depending on how an assembled QFA is translated into a\nquantum circuit.\n  We present a framework that provides a simple and intuitive way to build QFAs\nand maximize the simulation accuracy. Our framework relies on two methods:\nFirst, it offers a predefined construction for foundational types of QFAs that\nrecognize special languages MOD and EQU. They play a role of basic building\nblocks for more complex QFAs. In other words, one can obtain more complex QFAs\nfrom these foundational automata using standard language operations. Second, we\nimprove the simulation accuracy by converting these QFAs into quantum circuits\nsuch that the resulting circuits perform well on noisy quantum computers.\n  Our framework is available at https://github.com/sybaik1/qfa-toolkit.", "published": "2024-07-03 03:06:37", "link": "http://arxiv.org/abs/2407.02776v1", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "52B to 1T: Lessons Learned via Tele-FLM Series", "abstract": "Large Language Models (LLMs) represent a significant stride toward Artificial\nGeneral Intelligence. As scaling laws underscore the potential of increasing\nmodel sizes, the academic community has intensified its investigations into\nLLMs with capacities exceeding 50 billion parameters. This technical report\nbuilds on our prior work with Tele-FLM (also known as FLM-2), a publicly\navailable 52-billion-parameter model. We delve into two primary areas: we first\ndiscuss our observation of Supervised Fine-tuning (SFT) on Tele-FLM-52B, which\nsupports the \"less is more\" approach for SFT data construction; second, we\ndemonstrate our experiments and analyses on the best practices for\nprogressively growing a model from 52 billion to 102 billion, and subsequently\nto 1 trillion parameters. We will open-source a 1T model checkpoint, namely\nTele-FLM-1T, to advance further training and research.", "published": "2024-07-03 03:21:02", "link": "http://arxiv.org/abs/2407.02783v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Training of Language Models with Compact and Consistent Next\n  Token Distributions", "abstract": "Maximizing the likelihood of the next token is an established, statistically\nsound objective for pre-training language models. In this paper we show that we\ncan train better models faster by pre-aggregating the corpus with a collapsed\n$n$-gram distribution. Previous studies have proposed corpus-level $n$-gram\nstatistics as a regularizer; however, the construction and querying of such\n$n$-grams, if done naively, prove to be costly and significantly impede\ntraining speed, thereby limiting their application in modern large language\nmodel pre-training.\n  We introduce an alternative compact representation of the next token\ndistribution that, in expectation, aligns with the complete $n$-gram\ndistribution while markedly reducing variance across mini-batches compared to\nthe standard next-token loss. Empirically, we demonstrate that both the\n$n$-gram regularized model and our approximation yield substantial improvements\nin model quality and convergence rate compared to existing methods.\nFurthermore, our approximation facilitates scalability of gains to larger\ndatasets and models compared to the straightforward $n$-gram regularization\nmethod.", "published": "2024-07-03 05:40:41", "link": "http://arxiv.org/abs/2407.02819v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Spatio-Temporal Representation Learning as an Alternative to\n  Traditional Glosses in Sign Language Translation and Production", "abstract": "This work addresses the challenges associated with the use of glosses in both\nSign Language Translation (SLT) and Sign Language Production (SLP). While\nglosses have long been used as a bridge between sign language and spoken\nlanguage, they come with two major limitations that impede the advancement of\nsign language systems. First, annotating the glosses is a labor-intensive and\ntime-consuming process, which limits the scalability of datasets. Second, the\nglosses oversimplify sign language by stripping away its spatio-temporal\ndynamics, reducing complex signs to basic labels and missing the subtle\nmovements essential for precise interpretation. To address these limitations,\nwe introduce Universal Gloss-level Representation (UniGloR), a framework\ndesigned to capture the spatio-temporal features inherent in sign language,\nproviding a more dynamic and detailed alternative to the use of the glosses.\nThe core idea of UniGloR is simple yet effective: We derive dense\nspatio-temporal representations from sign keypoint sequences using\nself-supervised learning and seamlessly integrate them into SLT and SLP tasks.\nOur experiments in a keypoint-based setting demonstrate that UniGloR either\noutperforms or matches the performance of previous SLT and SLP methods on two\nwidely-used datasets: PHOENIX14T and How2Sign.", "published": "2024-07-03 07:12:36", "link": "http://arxiv.org/abs/2407.02854v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Contrast then Memorize: Semantic Neighbor Retrieval-Enhanced Inductive\n  Multimodal Knowledge Graph Completion", "abstract": "A large number of studies have emerged for Multimodal Knowledge Graph\nCompletion (MKGC) to predict the missing links in MKGs. However, fewer studies\nhave been proposed to study the inductive MKGC (IMKGC) involving emerging\nentities unseen during training. Existing inductive approaches focus on\nlearning textual entity representations, which neglect rich semantic\ninformation in visual modality. Moreover, they focus on aggregating structural\nneighbors from existing KGs, which of emerging entities are usually limited.\nHowever, the semantic neighbors are decoupled from the topology linkage and\nusually imply the true target entity. In this paper, we propose the IMKGC task\nand a semantic neighbor retrieval-enhanced IMKGC framework CMR, where the\ncontrast brings the helpful semantic neighbors close, and then the memorize\nsupports semantic neighbor retrieval to enhance inference. Specifically, we\nfirst propose a unified cross-modal contrastive learning to simultaneously\ncapture the textual-visual and textual-textual correlations of query-entity\npairs in a unified representation space. The contrastive learning increases the\nsimilarity of positive query-entity pairs, therefore making the representations\nof helpful semantic neighbors close. Then, we explicitly memorize the knowledge\nrepresentations to support the semantic neighbor retrieval. At test time, we\nretrieve the nearest semantic neighbors and interpolate them to the\nquery-entity similarity distribution to augment the final prediction. Extensive\nexperiments validate the effectiveness of CMR on three inductive MKGC datasets.\nCodes are available at https://github.com/OreOZhao/CMR.", "published": "2024-07-03 07:31:33", "link": "http://arxiv.org/abs/2407.02867v1", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "CoIR: A Comprehensive Benchmark for Code Information Retrieval Models", "abstract": "Despite the substantial success of Information Retrieval (IR) in various NLP\ntasks, most IR systems predominantly handle queries and corpora in natural\nlanguage, neglecting the domain of code retrieval. Code retrieval is critically\nimportant yet remains under-explored, with existing methods and benchmarks\ninadequately representing the diversity of code in various domains and tasks.\nAddressing this gap, we present COIR (Code Information Retrieval Benchmark), a\nrobust and comprehensive benchmark specifically designed to assess code\nretrieval capabilities. COIR comprises ten meticulously curated code datasets,\nspanning eight distinctive retrieval tasks across seven diverse domains. We\nfirst discuss the construction of COIR and its diverse dataset composition.\nFurther, we evaluate nine widely used retrieval models using COIR, uncovering\nsignificant difficulties in performing code retrieval tasks even with\nstate-of-the-art systems. To facilitate easy adoption and integration within\nexisting research workflows, COIR has been developed as a user-friendly Python\nframework, readily installable via pip. It shares same data schema as other\npopular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark\nevaluations. Through COIR, we aim to invigorate research in the code retrieval\ndomain, providing a versatile benchmarking tool that encourages further\ndevelopment and exploration of code retrieval systems\nhttps://github.com/CoIR-team/coir.", "published": "2024-07-03 07:58:20", "link": "http://arxiv.org/abs/2407.02883v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Translatotron-V(ison): An End-to-End Model for In-Image Machine\n  Translation", "abstract": "In-image machine translation (IIMT) aims to translate an image containing\ntexts in source language into an image containing translations in target\nlanguage. In this regard, conventional cascaded methods suffer from issues such\nas error propagation, massive parameters, and difficulties in deployment and\nretaining visual characteristics of the input image. Thus, constructing\nend-to-end models has become an option, which, however, faces two main\nchallenges: 1) the huge modeling burden, as it is required to simultaneously\nlearn alignment across languages and preserve the visual characteristics of the\ninput image; 2) the difficulties of directly predicting excessively lengthy\npixel sequences. In this paper, we propose \\textit{Translatotron-V(ision)}, an\nend-to-end IIMT model consisting of four modules. In addition to an image\nencoder, and an image decoder, our model contains a target text decoder and an\nimage tokenizer. Among them, the target text decoder is used to alleviate the\nlanguage alignment burden, and the image tokenizer converts long sequences of\npixels into shorter sequences of visual tokens, preventing the model from\nfocusing on low-level visual features. Besides, we present a two-stage training\nframework for our model to assist the model in learning alignment across\nmodalities and languages. Finally, we propose a location-aware evaluation\nmetric called Structure-BLEU to assess the translation quality of the generated\nimages. Experimental results demonstrate that our model achieves competitive\nperformance compared to cascaded models with only 70.9\\% of parameters, and\nsignificantly outperforms the pixel-level end-to-end IIMT model.", "published": "2024-07-03 08:15:39", "link": "http://arxiv.org/abs/2407.02894v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Negotiative Dialogue for the Talkamatic Dialogue Manager", "abstract": "The paper describes a number of dialogue phenomena associated with\nnegotiative dialogue, as implemented in a development version of the Talkamatic\nDialogue Manager (TDM). This implementation is an initial step towards full\ncoverage of general features of negotiative dialogue in TDM.", "published": "2024-07-03 08:49:18", "link": "http://arxiv.org/abs/2407.02917v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large\n  Language Models", "abstract": "Evaluating the graph comprehension and reasoning abilities of Large Language\nModels (LLMs) is challenging and often incomplete. Existing benchmarks focus\nprimarily on pure graph understanding, lacking a comprehensive evaluation\nacross all graph types and detailed capability definitions. This paper presents\nGraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and\nreasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and\ntest models on pure graph and heterogeneous graphs, subdividing capabilities\ninto 10 distinct areas tested through 19 tasks. Our benchmark includes 11\ndatasets with 5,140 graphs of varying complexity. We evaluate four\nclosed-source and eight open-source LLMs, conducting thorough analyses from\nboth ability and task perspectives. Key findings reveal that OpenAI o1 model\nhas amazing comprehension and reasoning capabilities, semantic enrichment\nenhances reasoning performance, node ordering impacts task success, and the\nability to process longer texts does not necessarily improve graph\ncomprehension or reasoning.GraCoRe is open-sourced at\nhttps://github.com/ZIKEYUAN/GraCoRe", "published": "2024-07-03 09:12:38", "link": "http://arxiv.org/abs/2407.02936v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Mast Kalandar at SemEval-2024 Task 8: On the Trail of Textual Origins:\n  RoBERTa-BiLSTM Approach to Detect AI-Generated Text", "abstract": "Large Language Models (LLMs) have showcased impressive abilities in\ngenerating fluent responses to diverse user queries. However, concerns\nregarding the potential misuse of such texts in journalism, educational, and\nacademic contexts have surfaced. SemEval 2024 introduces the task of\nMultigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection, aiming to develop automated systems for identifying\nmachine-generated text and detecting potential misuse. In this paper, we i)\npropose a RoBERTa-BiLSTM based classifier designed to classify text into two\ncategories: AI-generated or human ii) conduct a comparative study of our model\nwith baseline approaches to evaluate its effectiveness. This paper contributes\nto the advancement of automatic text detection systems in addressing the\nchallenges posed by machine-generated text misuse. Our architecture ranked 46th\non the official leaderboard with an accuracy of 80.83 among 125.", "published": "2024-07-03 10:22:23", "link": "http://arxiv.org/abs/2407.02978v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Consistent over Value-laden Questions?", "abstract": "Large language models (LLMs) appear to bias their survey answers toward\ncertain values. Nonetheless, some argue that LLMs are too inconsistent to\nsimulate particular values. Are they? To answer, we first define value\nconsistency as the similarity of answers across (1) paraphrases of one\nquestion, (2) related questions under one topic, (3) multiple-choice and\nopen-ended use-cases of one question, and (4) multilingual translations of a\nquestion to English, Chinese, German, and Japanese. We apply these measures to\nsmall and large, open LLMs including llama-3, as well as gpt-4o, using 8,000\nquestions spanning more than 300 topics. Unlike prior work, we find that models\nare relatively consistent across paraphrases, use-cases, translations, and\nwithin a topic. Still, some inconsistencies remain. Models are more consistent\non uncontroversial topics (e.g., in the U.S., \"Thanksgiving\") than on\ncontroversial ones (\"euthanasia\"). Base models are both more consistent\ncompared to fine-tuned models and are uniform in their consistency across\ntopics, while fine-tuned models are more inconsistent about some topics\n(\"euthanasia\") than others (\"women's rights\") like our human subjects (n=165).", "published": "2024-07-03 10:53:54", "link": "http://arxiv.org/abs/2407.02996v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values", "abstract": "Large vision language models (VLMs) have demonstrated significant potential\nfor integration into daily life, making it crucial for them to incorporate\nhuman values when making decisions in real-world situations. This paper\nintroduces VIVA, a benchmark for VIsion-grounded decision-making driven by\nhuman VAlues. While most large VLMs focus on physical-level skills, our work is\nthe first to examine their multimodal capabilities in leveraging human values\nto make decisions under a vision-depicted situation. VIVA contains 1,240 images\ndepicting diverse real-world situations and the manually annotated decisions\ngrounded in them. Given an image there, the model should select the most\nappropriate action to address the situation and provide the relevant human\nvalues and reason underlying the decision. Extensive experiments based on VIVA\nshow the limitation of VLMs in using human values to make multimodal decisions.\nFurther analyses indicate the potential benefits of exploiting action\nconsequences and predicted human values.", "published": "2024-07-03 10:59:06", "link": "http://arxiv.org/abs/2407.03000v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SemioLLM: Assessing Large Language Models for Semiological Analysis in\n  Epilepsy Research", "abstract": "Large Language Models have shown promising results in their ability to encode\ngeneral medical knowledge in standard medical question-answering datasets.\nHowever, their potential application in clinical practice requires evaluation\nin domain-specific tasks, where benchmarks are largely missing. In this study\nsemioLLM, we test the ability of state-of-the-art LLMs (GPT-3.5, GPT-4, Mixtral\n8x7B, and Qwen-72chat) to leverage their internal knowledge and reasoning for\nepilepsy diagnosis. Specifically, we obtain likelihood estimates linking\nunstructured text descriptions of seizures to seizure-generating brain regions,\nusing an annotated clinical database containing 1269 entries. We evaluate the\nLLM's performance, confidence, reasoning, and citation abilities in comparison\nto clinical evaluation. Models achieve above-chance classification performance\nwith prompt engineering significantly improving their outcome, with some models\nachieving close-to-clinical performance and reasoning. However, our analyses\nalso reveal significant pitfalls with several models being overly confident\nwhile showing poor performance, as well as exhibiting citation errors and\nhallucinations. In summary, our work provides the first extensive benchmark\ncomparing current SOTA LLMs in the medical domain of epilepsy and highlights\ntheir ability to leverage unstructured texts from patients' medical history to\naid diagnostic processes in health care.", "published": "2024-07-03 11:02:12", "link": "http://arxiv.org/abs/2407.03004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Affects the Stability of Tool Learning? An Empirical Study on the\n  Robustness of Tool Learning Frameworks", "abstract": "Tool learning methods have enhanced the ability of large language models\n(LLMs) to interact with real-world applications. Many existing works fine-tune\nLLMs or design prompts to enable LLMs to select appropriate tools and correctly\ninvoke them to meet user requirements. However, it is observed in previous\nworks that the performance of tool learning varies from tasks, datasets,\ntraining settings, and algorithms. Without understanding the impact of these\nfactors, it can lead to inconsistent results, inefficient model deployment, and\nsuboptimal tool utilization, ultimately hindering the practical integration and\nscalability of LLMs in real-world scenarios. Therefore, in this paper, we\nexplore the impact of both internal and external factors on the performance of\ntool learning frameworks. Through extensive experiments on two benchmark\ndatasets, we find several insightful conclusions for future work, including the\nobservation that LLMs can benefit significantly from increased trial and\nexploration. We believe our empirical study provides a new perspective for\nfuture tool learning research.", "published": "2024-07-03 11:06:05", "link": "http://arxiv.org/abs/2407.03007v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction\n  Tuning for Large Language Model", "abstract": "Instruction tuning as an effective technique aligns the outputs of large\nlanguage models (LLMs) with human preference. But how to generate the seasonal\nmulti-turn dialogues from raw documents for instruction tuning still requires\nfurther exploration. In this paper, we present a novel framework named R2S that\nleverages the CoD-Chain of Dialogue logic to guide large language models (LLMs)\nin generating knowledge-intensive multi-turn dialogues for instruction tuning.\nBy integrating raw documents from both open-source datasets and domain-specific\nweb-crawled documents into a benchmark K-BENCH, we cover diverse areas such as\nWikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach\nfirst decides the logic flow of the current dialogue and then prompts LLMs to\nproduce key phrases for sourcing relevant response content. This methodology\nenables the creation of the G I NSTRUCT instruction dataset, retaining raw\ndocument knowledge within dialoguestyle interactions. Utilizing this dataset,\nwe fine-tune GLLM, a model designed to transform raw documents into structured\nmulti-turn dialogues, thereby injecting comprehensive domain knowledge into the\nSFT model for enhanced instruction tuning. This work signifies a stride towards\nrefining the adaptability and effectiveness of LLMs in processing and\ngenerating more accurate, contextually nuanced responses across various fields.", "published": "2024-07-03 12:04:10", "link": "http://arxiv.org/abs/2407.03040v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CiteAssist: A System for Automated Preprint Citation and BibTeX\n  Generation", "abstract": "We present CiteAssist, a system to automate the generation of BibTeX entries\nfor preprints, streamlining the process of bibliographic annotation. Our system\nextracts metadata, such as author names, titles, publication dates, and\nkeywords, to create standardized annotations within the document. CiteAssist\nautomatically attaches the BibTeX citation to the end of a PDF and links it on\nthe first page of the document so other researchers gain immediate access to\nthe correct citation of the article. This method promotes platform flexibility\nby ensuring that annotations remain accessible regardless of the repository\nused to publish or access the preprint. The annotations remain available even\nif the preprint is viewed externally to CiteAssist. Additionally, the system\nadds relevant related papers based on extracted keywords to the preprint,\nproviding researchers with additional publications besides those in related\nwork for further reading. Researchers can enhance their preprints organization\nand reference management workflows through a free and publicly available web\ninterface.", "published": "2024-07-03 15:18:29", "link": "http://arxiv.org/abs/2407.03192v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "How Does Quantization Affect Multilingual LLMs?", "abstract": "Quantization techniques are widely used to improve inference speed and\ndeployment of large language models. While a wide body of work examines the\nimpact of quantization on LLMs in English, none have evaluated across\nlanguages. We conduct a thorough analysis of quantized multilingual LLMs,\nfocusing on performance across languages and at varying scales. We use\nautomatic benchmarks, LLM-as-a-Judge, and human evaluation, finding that (1)\nharmful effects of quantization are apparent in human evaluation, which\nautomatic metrics severely underestimate: a 1.7% average drop in Japanese\nacross automatic tasks corresponds to a 16.0% drop reported by human evaluators\non realistic prompts; (2) languages are disparately affected by quantization,\nwith non-Latin script languages impacted worst; and (3) challenging tasks like\nmathematical reasoning degrade fastest. As the ability to serve low-compute\nmodels is critical for wide global adoption of NLP technologies, our results\nurge consideration of multilingual performance as a key evaluation criterion\nfor efficient models.", "published": "2024-07-03 15:39:40", "link": "http://arxiv.org/abs/2407.03211v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Single Character Perturbations Break LLM Alignment", "abstract": "When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\navailable at https://github.com/hannah-aught/space_attack.", "published": "2024-07-03 16:03:10", "link": "http://arxiv.org/abs/2407.03232v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Similar Are Elected Politicians and Their Constituents? Quantitative\n  Evidence From Online Social Networks", "abstract": "How similar are politicians to those who vote for them? This is a critical\nquestion at the heart of democratic representation and particularly relevant at\ntimes when political dissatisfaction and populism are on the rise. To answer\nthis question we compare the online discourse of elected politicians and their\nconstituents. We collect a two and a half years (September 2020 - February\n2023) constituency-level dataset for USA and UK that includes: (i) the Twitter\ntimelines (5.6 Million tweets) of elected political representatives (595 UK\nMembers of Parliament and 433 USA Representatives), (ii) the Nextdoor posts\n(21.8 Million posts) of the constituency (98.4% USA and 91.5% UK\nconstituencies). We find that elected politicians tend to be equally similar to\ntheir constituents in terms of content and style regardless of whether a\nconstituency elects a right or left-wing politician. The size of the electoral\nvictory and the level of income of a constituency shows a nuanced picture. The\nnarrower the electoral victory, the more similar the style and the more\ndissimilar the content is. The lower the income of a constituency, the more\nsimilar the content is. In terms of style, poorer constituencies tend to have a\nmore similar sentiment and more dissimilar psychological text traits (i.e.\nmeasured with LIWC categories).", "published": "2024-07-03 16:36:26", "link": "http://arxiv.org/abs/2407.03255v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model\n  Supporting Long-Contextual Input and Output", "abstract": "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.", "published": "2024-07-03 17:59:21", "link": "http://arxiv.org/abs/2407.03320v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Collaborative Quest Completion with LLM-driven Non-Player Characters in\n  Minecraft", "abstract": "The use of generative AI in video game development is on the rise, and as the\nconversational and other capabilities of large language models continue to\nimprove, we expect LLM-driven non-player characters (NPCs) to become widely\ndeployed. In this paper, we seek to understand how human players collaborate\nwith LLM-driven NPCs to accomplish in-game goals. We design a minigame within\nMinecraft where a player works with two GPT4-driven NPCs to complete a quest.\nWe perform a user study in which 28 Minecraft players play this minigame and\nshare their feedback. On analyzing the game logs and recordings, we find that\nseveral patterns of collaborative behavior emerge from the NPCs and the human\nplayers. We also report on the current limitations of language-only models that\ndo not have rich game-state or visual understanding. We believe that this\npreliminary study and analysis will inform future game developers on how to\nbetter exploit these rapidly improving generative AI models for collaborative\nroles in games.", "published": "2024-07-03 19:11:21", "link": "http://arxiv.org/abs/2407.03460v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving LLM Abilities in Idiomatic Translation", "abstract": "For large language models (LLMs) like NLLB and GPT, translating idioms\nremains a challenge. Our goal is to enhance translation fidelity by improving\nLLM processing of idiomatic language while preserving the original linguistic\nstyle. This has a significant social impact, as it preserves cultural nuances\nand ensures translated texts retain their intent and emotional resonance,\nfostering better cross-cultural communication. Previous work has utilized\nknowledge bases like IdiomKB by providing the LLM with the meaning of an idiom\nto use in translation. Although this method yielded better results than a\ndirect translation, it is still limited in its ability to preserve idiomatic\nwriting style across languages. In this research, we expand upon the knowledge\nbase to find corresponding idioms in the target language. Our research performs\ntranslations using two methods: The first method employs the\nSentenceTransformers model to semantically generate cosine similarity scores\nbetween the meanings of the original and target language idioms, selecting the\nbest idiom (Cosine Similarity method). The second method uses an LLM to find a\ncorresponding idiom in the target language for use in the translation\n(LLM-generated idiom method). As a baseline, we performed a direct translation\nwithout providing additional information. Human evaluations on the English ->\nChinese, and Chinese -> English show the Cosine Similarity Lookup method\nout-performed others in all GPT4o translations. To further build upon IdiomKB,\nwe developed a low-resource Urdu dataset containing Urdu idioms and their\ntranslations. Despite dataset limitations, the Cosine Similarity Lookup method\nshows promise, potentially overcoming language barriers and enabling the\nexploration of diverse literary works in Chinese and Urdu.(LoResLM @ COLING\nPreprint)", "published": "2024-07-03 21:34:26", "link": "http://arxiv.org/abs/2407.03518v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Truth is Universal: Robust Detection of Lies in LLMs", "abstract": "Large Language Models (LLMs) have revolutionised natural language processing,\nexhibiting impressive human-like capabilities. In particular, LLMs are capable\nof \"lying\", knowingly outputting false statements. Hence, it is of interest and\nimportance to develop methods to detect when LLMs lie. Indeed, several authors\ntrained classifiers to detect LLM lies based on their internal model\nactivations. However, other researchers showed that these classifiers may fail\nto generalise, for example to negated statements. In this work, we aim to\ndevelop a robust method to detect when an LLM is lying. To this end, we make\nthe following key contributions: (i) We demonstrate the existence of a\ntwo-dimensional subspace, along which the activation vectors of true and false\nstatements can be separated. Notably, this finding is universal and holds for\nvarious LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our\nanalysis explains the generalisation failures observed in previous studies and\nsets the stage for more robust lie detection; (ii) Building upon (i), we\nconstruct an accurate LLM lie detector. Empirically, our proposed classifier\nachieves state-of-the-art performance, attaining 94% accuracy in both\ndistinguishing true from false factual statements and detecting lies generated\nin real-world scenarios.", "published": "2024-07-03 13:01:54", "link": "http://arxiv.org/abs/2407.12831v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ESQA: Event Sequences Question Answering", "abstract": "Event sequences (ESs) arise in many practical domains including finance,\nretail, social networks, and healthcare. In the context of machine learning,\nevent sequences can be seen as a special type of tabular data with annotated\ntimestamps. Despite the importance of ESs modeling and analysis, little effort\nwas made in adapting large language models (LLMs) to the ESs domain. In this\npaper, we highlight the common difficulties of ESs processing and propose a\nnovel solution capable of solving multiple downstream tasks with little or no\nfinetuning. In particular, we solve the problem of working with long sequences\nand improve time and numeric features processing. The resulting method, called\nESQA, effectively utilizes the power of LLMs and, according to extensive\nexperiments, achieves state-of-the-art results in the ESs domain.", "published": "2024-07-03 15:41:54", "link": "http://arxiv.org/abs/2407.12833v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explainable Natural Language Processing for Corporate Sustainability\n  Analysis", "abstract": "Sustainability commonly refers to entities, such as individuals, companies,\nand institutions, having a non-detrimental (or even positive) impact on the\nenvironment, society, and the economy. With sustainability becoming a synonym\nof acceptable and legitimate behaviour, it is being increasingly demanded and\nregulated. Several frameworks and standards have been proposed to measure the\nsustainability impact of corporations, including United Nations' sustainable\ndevelopment goals and the recently introduced global sustainability reporting\nframework, amongst others. However, the concept of corporate sustainability is\ncomplex due to the diverse and intricate nature of firm operations (i.e.\ngeography, size, business activities, interlinks with other stakeholders). As a\nresult, corporate sustainability assessments are plagued by subjectivity both\nwithin data that reflect corporate sustainability efforts (i.e. corporate\nsustainability disclosures) and the analysts evaluating them. This subjectivity\ncan be distilled into distinct challenges, such as incompleteness, ambiguity,\nunreliability and sophistication on the data dimension, as well as limited\nresources and potential bias on the analyst dimension. Put together,\nsubjectivity hinders effective cost attribution to entities non-compliant with\nprevailing sustainability expectations, potentially rendering sustainability\nefforts and its associated regulations futile. To this end, we argue that\nExplainable Natural Language Processing (XNLP) can significantly enhance\ncorporate sustainability analysis. Specifically, linguistic understanding\nalgorithms (lexical, semantic, syntactic), integrated with XAI capabilities\n(interpretability, explainability, faithfulness), can bridge gaps in analyst\nresources and mitigate subjectivity problems within data.", "published": "2024-07-03 08:27:51", "link": "http://arxiv.org/abs/2407.17487v3", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized\n  Retrieval Augmentation", "abstract": "Natural Language to Code Generation has made significant progress in recent\nyears with the advent of Large Language Models(LLMs). While generation for\ngeneral-purpose languages like C, C++, and Python has improved significantly,\nLLMs struggle with custom function names in Domain Specific Languages or DSLs.\nThis leads to higher hallucination rates and syntax errors, specially for DSLs\nhaving a high number of custom function names. Additionally, constant updates\nto function names add to the challenge as LLMs need to stay up-to-date. In this\npaper, we present optimizations for using Retrieval Augmented Generation (or\nRAG) with LLMs for DSL generation along with an ablation study comparing these\nstrategies. We generated a train as well as test dataset with a DSL to\nrepresent automation tasks across roughly 700 APIs in public domain. We used\nthe training dataset to fine-tune a Codex model for this DSL. Our results\nshowed that the fine-tuned model scored the best on code similarity metric.\nWith our RAG optimizations, we achieved parity for similarity metric. The\ncompilation rate, however, showed that both the models still got the syntax\nwrong many times, with RAG-based method being 2 pts better. Conversely,\nhallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for\nAPI parameter keys. We conclude that an optimized RAG model can match the\nquality of fine-tuned models and offer advantages for new, unseen APIs.", "published": "2024-07-03 01:28:51", "link": "http://arxiv.org/abs/2407.02742v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.2; I.2.7"], "primary_category": "cs.SE"}
{"title": "Gradient descent with generalized Newton's method", "abstract": "We propose the generalized Newton's method (GeN) -- a Hessian-informed\napproach that applies to any optimizer such as SGD and Adam, and covers the\nNewton-Raphson method as a sub-case. Our method automatically and dynamically\nselects the learning rate that accelerates the convergence, without the\nintensive tuning of the learning rate scheduler. In practice, our method is\neasily implementable, since it only requires additional forward passes with\nalmost zero computational overhead (in terms of training time and memory cost),\nif the overhead is amortized over many iterations. We present extensive\nexperiments on language and vision tasks (e.g. GPT and ResNet) to showcase that\nGeN optimizers match the state-of-the-art performance, which was achieved with\ncarefully tuned learning rate schedulers.", "published": "2024-07-03 03:01:43", "link": "http://arxiv.org/abs/2407.02772v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Images Speak Louder than Words: Understanding and Mitigating Bias in\n  Vision-Language Model from a Causal Mediation Perspective", "abstract": "Vision-language models (VLMs) pre-trained on extensive datasets can\ninadvertently learn biases by correlating gender information with specific\nobjects or scenarios. Current methods, which focus on modifying inputs and\nmonitoring changes in the model's output probability scores, often struggle to\ncomprehensively understand bias from the perspective of model components. We\npropose a framework that incorporates causal mediation analysis to measure and\nmap the pathways of bias generation and propagation within VLMs. This approach\nallows us to identify the direct effects of interventions on model bias and the\nindirect effects of interventions on bias mediated through different model\ncomponents. Our results show that image features are the primary contributors\nto bias, with significantly higher impacts than text features, specifically\naccounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE\ndatasets, respectively. Notably, the image encoder's contribution surpasses\nthat of the text encoder and the deep fusion encoder. Further experimentation\nconfirms that contributions from both language and vision modalities are\naligned and non-conflicting. Consequently, focusing on blurring gender\nrepresentations within the image encoder, which contributes most to the model\nbias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and\nPASCAL-SENTENCE datasets, respectively, with minimal performance loss or\nincreased computational demands.", "published": "2024-07-03 05:19:45", "link": "http://arxiv.org/abs/2407.02814v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.7"], "primary_category": "cs.AI"}
{"title": "LANE: Logic Alignment of Non-tuning Large Language Models and Online\n  Recommendation Systems for Explainable Reason Generation", "abstract": "The explainability of recommendation systems is crucial for enhancing user\ntrust and satisfaction. Leveraging large language models (LLMs) offers new\nopportunities for comprehensive recommendation logic generation. However, in\nexisting related studies, fine-tuning LLM models for recommendation tasks\nincurs high computational costs and alignment issues with existing systems,\nlimiting the application potential of proven proprietary/closed-source LLM\nmodels, such as GPT-4. In this work, our proposed effective strategy LANE\naligns LLMs with online recommendation systems without additional LLMs tuning,\nreducing costs and improving explainability. This innovative approach addresses\nkey challenges in integrating language models with recommendation systems while\nfully utilizing the capabilities of powerful proprietary models. Specifically,\nour strategy operates through several key components: semantic embedding, user\nmulti-preference extraction using zero-shot prompting, semantic alignment, and\nexplainable recommendation generation using Chain of Thought (CoT) prompting.\nBy embedding item titles instead of IDs and utilizing multi-head attention\nmechanisms, our approach aligns the semantic features of user preferences with\nthose of candidate items, ensuring coherent and user-aligned recommendations.\nSufficient experimental results including performance comparison, questionnaire\nvoting, and visualization cases prove that our method can not only ensure\nrecommendation performance, but also provide easy-to-understand and reasonable\nrecommendation logic.", "published": "2024-07-03 06:20:31", "link": "http://arxiv.org/abs/2407.02833v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "MindBench: A Comprehensive Benchmark for Mind Map Structure Recognition\n  and Analysis", "abstract": "Multimodal Large Language Models (MLLM) have made significant progress in the\nfield of document analysis. Despite this, existing benchmarks typically focus\nonly on extracting text and simple layout information, neglecting the complex\ninteractions between elements in structured documents such as mind maps and\nflowcharts. To address this issue, we introduce the new benchmark named\nMindBench, which not only includes meticulously constructed bilingual authentic\nor synthetic images, detailed annotations, evaluation metrics and baseline\nmodels, but also specifically designs five types of structured understanding\nand parsing tasks. These tasks include full parsing, partial parsing,\nposition-related parsing, structured Visual Question Answering (VQA), and\nposition-related VQA, covering key areas such as text recognition, spatial\nawareness, relationship discernment, and structured parsing. Extensive\nexperimental results demonstrate the substantial potential and significant room\nfor improvement in current models' ability to handle structured document\ninformation. We anticipate that the launch of MindBench will significantly\nadvance research and application development in structured document analysis\ntechnology. MindBench is available at:\nhttps://miasanlei.github.io/MindBench.github.io/.", "published": "2024-07-03 06:39:18", "link": "http://arxiv.org/abs/2407.02842v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to\n  Defend Against Jailbreak Attacks", "abstract": "LLMs are known to be vulnerable to jailbreak attacks, even after safety\nalignment. An important observation is that, while different types of jailbreak\nattacks can generate significantly different queries, they mostly result in\nsimilar responses that are rooted in the same harmful knowledge (e.g., detailed\nsteps to make a bomb). Therefore, we conjecture that directly unlearn the\nharmful knowledge in the LLM can be a more effective way to defend against\njailbreak attacks than the mainstream supervised fine-tuning (SFT) approaches.\nOur extensive experiments demonstrate the surprising generalizability of our\nunlearning-based approach: using only 20 raw harmful questions without any\njailbreak prompt during training, our solution reduced the Attack Success Rate\n(ASR) in Vicuna-7B from 82.6% to 7.7% on out-of-distribution (OOD) harmful\nquestions wrapped with various complex jailbreak prompts . This significantly\noutperforms Llama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment\nsamples but still has an ASR of 21.9% even under the help of an additional\nsafety system prompt. Further analysis reveals that the generalization ability\nof our solution may stem from the intrinsic relatedness among harmful responses\nacross harmful questions (e.g., response patterns, shared steps and actions in\nresponse, and similarity among their learned representations in the LLM). Our\ncode is available at \\url{https://github.com/thu-coai/SafeUnlearning}.", "published": "2024-07-03 07:14:05", "link": "http://arxiv.org/abs/2407.02855v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "CogErgLLM: Exploring Large Language Model Systems Design Perspective\n  Using Cognitive Ergonomics", "abstract": "Integrating cognitive ergonomics with LLMs is crucial for improving safety,\nreliability, and user satisfaction in human-AI interactions. Current LLM\ndesigns often lack this integration, resulting in systems that may not fully\nalign with human cognitive capabilities and limitations. This oversight\nexacerbates biases in LLM outputs and leads to suboptimal user experiences due\nto inconsistent application of user-centered design principles. Researchers are\nincreasingly leveraging NLP, particularly LLMs, to model and understand human\nbehavior across social sciences, psychology, psychiatry, health, and\nneuroscience. Our position paper explores the need to integrate cognitive\nergonomics into LLM design, providing a comprehensive framework and practical\nguidelines for ethical development. By addressing these challenges, we aim to\nadvance safer, more reliable, and ethically sound human-AI interactions.", "published": "2024-07-03 07:59:52", "link": "http://arxiv.org/abs/2407.02885v5", "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.HC"}
{"title": "GPTQT: Quantize Large Language Models Twice to Push the Efficiency", "abstract": "Due to their large size, generative Large Language Models (LLMs) require\nsignificant computing and storage resources. This paper introduces a new\npost-training quantization method, GPTQT, to reduce memory usage and enhance\nprocessing speed by expressing the weight of LLM in 3bit/2bit. Practice has\nshown that minimizing the quantization error of weights is ineffective, leading\nto overfitting. Therefore, GPTQT employs a progressive two-step approach:\ninitially quantizing weights using Linear quantization to a relatively high\nbit, followed by converting obtained int weight to lower bit binary coding. A\nre-explore strategy is proposed to optimize initial scaling factor. During\ninference, these steps are merged into pure binary coding, enabling efficient\ncomputation. Testing across various models and datasets confirms GPTQT's\neffectiveness. Compared to the strong 3-bit quantization baseline, GPTQT\nfurther reduces perplexity by 4.01 on opt-66B and increases speed by 1.24 times\non opt-30b. The results on Llama2 show that GPTQT is currently the best binary\ncoding quantization method for such kind of LLMs.", "published": "2024-07-03 08:08:01", "link": "http://arxiv.org/abs/2407.02891v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Probing the Feasibility of Multilingual Speaker Anonymization", "abstract": "In speaker anonymization, speech recordings are modified in a way that the\nidentity of the speaker remains hidden. While this technology could help to\nprotect the privacy of individuals around the globe, current research restricts\nthis by focusing almost exclusively on English data. In this study, we extend a\nstate-of-the-art anonymization system to nine languages by transforming\nlanguage-dependent components to their multilingual counterparts. Experiments\ntesting the robustness of the anonymized speech against privacy attacks and\nspeech deterioration show an overall success of this system for all languages.\nThe results suggest that speaker embeddings trained on English data can be\napplied across languages, and that the anonymization performance for a language\nis mainly affected by the quality of the speech synthesis component used for\nit.", "published": "2024-07-03 09:12:53", "link": "http://arxiv.org/abs/2407.02937v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PII-Compass: Guiding LLM training data extraction prompts towards the\n  target PII via grounding", "abstract": "The latest and most impactful advances in large models stem from their\nincreased size. Unfortunately, this translates into an improved memorization\ncapacity, raising data privacy concerns. Specifically, it has been shown that\nmodels can output personal identifiable information (PII) contained in their\ntraining data. However, reported PIII extraction performance varies widely, and\nthere is no consensus on the optimal methodology to evaluate this risk,\nresulting in underestimating realistic adversaries. In this work, we\nempirically demonstrate that it is possible to improve the extractability of\nPII by over ten-fold by grounding the prefix of the manually constructed\nextraction prompt with in-domain data. Our approach, PII-Compass, achieves\nphone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308\nqueries, respectively, i.e., the phone number of 1 person in 15 is extractable.", "published": "2024-07-03 09:20:04", "link": "http://arxiv.org/abs/2407.02943v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "IncogniText: Privacy-enhancing Conditional Text Anonymization via\n  LLM-based Private Attribute Randomization", "abstract": "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%\nacross 8 different private attributes. Finally, we demonstrate the maturity of\nIncogniText for real-world applications by distilling its anonymization\ncapability into a set of LoRA parameters associated with an on-device model.\nOur results show the possibility of reducing privacy leakage by more than half\nwith limited impact on utility.", "published": "2024-07-03 09:49:03", "link": "http://arxiv.org/abs/2407.02956v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary\n  LLMs on Private Datasets", "abstract": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.", "published": "2024-07-03 09:54:08", "link": "http://arxiv.org/abs/2407.02960v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Large Language Models as Evaluators for Scientific Synthesis", "abstract": "Our study explores how well the state-of-the-art Large Language Models\n(LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries\nor, more fittingly, scientific syntheses, comparing their evaluations to those\nof human annotators. We used a dataset of 100 research questions and their\nsyntheses made by GPT-4 from abstracts of five related papers, checked against\nhuman quality ratings. The study evaluates both the closed-source GPT-4 and the\nopen-source Mistral model's ability to rate these summaries and provide reasons\nfor their judgments. Preliminary results show that LLMs can offer logical\nexplanations that somewhat match the quality ratings, yet a deeper statistical\nanalysis shows a weak correlation between LLM and human ratings, suggesting the\npotential and current limitations of LLMs in scientific synthesis evaluation.", "published": "2024-07-03 10:21:27", "link": "http://arxiv.org/abs/2407.02977v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content\n  Moderation of Large Language Models", "abstract": "Guardrails have emerged as an alternative to safety alignment for content\nmoderation of large language models (LLMs). Existing model-based guardrails\nhave not been designed for resource-constrained computational portable devices,\nsuch as mobile phones, more and more of which are running LLM-based\napplications locally. We introduce LoRA-Guard, a parameter-efficient guardrail\nadaptation method that relies on knowledge sharing between LLMs and guardrail\nmodels. LoRA-Guard extracts language features from the LLMs and adapts them for\nthe content moderation task using low-rank adapters, while a dual-path design\nprevents any performance degradation on the generative task. We show that\nLoRA-Guard outperforms existing approaches with 100-1000x lower parameter\noverhead while maintaining accuracy, enabling on-device content moderation.", "published": "2024-07-03 10:38:40", "link": "http://arxiv.org/abs/2407.02987v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Human-like Linguistic Biases in Neural Speech Models: Phonetic\n  Categorization and Phonotactic Constraints in Wav2Vec2.0", "abstract": "What do deep neural speech models know about phonology? Existing work has\nexamined the encoding of individual linguistic units such as phonemes in these\nmodels. Here we investigate interactions between units. Inspired by classic\nexperiments on human speech perception, we study how Wav2Vec2 resolves\nphonotactic constraints. We synthesize sounds on an acoustic continuum between\n/l/ and /r/ and embed them in controlled contexts where only /l/, only /r/, or\nneither occur in English. Like humans, Wav2Vec2 models show a bias towards the\nphonotactically admissable category in processing such ambiguous sounds. Using\nsimple measures to analyze model internals on the level of individual stimuli,\nwe find that this bias emerges in early layers of the model's Transformer\nmodule. This effect is amplified by ASR finetuning but also present in fully\nself-supervised models. Our approach demonstrates how controlled stimulus\ndesigns can help localize specific linguistic knowledge in neural speech\nmodels.", "published": "2024-07-03 11:04:31", "link": "http://arxiv.org/abs/2407.03005v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Federated RLHF with Aggregated Client Preference for LLMs", "abstract": "Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained\nlarge language model (LLM) using user preference data, enabling it to generate\ncontent aligned with human preferences. However, due to privacy concerns, users\nmay be reluctant to share sensitive preference data. To address this, we\npropose utilizing Federated Learning (FL) techniques, allowing large-scale\npreference collection from diverse real-world users without requiring them to\ntransmit data to a central server. Our federated RLHF methods (i.e., FedBis and\nFedBiscuit) encode each client's preferences into binary selectors and\naggregate them to capture common preferences. In particular, FedBiscuit\novercomes key challenges, such as preference heterogeneity and reward hacking,\nthrough innovative solutions like grouping clients with similar preferences to\nreduce heterogeneity and using multiple binary selectors to enhance LLM output\nquality. To evaluate the performance of the proposed methods, we establish the\nfirst federated RLHF benchmark with a heterogeneous human preference dataset.\nExperimental results show that by integrating the LLM with aggregated client\npreferences, FedBis and FedBiscuit significantly enhance the professionalism\nand readability of the generated content.", "published": "2024-07-03 12:02:24", "link": "http://arxiv.org/abs/2407.03038v3", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts\n  Discovery from Large-Scale Human-LLM Conversational Datasets", "abstract": "Large Language Models (LLMs) have gained significant attention but also\nraised concerns due to the risk of misuse. Jailbreak prompts, a popular type of\nadversarial attack towards LLMs, have appeared and constantly evolved to breach\nthe safety protocols of LLMs. To address this issue, LLMs are regularly updated\nwith safety patches based on reported jailbreak prompts. However, malicious\nusers often keep their successful jailbreak prompts private to exploit LLMs. To\nuncover these private jailbreak prompts, extensive analysis of large-scale\nconversational datasets is necessary to identify prompts that still manage to\nbypass the system's defenses. This task is highly challenging due to the\nimmense volume of conversation data, diverse characteristics of jailbreak\nprompts, and their presence in complex multi-turn conversations. To tackle\nthese challenges, we introduce JailbreakHunter, a visual analytics approach for\nidentifying jailbreak prompts in large-scale human-LLM conversational datasets.\nWe have designed a workflow with three analysis levels: group-level,\nconversation-level, and turn-level. Group-level analysis enables users to grasp\nthe distribution of conversations and identify suspicious conversations using\nmultiple criteria, such as similarity with reported jailbreak prompts in\nprevious research and attack success rates. Conversation-level analysis\nfacilitates the understanding of the progress of conversations and helps\ndiscover jailbreak prompts within their conversation contexts. Turn-level\nanalysis allows users to explore the semantic similarity and token overlap\nbetween a singleturn prompt and the reported jailbreak prompts, aiding in the\nidentification of new jailbreak strategies. The effectiveness and usability of\nthe system were verified through multiple case studies and expert interviews.", "published": "2024-07-03 12:10:41", "link": "http://arxiv.org/abs/2407.03045v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "KeyVideoLLM: Towards Large-scale Video Keyframe Selection", "abstract": "Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets.", "published": "2024-07-03 13:41:44", "link": "http://arxiv.org/abs/2407.03104v3", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Let the Code LLM Edit Itself When You Edit the Code", "abstract": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.", "published": "2024-07-03 14:34:03", "link": "http://arxiv.org/abs/2407.03157v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "SOS! Soft Prompt Attack Against Open-Source Large Language Models", "abstract": "Open-source large language models (LLMs) have become increasingly popular\namong both the general public and industry, as they can be customized,\nfine-tuned, and freely used. However, some open-source LLMs require approval\nbefore usage, which has led to third parties publishing their own easily\naccessible versions. Similarly, third parties have been publishing fine-tuned\nor quantized variants of these LLMs. These versions are particularly appealing\nto users because of their ease of access and reduced computational resource\ndemands. This trend has increased the risk of training time attacks,\ncompromising the integrity and security of LLMs. In this work, we present a new\ntraining time attack, SOS, which is designed to be low in computational demand\nand does not require clean data or modification of the model weights, thereby\nmaintaining the model's utility intact. The attack addresses security issues in\nvarious scenarios, including the backdoor attack, jailbreak attack, and prompt\nstealing attack. Our experimental findings demonstrate that the proposed attack\nis effective across all evaluated targets. Furthermore, we present the other\nside of our SOS technique, namely the copyright token -- a novel technique that\nenables users to mark their copyrighted content and prevent models from using\nit.", "published": "2024-07-03 14:35:16", "link": "http://arxiv.org/abs/2407.03160v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Investigating Decoder-only Large Language Models for Speech-to-text\n  Translation", "abstract": "Large language models (LLMs), known for their exceptional reasoning\ncapabilities, generalizability, and fluency across diverse domains, present a\npromising avenue for enhancing speech-related tasks. In this paper, we focus on\nintegrating decoder-only LLMs to the task of speech-to-text translation (S2TT).\nWe propose a decoder-only architecture that enables the LLM to directly consume\nthe encoded speech representation and generate the text translation.\nAdditionally, we investigate the effects of different parameter-efficient\nfine-tuning techniques and task formulation. Our model achieves\nstate-of-the-art performance on CoVoST 2 and FLEURS among models trained\nwithout proprietary data. We also conduct analyses to validate the design\nchoices of our proposed model and bring insights to the integration of LLMs to\nS2TT.", "published": "2024-07-03 14:42:49", "link": "http://arxiv.org/abs/2407.03169v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and\n  Schema Pruning", "abstract": "We focus on Text-to-SQL semantic parsing from the perspective of\nretrieval-augmented generation. Motivated by challenges related to the size of\ncommercial database schemata and the deployability of business intelligence\nsolutions, we propose $\\text{ASTReS}$ that dynamically retrieves input database\ninformation and uses abstract syntax trees to select few-shot examples for\nin-context learning.\n  Furthermore, we investigate the extent to which an in-parallel semantic\nparser can be leveraged for generating approximated versions of the expected\nSQL queries, to support our retrieval. We take this approach to the extreme--we\nadapt a model consisting of less than $500$M parameters, to act as an extremely\nefficient approximator, enhancing it with the ability to process schemata in a\nparallelised manner. We apply $\\text{ASTReS}$ to monolingual and cross-lingual\nbenchmarks for semantic parsing, showing improvements over state-of-the-art\nbaselines. Comprehensive experiments highlight the contribution of modules\ninvolved in this retrieval-augmented generation setting, revealing interesting\ndirections for future work.", "published": "2024-07-03 15:55:14", "link": "http://arxiv.org/abs/2407.03227v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs", "abstract": "We introduce a defense against adversarial attacks on LLMs utilizing\nself-evaluation. Our method requires no model fine-tuning, instead using\npre-trained models to evaluate the inputs and outputs of a generator model,\nsignificantly reducing the cost of implementation in comparison to other,\nfinetuning-based methods. Our method can significantly reduce the attack\nsuccess rate of attacks on both open and closed-source LLMs, beyond the\nreductions demonstrated by Llama-Guard2 and commonly used content moderation\nAPIs. We present an analysis of the effectiveness of our method, including\nattempts to attack the evaluator in various settings, demonstrating that it is\nalso more resilient to attacks than existing methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.", "published": "2024-07-03 16:03:42", "link": "http://arxiv.org/abs/2407.03234v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs", "abstract": "Advancements in large Vision-Language Models have brought precise, accurate\nimage captioning, vital for advancing multi-modal image understanding and\nprocessing. Yet these captions often carry lengthy, intertwined contexts that\nare difficult to parse and frequently overlook essential cues, posing a great\nbarrier for models like GroundingDINO and SDXL, which lack the strong text\nencoding and syntax analysis needed to fully leverage dense captions. To\naddress this, we propose BACON, a prompting method that breaks down\nVLM-generated captions into disentangled, structured elements such as objects,\nrelationships, styles, and themes. This approach not only minimizes confusion\nfrom handling complex contexts but also allows for efficient transfer into a\nJSON dictionary, enabling models without linguistic processing capabilities to\neasily access key information. We annotated 100,000 image-caption pairs using\nBACON with GPT-4V and trained an LLaVA captioner on this dataset, enabling it\nto produce BACON-style captions without relying on costly GPT-4V. Evaluations\nof overall quality, precision, and recall-as well as user studies-demonstrate\nthat the resulting caption model consistently outperforms other SOTA VLM models\nin generating high-quality captions. Besides, we show that BACON-style captions\nexhibit better clarity when applied to various models, enabling them to\naccomplish previously unattainable tasks or surpass existing SOTA solutions\nwithout training. For example, BACON-style captions help GroundingDINO achieve\n1.51x higher recall scores on open-vocabulary object detection tasks compared\nto leading methods.", "published": "2024-07-03 17:55:27", "link": "http://arxiv.org/abs/2407.03314v2", "categories": ["cs.CV", "cs.CL", "cs.DB"], "primary_category": "cs.CV"}
{"title": "Planetarium: A Rigorous Benchmark for Translating Text to Structured\n  Planning Languages", "abstract": "Recent works have explored using language models for planning problems. One\napproach examines translating natural language descriptions of planning tasks\ninto structured planning languages, such as the planning domain definition\nlanguage (PDDL). Existing evaluation methods struggle to ensure semantic\ncorrectness and rely on simple or unrealistic datasets. To bridge this gap, we\nintroduce \\textit{Planetarium}, a benchmark designed to evaluate language\nmodels' ability to generate PDDL code from natural language descriptions of\nplanning tasks. \\textit{Planetarium} features a novel PDDL equivalence\nalgorithm that flexibly evaluates the correctness of generated PDDL, along with\na dataset of 145,918 text-to-PDDL pairs across 73 unique state combinations\nwith varying levels of difficulty. Finally, we evaluate several API-access and\nopen-weight language models that reveal this task's complexity. For example,\n96.1\\% of the PDDL problem descriptions generated by GPT-4o are syntactically\nparseable, 94.4\\% are solvable, but only 24.8\\% are semantically correct,\nhighlighting the need for a more rigorous benchmark for this problem.", "published": "2024-07-03 17:59:53", "link": "http://arxiv.org/abs/2407.03321v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ConCodeEval: Evaluating Large Language Models for Code Constraints in\n  Domain-Specific Languages", "abstract": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints.", "published": "2024-07-03 08:36:13", "link": "http://arxiv.org/abs/2407.03387v3", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Soft Begging: Modular and Efficient Shielding of LLMs against Prompt\n  Injection and Jailbreaking based on Prompt Tuning", "abstract": "Prompt injection (both direct and indirect) and jailbreaking are now\nrecognized as significant issues for large language models (LLMs), particularly\ndue to their potential for harm in application-integrated contexts. This\nextended abstract explores a novel approach to protecting LLMs from such\nattacks, termed \"soft begging.\" This method involves training soft prompts to\ncounteract the effects of corrupted prompts on the LLM's output. We provide an\noverview of prompt injections and jailbreaking, introduce the theoretical basis\nof the \"soft begging\" technique, and discuss an evaluation of its\neffectiveness.", "published": "2024-07-03 14:52:09", "link": "http://arxiv.org/abs/2407.03391v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "HEMM: Holistic Evaluation of Multimodal Foundation Models", "abstract": "Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models.", "published": "2024-07-03 18:00:48", "link": "http://arxiv.org/abs/2407.03418v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Exploring LGBTQ+ Bias in Generative AI Answers across Different Country\n  and Religious Contexts", "abstract": "Previous discussions have highlighted the need for generative AI tools to\nbecome more culturally sensitive, yet often neglect the complexities of\nhandling content about minorities, who are perceived differently across\ncultures and religions. Our study examined how two generative AI systems\nrespond to homophobic statements with varying cultural and religious context\ninformation. Findings showed ChatGPT 3.5's replies exhibited cultural\nrelativism, in contrast to Bard's, which stressed human rights and provided\nmore support for LGBTQ+ issues. Both demonstrated significant change in\nresponses based on contextual information provided in the prompts, suggesting\nthat AI systems may adjust in their responses the degree and forms of support\nfor LGBTQ+ people according to information they receive about the user's\nbackground. The study contributes to understanding the social and ethical\nimplications of AI responses and argues that any work to make generative AI\noutputs more culturally diverse requires a grounding in fundamental human\nrights.", "published": "2024-07-03 19:38:19", "link": "http://arxiv.org/abs/2407.03473v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Codec-ASR: Training Performant Automatic Speech Recognition Systems with\n  Discrete Speech Representations", "abstract": "Discrete speech representations have garnered recent attention for their\nefficacy in training transformer-based models for various speech-related tasks\nsuch as automatic speech recognition (ASR), translation, speaker verification,\nand joint speech-text foundational models. In this work, we present a\ncomprehensive analysis on building ASR systems with discrete codes. We\ninvestigate different methods for codec training such as quantization schemes\nand time-domain vs spectral feature encodings. We further explore ASR training\ntechniques aimed at enhancing performance, training efficiency, and noise\nrobustness. Drawing upon our findings, we introduce a codec ASR pipeline that\noutperforms Encodec at similar bit-rate. Remarkably, it also surpasses the\nstate-of-the-art results achieved by strong self-supervised models on the 143\nlanguages ML-SUPERB benchmark despite being smaller in size and pretrained on\nsignificantly less data.", "published": "2024-07-03 20:51:41", "link": "http://arxiv.org/abs/2407.03495v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "AgentInstruct: Toward Generative Teaching with Agentic Flows", "abstract": "Synthetic data is becoming increasingly important for accelerating the\ndevelopment of language models, both large and small. Despite several\nsuccessful use cases, researchers also raised concerns around model collapse\nand drawbacks of imitating other models. This discrepancy can be attributed to\nthe fact that synthetic data varies in quality and diversity. Effective use of\nsynthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data\nby powerful models to teach a new skill or behavior to another model, we refer\nto this setting as Generative Teaching. We introduce AgentInstruct, an\nextensible agentic framework for automatically creating large amounts of\ndiverse and high-quality synthetic data. AgentInstruct can create both the\nprompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post\ntraining dataset of 25M pairs to teach language models different skills, such\nas text editing, creative writing, tool usage, coding, reading comprehension,\netc. The dataset can be used for instruction tuning of any base model. We\npost-train Mistral-7b with the data. When comparing the resulting model Orca-3\nto Mistral-7b-Instruct (which uses the same base model), we observe significant\nimprovements across many benchmarks. For example, 40% improvement on AGIEval,\n19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and\n45% improvement on AlpacaEval. Additionally, it consistently outperforms other\nmodels such as LLAMA-8B-instruct and GPT-3.5-turbo.", "published": "2024-07-03 21:01:12", "link": "http://arxiv.org/abs/2407.03502v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "On Evaluating Explanation Utility for Human-AI Decision Making in NLP", "abstract": "Is explainability a false promise? This debate has emerged from the\ninsufficient evidence that explanations help people in situations they are\nintroduced for. More human-centered, application-grounded evaluations of\nexplanations are needed to settle this. Yet, with no established guidelines for\nsuch studies in NLP, researchers accustomed to standardized proxy evaluations\nmust discover appropriate measurements, tasks, datasets, and sensible models\nfor human-AI teams in their studies.\n  To aid with this, we first review existing metrics suitable for\napplication-grounded evaluation. We then establish criteria to select\nappropriate datasets, and using them, we find that only 4 out of over 50\ndatasets available for explainability research in NLP meet them. We then\ndemonstrate the importance of reassessing the state of the art to form and\nstudy human-AI teams: teaming people with models for certain tasks might only\nnow start to make sense, and for others, it remains unsound. Finally, we\npresent the exemplar studies of human-AI decision-making for one of the\nidentified tasks -- verifying the correctness of a legal claim given a\ncontract. Our results show that providing AI predictions, with or without\nexplanations, does not cause decision makers to speed up their work without\ncompromising performance. We argue for revisiting the setup of human-AI teams\nand improving automatic deferral of instances to AI, where explanations could\nplay a useful role.", "published": "2024-07-03 23:53:27", "link": "http://arxiv.org/abs/2407.03545v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMs", "abstract": "In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach\nfor vertically scaling transformer-based Large Language Models (LLMs) by\ndynamically expanding, activating, or skipping layers using a sophisticated\nrouting policy based on layerwise feature similarity. Unlike traditional\nMixture-of-Experts (MoE) methods that focus on extending the model width, our\napproach targets model depth, addressing the redundancy observed across layer\nrepresentations for various input samples. Our framework is integrated with the\nSupervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive\nContinual Pre-Training (CPT). Experimental results demonstrate that DLO not\nonly outperforms the original unscaled models but also achieves comparable\nresults to densely expanded models with significantly improved efficiency. Our\nwork offers a promising direction for building efficient yet powerful LLMs. We\nwill release our implementation and model weights upon acceptance.", "published": "2024-07-03 18:34:08", "link": "http://arxiv.org/abs/2407.11030v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Knowledge-based Consistency Testing of Large Language Models", "abstract": "In this work, we systematically expose and measure the inconsistency and\nknowledge gaps of Large Language Models (LLMs). Specifically, we propose an\nautomated testing framework (called KonTest) which leverages a knowledge graph\nto construct test cases. KonTest probes and measures the inconsistencies in the\nLLM's knowledge of the world via a combination of semantically-equivalent\nqueries and test oracles (metamorphic or ontological oracle). KonTest further\nmitigates knowledge gaps via a weighted LLM model ensemble. Using four\nstate-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that\nKonTest generates 19.2% error inducing inputs (1917 errors from 9979 test\ninputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A\nmitigation method informed by KonTest's test suite reduces LLM knowledge gap by\n32.48%. Our ablation study further shows that GPT3.5 is not suitable for\nknowledge-based consistency testing because it is only 60%-68% effective in\nknowledge construction.", "published": "2024-07-03 11:16:54", "link": "http://arxiv.org/abs/2407.12830v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Regurgitative Training: The Value of Real Data in Training Large\n  Language Models", "abstract": "What happens if we train a new Large Language Model (LLM) using data that are\nat least partially generated by other LLMs? The explosive success of LLMs means\nthat a substantial amount of content online will be generated by LLMs rather\nthan humans, which will inevitably enter the training datasets of\nnext-generation LLMs. We evaluate the implications of such \"regurgitative\ntraining\" on LLM performance. Through fine-tuning GPT-3.5 with data generated\neither by itself or by other LLMs in a machine translation task, we find strong\nevidence that regurgitative training clearly handicaps the performance of LLMs.\nThe same performance loss of regurgitative training is observed on transformer\nmodels that we train from scratch. We find suggestive evidence that the\nperformance disadvantage of regurgitative training can be attributed to at\nleast two mechanisms: (1) higher error rates and (2) lower lexical diversity in\nLLM-generated data as compared to real data. Based on these mechanisms, we\npropose and evaluate three different strategies to mitigate the performance\nloss of regurgitative training. First, we devise data-driven metrics to gauge\nthe quality of each LLM-generated data instance, and then carry out an ordered\ntraining process where high-quality data are added before low-quality ones.\nSecond, we combine data generated by multiple different LLMs (as an attempt to\nincrease lexical diversity). Third, we train an AI detection classifier to\ndifferentiate between LLM- and human-generated data, and include LLM-generated\ndata in the order of resemblance to human-generated data. All three strategies\ncan improve the performance of regurgitative training to some extent but are\nnot always able to fully close the gap from training with real data. Our\nresults highlight the value of real, human-generated data in training LLMs,\nwhich cannot be easily substituted by synthetic, LLM-generated data.", "published": "2024-07-03 18:42:55", "link": "http://arxiv.org/abs/2407.12835v2", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "OSPC: Artificial VLM Features for Hateful Meme Detection", "abstract": "The digital revolution and the advent of the world wide web have transformed\nhuman communication, notably through the emergence of memes. While memes are a\npopular and straightforward form of expression, they can also be used to spread\nmisinformation and hate due to their anonymity and ease of use. In response to\nthese challenges, this paper introduces a solution developed by team 'Baseline'\nfor the AI Singapore Online Safety Prize Challenge. Focusing on computational\nefficiency and feature engineering, the solution achieved an AUROC of 0.76 and\nan accuracy of 0.69 on the test dataset. As key features, the solution\nleverages the inherent probabilistic capabilities of large Vision-Language\nModels (VLMs) to generate task-adapted feature encodings from text, and applies\na distilled quantization tailored to the specific cultural nuances present in\nSingapore. This type of processing and fine-tuning can be adapted to various\nvisual and textual understanding and classification tasks, and even applied on\nprivate VLMs such as OpenAI's GPT. Finally it can eliminate the need for\nextensive model training on large GPUs for resource constrained applications,\nalso offering a solution when little or no data is available.", "published": "2024-07-03 21:35:52", "link": "http://arxiv.org/abs/2407.12836v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RDBE: Reasoning Distillation-Based Evaluation Enhances Automatic Essay\n  Scoring", "abstract": "Recently, various encoder-only and encoder-decoder pre-trained models like\nBERT and T5 have been applied to automatic essay scoring (AES) as small\nlanguage models. However, existing studies have primarily treated this task\nakin to a classification problem, focusing solely on outputting scores in the\ntarget text without offering interpretations for the generated scores.\nDeparting from the approaches, we introduce Reasoning Distillation-Based\nEvaluation (RDBE), which integrates interpretability to elucidate the rationale\nbehind model scores while enhancing performance through initial reasoning. This\ninterpretive capability is acquired during training by leveraging generated\nreasoning from a large language model (LLM) to distill a small language model\n(SLM). Our experimental results demonstrate the efficacy of RDBE across all\nscoring rubrics considered in the dataset. RDBE outperforms both zero-shot LLM\ngeneration and generation from a baseline fine-tuned model, establishing itself\nas state-of-the-art in the corresponding dataset. This highlights its practical\ninterpretative output and enhanced performance.", "published": "2024-07-03 05:49:01", "link": "http://arxiv.org/abs/2407.13781v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker- and Text-Independent Estimation of Articulatory Movements and\n  Phoneme Alignments from Speech", "abstract": "This paper introduces a novel combination of two tasks, previously treated\nseparately: acoustic-to-articulatory speech inversion (AAI) and\nphoneme-to-articulatory (PTA) motion estimation. We refer to this joint task as\nacoustic phoneme-to-articulatory speech inversion (APTAI) and explore two\ndifferent approaches, both working speaker- and text-independently during\ninference. We use a multi-task learning setup, with the end-to-end goal of\ntaking raw speech as input and estimating the corresponding articulatory\nmovements, phoneme sequence, and phoneme alignment. While both proposed\napproaches share these same requirements, they differ in their way of achieving\nphoneme-related predictions: one is based on frame classification, the other on\na two-staged training procedure and forced alignment. We reach competitive\nperformance of 0.73 mean correlation for the AAI task and achieve up to\napproximately 87% frame overlap compared to a state-of-the-art text-dependent\nphoneme force aligner.", "published": "2024-07-03 14:13:04", "link": "http://arxiv.org/abs/2407.03132v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prosody-Driven Privacy-Preserving Dementia Detection", "abstract": "Speaker embeddings extracted from voice recordings have been proven valuable\nfor dementia detection. However, by their nature, these embeddings contain\nidentifiable information which raises privacy concerns. In this work, we aim to\nanonymize embeddings while preserving the diagnostic utility for dementia\ndetection. Previous studies rely on adversarial learning and models trained on\nthe target attribute and struggle in limited-resource settings. We propose a\nnovel approach that leverages domain knowledge to disentangle prosody features\nrelevant to dementia from speaker embeddings without relying on a dementia\nclassifier. Our experiments show the effectiveness of our approach in\npreserving speaker privacy (speaker recognition F1-score .01%) while\nmaintaining high dementia detection score F1-score of 74% on the ADReSS\ndataset. Our results are also on par with a more constrained\nclassifier-dependent system on ADReSSo (.01% and .66%), and have no impact on\nsynthesized speech naturalness.", "published": "2024-07-03 19:34:47", "link": "http://arxiv.org/abs/2407.03470v1", "categories": ["cs.SD", "cs.CL", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AMA-LSTM: Pioneering Robust and Fair Financial Audio Analysis for Stock\n  Volatility Prediction", "abstract": "Stock volatility prediction is an important task in the financial industry.\nRecent advancements in multimodal methodologies, which integrate both textual\nand auditory data, have demonstrated significant improvements in this domain,\nsuch as earnings calls (Earnings calls are public available and often involve\nthe management team of a public company and interested parties to discuss the\ncompany's earnings). However, these multimodal methods have faced two\ndrawbacks. First, they often fail to yield reliable models and overfit the data\ndue to their absorption of stochastic information from the stock market.\nMoreover, using multimodal models to predict stock volatility suffers from\ngender bias and lacks an efficient way to eliminate such bias. To address these\naforementioned problems, we use adversarial training to generate perturbations\nthat simulate the inherent stochasticity and bias, by creating areas resistant\nto random information around the input space to improve model robustness and\nfairness. Our comprehensive experiments on two real-world financial audio\ndatasets reveal that this method exceeds the performance of current\nstate-of-the-art solution. This confirms the value of adversarial training in\nreducing stochasticity and bias for stock volatility prediction tasks.", "published": "2024-07-03 18:40:53", "link": "http://arxiv.org/abs/2407.18324v1", "categories": ["cs.LG", "cs.CL", "eess.AS", "q-fin.CP", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "SA-WavLM: Speaker-Aware Self-Supervised Pre-training for Mixture Speech", "abstract": "It was shown that pre-trained models with self-supervised learning (SSL)\ntechniques are effective in various downstream speech tasks. However, most such\nmodels are trained on single-speaker speech data, limiting their effectiveness\nin mixture speech. This motivates us to explore pre-training on mixture speech.\nThis work presents SA-WavLM, a novel pre-trained model for mixture speech.\nSpecifically, SA-WavLM follows an \"extract-merge-predict\" pipeline in which the\nrepresentations of each speaker in the input mixture are first extracted\nindividually and then merged before the final prediction. In this pipeline,\nSA-WavLM performs speaker-informed extractions with the consideration of the\ninteractions between different speakers. Furthermore, a speaker shuffling\nstrategy is proposed to enhance the robustness towards the speaker absence.\nExperiments show that SA-WavLM either matches or improves upon the\nstate-of-the-art pre-trained models.", "published": "2024-07-03 06:07:42", "link": "http://arxiv.org/abs/2407.02826v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "VAE-based Phoneme Alignment Using Gradient Annealing and SSL Acoustic\n  Features", "abstract": "This paper presents an accurate phoneme alignment model that aims for speech\nanalysis and video content creation. We propose a variational autoencoder\n(VAE)-based alignment model in which a probable path is searched using encoded\nacoustic and linguistic embeddings in an unsupervised manner. Our proposed\nmodel is based on one TTS alignment (OTA) and extended to obtain phoneme\nboundaries. Specifically, we incorporate a VAE architecture to maintain\nconsistency between the embedding and input, apply gradient annealing to avoid\nlocal optimum during training, and introduce a self-supervised learning\n(SSL)-based acoustic-feature input and state-level linguistic unit to utilize\nrich and detailed information. Experimental results show that the proposed\nmodel generated phoneme boundaries closer to annotated ones compared with the\nconventional OTA model, the CTC-based segmentation model, and the widely-used\ntool MFA.", "published": "2024-07-03 01:51:24", "link": "http://arxiv.org/abs/2407.02749v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioTime: A Temporally-aligned Audio-text Benchmark Dataset", "abstract": "Recent advancements in audio generation have enabled the creation of\nhigh-fidelity audio clips from free-form textual descriptions. However,\ntemporal relationships, a critical feature for audio content, are currently\nunderrepresented in mainstream models, resulting in an imprecise temporal\ncontrollability. Specifically, users cannot accurately control the timestamps\nof sound events using free-form text. We acknowledge that a significant factor\nis the absence of high-quality, temporally-aligned audio-text datasets, which\nare essential for training models with temporal control. The more\ntemporally-aligned the annotations, the better the models can understand the\nprecise relationship between audio outputs and temporal textual prompts.\nTherefore, we present a strongly aligned audio-text dataset, AudioTime. It\nprovides text annotations rich in temporal information such as timestamps,\nduration, frequency, and ordering, covering almost all aspects of temporal\ncontrol. Additionally, we offer a comprehensive test set and evaluation metric\nto assess the temporal control performance of various models. Examples are\navailable on the https://zeyuxie29.github.io/AudioTime/", "published": "2024-07-03 07:15:04", "link": "http://arxiv.org/abs/2407.02857v1", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "primary_category": "cs.SD"}
{"title": "PicoAudio: Enabling Precise Timestamp and Frequency Controllability of\n  Audio Events in Text-to-audio Generation", "abstract": "Recently, audio generation tasks have attracted considerable research\ninterests. Precise temporal controllability is essential to integrate audio\ngeneration with real applications. In this work, we propose a temporal\ncontrolled audio generation framework, PicoAudio. PicoAudio integrates temporal\ninformation to guide audio generation through tailored model design. It\nleverages data crawling, segmentation, filtering, and simulation of\nfine-grained temporally-aligned audio-text data. Both subjective and objective\nevaluations demonstrate that PicoAudio dramantically surpasses current\nstate-of-the-art generation models in terms of timestamp and occurrence\nfrequency controllability. The generated samples are available on the demo\nwebsite https://zeyuxie29.github.io/PicoAudio.github.io.", "published": "2024-07-03 07:33:14", "link": "http://arxiv.org/abs/2407.02869v2", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "primary_category": "cs.SD"}
{"title": "Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End\n  Multi-Accent Speech Recognition", "abstract": "Currently, end-to-end (E2E) speech recognition methods have achieved\npromising performance. However, auto speech recognition (ASR) models still face\nchallenges in recognizing multi-accent speech accurately. We propose a\nlayer-adapted fusion (LAF) model, called Qifusion-Net, which does not require\nany prior knowledge about the target accent. Based on dynamic chunk strategy,\nour approach enables streaming decoding and can extract frame-level acoustic\nfeature, facilitating fine-grained information fusion. Experiment results\ndemonstrate that our proposed methods outperform the baseline with relative\nreductions of 22.1$\\%$ and 17.2$\\%$ in character error rate (CER) across multi\naccent test datasets on KeSpeech and MagicData-RMAC.", "published": "2024-07-03 11:35:52", "link": "http://arxiv.org/abs/2407.03026v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GMM-ResNext: Combining Generative and Discriminative Models for Speaker\n  Verification", "abstract": "With the development of deep learning, many different network architectures\nhave been explored in speaker verification. However, most network architectures\nrely on a single deep learning architecture, and hybrid networks combining\ndifferent architectures have been little studied in ASV tasks. In this paper,\nwe propose the GMM-ResNext model for speaker verification. Conventional GMM\ndoes not consider the score distribution of each frame feature over all\nGaussian components and ignores the relationship between neighboring speech\nframes. So, we extract the log Gaussian probability features based on the raw\nacoustic features and use ResNext-based network as the backbone to extract the\nspeaker embedding. GMM-ResNext combines Generative and Discriminative Models to\nimprove the generalization ability of deep learning models and allows one to\nmore easily specify meaningful priors on model parameters. A two-path\nGMM-ResNext model based on two gender-related GMMs has also been proposed. The\nExperimental results show that the proposed GMM-ResNext achieves relative\nimprovements of 48.1\\% and 11.3\\% in EER compared with ResNet34 and ECAPA-TDNN\non VoxCeleb1-O test set.", "published": "2024-07-03 14:14:18", "link": "http://arxiv.org/abs/2407.03135v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MuDiT & MuSiT: Alignment with Colloquial Expression in\n  Description-to-Song Generation", "abstract": "Amid the rising intersection of generative AI and human artistic processes,\nthis study probes the critical yet less-explored terrain of alignment in\nhuman-centric automatic song composition. We propose a novel task of Colloquial\nDescription-to-Song Generation, which focuses on aligning the generated content\nwith colloquial human expressions. This task is aimed at bridging the gap\nbetween colloquial language understanding and auditory expression within an AI\nmodel, with the ultimate goal of creating songs that accurately satisfy human\nauditory expectations and structurally align with musical norms. Current\ndatasets are limited due to their narrow descriptive scope, semantic gaps and\ninaccuracies. To overcome data scarcity in this domain, we present the Caichong\nMusic Dataset (CaiMD). CaiMD is manually annotated by both professional\nmusicians and amateurs, offering diverse perspectives and a comprehensive\nunderstanding of colloquial descriptions. Unlike existing datasets pre-set with\nexpert annotations or auto-generated ones with inherent biases, CaiMD caters\nmore sufficiently to our purpose of aligning AI-generated music with widespread\nuser-desired results. Moreover, we propose an innovative single-stage framework\ncalled MuDiT/MuSiT for enabling effective human-machine alignment in song\ncreation. This framework not only achieves cross-modal comprehension between\ncolloquial language and auditory music perceptions but also ensures generated\nsongs align with user-desired results. MuDiT/MuSiT employs one DiT/SiT model\nfor end-to-end generation of musical components like melody, harmony, rhythm,\nvocals, and instrumentation. The approach ensures harmonious sonic cohesiveness\namongst all generated musical components, facilitating better resonance with\nhuman auditory expectations.", "published": "2024-07-03 15:12:36", "link": "http://arxiv.org/abs/2407.03188v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68Txx(Primary)14F05, 91Fxx(Secondary)", "I.2.7; J.5"], "primary_category": "cs.SD"}
{"title": "Advanced Framework for Animal Sound Classification With Features\n  Optimization", "abstract": "The automatic classification of animal sounds presents an enduring challenge\nin bioacoustics, owing to the diverse statistical properties of sound signals,\nvariations in recording equipment, and prevalent low Signal-to-Noise Ratio\n(SNR) conditions. Deep learning models like Convolutional Neural Networks (CNN)\nand Long Short-Term Memory (LSTM) have excelled in human speech recognition but\nhave not been effectively tailored to the intricate nature of animal sounds,\nwhich exhibit substantial diversity even within the same domain. We propose an\nautomated classification framework applicable to general animal sound\nclassification. Our approach first optimizes audio features from Mel-frequency\ncepstral coefficients (MFCC) including feature rearrangement and feature\nreduction. It then uses the optimized features for the deep learning model,\ni.e., an attention-based Bidirectional LSTM (Bi-LSTM), to extract deep semantic\nfeatures for sound classification. We also contribute an animal sound benchmark\ndataset encompassing oceanic animals and birds1. Extensive experimentation with\nreal-world datasets demonstrates that our approach consistently outperforms\nbaseline methods by over 25% in precision, recall, and accuracy, promising\nadvancements in animal sound classification.", "published": "2024-07-03 18:33:47", "link": "http://arxiv.org/abs/2407.03440v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Attention-based Contrastive Learning for Audio Spoof Detection", "abstract": "Vision transformers (ViT) have made substantial progress for classification\ntasks in computer vision. Recently, Gong et. al. '21, introduced\nattention-based modeling for several audio tasks. However, relatively\nunexplored is the use of a ViT for audio spoof detection task. We bridge this\ngap and introduce ViTs for this task. A vanilla baseline built on fine-tuning\nthe SSAST (Gong et. al. '22) audio ViT model achieves sub-optimal equal error\nrates (EERs). To improve performance, we propose a novel attention-based\ncontrastive learning framework (SSAST-CL) that uses cross-attention to aid the\nrepresentation learning. Experiments show that our framework successfully\ndisentangles the bonafide and spoof classes and helps learn better classifiers\nfor the task. With appropriate data augmentations policy, a model trained on\nour framework achieves competitive performance on the ASVSpoof 2021 challenge.\nWe provide comparisons and ablation studies to justify our claim.", "published": "2024-07-03 21:25:12", "link": "http://arxiv.org/abs/2407.03514v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised ASR Models and Features For Dysarthric and Elderly\n  Speech Recognition", "abstract": "Self-supervised learning (SSL) based speech foundation models have been\napplied to a wide range of ASR tasks. However, their application to dysarthric\nand elderly speech via data-intensive parameter fine-tuning is confronted by\nin-domain data scarcity and mismatch. To this end, this paper explores a series\nof approaches to integrate domain fine-tuned SSL pre-trained models and their\nfeatures into TDNN and Conformer ASR systems for dysarthric and elderly speech\nrecognition. These include: a) input feature fusion between standard acoustic\nfrontends and domain fine-tuned SSL speech representations; b) frame-level\njoint decoding between TDNN systems separately trained using standard acoustic\nfeatures alone and those with additional domain fine-tuned SSL features; and c)\nmulti-pass decoding involving the TDNN/Conformer system outputs to be rescored\nusing domain fine-tuned pre-trained ASR models. In addition, fine-tuned SSL\nspeech features are used in acoustic-to-articulatory (A2A) inversion to\nconstruct multi-modal ASR systems. Experiments are conducted on four tasks: the\nEnglish UASpeech and TORGO dysarthric speech corpora; and the English\nDementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets. The TDNN\nsystems constructed by integrating domain-adapted HuBERT, wav2vec2-conformer or\nmulti-lingual XLSR models and their features consistently outperform the\nstandalone fine-tuned SSL pre-trained models. These systems produced\nstatistically significant WER or CER reductions of 6.53%, 1.90%, 2.04% and\n7.97% absolute (24.10%, 23.84%, 10.14% and 31.39% relative) on the four tasks\nrespectively. Consistent improvements in Alzheimer's Disease detection accuracy\nare also obtained using the DementiaBank Pitt elderly speech recognition\noutputs.", "published": "2024-07-03 08:33:39", "link": "http://arxiv.org/abs/2407.13782v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Framework for AI assisted Musical Devices", "abstract": "In this paper we present a novel framework for the study and design of AI\nassisted musical devices (AIMEs). Initially, we present a taxonomy of these\ndevices and illustrate it with a set of scenarios and personas. Later, we\npropose a generic architecture for the implementation of AIMEs and present some\nexamples from the scenarios. We show that the proposed framework and\narchitecture are a valid tool for the study of intelligent musical devices.", "published": "2024-07-03 17:52:25", "link": "http://arxiv.org/abs/2407.16899v2", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
