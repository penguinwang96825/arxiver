{"title": "Accelerating Neural Transformer via an Average Attention Network", "abstract": "With parallelizable attention networks, the neural Transformer is very fast\nto train. However, due to the auto-regressive architecture and self-attention\nin the decoder, the decoding procedure becomes slow. To alleviate this issue,\nwe propose an average attention network as an alternative to the self-attention\nnetwork in the decoder of the neural Transformer. The average attention network\nconsists of two layers, with an average layer that models dependencies on\nprevious positions and a gating layer that is stacked over the average layer to\nenhance the expressiveness of the proposed attention network. We apply this\nnetwork on the decoder part of the neural Transformer to replace the original\ntarget-side self-attention model. With masking tricks and dynamic programming,\nour model enables the neural Transformer to decode sentences over four times\nfaster than its original version with almost no loss in training time and\ntranslation performance. We conduct a series of experiments on WMT17\ntranslation tasks, where on 6 different language pairs, we obtain robust and\nconsistent speed-ups in decoding.", "published": "2018-05-02 05:25:43", "link": "http://arxiv.org/abs/1805.00631v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Emoji Usage and Prediction Through a Temporal Variation Lens", "abstract": "The frequent use of Emojis on social media platforms has created a new form\nof multimodal social interaction. Developing methods for the study and\nrepresentation of emoji semantics helps to improve future multimodal\ncommunication systems. In this paper, we explore the usage and semantics of\nemojis over time. We compare emoji embeddings trained on a corpus of different\nseasons and show that some emojis are used differently depending on the time of\nthe year. Moreover, we propose a method to take into account the time\ninformation for emoji prediction systems, outperforming state-of-the-art\nsystems. We show that, using the time information, the accuracy of some emojis\ncan be significantly improved.", "published": "2018-05-02 11:03:52", "link": "http://arxiv.org/abs/1805.00731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect Term Extraction with History Attention and Selective\n  Transformation", "abstract": "Aspect Term Extraction (ATE), a key sub-task in Aspect-Based Sentiment\nAnalysis, aims to extract explicit aspect expressions from online user reviews.\nWe present a new framework for tackling ATE. It can exploit two useful clues,\nnamely opinion summary and aspect detection history. Opinion summary is\ndistilled from the whole input sentence, conditioned on each current token for\naspect prediction, and thus the tailor-made summary can help aspect prediction\non this token. Another clue is the information of aspect detection history, and\nit is distilled from the previous aspect predictions so as to leverage the\ncoordinate structure and tagging schema constraints to upgrade the aspect\nprediction. Experimental results over four benchmark datasets clearly\ndemonstrate that our framework can outperform all state-of-the-art methods.", "published": "2018-05-02 12:14:11", "link": "http://arxiv.org/abs/1805.00760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Cross-Lingual Information Retrieval using Monolingual Data\n  Only", "abstract": "We propose a fully unsupervised framework for ad-hoc cross-lingual\ninformation retrieval (CLIR) which requires no bilingual data at all. The\nframework leverages shared cross-lingual word embedding spaces in which terms,\nqueries, and documents can be represented, irrespective of their actual\nlanguage. The shared embedding spaces are induced solely on the basis of\nmonolingual corpora in two languages through an iterative process based on\nadversarial neural networks. Our experiments on the standard CLEF CLIR\ncollections for three language pairs of varying degrees of language similarity\n(English-Dutch/Italian/Finnish) demonstrate the usefulness of the proposed\nfully unsupervised approach. Our CLIR models with unsupervised cross-lingual\nembeddings outperform baselines that utilize cross-lingual embeddings induced\nrelying on word-level and document-level alignments. We then demonstrate that\nfurther improvements can be achieved by unsupervised ensemble CLIR models. We\nbelieve that the proposed framework is the first step towards development of\neffective CLIR models for language pairs and domains where parallel data are\nscarce or non-existent.", "published": "2018-05-02 15:52:48", "link": "http://arxiv.org/abs/1805.00879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Split and Rephrase: Better Evaluation and a Stronger Baseline", "abstract": "Splitting and rephrasing a complex sentence into several shorter sentences\nthat convey the same meaning is a challenging problem in NLP. We show that\nwhile vanilla seq2seq models can reach high scores on the proposed benchmark\n(Narayan et al., 2017), they suffer from memorization of the training set which\ncontains more than 89% of the unique simple sentences from the validation and\ntest sets. To aid this, we present a new train-development-test data split and\nneural models augmented with a copy-mechanism, outperforming the best reported\nbaseline by 8.68 BLEU and fostering further progress on the task.", "published": "2018-05-02 21:36:38", "link": "http://arxiv.org/abs/1805.01035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hypothesis Only Baselines in Natural Language Inference", "abstract": "We propose a hypothesis only baseline for diagnosing Natural Language\nInference (NLI). Especially when an NLI dataset assumes inference is occurring\nbased purely on the relationship between a context and a hypothesis, it follows\nthat assessing entailment relations while ignoring the provided context is a\ndegenerate solution. Yet, through experiments on ten distinct NLI datasets, we\nfind that this approach, which we refer to as a hypothesis-only model, is able\nto significantly outperform a majority class baseline across a number of NLI\ndatasets. Our analysis suggests that statistical irregularities may allow a\nmodel to perform NLI in some datasets beyond what should be achievable without\naccess to the context.", "published": "2018-05-02 22:16:21", "link": "http://arxiv.org/abs/1805.01042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constituency Parsing with a Self-Attentive Encoder", "abstract": "We demonstrate that replacing an LSTM encoder with a self-attentive\narchitecture can lead to improvements to a state-of-the-art discriminative\nconstituency parser. The use of attention makes explicit the manner in which\ninformation is propagated between different locations in the sentence, which we\nuse to both analyze our model and propose potential improvements. For example,\nwe find that separating positional and content information in the encoder can\nlead to improved parsing accuracy. Additionally, we evaluate different\napproaches for lexical representation. Our parser achieves new state-of-the-art\nresults for single models trained on the Penn Treebank: 93.55 F1 without the\nuse of any external data, and 95.13 F1 when using pre-trained word\nrepresentations. Our parser also outperforms the previous best-published\naccuracy figures on 8 of the 9 languages in the SPMRL dataset.", "published": "2018-05-02 23:21:12", "link": "http://arxiv.org/abs/1805.01052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Coding for Neonatal Jaundice From Free Text Data Using\n  Ensemble Methods", "abstract": "This study explores the creation of a machine learning model to automatically\nidentify whether a Neonatal Intensive Care Unit (NICU) patient was diagnosed\nwith neonatal jaundice during a particular hospitalization based on their\nassociated clinical notes. We develop a number of techniques for text\npreprocessing and feature selection and compare the effectiveness of different\nclassification models. We show that using ensemble decision tree\nclassification, both with AdaBoost and with bagging, outperforms support vector\nmachines (SVM), the current state-of-the-art technique for neonatal jaundice\ncoding.", "published": "2018-05-02 23:29:19", "link": "http://arxiv.org/abs/1805.01054v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Text to Image Synthesis Using Generative Adversarial Networks", "abstract": "Generating images from natural language is one of the primary applications of\nrecent conditional generative models. Besides testing our ability to model\nconditional, highly dimensional distributions, text to image synthesis has many\nexciting and practical applications such as photo editing or computer-aided\ncontent creation. Recent progress has been made using Generative Adversarial\nNetworks (GANs). This material starts with a gentle introduction to these\ntopics and discusses the existent state of the art models. Moreover, I propose\nWasserstein GAN-CLS, a new model for conditional image generation based on the\nWasserstein distance which offers guarantees of stability. Then, I show how the\nnovel loss function of Wasserstein GAN-CLS can be used in a Conditional\nProgressive Growing GAN. In combination with the proposed loss, the model\nboosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the\nmodels which use only the sentence-level visual semantics. The only model which\nperforms better than the Conditional Wasserstein Progressive Growing GAN is the\nrecently proposed AttnGAN which uses word-level visual semantics as well.", "published": "2018-05-02 08:47:38", "link": "http://arxiv.org/abs/1805.00676v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "KNPTC: Knowledge and Neural Machine Translation Powered Chinese Pinyin\n  Typo Correction", "abstract": "Chinese pinyin input methods are very important for Chinese language\nprocessing. Actually, users may make typos inevitably when they input pinyin.\nMoreover, pinyin typo correction has become an increasingly important task with\nthe popularity of smartphones and the mobile Internet. How to exploit the\nknowledge of users typing behaviors and support the typo correction for acronym\npinyin remains a challenging problem. To tackle these challenges, we propose\nKNPTC, a novel approach based on neural machine translation (NMT). In contrast\nto previous work, KNPTC is able to integrate explicit knowledge into NMT for\npinyin typo correction, and is able to learn to correct a variety of typos\nwithout the guidance of manually selected constraints or languagespecific\nfeatures. In this approach, we first obtain the transition probabilities\nbetween adjacent letters based on large-scale real-life datasets. Then, we\nconstruct the \"ground-truth\" alignments of training sentence pairs by utilizing\nthese probabilities. Furthermore, these alignments are integrated into NMT to\ncapture sensible pinyin typo correction patterns. KNPTC is applied to correct\ntypos in real-life datasets, which achieves 32.77% increment on average in\naccuracy rate of typo correction compared against the state-of-the-art system.", "published": "2018-05-02 11:33:45", "link": "http://arxiv.org/abs/1805.00741v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tensorized Self-Attention: Efficiently Modeling Pairwise and Global\n  Dependencies Together", "abstract": "Neural networks equipped with self-attention have parallelizable computation,\nlight-weight structure, and the ability to capture both long-range and local\ndependencies. Further, their expressive power and performance can be boosted by\nusing a vector to measure pairwise dependency, but this requires to expand the\nalignment matrix to a tensor, which results in memory and computation\nbottlenecks. In this paper, we propose a novel attention mechanism called\n\"Multi-mask Tensorized Self-Attention\" (MTSA), which is as fast and as\nmemory-efficient as a CNN, but significantly outperforms previous\nCNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token)\nand global (source2token) dependencies by a novel compatibility function\ncomposed of dot-product and additive attentions, 2) uses a tensor to represent\nthe feature-wise alignment scores for better expressive power but only requires\nparallelizable matrix multiplications, and 3) combines multi-head with\nmulti-dimensional attentions, and applies a distinct positional mask to each\nhead (subspace), so the memory and computation can be distributed to multiple\nheads, each with sequential information encoded independently. The experiments\nshow that a CNN/RNN-free model based on MTSA achieves state-of-the-art or\ncompetitive performance on nine NLP benchmarks with compelling memory- and\ntime-efficiency.", "published": "2018-05-02 17:16:48", "link": "http://arxiv.org/abs/1805.00912v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robustness of sentence length measures in written texts", "abstract": "Hidden structural patterns in written texts have been subject of considerable\nresearch in the last decades. In particular, mapping a text into a time series\nof sentence lengths is a natural way to investigate text structure. Typically,\nsentence length has been quantified by using measures based on the number of\nwords and the number of characters, but other variations are possible. To\nquantify the robustness of different sentence length measures, we analyzed a\ndatabase containing about five hundred books in English. For each book, we\nextracted six distinct measures of sentence length, including number of words\nand number of characters (taking into account lemmatization and stop words\nremoval). We compared these six measures for each book by using i) Pearson's\ncoefficient to investigate linear correlations; ii) Kolmogorov--Smirnov test to\ncompare distributions; and iii) detrended fluctuation analysis (DFA) to\nquantify auto-correlations. We have found that all six measures exhibit very\nsimilar behavior, suggesting that sentence length is a robust measure related\nto text structure.", "published": "2018-05-02 23:07:31", "link": "http://arxiv.org/abs/1805.01460v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Convolutional-Recurrent Neural Networks for Speech Enhancement", "abstract": "We propose an end-to-end model based on convolutional and recurrent neural\nnetworks for speech enhancement. Our model is purely data-driven and does not\nmake any assumptions about the type or the stationarity of the noise. In\ncontrast to existing methods that use multilayer perceptrons (MLPs), we employ\nboth convolutional and recurrent neural network architectures. Thus, our\napproach allows us to exploit local structures in both the frequency and\ntemporal domains. By incorporating prior knowledge of speech signals into the\ndesign of model structures, we build a model that is more data-efficient and\nachieves better generalization on both seen and unseen noise. Based on\nexperiments with synthetic data, we demonstrate that our model outperforms\nexisting methods, improving PESQ by up to 0.6 on seen noise and 0.64 on unseen\nnoise.", "published": "2018-05-02 00:06:53", "link": "http://arxiv.org/abs/1805.00579v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text-Independent Speaker Verification Using Long Short-Term Memory\n  Networks", "abstract": "In this paper, an architecture based on Long Short-Term Memory Networks has\nbeen proposed for the text-independent scenario which is aimed to capture the\ntemporal speaker-related information by operating over traditional speech\nfeatures. For speaker verification, at first, a background model must be\ncreated for speaker representation. Then, in enrollment stage, the speaker\nmodels will be created based on the enrollment utterances. For this work, the\nmodel will be trained in an end-to-end fashion to combine the first two stages.\nThe main goal of end-to-end training is the model being optimized to be\nconsistent with the speaker verification protocol. The end- to-end training\njointly learns the background and speaker models by creating the representation\nspace. The LSTM architecture is trained to create a discrimination space for\nvalidating the match and non-match pairs for speaker verification. The proposed\narchitecture demonstrate its superiority in the text-independent compared to\nother traditional methods.", "published": "2018-05-02 02:30:20", "link": "http://arxiv.org/abs/1805.00604v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Utterance-level Affect Analysis using Visual, Audio and Text\n  Features", "abstract": "The integration of information across multiple modalities and across time is\na promising way to enhance the emotion recognition performance of affective\nsystems. Much previous work has focused on instantaneous emotion recognition.\nThe 2018 One-Minute Gradual-Emotion Recognition (OMG-Emotion) challenge, which\nwas held in conjunction with the IEEE World Congress on Computational\nIntelligence, encouraged participants to address long-term emotion recognition\nby integrating cues from multiple modalities, including facial expression,\naudio and language. Intuitively, a multi-modal inference network should be able\nto leverage information from each modality and their correlations to improve\nrecognition over that achievable by a single modality network. We describe here\na multi-modal neural architecture that integrates visual information over time\nusing an LSTM, and combines it with utterance level audio and text cues to\nrecognize human sentiment from multimodal clips. Our model outperforms the\nunimodal baseline, achieving the concordance correlation coefficients (CCC) of\n0.400 on the arousal task, and 0.353 on the valence task.", "published": "2018-05-02 05:05:32", "link": "http://arxiv.org/abs/1805.00625v2", "categories": ["eess.IV", "cs.CL", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Images & Recipes: Retrieval in the cooking context", "abstract": "Recent advances in the machine learning community allowed different use cases\nto emerge, as its association to domains like cooking which created the\ncomputational cuisine. In this paper, we tackle the picture-recipe alignment\nproblem, having as target application the large-scale retrieval task (finding a\nrecipe given a picture, and vice versa). Our approach is validated on the\nRecipe1M dataset, composed of one million image-recipe pairs and additional\nclass information, for which we achieve state-of-the-art results.", "published": "2018-05-02 16:34:01", "link": "http://arxiv.org/abs/1805.00900v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Graph Based Semi-supervised Learning with Convolution Neural Networks to\n  Classify Crisis Related Tweets", "abstract": "During time-critical situations such as natural disasters, rapid\nclassification of data posted on social networks by affected people is useful\nfor humanitarian organizations to gain situational awareness and to plan\nresponse efforts. However, the scarcity of labeled data in the early hours of a\ncrisis hinders machine learning tasks thus delays crisis response. In this\nwork, we propose to use an inductive semi-supervised technique to utilize\nunlabeled data, which is often abundant at the onset of a crisis event, along\nwith fewer labeled data. Specif- ically, we adopt a graph-based deep learning\nframework to learn an inductive semi-supervised model. We use two real-world\ncrisis datasets from Twitter to evaluate the proposed approach. Our results\nshow significant improvements using unlabeled data as compared to only using\nlabeled data.", "published": "2018-05-02 10:38:57", "link": "http://arxiv.org/abs/1805.06289v1", "categories": ["cs.CY", "cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CY"}
{"title": "OMG Emotion Challenge - ExCouple Team", "abstract": "The proposed model is only for the audio module. All videos in the OMG\nEmotion Dataset are converted to WAV files. The proposed model makes use of\nsemi-supervised learning for the emotion recognition. A GAN is trained with\nunsupervised learning, with another database (IEMOCAP), and part of the GAN\nstructure (part of the autoencoder) will be used for the audio representation.\nThe audio spectrogram will be extracted in 1-second windows of 16khz frequency,\nand this will serve as input to the model of audio representation trained with\nanother database in an unsupervised way. This audio representation will serve\nas input to a convolutional network and a Dense layer with 'tanh' activation\nthat performs the prediction of Arousal and Valence values. For joining the\n1-second pieces of audio, the median of the predicted values of a given\nutterance will be taken.", "published": "2018-05-02 21:53:23", "link": "http://arxiv.org/abs/1805.01576v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SONYC: A System for the Monitoring, Analysis and Mitigation of Urban\n  Noise Pollution", "abstract": "We present the Sounds of New York City (SONYC) project, a smart cities\ninitiative focused on developing a cyber-physical system for the monitoring,\nanalysis and mitigation of urban noise pollution. Noise pollution is one of the\ntopmost quality of life issues for urban residents in the U.S. with proven\neffects on health, education, the economy, and the environment. Yet, most\ncities lack the resources to continuously monitor noise and understand the\ncontribution of individual sources, the tools to analyze patterns of noise\npollution at city-scale, and the means to empower city agencies to take\neffective, data-driven action for noise mitigation. The SONYC project advances\nnovel technological and socio-technical solutions that help address these\nneeds.\n  SONYC includes a distributed network of both sensors and people for\nlarge-scale noise monitoring. The sensors use low-cost, low-power technology,\nand cutting-edge machine listening techniques, to produce calibrated acoustic\nmeasurements and recognize individual sound sources in real time. Citizen\nscience methods are used to help urban residents connect to city agencies and\neach other, understand their noise footprint, and facilitate reporting and\nself-regulation. Crucially, SONYC utilizes big data solutions to analyze,\nretrieve and visualize information from sensors and citizens, creating a\ncomprehensive acoustic model of the city that can be used to identify\nsignificant patterns of noise pollution. These data can be used to drive the\nstrategic application of noise code enforcement by city agencies to optimize\nthe reduction of noise pollution. The entire system, integrating cyber,\nphysical and social infrastructure, forms a closed loop of continuous sensing,\nanalysis and actuation on the environment.\n  SONYC provides a blueprint for the mitigation of noise pollution that can\npotentially be applied to other cities in the US and abroad.", "published": "2018-05-02 16:07:39", "link": "http://arxiv.org/abs/1805.00889v2", "categories": ["cs.SD", "cs.CY", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generation of Infra sound to replicate a wind turbine", "abstract": "We have successfully produced infrasound, as a duplicate of that produced by\nIndustrial Wind Turbines. We have been able to produce this Infrasound inside a\nresearch chamber, capable of accommodating a human test subject. It is our\nvision that this project will permit others, with appropriate medical training\nand ethical oversight, to research human thresholds and the effects of this\ninfrasound on humans. Our role has focused on producing the tools, systems, and\nhardware required, to permit this research to go forward. This paper describes\nthe evolution of our project from the original vision, through the construction\nof proof of concept prototypes, a series of improved models and their\nassociated accessories /operating systems, to the final test chamber as it\nstands now ready to deploy. Also included are the mathematical and\ncomputational data supporting our claim that infrasound conditions inside the\nchamber can be made to duplicate those from actual Industrial wind turbines at\napproved setback distances.", "published": "2018-05-02 16:53:07", "link": "http://arxiv.org/abs/1805.01297v1", "categories": ["cs.SD", "eess.AS", "physics.med-ph"], "primary_category": "cs.SD"}
{"title": "Boosting Noise Robustness of Acoustic Model via Deep Adversarial\n  Training", "abstract": "In realistic environments, speech is usually interfered by various noise and\nreverberation, which dramatically degrades the performance of automatic speech\nrecognition (ASR) systems. To alleviate this issue, the commonest way is to use\na well-designed speech enhancement approach as the front-end of ASR. However,\nmore complex pipelines, more computations and even higher hardware costs\n(microphone array) are additionally consumed for this kind of methods. In\naddition, speech enhancement would result in speech distortions and mismatches\nto training. In this paper, we propose an adversarial training method to\ndirectly boost noise robustness of acoustic model. Specifically, a jointly\ncompositional scheme of generative adversarial net (GAN) and neural\nnetwork-based acoustic model (AM) is used in the training phase. GAN is used to\ngenerate clean feature representations from noisy features by the guidance of a\ndiscriminator that tries to distinguish between the true clean signals and\ngenerated signals. The joint optimization of generator, discriminator and AM\nconcentrates the strengths of both GAN and AM for speech recognition.\nSystematic experiments on CHiME-4 show that the proposed method significantly\nimproves the noise robustness of AM and achieves the average relative error\nrate reduction of 23.38% and 11.54% on the development and test set,\nrespectively.", "published": "2018-05-02 06:06:24", "link": "http://arxiv.org/abs/1805.01357v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
