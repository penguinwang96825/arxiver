{"title": "Unsupervised Neural Stylistic Text Generation using Transfer learning\n  and Adapters", "abstract": "Research has shown that personality is a key driver to improve engagement and\nuser experience in conversational systems. Conversational agents should also\nmaintain a consistent persona to have an engaging conversation with a user.\nHowever, text generation datasets are often crowd sourced and thereby have an\naveraging effect where the style of the generation model is an average style of\nall the crowd workers that have contributed to the dataset. While one can\ncollect persona-specific datasets for each task, it would be an expensive and\ntime consuming annotation effort. In this work, we propose a novel transfer\nlearning framework which updates only $0.3\\%$ of model parameters to learn\nstyle specific attributes for response generation. For the purpose of this\nstudy, we tackle the problem of stylistic story ending generation using the ROC\nstories Corpus. We learn style specific attributes from the\nPERSONALITY-CAPTIONS dataset. Through extensive experiments and evaluation\nmetrics we show that our novel training procedure can improve the style\ngeneration by 200 over Encoder-Decoder baselines while maintaining on-par\ncontent relevance metrics with", "published": "2022-10-07 00:09:22", "link": "http://arxiv.org/abs/2210.03264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Encoder-Decoder Framework with Entity Memory", "abstract": "Entities, as important carriers of real-world knowledge, play a key role in\nmany NLP tasks. We focus on incorporating entity knowledge into an\nencoder-decoder framework for informative text generation. Existing approaches\ntried to index, retrieve, and read external documents as evidence, but they\nsuffered from a large computational overhead. In this work, we propose an\nencoder-decoder framework with an entity memory, namely EDMem. The entity\nknowledge is stored in the memory as latent representations, and the memory is\npre-trained on Wikipedia along with encoder-decoder parameters. To precisely\ngenerate entity names, we design three decoding methods to constrain entity\ngeneration by linking entities in the memory. EDMem is a unified framework that\ncan be used on various entity-intensive question answering and generation\ntasks. Extensive experimental results show that EDMem outperforms both\nmemory-based auto-encoder models and non-memory encoder-decoder models.", "published": "2022-10-07 01:15:30", "link": "http://arxiv.org/abs/2210.03273v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD\n  Coding", "abstract": "Automatic International Classification of Diseases (ICD) coding aims to\nassign multiple ICD codes to a medical note with average length of 3,000+\ntokens. This task is challenging due to a high-dimensional space of multi-label\nassignment (tens of thousands of ICD codes) and the long-tail challenge: only a\nfew codes (common diseases) are frequently assigned while most codes (rare\ndiseases) are infrequently assigned. This study addresses the long-tail\nchallenge by adapting a prompt-based fine-tuning technique with label\nsemantics, which has been shown to be effective under few-shot setting. To\nfurther enhance the performance in medical domain, we propose a\nknowledge-enhanced longformer by injecting three domain-specific knowledge:\nhierarchy, synonym, and abbreviation with additional pretraining using\ncontrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of\ncode assignment, show that our proposed method outperforms previous\nstate-of-the-art method in 14.5% in marco F1 (from 10.3 to 11.8, P<0.001). To\nfurther test our model on few-shot setting, we created a new rare diseases\ncoding dataset, MIMIC-III-rare50, on which our model improves marco F1 from\n17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method.", "published": "2022-10-07 03:25:58", "link": "http://arxiv.org/abs/2210.03304v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Unsupervised Cross-Lingual Word Embedding using Domain Flow\n  Interpolation", "abstract": "This paper investigates an unsupervised approach towards deriving a\nuniversal, cross-lingual word embedding space, where words with similar\nsemantics from different languages are close to one another. Previous\nadversarial approaches have shown promising results in inducing cross-lingual\nword embedding without parallel data. However, the training stage shows\ninstability for distant language pairs. Instead of mapping the source language\nspace directly to the target language space, we propose to make use of a\nsequence of intermediate spaces for smooth bridging. Each intermediate space\nmay be conceived as a pseudo-language space and is introduced via simple linear\ninterpolation. This approach is modeled after domain flow in computer vision,\nbut with a modified objective function. Experiments on intrinsic Bilingual\nDictionary Induction tasks show that the proposed approach can improve the\nrobustness of adversarial models with comparable and even better precision.\nFurther experiments on the downstream task of Cross-Lingual Natural Language\nInference show that the proposed model achieves significant performance\nimprovement for distant language pairs in downstream tasks compared to\nstate-of-the-art adversarial and non-adversarial models.", "published": "2022-10-07 04:37:47", "link": "http://arxiv.org/abs/2210.03319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrating Factual Knowledge in Pretrained Language Models", "abstract": "Previous literature has proved that Pretrained Language Models (PLMs) can\nstore factual knowledge. However, we find that facts stored in the PLMs are not\nalways correct. It motivates us to explore a fundamental question: How do we\ncalibrate factual knowledge in PLMs without re-training from scratch? In this\nwork, we propose a simple and lightweight method CaliNet to achieve this goal.\nTo be specific, we first detect whether PLMs can learn the right facts via a\ncontrastive score between right and fake facts. If not, we then use a\nlightweight method to add and adapt new parameters to specific factual texts.\nExperiments on the knowledge probing task show the calibration effectiveness\nand efficiency. In addition, through closed-book question answering, we find\nthat the calibrated PLM possesses knowledge generalization ability after\nfine-tuning. Beyond the calibration performance, we further investigate and\nvisualize the knowledge calibration mechanism.", "published": "2022-10-07 05:14:58", "link": "http://arxiv.org/abs/2210.03329v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring and Narrowing the Compositionality Gap in Language Models", "abstract": "We investigate the ability of language models to perform compositional\nreasoning tasks where the overall solution depends on correctly composing the\nanswers to sub-problems. We measure how often models can correctly answer all\nsub-problems but not generate the overall solution, a ratio we call the\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\nanswers that require composing multiple facts unlikely to have been observed\ntogether during pretraining. In the GPT-3 family of models, as model size\nincreases we show that the single-hop question answering performance improves\nfaster than the multi-hop performance does, therefore the compositionality gap\ndoes not decrease. This surprising result suggests that while more powerful\nmodels memorize and recall more factual knowledge, they show no corresponding\nimprovement in their ability to perform this kind of compositional reasoning.\n  We then demonstrate how elicitive prompting (such as chain of thought)\nnarrows the compositionality gap by reasoning explicitly. We present a new\nmethod, self-ask, that further improves on chain of thought. In our method, the\nmodel explicitly asks itself (and answers) follow-up questions before answering\nthe initial question. We finally show that self-ask's structured prompting lets\nus easily plug in a search engine to answer the follow-up questions, which\nadditionally improves accuracy.", "published": "2022-10-07 06:50:23", "link": "http://arxiv.org/abs/2210.03350v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PARAGEN : A Parallel Generation Toolkit", "abstract": "PARAGEN is a PyTorch-based NLP toolkit for further development on parallel\ngeneration. PARAGEN provides thirteen types of customizable plugins, helping\nusers to experiment quickly with novel ideas across model architectures,\noptimization, and learning strategies. We implement various features, such as\nunlimited data loading and automatic model selection, to enhance its industrial\nusage. ParaGen is now deployed to support various research and industry\napplications at ByteDance. PARAGEN is available at\nhttps://github.com/bytedance/ParaGen.", "published": "2022-10-07 08:55:10", "link": "http://arxiv.org/abs/2210.03405v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DABERT: Dual Attention Enhanced BERT for Semantic Matching", "abstract": "Transformer-based pre-trained language models such as BERT have achieved\nremarkable results in Semantic Sentence Matching. However, existing models\nstill suffer from insufficient ability to capture subtle differences. Minor\nnoise like word addition, deletion, and modification of sentences may cause\nflipped predictions. To alleviate this problem, we propose a novel Dual\nAttention Enhanced BERT (DABERT) to enhance the ability of BERT to capture\nfine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention\nmodule, which measures soft word matches by introducing a new dual channel\nalignment mechanism to model affinity and difference attention. (2) Adaptive\nFusion module, this module uses attention to learn the aggregation of\ndifference and affinity features, and generates a vector describing the\nmatching details of sentence pairs. We conduct extensive experiments on\nwell-studied semantic matching and robustness test datasets, and the\nexperimental results show the effectiveness of our proposed method.", "published": "2022-10-07 10:54:49", "link": "http://arxiv.org/abs/2210.03454v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Speech and Offensive Language Detection in Bengali", "abstract": "Social media often serves as a breeding ground for various hateful and\noffensive content. Identifying such content on social media is crucial due to\nits impact on the race, gender, or religion in an unprejudiced society.\nHowever, while there is extensive research in hate speech detection in English,\nthere is a gap in hateful content detection in low-resource languages like\nBengali. Besides, a current trend on social media is the use of Romanized\nBengali for regular interactions. To overcome the existing research's\nlimitations, in this study, we develop an annotated dataset of 10K Bengali\nposts consisting of 5K actual and 5K Romanized Bengali tweets. We implement\nseveral baseline models for the classification of such hateful posts. We\nfurther explore the interlingual transfer mechanism to boost classification\nperformance. Finally, we perform an in-depth error analysis by looking into the\nmisclassified posts by the models. While training actual and Romanized datasets\nseparately, we observe that XLM-Roberta performs the best. Further, we witness\nthat on joint training and few-shot training, MuRIL outperforms other models by\ninterpreting the semantic expressions better. We make our code and dataset\npublic for others.", "published": "2022-10-07 12:06:04", "link": "http://arxiv.org/abs/2210.03479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PCAE: A Framework of Plug-in Conditional Auto-Encoder for Controllable\n  Text Generation", "abstract": "Controllable text generation has taken a gigantic step forward these days.\nYet existing methods are either constrained in a one-off pattern or not\nefficient enough for receiving multiple conditions at every generation stage.\nWe propose a model-agnostic framework Plug-in Conditional Auto-Encoder for\nControllable Text Generation (PCAE) towards flexible and semi-supervised text\ngeneration. Our framework is \"plug-and-play\" with partial parameters to be\nfine-tuned in the pre-trained model (less than a half). Crucial to the success\nof PCAE is the proposed broadcasting label fusion network for navigating the\nglobal latent code to a specified local and confined space. Visualization of\nthe local latent prior well confirms the primary devotion in hidden space of\nthe proposed model. Moreover, extensive experiments across five related\ngeneration tasks (from 2 conditions up to 10 conditions) on both RNN- based and\npre-trained BART [26] based auto-encoders reveal the high capability of PCAE,\nwhich enables generation that is highly manipulable, syntactically diverse and\ntime-saving with minimum labeled samples. We will release our code at\nhttps://github.com/ImKeTT/pcae.", "published": "2022-10-07 12:31:47", "link": "http://arxiv.org/abs/2210.03496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Multi-Modal Sarcasm Detection via Hierarchical Congruity\n  Modeling with Knowledge Enhancement", "abstract": "Sarcasm is a linguistic phenomenon indicating a discrepancy between literal\nmeanings and implied intentions. Due to its sophisticated nature, it is usually\nchallenging to be detected from the text itself. As a result, multi-modal\nsarcasm detection has received more attention in both academia and industries.\nHowever, most existing techniques only modeled the atomic-level inconsistencies\nbetween the text input and its accompanying image, ignoring more complex\ncompositions for both modalities. Moreover, they neglected the rich information\ncontained in external knowledge, e.g., image captions. In this paper, we\npropose a novel hierarchical framework for sarcasm detection by exploring both\nthe atomic-level congruity based on multi-head cross attention mechanism and\nthe composition-level congruity based on graph neural networks, where a post\nwith low congruity can be identified as sarcasm. In addition, we exploit the\neffect of various knowledge resources for sarcasm detection. Evaluation results\non a public multi-modal sarcasm detection dataset based on Twitter demonstrate\nthe superiority of our proposed model.", "published": "2022-10-07 12:44:33", "link": "http://arxiv.org/abs/2210.03501v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Representations Built from the Ground Up? An Empirical Examination\n  of Local Composition in Language Models", "abstract": "Compositionality, the phenomenon where the meaning of a phrase can be derived\nfrom its constituent parts, is a hallmark of human language. At the same time,\nmany phrases are non-compositional, carrying a meaning beyond that of each part\nin isolation. Representing both of these types of phrases is critical for\nlanguage understanding, but it is an open question whether modern language\nmodels (LMs) learn to do so; in this work we examine this question. We first\nformulate a problem of predicting the LM-internal representations of longer\nphrases given those of their constituents. We find that the representation of a\nparent phrase can be predicted with some accuracy given an affine\ntransformation of its children. While we would expect the predictive accuracy\nto correlate with human judgments of semantic compositionality, we find this is\nlargely not the case, indicating that LMs may not accurately distinguish\nbetween compositional and non-compositional phrases. We perform a variety of\nanalyses, shedding light on when different varieties of LMs do and do not\ngenerate compositional representations, and discuss implications for future\nmodeling work.", "published": "2022-10-07 14:21:30", "link": "http://arxiv.org/abs/2210.03575v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Transformer Memorization Recall Through Idioms", "abstract": "To produce accurate predictions, language models (LMs) must balance between\ngeneralization and memorization. Yet, little is known about the mechanism by\nwhich transformer LMs employ their memorization capacity. When does a model\ndecide to output a memorized phrase, and how is this phrase then retrieved from\nmemory? In this work, we offer the first methodological framework for probing\nand characterizing recall of memorized sequences in transformer LMs. First, we\nlay out criteria for detecting model inputs that trigger memory recall, and\npropose idioms as inputs that typically fulfill these criteria. Next, we\nconstruct a dataset of English idioms and use it to compare model behavior on\nmemorized vs. non-memorized inputs. Specifically, we analyze the internal\nprediction construction process by interpreting the model's hidden\nrepresentations as a gradual refinement of the output probability distribution.\nWe find that across different model sizes and architectures, memorized\npredictions are a two-step process: early layers promote the predicted token to\nthe top of the output distribution, and upper layers increase model confidence.\nThis suggests that memorized information is stored and retrieved in the early\nlayers of the network. Last, we demonstrate the utility of our methodology\nbeyond idioms in memorized factual statements. Overall, our work makes a first\nstep towards understanding memory recall, and provides a methodological basis\nfor future studies of transformer memorization.", "published": "2022-10-07 14:45:31", "link": "http://arxiv.org/abs/2210.03588v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition in Twitter: A Dataset and Analysis on\n  Short-Term Temporal Shifts", "abstract": "Recent progress in language model pre-training has led to important\nimprovements in Named Entity Recognition (NER). Nonetheless, this progress has\nbeen mainly tested in well-formatted documents such as news, Wikipedia, or\nscientific articles. In social media the landscape is different, in which it\nadds another layer of complexity due to its noisy and dynamic nature. In this\npaper, we focus on NER in Twitter, one of the largest social media platforms,\nand construct a new NER dataset, TweetNER7, which contains seven entity types\nannotated over 11,382 tweets from September 2019 to August 2021. The dataset\nwas constructed by carefully distributing the tweets over time and taking\nrepresentative trends as a basis. Along with the dataset, we provide a set of\nlanguage model baselines and perform an analysis on the language model\nperformance on the task, especially analyzing the impact of different time\nperiods. In particular, we focus on three important temporal aspects in our\nanalysis: short-term degradation of NER models over time, strategies to\nfine-tune a language model over different periods, and self-labeling as an\nalternative to lack of recently-labeled data. TweetNER7 is released publicly\n(https://huggingface.co/datasets/tner/tweetner7) along with the models\nfine-tuned on it.", "published": "2022-10-07 19:58:47", "link": "http://arxiv.org/abs/2210.03797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval Augmented Visual Question Answering with Outside Knowledge", "abstract": "Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA\ntask that requires retrieval of external knowledge to answer questions about\nimages. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve\ndocuments from external knowledge bases, such as Wikipedia, but with DPR\ntrained separately from answer generation, introducing a potential limit on the\noverall system performance. Instead, we propose a joint training scheme which\nincludes differentiable DPR integrated with answer generation so that the\nsystem can be trained in an end-to-end fashion. Our experiments show that our\nscheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also\nintroduce new diagnostic metrics to analyze how retrieval and generation\ninteract. The strong retrieval ability of our model significantly reduces the\nnumber of retrieved documents needed in training, yielding significant benefits\nin answer quality and computation required for training.", "published": "2022-10-07 20:35:58", "link": "http://arxiv.org/abs/2210.03809v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking BERT: Evaluating and Optimizing Sparsified Attention", "abstract": "Transformers allow attention between all pairs of tokens, but there is reason\nto believe that most of these connections - and their quadratic time and memory\n- may not be necessary. But which ones? We evaluate the impact of\nsparsification patterns with a series of ablation experiments. First, we\ncompare masks based on syntax, lexical similarity, and token position to random\nconnections, and measure which patterns reduce performance the least. We find\nthat on three common finetuning tasks even using attention that is at least 78%\nsparse can have little effect on performance if applied at later transformer\nlayers, but that applying sparsity throughout the network reduces performance\nsignificantly. Second, we vary the degree of sparsity for three patterns\nsupported by previous work, and find that connections to neighbouring tokens\nare the most significant. Finally, we treat sparsity as an optimizable\nparameter, and present an algorithm to learn degrees of neighboring connections\nthat gives a fine-grained control over the accuracy-sparsity trade-off while\napproaching the performance of existing methods.", "published": "2022-10-07 22:32:27", "link": "http://arxiv.org/abs/2210.03841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational\n  Finance Question Answering", "abstract": "With the recent advance in large pre-trained language models, researchers\nhave achieved record performances in NLP tasks that mostly focus on language\npattern matching. The community is experiencing the shift of the challenge from\nhow to model language to the imitation of complex reasoning abilities like\nhuman beings. In this work, we investigate the application domain of finance\nthat involves real-world, complex numerical reasoning. We propose a new\nlarge-scale dataset, ConvFinQA, aiming to study the chain of numerical\nreasoning in conversational question answering. Our dataset poses great\nchallenge in modeling long-range, complex numerical reasoning paths in\nreal-world conversations. We conduct comprehensive experiments and analyses\nwith both the neural symbolic methods and the prompting-based methods, to\nprovide insights into the reasoning mechanisms of these two divisions. We\nbelieve our new dataset should serve as a valuable resource to push forward the\nexploration of real-world, complex reasoning tasks as the next research focus.\nOur dataset and code is publicly available at\nhttps://github.com/czyssrs/ConvFinQA.", "published": "2022-10-07 23:48:50", "link": "http://arxiv.org/abs/2210.03849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Keyword Based Approach to Understanding the Overpenalization of\n  Marginalized Groups by English Marginal Abuse Models on Twitter", "abstract": "Harmful content detection models tend to have higher false positive rates for\ncontent from marginalized groups. In the context of marginal abuse modeling on\nTwitter, such disproportionate penalization poses the risk of reduced\nvisibility, where marginalized communities lose the opportunity to voice their\nopinion on the platform. Current approaches to algorithmic harm mitigation, and\nbias detection for NLP models are often very ad hoc and subject to human bias.\nWe make two main contributions in this paper. First, we design a novel\nmethodology, which provides a principled approach to detecting and measuring\nthe severity of potential harms associated with a text-based model. Second, we\napply our methodology to audit Twitter's English marginal abuse model, which is\nused for removing amplification eligibility of marginally abusive content.\nWithout utilizing demographic labels or dialect classifiers, we are still able\nto detect and measure the severity of issues related to the over-penalization\nof the speech of marginalized communities, such as the use of reclaimed speech,\ncounterspeech, and identity related terms. In order to mitigate the associated\nharms, we experiment with adding additional true negative examples and find\nthat doing so provides improvements to our fairness metrics without large\ndegradations in model performance.", "published": "2022-10-07 20:28:00", "link": "http://arxiv.org/abs/2210.06351v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-driven Approach to Differentiating between Depression and Dementia\n  from Noisy Speech and Language Data", "abstract": "A significant number of studies apply acoustic and linguistic characteristics\nof human speech as prominent markers of dementia and depression. However,\nstudies on discriminating depression from dementia are rare. Co-morbid\ndepression is frequent in dementia and these clinical conditions share many\noverlapping symptoms, but the ability to distinguish between depression and\ndementia is essential as depression is often curable. In this work, we\ninvestigate the ability of clustering approaches in distinguishing between\ndepression and dementia from human speech. We introduce a novel aggregated\ndataset, which combines narrative speech data from multiple conditions, i.e.,\nAlzheimer's disease, mild cognitive impairment, healthy control, and\ndepression. We compare linear and non-linear clustering approaches and show\nthat non-linear clustering techniques distinguish better between distinct\ndisease clusters. Our interpretability analysis shows that the main\ndifferentiating symptoms between dementia and depression are acoustic\nabnormality, repetitiveness (or circularity) of speech, word finding\ndifficulty, coherence impairment, and differences in lexical complexity and\nrichness.", "published": "2022-10-07 03:20:02", "link": "http://arxiv.org/abs/2210.03303v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distillation-Resistant Watermarking for Model Protection in NLP", "abstract": "How can we protect the intellectual property of trained NLP models? Modern\nNLP models are prone to stealing by querying and distilling from their publicly\nexposed APIs. However, existing protection methods such as watermarking only\nwork for images but are not applicable to text. We propose\nDistillation-Resistant Watermarking (DRW), a novel technique to protect NLP\nmodels from being stolen via distillation. DRW protects a model by injecting\nwatermarks into the victim's prediction probability corresponding to a secret\nkey and is able to detect such a key by probing a suspect model. We prove that\na protected model still retains the original accuracy within a certain bound.\nWe evaluate DRW on a diverse set of NLP tasks including text classification,\npart-of-speech tagging, and named entity recognition. Experiments show that DRW\nprotects the original model and detects stealing suspects at 100% mean average\nprecision for all four tasks while the prior method fails on two.", "published": "2022-10-07 04:14:35", "link": "http://arxiv.org/abs/2210.03312v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Unified Framework for Multi-intent Spoken Language Understanding with\n  prompting", "abstract": "Multi-intent Spoken Language Understanding has great potential for widespread\nimplementation. Jointly modeling Intent Detection and Slot Filling in it\nprovides a channel to exploit the correlation between intents and slots.\nHowever, current approaches are apt to formulate these two sub-tasks\ndifferently, which leads to two issues: 1) It hinders models from effective\nextraction of shared features. 2) Pretty complicated structures are involved to\nenhance expression ability while causing damage to the interpretability of\nframeworks. In this work, we describe a Prompt-based Spoken Language\nUnderstanding (PromptSLU) framework, to intuitively unify two sub-tasks into\nthe same form by offering a common pre-trained Seq2Seq model. In detail, ID and\nSF are completed by concisely filling the utterance into task-specific prompt\ntemplates as input, and sharing output formats of key-value pairs sequence.\nFurthermore, variable intents are predicted first, then naturally embedded into\nprompts to guide slot-value pairs inference from a semantic perspective.\nFinally, we are inspired by prevalent multi-task learning to introduce an\nauxiliary sub-task, which helps to learn relationships among provided labels.\nExperiment results show that our framework outperforms several state-of-the-art\nbaselines on two public datasets.", "published": "2022-10-07 05:58:05", "link": "http://arxiv.org/abs/2210.03337v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language\n  Understanding", "abstract": "Visually-situated language is ubiquitous -- sources range from textbooks with\ndiagrams to web pages with images and tables, to mobile apps with buttons and\nforms. Perhaps due to this diversity, previous work has typically relied on\ndomain-specific recipes with limited sharing of the underlying data, model\narchitectures, and objectives. We present Pix2Struct, a pretrained\nimage-to-text model for purely visual language understanding, which can be\nfinetuned on tasks containing visually-situated language. Pix2Struct is\npretrained by learning to parse masked screenshots of web pages into simplified\nHTML. The web, with its richness of visual elements cleanly reflected in the\nHTML structure, provides a large source of pretraining data well suited to the\ndiversity of downstream tasks. Intuitively, this objective subsumes common\npretraining signals such as OCR, language modeling, image captioning. In\naddition to the novel pretraining strategy, we introduce a variable-resolution\ninput representation and a more flexible integration of language and vision\ninputs, where language prompts such as questions are rendered directly on top\nof the input image. For the first time, we show that a single pretrained model\ncan achieve state-of-the-art results in six out of nine tasks across four\ndomains: documents, illustrations, user interfaces, and natural images.", "published": "2022-10-07 06:42:06", "link": "http://arxiv.org/abs/2210.03347v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Zero-shot stance detection based on cross-domain feature enhancement by\n  contrastive learning", "abstract": "Zero-shot stance detection is challenging because it requires detecting the\nstance of previously unseen targets in the inference phase. The ability to\nlearn transferable target-invariant features is critical for zero-shot stance\ndetection. In this work, we propose a stance detection approach that can\nefficiently adapt to unseen targets, the core of which is to capture\ntarget-invariant syntactic expression patterns as transferable knowledge.\nSpecifically, we first augment the data by masking the topic words of\nsentences, and then feed the augmented data to an unsupervised contrastive\nlearning module to capture transferable features. Then, to fit a specific\ntarget, we encode the raw texts as target-specific features. Finally, we adopt\nan attention mechanism, which combines syntactic expression patterns with\ntarget-specific features to obtain enhanced features for predicting previously\nunseen targets. Experiments demonstrate that our model outperforms competitive\nbaselines on four benchmark datasets.", "published": "2022-10-07 07:45:40", "link": "http://arxiv.org/abs/2210.03380v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpaceQA: Answering Questions about the Design of Space Missions and\n  Space Craft Concepts", "abstract": "We present SpaceQA, to the best of our knowledge the first open-domain QA\nsystem in Space mission design. SpaceQA is part of an initiative by the\nEuropean Space Agency (ESA) to facilitate the access, sharing and reuse of\ninformation about Space mission design within the agency and with the public.\nWe adopt a state-of-the-art architecture consisting of a dense retriever and a\nneural reader and opt for an approach based on transfer learning rather than\nfine-tuning due to the lack of domain-specific annotated data. Our evaluation\non a test set produced by ESA is largely consistent with the results originally\nreported by the evaluated retrievers and confirms the need of fine tuning for\nreading comprehension. As of writing this paper, ESA is piloting SpaceQA\ninternally.", "published": "2022-10-07 09:41:39", "link": "http://arxiv.org/abs/2210.03422v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Quizzes to Support Training on Quality Management and\n  Assurance in Space Science and Engineering", "abstract": "Quality management and assurance is key for space agencies to guarantee the\nsuccess of space missions, which are high-risk and extremely costly. In this\npaper, we present a system to generate quizzes, a common resource to evaluate\nthe effectiveness of training sessions, from documents about quality assurance\nprocedures in the Space domain. Our system leverages state of the art\nauto-regressive models like T5 and BART to generate questions, and a RoBERTa\nmodel to extract answers for such questions, thus verifying their suitability.", "published": "2022-10-07 09:52:10", "link": "http://arxiv.org/abs/2210.03427v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Chain of Thought Prompting in Large Language Models", "abstract": "Large language models (LLMs) can perform complex reasoning by generating\nintermediate reasoning steps. Providing these steps for prompting\ndemonstrations is called chain-of-thought (CoT) prompting. CoT prompting has\ntwo major paradigms. One leverages a simple prompt like \"Let's think step by\nstep\" to facilitate step-by-step thinking before answering a question. The\nother uses a few manual demonstrations one by one, each composed of a question\nand a reasoning chain that leads to an answer. The superior performance of the\nsecond paradigm hinges on the hand-crafting of task-specific demonstrations one\nby one. We show that such manual efforts may be eliminated by leveraging LLMs\nwith the \"Let's think step by step\" prompt to generate reasoning chains for\ndemonstrations one by one, i.e., let's think not just step by step, but also\none by one. However, these generated chains often come with mistakes. To\nmitigate the effect of such mistakes, we find that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT\nprompting method: Auto-CoT. It samples questions with diversity and generates\nreasoning chains to construct demonstrations. On ten public benchmark reasoning\ntasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of\nthe CoT paradigm that requires manual designs of demonstrations. Code is\navailable at https://github.com/amazon-research/auto-cot", "published": "2022-10-07 12:28:21", "link": "http://arxiv.org/abs/2210.03493v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Large Language Models are Transforming Machine-Paraphrased\n  Plagiarism", "abstract": "The recent success of large language models for text generation poses a\nsevere threat to academic integrity, as plagiarists can generate realistic\nparaphrases indistinguishable from original work. However, the role of large\nautoregressive transformers in generating machine-paraphrased plagiarism and\ntheir detection is still developing in the literature. This work explores T5\nand GPT-3 for machine-paraphrase generation on scientific articles from arXiv,\nstudent theses, and Wikipedia. We evaluate the detection performance of six\nautomated solutions and one commercial plagiarism detection software and\nperform a human study with 105 participants regarding their detection\nperformance and the quality of generated examples. Our results suggest that\nlarge models can rewrite text humans have difficulty identifying as\nmachine-paraphrased (53% mean acc.). Human experts rate the quality of\nparaphrases generated by GPT-3 as high as original texts (clarity 4.0/5,\nfluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)\nachieves a 66% F1-score in detecting paraphrases.", "published": "2022-10-07 14:08:57", "link": "http://arxiv.org/abs/2210.03568v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cloud-based Automatic Speech Recognition Systems for Southeast Asian\n  Languages", "abstract": "This paper provides an overall introduction of our Automatic Speech\nRecognition (ASR) systems for Southeast Asian languages. As not much existing\nwork has been carried out on such regional languages, a few difficulties should\nbe addressed before building the systems: limitation on speech and text\nresources, lack of linguistic knowledge, etc. This work takes Bahasa Indonesia\nand Thai as examples to illustrate the strategies of collecting various\nresources required for building ASR systems.", "published": "2022-10-07 14:28:40", "link": "http://arxiv.org/abs/2210.03580v1", "categories": ["cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Pronunciation Modeling of Foreign Words for Mandarin ASR by Considering\n  the Effect of Language Transfer", "abstract": "One of the challenges in automatic speech recognition is foreign words\nrecognition. It is observed that a speaker's pronunciation of a foreign word is\ninfluenced by his native language knowledge, and such phenomenon is known as\nthe effect of language transfer. This paper focuses on examining the phonetic\neffect of language transfer in automatic speech recognition. A set of lexical\nrules is proposed to convert an English word into Mandarin phonetic\nrepresentation. In this way, a Mandarin lexicon can be augmented by including\nEnglish words. Hence, the Mandarin ASR system becomes capable to recognize\nEnglish words without retraining or re-estimation of the acoustic model\nparameters. Using the lexicon that derived from the proposed rules, the ASR\nperformance of Mandarin English mixed speech is improved without harming the\naccuracy of Mandarin only speech. The proposed lexical rules are generalized\nand they can be directly applied to unseen English words.", "published": "2022-10-07 14:59:44", "link": "http://arxiv.org/abs/2210.03603v1", "categories": ["cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Artificial Intelligence and Natural Language Processing and\n  Understanding in Space: A Methodological Framework and Four ESA Case Studies", "abstract": "The European Space Agency is well known as a powerful force for scientific\ndiscovery in numerous areas related to Space. The amount and depth of the\nknowledge produced throughout the different missions carried out by ESA and\ntheir contribution to scientific progress is enormous, involving large\ncollections of documents like scientific publications, feasibility studies,\ntechnical reports, and quality management procedures, among many others.\nThrough initiatives like the Open Space Innovation Platform, ESA also acts as a\nhub for new ideas coming from the wider community across different challenges,\ncontributing to a virtuous circle of scientific discovery and innovation.\nHandling such wealth of information, of which large part is unstructured text,\nis a colossal task that goes beyond human capabilities, hence requiring\nautomation. In this paper, we present a methodological framework based on\nartificial intelligence and natural language processing and understanding to\nautomatically extract information from Space documents, generating value from\nit, and illustrate such framework through several case studies implemented\nacross different functional areas of ESA, including Mission Design, Quality\nAssurance, Long-Term Data Preservation, and the Open Space Innovation Platform.\nIn doing so, we demonstrate the value of these technologies in several tasks\nranging from effortlessly searching and recommending Space information to\nautomatically determining how innovative an idea can be, answering questions\nabout Space, and generating quizzes regarding quality procedures. Each of these\naccomplishments represents a step forward in the application of increasingly\nintelligent AI systems in Space, from structuring and facilitating information\naccess to intelligent systems capable to understand and reason with such\ninformation.", "published": "2022-10-07 15:50:17", "link": "http://arxiv.org/abs/2210.03640v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Longtonotes: OntoNotes with Longer Coreference Chains", "abstract": "Ontonotes has served as the most important benchmark for coreference\nresolution. However, for ease of annotation, several long documents in\nOntonotes were split into smaller parts. In this work, we build a corpus of\ncoreference-annotated documents of significantly longer length than what is\ncurrently available. We do so by providing an accurate, manually-curated,\nmerging of annotations from documents that were split into multiple parts in\nthe original Ontonotes annotation process. The resulting corpus, which we call\nLongtoNotes contains documents in multiple genres of the English language with\nvarying lengths, the longest of which are up to 8x the length of documents in\nOntonotes, and 2x those in Litbank. We evaluate state-of-the-art neural\ncoreference systems on this new corpus, analyze the relationships between model\narchitectures/hyperparameters and document length on performance and efficiency\nof the models, and demonstrate areas of improvement in long-document\ncoreference modeling revealed by our new corpus. Our data and code is available\nat: https://github.com/kumar-shridhar/LongtoNotes.", "published": "2022-10-07 15:58:41", "link": "http://arxiv.org/abs/2210.03650v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of\n  In-Context Experts", "abstract": "Anaphora resolution is an important task for information extraction across a\nrange of languages, text genres, and domains, motivating the need for methods\nthat do not require large annotated datasets. In-context learning has emerged\nas a promising approach, yet there are a number of challenges in applying\nin-context learning to resolve anaphora. For example, encoding a single\nin-context demonstration that consists of: an anaphor, a paragraph-length\ncontext, and a list of corresponding antecedents, requires conditioning a\nlanguage model on a long sequence of tokens, limiting the number of\ndemonstrations per prompt. In this paper, we present MICE (Mixtures of\nIn-Context Experts), which we demonstrate is effective for few-shot anaphora\nresolution in scientific protocols (Tamari et al., 2021). Given only a handful\nof training examples, MICE combines the predictions of hundreds of in-context\nexperts, yielding a 30% increase in F1 score over a competitive prompt\nretrieval baseline. Furthermore, we show MICE can be used to train compact\nstudent models without sacrificing performance. As far as we are aware, this is\nthe first work to present experimental results demonstrating the effectiveness\nof in-context learning on the task of few-shot anaphora resolution in\nscientific protocols.", "published": "2022-10-07 16:51:45", "link": "http://arxiv.org/abs/2210.03690v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder\n  Based Speech-Text Pre-training", "abstract": "The rapid development of single-modal pre-training has prompted researchers\nto pay more attention to cross-modal pre-training methods. In this paper, we\npropose a unified-modal speech-unit-text pre-training model, SpeechUT, to\nconnect the representations of a speech encoder and a text decoder with a\nshared unit encoder. Leveraging hidden-unit as an interface to align speech and\ntext, we can decompose the speech-to-text model into a speech-to-unit model and\na unit-to-text model, which can be jointly pre-trained with unpaired speech and\ntext data respectively. Our proposed SpeechUT is fine-tuned and evaluated on\nautomatic speech recognition (ASR) and speech translation (ST) tasks.\nExperimental results show that SpeechUT gets substantial improvements over\nstrong baselines, and achieves state-of-the-art performance on both the\nLibriSpeech ASR and MuST-C ST tasks. To better understand the proposed\nSpeechUT, detailed analyses are conducted. The code and pre-trained models are\navailable at https://aka.ms/SpeechUT.", "published": "2022-10-07 17:57:45", "link": "http://arxiv.org/abs/2210.03730v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Visualize Before You Write: Imagination-Guided Open-Ended Text\n  Generation", "abstract": "Recent advances in text-to-image synthesis make it possible to visualize\nmachine imaginations for a given context. On the other hand, when generating\ntext, human writers are gifted at creative visualization, which enhances their\nwritings by forming imaginations as blueprints before putting down the stories\nin words. Inspired by such a cognitive process, we ask the natural question of\nwhether we can endow machines with the same ability to utilize visual\ninformation and construct a general picture of the context to guide text\ngeneration. In this work, we propose iNLG that uses machine-generated images to\nguide language models in open-ended text generation. The experiments and\nanalyses demonstrate the effectiveness of iNLG on open-ended text generation\ntasks, including text completion, story generation, and concept-to-text\ngeneration in both few-shot and full-data scenarios. Both automatic metrics and\nhuman evaluations verify that the text snippets generated by our iNLG are\ncoherent and informative while displaying minor degeneration.", "published": "2022-10-07 18:01:09", "link": "http://arxiv.org/abs/2210.03765v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FedPC: Federated Learning for Language Generation with Personal and\n  Context Preference Embeddings", "abstract": "Federated learning is a training paradigm that learns from multiple\ndistributed users without aggregating data on a centralized server. Such a\nparadigm promises the ability to deploy machine-learning at-scale to a diverse\npopulation of end-users without first collecting a large, labeled dataset for\nall possible tasks. As federated learning typically averages learning updates\nacross a decentralized population, there is a growing need for personalization\nof federated learning systems (i.e conversational agents must be able to\npersonalize to a specific user's preferences). In this work, we propose a new\ndirection for personalization research within federated learning, leveraging\nboth personal embeddings and shared context embeddings. We also present an\napproach to predict these ``preference'' embeddings, enabling personalization\nwithout backpropagation. Compared to state-of-the-art personalization\nbaselines, our approach achieves a 50\\% improvement in test-time perplexity\nusing 0.001\\% of the memory required by baseline approaches, and achieving\ngreater sample- and compute-efficiency.", "published": "2022-10-07 18:01:19", "link": "http://arxiv.org/abs/2210.03766v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Lifecycle of \"Facts\": A Survey of Social Bias in Knowledge Graphs", "abstract": "Knowledge graphs are increasingly used in a plethora of downstream tasks or\nin the augmentation of statistical models to improve factuality. However,\nsocial biases are engraved in these representations and propagate downstream.\nWe conducted a critical analysis of literature concerning biases at different\nsteps of a knowledge graph lifecycle. We investigated factors introducing bias,\nas well as the biases that are rendered by knowledge graphs and their embedded\nversions afterward. Limitations of existing measurement and mitigation\nstrategies are discussed and paths forward are proposed.", "published": "2022-10-07 06:55:13", "link": "http://arxiv.org/abs/2210.03353v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "UU-Tax at SemEval-2022 Task 3: Improving the generalizability of\n  language models for taxonomy classification through data augmentation", "abstract": "This paper presents our strategy to address the SemEval-2022 Task 3 PreTENS:\nPresupposed Taxonomies Evaluating Neural Network Semantics. The goal of the\ntask is to identify if a sentence is deemed acceptable or not, depending on the\ntaxonomic relationship that holds between a noun pair contained in the\nsentence. For sub-task 1 -- binary classification -- we propose an effective\nway to enhance the robustness and the generalizability of language models for\nbetter classification on this downstream task. We design a two-stage\nfine-tuning procedure on the ELECTRA language model using data augmentation\ntechniques. Rigorous experiments are carried out using multi-task learning and\ndata-enriched fine-tuning. Experimental results demonstrate that our proposed\nmodel, UU-Tax, is indeed able to generalize well for our downstream task. For\nsub-task 2 -- regression -- we propose a simple classifier that trains on\nfeatures obtained from Universal Sentence Encoder (USE). In addition to\ndescribing the submitted systems, we discuss other experiments that employ\npre-trained language models and data augmentation techniques. For both\nsub-tasks, we perform error analysis to further understand the behaviour of the\nproposed models. We achieved a global F1_Binary score of 91.25% in sub-task 1\nand a rho score of 0.221 in sub-task 2.", "published": "2022-10-07 07:41:28", "link": "http://arxiv.org/abs/2210.03378v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Event Extraction: A Survey", "abstract": "Extracting the reported events from text is one of the key research themes in\nnatural language processing. This process includes several tasks such as event\ndetection, argument extraction, role labeling. As one of the most important\ntopics in natural language processing and natural language understanding, the\napplications of event extraction spans across a wide range of domains such as\nnewswire, biomedical domain, history and humanity, and cyber security. This\nreport presents a comprehensive survey for event detection from textual\ndocuments. In this report, we provide the task definition, the evaluation\nmethod, as well as the benchmark datasets and a taxonomy of methodologies for\nevent extraction. We also present our vision of future research direction in\nevent detection.", "published": "2022-10-07 09:36:44", "link": "http://arxiv.org/abs/2210.03419v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mutual Learning of Single- and Multi-Channel End-to-End Neural\n  Diarization", "abstract": "Due to the high performance of multi-channel speech processing, we can use\nthe outputs from a multi-channel model as teacher labels when training a\nsingle-channel model with knowledge distillation. To the contrary, it is also\nknown that single-channel speech data can benefit multi-channel models by\nmixing it with multi-channel speech data during training or by using it for\nmodel pretraining. This paper focuses on speaker diarization and proposes to\nconduct the above bi-directional knowledge transfer alternately. We first\nintroduce an end-to-end neural diarization model that can handle both single-\nand multi-channel inputs. Using this model, we alternately conduct i) knowledge\ndistillation from a multi-channel model to a single-channel model and ii)\nfinetuning from the distilled single-channel model to a multi-channel model.\nExperimental results on two-speaker data show that the proposed method mutually\nimproved single- and multi-channel speaker diarization performances.", "published": "2022-10-07 11:03:32", "link": "http://arxiv.org/abs/2210.03459v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synthetic Voice Detection and Audio Splicing Detection using\n  SE-Res2Net-Conformer Architecture", "abstract": "Synthetic voice and splicing audio clips have been generated to spoof\nInternet users and artificial intelligence (AI) technologies such as voice\nauthentication. Existing research work treats spoofing countermeasures as a\nbinary classification problem: bonafide vs. spoof. This paper extends the\nexisting Res2Net by involving the recent Conformer block to further exploit the\nlocal patterns on acoustic features. Experimental results on ASVspoof 2019\ndatabase show that the proposed SE-Res2Net-Conformer architecture is able to\nimprove the spoofing countermeasures performance for the logical access\nscenario.\n  In addition, this paper also proposes to re-formulate the existing audio\nsplicing detection problem. Instead of identifying the complete splicing\nsegments, it is more useful to detect the boundaries of the spliced segments.\nMoreover, a deep learning approach can be used to solve the problem, which is\ndifferent from the previous signal processing techniques.", "published": "2022-10-07 14:30:13", "link": "http://arxiv.org/abs/2210.03581v2", "categories": ["eess.AS", "cs.CL", "cs.CR", "I.2.7"], "primary_category": "eess.AS"}
{"title": "C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual\n  Text-Video Retrieval", "abstract": "Multilingual text-video retrieval methods have improved significantly in\nrecent years, but the performance for other languages lags behind English. We\npropose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve\nmultilingual text-video retrieval. Inspired by the fact that English text-video\nretrieval outperforms other languages, we train a student model using input\ntext in different languages to match the cross-modal predictions from teacher\nmodels using input text in English. We propose a cross entropy based objective\nwhich forces the distribution over the student's text-video similarity scores\nto be similar to those of the teacher models. We introduce a new multilingual\nvideo dataset, Multi-YouCook2, by translating the English captions in the\nYouCook2 video dataset to 8 other languages. Our method improves multilingual\ntext-video retrieval performance on Multi-YouCook2 and several other datasets\nsuch as Multi-MSRVTT and VATEX. We also conducted an analysis on the\neffectiveness of different multilingual text models as teachers. The code,\nmodels, and dataset are available at https://github.com/roudimit/c2kd.", "published": "2022-10-07 15:30:24", "link": "http://arxiv.org/abs/2210.03625v2", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "LLMEffiChecker: Understanding and Testing Efficiency Degradation of\n  Large Language Models", "abstract": "In this paper, we make the first attempt to understand and test potential\ncomputation efficiency robustness in state-of-the-art LLMs. By analyzing the\nworking mechanism and implementation of 20,543 public-accessible LLMs, we\nobserve a fundamental property in LLMs that could be manipulated in an\nadversarial manner to reduce computation efficiency significantly. Our key\nmotivation is to generate test inputs that could sufficiently delay the\ngeneration of EOS such that LLMs would have to go through enough iterations to\nsatisfy the pre-configured threshold. We present \\tool, which can work under\nboth white-box setting and black-box setting. In the white-box scenario, \\tool\ndevelops a gradient-guided technique that searches for a minimal and\nunnoticeable perturbation at character-level, token-level, and structure-level.\nIn the black-box scenario, \\tool employs a causal inference-based approach to\nfind critical tokens and similarly applies three levels of imperceptible\nperturbation to them. Both the white-box and black-box settings effectively\ndelay the appearance of EOS, compelling these inputs to reach the\nnaturally-unreachable threshold. To demonstrate the effectiveness of \\tool, we\nconduct a systematic evaluation on nine public-available LLMs: Google T5,\nAllenAI WMT14, Helsinki-NLP translator, Facebook FairSeq, UNICAMP-DL\ntranslator, MarianMT, Google FLAN-T5, MBZUAI LaMini-GPT and Salesforce CodeGen.\nExperimental results show that \\tool can increase on average LLMs' response\nlatency and energy consumption by 325\\% to 3244\\% and 344\\% to 3616\\%,\nrespectively, by perturbing just one character or token in the input sentence.", "published": "2022-10-07 17:01:01", "link": "http://arxiv.org/abs/2210.03696v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "xDBTagger: Explainable Natural Language Interface to Databases Using\n  Keyword Mappings and Schema Graph", "abstract": "Translating natural language queries (NLQ) into structured query language\n(SQL) in interfaces to relational databases is a challenging task that has been\nwidely studied by researchers from both the database and natural language\nprocessing communities. Numerous works have been proposed to attack the natural\nlanguage interfaces to databases (NLIDB) problem either as a conventional\npipeline-based or an end-to-end deep-learning-based solution. Nevertheless,\nregardless of the approach preferred, such solutions exhibit black-box nature,\nwhich makes it difficult for potential users targeted by these systems to\ncomprehend the decisions made to produce the translated SQL. To this end, we\npropose xDBTagger, an explainable hybrid translation pipeline that explains the\ndecisions made along the way to the user both textually and visually. We also\nevaluate xDBTagger quantitatively in three real-world relational databases. The\nevaluation results indicate that in addition to being fully interpretable,\nxDBTagger is effective in terms of accuracy and translates the queries more\nefficiently compared to other state-of-the-art pipeline-based systems up to\n10000 times.", "published": "2022-10-07 18:17:09", "link": "http://arxiv.org/abs/2210.03768v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.HC", "H.2.6; H.5.2; I.2.7"], "primary_category": "cs.DB"}
{"title": "An Analysis of the Effects of Decoding Algorithms on Fairness in\n  Open-Ended Language Generation", "abstract": "Several prior works have shown that language models (LMs) can generate text\ncontaining harmful social biases and stereotypes. While decoding algorithms\nplay a central role in determining properties of LM generated text, their\nimpact on the fairness of the generations has not been studied. We present a\nsystematic analysis of the impact of decoding algorithms on LM fairness, and\nanalyze the trade-off between fairness, diversity and quality. Our experiments\nwith top-$p$, top-$k$ and temperature decoding algorithms, in open-ended\nlanguage generation, show that fairness across demographic groups changes\nsignificantly with change in decoding algorithm's hyper-parameters. Notably,\ndecoding algorithms that output more diverse text also output more texts with\nnegative sentiment and regard. We present several findings and provide\nrecommendations on standardized reporting of decoding details in fairness\nevaluations and optimization of decoding algorithms for fairness alongside\nquality and diversity.", "published": "2022-10-07 21:33:34", "link": "http://arxiv.org/abs/2210.03826v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "The PerspectiveLiberator -- an upmixing 6DoF rendering plugin for\n  single-perspective Ambisonic room impulse responses", "abstract": "Nowadays, virtual reality interfaces allow the user to change perspectives in\nsix degrees of freedom (6DoF) virtually, and consistently with the visual part,\nthe acoustic perspective needs to be updated interactively. Single-perspective\nrendering with dynamic head rotation already works quite reliably with upmixed\nfirst-order Ambisonic room impulse responses (ASDM, SIRR, etc.). This\ncontribution presents a plugin to free the virtual perspective from the\nmeasured one by real-time perspective extrapolation: The PerspectiveLiberator.\nThe plugin permits selecting between two different algorithms for directional\nresolution enhancement (ASDM, 4DE). And for its main task of convolution-based\n6DoF rendering, the plugin detects and localizes prominent directional sound\nevents in the early Ambisonic room impulse response and re-encodes them with\ndirection, time of arrival, and level adapted to the variable perspective of\nthe virtual listener. The diffuse residual is enhanced in directional\nresolution but remains unaffected by translatory movement to preserve as much\nof the original room impression as possible.", "published": "2022-10-07 07:11:01", "link": "http://arxiv.org/abs/2210.03360v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Model-based estimation of in-car-communication feedback applied to\n  speech zone detection", "abstract": "Modern cars provide versatile tools to enhance speech communication. While an\nin-car communication (ICC) system aims at enhancing communication between the\npassengers by playing back desired speech via loudspeakers in the car, these\nloudspeaker signals may disturb a speech enhancement system required for\nhands-free telephony and automatic speech recognition. In this paper, we focus\non speech zone detection, i.e. detecting which passenger in the car is\nspeaking, which is a crucial component of the speech enhancement system. We\npropose a model-based feedback estimation method to improve robustness of\nspeech zone detection against ICC feedback. Specifically, since the zone\ndetection system typically does not have access to the ICC loudspeaker signals,\nthe proposed method estimates the feedback signal from the observed microphone\nsignals based on a free-field propagation model between the loudspeakers and\nthe microphones as well as the ICC gain. We propose an efficient recursive\nimplementation in the short-time Fourier transform domain using convolutive\ntransfer functions. A realistic simulation study indicates that the proposed\nmethod allows to increase the ICC gain by about 6dB while still achieving\nrobust speech zone detection results.", "published": "2022-10-07 07:13:06", "link": "http://arxiv.org/abs/2210.03363v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Supervised and Unsupervised Learning of Audio Representations for Music\n  Understanding", "abstract": "In this work, we provide a broad comparative analysis of strategies for\npre-training audio understanding models for several tasks in the music domain,\nincluding labelling of genre, era, origin, mood, instrumentation, key, pitch,\nvocal characteristics, tempo and sonority. Specifically, we explore how the\ndomain of pre-training datasets (music or generic audio) and the pre-training\nmethodology (supervised or unsupervised) affects the adequacy of the resulting\naudio embeddings for downstream tasks.\n  We show that models trained via supervised learning on large-scale\nexpert-annotated music datasets achieve state-of-the-art performance in a wide\nrange of music labelling tasks, each with novel content and vocabularies. This\ncan be done in an efficient manner with models containing less than 100 million\nparameters that require no fine-tuning or reparameterization for downstream\ntasks, making this approach practical for industry-scale audio catalogs.\n  Within the class of unsupervised learning strategies, we show that the domain\nof the training dataset can significantly impact the performance of\nrepresentations learned by the model. We find that restricting the domain of\nthe pre-training dataset to music allows for training with smaller batch sizes\nwhile achieving state-of-the-art in unsupervised learning -- and in some cases,\nsupervised learning -- for music understanding.\n  We also corroborate that, while achieving state-of-the-art performance on\nmany tasks, supervised learning can cause models to specialize to the\nsupervised information provided, somewhat compromising a model's generality.", "published": "2022-10-07 20:07:35", "link": "http://arxiv.org/abs/2210.03799v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
