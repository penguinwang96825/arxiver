{"title": "Nutribullets Hybrid: Multi-document Health Summarization", "abstract": "We present a method for generating comparative summaries that highlights\nsimilarities and contradictions in input documents. The key challenge in\ncreating such summaries is the lack of large parallel training data required\nfor training typical summarization systems. To this end, we introduce a hybrid\ngeneration approach inspired by traditional concept-to-text systems. To enable\naccurate comparison between different sources, the model first learns to\nextract pertinent relations from input documents. The content planning\ncomponent uses deterministic operators to aggregate these relations after\nidentifying a subset for inclusion into a summary. The surface realization\ncomponent lexicalizes this information using a text-infilling language model.\nBy separately modeling content selection and realization, we can effectively\ntrain them with limited annotations. We implemented and tested the model in the\ndomain of nutrition and health -- rife with inconsistencies. Compared to\nconventional methods, our framework leads to more faithful, relevant and\naggregation-sensitive summarization -- while being equally fluent.", "published": "2021-04-08 01:44:29", "link": "http://arxiv.org/abs/2104.03465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Simple Neural Probabilistic Language Models", "abstract": "Recent progress in language modeling has been driven not only by advances in\nneural architectures, but also through hardware and optimization improvements.\nIn this paper, we revisit the neural probabilistic language model (NPLM)\nof~\\citet{Bengio2003ANP}, which simply concatenates word embeddings within a\nfixed window and passes the result through a feed-forward network to predict\nthe next word. When scaled up to modern hardware, this model (despite its many\nlimitations) performs much better than expected on word-level language model\nbenchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a\nbaseline Transformer with short input contexts but struggles to handle\nlong-term dependencies. Inspired by this result, we modify the Transformer by\nreplacing its first self-attention layer with the NPLM's local concatenation\nlayer, which results in small but consistent perplexity decreases across three\nword-level language modeling datasets.", "published": "2021-04-08 02:18:47", "link": "http://arxiv.org/abs/2104.03474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Complexity Probing via Finding Subnetworks", "abstract": "The dominant approach in probing neural networks for linguistic properties is\nto train a new shallow multi-layer perceptron (MLP) on top of the model's\ninternal representations. This approach can detect properties encoded in the\nmodel, but at the cost of adding new parameters that may learn the task\ndirectly. We instead propose a subtractive pruning-based probe, where we find\nan existing subnetwork that performs the linguistic task of interest. Compared\nto an MLP, the subnetwork probe achieves both higher accuracy on pre-trained\nmodels and lower accuracy on random models, so it is both better at finding\nproperties of interest and worse at learning on its own. Next, by varying the\ncomplexity of each probe, we show that subnetwork probing Pareto-dominates MLP\nprobing in that it achieves higher accuracy given any budget of probe\ncomplexity. Finally, we analyze the resulting subnetworks across various tasks\nto locate where each task is encoded, and we find that lower-level tasks are\ncaptured in lower layers, reproducing similar findings in past work.", "published": "2021-04-08 05:11:21", "link": "http://arxiv.org/abs/2104.03514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User-Generated Text Corpus for Evaluating Japanese Morphological\n  Analysis and Lexical Normalization", "abstract": "Morphological analysis (MA) and lexical normalization (LN) are both important\ntasks for Japanese user-generated text (UGT). To evaluate and compare different\nMA/LN systems, we have constructed a publicly available Japanese UGT corpus.\nOur corpus comprises 929 sentences annotated with morphological and\nnormalization information, along with category information we classified for\nfrequent UGT-specific phenomena. Experiments on the corpus demonstrated the low\nperformance of existing MA/LN methods for non-general words and non-standard\nforms, indicating that the corpus would be a challenging benchmark for further\nresearch on UGT.", "published": "2021-04-08 05:53:46", "link": "http://arxiv.org/abs/2104.03523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BSTC: A Large-Scale Chinese-English Speech Translation Dataset", "abstract": "This paper presents BSTC (Baidu Speech Translation Corpus), a large-scale\nChinese-English speech translation dataset. This dataset is constructed based\non a collection of licensed videos of talks or lectures, including about 68\nhours of Mandarin data, their manual transcripts and translations into English,\nas well as automated transcripts by an automatic speech recognition (ASR)\nmodel. We have further asked three experienced interpreters to simultaneously\ninterpret the testing talks in a mock conference setting. This corpus is\nexpected to promote the research of automatic simultaneous translation as well\nas the development of practical systems. We have organized simultaneous\ntranslation tasks and used this corpus to evaluate automatic simultaneous\ntranslation systems.", "published": "2021-04-08 07:38:51", "link": "http://arxiv.org/abs/2104.03575v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Should Go First? A Self-Supervised Concept Sorting Model for\n  Improving Taxonomy Expansion", "abstract": "Taxonomies have been widely used in various machine learning and text mining\nsystems to organize knowledge and facilitate downstream tasks. One critical\nchallenge is that, as data and business scope grow in real applications,\nexisting taxonomies need to be expanded to incorporate new concepts. Previous\nworks on taxonomy expansion process the new concepts independently and\nsimultaneously, ignoring the potential relationships among them and the\nappropriate order of inserting operations. However, in reality, the new\nconcepts tend to be mutually correlated and form local hypernym-hyponym\nstructures. In such a scenario, ignoring the dependencies of new concepts and\nthe order of insertion may trigger error propagation. For example, existing\ntaxonomy expansion systems may insert hyponyms to existing taxonomies before\ntheir hypernym, leading to sub-optimal expanded taxonomies. To complement\nexisting taxonomy expansion systems, we propose TaxoOrder, a novel\nself-supervised framework that simultaneously discovers the local\nhypernym-hyponym structure among new concepts and decides the order of\ninsertion. TaxoOrder can be directly plugged into any taxonomy expansion system\nand improve the quality of expanded taxonomies. Experiments on the real-world\ndataset validate the effectiveness of TaxoOrder to enhance taxonomy expansion\nsystems, leading to better-resulting taxonomies with comparison to baselines\nunder various evaluation metrics.", "published": "2021-04-08 11:00:43", "link": "http://arxiv.org/abs/2104.03682v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Languages for Smart and Computable Contracts", "abstract": "Smart Contracts use computer technology to automate the performance of\naspects of commercial agreements. Yet how can there be confidence that the\ncomputer code is faithful to the intentions of the parties? To understand the\ndepth and subtlety of this question requires an exploration of natural and\ncomputer languages, of the semantics of expressions in those languages, and of\nthe gap that exists between the disciplines of law and computer science. Here\nwe provide a perspective on some of the key issues, explore some current\nresearch directions, and explain the importance of language design in the\ndevelopment of reliable Smart Contracts, including the specific methodology of\nComputable Contracts.", "published": "2021-04-08 13:32:17", "link": "http://arxiv.org/abs/2104.03764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uppsala NLP at SemEval-2021 Task 2: Multilingual Language Models for\n  Fine-tuning and Feature Extraction in Word-in-Context Disambiguation", "abstract": "We describe the Uppsala NLP submission to SemEval-2021 Task 2 on multilingual\nand cross-lingual word-in-context disambiguation. We explore the usefulness of\nthree pre-trained multilingual language models, XLM-RoBERTa (XLMR),\nMultilingual BERT (mBERT) and multilingual distilled BERT (mDistilBERT). We\ncompare these three models in two setups, fine-tuning and as feature\nextractors. In the second case we also experiment with using dependency-based\ninformation. We find that fine-tuning is better than feature extraction. XLMR\nperforms better than mBERT in the cross-lingual setting both with fine-tuning\nand feature extraction, whereas these two models give a similar performance in\nthe multilingual setting. mDistilBERT performs poorly with fine-tuning but\ngives similar results to the other models when used as a feature extractor. We\nsubmitted our two best systems, fine-tuned with XLMR and mBERT.", "published": "2021-04-08 13:40:41", "link": "http://arxiv.org/abs/2104.03767v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistically significant detection of semantic shifts using contextual\n  word embeddings", "abstract": "Detecting lexical semantic change in smaller data sets, e.g. in historical\nlinguistics and digital humanities, is challenging due to a lack of statistical\npower. This issue is exacerbated by non-contextual embedding models that\nproduce one embedding per word and, therefore, mask the variability present in\nthe data. In this article, we propose an approach to estimate semantic shift by\ncombining contextual word embeddings with permutation-based statistical tests.\nWe use the false discovery rate procedure to address the large number of\nhypothesis tests being conducted simultaneously. We demonstrate the performance\nof this approach in simulation where it achieves consistently high precision by\nsuppressing false positives. We additionally analyze real-world data from\nSemEval-2020 Task 1 and the Liverpool FC subreddit corpus. We show that by\ntaking sample variation into account, we can improve the robustness of\nindividual semantic shift estimates without degrading overall performance.", "published": "2021-04-08 13:58:54", "link": "http://arxiv.org/abs/2104.03776v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Relationship Between Algorithm Performance, Vocabulary,\n  and Run-Time in Text Classification", "abstract": "Text classification is a significant branch of natural language processing,\nand has many applications including document classification and sentiment\nanalysis. Unsurprisingly, those who do text classification are concerned with\nthe run-time of their algorithms, many of which depend on the size of the\ncorpus' vocabulary due to their bag-of-words representation. Although many\nstudies have examined the effect of preprocessing techniques on vocabulary size\nand accuracy, none have examined how these methods affect a model's run-time.\nTo fill this gap, we provide a comprehensive study that examines how\npreprocessing techniques affect the vocabulary size, model performance, and\nmodel run-time, evaluating ten techniques over four models and two datasets. We\nshow that some individual methods can reduce run-time with no loss of accuracy,\nwhile some combinations of methods can trade 2-5% of the accuracy for up to a\n65% reduction of run-time. Furthermore, some combinations of preprocessing\ntechniques can even provide a 15% reduction in run-time while simultaneously\nimproving model accuracy.", "published": "2021-04-08 15:49:59", "link": "http://arxiv.org/abs/2104.03848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing BERT in Hyperbolic Spaces", "abstract": "Recently, a variety of probing tasks are proposed to discover linguistic\nproperties learned in contextualized word embeddings. Many of these works\nimplicitly assume these embeddings lay in certain metric spaces, typically the\nEuclidean space. This work considers a family of geometrically special spaces,\nthe hyperbolic spaces, that exhibit better inductive biases for hierarchical\nstructures and may better reveal linguistic hierarchies encoded in\ncontextualized representations. We introduce a Poincare probe, a structural\nprobe projecting these embeddings into a Poincare subspace with explicitly\ndefined hierarchies. We focus on two probing objectives: (a) dependency trees\nwhere the hierarchy is defined as head-dependent structures; (b) lexical\nsentiments where the hierarchy is defined as the polarity of words (positivity\nand negativity). We argue that a key desideratum of a probe is its sensitivity\nto the existence of linguistic structures. We apply our probes on BERT, a\ntypical contextualized embedding model. In a syntactic subspace, our probe\nbetter recovers tree structures than Euclidean probes, revealing the\npossibility that the geometry of BERT syntax may not necessarily be Euclidean.\nIn a sentiment subspace, we reveal two possible meta-embeddings for positive\nand negative sentiments and show how lexically-controlled contextualization\nwould change the geometric localization of embeddings. We demonstrate the\nfindings with our Poincare probe via extensive experiments and visualization.\nOur results can be reproduced at https://github.com/FranxYao/PoincareProbe.", "published": "2021-04-08 16:24:53", "link": "http://arxiv.org/abs/2104.03869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVID-19 Named Entity Recognition for Vietnamese", "abstract": "The current COVID-19 pandemic has lead to the creation of many corpora that\nfacilitate NLP research and downstream applications to help fight the pandemic.\nHowever, most of these corpora are exclusively for English. As the pandemic is\na global problem, it is worth creating COVID-19 related datasets for languages\nother than English. In this paper, we present the first manually-annotated\nCOVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is\nannotated for the named entity recognition (NER) task with newly-defined entity\ntypes that can be used in other future epidemics. Our dataset also contains the\nlargest number of entities compared to existing Vietnamese NER datasets. We\nempirically conduct experiments using strong baselines on our dataset, and find\nthat: automatic Vietnamese word segmentation helps improve the NER results and\nthe highest performances are obtained by fine-tuning pre-trained language\nmodels where the monolingual model PhoBERT for Vietnamese (Nguyen and Nguyen,\n2020) produces higher results than the multilingual model XLM-R (Conneau et\nal., 2020). We publicly release our dataset at:\nhttps://github.com/VinAIResearch/PhoNER_COVID19", "published": "2021-04-08 16:35:34", "link": "http://arxiv.org/abs/2104.03879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Biasing Transformer Attention Towards Monotonicity", "abstract": "Many sequence-to-sequence tasks in natural language processing are roughly\nmonotonic in the alignment between source and target sequence, and previous\nwork has facilitated or enforced learning of monotonic attention behavior via\nspecialized attention functions or pretraining. In this work, we introduce a\nmonotonicity loss function that is compatible with standard attention\nmechanisms and test it on several sequence-to-sequence tasks:\ngrapheme-to-phoneme conversion, morphological inflection, transliteration, and\ndialect normalization. Experiments show that we can achieve largely monotonic\nbehavior. Performance is mixed, with larger gains on top of RNN baselines.\nGeneral monotonicity does not benefit transformer multihead attention, however,\nwe see isolated improvements when only a subset of heads is biased towards\nmonotonic behavior.", "published": "2021-04-08 17:42:05", "link": "http://arxiv.org/abs/2104.03945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AlephBERT:A Hebrew Large Pre-Trained Language Model to Start-off your\n  Hebrew NLP Application With", "abstract": "Large Pre-trained Language Models (PLMs) have become ubiquitous in the\ndevelopment of language understanding technology and lie at the heart of many\nartificial intelligence advances. While advances reported for English using\nPLMs are unprecedented, reported advances using PLMs in Hebrew are few and far\nbetween. The problem is twofold. First, Hebrew resources available for training\nNLP models are not at the same order of magnitude as their English\ncounterparts. Second, there are no accepted tasks and benchmarks to evaluate\nthe progress of Hebrew PLMs on. In this work we aim to remedy both aspects.\nFirst, we present AlephBERT, a large pre-trained language model for Modern\nHebrew, which is trained on larger vocabulary and a larger dataset than any\nHebrew PLM before. Second, using AlephBERT we present new state-of-the-art\nresults on multiple Hebrew tasks and benchmarks, including: Segmentation,\nPart-of-Speech Tagging, full Morphological Tagging, Named-Entity Recognition\nand Sentiment Analysis. We make our AlephBERT model publicly available,\nproviding a single point of entry for the development of Hebrew NLP\napplications.", "published": "2021-04-08 20:51:29", "link": "http://arxiv.org/abs/2104.04052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lone Pine at SemEval-2021 Task 5: Fine-Grained Detection of Hate Speech\n  Using BERToxic", "abstract": "This paper describes our approach to the Toxic Spans Detection problem\n(SemEval-2021 Task 5). We propose BERToxic, a system that fine-tunes a\npre-trained BERT model to locate toxic text spans in a given text and utilizes\nadditional post-processing steps to refine the boundaries. The post-processing\nsteps involve (1) labeling character offsets between consecutive toxic tokens\nas toxic and (2) assigning a toxic label to words that have at least one token\nlabeled as toxic. Through experiments, we show that these two post-processing\nsteps improve the performance of our model by 4.16% on the test set. We also\nstudied the effects of data augmentation and ensemble modeling strategies on\nour system. Our system significantly outperformed the provided baseline and\nachieved an F1-score of 0.683, placing Lone Pine in the 17th place out of 91\nteams in the competition. Our code is made available at\nhttps://github.com/Yakoob-Khan/Toxic-Spans-Detection", "published": "2021-04-08 04:46:14", "link": "http://arxiv.org/abs/2104.03506v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extended Parallel Corpus for Amharic-English Machine Translation", "abstract": "This paper describes the acquisition, preprocessing, segmentation, and\nalignment of an Amharic-English parallel corpus. It will be helpful for machine\ntranslation of a low-resource language, Amharic. We freely released the corpus\nfor research purposes. Furthermore, we developed baseline statistical and\nneural machine translation systems; we trained statistical and neural machine\ntranslation models using the corpus. In the experiments, we also used a large\nmonolingual corpus for the language model of statistical machine translation\nand back-translation of neural machine translation. In the automatic\nevaluation, neural machine translation models outperform statistical machine\ntranslation models by approximately six to seven Bilingual Evaluation\nUnderstudy (BLEU) points. Besides, among the neural machine translation models,\nthe subword models outperform the word-based models by three to four BLEU\npoints. Moreover, two other relevant automatic evaluation metrics, Translation\nEdit Rate on Character Level and Better Evaluation as Ranking, reflect\ncorresponding differences among the trained models.", "published": "2021-04-08 06:51:08", "link": "http://arxiv.org/abs/2104.03543v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Simple Geometric Method for Cross-Lingual Linguistic Transformations\n  with Pre-trained Autoencoders", "abstract": "Powerful sentence encoders trained for multiple languages are on the rise.\nThese systems are capable of embedding a wide range of linguistic properties\ninto vector representations. While explicit probing tasks can be used to verify\nthe presence of specific linguistic properties, it is unclear whether the\nvector representations can be manipulated to indirectly steer such properties.\nFor efficient learning, we investigate the use of a geometric mapping in\nembedding space to transform linguistic properties, without any tuning of the\npre-trained sentence encoder or decoder. We validate our approach on three\nlinguistic properties using a pre-trained multilingual autoencoder and analyze\nthe results in both monolingual and cross-lingual settings.", "published": "2021-04-08 09:33:50", "link": "http://arxiv.org/abs/2104.03630v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Proposal for an Interactive Shell Based on a Typed Lambda Calculus", "abstract": "This paper presents Favalon, a functional programming language built on the\npremise of a lambda calculus for use as an interactive shell replacement.\nFavalon seamlessly integrates with typed versions of existing libraries and\ncommands using type inference, flexible runtime type metadata, and the same\ntechniques employed by shells to link commands together. Much of Favalon's\nsyntax is customizable via user-defined functions, allowing it to be extended\nby anyone who is familiar with a command-line shell. Furthermore, Favalon's\ntype inference engine can be separated from its runtime library and easily\nrepurposed for other applications.", "published": "2021-04-08 10:46:28", "link": "http://arxiv.org/abs/2104.03678v1", "categories": ["cs.PL", "cs.CL", "D.3.2; D.4.9"], "primary_category": "cs.PL"}
{"title": "Video Question Answering with Phrases via Semantic Roles", "abstract": "Video Question Answering (VidQA) evaluation metrics have been limited to a\nsingle-word answer or selecting a phrase from a fixed set of phrases. These\nmetrics limit the VidQA models' application scenario. In this work, we leverage\nsemantic roles derived from video descriptions to mask out certain phrases, to\nintroduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable\nevaluation of answer phrases, we compute the relative improvement of the\npredicted answer compared to an empty string. To reduce the influence of\nlanguage bias in VidQA datasets, we retrieve a video having a different answer\nfor the same question. To facilitate research, we construct ActivityNet-SRL-QA\nand Charades-SRL-QA and benchmark them by extending three vision-language\nmodels. We further perform extensive analysis and ablative studies to guide\nfuture work.", "published": "2021-04-08 13:27:43", "link": "http://arxiv.org/abs/2104.03762v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "How Metaphors Impact Political Discourse: A Large-Scale Topic-Agnostic\n  Study Using Neural Metaphor Detection", "abstract": "Metaphors are widely used in political rhetoric as an effective framing\ndevice. While the efficacy of specific metaphors such as the war metaphor in\npolitical discourse has been documented before, those studies often rely on\nsmall number of hand-coded instances of metaphor use. Larger-scale\ntopic-agnostic studies are required to establish the general persuasiveness of\nmetaphors as a device, and to shed light on the broader patterns that guide\ntheir persuasiveness. In this paper, we present a large-scale data-driven study\nof metaphors used in political discourse. We conduct this study on a publicly\navailable dataset of over 85K posts made by 412 US politicians in their\nFacebook public pages, up until Feb 2017. Our contributions are threefold: we\nshow evidence that metaphor use correlates with ideological leanings in complex\nways that depend on concurrent political events such as winning or losing\nelections; we show that posts with metaphors elicit more engagement from their\naudience overall even after controlling for various socio-political factors\nsuch as gender and political party affiliation; and finally, we demonstrate\nthat metaphoricity is indeed the reason for increased engagement of posts,\nthrough a fine-grained linguistic analysis of metaphorical vs. literal usages\nof 513 words across 70K posts.", "published": "2021-04-08 17:16:31", "link": "http://arxiv.org/abs/2104.03928v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Machine Learning Based on Natural Language Processing to Detect Cardiac\n  Failure in Clinical Narratives", "abstract": "The purpose of the study presented herein is to develop a machine learning\nalgorithm based on natural language processing that automatically detects\nwhether a patient has a cardiac failure or a healthy condition by using\nphysician notes in Research Data Warehouse at CHU Sainte Justine Hospital.\nFirst, a word representation learning technique was employed by using\nbag-of-word (BoW), term frequency inverse document frequency (TFIDF), and\nneural word embeddings (word2vec). Each representation technique aims to retain\nthe words semantic and syntactic analysis in critical care data. It helps to\nenrich the mutual information for the word representation and leads to an\nadvantage for further appropriate analysis steps. Second, a machine learning\nclassifier was used to detect the patients condition for either cardiac failure\nor stable patient through the created word representation vector space from the\nprevious step. This machine learning approach is based on a supervised binary\nclassification algorithm, including logistic regression (LR), Gaussian\nNaive-Bayes (GaussianNB), and multilayer perceptron neural network (MLPNN).\nTechnically, it mainly optimizes the empirical loss during training the\nclassifiers. As a result, an automatic learning algorithm would be accomplished\nto draw a high classification performance, including accuracy (acc), precision\n(pre), recall (rec), and F1 score (f1). The results show that the combination\nof TFIDF and MLPNN always outperformed other combinations with all overall\nperformance. In the case without any feature selection, the proposed framework\nyielded an overall classification performance with acc, pre, rec, and f1 of 84%\nand 82%, 85%, and 83%, respectively. Significantly, if the feature selection\nwas well applied, the overall performance would finally improve up to 4% for\neach evaluation.", "published": "2021-04-08 17:28:43", "link": "http://arxiv.org/abs/2104.03934v2", "categories": ["cs.CL", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Detecting of a Patient's Condition From Clinical Narratives Using\n  Natural Language Representation", "abstract": "The rapid progress in clinical data management systems and artificial\nintelligence approaches enable the era of personalized medicine. Intensive care\nunits (ICUs) are the ideal clinical research environment for such development\nbecause they collect many clinical data and are highly computerized\nenvironments. We designed a retrospective clinical study on a prospective ICU\ndatabase using clinical natural language to help in the early diagnosis of\nheart failure in critically ill children. The methodology consisted of\nempirical experiments of a learning algorithm to learn the hidden\ninterpretation and presentation of the French clinical note data. This study\nincluded 1386 patients' clinical notes with 5444 single lines of notes. There\nwere 1941 positive cases (36 % of total) and 3503 negative cases classified by\ntwo independent physicians using a standardized approach. The multilayer\nperceptron neural network outperforms other discriminative and generative\nclassifiers. Consequently, the proposed framework yields an overall\nclassification performance with 89 % accuracy, 88 % recall, and 89 % precision.\nThis study successfully applied learning representation and machine learning\nalgorithms to detect heart failure from clinical natural language in a single\nFrench institution. Further work is needed to use the same methodology in other\ninstitutions and other languages.", "published": "2021-04-08 17:16:04", "link": "http://arxiv.org/abs/2104.03969v7", "categories": ["cs.CL", "eess.SP"], "primary_category": "cs.CL"}
{"title": "A Sketch-Based Neural Model for Generating Commit Messages from Diffs", "abstract": "Commit messages have an important impact in software development, especially\nwhen working in large teams. Multiple developers who have a different style of\nwriting may often be involved in the same project. For this reason, it may be\ndifficult to maintain a strict pattern of writing informative commit messages,\nwith the most frequent issue being that these messages are not descriptive\nenough. In this paper we apply neural machine translation (NMT) techniques to\nconvert code diffs into commit messages and we present an improved sketch-based\nencoder for this task. We split the approach into three parts. Firstly, we\nfocus on finding a more suitable NMT baseline for this problem. Secondly, we\nshow that the performance of the NMT models can be improved by training on\nexamples containing a specific file type. Lastly, we introduce a novel\nsketch-based neural model inspired by recent approaches used for code\ngeneration and we show that the sketch-based encoder significantly outperforms\nexisting state of the art solutions. The results highlight that this\nimprovement is relevant especially for Java source code files, by examining two\ndifferent datasets introduced in recent years for this task.", "published": "2021-04-08 21:21:28", "link": "http://arxiv.org/abs/2104.04087v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "XFORMAL: A Benchmark for Multilingual Formality Style Transfer", "abstract": "We take the first step towards multilingual style transfer by creating and\nreleasing XFORMAL, a benchmark of multiple formal reformulations of informal\ntext in Brazilian Portuguese, French, and Italian. Results on XFORMAL suggest\nthat state-of-the-art style transfer approaches perform close to simple\nbaselines, indicating that style transfer is even more challenging when moving\nmultilingual.", "published": "2021-04-08 23:01:17", "link": "http://arxiv.org/abs/2104.04108v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Multi-Scale Style Control for Expressive Speech Synthesis", "abstract": "This paper introduces a multi-scale speech style modeling method for\nend-to-end expressive speech synthesis. The proposed method employs a\nmulti-scale reference encoder to extract both the global-scale utterance-level\nand the local-scale quasi-phoneme-level style features of the target speech,\nwhich are then fed into the speech synthesis model as an extension to the input\nphoneme sequence. During training time, the multi-scale style model could be\njointly trained with the speech synthesis model in an end-to-end fashion. By\napplying the proposed method to style transfer task, experimental results\nindicate that the controllability of the multi-scale speech style model and the\nexpressiveness of the synthesized speech are greatly improved. Moreover, by\nassigning different reference speeches to extraction of style on each scale,\nthe flexibility of the proposed method is further revealed.", "published": "2021-04-08 05:50:09", "link": "http://arxiv.org/abs/2104.03521v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WNARS: WFST based Non-autoregressive Streaming End-to-End Speech\n  Recognition", "abstract": "Recently, attention-based encoder-decoder (AED) end-to-end (E2E) models have\ndrawn more and more attention in the field of automatic speech recognition\n(ASR). AED models, however, still have drawbacks when deploying in commercial\napplications. Autoregressive beam search decoding makes it inefficient for\nhigh-concurrency applications. It is also inconvenient to integrate external\nword-level language models. The most important thing is that AED models are\ndifficult for streaming recognition due to global attention mechanism. In this\npaper, we propose a novel framework, namely WNARS, using hybrid CTC-attention\nAED models and weighted finite-state transducers (WFST) to solve these problems\ntogether. We switch from autoregressive beam search to CTC branch decoding,\nwhich performs first-pass decoding with WFST in chunk-wise streaming way. The\ndecoder branch then performs second-pass rescoring on the generated hypotheses\nnon-autoregressively. On the AISHELL-1 task, our WNARS achieves a character\nerror rate of 5.22% with 640ms latency, to the best of our knowledge, which is\nthe state-of-the-art performance for online ASR. Further experiments on our\n10,000-hour Mandarin task show the proposed method achieves more than 20%\nimprovements with 50% latency compared to a strong TDNN-BLSTM lattice-free MMI\nbaseline.", "published": "2021-04-08 07:56:03", "link": "http://arxiv.org/abs/2104.03587v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Half-Truth: A Partially Fake Audio Detection Dataset", "abstract": "Diverse promising datasets have been designed to hold back the development of\nfake audio detection, such as ASVspoof databases. However, previous datasets\nignore an attacking situation, in which the hacker hides some small fake clips\nin real speech audio. This poses a serious threat since that it is difficult to\ndistinguish the small fake clip from the whole speech utterance. Therefore,\nthis paper develops such a dataset for half-truth audio detection (HAD).\nPartially fake audio in the HAD dataset involves only changing a few words in\nan utterance.The audio of the words is generated with the very latest\nstate-of-the-art speech synthesis technology. We can not only detect fake\nuttrances but also localize manipulated regions in a speech using this dataset.\nSome benchmark results are presented on this dataset. The results show that\npartially fake audio presents much more challenging than fully fake audio for\nfake audio detection. The HAD dataset is publicly available:\nhttps://zenodo.org/records/10377492.", "published": "2021-04-08 08:57:13", "link": "http://arxiv.org/abs/2104.03617v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contextual Semi-Supervised Learning: An Approach To Leverage\n  Air-Surveillance and Untranscribed ATC Data in ASR Systems", "abstract": "Air traffic management and specifically air-traffic control (ATC) rely mostly\non voice communications between Air Traffic Controllers (ATCos) and pilots. In\nmost cases, these voice communications follow a well-defined grammar that could\nbe leveraged in Automatic Speech Recognition (ASR) technologies. The callsign\nused to address an airplane is an essential part of all ATCo-pilot\ncommunications. We propose a two-steps approach to add contextual knowledge\nduring semi-supervised training to reduce the ASR system error rates at\nrecognizing the part of the utterance that contains the callsign. Initially, we\nrepresent in a WFST the contextual knowledge (i.e. air-surveillance data) of an\nATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the\ncontextual knowledge is added by second-pass decoding (i.e. lattice\nre-scoring). Results show that `unseen domains' (e.g. data from airports not\npresent in the supervised training data) are further aided by contextual SSL\nwhen compared to standalone SSL. For this task, we introduce the Callsign Word\nError Rate (CA-WER) as an evaluation metric, which only assesses ASR\nperformance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER\nrelative improvement applying SSL with an additional 17.5% CA-WER improvement\nby adding contextual knowledge during SSL on a challenging ATC-based test set\ngathered from LiveATC.", "published": "2021-04-08 09:53:54", "link": "http://arxiv.org/abs/2104.03643v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enabling Cross-Domain Communication: How to Bridge the Gap between AI\n  and HW Engineers", "abstract": "A key issue in system design is the lack of communication between hardware,\nsoftware and domain expert. Recent research work shows progress in automatic\nHW/SW co-design flows of neural accelerators that seems to make this kind of\ncommunication obsolete. Most real-world systems, however, are a composition of\nmultiple processing units, communication networks and memories. A HW/SW\nco-design process of (reconfigurable) neural accelerators, therefore, is an\nimportant sub-problem towards a common co-design methodology. The ultimate\nchallenge is to define the constraints for the design space exploration on\nsystem level - a task which requires deep knowledge and understanding of\nhardware architectures, mapping of workloads onto hardware and the application\ndomain, e.g. artificial intelligence.\n  For most projects, these skills are distributed among several people or even\ndifferent teams which is one of the major reasons why there is no established\nend-to-end development methodology for digital systems. This position paper\ndiscusses possibilities how to establish such a methodology for systems that\ninclude (reconfigurable) dedicated accelerators and outlines the central role\nthat languages and tools play in the process.", "published": "2021-04-08 14:05:15", "link": "http://arxiv.org/abs/2104.03780v1", "categories": ["cs.AR", "cs.CL", "cs.SE"], "primary_category": "cs.AR"}
{"title": "Exploring Machine Speech Chain for Domain Adaptation and Few-Shot\n  Speaker Adaptation", "abstract": "Machine Speech Chain, which integrates both end-to-end (E2E) automatic speech\nrecognition (ASR) and text-to-speech (TTS) into one circle for joint training,\nhas been proven to be effective in data augmentation by leveraging large\namounts of unpaired data. In this paper, we explore the TTS->ASR pipeline in\nspeech chain to do domain adaptation for both neural TTS and E2E ASR models,\nwith only text data from target domain. We conduct experiments by adapting from\naudiobook domain (LibriSpeech) to presentation domain (TED-LIUM), there is a\nrelative word error rate (WER) reduction of 10% for the E2E ASR model on the\nTED-LIUM test set, and a relative WER reduction of 51.5% in synthetic speech\ngenerated by neural TTS in the presentation domain. Further, we apply few-shot\nspeaker adaptation for the E2E ASR by using a few utterances from target\nspeakers in an unsupervised way, results in additional gains.", "published": "2021-04-08 14:52:37", "link": "http://arxiv.org/abs/2104.03815v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "RNN Transducer Models For Spoken Language Understanding", "abstract": "We present a comprehensive study on building and adapting RNN transducer\n(RNN-T) models for spoken language understanding(SLU). These end-to-end (E2E)\nmodels are constructed in three practical settings: a case where verbatim\ntranscripts are available, a constrained case where the only available\nannotations are SLU labels and their values, and a more restrictive case where\ntranscripts are available but not corresponding audio. We show how RNN-T SLU\nmodels can be developed starting from pre-trained automatic speech recognition\n(ASR) systems, followed by an SLU adaptation step. In settings where real audio\ndata is not available, artificially synthesized speech is used to successfully\nadapt various SLU models. When evaluated on two SLU data sets, the ATIS corpus\nand a customer call center data set, the proposed models closely track the\nperformance of other E2E models and achieve state-of-the-art results.", "published": "2021-04-08 15:35:22", "link": "http://arxiv.org/abs/2104.03842v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dataset Summarization by K Principal Concepts", "abstract": "We propose the new task of K principal concept identification for dataset\nsummarizarion. The objective is to find a set of K concepts that best explain\nthe variation within the dataset. Concepts are high-level human interpretable\nterms such as \"tiger\", \"kayaking\" or \"happy\". The K concepts are selected from\na (potentially long) input list of candidates, which we denote the\nconcept-bank. The concept-bank may be taken from a generic dictionary or\nconstructed by task-specific prior knowledge. An image-language embedding\nmethod (e.g. CLIP) is used to map the images and the concept-bank into a shared\nfeature space. To select the K concepts that best explain the data, we\nformulate our problem as a K-uncapacitated facility location problem. An\nefficient optimization technique is used to scale the local search algorithm to\nvery large concept-banks. The output of our method is a set of K principal\nconcepts that summarize the dataset. Our approach provides a more explicit\nsummary in comparison to selecting K representative images, which are often\nambiguous. As a further application of our method, the K principal concepts can\nbe used to classify the dataset into K groups. Extensive experiments\ndemonstrate the efficacy of our approach.", "published": "2021-04-08 17:54:37", "link": "http://arxiv.org/abs/2104.03952v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "GrASP: A Library for Extracting and Exploring Human-Interpretable\n  Textual Patterns", "abstract": "Data exploration is an important step of every data science and machine\nlearning project, including those involving textual data. We provide a novel\nlanguage tool, in the form of a publicly available Python library for\nextracting patterns from textual data. The library integrates a first public\nimplementation of the existing GrASP algorithm. It allows users to extract\npatterns using a number of general-purpose built-in linguistic attributes (such\nas hypernyms, part-of-speech tags, and syntactic dependency tags), as envisaged\nfor the original algorithm, as well as domain-specific custom attributes which\ncan be incorporated into the library by implementing two functions. The library\nis equipped with a web-based interface empowering human users to conveniently\nexplore data via the extracted patterns, using complementary pattern-centric\nand example-centric views: the former includes a reading in natural language\nand statistics of each extracted pattern; the latter shows applications of each\nextracted pattern to training examples. We demonstrate the usefulness of the\nlibrary in classification (spam detection and argument mining), model analysis\n(machine translation), and artifact discovery in datasets (SNLI and\n20Newsgroups).", "published": "2021-04-08 17:58:03", "link": "http://arxiv.org/abs/2104.03958v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Flavored Tacotron: Conditional Learning for Prosodic-linguistic Features", "abstract": "Neural sequence-to-sequence text-to-speech synthesis (TTS), such as\nTacotron-2, transforms text into high-quality speech. However, generating\nspeech with natural prosody still remains a challenge. Yasuda et. al. show that\nunlike natural speech, Tacotron-2's encoder doesn't fully represent prosodic\nfeatures (e.g. syllable stress in English) from characters, and result in flat\nfundamental frequency variations.\n  In this work, we propose a novel carefully designed strategy for conditioning\nTacotron-2 on two fundamental prosodic features in English -- stress syllable\nand pitch accent, that help achieve more natural prosody. To this end, we use\nof a classifier to learn these features in an end-to-end fashion, and apply\nfeature conditioning at three parts of Tacotron-2's Text-To-Mel Spectrogram:\npre-encoder, post-encoder, and intra-decoder. Further, we show that jointly\nconditioned features at pre-encoder and intra-decoder stages result in\nprosodically natural synthesized speech (vs. Tacotron-2), and allows the model\nto produce speech with more accurate pitch accent and stress patterns.\n  Quantitative evaluations show that our formulation achieves higher\nfundamental frequency contour correlation, and lower Mel Cepstral Distortion\nmeasure between synthesized and natural speech. And subjective evaluation shows\nthat the proposed method's Mean Opinion Score of 4.14 fairs higher than\nbaseline Tacotron-2, 3.91, when compared against natural speech (LJSpeech\ncorpus), 4.28.", "published": "2021-04-08 20:50:15", "link": "http://arxiv.org/abs/2104.04050v1", "categories": ["cs.SD", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Grapheme-to-Phoneme Transformer Model for Transfer Learning Dialects", "abstract": "Grapheme-to-Phoneme (G2P) models convert words to their phonetic\npronunciations. Classic G2P methods include rule-based systems and\npronunciation dictionaries, while modern G2P systems incorporate learning, such\nas, LSTM and Transformer-based attention models. Usually, dictionary-based\nmethods require significant manual effort to build, and have limited adaptivity\non unseen words. And transformer-based models require significant training\ndata, and do not generalize well, especially for dialects with limited data.\n  We propose a novel use of transformer-based attention model that can adapt to\nunseen dialects of English language, while using a small dictionary. We show\nthat our method has potential applications for accent transfer for\ntext-to-speech, and for building robust G2P models for dialects with limited\npronunciation dictionary size.\n  We experiment with two English dialects: Indian and British. A model trained\nfrom scratch using 1000 words from British English dictionary, with 14211 words\nheld out, leads to phoneme error rate (PER) of 26.877%, on a test set generated\nusing the full dictionary. The same model pretrained on CMUDict American\nEnglish dictionary, and fine-tuned on the same dataset leads to PER of 2.469%\non the test set.", "published": "2021-04-08 21:36:21", "link": "http://arxiv.org/abs/2104.04091v1", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Predicting the Reproducibility of Social and Behavioral Science Papers\n  Using Supervised Learning Models", "abstract": "In recent years, significant effort has been invested verifying the\nreproducibility and robustness of research claims in social and behavioral\nsciences (SBS), much of which has involved resource-intensive replication\nprojects. In this paper, we investigate prediction of the reproducibility of\nSBS papers using machine learning methods based on a set of features. We\npropose a framework that extracts five types of features from scholarly work\nthat can be used to support assessments of reproducibility of published\nresearch claims. Bibliometric features, venue features, and author features are\ncollected from public APIs or extracted using open source machine learning\nlibraries with customized parsers. Statistical features, such as p-values, are\nextracted by recognizing patterns in the body text. Semantic features, such as\nfunding information, are obtained from public APIs or are extracted using\nnatural language processing models. We analyze pairwise correlations between\nindividual features and their importance for predicting a set of human-assessed\nground truth labels. In doing so, we identify a subset of 9 top features that\nplay relatively more important roles in predicting the reproducibility of SBS\npapers in our corpus. Results are verified by comparing performances of 10\nsupervised predictive classifiers trained on different sets of features.", "published": "2021-04-08 00:45:20", "link": "http://arxiv.org/abs/2104.04580v2", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DL"}
{"title": "Layer Reduction: Accelerating Conformer-Based Self-Supervised Model via\n  Layer Consistency", "abstract": "Transformer-based self-supervised models are trained as feature extractors\nand have empowered many downstream speech tasks to achieve state-of-the-art\nperformance. However, both the training and inference process of these models\nmay encounter prohibitively high computational cost and large parameter budget.\nAlthough Parameter Sharing Strategy (PSS) proposed in ALBERT paves the way for\nparameter reduction, the computation required remains the same. Interestingly,\nwe found in experiments that distributions of feature embeddings from different\nTransformer layers are similar when PSS is integrated: a property termed as\nLayer Consistency (LC) in this paper. Given this similarity of feature\ndistributions, we assume that feature embeddings from different layers would\nhave similar representing power. In this work, Layer Consistency enables us to\nadopt Transformer-based models in a more efficient manner: the number of\nConformer layers in each training iteration could be uniformly sampled and\nShallow Layer Inference (SLI) could be applied to reduce the number of layers\nin inference stage. In experiments, our models are trained with LibriSpeech\ndataset and then evaluated on both phone classification and Speech Recognition\ntasks. We experimentally achieve 7.8X parameter reduction, 41.9% training\nspeedup and 37.7% inference speedup while maintaining comparable performance\nwith conventional BERT-like self-supervised methods.", "published": "2021-04-08 08:21:59", "link": "http://arxiv.org/abs/2105.00812v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation,\n  Recognition and Speaker Diarization in Conference Scenario", "abstract": "In this paper, we present AISHELL-4, a sizable real-recorded Mandarin speech\ndataset collected by 8-channel circular microphone array for speech processing\nin conference scenario. The dataset consists of 211 recorded meeting sessions,\neach containing 4 to 8 speakers, with a total length of 120 hours. This dataset\naims to bridge the advanced research on multi-speaker processing and the\npractical application scenario in three aspects. With real recorded meetings,\nAISHELL-4 provides realistic acoustics and rich natural speech characteristics\nin conversation such as short pause, speech overlap, quick speaker turn, noise,\netc. Meanwhile, accurate transcription and speaker voice activity are provided\nfor each meeting in AISHELL-4. This allows the researchers to explore different\naspects in meeting processing, ranging from individual tasks such as speech\nfront-end processing, speech recognition and speaker diarization, to\nmulti-modality modeling and joint optimization of relevant tasks. Given most\nopen source dataset for multi-speaker tasks are in English, AISHELL-4 is the\nonly Mandarin dataset for conversation speech, providing additional value for\ndata diversity in speech community. We also release a PyTorch-based training\nand evaluation framework as baseline system to promote reproducible research in\nthis field.", "published": "2021-04-08 08:38:44", "link": "http://arxiv.org/abs/2104.03603v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phoneme-based Distribution Regularization for Speech Enhancement", "abstract": "Existing speech enhancement methods mainly separate speech from noises at the\nsignal level or in the time-frequency domain. They seldom pay attention to the\nsemantic information of a corrupted signal. In this paper, we aim to bridge\nthis gap by extracting phoneme identities to help speech enhancement.\nSpecifically, we propose a phoneme-based distribution regularization (PbDr) for\nspeech enhancement, which incorporates frame-wise phoneme information into\nspeech enhancement network in a conditional manner. As different phonemes\nalways lead to different feature distributions in frequency, we propose to\nlearn a parameter pair, i.e. scale and bias, through a phoneme classification\nvector to modulate the speech enhancement network. The modulation parameter\npair includes not only frame-wise but also frequency-wise conditions, which\neffectively map features to phoneme-related distributions. In this way, we\nexplicitly regularize speech enhancement features by recognition vectors.\nExperiments on public datasets demonstrate that the proposed PbDr module can\nnot only boost the perceptual quality for speech enhancement but also the\nrecognition accuracy of an ASR system on the enhanced speech. This PbDr module\ncould be readily incorporated into other speech enhancement networks as well.", "published": "2021-04-08 13:21:29", "link": "http://arxiv.org/abs/2104.03759v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end speaker segmentation for overlap-aware resegmentation", "abstract": "Speaker segmentation consists in partitioning a conversation between one or\nmore speakers into speaker turns. Usually addressed as the late combination of\nthree sub-tasks (voice activity detection, speaker change detection, and\noverlapped speech detection), we propose to train an end-to-end segmentation\nmodel that does it directly. Inspired by the original end-to-end neural speaker\ndiarization approach (EEND), the task is modeled as a multi-label\nclassification problem using permutation-invariant training. The main\ndifference is that our model operates on short audio chunks (5 seconds) but at\na much higher temporal resolution (every 16ms). Experiments on multiple speaker\ndiarization datasets conclude that our model can be used with great success on\nboth voice activity detection and overlapped speech detection. Our proposed\nmodel can also be used as a post-processing step, to detect and correctly\nassign overlapped speech regions. Relative diarization error rate improvement\nover the best considered baseline (VBx) reaches 17% on AMI, 13% on DIHARD 3,\nand 13% on VoxConverse.", "published": "2021-04-08 20:38:17", "link": "http://arxiv.org/abs/2104.04045v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings", "abstract": "Emotion recognition datasets are relatively small, making the use of the more\nsophisticated deep learning approaches challenging. In this work, we propose a\ntransfer learning method for speech emotion recognition where features\nextracted from pre-trained wav2vec 2.0 models are modeled using simple neural\nnetworks. We propose to combine the output of several layers from the\npre-trained model using trainable weights which are learned jointly with the\ndownstream model. Further, we compare performance using two different wav2vec\n2.0 models, with and without finetuning for speech recognition. We evaluate our\nproposed approaches on two standard emotion databases IEMOCAP and RAVDESS,\nshowing superior performance compared to results in the literature.", "published": "2021-04-08 04:31:58", "link": "http://arxiv.org/abs/2104.03502v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement", "abstract": "The discrepancy between the cost function used for training a speech\nenhancement model and human auditory perception usually makes the quality of\nenhanced speech unsatisfactory. Objective evaluation metrics which consider\nhuman perception can hence serve as a bridge to reduce the gap. Our previously\nproposed MetricGAN was designed to optimize objective metrics by connecting the\nmetric with a discriminator. Because only the scores of the target evaluation\nfunctions are needed during training, the metrics can even be\nnon-differentiable. In this study, we propose a MetricGAN+ in which three\ntraining techniques incorporating domain-knowledge of speech processing are\nproposed. With these techniques, experimental results on the VoiceBank-DEMAND\ndataset show that MetricGAN+ can increase PESQ score by 0.3 compared to the\nprevious MetricGAN and achieve state-of-the-art results (PESQ score = 3.15).", "published": "2021-04-08 06:46:35", "link": "http://arxiv.org/abs/2104.03538v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Graph Attention Networks for Anti-Spoofing", "abstract": "The cues needed to detect spoofing attacks against automatic speaker\nverification are often located in specific spectral sub-bands or temporal\nsegments. Previous works show the potential to learn these using either\nspectral or temporal self-attention mechanisms but not the relationships\nbetween neighbouring sub-bands or segments. This paper reports our use of graph\nattention networks (GATs) to model these relationships and to improve spoofing\ndetection performance. GATs leverage a self-attention mechanism over graph\nstructured data to model the data manifold and the relationships between nodes.\nOur graph is constructed from representations produced by a ResNet. Nodes in\nthe graph represent information either in specific sub-bands or temporal\nsegments. Experiments performed on the ASVspoof 2019 logical access database\nshow that our GAT-based model with temporal attention outperforms all of our\nbaseline single systems. Furthermore, GAT-based systems are complementary to a\nset of existing systems. The fusion of GAT-based models with more conventional\ncountermeasures delivers a 47% relative improvement in performance compared to\nthe best performing single GAT system.", "published": "2021-04-08 10:18:17", "link": "http://arxiv.org/abs/2104.03654v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Denoising Without Clean Training Data: A Noise2Noise Approach", "abstract": "This paper tackles the problem of the heavy dependence of clean speech data\nrequired by deep learning based audio-denoising methods by showing that it is\npossible to train deep speech denoising networks using only noisy speech\nsamples. Conventional wisdom dictates that in order to achieve good speech\ndenoising performance, there is a requirement for a large quantity of both\nnoisy speech samples and perfectly clean speech samples, resulting in a need\nfor expensive audio recording equipment and extremely controlled soundproof\nrecording studios. These requirements pose significant challenges in data\ncollection, especially in economically disadvantaged regions and for low\nresource languages. This work shows that speech denoising deep neural networks\ncan be successfully trained utilizing only noisy training audio. Furthermore it\nis revealed that such training regimes achieve superior denoising performance\nover conventional training regimes utilizing clean training audio targets, in\ncases involving complex noise distributions and low Signal-to-Noise ratios\n(high noise environments). This is demonstrated through experiments studying\nthe efficacy of our proposed approach over both real-world noises and synthetic\nnoises using the 20 layered Deep Complex U-Net architecture.", "published": "2021-04-08 15:27:49", "link": "http://arxiv.org/abs/2104.03838v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2.6; I.5.4"], "primary_category": "cs.SD"}
{"title": "SerumRNN: Step by Step Audio VST Effect Programming", "abstract": "Learning to program an audio production VST synthesizer is a time consuming\nprocess, usually obtained through inefficient trial and error and only mastered\nafter years of experience. As an educational and creative tool for sound\ndesigners, we propose SerumRNN: a system that provides step-by-step\ninstructions for applying audio effects to change a user's input audio towards\na desired sound. We apply our system to Xfer Records Serum: currently one of\nthe most popular and complex VST synthesizers used by the audio production\ncommunity. Our results indicate that SerumRNN is consistently able to provide\nuseful feedback for a variety of different audio effects and synthesizer\npresets. We demonstrate the benefits of using an iterative system and show that\nSerumRNN learns to prioritize effects and can discover more efficient effect\norder sequences than a variety of baselines.", "published": "2021-04-08 16:32:14", "link": "http://arxiv.org/abs/2104.03876v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalized Spoofing Detection Inspired from Audio Generation Artifacts", "abstract": "State-of-the-art methods for audio generation suffer from fingerprint\nartifacts and repeated inconsistencies across temporal and spectral domains.\nSuch artifacts could be well captured by the frequency domain analysis over the\nspectrogram. Thus, we propose a novel use of long-range spectro-temporal\nmodulation feature -- 2D DCT over log-Mel spectrogram for the audio deepfake\ndetection. We show that this feature works better than log-Mel spectrogram,\nCQCC, MFCC, as a suitable candidate to capture such artifacts. We employ\nspectrum augmentation and feature normalization to decrease overfitting and\nbridge the gap between training and test dataset along with this novel feature\nintroduction. We developed a CNN-based baseline that achieved a 0.0849 t-DCF\nand outperformed the previously top single systems reported in the ASVspoof\n2019 challenge. Finally, by combining our baseline with our proposed 2D DCT\nspectro-temporal feature, we decrease the t-DCF score down by 14% to 0.0737,\nmaking it a state-of-the-art system for spoofing detection. Furthermore, we\nevaluate our model using two external datasets, showing the proposed feature's\ngeneralization ability. We also provide analysis and ablation studies for our\nproposed feature and results.", "published": "2021-04-08 23:02:56", "link": "http://arxiv.org/abs/2104.04111v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On tuning consistent annealed sampling for denoising score matching", "abstract": "Score-based generative models provide state-of-the-art quality for image and\naudio synthesis. Sampling from these models is performed iteratively, typically\nemploying a discretized series of noise levels and a predefined scheme. In this\nnote, we first overview three common sampling schemes for models trained with\ndenoising score matching. Next, we focus on one of them, consistent annealed\nsampling, and study its hyper-parameter boundaries. We then highlight a\npossible formulation of such hyper-parameter that explicitly considers those\nboundaries and facilitates tuning when using few or a variable number of steps.\nFinally, we highlight some connections of the formulation with other sampling\nschemes.", "published": "2021-04-08 12:19:10", "link": "http://arxiv.org/abs/2104.03725v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
