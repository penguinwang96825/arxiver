{"title": "Towards Using Machine Translation Techniques to Induce Multilingual\n  Lexica of Discourse Markers", "abstract": "Discourse markers are universal linguistic events subject to language\nvariation. Although an extensive literature has already reported language\nspecific traits of these events, little has been said on their cross-language\nbehavior and on building an inventory of multilingual lexica of discourse\nmarkers. This work describes new methods and approaches for the description,\nclassification, and annotation of discourse markers in the specific domain of\nthe Europarl corpus. The study of discourse markers in the context of\ntranslation is crucial due to the idiomatic nature of these structures.\nMultilingual lexica together with the functional analysis of such structures\nare useful tools for the hard task of translating discourse markers into\npossible equivalents from one language to another. Using Daniel Marcu's\nvalidated discourse markers for English, extracted from the Brown Corpus, our\npurpose is to build multilingual lexica of discourse markers for other\nlanguages, based on machine translation techniques. The major assumption in\nthis study is that the usage of a discourse marker is independent of the\nlanguage, i.e., the rhetorical function of a discourse marker in a sentence in\none language is equivalent to the rhetorical function of the same discourse\nmarker in another language.", "published": "2015-03-31 17:56:07", "link": "http://arxiv.org/abs/1503.09144v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "End-To-End Memory Networks", "abstract": "We introduce a neural network with a recurrent attention model over a\npossibly large external memory. The architecture is a form of Memory Network\n(Weston et al., 2015) but unlike the model in that work, it is trained\nend-to-end, and hence requires significantly less supervision during training,\nmaking it more generally applicable in realistic settings. It can also be seen\nas an extension of RNNsearch to the case where multiple computational steps\n(hops) are performed per output symbol. The flexibility of the model allows us\nto apply it to tasks as diverse as (synthetic) question answering and to\nlanguage modeling. For the former our approach is competitive with Memory\nNetworks, but with less supervision. For the latter, on the Penn TreeBank and\nText8 datasets our approach demonstrates comparable performance to RNNs and\nLSTMs. In both cases we show that the key concept of multiple computational\nhops yields improved results.", "published": "2015-03-31 03:05:37", "link": "http://arxiv.org/abs/1503.08895v5", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
