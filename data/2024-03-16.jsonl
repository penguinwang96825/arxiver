{"title": "Rules still work for Open Information Extraction", "abstract": "Open information extraction (OIE) aims to extract surface relations and their\ncorresponding arguments from natural language text, irrespective of domain.\nThis paper presents an innovative OIE model, APRCOIE, tailored for Chinese\ntext. Diverging from previous models, our model generates extraction patterns\nautonomously. The model defines a new pattern form for Chinese OIE and proposes\nan automated pattern generation methodology. In that way, the model can handle\na wide array of complex and diverse Chinese grammatical phenomena. We design a\npreliminary filter based on tensor computing to conduct the extraction\nprocedure efficiently. To train the model, we manually annotated a large-scale\nChinese OIE dataset. In the comparative evaluation, we demonstrate that APRCOIE\noutperforms state-of-the-art Chinese OIE models and significantly expands the\nboundaries of achievable OIE performance. The code of APRCOIE and the annotated\ndataset are released on GitHub (https://github.com/jialin666/APRCOIE_v1)", "published": "2024-03-16 01:40:36", "link": "http://arxiv.org/abs/2403.10758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT", "abstract": "The rapid advancement of large language models (LLMs) has enabled natural\nlanguage processing capabilities similar to those of humans, and LLMs are being\nwidely utilized across various societal domains such as education and\nhealthcare. While the versatility of these models has increased, they have the\npotential to generate subjective and normative language, leading to\ndiscriminatory treatment or outcomes among social groups, especially due to\nonline offensive language. In this paper, we define such harm as societal bias\nand assess ethnic, gender, and racial biases in a model fine-tuned with Korean\ncomments using Bidirectional Encoder Representations from Transformers (KcBERT)\nand KOLD data through template-based Masked Language Modeling (MLM). To\nquantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to\nKcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates\nsignificant changes in gender and racial biases. Based on these results, we\npropose two methods to mitigate societal bias. Firstly, a data balancing\napproach during the pre-training phase adjusts the uniformity of data by\naligning the distribution of the occurrences of specific words and converting\nsurrounding harmful words into non-harmful words. Secondly, during the\nin-training phase, we apply Debiasing Regularization by adjusting dropout and\nregularization, confirming a decrease in training loss. Our contribution lies\nin demonstrating that societal bias exists in Korean language models due to\nlanguage-dependent characteristics.", "published": "2024-03-16 02:27:19", "link": "http://arxiv.org/abs/2403.10774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-based Conversational AI Therapist for Daily Functioning Screening\n  and Psychotherapeutic Intervention via Everyday Smart Devices", "abstract": "Despite the global mental health crisis, access to screenings, professionals,\nand treatments remains high. In collaboration with licensed psychotherapists,\nwe propose a Conversational AI Therapist with psychotherapeutic Interventions\n(CaiTI), a platform that leverages large language models (LLM)s and smart\ndevices to enable better mental health self-care. CaiTI can screen the\nday-to-day functioning using natural and psychotherapeutic conversations. CaiTI\nleverages reinforcement learning to provide personalized conversation flow.\nCaiTI can accurately understand and interpret user responses. When the user\nneeds further attention during the conversation, CaiTI can provide\nconversational psychotherapeutic interventions, including cognitive behavioral\ntherapy (CBT) and motivational interviewing (MI). Leveraging the datasets\nprepared by the licensed psychotherapists, we experiment and microbenchmark\nvarious LLMs' performance in tasks along CaiTI's conversation flow and discuss\ntheir strengths and weaknesses. With the psychotherapists, we implement CaiTI\nand conduct 14-day and 24-week studies. The study results, validated by\ntherapists, demonstrate that CaiTI can converse with users naturally,\naccurately understand and interpret user responses, and provide\npsychotherapeutic interventions appropriately and effectively. We showcase the\npotential of CaiTI LLMs to assist the mental therapy diagnosis and treatment\nand improve day-to-day functioning screening and precautionary\npsychotherapeutic intervention systems.", "published": "2024-03-16 02:48:50", "link": "http://arxiv.org/abs/2403.10779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models abstract Medical Coded Language?", "abstract": "Large Language Models (LLMs) have become a pivotal research area, potentially\nmaking beneficial contributions in fields like healthcare where they can\nstreamline automated billing and decision support. However, the frequent use of\nspecialized coded languages like ICD-10, which are regularly updated and\ndeviate from natural language formats, presents potential challenges for LLMs\nin creating accurate and meaningful latent representations. This raises\nconcerns among healthcare professionals about potential inaccuracies or\n``hallucinations\" that could result in the direct impact of a patient.\nTherefore, this study evaluates whether large language models (LLMs) are aware\nof medical code ontologies and can accurately generate names from these codes.\nWe assess the capabilities and limitations of both general and\nbiomedical-specific generative models, such as GPT, LLaMA-2, and Meditron,\nfocusing on their proficiency with domain-specific terminologies. While the\nresults indicate that LLMs struggle with coded language, we offer insights on\nhow to adapt these models to reason more effectively.", "published": "2024-03-16 06:18:15", "link": "http://arxiv.org/abs/2403.10822v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-party Response Generation with Relation Disentanglement", "abstract": "Existing neural response generation models have achieved impressive\nimprovements for two-party conversations, which assume that utterances are\nsequentially organized. However, many real-world dialogues involve multiple\ninterlocutors and the structure of conversational context is much more complex,\ne.g. utterances from different interlocutors can occur \"in parallel\". Facing\nthis challenge, there are works trying to model the relations among utterances\nor interlocutors to facilitate response generation with clearer context.\nNonetheless, these methods rely heavily on such relations and all assume that\nthese are given beforehand, which is impractical and hinders the generality of\nsuch methods. In this work, we propose to automatically infer the relations via\nrelational thinking on subtle clues inside the conversation context without any\nhuman label, and leverage these relations to guide the neural response\ngeneration. Specifically, we first apply a deep graph random process to fully\nconsider all possible relations among utterances in the conversational context.\nThen the inferred relation graphs are integrated with a variational\nauto-encoder framework to train a GAN for structure-aware response generation.\nExperimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark\nand the most recent Movie Dialogues show that our method outperforms various\nbaseline models for multi-party response generation.", "published": "2024-03-16 06:33:44", "link": "http://arxiv.org/abs/2403.10827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deciphering Hate: Identifying Hateful Memes and Their Targets", "abstract": "Internet memes have become a powerful means for individuals to express\nemotions, thoughts, and perspectives on social media. While often considered as\na source of humor and entertainment, memes can also disseminate hateful content\ntargeting individuals or communities. Most existing research focuses on the\nnegative aspects of memes in high-resource languages, overlooking the\ndistinctive challenges associated with low-resource languages like Bengali\n(also known as Bangla). Furthermore, while previous work on Bengali memes has\nfocused on detecting hateful memes, there has been no work on detecting their\ntargeted entities. To bridge this gap and facilitate research in this arena, we\nintroduce a novel multimodal dataset for Bengali, BHM (Bengali Hateful Memes).\nThe dataset consists of 7,148 memes with Bengali as well as code-mixed\ncaptions, tailored for two tasks: (i) detecting hateful memes, and (ii)\ndetecting the social entities they target (i.e., Individual, Organization,\nCommunity, and Society). To solve these tasks, we propose DORA (Dual cO\nattention fRAmework), a multimodal deep neural network that systematically\nextracts the significant modality features from the memes and jointly evaluates\nthem with the modality-specific features to understand the context better. Our\nexperiments show that DORA is generalizable on other low-resource hateful meme\ndatasets and outperforms several state-of-the-art rivaling baselines.", "published": "2024-03-16 06:39:41", "link": "http://arxiv.org/abs/2403.10829v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two-step Automated Cybercrime Coded Word Detection using Multi-level\n  Representation Learning", "abstract": "In social network service platforms, crime suspects are likely to use\ncybercrime coded words for communication by adding criminal meanings to\nexisting words or replacing them with similar words. For instance, the word\n'ice' is often used to mean methamphetamine in drug crimes. To analyze the\nnature of cybercrime and the behavior of criminals, quickly detecting such\nwords and further understanding their meaning are critical. In the automated\ncybercrime coded word detection problem, it is difficult to collect a\nsufficient amount of training data for supervised learning and to directly\napply language models that utilize context information to better understand\nnatural language. To overcome these limitations, we propose a new two-step\napproach, in which a mean latent vector is constructed for each cybercrime\nthrough one of five different AutoEncoder models in the first step, and\ncybercrime coded words are detected based on multi-level latent representations\nin the second step. Moreover, to deeply understand cybercrime coded words\ndetected through the two-step approach, we propose three novel methods: (1)\nDetection of new words recently coined, (2) Detection of words frequently\nappeared in both drug and sex crimes, and (3) Automatic generation of word\ntaxonomy. According to our experimental results, among various AutoEncoder\nmodels, the stacked AutoEncoder model shows the best performance. Additionally,\nthe F1-score of the two-step approach is 0.991, which is higher than 0.987 and\n0.903 of the existing dark-GloVe and dark-BERT models. By analyzing the\nexperimental results of the three proposed methods, we can gain a deeper\nunderstanding of drug and sex crimes.", "published": "2024-03-16 07:18:29", "link": "http://arxiv.org/abs/2403.10838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetinaQA: A Robust Knowledge Base Question Answering Model for both\n  Answerable and Unanswerable Questions", "abstract": "An essential requirement for a real-world Knowledge Base Question Answering\n(KBQA) system is the ability to detect the answerability of questions when\ngenerating logical forms. However, state-of-the-art KBQA models assume all\nquestions to be answerable. Recent research has found that such models, when\nsuperficially adapted to detect answerability, struggle to satisfactorily\nidentify the different categories of unanswerable questions, and simultaneously\npreserve good performance for answerable questions. Towards addressing this\nissue, we propose RetinaQA, a new KBQA model that unifies two key ideas in a\nsingle KBQA architecture: (a) discrimination over candidate logical forms,\nrather than generating these, for handling schema-related unanswerability, and\n(b) sketch-filling-based construction of candidate logical forms for handling\ndata-related unaswerability. Our results show that RetinaQA significantly\noutperforms adaptations of state-of-the-art KBQA models in handling both\nanswerable and unanswerable questions and demonstrates robustness across all\ncategories of unanswerability. Notably, RetinaQA also sets a new\nstate-of-the-art for answerable KBQA, surpassing existing models.", "published": "2024-03-16 08:08:20", "link": "http://arxiv.org/abs/2403.10849v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robustness and Diversity: Continual Learning in Dialog\n  Generation with Text-Mixup and Batch Nuclear-Norm Maximization", "abstract": "In our dynamic world where data arrives in a continuous stream, continual\nlearning enables us to incrementally add new tasks/domains without the need to\nretrain from scratch. A major challenge in continual learning of language model\nis catastrophic forgetting, the tendency of models to forget knowledge from\npreviously trained tasks/domains when training on new ones. This paper studies\ndialog generation under the continual learning setting. We propose a novel\nmethod that 1) uses \\textit{Text-Mixup} as data augmentation to avoid model\noverfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization\n(BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain\ntask-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset)\ndemonstrate that our proposed approach outperforms the state-of-the-art in\ncontinual learning.", "published": "2024-03-16 11:09:27", "link": "http://arxiv.org/abs/2403.10894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BEnQA: A Question Answering and Reasoning Benchmark for Bengali and\n  English", "abstract": "In this study, we introduce BEnQA, a dataset comprising parallel Bengali and\nEnglish exam questions for middle and high school levels in Bangladesh. Our\ndataset consists of approximately 5K questions covering several subjects in\nscience with different types of questions, including factual, application, and\nreasoning-based questions. We benchmark several Large Language Models (LLMs)\nwith our parallel dataset and observe a notable performance disparity between\nthe models in Bengali and English. We also investigate some prompting methods,\nand find that Chain-of-Thought prompting is beneficial mostly on reasoning\nquestions, but not so much on factual ones. We also find that appending English\ntranslation helps to answer questions in Bengali. Our findings point to\npromising future research directions for improving the performance of LLMs in\nBengali and more generally in low-resource languages.", "published": "2024-03-16 11:27:42", "link": "http://arxiv.org/abs/2403.10900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pointer-Generator Networks for Low-Resource Machine Translation: Don't\n  Copy That!", "abstract": "While Transformer-based neural machine translation (NMT) is very effective in\nhigh-resource settings, many languages lack the necessary large parallel\ncorpora to benefit from it. In the context of low-resource (LR) MT between two\nclosely-related languages, a natural intuition is to seek benefits from\nstructural \"shortcuts\", such as copying subwords from the source to the target,\ngiven that such language pairs often share a considerable number of identical\nwords, cognates, and borrowings. We test Pointer-Generator Networks for this\npurpose for six language pairs over a variety of resource ranges, and find weak\nimprovements for most settings. However, analysis shows that the model does not\nshow greater improvements for closely-related vs. more distant language pairs,\nor for lower resource ranges, and that the models do not exhibit the expected\nusage of the mechanism for shared subwords. Our discussion of the reasons for\nthis behaviour highlights several general challenges for LR NMT, such as modern\ntokenization strategies, noisy real-world conditions, and linguistic\ncomplexities. We call for better scrutiny of linguistically motivated\nimprovements to NMT given the blackbox nature of Transformer models, as well as\nfor a focus on the above problems in the field.", "published": "2024-03-16 16:17:47", "link": "http://arxiv.org/abs/2403.10963v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-Trained Language Models Represent Some Geographic Populations Better\n  Than Others", "abstract": "This paper measures the skew in how well two families of LLMs represent\ndiverse geographic populations. A spatial probing task is used with\ngeo-referenced corpora to measure the degree to which pre-trained language\nmodels from the OPT and BLOOM series represent diverse populations around the\nworld. Results show that these models perform much better for some populations\nthan others. In particular, populations across the US and the UK are\nrepresented quite well while those in South and Southeast Asia are poorly\nrepresented. Analysis shows that both families of models largely share the same\nskew across populations. At the same time, this skew cannot be fully explained\nby sociolinguistic factors, economic factors, or geographic factors. The basic\nconclusion from this analysis is that pre-trained models do not equally\nrepresent the world's population: there is a strong skew towards specific\ngeographic populations. This finding challenges the idea that a single model\ncan be used for all populations.", "published": "2024-03-16 22:01:39", "link": "http://arxiv.org/abs/2403.11025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistics from a topological viewpoint", "abstract": "Typological databases in linguistics are usually categorical-valued. As a\nresult, it is difficult to have a clear visualization of the data. In this\npaper, we describe a workflow to analyze the topological shapes of South\nAmerican languages by applying multiple correspondence analysis technique and\ntopological data analysis methods.", "published": "2024-03-16 23:10:42", "link": "http://arxiv.org/abs/2403.15440v1", "categories": ["cs.CL", "91F20, 62R40, 55N31, 55U10"], "primary_category": "cs.CL"}
{"title": "Depression Detection on Social Media with Large Language Models", "abstract": "Depression harms. However, due to a lack of mental health awareness and fear\nof stigma, many patients do not actively seek diagnosis and treatment, leading\nto detrimental outcomes. Depression detection aims to determine whether an\nindividual suffers from depression by analyzing their history of posts on\nsocial media, which can significantly aid in early detection and intervention.\nIt mainly faces two key challenges: 1) it requires professional medical\nknowledge, and 2) it necessitates both high accuracy and explainability. To\naddress it, we propose a novel depression detection system called DORIS,\ncombining medical knowledge and the recent advances in large language models\n(LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based\nsolution to first annotate whether high-risk texts meet medical diagnostic\ncriteria. Further, we retrieve texts with high emotional intensity and\nsummarize critical information from the historical mood records of users,\nso-called mood courses. To tackle the second challenge, we combine LLM and\ntraditional classifiers to integrate medical knowledge-guided features, for\nwhich the model can also explain its prediction results, achieving both high\naccuracy and explainability. Extensive experimental results on benchmarking\ndatasets show that, compared to the current best baseline, our approach\nimproves by 0.036 in AUPRC, which can be considered significant, demonstrating\nthe effectiveness of our approach and its high value as an NLP application.", "published": "2024-03-16 01:01:16", "link": "http://arxiv.org/abs/2403.10750v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ECRC: Emotion-Causality Recognition in Korean Conversation for GCN", "abstract": "In this multi-task learning study on simultaneous analysis of emotions and\ntheir underlying causes in conversational contexts, deep neural network methods\nwere employed to effectively process and train large labeled datasets. However,\nthese approaches are typically limited to conducting context analyses across\nthe entire corpus because they rely on one of the two methods: word- or\nsentence-level embedding. The former struggles with polysemy and homonyms,\nwhereas the latter causes information loss when processing long sentences. In\nthis study, we overcome the limitations of previous embeddings by utilizing\nboth word- and sentence-level embeddings. Furthermore, we propose the\nemotion-causality recognition in conversation (ECRC) model, which is based on a\nnovel graph structure, thereby leveraging the strengths of both embedding\nmethods. This model uniquely integrates the bidirectional long short-term\nmemory (Bi-LSTM) and graph neural network (GCN) models for Korean conversation\nanalysis. Compared with models that rely solely on one embedding method, the\nproposed model effectively structures abstract concepts, such as language\nfeatures and relationships, thereby minimizing information loss. To assess\nmodel performance, we compared the multi-task learning results of three deep\nneural network models with varying graph structures. Additionally, we evaluated\nthe proposed model using Korean and English datasets. The experimental results\nshow that the proposed model performs better in emotion and causality\nmulti-task learning (74.62% and 75.30%, respectively) when node and edge\ncharacteristics are incorporated into the graph structure. Similar results were\nrecorded for the Korean ECC and Wellness datasets (74.62% and 73.44%,\nrespectively) with 71.35% on the IEMOCAP English dataset.", "published": "2024-03-16 02:07:31", "link": "http://arxiv.org/abs/2403.10764v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Chinese Humor Generation: A Study on Two-Part Allegorical\n  Sayings", "abstract": "Humor, a culturally nuanced aspect of human language, poses challenges for\ncomputational understanding and generation, especially in Chinese humor, which\nremains relatively unexplored in the NLP community. This paper investigates the\ncapability of state-of-the-art language models to comprehend and generate\nChinese humor, specifically focusing on training them to create allegorical\nsayings. We employ two prominent training methods: fine-tuning a medium-sized\nlanguage model and prompting a large one. Our novel fine-tuning approach\nincorporates fused Pinyin embeddings to consider homophones and employs\ncontrastive learning with synthetic hard negatives to distinguish humor\nelements. Human-annotated results show that these models can generate humorous\nallegorical sayings, with prompting proving to be a practical and effective\nmethod. However, there is still room for improvement in generating allegorical\nsayings that match human creativity.", "published": "2024-03-16 02:58:57", "link": "http://arxiv.org/abs/2403.10781v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot Generative Linguistic Steganography", "abstract": "Generative linguistic steganography attempts to hide secret messages into\ncovertext. Previous studies have generally focused on the statistical\ndifferences between the covertext and stegotext, however, ill-formed stegotext\ncan readily be identified by humans. In this paper, we propose a novel\nzero-shot approach based on in-context learning for linguistic steganography to\nachieve better perceptual and statistical imperceptibility. We also design\nseveral new metrics and reproducible language evaluations to measure the\nimperceptibility of the stegotext. Our experimental results indicate that our\nmethod produces $1.926\\times$ more innocent and intelligible stegotext than any\nother method.", "published": "2024-03-16 08:31:25", "link": "http://arxiv.org/abs/2403.10856v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Optimizing Language Augmentation for Multilingual Large Language Models:\n  A Case Study on Korean", "abstract": "Large language models (LLMs) use pretraining to predict the subsequent word;\nhowever, their expansion requires significant computing resources. Numerous big\ntech companies and research institutes have developed multilingual LLMs (MLLMs)\nto meet current demands, overlooking less-resourced languages (LRLs). This\nstudy proposed three strategies to enhance the performance of LRLs based on the\npublicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to\nenhance expressiveness. Second, bilingual data were used for pretraining to\nalign the high- and less-resourced languages. Third, a high-quality small-scale\ninstruction dataset was constructed and instruction-tuning was performed to\naugment the LRL. The experiments employed the Llama2 model and Korean was used\nas the LRL, which was quantitatively evaluated against other developed LLMs\nacross eight tasks. Furthermore, a qualitative assessment was performed based\non human evaluation and GPT4. Experimental results showed that our proposed\nBllossom model exhibited superior performance in qualitative analyses compared\nto previously proposed Korean monolingual models.", "published": "2024-03-16 10:26:38", "link": "http://arxiv.org/abs/2403.10882v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent\n  Recognition and Out-of-scope Detection in Conversations", "abstract": "Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.", "published": "2024-03-16 15:14:15", "link": "http://arxiv.org/abs/2403.10943v4", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Lambda: Learning Matchable Prior For Entity Alignment with Unlabeled\n  Dangling Cases", "abstract": "We investigate the entity alignment (EA) problem with unlabeled dangling\ncases, meaning that partial entities have no counterparts in the other\nknowledge graph (KG), and this type of entity remains unlabeled. To address\nthis challenge, we propose the framework \\textit{Lambda} for dangling detection\nand then entity alignment. Lambda features a GNN-based encoder called KEESA\nwith spectral contrastive learning for EA and a positive-unlabeled learning\nalgorithm for dangling detection called iPULE. iPULE offers theoretical\nguarantees of unbiasedness, uniform deviation bounds, and convergence.\nExperimental results demonstrate that each component contributes to overall\nperformances that are superior to baselines, even when baselines additionally\nexploit 30\\% of dangling entities labeled for training.", "published": "2024-03-16 17:21:58", "link": "http://arxiv.org/abs/2403.10978v2", "categories": ["cs.CL", "cs.IR", "I.2.4; H.3.3"], "primary_category": "cs.CL"}
{"title": "DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and\n  Closely-Related Languages", "abstract": "Language technologies should be judged on their usefulness in real-world use\ncases. An often overlooked aspect in natural language processing (NLP) research\nand evaluation is language variation in the form of non-standard dialects or\nlanguage varieties (hereafter, varieties). Most NLP benchmarks are limited to\nstandard language varieties. To fill this gap, we propose DIALECTBENCH, the\nfirst-ever large-scale benchmark for NLP on varieties, which aggregates an\nextensive set of task-varied variety datasets (10 text-level tasks covering 281\nvarieties). This allows for a comprehensive evaluation of NLP system\nperformance on different language varieties. We provide substantial evidence of\nperformance disparities between standard and non-standard language varieties,\nand we also identify language clusters with large performance divergence across\ntasks. We believe DIALECTBENCH provides a comprehensive view of the current\nstate of NLP for language varieties and one step towards advancing it further.\nCode/data: https://github.com/ffaisal93/DialectBench", "published": "2024-03-16 20:18:36", "link": "http://arxiv.org/abs/2403.11009v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Solve Robot Routing?", "abstract": "Routing problems are common in mobile robotics, encompassing tasks such as\ninspection, surveillance, and coverage. Depending on the objective and\nconstraints, these problems often reduce to variants of the Traveling Salesman\nProblem (TSP), with solutions traditionally derived by translating high-level\nobjectives into an optimization formulation and using modern solvers to arrive\nat a solution. Here, we explore the potential of Large Language Models (LLMs)\nto replace the entire pipeline from tasks described in natural language to the\ngeneration of robot routes. We systematically investigate the performance of\nLLMs in robot routing by constructing a dataset with 80 unique robot routing\nproblems across 8 variants in both single and multi-robot settings. We evaluate\nLLMs through three frameworks: single attempt, self-debugging, and\nself-debugging with self-verification and various contexts, including\nmathematical formulations, pseudo-code, and related research papers. Our\nfindings reveal that both self-debugging and self-verification enhance success\nrates without significantly lowering the optimality gap. We observe\ncontext-sensitive behavior - providing mathematical formulations as context\ndecreases the optimality gap but significantly decreases success rates and\nproviding pseudo-code and related research papers as context does not\nconsistently improve success rates or decrease the optimality gap. We identify\nkey challenges and propose future directions to enhance LLM performance in\nsolving robot routing problems. Our source code is available on the project\nwebsite: https://sites.google.com/view/words-to-routes/.", "published": "2024-03-16 03:54:38", "link": "http://arxiv.org/abs/2403.10795v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment", "abstract": "Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82% in accuracy\nacross seven downstream tasks when pruning LLaMA-7B by 50%.", "published": "2024-03-16 04:12:50", "link": "http://arxiv.org/abs/2403.10799v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Initial Decoding with Minimally Augmented Language Model for Improved\n  Lattice Rescoring in Low Resource ASR", "abstract": "This paper addresses the problem of improving speech recognition accuracy\nwith lattice rescoring in low-resource languages where the baseline language\nmodel is insufficient for generating inclusive lattices. We minimally augment\nthe baseline language model with word unigram counts that are present in a\nlarger text corpus of the target language but absent in the baseline. The\nlattices generated after decoding with such an augmented baseline language\nmodel are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada)\nrelative word error reduction with our proposed method. This reduction in word\nerror rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word\nerror reduction obtained by decoding with full Wikipedia text augmented\nlanguage mode while our approach consumes only 1/8th the memory. We demonstrate\nthat our method is comparable with various text selection-based language model\naugmentation and also consistent for data sets of different sizes. Our approach\nis applicable for training speech recognition systems under low resource\nconditions where speech data and compute resources are insufficient, while\nthere is a large text corpus that is available in the target language. Our\nresearch involves addressing the issue of out-of-vocabulary words of the\nbaseline in general and does not focus on resolving the absence of named\nentities. Our proposed method is simple and yet computationally less expensive.", "published": "2024-03-16 14:34:31", "link": "http://arxiv.org/abs/2403.10937v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "SelfIE: Self-Interpretation of Large Language Model Embeddings", "abstract": "How do large language models (LLMs) obtain their answers? The ability to\nexplain and control an LLM's reasoning process is key for reliability,\ntransparency, and future model developments. We propose SelfIE\n(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret\ntheir own embeddings in natural language by leveraging their ability to respond\nto inquiries about a given passage. Capable of interpreting open-world concepts\nin the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such\nas making ethical decisions, internalizing prompt injection, and recalling\nharmful knowledge. SelfIE's text descriptions on hidden embeddings also open up\nnew avenues to control LLM reasoning. We propose Supervised Control, which\nallows editing open-ended concepts while only requiring gradient computation of\nindividual layer. We extend RLHF to hidden embeddings and propose Reinforcement\nControl that erases harmful knowledge in LLM without supervision targets.", "published": "2024-03-16 15:30:34", "link": "http://arxiv.org/abs/2403.10949v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Energy-Based Models with Applications to Speech and Language Processing", "abstract": "Energy-Based Models (EBMs) are an important class of probabilistic models,\nalso known as random fields and undirected graphical models. EBMs are\nun-normalized and thus radically different from other popular self-normalized\nprobabilistic models such as hidden Markov models (HMMs), autoregressive\nmodels, generative adversarial nets (GANs) and variational auto-encoders\n(VAEs). Over the past years, EBMs have attracted increasing interest not only\nfrom the core machine learning community, but also from application domains\nsuch as speech, vision, natural language processing (NLP) and so on, due to\nsignificant theoretical and algorithmic progress. The sequential nature of\nspeech and language also presents special challenges and needs a different\ntreatment from processing fix-dimensional data (e.g., images). Therefore, the\npurpose of this monograph is to present a systematic introduction to\nenergy-based models, including both algorithmic progress and applications in\nspeech and language processing. First, the basics of EBMs are introduced,\nincluding classic models, recent models parameterized by neural networks,\nsampling methods, and various learning methods from the classic learning\nalgorithms to the most advanced ones. Then, the application of EBMs in three\ndifferent scenarios is presented, i.e., for modeling marginal, conditional and\njoint distributions, respectively. 1) EBMs for sequential data with\napplications in language modeling, where the main focus is on the marginal\ndistribution of a sequence itself; 2) EBMs for modeling conditional\ndistributions of target sequences given observation sequences, with\napplications in speech recognition, sequence labeling and text generation; 3)\nEBMs for modeling joint distributions of both sequences of observations and\ntargets, and their applications in semi-supervised learning and calibrated\nnatural language understanding.", "published": "2024-03-16 16:16:31", "link": "http://arxiv.org/abs/2403.10961v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Refining Knowledge Transfer on Audio-Image Temporal Agreement for\n  Audio-Text Cross Retrieval", "abstract": "The aim of this research is to refine knowledge transfer on audio-image\ntemporal agreement for audio-text cross retrieval. To address the limited\navailability of paired non-speech audio-text data, learning methods for\ntransferring the knowledge acquired from a large amount of paired audio-image\ndata to shared audio-text representation have been investigated, suggesting the\nimportance of how audio-image co-occurrence is learned. Conventional approaches\nin audio-image learning assign a single image randomly selected from the\ncorresponding video stream to the entire audio clip, assuming their\nco-occurrence. However, this method may not accurately capture the temporal\nagreement between the target audio and image because a single image can only\nrepresent a snapshot of a scene, though the target audio changes from moment to\nmoment. To address this problem, we propose two methods for audio and image\nmatching that effectively capture the temporal information: (i) Nearest Match\nwherein an image is selected from multiple time frames based on similarity with\naudio, and (ii) Multiframe Match wherein audio and image pairs of multiple time\nframes are used. Experimental results show that method (i) improves the\naudio-text retrieval performance by selecting the nearest image that aligns\nwith the audio information and transferring the learned knowledge. Conversely,\nmethod (ii) improves the performance of audio-image retrieval while not showing\nsignificant improvements in audio-text retrieval performance. These results\nindicate that refining audio-image temporal agreement may contribute to better\nknowledge transfer to audio-text retrieval.", "published": "2024-03-16 01:38:36", "link": "http://arxiv.org/abs/2403.10756v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CoPlay: Audio-agnostic Cognitive Scaling for Acoustic Sensing", "abstract": "Acoustic sensing manifests great potential in various applications that\nencompass health monitoring, gesture interface and imaging by leveraging the\nspeakers and microphones on smart devices. However, in ongoing research and\ndevelopment in acoustic sensing, one problem is often overlooked: the same\nspeaker, when used concurrently for sensing and other traditional applications\n(like playing music), could cause interference in both making it impractical to\nuse in the real world. The strong ultrasonic sensing signals mixed with music\nwould overload the speaker's mixer. To confront this issue of overloaded\nsignals, current solutions are clipping or down-scaling, both of which affect\nthe music playback quality and also sensing range and accuracy. To address this\nchallenge, we propose CoPlay, a deep learning based optimization algorithm to\ncognitively adapt the sensing signal. It can 1) maximize the sensing signal\nmagnitude within the available bandwidth left by the concurrent music to\noptimize sensing range and accuracy and 2) minimize any consequential frequency\ndistortion that can affect music playback. In this work, we design a deep\nlearning model and test it on common types of sensing signals (sine wave or\nFrequency Modulated Continuous Wave FMCW) as inputs with various agnostic\nconcurrent music and speech. First, we evaluated the model performance to show\nthe quality of the generated signals. Then we conducted field studies of\ndownstream acoustic sensing tasks in the real world. A study with 12 users\nproved that respiration monitoring and gesture recognition using our adapted\nsignal achieve similar accuracy as no-concurrent-music scenarios, while\nclipping or down-scaling manifests worse accuracy. A qualitative study also\nmanifests that the music play quality is not degraded, unlike traditional\nclipping or down-scaling methods.", "published": "2024-03-16 03:59:33", "link": "http://arxiv.org/abs/2403.10796v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-Grained Engine Fault Sound Event Detection Using Multimodal Signals", "abstract": "Sound event detection (SED) is an active area of audio research that aims to\ndetect the temporal occurrence of sounds. In this paper, we apply SED to engine\nfault detection by introducing a multimodal SED framework that detects\nfine-grained engine faults of automobile engines using audio and\naccelerometer-recorded vibration. We first introduce the problem of engine\nfault SED on a dataset collected from a large variety of vehicles with\nexpertly-labeled engine fault sound events. Next, we propose a SED model to\ntemporally detect ten fine-grained engine faults that occur within vehicle\nengines and further explore a pretraining strategy using a large-scale\nweakly-labeled engine fault dataset. Through multiple evaluations, we show our\nproposed framework is able to effectively detect engine fault sound events.\nFinally, we investigate the interaction and characteristics of each modality\nand show that fusing features from audio and vibration improves overall engine\nfault SED capabilities.", "published": "2024-03-16 22:51:02", "link": "http://arxiv.org/abs/2403.11037v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of\n  Complex Physical Systems", "abstract": "Data-driven modeling of complex physical systems is receiving a growing\namount of attention in the simulation and machine learning communities. Since\nmost physical simulations are based on compute-intensive, iterative\nimplementations of differential equation systems, a (partial) replacement with\nlearned, 1-step inference models has the potential for significant speedups in\na wide range of application areas. In this context, we present a novel\nbenchmark for the evaluation of 1-step generative learning models in terms of\nspeed and physical correctness. Our Urban Sound Propagation benchmark is based\non the physically complex and practically relevant, yet intuitively easy to\ngrasp task of modeling the 2d propagation of waves from a sound source in an\nurban environment. We provide a dataset with 100k samples, where each sample\nconsists of pairs of real 2d building maps drawn from OpenStreetmap, a\nparameterized sound source, and a simulated ground truth sound propagation for\nthe given scene. The dataset provides four different simulation tasks with\nincreasing complexity regarding reflection, diffraction and source variance. A\nfirst baseline evaluation of common generative U-Net, GAN and Diffusion models\nshows, that while these models are very well capable of modeling sound\npropagations in simple cases, the approximation of sub-systems represented by\nhigher order equations systematically fails. Information about the dataset,\ndownload instructions and source codes are provided on our website:\nhttps://www.urban-sound-data.org.", "published": "2024-03-16 11:38:58", "link": "http://arxiv.org/abs/2403.10904v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech-driven Personalized Gesture Synthetics: Harnessing Automatic\n  Fuzzy Feature Inference", "abstract": "Speech-driven gesture generation is an emerging field within virtual human\ncreation. However, a significant challenge lies in accurately determining and\nprocessing the multitude of input features (such as acoustic, semantic,\nemotional, personality, and even subtle unknown features). Traditional\napproaches, reliant on various explicit feature inputs and complex multimodal\nprocessing, constrain the expressiveness of resulting gestures and limit their\napplicability. To address these challenges, we present Persona-Gestor, a novel\nend-to-end generative model designed to generate highly personalized 3D\nfull-body gestures solely relying on raw speech audio. The model combines a\nfuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization\n(AdaLN) transformer diffusion architecture. The fuzzy feature extractor\nharnesses a fuzzy inference strategy that automatically infers implicit,\ncontinuous fuzzy features. These fuzzy features, represented as a unified\nlatent feature, are fed into the AdaLN transformer. The AdaLN transformer\nintroduces a conditional mechanism that applies a uniform function across all\ntokens, thereby effectively modeling the correlation between the fuzzy features\nand the gesture sequence. This module ensures a high level of gesture-speech\nsynchronization while preserving naturalness. Finally, we employ the diffusion\nmodel to train and infer various gestures. Extensive subjective and objective\nevaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's\nsuperior performance to the current state-of-the-art approaches. Persona-Gestor\nimproves the system's usability and generalization capabilities, setting a new\nbenchmark in speech-driven gesture synthesis and broadening the horizon for\nvirtual human technology. Supplementary videos and code can be accessed at\nhttps://zf223669.github.io/Diffmotion-v2-website/", "published": "2024-03-16 04:40:10", "link": "http://arxiv.org/abs/2403.10805v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.GR", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
