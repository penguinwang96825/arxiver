{"title": "Exploring sentence informativeness", "abstract": "This study is a preliminary exploration of the concept of informativeness\n-how much information a sentence gives about a word it contains- and its\npotential benefits to building quality word representations from scarce data.\nWe propose several sentence-level classifiers to predict informativeness, and\nwe perform a manual annotation on a set of sentences. We conclude that these\ntwo measures correspond to different notions of informativeness. However, our\nexperiments show that using the classifiers' predictions to train word\nembeddings has an impact on embedding quality.", "published": "2019-07-19 11:39:41", "link": "http://arxiv.org/abs/1907.08469v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Human Activities from User-Generated Content", "abstract": "The activities we do are linked to our interests, personality, political\npreferences, and decisions we make about the future. In this paper, we explore\nthe task of predicting human activities from user-generated content. We collect\na dataset containing instances of social media users writing about a range of\neveryday activities. We then use a state-of-the-art sentence embedding\nframework tailored to recognize the semantics of human activities and perform\nan automatic clustering of these activities. We train a neural network model to\nmake predictions about which clusters contain activities that were performed by\na given user based on the text of their previous posts and self-description.\nAdditionally, we explore the degree to which incorporating inferred user traits\ninto our model helps with this prediction task.", "published": "2019-07-19 15:02:16", "link": "http://arxiv.org/abs/1907.08540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Pragmatics-Centered Evaluation Framework for Natural Language\n  Understanding", "abstract": "New models for natural language understanding have recently made an\nunparalleled amount of progress, which has led some researchers to suggest that\nthe models induce universal text representations. However, current benchmarks\nare predominantly targeting semantic phenomena; we make the case that\npragmatics needs to take center stage in the evaluation of natural language\nunderstanding. We introduce PragmEval, a new benchmark for the evaluation of\nnatural language understanding, that unites 11 pragmatics-focused evaluation\ndatasets for English. PragmEval can be used as supplementary training data in a\nmulti-task learning setup, and is publicly available, alongside the code for\ngathering and preprocessing the datasets. Using our evaluation suite, we show\nthat natural language inference, a widely used pretraining task, does not\nresult in genuinely universal representations, which presents a new challenge\nfor multi-task learning.", "published": "2019-07-19 20:09:03", "link": "http://arxiv.org/abs/1907.08672v2", "categories": ["cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "What is this Article about? Extreme Summarization with Topic-aware\n  Convolutional Neural Networks", "abstract": "We introduce 'extreme summarization', a new single-document summarization\ntask which aims at creating a short, one-sentence news summary answering the\nquestion ``What is the article about?''. We argue that extreme summarization,\nby nature, is not amenable to extractive strategies and requires an abstractive\nmodeling approach. In the hope of driving research on this task further: (a) we\ncollect a real-world, large scale dataset by harvesting online articles from\nthe British Broadcasting Corporation (BBC); and (b) propose a novel abstractive\nmodel which is conditioned on the article's topics and based entirely on\nconvolutional neural networks. We demonstrate experimentally that this\narchitecture captures long-range dependencies in a document and recognizes\npertinent content, outperforming an oracle extractive system and\nstate-of-the-art abstractive approaches when evaluated automatically and by\nhumans on the extreme summarization dataset.", "published": "2019-07-19 22:57:21", "link": "http://arxiv.org/abs/1907.08722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I Stand With You: Using Emojis to Study Solidarity in Crisis Events", "abstract": "We study how emojis are used to express solidarity in social media in the\ncontext of two major crisis events - a natural disaster, Hurricane Irma in 2017\nand terrorist attacks that occurred on November 2015 in Paris. Using annotated\ncorpora, we first train a recurrent neural network model to classify\nexpressions of solidarity in text. Next, we use these expressions of solidarity\nto characterize human behavior in online social networks, through the temporal\nand geospatial diffusion of emojis. Our analysis reveals that emojis are a\npowerful indicator of sociolinguistic behaviors (solidarity) that are exhibited\non social media as the crisis events unfold.", "published": "2019-07-19 00:40:28", "link": "http://arxiv.org/abs/1907.08326v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Comparative Evaluation of Visual and Natural Language Question\n  Answering Over Linked Data", "abstract": "With the growing number and size of Linked Data datasets, it is crucial to\nmake the data accessible and useful for users without knowledge of formal query\nlanguages. Two approaches towards this goal are knowledge graph visualization\nand natural language interfaces. Here, we investigate specifically question\nanswering (QA) over Linked Data by comparing a diagrammatic visual approach\nwith existing natural language-based systems. Given a QA benchmark (QALD7), we\nevaluate a visual method which is based on iteratively creating diagrams until\nthe answer is found, against four QA systems that have natural language queries\nas input. Besides other benefits, the visual approach provides higher\nperformance, but also requires more manual input. The results indicate that the\nmethods can be used complementary, and that such a combination has a large\npositive impact on QA performance, and also facilitates additional features\nsuch as data exploration.", "published": "2019-07-19 13:09:32", "link": "http://arxiv.org/abs/1907.08501v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Multi-Granular Text Encoding for Self-Explaining Categorization", "abstract": "Self-explaining text categorization requires a classifier to make a\nprediction along with supporting evidence. A popular type of evidence is\nsub-sequences extracted from the input text which are sufficient for the\nclassifier to make the prediction. In this work, we define multi-granular\nngrams as basic units for explanation, and organize all ngrams into a\nhierarchical structure, so that shorter ngrams can be reused while computing\nlonger ngrams. We leverage a tree-structured LSTM to learn a\ncontext-independent representation for each unit via parameter sharing.\nExperiments on medical disease classification show that our model is more\naccurate, efficient and compact than BiLSTM and CNN baselines. More\nimportantly, our model can extract intuitive multi-granular evidence to support\nits predictions.", "published": "2019-07-19 14:43:51", "link": "http://arxiv.org/abs/1907.08532v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structure-Invariant Testing for Machine Translation", "abstract": "In recent years, machine translation software has increasingly been\nintegrated into our daily lives. People routinely use machine translation for\nvarious applications, such as describing symptoms to a foreign doctor and\nreading political news in a foreign language. However, the complexity and\nintractability of neural machine translation (NMT) models that power modern\nmachine translation make the robustness of these systems difficult to even\nassess, much less guarantee. Machine translation systems can return inferior\nresults that lead to misunderstanding, medical misdiagnoses, threats to\npersonal safety, or political conflicts. Despite its apparent importance,\nvalidating the robustness of machine translation systems is very difficult and\nhas, therefore, been much under-explored.\n  To tackle this challenge, we introduce structure-invariant testing (SIT), a\nnovel metamorphic testing approach for validating machine translation software.\nOur key insight is that the translation results of \"similar\" source sentences\nshould typically exhibit similar sentence structures. Specifically, SIT (1)\ngenerates similar source sentences by substituting one word in a given sentence\nwith semantically similar, syntactically equivalent words; (2) represents\nsentence structure by syntax parse trees (obtained via constituency or\ndependency parsing); (3) reports sentence pairs whose structures differ\nquantitatively by more than some threshold. To evaluate SIT, we use it to test\nGoogle Translate and Bing Microsoft Translator with 200 source sentences as\ninput, which led to 64 and 70 buggy issues with 69.5\\% and 70\\% top-1 accuracy,\nrespectively. The translation errors are diverse, including under-translation,\nover-translation, incorrect modification, word/phrase mistranslation, and\nunclear logic.", "published": "2019-07-19 22:20:01", "link": "http://arxiv.org/abs/1907.08710v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "An Unsupervised Character-Aware Neural Approach to Word and Context\n  Representation Learning", "abstract": "In the last few years, neural networks have been intensively used to develop\nmeaningful distributed representations of words and contexts around them. When\nthese representations, also known as \"embeddings\", are learned from\nunsupervised large corpora, they can be transferred to different tasks with\npositive effects in terms of performances, especially when only a few\nsupervisions are available. In this work, we further extend this concept, and\nwe present an unsupervised neural architecture that jointly learns word and\ncontext embeddings, processing words as sequences of characters. This allows\nour model to spot the regularities that are due to the word morphology, and to\navoid the need of a fixed-sized input vocabulary of words. We show that we can\nlearn compact encoders that, despite the relatively small number of parameters,\nreach high-level performances in downstream tasks, comparing them with related\nstate-of-the-art approaches or with fully supervised methods.", "published": "2019-07-19 09:34:11", "link": "http://arxiv.org/abs/1908.01819v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "DREAMT -- Embodied Motivational Conversational Storytelling", "abstract": "Storytelling is fundamental to language, including culture, conversation and\ncommunication in their broadest senses. It thus emerges as an essential\ncomponent of intelligent systems, including systems where natural language is\nnot a primary focus or where we do not usually think of a story being involved.\nIn this paper we explore the emergence of storytelling as a requirement in\nembodied conversational agents, including its role in educational and health\ninterventions, as well as in a general-purpose computer interface for people\nwith disabilities or other constraints that prevent the use of traditional\nkeyboard and speech interfaces. We further present a characterization of\nstorytelling as an inventive fleshing out of detail according to a particular\npersonal perspective, and propose the DREAMT model to focus attention on the\ndifferent layers that need to be present in a character-driven storytelling\nsystem. Most if not all aspects of the DREAMT model have arisen from or been\nexplored in some aspect of our implemented research systems, but currently only\nat a primitive and relatively unintegrated level. However, this experience\nleads us to formalize and elaborate the DREAMT model mnemonically as follows: -\nDescription/Dialogue/Definition/Denotation - Realization/Representation/Role -\nExplanation/Education/Entertainment - Actualization/Activation -\nMotivation/Modelling - Topicalization/Transformation", "published": "2019-07-19 01:49:37", "link": "http://arxiv.org/abs/1907.09293v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Data Augmentation for Instrument Classification Robust to Audio Effects", "abstract": "Reusing recorded sounds (sampling) is a key component in Electronic Music\nProduction (EMP), which has been present since its early days and is at the\ncore of genres like hip-hop or jungle. Commercial and non-commercial services\nallow users to obtain collections of sounds (sample packs) to reuse in their\ncompositions. Automatic classification of one-shot instrumental sounds allows\nautomatically categorising the sounds contained in these collections, allowing\neasier navigation and better characterisation. Automatic instrument\nclassification has mostly targeted the classification of unprocessed isolated\ninstrumental sounds or detecting predominant instruments in mixed music tracks.\nFor this classification to be useful in audio databases for EMP, it has to be\nrobust to the audio effects applied to unprocessed sounds. In this paper we\nevaluate how a state of the art model trained with a large dataset of one-shot\ninstrumental sounds performs when classifying instruments processed with audio\neffects. In order to evaluate the robustness of the model, we use data\naugmentation with audio effects and evaluate how each effect influences the\nclassification accuracy.", "published": "2019-07-19 14:04:32", "link": "http://arxiv.org/abs/1907.08520v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DNN-based Speaker Embedding Using Subjective Inter-speaker Similarity\n  for Multi-speaker Modeling in Speech Synthesis", "abstract": "This paper proposes novel algorithms for speaker embedding using subjective\ninter-speaker similarity based on deep neural networks (DNNs). Although\nconventional DNN-based speaker embedding such as a $d$-vector can be applied to\nmulti-speaker modeling in speech synthesis, it does not correlate with the\nsubjective inter-speaker similarity and is not necessarily appropriate speaker\nrepresentation for open speakers whose speech utterances are not included in\nthe training data. We propose two training algorithms for DNN-based speaker\nembedding model using an inter-speaker similarity matrix obtained by\nlarge-scale subjective scoring. One is based on similarity vector embedding and\ntrains the model to predict a vector of the similarity matrix as speaker\nrepresentation. The other is based on similarity matrix embedding and trains\nthe model to minimize the squared Frobenius norm between the similarity matrix\nand the Gram matrix of $d$-vectors, i.e., the inter-speaker similarity derived\nfrom the $d$-vectors. We crowdsourced the inter-speaker similarity scores of\n153 Japanese female speakers, and the experimental results demonstrate that our\nalgorithms learn speaker embedding that is highly correlated with the\nsubjective similarity. We also apply the proposed speaker embedding to\nmulti-speaker modeling in DNN-based speech synthesis and reveal that the\nproposed similarity vector embedding improves synthetic speech quality for open\nspeakers whose speech utterances are unseen during the training.", "published": "2019-07-19 09:11:35", "link": "http://arxiv.org/abs/1907.08294v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Batch Uniformization for Minimizing Maximum Anomaly Score of DNN-based\n  Anomaly Detection in Sounds", "abstract": "Use of an autoencoder (AE) as a normal model is a state-of-the-art technique\nfor unsupervised-anomaly detection in sounds (ADS). The AE is trained to\nminimize the sample mean of the anomaly score of normal sounds in a mini-batch.\nOne problem with this approach is that the anomaly score of rare-normal sounds\nbecomes higher than that of frequent-normal sounds, because the sample mean is\nstrongly affected by frequent-normal samples, resulting in preferentially\ndecreasing the anomaly score of frequent-normal samples. To decrease anomaly\nscores for both frequent- and rare-normal sounds, we propose batch\nuniformization, a training method for unsupervised-ADS for minimizing a\nweighted average of the anomaly score on each sample in a mini-batch. We used\nthe reciprocal of the probabilistic density of each sample as the weight, more\nintuitively, a large weight is given for rare-normal sounds. Such a weight\nworks to give a constant anomaly score for both frequent- and rare-normal\nsounds. Since the probabilistic density is unknown, we estimate it by using the\nkernel density estimation on each training mini-batch. Verification- and\nobjective-experiments show that the proposed batch uniformization improves the\nperformance of unsupervised-ADS.", "published": "2019-07-19 01:58:20", "link": "http://arxiv.org/abs/1907.08338v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Language Modelling for Sound Event Detection with Teacher Forcing and\n  Scheduled Sampling", "abstract": "A sound event detection (SED) method typically takes as an input a sequence\nof audio frames and predicts the activities of sound events in each frame. In\nreal-life recordings, the sound events exhibit some temporal structure: for\ninstance, a \"car horn\" will likely be followed by a \"car passing by\". While\nthis temporal structure is widely exploited in sequence prediction tasks (e.g.,\nin machine translation), where language models (LM) are exploited, it is not\nsatisfactorily modeled in SED. In this work we propose a method which allows a\nrecurrent neural network (RNN) to learn an LM for the SED task. The method\nconditions the input of the RNN with the activities of classes at the previous\ntime step. We evaluate our method using F1 score and error rate (ER) over three\ndifferent and publicly available datasets; the TUT-SED Synthetic 2016 and the\nTUT Sound Events 2016 and 2017 datasets. The obtained results show an increase\nof 9% and 2% at the F1 (higher is better) and a decrease of 7% and 2% at ER\n(lower is better) for the TUT Sound Events 2016 and 2017 datasets,\nrespectively, when using our method. On the contrary, with our method there is\na decrease of 4% at F1 score and an increase of 7% at ER for the TUT-SED\nSynthetic 2016 dataset.", "published": "2019-07-19 13:26:03", "link": "http://arxiv.org/abs/1907.08506v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Search by Text Description or Vocal Imitation?", "abstract": "Searching sounds by text labels is often difficult, as text descriptions\ncannot describe the audio content in detail. Query by vocal imitation bridges\nsuch gap and provides a novel way to sound search. Several algorithms for sound\nsearch by vocal imitation have been proposed and evaluated in a simulation\nenvironment, however, they have not been deployed into a real search engine nor\nevaluated by real users. This pilot work conducts a subjective study to compare\nthese two approaches to sound search, and tries to answer the question of which\napproach works better for what kinds of sounds. To do so, we developed two\nweb-based search engines for sound, one by vocal imitation (Vroom!) and the\nother by text description (TextSearch). We also developed an experimental\nframework to host these engines to collect statistics of user behaviors and\nratings. Results showed that Vroom! received significantly higher search\nsatisfaction ratings than TextSearch did for sound categories that were\ndifficult for subjects to describe by text. Results also showed a better\noverall ease-of-use rating for Vroom! than TextSearch on the limited sound\nlibrary in our experiments. These findings suggest advantages of\nvocal-imitation-based search for sound in practice.", "published": "2019-07-19 19:33:56", "link": "http://arxiv.org/abs/1907.08661v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
