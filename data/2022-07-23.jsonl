{"title": "Enhancing Document-level Relation Extraction by Entity Knowledge\n  Injection", "abstract": "Document-level relation extraction (RE) aims to identify the relations\nbetween entities throughout an entire document. It needs complex reasoning\nskills to synthesize various knowledge such as coreferences and commonsense.\nLarge-scale knowledge graphs (KGs) contain a wealth of real-world facts, and\ncan provide valuable knowledge to document-level RE. In this paper, we propose\nan entity knowledge injection framework to enhance current document-level RE\nmodels. Specifically, we introduce coreference distillation to inject\ncoreference knowledge, endowing an RE model with the more general capability of\ncoreference reasoning. We also employ representation reconciliation to inject\nfactual knowledge and aggregate KG representations and document representations\ninto a unified space. The experiments on two benchmark datasets validate the\ngeneralization of our entity knowledge injection framework and the consistent\nimprovement to several document-level RE models.", "published": "2022-07-23 06:45:11", "link": "http://arxiv.org/abs/2207.11433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Reasoning Behind Classification Predictions with BERT for Fake\n  News Detection", "abstract": "Fake news detection has become a major task to solve as there has been an\nincreasing number of fake news on the internet in recent years. Although many\nclassification models have been proposed based on statistical learning methods\nshowing good results, reasoning behind the classification performances may not\nbe enough. In the self-supervised learning studies, it has been highlighted\nthat a quality of representation (embedding) space matters and directly affects\na downstream task performance. In this study, a quality of the representation\nspace is analyzed visually and analytically in terms of linear separability for\ndifferent classes on a real and fake news dataset. To further add\ninterpretability to a classification model, a modification of Class Activation\nMapping (CAM) is proposed. The modified CAM provides a CAM score for each word\ntoken, where the CAM score on a word token denotes a level of focus on that\nword token to make the prediction. Finally, it is shown that the naive BERT\nmodel topped with a learnable linear layer is enough to achieve robust\nperformance while being compatible with CAM.", "published": "2022-07-23 17:54:48", "link": "http://arxiv.org/abs/2207.11562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facing Changes: Continual Entity Alignment for Growing Knowledge Graphs", "abstract": "Entity alignment is a basic and vital technique in knowledge graph (KG)\nintegration. Over the years, research on entity alignment has resided on the\nassumption that KGs are static, which neglects the nature of growth of\nreal-world KGs. As KGs grow, previous alignment results face the need to be\nrevisited while new entity alignment waits to be discovered. In this paper, we\npropose and dive into a realistic yet unexplored setting, referred to as\ncontinual entity alignment. To avoid retraining an entire model on the whole\nKGs whenever new entities and triples come, we present a continual alignment\nmethod for this task. It reconstructs an entity's representation based on\nentity adjacency, enabling it to generate embeddings for new entities quickly\nand inductively using their existing neighbors. It selects and replays partial\npre-aligned entity pairs to train only parts of KGs while extracting\ntrustworthy alignment for knowledge augmentation. As growing KGs inevitably\ncontain non-matchable entities, different from previous works, the proposed\nmethod employs bidirectional nearest neighbor matching to find new entity\nalignment and update old alignment. Furthermore, we also construct new datasets\nby simulating the growth of multilingual DBpedia. Extensive experiments\ndemonstrate that our continual alignment method is more effective than\nbaselines based on retraining or inductive learning.", "published": "2022-07-23 06:52:44", "link": "http://arxiv.org/abs/2207.11436v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to\n  Protect Privacy of Individuals on Twitter", "abstract": "The recent advances in natural language processing have yielded many exciting\ndevelopments in text analysis and language understanding models; however, these\nmodels can also be used to track people, bringing severe privacy concerns. In\nthis work, we investigate what individuals can do to avoid being detected by\nthose models while using social media platforms. We ground our investigation in\ntwo exposure-risky tasks, stance detection and geotagging. We explore a variety\nof simple techniques for modifying text, such as inserting typos in salient\nwords, paraphrasing, and adding dummy social media posts. Our experiments show\nthat the performance of BERT-based models fined tuned for stance detection\ndecreases significantly due to typos, but it is not affected by paraphrasing.\nMoreover, we find that typos have minimal impact on state-of-the-art geotagging\nmodels due to their increased reliance on social networks; however, we show\nthat users can deceive those models by interacting with different users,\nreducing their performance by almost 50%.", "published": "2022-07-23 11:55:18", "link": "http://arxiv.org/abs/2207.11500v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Context based lemmatizer for Polish language", "abstract": "Lemmatization is the process of grouping together the inflected forms of a\nword so they can be analysed as a single item, identified by the word's lemma,\nor dictionary form. In computational linguistics, lemmatisation is the\nalgorithmic process of determining the lemma of a word based on its intended\nmeaning. Unlike stemming, lemmatisation depends on correctly identifying the\nintended part of speech and meaning of a word in a sentence, as well as within\nthe larger context surrounding that sentence. As a result, developing efficient\nlemmatisation algorithm is the complex task. In recent years it can be observed\nthat deep learning models used for this task outperform other methods including\nmachine learning algorithms. In this paper the polish lemmatizer based on\nGoogle T5 model is presented. The training was run with different context\nlengths. The model achieves the best results for polish language lemmatisation\nprocess.", "published": "2022-07-23 18:02:16", "link": "http://arxiv.org/abs/2207.11565v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chunk-aware Alignment and Lexical Constraint for Visual Entailment with\n  Natural Language Explanations", "abstract": "Visual Entailment with natural language explanations aims to infer the\nrelationship between a text-image pair and generate a sentence to explain the\ndecision-making process. Previous methods rely mainly on a pre-trained\nvision-language model to perform the relation inference and a language model to\ngenerate the corresponding explanation. However, the pre-trained\nvision-language models mainly build token-level alignment between text and\nimage yet ignore the high-level semantic alignment between the phrases (chunks)\nand visual contents, which is critical for vision-language reasoning. Moreover,\nthe explanation generator based only on the encoded joint representation does\nnot explicitly consider the critical decision-making points of relation\ninference. Thus the generated explanations are less faithful to visual-language\nreasoning. To mitigate these problems, we propose a unified Chunk-aware\nAlignment and Lexical Constraint based method, dubbed as CALeC. It contains a\nChunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical\nConstraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence\nstructure inherent in language and various image regions to build chunk-aware\nsemantic alignment. Relation inferrer uses an attention-based reasoning network\nto incorporate the token-level and chunk-level vision-language representations.\nLeCG utilizes lexical constraints to expressly incorporate the words or chunks\nfocused by the relation inferrer into explanation generation, improving the\nfaithfulness and informativeness of the explanations. We conduct extensive\nexperiments on three datasets, and experimental results indicate that CALeC\nsignificantly outperforms other competitor models on inference accuracy and\nquality of generated explanations.", "published": "2022-07-23 03:19:50", "link": "http://arxiv.org/abs/2207.11401v2", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "$\u03bc\\text{KG}$: A Library for Multi-source Knowledge Graph Embeddings\n  and Applications", "abstract": "This paper presents $\\mu\\text{KG}$, an open-source Python library for\nrepresentation learning over knowledge graphs. $\\mu\\text{KG}$ supports joint\nrepresentation learning over multi-source knowledge graphs (and also a single\nknowledge graph), multiple deep learning libraries (PyTorch and TensorFlow2),\nmultiple embedding tasks (link prediction, entity alignment, entity typing, and\nmulti-source link prediction), and multiple parallel computing modes\n(multi-process and multi-GPU computing). It currently implements 26 popular\nknowledge graph embedding models and supports 16 benchmark datasets.\n$\\mu\\text{KG}$ provides advanced implementations of embedding techniques with\nsimplified pipelines of different tasks. It also comes with high-quality\ndocumentation for ease of use. $\\mu\\text{KG}$ is more comprehensive than\nexisting knowledge graph embedding libraries. It is useful for a thorough\ncomparison and analysis of various embedding models and tasks. We show that the\njointly learned embeddings can greatly help knowledge-powered downstream tasks,\nsuch as multi-hop knowledge graph question answering. We will stay abreast of\nthe latest developments in the related fields and incorporate them into\n$\\mu\\text{KG}$.", "published": "2022-07-23 07:11:42", "link": "http://arxiv.org/abs/2207.11442v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vaccine Discourse on Twitter During the COVID-19 Pandemic", "abstract": "Since the onset of the COVID-19 pandemic, vaccines have been an important\ntopic in public discourse. The discussions around vaccines are polarized as\nsome see them as an important measure to end the pandemic, and others are\nhesitant or find them harmful. This study investigates posts related to\nCOVID-19 vaccines on Twitter and focuses on those which have a negative stance\ntoward vaccines. A dataset of 16,713,238 English tweets related to COVID-19\nvaccines was collected covering the period from March 1, 2020, to July 31,\n2021. We used the Scikit-learn Python library to apply a support vector machine\n(SVM) classifier to identify the tweets with a negative stance toward the\nCOVID-19 vaccines. A total of 5,163 tweets were used to train the classifier,\nout of which a subset of 2,484 tweets were manually annotated by us and made\npublicly available. We used the BERTtopic model to extract and investigate the\ntopics discussed within the negative tweets and how they changed over time. We\nshow that the negativity with respect to COVID-19 vaccines has decreased over\ntime along with the vaccine roll-outs. We identify 37 topics of discussion and\npresent their respective importance over time. We show that popular topics\nconsist of conspiratorial discussions such as 5G towers and microchips, but\nalso contain legitimate concerns around vaccination safety and side effects as\nwell as concerns about policies. Our study shows that even unpopular opinions\nor conspiracy theories can become widespread when paired with a widely popular\ndiscussion topic such as COVID-19 vaccines. Understanding the concerns and the\ndiscussed topics and how they change over time is essential for policymakers\nand public health authorities to provide better and in-time information and\npolicies, to facilitate vaccination of the population in future similar crises.", "published": "2022-07-23 13:50:51", "link": "http://arxiv.org/abs/2207.11521v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Supporting peace negotiations in the Yemen war through machine learning", "abstract": "Today's conflicts are becoming increasingly complex, fluid and fragmented,\noften involving a host of national and international actors with multiple and\noften divergent interests. This development poses significant challenges for\nconflict mediation, as mediators struggle to make sense of conflict dynamics,\nsuch as the range of conflict parties and the evolution of their political\npositions, the distinction between relevant and less relevant actors in\npeace-making, or the identification of key conflict issues and their\ninterdependence. International peace efforts appear ill-equipped to\nsuccessfully address these challenges. While technology is already being\nexperimented with and used in a range of conflict related fields, such as\nconflict predicting or information gathering, less attention has been given to\nhow technology can contribute to conflict mediation. This case study\ncontributes to emerging research on the use of state-of-the-art machine\nlearning technologies and techniques in conflict mediation processes. Using\ndialogue transcripts from peace negotiations in Yemen, this study shows how\nmachine-learning can effectively support mediating teams by providing them with\ntools for knowledge management, extraction and conflict analysis. Apart from\nillustrating the potential of machine learning tools in conflict mediation, the\npaper also emphasises the importance of interdisciplinary and participatory,\nco-creation methodology for the development of context-sensitive and targeted\ntools and to ensure meaningful and responsible implementation.", "published": "2022-07-23 14:24:38", "link": "http://arxiv.org/abs/2207.11528v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-Complexity Acoustic Echo Cancellation with Neural Kalman Filtering", "abstract": "The Kalman filter has been adopted in acoustic echo cancellation due to its\nrobustness to double-talk, fast convergence, and good steady-state performance.\nThe performance of Kalman filter is closely related to the estimation accuracy\nof the state noise covariance and the observation noise covariance. The\nestimation error may lead to unacceptable results, especially when the echo\npath suffers abrupt changes, the tracking performance of the Kalman filter\ncould be degraded significantly. In this paper, we propose the neural Kalman\nfiltering (NKF), which uses neural networks to implicitly model the covariance\nof the state noise and observation noise and to output the Kalman gain in\nreal-time. Experimental results on both synthetic test sets and real-recorded\ntest sets show that, the proposed NKF has superior convergence and\nre-convergence performance while ensuring low near-end speech degradation\ncomparing with the state-of-the-art model-based methods. Moreover, the model\nsize of the proposed NKF is merely 5.3 K and the RTF is as low as 0.09, which\nindicates that it can be deployed in low-resource platforms.", "published": "2022-07-23 01:41:17", "link": "http://arxiv.org/abs/2207.11388v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-complexity CNNs for Acoustic Scene Classification", "abstract": "This paper presents a low-complexity framework for acoustic scene\nclassification (ASC). Most of the frameworks designed for ASC use convolutional\nneural networks (CNNs) due to their learning ability and improved performance\ncompared to hand-engineered features. However, CNNs are resource hungry due to\ntheir large size and high computational complexity. Therefore, CNNs are\ndifficult to deploy on resource constrained devices. This paper addresses the\nproblem of reducing the computational complexity and memory requirement in\nCNNs. We propose a low-complexity CNN architecture, and apply pruning and\nquantization to further reduce the parameters and memory. We then propose an\nensemble framework that combines various low-complexity CNNs to improve the\noverall performance. An experimental evaluation of the proposed framework is\nperformed on the publicly available DCASE 2022 Task 1 that focuses on ASC. The\nproposed ensemble framework has approximately 60K parameters, requires 19M\nmultiply-accumulate operations and improves the performance by approximately\n2-4 percentage points compared to the DCASE 2022 Task 1 baseline network.", "published": "2022-07-23 14:37:39", "link": "http://arxiv.org/abs/2207.11529v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For\n  Gesture And Speech Recognition", "abstract": "In this article gesture recognition and speech recognition applications are\nimplemented on embedded systems with Tiny Machine Learning (TinyML). It\nfeatures 3-axis accelerometer, 3-axis gyroscope and 3-axis magnetometer. The\ngesture recognition,provides an innovative approach nonverbal communication. It\nhas wide applications in human-computer interaction and sign language. Here in\nthe implementation of hand gesture recognition, TinyML model is trained and\ndeployed from EdgeImpulse framework for hand gesture recognition and based on\nthe hand movements, Arduino Nano 33 BLE device having 6-axis IMU can find out\nthe direction of movement of hand. The Speech is a mode of communication.\nSpeech recognition is a way by which the statements or commands of human speech\nis understood by the computer which reacts accordingly. The main aim of speech\nrecognition is to achieve communication between man and machine. Here in the\nimplementation of speech recognition, TinyML model is trained and deployed from\nEdgeImpulse framework for speech recognition and based on the keywords\npronounced by human, Arduino Nano 33 BLE device having built-in microphone can\nmake an RGB LED glow like red, green or blue based on keyword pronounced. The\nresults of each application are obtained and listed in the results section and\ngiven the analysis upon the results.", "published": "2022-07-23 10:53:26", "link": "http://arxiv.org/abs/2207.12866v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
