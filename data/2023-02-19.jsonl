{"title": "SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface\n  for Pedagogical and Annotation Purposes", "abstract": "We present a neural Sanskrit Natural Language Processing (NLP) toolkit named\nSanskritShala (a school of Sanskrit) to facilitate computational linguistic\nanalyses for several tasks such as word segmentation, morphological tagging,\ndependency parsing, and compound type identification. Our systems currently\nreport state-of-the-art performance on available benchmark datasets for all\ntasks. SanskritShala is deployed as a web-based application, which allows a\nuser to get real-time analysis for the given input. It is built with\neasy-to-use interactive data annotation features that allow annotators to\ncorrect the system predictions when it makes mistakes. We publicly release the\nsource codes of the 4 modules included in the toolkit, 7 word embedding models\nthat have been trained on publicly available Sanskrit corpora and multiple\nannotated datasets such as word similarity, relatedness, categorization,\nanalogy prediction to assess intrinsic properties of word embeddings. So far as\nwe know, this is the first neural-based Sanskrit NLP toolkit that has a\nweb-based interface and a number of NLP modules. We are sure that the people\nwho are willing to work with Sanskrit will find it useful for pedagogical and\nannotative purposes. SanskritShala is available at:\nhttps://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can be\naccessed at: https://youtu.be/x0X31Y9k0mw4.", "published": "2023-02-19 09:58:55", "link": "http://arxiv.org/abs/2302.09527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Content Moderation: A Case Study on Reddit", "abstract": "Content moderation is the process of flagging content based on pre-defined\nplatform rules. There has been a growing need for AI moderators to safeguard\nusers as well as protect the mental health of human moderators from traumatic\ncontent. While prior works have focused on identifying hateful/offensive\nlanguage, they are not adequate for meeting the challenges of content\nmoderation since 1) moderation decisions are based on violation of rules, which\nsubsumes detection of offensive speech, and 2) such rules often differ across\ncommunities which entails an adaptive solution. We propose to study the\nchallenges of content moderation by introducing a multilingual dataset of 1.8\nMillion Reddit comments spanning 56 subreddits in English, German, Spanish and\nFrench. We perform extensive experimental analysis to highlight the underlying\nchallenges and suggest related research problems such as cross-lingual\ntransfer, learning under label noise (human biases), transfer of moderation\nmodels, and predicting the violated rule. Our dataset and analysis can help\nbetter prepare for the challenges and opportunities of auto moderation.", "published": "2023-02-19 16:36:33", "link": "http://arxiv.org/abs/2302.09618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and\n  Fine-tuned BERT", "abstract": "Recently, ChatGPT has attracted great attention, as it can generate fluent\nand high-quality responses to human inquiries. Several prior studies have shown\nthat ChatGPT attains remarkable generation ability compared with existing\nmodels. However, the quantitative analysis of ChatGPT's understanding ability\nhas been given little attention. In this report, we explore the understanding\nability of ChatGPT by evaluating it on the most popular GLUE benchmark, and\ncomparing it with 4 representative fine-tuned BERT-style models. We find that:\n1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT\noutperforms all BERT models on inference tasks by a large margin; 3) ChatGPT\nachieves comparable performance compared with BERT on sentiment analysis and\nquestion-answering tasks. Additionally, by combining some advanced prompting\nstrategies, we show that the understanding ability of ChatGPT can be further\nimproved.", "published": "2023-02-19 12:29:33", "link": "http://arxiv.org/abs/2302.10198v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT (Feb 13 Version) is a Chinese Room", "abstract": "ChatGPT has gained both positive and negative publicity after reports\nsuggesting that it is able to pass various professional and licensing\nexaminations. This suggests that ChatGPT may pass Turing Test in the near\nfuture. However, a computer program that passing Turing Test can either mean\nthat it is a Chinese Room or artificially conscious. Hence, the question of\nwhether the current state of ChatGPT is more of a Chinese Room or approaching\nartificial consciousness remains. Here, I demonstrate that the current version\nof ChatGPT (Feb 13 version) is a Chinese Room. Despite potential evidence of\ncognitive connections, ChatGPT exhibits critical errors in causal reasoning. At\nthe same time, I demonstrate that ChatGPT can generate all possible categorical\nresponses to the same question and response with erroneous examples; thus,\nquestioning its utility as a learning tool. I also show that ChatGPT is capable\nof artificial hallucination, which is defined as generating confidently wrong\nreplies. It is likely that errors in causal reasoning leads to hallucinations.\nMore critically, ChatGPT generates false references to mimic real publications.\nTherefore, its utility is cautioned.", "published": "2023-02-19 01:52:06", "link": "http://arxiv.org/abs/2304.12411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Classification in the Wild: a Large-scale Long-tailed Name\n  Normalization Dataset", "abstract": "Real-world data usually exhibits a long-tailed distribution,with a few\nfrequent labels and a lot of few-shot labels. The study of institution name\nnormalization is a perfect application case showing this phenomenon. There are\nmany institutions worldwide with enormous variations of their names in the\npublicly available literature. In this work, we first collect a large-scale\ninstitution name normalization dataset LoT-insts1, which contains over 25k\nclasses that exhibit a naturally long-tailed distribution. In order to isolate\nthe few-shot and zero-shot learning scenarios from the massive many-shot\nclasses, we construct our test set from four different subsets: many-, medium-,\nand few-shot sets, as well as a zero-shot open set. We also replicate several\nimportant baseline methods on our data, covering a wide range from search-based\nmethods to neural network methods that use the pretrained BERT model. Further,\nwe propose our specially pretrained, BERT-based model that shows better\nout-of-distribution generalization on few-shot and zero-shot test sets.\nCompared to other datasets focusing on the long-tailed phenomenon, our dataset\nhas one order of magnitude more training data than the largest existing\nlong-tailed datasets and is naturally long-tailed rather than manually\nsynthesized. We believe it provides an important and different scenario to\nstudy this problem. To our best knowledge, this is the first natural language\ndataset that focuses on long-tailed and open-set classification problems.", "published": "2023-02-19 08:44:21", "link": "http://arxiv.org/abs/2302.09509v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language-Specific Representation of Emotion-Concept Knowledge Causally\n  Supports Emotion Inference", "abstract": "Humans no doubt use language to communicate about their emotional\nexperiences, but does language in turn help humans understand emotions, or is\nlanguage just a vehicle of communication? This study used a form of artificial\nintelligence (AI) known as large language models (LLMs) to assess whether\nlanguage-based representations of emotion causally contribute to the AI's\nability to generate inferences about the emotional meaning of novel situations.\nFourteen attributes of human emotion concept representation were found to be\nrepresented by the LLM's distinct artificial neuron populations. By\nmanipulating these attribute-related neurons, we in turn demonstrated the role\nof emotion concept knowledge in generative emotion inference. The\nattribute-specific performance deterioration was related to the importance of\ndifferent attributes in human mental space. Our findings provide a\nproof-in-concept that even a LLM can learn about emotions in the absence of\nsensory-motor representations and highlight the contribution of\nlanguage-derived emotion-concept knowledge for emotion inference.", "published": "2023-02-19 14:21:33", "link": "http://arxiv.org/abs/2302.09582v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Exploring the Potential of Machine Translation for Generating Named\n  Entity Datasets: A Case Study between Persian and English", "abstract": "This study focuses on the generation of Persian named entity datasets through\nthe application of machine translation on English datasets. The generated\ndatasets were evaluated by experimenting with one monolingual and one\nmultilingual transformer model. Notably, the CoNLL 2003 dataset has achieved\nthe highest F1 score of 85.11%. In contrast, the WNUT 2017 dataset yielded the\nlowest F1 score of 40.02%. The results of this study highlight the potential of\nmachine translation in creating high-quality named entity recognition datasets\nfor low-resource languages like Persian. The study compares the performance of\nthese generated datasets with English named entity recognition systems and\nprovides insights into the effectiveness of machine translation for this task.\nAdditionally, this approach could be used to augment data in low-resource\nlanguage or create noisy data to make named entity systems more robust and\nimprove them.", "published": "2023-02-19 16:12:21", "link": "http://arxiv.org/abs/2302.09611v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained\n  Transformers", "abstract": "Knowledge distillation has been shown to be a powerful model compression\napproach to facilitate the deployment of pre-trained language models in\npractice. This paper focuses on task-agnostic distillation. It produces a\ncompact pre-trained model that can be easily fine-tuned on various tasks with\nsmall computational costs and memory footprints. Despite the practical\nbenefits, task-agnostic distillation is challenging. Since the teacher model\nhas a significantly larger capacity and stronger representation power than the\nstudent model, it is very difficult for the student to produce predictions that\nmatch the teacher's over a massive amount of open-domain training data. Such a\nlarge prediction discrepancy often diminishes the benefits of knowledge\ndistillation. To address this challenge, we propose Homotopic Distillation\n(HomoDistil), a novel task-agnostic distillation approach equipped with\niterative pruning. Specifically, we initialize the student model from the\nteacher model, and iteratively prune the student's neurons until the target\nwidth is reached. Such an approach maintains a small discrepancy between the\nteacher's and student's predictions throughout the distillation process, which\nensures the effectiveness of knowledge transfer. Extensive experiments\ndemonstrate that HomoDistil achieves significant improvements on existing\nbaselines.", "published": "2023-02-19 17:37:24", "link": "http://arxiv.org/abs/2302.09632v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Multilingual Neural Machine Translation", "abstract": "In this work, we provide a large-scale empirical study of the scaling\nproperties of multilingual neural machine translation models. We examine how\nincreases in the model size affect the model performance and investigate the\nrole of the training mixture composition on the scaling behavior. We find that\nchanging the weightings of the individual language pairs in the training\nmixture only affect the multiplicative factor of the scaling law. In\nparticular, we observe that multilingual models trained using different mixing\nrates all exhibit the same scaling exponent. Through a novel joint scaling law\nformulation, we compute the effective number of parameters allocated to each\nlanguage pair and examine the role of language similarity in the scaling\nbehavior of our models. We find little evidence that language similarity has\nany impact. In contrast, the direction of the multilinguality plays a\nsignificant role, with models translating from multiple languages into English\nhaving a larger number of effective parameters per task than their reversed\ncounterparts. Finally, we leverage our observations to predict the performance\nof multilingual models trained with any language weighting at any scale,\nsignificantly reducing efforts required for language balancing in large\nmultilingual models. Our findings apply to both in-domain and out-of-domain\ntest sets and to multiple evaluation metrics, such as ChrF and BLEURT.", "published": "2023-02-19 18:43:24", "link": "http://arxiv.org/abs/2302.09650v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Intent Identification and Entity Extraction for Healthcare Queries in\n  Indic Languages", "abstract": "Scarcity of data and technological limitations for resource-poor languages in\ndeveloping countries like India poses a threat to the development of\nsophisticated NLU systems for healthcare. To assess the current status of\nvarious state-of-the-art language models in healthcare, this paper studies the\nproblem by initially proposing two different Healthcare datasets, Indian\nHealthcare Query Intent-WebMD and 1mg (IHQID-WebMD and IHQID-1mg) and one real\nworld Indian hospital query data in English and multiple Indic languages\n(Hindi, Bengali, Tamil, Telugu, Marathi and Gujarati) which are annotated with\nthe query intents as well as entities. Our aim is to detect query intents and\nextract corresponding entities. We perform extensive experiments on a set of\nmodels in various realistic settings and explore two scenarios based on the\naccess to English data only (less costly) and access to target language data\n(more expensive). We analyze context specific practical relevancy through\nempirical analysis. The results, expressed in terms of overall F1 score show\nthat our approach is practically useful to identify intents and entities.", "published": "2023-02-19 22:53:03", "link": "http://arxiv.org/abs/2302.09685v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Evaluating the Effectiveness of Pre-trained Language Models in\n  Predicting the Helpfulness of Online Product Reviews", "abstract": "Businesses and customers can gain valuable information from product reviews.\nThe sheer number of reviews often necessitates ranking them based on their\npotential helpfulness. However, only a few reviews ever receive any helpfulness\nvotes on online marketplaces. Sorting all reviews based on the few existing\nvotes can cause helpful reviews to go unnoticed because of the limited\nattention span of readers. The problem of review helpfulness prediction is even\nmore important for higher review volumes, and newly written reviews or launched\nproducts. In this work we compare the use of RoBERTa and XLM-R language models\nto predict the helpfulness of online product reviews. The contributions of our\nwork in relation to literature include extensively investigating the efficacy\nof state-of-the-art language models -- both monolingual and multilingual --\nagainst a robust baseline, taking ranking metrics into account when assessing\nthese approaches, and assessing multilingual models for the first time. We\nemploy the Amazon review dataset for our experiments. According to our study on\nseveral product categories, multilingual and monolingual pre-trained language\nmodels outperform the baseline that utilizes random forest with handcrafted\nfeatures as much as 23% in RMSE. Pre-trained language models reduce the need\nfor complex text feature engineering. However, our results suggest that\npre-trained multilingual models may not be used for fine-tuning only one\nlanguage. We assess the performance of language models with and without\nadditional features. Our results show that including additional features like\nproduct rating by the reviewer can further help the predictive methods.", "published": "2023-02-19 18:22:59", "link": "http://arxiv.org/abs/2302.10199v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Language Representations with Logical Inductive Bias", "abstract": "Transformer architectures have achieved great success in solving natural\nlanguage tasks, which learn strong language representations from large-scale\nunlabeled texts. In this paper, we seek to go further beyond and explore a new\nlogical inductive bias for better language representation learning. Logic\nreasoning is known as a formal methodology to reach answers from given\nknowledge and facts. Inspired by such a view, we develop a novel neural\narchitecture named FOLNet (First-Order Logic Network), to encode this new\ninductive bias. We construct a set of neural logic operators as learnable Horn\nclauses, which are further forward-chained into a fully differentiable neural\narchitecture (FOLNet). Interestingly, we find that the self-attention module in\ntransformers can be composed by two of our neural logic operators, which\nprobably explains their strong reasoning performance. Our proposed FOLNet has\nthe same input and output interfaces as other pretrained models and thus could\nbe pretrained/finetuned by using similar losses. It also allows FOLNet to be\nused in a plug-and-play manner when replacing other pretrained models. With our\nlogical inductive bias, the same set of ``logic deduction skills'' learned\nthrough pretraining are expected to be equally capable of solving diverse\ndownstream tasks. For this reason, FOLNet learns language representations that\nhave much stronger transfer capabilities. Experimental results on several\nlanguage understanding tasks show that our pretrained FOLNet model outperforms\nthe existing strong transformer-based approaches.", "published": "2023-02-19 02:21:32", "link": "http://arxiv.org/abs/2302.09458v1", "categories": ["cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Upvotes? Downvotes? No Votes? Understanding the relationship between\n  reaction mechanisms and political discourse on Reddit", "abstract": "A significant share of political discourse occurs online on social media\nplatforms. Policymakers and researchers try to understand the role of social\nmedia design in shaping the quality of political discourse around the globe. In\nthe past decades, scholarship on political discourse theory has produced\ndistinct characteristics of different types of prominent political rhetoric\nsuch as deliberative, civic, or demagogic discourse. This study investigates\nthe relationship between social media reaction mechanisms (i.e., upvotes,\ndownvotes) and political rhetoric in user discussions by engaging in an\nin-depth conceptual analysis of political discourse theory. First, we analyze\n155 million user comments in 55 political subforums on Reddit between 2010 and\n2018 to explore whether users' style of political discussion aligns with the\nessential components of deliberative, civic, and demagogic discourse. Second,\nwe perform a quantitative study that combines confirmatory factor analysis with\ndifference in differences models to explore whether different reaction\nmechanism schemes (e.g., upvotes only, upvotes and downvotes, no reaction\nmechanisms) correspond with political user discussion that is more or less\ncharacteristic of deliberative, civic, or demagogic discourse. We produce three\nmain takeaways. First, despite being \"ideal constructs of political rhetoric,\"\nwe find that political discourse theories describe political discussions on\nReddit to a large extent. Second, we find that discussions in subforums with\nonly upvotes, or both up- and downvotes are associated with user discourse that\nis more deliberate and civic. Third, social media discussions are most\ndemagogic in subreddits with no reaction mechanisms at all. These findings\noffer valuable contributions for ongoing policy discussions on the relationship\nbetween social media interface design and respectful political discussion among\nusers.", "published": "2023-02-19 11:12:45", "link": "http://arxiv.org/abs/2302.09540v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation\n  in Natural Language Generation", "abstract": "We introduce a method to measure uncertainty in large language models. For\ntasks like question answering, it is essential to know when we can trust the\nnatural language outputs of foundation models. We show that measuring\nuncertainty in natural language is challenging because of \"semantic\nequivalence\" -- different sentences can mean the same thing. To overcome these\nchallenges we introduce semantic entropy -- an entropy which incorporates\nlinguistic invariances created by shared meanings. Our method is unsupervised,\nuses only a single model, and requires no modifications to off-the-shelf\nlanguage models. In comprehensive ablation studies we show that the semantic\nentropy is more predictive of model accuracy on question answering data sets\nthan comparable baselines.", "published": "2023-02-19 20:10:07", "link": "http://arxiv.org/abs/2302.09664v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Reward-based Deep Reinforcement Learning for Intent\n  Analysis of Social Media Information", "abstract": "Due to various and serious adverse impacts of spreading fake news, it is\noften known that only people with malicious intent would propagate fake news.\nHowever, it is not necessarily true based on social science studies.\nDistinguishing the types of fake news spreaders based on their intent is\ncritical because it will effectively guide how to intervene to mitigate the\nspread of fake news with different approaches. To this end, we propose an\nintent classification framework that can best identify the correct intent of\nfake news. We will leverage deep reinforcement learning (DRL) that can optimize\nthe structural representation of each tweet by removing noisy words from the\ninput sequence when appending an actor to the long short-term memory (LSTM)\nintent classifier. Policy gradient DRL model (e.g., REINFORCE) can lead the\nactor to a higher delayed reward. We also devise a new uncertainty-aware\nimmediate reward using a subjective opinion that can explicitly deal with\nmultidimensional uncertainty for effective decision-making. Via 600K training\nepisodes from a fake news tweets dataset with an annotated intent class, we\nevaluate the performance of uncertainty-aware reward in DRL. Evaluation results\ndemonstrate that our proposed framework efficiently reduces the number of\nselected words to maintain a high 95\\% multi-class accuracy.", "published": "2023-02-19 00:54:33", "link": "http://arxiv.org/abs/2302.10195v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Video-Text Retrieval by Supervised Sparse Multi-Grained Learning", "abstract": "While recent progress in video-text retrieval has been advanced by the\nexploration of better representation learning, in this paper, we present a\nnovel multi-grained sparse learning framework, S3MA, to learn an aligned sparse\nspace shared between the video and the text for video-text retrieval. The\nshared sparse space is initialized with a finite number of sparse concepts,\neach of which refers to a number of words. With the text data at hand, we learn\nand update the shared sparse space in a supervised manner using the proposed\nsimilarity and alignment losses. Moreover, to enable multi-grained alignment,\nwe incorporate frame representations for better modeling the video modality and\ncalculating fine-grained and coarse-grained similarities. Benefiting from the\nlearned shared sparse space and multi-grained similarities, extensive\nexperiments on several video-text retrieval benchmarks demonstrate the\nsuperiority of S3MA over existing methods. Our code is available at\nhttps://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.", "published": "2023-02-19 04:03:22", "link": "http://arxiv.org/abs/2302.09473v2", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Few-shot Multimodal Multitask Multilingual Learning", "abstract": "While few-shot learning as a transfer learning paradigm has gained\nsignificant traction for scenarios with limited data, it has primarily been\nexplored in the context of building unimodal and unilingual models.\nFurthermore, a significant part of the existing literature in the domain of\nfew-shot multitask learning perform in-context learning which requires manually\ngenerated prompts as the input, yielding varying outcomes depending on the\nlevel of manual prompt-engineering. In addition, in-context learning suffers\nfrom substantial computational, memory, and storage costs which eventually\nleads to high inference latency because it involves running all of the prompt's\nexamples through the model every time a prediction is made. In contrast,\nmethods based on the transfer learning via the fine-tuning paradigm avoid the\naforementioned issues at a one-time cost of fine-tuning weights on a per-task\nbasis. However, such methods lack exposure to few-shot multimodal multitask\nlearning. In this paper, we propose few-shot learning for a multimodal\nmultitask multilingual (FM3) setting by adapting pre-trained vision and\nlanguage models using task-specific hypernetworks and contrastively fine-tuning\nthem to enable few-shot learning. FM3's architecture combines the best of both\nworlds of in-context and fine-tuning based learning and consists of three major\ncomponents: (i) multimodal contrastive fine-tuning to enable few-shot learning,\n(ii) hypernetwork task adaptation to perform multitask learning, and (iii)\ntask-specific output heads to cater to a plethora of diverse tasks. FM3 learns\nthe most prominent tasks in the vision and language domains along with their\nintersections, namely visual entailment (VE), visual question answering (VQA),\nand natural language understanding (NLU) tasks such as neural entity\nrecognition (NER) and the GLUE benchmark including QNLI, MNLI, QQP, and SST-2.", "published": "2023-02-19 03:48:46", "link": "http://arxiv.org/abs/2303.12489v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Probabilistic Back-ends for Online Speaker Recognition and Clustering", "abstract": "This paper focuses on multi-enrollment speaker recognition which naturally\noccurs in the task of online speaker clustering, and studies the properties of\ndifferent scoring back-ends in this scenario. First, we show that popular\ncosine scoring suffers from poor score calibration with a varying number of\nenrollment utterances. Second, we propose a simple replacement for cosine\nscoring based on an extremely constrained version of probabilistic linear\ndiscriminant analysis (PLDA). The proposed model improves over the cosine\nscoring for multi-enrollment recognition while keeping the same performance in\nthe case of one-to-one comparisons. Finally, we consider an online speaker\nclustering task where each step naturally involves multi-enrollment\nrecognition. We propose an online clustering algorithm allowing us to take\nbenefits from the PLDA model such as the ability to handle uncertainty and\nbetter score calibration. Our experiments demonstrate the effectiveness of the\nproposed algorithm.", "published": "2023-02-19 09:48:26", "link": "http://arxiv.org/abs/2302.09523v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
