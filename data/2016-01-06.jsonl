{"title": "Incorporating Structural Alignment Biases into an Attentional Neural\n  Translation Model", "abstract": "Neural encoder-decoder models of machine translation have achieved impressive\nresults, rivalling traditional translation models. However their modelling\nformulation is overly simplistic, and omits several key inductive biases built\ninto traditional models. In this paper we extend the attentional neural\ntranslation model to include structural biases from word based alignment\nmodels, including positional bias, Markov conditioning, fertility and agreement\nover translation directions. We show improvements over a baseline attentional\nmodel and standard phrase-based model over several language pairs, evaluating\non difficult languages in a low resource setting.", "published": "2016-01-06 06:03:17", "link": "http://arxiv.org/abs/1601.01085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON\n  2015", "abstract": "This paper discusses the experiments carried out by us at Jadavpur University\nas part of the participation in ICON 2015 task: POS Tagging for Code-mixed\nIndian Social Media Text. The tool that we have developed for the task is based\non Trigram Hidden Markov Model that utilizes information from dictionary as\nwell as some other word level features to enhance the observation probabilities\nof the known tokens as well as unknown tokens. We submitted runs for\nBengali-English, Hindi-English and Tamil-English Language pairs. Our system has\nbeen trained and tested on the datasets released for ICON 2015 shared task: POS\nTagging For Code-mixed Indian Social Media Text. In constrained mode, our\nsystem obtains average overall accuracy (averaged over all three language\npairs) of 75.60% which is very close to other participating two systems (76.79%\nfor IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In\nunconstrained mode, our system obtains average overall accuracy of 70.65% which\nis also close to the system (72.85% for AMRITA_CEN) which obtains the highest\naverage overall accuracy.", "published": "2016-01-06 14:40:38", "link": "http://arxiv.org/abs/1601.01195v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Recurrent Memory Networks for Language Modeling", "abstract": "Recurrent Neural Networks (RNN) have obtained excellent result in many\nnatural language processing (NLP) tasks. However, understanding and\ninterpreting the source of this success remains a challenge. In this paper, we\npropose Recurrent Memory Network (RMN), a novel RNN architecture, that not only\namplifies the power of RNN but also facilitates our understanding of its\ninternal functioning and allows us to discover underlying patterns in data. We\ndemonstrate the power of RMN on language modeling and sentence completion\ntasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)\nnetwork on three large German, Italian, and English dataset. Additionally we\nperform in-depth analysis of various linguistic dimensions that RMN captures.\nOn Sentence Completion Challenge, for which it is essential to capture sentence\ncoherence, our RMN obtains 69.2% accuracy, surpassing the previous\nstate-of-the-art by a large margin.", "published": "2016-01-06 18:44:07", "link": "http://arxiv.org/abs/1601.01272v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language to Logical Form with Neural Attention", "abstract": "Semantic parsing aims at mapping natural language to machine interpretable\nmeaning representations. Traditional approaches rely on high-quality lexicons,\nmanually-built templates, and linguistic features which are either domain- or\nrepresentation-specific. In this paper we present a general method based on an\nattention-enhanced encoder-decoder model. We encode input utterances into\nvector representations, and generate their logical forms by conditioning the\noutput sequences or trees on the encoding vectors. Experimental results on four\ndatasets show that our approach performs competitively without using\nhand-engineered features and is easy to adapt across domains and meaning\nrepresentations.", "published": "2016-01-06 19:13:12", "link": "http://arxiv.org/abs/1601.01280v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Learning of the Embedding of Words and Entities for Named Entity\n  Disambiguation", "abstract": "Named Entity Disambiguation (NED) refers to the task of resolving multiple\nnamed entity mentions in a document to their correct references in a knowledge\nbase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method\nspecifically designed for NED. The proposed method jointly maps words and\nentities into the same continuous vector space. We extend the skip-gram model\nby using two models. The KB graph model learns the relatedness of entities\nusing the link structure of the KB, whereas the anchor context model aims to\nalign vectors such that similar words and entities occur close to one another\nin the vector space by leveraging KB anchors and their context words. By\ncombining contexts based on the proposed embedding with standard NED features,\nwe achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset\nand 85.2% on the TAC 2010 dataset.", "published": "2016-01-06 22:19:20", "link": "http://arxiv.org/abs/1601.01343v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Way, Multilingual Neural Machine Translation with a Shared\n  Attention Mechanism", "abstract": "We propose multi-way, multilingual neural machine translation. The proposed\napproach enables a single neural translation model to translate between\nmultiple languages, with a number of parameters that grows only linearly with\nthe number of languages. This is made possible by having a single attention\nmechanism that is shared across all language pairs. We train the proposed\nmulti-way, multilingual model on ten language pairs from WMT'15 simultaneously\nand observe clear performance improvements over models trained on only one\nlanguage pair. In particular, we observe that the proposed model significantly\nimproves the translation quality of low-resource language pairs.", "published": "2016-01-06 04:00:50", "link": "http://arxiv.org/abs/1601.01073v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
