{"title": "Semi-supervised sequence tagging with bidirectional language models", "abstract": "Pre-trained word embeddings learned from unlabeled text have become a\nstandard component of neural network architectures for NLP tasks. However, in\nmost cases, the recurrent network that operates on word-level representations\nto produce context sensitive representations is trained on relatively little\nlabeled data. In this paper, we demonstrate a general semi-supervised approach\nfor adding pre- trained context embeddings from bidirectional language models\nto NLP systems and apply it to sequence labeling tasks. We evaluate our model\non two standard datasets for named entity recognition (NER) and chunking, and\nin both cases achieve state of the art results, surpassing previous systems\nthat use other forms of transfer or joint learning with additional labeled data\nand task specific gazetteers.", "published": "2017-04-29 01:13:04", "link": "http://arxiv.org/abs/1705.00108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lifelong Learning CRF for Supervised Aspect Extraction", "abstract": "This paper makes a focused contribution to supervised aspect extraction. It\nshows that if the system has performed aspect extraction from many past domains\nand retained their results as knowledge, Conditional Random Fields (CRF) can\nleverage this knowledge in a lifelong learning manner to extract in a new\ndomain markedly better than the traditional CRF without using this prior\nknowledge. The key innovation is that even after CRF training, the model can\nstill improve its extraction with experiences in its applications.", "published": "2017-04-29 23:33:13", "link": "http://arxiv.org/abs/1705.00251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "abstract": "We study automatic question generation for sentences from text passages in\nreading comprehension. We introduce an attention-based sequence learning model\nfor the task and investigate the effect of encoding sentence- vs.\nparagraph-level information. In contrast to all previous work, our model does\nnot rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead\ntrainable end-to-end via sequence-to-sequence learning. Automatic evaluation\nresults show that our system significantly outperforms the state-of-the-art\nrule-based system. In human evaluations, questions generated by our system are\nalso rated as being more natural (i.e., grammaticality, fluency) and as more\ndifficult to answer (in terms of syntactic and lexical divergence from the\noriginal text and reasoning needed to answer).", "published": "2017-04-29 01:08:48", "link": "http://arxiv.org/abs/1705.00106v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extending and Improving Wordnet via Unsupervised Word Embeddings", "abstract": "This work presents an unsupervised approach for improving WordNet that builds\nupon recent advances in document and sense representation via distributional\nsemantics. We apply our methods to construct Wordnets in French and Russian,\nlanguages which both lack good manual constructions.1 These are evaluated on\ntwo new 600-word test sets for word-to-synset matching and found to improve\ngreatly upon synset recall, outperforming the best automated Wordnets in\nF-score. Our methods require very few linguistic resources, thus being\napplicable for Wordnet construction in low-resources languages, and may further\nbe applied to sense clustering and other Wordnet improvements.", "published": "2017-04-29 17:50:02", "link": "http://arxiv.org/abs/1705.00217v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Representation Learning and Pairwise Ranking for Implicit Feedback in\n  Recommendation Systems", "abstract": "In this paper, we propose a novel ranking framework for collaborative\nfiltering with the overall aim of learning user preferences over items by\nminimizing a pairwise ranking loss. We show the minimization problem involves\ndependent random variables and provide a theoretical analysis by proving the\nconsistency of the empirical risk minimization in the worst case where all\nusers choose a minimal number of positive and negative items. We further derive\na Neural-Network model that jointly learns a new representation of users and\nitems in an embedded space as well as the preference relation of users over the\npairs of items. The learning objective is based on three scenarios of ranking\nlosses that control the ability of the model to maintain the ordering over the\nitems induced from the users' preferences, as well as, the capacity of the\ndot-product defined in the learned embedded space to produce the ordering. The\nproposed model is by nature suitable for implicit feedback and involves the\nestimation of only very few parameters. Through extensive experiments on\nseveral real-world benchmarks on implicit data, we show the interest of\nlearning the preference and the embedding simultaneously when compared to\nlearning those separately. We also demonstrate that our approach is very\ncompetitive with the best state-of-the-art collaborative filtering techniques\nproposed for implicit feedback.", "published": "2017-04-29 01:03:40", "link": "http://arxiv.org/abs/1705.00105v4", "categories": ["stat.ML", "cs.CL", "cs.IR"], "primary_category": "stat.ML"}
