{"title": "Combining Acoustics, Content and Interaction Features to Find Hot Spots\n  in Meetings", "abstract": "Involvement hot spots have been proposed as a useful concept for meeting\nanalysis and studied off and on for over 15 years. These are regions of\nmeetings that are marked by high participant involvement, as judged by human\nannotators. However, prior work was either not conducted in a formal machine\nlearning setting, or focused on only a subset of possible meeting features or\ndownstream applications (such as summarization). In this paper we investigate\nto what extent various acoustic, linguistic and pragmatic aspects of the\nmeetings, both in isolation and jointly, can help detect hot spots. In this\ncontext, the openSMILE toolkit is to used to extract features based on\nacoustic-prosodic cues, BERT word embeddings are used for encoding the lexical\ncontent, and a variety of statistics based on speech activity are used to\ndescribe the verbal interaction among participants. In experiments on the\nannotated ICSI meeting corpus, we find that the lexical model is the most\ninformative, with incremental contributions from interaction and\nacoustic-prosodic model components.", "published": "2019-10-24 01:18:24", "link": "http://arxiv.org/abs/1910.10869v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Sequence Labeling via Unsupervised Multilingual\n  Contextualized Representations", "abstract": "Previous work on cross-lingual sequence labeling tasks either requires\nparallel data or bridges the two languages through word-byword matching. Such\nrequirements and assumptions are infeasible for most languages, especially for\nlanguages with large linguistic distances, e.g., English and Chinese. In this\nwork, we propose a Multilingual Language Model with deep semantic Alignment\n(MLMA) to generate language-independent representations for cross-lingual\nsequence labeling. Our methods require only monolingual corpora with no\nbilingual resources at all and take advantage of deep contextualized\nrepresentations. Experimental results show that our approach achieves new\nstate-of-the-art NER and POS performance across European languages, and is also\neffective on distant language pairs such as English and Chinese.", "published": "2019-10-24 03:00:53", "link": "http://arxiv.org/abs/1910.10893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pun-GAN: Generative Adversarial Network for Pun Generation", "abstract": "In this paper, we focus on the task of generating a pun sentence given a pair\nof word senses. A major challenge for pun generation is the lack of large-scale\npun corpus to guide the supervised learning. To remedy this, we propose an\nadversarial generative network for pun generation (Pun-GAN), which does not\nrequire any pun corpus. It consists of a generator to produce pun sentences,\nand a discriminator to distinguish between the generated pun sentences and the\nreal sentences with specific word senses. The output of the discriminator is\nthen used as a reward to train the generator via reinforcement learning,\nencouraging it to produce pun sentences that can support two word senses\nsimultaneously. Experiments show that the proposed Pun-GAN can generate\nsentences that are more ambiguous and diverse in both automatic and human\nevaluation.", "published": "2019-10-24 07:26:36", "link": "http://arxiv.org/abs/1910.10950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wasserstein distances for evaluating cross-lingual embeddings", "abstract": "Word embeddings are high dimensional vector representations of words that\ncapture their semantic similarity in the vector space. There exist several\nalgorithms for learning such embeddings both for a single language as well as\nfor several languages jointly. In this work we propose to evaluate collections\nof embeddings by adapting downstream natural language tasks to the optimal\ntransport framework. We show how the family of Wasserstein distances can be\nused to solve cross-lingual document retrieval and the cross-lingual document\nclassification problems. We argue on the advantages of this approach compared\nto more traditional evaluation methods of embeddings like bilingual lexical\ninduction. Our experimental results suggest that using Wasserstein distances on\nthese problems out-performs several strong baselines and performs on par with\nstate-of-the-art models.", "published": "2019-10-24 10:04:12", "link": "http://arxiv.org/abs/1910.11005v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "abstract": "As a fundamental NLP task, semantic role labeling (SRL) aims to discover the\nsemantic roles for each predicate within one sentence. This paper investigates\nhow to incorporate syntactic knowledge into the SRL task effectively. We\npresent different approaches of encoding the syntactic information derived from\ndependency trees of different quality and representations; we propose a\nsyntax-enhanced self-attention model and compare it with other two strong\nbaseline methods; and we conduct experiments with newly published deep\ncontextualized word representations as well. The experiment results demonstrate\nthat with proper incorporation of the high quality syntactic information, our\nmodel achieves a new state-of-the-art performance for the Chinese SRL task on\nthe CoNLL-2009 dataset.", "published": "2019-10-24 15:05:01", "link": "http://arxiv.org/abs/1910.11204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Promoting the Knowledge of Source Syntax in Transformer NMT Is Not\n  Needed", "abstract": "The utility of linguistic annotation in neural machine translation seemed to\nhad been established in past papers. The experiments were however limited to\nrecurrent sequence-to-sequence architectures and relatively small data\nsettings. We focus on the state-of-the-art Transformer model and use comparably\nlarger corpora. Specifically, we try to promote the knowledge of source-side\nsyntax using multi-task learning either through simple data manipulation\ntechniques or through a dedicated model component. In particular, we train one\nof Transformer attention heads to produce source-side dependency tree. Overall,\nour results cast some doubt on the utility of multi-task setups with linguistic\ninformation. The data manipulation techniques, recommended in previous works,\nprove ineffective in large data settings. The treatment of self-attention as\ndependencies seems much more promising: it helps in translation and reveals\nthat Transformer model can very easily grasp the syntactic structure. An\nimportant but curious result is, however, that identical gains are obtained by\nusing trivial \"linear trees\" instead of true dependencies. The reason for the\ngain thus may not be coming from the added linguistic knowledge but from some\nsimpler regularizing effect we induced on self-attention matrices.", "published": "2019-10-24 15:23:08", "link": "http://arxiv.org/abs/1910.11218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00daFAL MRPipe at MRP 2019: UDPipe Goes Semantic in the Meaning\n  Representation Parsing Shared Task", "abstract": "We present a system description of our contribution to the CoNLL 2019 shared\ntask, Cross-Framework Meaning Representation Parsing (MRP 2019). The proposed\narchitecture is our first attempt towards a semantic parsing extension of the\nUDPipe 2.0, a lemmatization, POS tagging and dependency parsing pipeline.\n  For the MRP 2019, which features five formally and linguistically different\napproaches to meaning representation (DM, PSD, EDS, UCCA and AMR), we propose a\nuniform, language and framework agnostic graph-to-graph neural network\narchitecture. Without any knowledge about the graph structure, and specifically\nwithout any linguistically or framework motivated features, our system\nimplicitly models the meaning representation graphs.\n  After fixing a human error (we used earlier incorrect version of provided\ntest set analyses), our submission would score third in the competition\nevaluation. The source code of our system is available at\nhttps://github.com/ufal/mrpipe-conll2019.", "published": "2019-10-24 17:17:10", "link": "http://arxiv.org/abs/1910.11295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Quality Indicators in User-generated Content Using Social\n  Media and Scholarly Text", "abstract": "Predicting the quality of a text document is a critical task when presented\nwith the problem of measuring the performance of a document before its release.\nIn this work, we evaluate various features including those extracted from the\ntext content (textual) and those describing higher-level characteristics of the\ntext (meta) features that are not directly available from the text, and show\nhow these features inform prediction of document quality in different ways.\nMoreover, we also compare our methods on both social user-generated data such\nas tweets, and scholarly user-generated data such as academic articles, showing\nhow the same features differently influence prediction of quality across these\ndisparate domains.", "published": "2019-10-24 19:54:32", "link": "http://arxiv.org/abs/1910.11399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Document Summarization with Determinantal Point Processes and\n  Contextualized Representations", "abstract": "Emerged as one of the best performing techniques for extractive\nsummarization, determinantal point processes select the most probable set of\nsentences to form a summary according to a probability measure defined by\nmodeling sentence prominence and pairwise repulsion. Traditionally, these\naspects are modelled using shallow and linguistically informed features, but\nthe rise of deep contextualized representations raises an interesting question\nof whether, and to what extent, contextualized representations can be used to\nimprove DPP modeling. Our findings suggest that, despite the success of deep\nrepresentations, it remains necessary to combine them with surface indicators\nfor effective identification of summary sentences.", "published": "2019-10-24 20:14:12", "link": "http://arxiv.org/abs/1910.11411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GF + MMT = GLF -- From Language to Semantics through LF", "abstract": "These days, vast amounts of knowledge are available online, most of it in\nwritten form. Search engines help us access this knowledge, but aggregating,\nrelating and reasoning with it is still a predominantly human effort. One of\nthe key challenges for automated reasoning based on natural-language texts is\nthe need to extract meaning (semantics) from texts. Natural language\nunderstanding (NLU) systems describe the conversion from a set of natural\nlanguage utterances to terms in a particular logic. Tools for the\nco-development of grammar and target logic are currently largely missing.\n  We will describe the Grammatical Logical Framework (GLF), a combination of\ntwo existing frameworks, in which large parts of a symbolic, rule-based NLU\nsystem can be developed and implemented: the Grammatical Framework (GF) and\nMMT. GF is a tool for syntactic analysis, generation, and translation with\ncomplex natural language grammars and MMT can be used to specify logical\nsystems and to represent knowledge in them. Combining these tools is possible,\nbecause they are based on compatible logical frameworks: Martin-L\\\"of type\ntheory and LF. The flexibility of logical frameworks is needed, as NLU research\nhas not settled on a particular target logic for meaning representation.\nInstead, new logics are developed all the time to handle various language\nphenomena. GLF allows users to develop the logic and the language parsing\ncomponents in parallel, and to connect them for experimentation with the entire\npipeline.", "published": "2019-10-24 00:22:43", "link": "http://arxiv.org/abs/1910.10849v1", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Selective Attention Based Graph Convolutional Networks for Aspect-Level\n  Sentiment Classification", "abstract": "Aspect-level sentiment classification aims to identify the sentiment polarity\ntowards a specific aspect term in a sentence. Most current approaches mainly\nconsider the semantic information by utilizing attention mechanisms to capture\nthe interactions between the context and the aspect term. In this paper, we\npropose to employ graph convolutional networks (GCNs) on the dependency tree to\nlearn syntax-aware representations of aspect terms. GCNs often show the best\nperformance with two layers, and deeper GCNs do not bring additional gain due\nto over-smoothing problem. However, in some cases, important context words\ncannot be reached within two hops on the dependency tree. Therefore we design a\nselective attention based GCN block (SA-GCN) to find the most important context\nwords, and directly aggregate these information into the aspect-term\nrepresentation. We conduct experiments on the SemEval 2014 Task 4 datasets. Our\nexperimental results show that our model outperforms the current\nstate-of-the-art.", "published": "2019-10-24 00:33:39", "link": "http://arxiv.org/abs/1910.10857v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Man is to Person as Woman is to Location: Measuring Gender Bias in Named\n  Entity Recognition", "abstract": "We study the bias in several state-of-the-art named entity recognition (NER)\nmodels---specifically, a difference in the ability to recognize male and female\nnames as PERSON entity types. We evaluate NER models on a dataset containing\n139 years of U.S. census baby names and find that relatively more female names,\nas opposed to male names, are not recognized as PERSON entities. We study the\nextent of this bias in several NER systems that are used prominently in\nindustry and academia. In addition, we also report a bias in the datasets on\nwhich these models were trained. The result of this analysis yields a new\nbenchmark for gender bias evaluation in named entity recognition systems. The\ndata and code for the application of this benchmark will be publicly available\nfor researchers to use.", "published": "2019-10-24 01:32:24", "link": "http://arxiv.org/abs/1910.10872v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ESPnet-TTS: Unified, Reproducible, and Integratable Open Source\n  End-to-End Text-to-Speech Toolkit", "abstract": "This paper introduces a new end-to-end text-to-speech (E2E-TTS) toolkit named\nESPnet-TTS, which is an extension of the open-source speech processing toolkit\nESPnet. The toolkit supports state-of-the-art E2E-TTS models, including\nTacotron~2, Transformer TTS, and FastSpeech, and also provides recipes inspired\nby the Kaldi automatic speech recognition (ASR) toolkit. The recipes are based\non the design unified with the ESPnet ASR recipe, providing high\nreproducibility. The toolkit also provides pre-trained models and samples of\nall of the recipes so that users can use it as a baseline. Furthermore, the\nunified design enables the integration of ASR functions with TTS, e.g.,\nASR-based objective evaluation and semi-supervised learning with both ASR and\nTTS models. This paper describes the design of the toolkit and experimental\nevaluation in comparison with other toolkits. The experimental results show\nthat our models can achieve state-of-the-art performance comparable to the\nother latest toolkits, resulting in a mean opinion score (MOS) of 4.25 on the\nLJSpeech dataset. The toolkit is publicly available at\nhttps://github.com/espnet/espnet.", "published": "2019-10-24 04:24:27", "link": "http://arxiv.org/abs/1910.10909v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Diversifying Topic-Coherent Response Generation for Natural Multi-turn\n  Conversations", "abstract": "Although response generation (RG) diversification for single-turn dialogs has\nbeen well developed, it is less investigated for natural multi-turn\nconversations. Besides, past work focused on diversifying responses without\nconsidering topic coherence to the context, producing uninformative replies. In\nthis paper, we propose the Topic-coherent Hierarchical Recurrent\nEncoder-Decoder model (THRED) to diversify the generated responses without\ndeviating the contextual topics for multi-turn conversations. In overall, we\nbuild a sequence-to-sequence net (Seq2Seq) to model multi-turn conversations.\nAnd then we resort to the latent Variable Hierarchical Recurrent\nEncoder-Decoder model (VHRED) to learn global contextual distribution of\ndialogs. Besides, we construct a dense topic matrix which implies word-level\ncorrelations of the conversation corpora. The topic matrix is used to learn\nlocal topic distribution of the contextual utterances. By incorporating both\nthe global contextual distribution and the local topic distribution, THRED\nproduces both diversified and topic-coherent replies. In addition, we propose\nan explicit metric (\\emph{TopicDiv}) to measure the topic divergence between\nthe post and generated response, and we also propose an overall metric\ncombining the diversification metric (\\emph{Distinct}) and \\emph{TopicDiv}. We\nevaluate our model comparing with three baselines (Seq2Seq, HRED and VHRED) on\ntwo real-world corpora, respectively, and demonstrate its outstanding\nperformance in both diversification and topic coherence.", "published": "2019-10-24 14:18:55", "link": "http://arxiv.org/abs/1910.11161v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Clinical Concept Extraction: a Methodology Review", "abstract": "Background Concept extraction, a subdomain of natural language processing\n(NLP) with a focus on extracting concepts of interest, has been adopted to\ncomputationally extract clinical information from text for a wide range of\napplications ranging from clinical decision support to care quality\nimprovement.\n  Objectives In this literature review, we provide a methodology review of\nclinical concept extraction, aiming to catalog development processes, available\nmethods and tools, and specific considerations when developing clinical concept\nextraction applications.\n  Methods Based on the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) guidelines, a literature search was conducted for\nretrieving EHR-based information extraction articles written in English and\npublished from January 2009 through June 2019 from Ovid MEDLINE In-Process &\nOther Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science,\nand the ACM Digital Library.\n  Results A total of 6,686 publications were retrieved. After title and\nabstract screening, 228 publications were selected. The methods used for\ndeveloping clinical concept extraction applications were discussed in this\nreview.", "published": "2019-10-24 18:54:25", "link": "http://arxiv.org/abs/1910.11377v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "An Empirical Study of Efficient ASR Rescoring with Transformers", "abstract": "Neural language models (LMs) have been proved to significantly outperform\nclassical n-gram LMs for language modeling due to their superior abilities to\nmodel long-range dependencies in text and handle data sparsity problems. And\nrecently, well configured deep Transformers have exhibited superior performance\nover shallow stack of recurrent neural network layers for language modeling.\nHowever, these state-of-the-art deep Transformer models were mostly engineered\nto be deep with high model capacity, which makes it computationally inefficient\nand challenging to be deployed into large-scale real-world applications.\nTherefore, it is important to develop Transformer LMs that have relatively\nsmall model sizes, while still retaining good performance of those much larger\nmodels. In this paper, we aim to conduct empirical study on training\nTransformers with small parameter sizes in the context of ASR rescoring. By\ncombining techniques including subword units, adaptive softmax, large-scale\nmodel pre-training, and knowledge distillation, we show that we are able to\nsuccessfully train small Transformer LMs with significant relative word error\nrate reductions (WERR) through n-best rescoring. In particular, our experiments\non a video speech recognition dataset show that we are able to achieve WERRs\nranging from 6.46% to 7.17% while only with 5.5% to 11.9% parameter sizes of\nthe well-known large GPT model [1], whose WERR with rescoring on the same\ndataset is 7.58%.", "published": "2019-10-24 23:00:12", "link": "http://arxiv.org/abs/1910.11450v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Conversational Emotion Analysis via Attention Mechanisms", "abstract": "Different from the emotion recognition in individual utterances, we propose a\nmultimodal learning framework using relation and dependencies among the\nutterances for conversational emotion analysis. The attention mechanism is\napplied to the fusion of the acoustic and lexical features. Then these fusion\nrepresentations are fed into the self-attention based bi-directional gated\nrecurrent unit (GRU) layer to capture long-term contextual information. To\nimitate real interaction patterns of different speakers, speaker embeddings are\nalso utilized as additional inputs to distinguish the speaker identities during\nconversational dialogs. To verify the effectiveness of the proposed method, we\nconduct experiments on the IEMOCAP database. Experimental results demonstrate\nthat our method shows absolute 2.42% performance improvement over the\nstate-of-the-art strategies.", "published": "2019-10-24 16:16:45", "link": "http://arxiv.org/abs/1910.11263v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Predicting In-game Actions from Interviews of NBA Players", "abstract": "Sports competitions are widely researched in computer and social science,\nwith the goal of understanding how players act under uncertainty. While there\nis an abundance of computational work on player metrics prediction based on\npast performance, very few attempts to incorporate out-of-game signals have\nbeen made. Specifically, it was previously unclear whether linguistic signals\ngathered from players' interviews can add information which does not appear in\nperformance metrics. To bridge that gap, we define text classification tasks of\npredicting deviations from mean in NBA players' in-game actions, which are\nassociated with strategic choices, player behavior and risk, using their choice\nof language prior to the game. We collected a dataset of transcripts from key\nNBA players' pre-game interviews and their in-game performance metrics,\ntotalling in 5,226 interview-metric pairs. We design neural models for players'\naction prediction based on increasingly more complex aspects of the language\nsignals in their open-ended interviews. Our models can make their predictions\nbased on the textual signal alone, or on a combination with signals from\npast-performance metrics. Our text-based models outperform strong baselines\ntrained on performance metrics only, demonstrating the importance of language\nusage for action prediction. Moreover, the models that employ both textual\ninput and past-performance metrics produced the best results. Finally, as\nneural networks are notoriously difficult to interpret, we propose a method for\ngaining further insight into what our models have learned. Particularly, we\npresent an LDA-based analysis, where we interpret model predictions in terms of\ncorrelated topics. We find that our best performing textual model is most\nassociated with topics that are intuitively related to each prediction task and\nthat better models yield higher correlation with more informative topics.", "published": "2019-10-24 17:10:34", "link": "http://arxiv.org/abs/1910.11292v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Vision-Language Navigation", "abstract": "Commanding a robot to navigate with natural language instructions is a\nlong-term goal for grounded language understanding and robotics. But the\ndominant language is English, according to previous studies on vision-language\nnavigation (VLN). To go beyond English and serve people speaking different\nlanguages, we collect a bilingual Room-to-Room (BL-R2R) dataset, extending the\noriginal benchmark with new Chinese instructions. Based on this newly\nintroduced dataset, we study how an agent can be trained on existing English\ninstructions but navigate effectively with another language under a zero-shot\nlearning scenario. Without any training data of the target language, our model\nshows competitive results even compared to a model with full access to the\ntarget language training data. Moreover, we investigate the transferring\nability of our model when given a certain amount of target language training\ndata.", "published": "2019-10-24 17:32:38", "link": "http://arxiv.org/abs/1910.11301v3", "categories": ["cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Extending Event Detection to New Types with Learning from Keywords", "abstract": "Traditional event detection classifies a word or a phrase in a given sentence\nfor a set of predefined event types. The limitation of such predefined set is\nthat it prevents the adaptation of the event detection models to new event\ntypes. We study a novel formulation of event detection that describes types via\nseveral keywords to match the contexts in documents. This facilitates the\noperation of the models to new types. We introduce a novel feature-based\nattention mechanism for convolutional neural networks for event detection in\nthe new formulation. Our extensive experiments demonstrate the benefits of the\nnew formulation for new type extension for event detection as well as the\nproposed attention mechanism for this problem.", "published": "2019-10-24 18:20:48", "link": "http://arxiv.org/abs/1910.11368v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Detecting gender differences in perception of emotion in crowdsourced\n  data", "abstract": "Do men and women perceive emotions differently? Popular convictions place\nwomen as more emotionally perceptive than men. Empirical findings, however,\nremain inconclusive. Most prior studies focus on visual modalities. In\naddition, almost all of the studies are limited to experiments within\ncontrolled environments. Generalizability and scalability of these studies has\nnot been sufficiently established. In this paper, we study the differences in\nperception of emotion between genders from speech data in the wild, annotated\nthrough crowdsourcing. While we limit ourselves to a single modality (i.e.\nspeech), our framework is applicable to studies of emotion perception from all\nsuch loosely annotated data in general. Our paper addresses multiple serious\nchallenges related to making statistically viable conclusions from crowdsourced\ndata. Overall, the contributions of this paper are two fold: a reliable novel\nframework for perceptual studies from crowdsourced data; and the demonstration\nof statistically significant differences in speech-based emotion perception\nbetween genders.", "published": "2019-10-24 19:13:21", "link": "http://arxiv.org/abs/1910.11386v4", "categories": ["cs.CL", "cs.DB", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Recognizing long-form speech using streaming end-to-end models", "abstract": "All-neural end-to-end (E2E) automatic speech recognition (ASR) systems that\nuse a single neural network to transduce audio to word sequences have been\nshown to achieve state-of-the-art results on several tasks. In this work, we\nexamine the ability of E2E models to generalize to unseen domains, where we\nfind that models trained on short utterances fail to generalize to long-form\nspeech. We propose two complementary solutions to address this: training on\ndiverse acoustic data, and LSTM state manipulation to simulate long-form audio\nwhen training using short utterances. On a synthesized long-form test set,\nadding data diversity improves word error rate (WER) by 90% relative, while\nsimulating long-form training improves it by 67% relative, though the\ncombination doesn't improve over data diversity alone. On a real long-form\ncall-center test set, adding data diversity improves WER by 40% relative.\nSimulating long-form training on top of data diversity improves performance by\nan additional 27% relative.", "published": "2019-10-24 23:18:07", "link": "http://arxiv.org/abs/1910.11455v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Diarization Robustness using Diversification, Randomization\n  and the DOVER Algorithm", "abstract": "Speaker diarization based on bottom-up clustering of speech segments by\nacoustic similarity is often highly sensitive to the choice of hyperparameters,\nsuch as the initial number of clusters and feature weighting. Optimizing these\nhyperparameters is difficult and often not robust across different data sets.\nWe recently proposed the DOVER algorithm for combining multiple diarization\nhypotheses by voting. Here we propose to mitigate the robustness problem in\ndiarization by using DOVER to average across different parameter choices. We\nalso investigate the combination of diverse outputs obtained by following\ndifferent merge choices pseudo-randomly in the course of clustering, thereby\nmitigating the greediness of best-first clustering. We show on two conference\nmeeting data sets drawn from NIST evaluations that the proposed methods indeed\nyield more robust, and in several cases overall improved, results.", "published": "2019-10-24 01:57:34", "link": "http://arxiv.org/abs/1910.11691v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Capacity, Bandwidth, and Compositionality in Emergent Language Learning", "abstract": "Many recent works have discussed the propensity, or lack thereof, for\nemergent languages to exhibit properties of natural languages. A favorite in\nthe literature is learning compositionality. We note that most of those works\nhave focused on communicative bandwidth as being of primary importance. While\nimportant, it is not the only contributing factor. In this paper, we\ninvestigate the learning biases that affect the efficacy and compositionality\nof emergent languages. Our foremost contribution is to explore how capacity of\na neural network impacts its ability to learn a compositional language. We\nadditionally introduce a set of evaluation metrics with which we analyze the\nlearned languages. Our hypothesis is that there should be a specific range of\nmodel capacity and channel bandwidth that induces compositional structure in\nthe resulting language and consequently encourages systematic generalization.\nWhile we empirically see evidence for the bottom of this range, we curiously do\nnot find evidence for the top part of the range and believe that this is an\nopen question for the community.", "published": "2019-10-24 21:06:38", "link": "http://arxiv.org/abs/1910.11424v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning deep representations by multilayer bootstrap networks for\n  speaker diarization", "abstract": "The performance of speaker diarization is strongly affected by its clustering\nalgorithm at the test stage. However, it is known that clustering algorithms\nare sensitive to random noises and small variations, particularly when the\nclustering algorithms themselves suffer some weaknesses, such as bad local\nminima and prior assumptions. To deal with the problem, a compact\nrepresentation of speech segments with small within-class variances and large\nbetween-class distances is usually needed. In this paper, we apply an\nunsupervised deep model, named multilayer bootstrap network (MBN), to further\nprocess the embedding vectors of speech segments for the above problem. MBN is\nan unsupervised deep model for nonlinear dimensionality reduction. Unlike\ntraditional neural network based deep model, it is a stack of $k$-centroids\nclustering ensembles, each of which is trained simply by random resampling of\ndata and one-nearest-neighbor optimization. We construct speaker diarization\nsystems by combining MBN with either the i-vector frontend or x-vector\nfrontend, and evaluated their effectiveness on a simulated NIST diarization\ndataset, the AMI meeting corpus, and NIST SRE 2000 CALLHOME database.\nExperimental results show that the proposed systems are better than or at least\ncomparable to the systems that do not use MBN.", "published": "2019-10-24 08:11:55", "link": "http://arxiv.org/abs/1910.10969v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Analyzing the impact of speaker localization errors on speech separation\n  for automatic speech recognition", "abstract": "We investigate the effect of speaker localization on the performance of\nspeech recognition systems in a multispeaker, multichannel environment. Given\nthe speaker location information, speech separation is performed in three\nstages. In the first stage, a simple delay-and-sum (DS) beamformer is used to\nenhance the signal impinging from the speaker location which is then used to\nestimate a time-frequency mask corresponding to the localized speaker using a\nneural network. This mask is used to compute the second order statistics and to\nderive an adaptive beamformer in the third stage. We generated a multichannel,\nmultispeaker, reverberated, noisy dataset inspired from the well studied\nWSJ0-2mix and study the performance of the proposed pipeline in terms of the\nword error rate (WER). An average WER of $29.4$% was achieved using the ground\ntruth localization information and $42.4$% using the localization information\nestimated via GCC-PHAT. The signal-to-interference ratio (SIR) between the\nspeakers has a higher impact on the ASR performance, to the extent of reducing\nthe WER by $59$% relative for a SIR increase of $15$ dB. By contrast,\nincreasing the spatial distance to $50^\\circ$ or more improves the WER by $23$%\nrelative only", "published": "2019-10-24 13:53:00", "link": "http://arxiv.org/abs/1910.11114v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SLOGD: Speaker LOcation Guided Deflation approach to speech separation", "abstract": "Speech separation is the process of separating multiple speakers from an\naudio recording. In this work we propose to separate the sources using a\nSpeaker LOcalization Guided Deflation (SLOGD) approach wherein we estimate the\nsources iteratively. In each iteration we first estimate the location of the\nspeaker and use it to estimate a mask corresponding to the localized speaker.\nThe estimated source is removed from the mixture before estimating the location\nand mask of the next source. Experiments are conducted on a reverberated, noisy\nmultichannel version of the well-studied WSJ-2MIX dataset using word error rate\n(WER) as a metric. The proposed method achieves a WER of $44.2$%, a $34$%\nrelative improvement over the system without separation and $17$% relative\nimprovement over Conv-TasNet.", "published": "2019-10-24 14:00:58", "link": "http://arxiv.org/abs/1910.11131v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Superposition as Data Augmentation using LSTM and HMM in Small Training\n  Sets", "abstract": "Considering audio and image data as having quantum nature (data are\nrepresented by density matrices), we achieved better results on training\narchitectures such as 3-layer stacked LSTM and HMM by mixing training samples\nusing superposition augmentation and compared with plain default training and\nmix-up augmentation. This augmentation technique originates from the mix-up\napproach but provides more solid theoretical reasoning based on quantum\nproperties. We achieved 3% improvement (from 68% to 71%) by using 38% lesser\nnumber of training samples in Russian audio-digits recognition task and 7,16%\nbetter accuracy than mix-up augmentation by training only 500 samples using HMM\non the same task. Also, we achieved 1.1% better accuracy than mix-up on first\n900 samples in MNIST using 3-layer stacked LSTM.", "published": "2019-10-24 02:14:09", "link": "http://arxiv.org/abs/1910.10881v1", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Syntonets: Toward A Harmony-Inspired General Model of Complex Networks", "abstract": "We report an approach to obtaining complex networks with diverse topology,\nhere called syntonets, taking into account the consonances and dissonances\nbetween notes as defined by scale temperaments. Though the fundamental\nfrequency is usually considered, in real-world sounds several additional\nfrequencies (partials) accompany the respective fundamental, influencing both\ntimber and consonance between simultaneous notes. We use a method based on\nHelmholtz's consonance approach to quantify the consonances and dissonances\nbetween each of the pairs of notes in a given temperament. We adopt two\ndistinct partials structures: (i) harmonic; and (ii) shifted, obtained by\ntaking the harmonic components to a given power $\\beta$, which is henceforth\ncalled the anharmonicity index. The latter type of sounds is more realistic in\nthe sense that they reflect non-linearities implied by real-world instruments.\nWhen these consonances/dissonances are estimated along several octaves,\nrespective syntonets can be obtained, in which nodes and weighted edge\nrepresent notes, and consonance/dissonance, respectively. The obtained results\nare organized into two main groups, those related to network science and\nmusical theory. Regarding the former group, we have that the syntonets can\nprovide, for varying values of $\\beta$, a wide range of topologies spanning the\nspace comprised between traditional models. Indeed, it is suggested here that\nsyntony may provide a kind of universal complex network model. The musical\ninterpretations of the results include the confirmation of the more regular\nconsonance pattern of the equal temperament, obtained at the expense of a wider\nrange of consonances such as that in the meantone temperament. We also have\nthat scales derived for shifted partials tend to have a wider range of\nconsonances/dissonances, depending on the temperament and anharmonicity\nstrength.", "published": "2019-10-24 12:21:59", "link": "http://arxiv.org/abs/1910.11047v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Fine-Grained Prosody Control for Voice Conversion", "abstract": "In a typical voice conversion system, prior works utilize various acoustic\nfeatures (e.g., the pitch, voiced/unvoiced flag, aperiodicity) of the source\nspeech to control the prosody of generated waveform. However, the prosody is\nrelated with many factors, such as the intonation, stress and rhythm. It is a\nchallenging task to perfectly describe the prosody through acoustic features.\nTo deal with this problem, we propose prosody embeddings to model prosody.\nThese embeddings are learned from the source speech in an unsupervised manner.\nWe conduct experiments on our Mandarin corpus recoded by professional speakers.\nExperimental results demonstrate that the proposed method enables fine-grained\ncontrol of the prosody. In challenging situations (such as the source speech is\na singing song), our proposed method can also achieve promising results.", "published": "2019-10-24 16:36:25", "link": "http://arxiv.org/abs/1910.11269v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker diarization using latent space clustering in generative\n  adversarial network", "abstract": "In this work, we propose deep latent space clustering for speaker diarization\nusing generative adversarial network (GAN) backprojection with the help of an\nencoder network. The proposed diarization system is trained jointly with GAN\nloss, latent variable recovery loss, and a clustering-specific loss. It uses\nx-vector speaker embeddings at the input, while the latent variables are\nsampled from a combination of continuous random variables and discrete one-hot\nencoded variables using the original speaker labels. We benchmark our proposed\nsystem on the AMI meeting corpus, and two child-clinician interaction corpora\n(ADOS and BOSCC) from the autism diagnosis domain. ADOS and BOSCC contain\ndiagnostic and treatment outcome sessions respectively obtained in clinical\nsettings for verbal children and adolescents with autism. Experimental results\nshow that our proposed system significantly outperform the state-of-the-art\nx-vector based diarization system on these databases. Further, we perform\nembedding fusion with x-vectors to achieve a relative DER improvement of 31%,\n36% and 49% on AMI eval, ADOS and BOSCC corpora respectively, when compared to\nthe x-vector baseline using oracle speech segmentation.", "published": "2019-10-24 19:52:45", "link": "http://arxiv.org/abs/1910.11398v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Meta-learning for robust child-adult classification from speech", "abstract": "Computational modeling of naturalistic conversations in clinical applications\nhas seen growing interest in the past decade. An important use-case involves\nchild-adult interactions within the autism diagnosis and intervention domain.\nIn this paper, we address a specific sub-problem of speaker diarization, namely\nchild-adult speaker classification in such dyadic conversations with specified\nroles. Training a speaker classification system robust to speaker and channel\nconditions is challenging due to inherent variability in the speech within\nchildren and the adult interlocutors. In this work, we propose the use of\nmeta-learning, in particular, prototypical networks which optimize a metric\nspace across multiple tasks. By modeling every child-adult pair in the training\nset as a separate task during meta-training, we learn a representation with\nimproved generalizability compared to conventional supervised learning. We\ndemonstrate improvements over state-of-the-art speaker embeddings (x-vectors)\nunder two evaluation settings: weakly supervised classification (up to 14.53%\nrelative improvement in F1-scores) and clustering (up to relative 9.66%\nimprovement in cluster purity). Our results show that protonets can potentially\nextract robust speaker embeddings for child-adult classification from speech.", "published": "2019-10-24 19:55:24", "link": "http://arxiv.org/abs/1910.11400v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A study of semi-supervised speaker diarization system using gan mixture\n  model", "abstract": "We propose a new speaker diarization system based on a recently introduced\nunsupervised clustering technique namely, generative adversarial network\nmixture model (GANMM). The proposed system uses x-vectors as front-end\nrepresentation. Spectral embedding is used for dimensionality reduction\nfollowed by k-means initialization during GANMM pre-training. GANMM performs\nunsupervised speaker clustering by efficiently capturing complex data\ndistributions. Experimental results on the AMI meeting corpus show that the\nproposed semi-supervised diarization system matches or exceeds the performance\nof competitive baselines. On an evaluation set containing fifty sessions with\nvarying durations, the best achieved average diarization error rate (DER) is\n17.11%, a relative improvement of 33% over the information bottleneck baseline\nand comparable to xvector baseline.", "published": "2019-10-24 20:34:35", "link": "http://arxiv.org/abs/1910.11416v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-channel Speech Separation Using Deep Embedding Model with\n  Multilayer Bootstrap Networks", "abstract": "Recently, deep clustering (DPCL) based speaker-independent speech separation\nhas drawn much attention, since it needs little speaker prior information.\nHowever, it still has much room of improvement, particularly in reverberant\nenvironments. If the training and test environments mismatch which is a common\ncase, the embedding vectors produced by DPCL may contain much noise and many\nsmall variations. To deal with the problem, we propose a variant of DPCL, named\nDPCL++, by applying a recent unsupervised deep learning method---multilayer\nbootstrap networks(MBN)---to further reduce the noise and small variations of\nthe embedding vectors in an unsupervised way in the test stage, which\nfascinates k-means to produce a good result. MBN builds a gradually narrowed\nnetwork from bottom-up via a stack of k-centroids clustering ensembles, where\nthe k-centroids clusterings are trained independently by random sampling and\none-nearest-neighbor optimization. To further improve the robustness of DPCL++\nin reverberant environments, we take spatial features as part of its input.\nExperimental results demonstrate the effectiveness of the proposed method.", "published": "2019-10-24 04:35:11", "link": "http://arxiv.org/abs/1910.10912v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Delving into VoxCeleb: environment invariant speaker recognition", "abstract": "Research in speaker recognition has recently seen significant progress due to\nthe application of neural network models and the availability of new\nlarge-scale datasets. There has been a plethora of work in search for more\npowerful architectures or loss functions suitable for the task, but these works\ndo not consider what information is learnt by the models, apart from being able\nto predict the given labels.\n  In this work, we introduce an environment adversarial training framework in\nwhich the network can effectively learn speaker-discriminative and\nenvironment-invariant embeddings without explicit domain shift during training.\nWe achieve this by utilising the previously unused `video' information in the\nVoxCeleb dataset. The environment adversarial training allows the network to\ngeneralise better to unseen conditions. The method is evaluated on both speaker\nidentification and verification tasks using the VoxCeleb dataset, on which we\ndemonstrate significant performance improvements over baselines.", "published": "2019-10-24 15:41:35", "link": "http://arxiv.org/abs/1910.11238v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fast and High-Quality Singing Voice Synthesis System based on\n  Convolutional Neural Networks", "abstract": "The present paper describes singing voice synthesis based on convolutional\nneural networks (CNNs). Singing voice synthesis systems based on deep neural\nnetworks (DNNs) are currently being proposed and are improving the naturalness\nof synthesized singing voices. As singing voices represent a rich form of\nexpression, a powerful technique to model them accurately is required. In the\nproposed technique, long-term dependencies of singing voices are modeled by\nCNNs. An acoustic feature sequence is generated for each segment that consists\nof long-term frames, and a natural trajectory is obtained without the parameter\ngeneration algorithm. Furthermore, a computational complexity reduction\ntechnique, which drives the DNNs in different time units depending on type of\nmusical score features, is proposed. Experimental results show that the\nproposed method can synthesize natural sounding singing voices much faster than\nthe conventional method.", "published": "2019-10-24 04:25:47", "link": "http://arxiv.org/abs/1910.11690v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Representation Learning with Future Observation Prediction\n  for Speech Emotion Recognition", "abstract": "Prior works on speech emotion recognition utilize various unsupervised\nlearning approaches to deal with low-resource samples. However, these methods\npay less attention to modeling the long-term dynamic dependency, which is\nimportant for speech emotion recognition. To deal with this problem, this paper\ncombines the unsupervised representation learning strategy -- Future\nObservation Prediction (FOP), with transfer learning approaches (such as\nFine-tuning and Hypercolumns). To verify the effectiveness of the proposed\nmethod, we conduct experiments on the IEMOCAP database. Experimental results\ndemonstrate that our method is superior to currently advanced unsupervised\nlearning strategies.", "published": "2019-10-24 16:29:16", "link": "http://arxiv.org/abs/1910.13806v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain adversarial learning for emotion recognition", "abstract": "In practical applications for emotion recognition, users do not always exist\nin the training corpus. The mismatch between training speakers and testing\nspeakers affects the performance of the trained model. To deal with this\nproblem, we need our model to focus on emotion-related information, while\nignoring the difference between speaker identities. In this paper, we look into\nthe use of the domain adversarial neural network (DANN) to extract a common\nrepresentation between different speakers. The primary task is to predict\nemotion labels. The secondary task is to learn a common representation where\nspeaker identities can not be distinguished. By using the gradient reversal\nlayer, the gradients coming from the secondary task are used to bring the\nrepresentations for different speakers closer. To verify the effectiveness of\nthe proposed method, we conduct experiments on the IEMOCAP database.\nExperimental results demonstrate that the proposed framework shows an absolute\nimprovement of 3.48% over state-of-the-art strategies.", "published": "2019-10-24 16:33:48", "link": "http://arxiv.org/abs/1910.13807v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AI in Pursuit of Happiness, Finding Only Sadness: Multi-Modal Facial\n  Emotion Recognition Challenge", "abstract": "The importance of automated Facial Emotion Recognition (FER) grows the more\ncommon human-machine interactions become, which will only continue to increase\ndramatically with time. A common method to describe human sentiment or feeling\nis the categorical model the `7 basic emotions', consisting of `Angry',\n`Disgust', `Fear', `Happiness', `Sadness', `Surprise' and `Neutral'. The\n`Emotion Recognition in the Wild' (EmotiW) competition is now in its 7th year\nand has become the standard benchmark for measuring FER performance. The focus\nof this paper is the EmotiW sub-challenge of classifying videos in the `Acted\nFacial Expression in the Wild' (AFEW) dataset, consisting of both visual and\naudio modalities, into one of the above classes. Machine learning has exploded\nas a research topic in recent years, with advancements in `Deep Learning' a key\npart of this. Although Deep Learning techniques have been widely applied to the\nFER task by entrants in previous years, this paper has two main contributions:\n(i) to apply the latest `state-of-the-art' visual and temporal networks and\n(ii) exploring various methods of fusing features extracted from the visual and\naudio elements to enrich the information available to the final model making\nthe prediction. There are a number of complex issues that arise when trying to\nclassify emotions for `in-the-wild' video sequences, which the above two\napproaches attempt to directly address. There are some positive findings when\ncomparing the results of this paper to past submissions, indicating that\nfurther research into the proposed methods and fine-tuning of the models\ndeployed, could result in another step forwards in the field of automated FER.", "published": "2019-10-24 14:49:39", "link": "http://arxiv.org/abs/1911.05187v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CV"}
{"title": "A Recurrent Variational Autoencoder for Speech Enhancement", "abstract": "This paper presents a generative approach to speech enhancement based on a\nrecurrent variational autoencoder (RVAE). The deep generative speech model is\ntrained using clean speech signals only, and it is combined with a nonnegative\nmatrix factorization noise model for speech enhancement. We propose a\nvariational expectation-maximization algorithm where the encoder of the RVAE is\nfine-tuned at test time, to approximate the distribution of the latent\nvariables given the noisy speech observations. Compared with previous\napproaches based on feed-forward fully-connected architectures, the proposed\nrecurrent deep generative speech model induces a posterior temporal dynamic\nover the latent variables, which is shown to improve the speech enhancement\nresults.", "published": "2019-10-24 06:54:36", "link": "http://arxiv.org/abs/1910.10942v2", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
