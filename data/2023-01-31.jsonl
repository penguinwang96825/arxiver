{"title": "Differentiable Entailment for Parameter Efficient Few Shot Learning", "abstract": "Few-shot learning allows pre-trained language models to adapt to downstream\ntasks while using a limited number of training examples. However, practical\napplications are limited when all model parameters must be optimized. In this\nwork we apply a new technique for parameter efficient few shot learning while\nadopting a strict definition of parameter efficiency. Our training method\ncombines 1) intermediate training by reformulating natural language tasks as\nentailment tasks \\cite{wang_entailment_2021} and 2) differentiable optimization\nof template and label tokens \\cite{zhang_differentiable_2021}. We quantify the\ntradeoff between parameter efficiency and performance in the few-shot regime\nand propose a simple model agnostic approach that can be extended to any task\nBy achieving competitive performance while only optimizing 3\\% of a model's\nparameters and allowing for batched inference, we allow for more efficient\npractical deployment of models.", "published": "2023-01-31 00:31:11", "link": "http://arxiv.org/abs/2301.13345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithful Chain-of-Thought Reasoning", "abstract": "While Chain-of-Thought (CoT) prompting boosts Language Models' (LM)\nperformance on a gamut of complex reasoning tasks, the generated reasoning\nchain does not necessarily reflect how the model arrives at the answer (aka.\nfaithfulness). We propose Faithful CoT, a reasoning framework involving two\nstages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning\nchain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM\nand a deterministic solver respectively. This guarantees that the reasoning\nchain provides a faithful explanation of the final answer. Aside from\ninterpretability, Faithful CoT also improves empirical performance: it\noutperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a\nrelative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning,\n5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference.\nFurthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot\nperformance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong\nsynergy between faithfulness and accuracy.", "published": "2023-01-31 03:04:26", "link": "http://arxiv.org/abs/2301.13379v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Numeracy from Literacy: Data Science as an Emergent Skill from Large\n  Language Models", "abstract": "Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique\ntestbeds for exploring the translation challenges of turning literacy into\nnumeracy. Previous publicly-available transformer models from eighteen months\nprior and 1000 times smaller failed to provide basic arithmetic. The\nstatistical analysis of four complex datasets described here combines\narithmetic manipulations that cannot be memorized or encoded by simple rules.\nThe work examines whether next-token prediction succeeds from sentence\ncompletion into the realm of actual numerical understanding. For example, the\nwork highlights cases for descriptive statistics on in-memory datasets that the\nLLM initially loads from memory or generates randomly using python libraries.\nThe resulting exploratory data analysis showcases the model's capabilities to\ngroup by or pivot categorical sums, infer feature importance, derive\ncorrelations, and predict unseen test cases using linear regression. To extend\nthe model's testable range, the research deletes and appends random rows such\nthat recall alone cannot explain emergent numeracy.", "published": "2023-01-31 03:14:57", "link": "http://arxiv.org/abs/2301.13382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZhichunRoad at Amazon KDD Cup 2022: MultiTask Pre-Training for\n  E-Commerce Product Search", "abstract": "In this paper, we propose a robust multilingual model to improve the quality\nof search results. Our model not only leverage the processed class-balanced\ndataset, but also benefit from multitask pre-training that leads to more\ngeneral representations. In pre-training stage, we adopt mlm task,\nclassification task and contrastive learning task to achieve considerably\nperformance. In fine-tuning stage, we use confident learning, exponential\nmoving average method (EMA), adversarial training (FGM) and regularized dropout\nstrategy (R-Drop) to improve the model's generalization and robustness.\nMoreover, we use a multi-granular semantic unit to discover the queries and\nproducts textual metadata for enhancing the representation of the model. Our\napproach obtained competitive results and ranked top-8 in three tasks. We\nrelease the source code and pre-trained models associated with this work.", "published": "2023-01-31 07:31:34", "link": "http://arxiv.org/abs/2301.13455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Archive TimeLine Summarization (ATLS): Conceptual Framework for Timeline\n  Generation over Historical Document Collections", "abstract": "Archive collections are nowadays mostly available through search engines\ninterfaces, which allow a user to retrieve documents by issuing queries. The\nstudy of these collections may be, however, impaired by some aspects of search\nengines, such as the overwhelming number of documents returned or the lack of\ncontextual knowledge provided. New methods that could work independently or in\ncombination with search engines are then required to access these collections.\nIn this position paper, we propose to extend TimeLine Summarization (TLS)\nmethods on archive collections to assist in their studies. We provide an\noverview of existing TLS methods and we describe a conceptual framework for an\nArchive TimeLine Summarization (ATLS) system, which aims to generate\ninformative, readable and interpretable timelines.", "published": "2023-01-31 08:58:47", "link": "http://arxiv.org/abs/2301.13479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Friend-training: Learning from Models of Different but Related Tasks", "abstract": "Current self-training methods such as standard self-training, co-training,\ntri-training, and others often focus on improving model performance on a single\ntask, utilizing differences in input features, model architectures, and\ntraining processes. However, many tasks in natural language processing are\nabout different but related aspects of language, and models trained for one\ntask can be great teachers for other related tasks. In this work, we propose\nfriend-training, a cross-task self-training framework, where models trained to\ndo different tasks are used in an iterative training, pseudo-labeling, and\nretraining process to help each other for better selection of pseudo-labels.\nWith two dialogue understanding tasks, conversational semantic role labeling\nand dialogue rewriting, chosen for a case study, we show that the models\ntrained with the friend-training framework achieve the best performance\ncompared to strong baselines.", "published": "2023-01-31 15:00:56", "link": "http://arxiv.org/abs/2301.13683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recursive Neural Networks with Bottlenecks Diagnose\n  (Non-)Compositionality", "abstract": "A recent line of work in NLP focuses on the (dis)ability of models to\ngeneralise compositionally for artificial languages. However, when considering\nnatural language tasks, the data involved is not strictly, or locally,\ncompositional. Quantifying the compositionality of data is a challenging task,\nwhich has been investigated primarily for short utterances. We use recursive\nneural models (Tree-LSTMs) with bottlenecks that limit the transfer of\ninformation between nodes. We illustrate that comparing data's representations\nin models with and without the bottleneck can be used to produce a\ncompositionality metric. The procedure is applied to the evaluation of\narithmetic expressions using synthetic data, and sentiment classification using\nnatural language data. We demonstrate that compression through a bottleneck\nimpacts non-compositional examples disproportionately and then use the\nbottleneck compositionality metric (BCM) to distinguish compositional from\nnon-compositional samples, yielding a compositionality ranking over a dataset.", "published": "2023-01-31 15:46:39", "link": "http://arxiv.org/abs/2301.13714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot cross-lingual transfer language selection using linguistic\n  similarity", "abstract": "We study the selection of transfer languages for different Natural Language\nProcessing tasks, specifically sentiment analysis, named entity recognition and\ndependency parsing. In order to select an optimal transfer language, we propose\nto utilize different linguistic similarity metrics to measure the distance\nbetween languages and make the choice of transfer language based on this\ninformation instead of relying on intuition. We demonstrate that linguistic\nsimilarity correlates with cross-lingual transfer performance for all of the\nproposed tasks. We also show that there is a statistically significant\ndifference in choosing the optimal language as the transfer source instead of\nEnglish. This allows us to select a more suitable transfer language which can\nbe used to better leverage knowledge from high-resource languages in order to\nimprove the performance of language applications lacking data. For the study,\nwe used datasets from eight different languages from three language families.", "published": "2023-01-31 15:56:40", "link": "http://arxiv.org/abs/2301.13720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Scheduled Sampling with Imitation Loss for Neural Text\n  Generation", "abstract": "State-of-the-art neural text generation models are typically trained to\nmaximize the likelihood of each token in the ground-truth sequence conditioned\non the previous target tokens. However, during inference, the model needs to\nmake a prediction conditioned on the tokens generated by itself. This\ntrain-test discrepancy is referred to as exposure bias. Scheduled sampling is a\ncurriculum learning strategy that gradually exposes the model to its own\npredictions during training to mitigate this bias. Most of the proposed\napproaches design a scheduler based on training steps, which generally requires\ncareful tuning depending on the training setup. In this work, we introduce\nDynamic Scheduled Sampling with Imitation Loss (DySI), which maintains the\nschedule based solely on the training time accuracy, while enhancing the\ncurriculum learning by introducing an imitation loss, which attempts to make\nthe behavior of the decoder indistinguishable from the behavior of a\nteacher-forced decoder. DySI is universally applicable across training setups\nwith minimal tuning. Extensive experiments and analysis show that DySI not only\nachieves notable improvements on standard machine translation benchmarks, but\nalso significantly improves the robustness of other text generation models.", "published": "2023-01-31 16:41:06", "link": "http://arxiv.org/abs/2301.13753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Touch\u00e923-ValueEval Dataset for Identifying Human Values behind\n  Arguments", "abstract": "We present the Touch\\'e23-ValueEval Dataset for Identifying Human Values\nbehind Arguments. To investigate approaches for the automated detection of\nhuman values behind arguments, we collected 9324 arguments from 6 diverse\nsources, covering religious texts, political discussions, free-text arguments,\nnewspaper editorials, and online democracy platforms. Each argument was\nannotated by 3 crowdworkers for 54 values. The Touch\\'e23-ValueEval dataset\nextends the Webis-ArgValues-22. In comparison to the previous dataset, the\neffectiveness of a 1-Baseline decreases, but that of an out-of-the-box BERT\nmodel increases. Therefore, though the classification difficulty increased as\nper the label distribution, the larger dataset allows for training better\nmodels.", "published": "2023-01-31 17:15:33", "link": "http://arxiv.org/abs/2301.13771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Versatile Decomposers: Decompose Evidence and\n  Questions for Table-based Reasoning", "abstract": "Table-based reasoning has shown remarkable progress in combining deep models\nwith discrete reasoning, which requires reasoning over both free-form natural\nlanguage (NL) questions and structured tabular data. However, previous\ntable-based reasoning solutions usually suffer from significant performance\ndegradation on huge evidence (tables). In addition, most existing methods\nstruggle to reason over complex questions since the required information is\nscattered in different places. To alleviate the above challenges, we exploit\nlarge language models (LLMs) as decomposers for effective table-based\nreasoning, which (i) decompose huge evidence (a huge table) into sub-evidence\n(a small table) to mitigate the interference of useless information for table\nreasoning; and (ii) decompose complex questions into simpler sub-questions for\ntext reasoning. Specifically, we first use the LLMs to break down the evidence\n(tables) involved in the current question, retaining the relevant evidence and\nexcluding the remaining irrelevant evidence from the huge table. In addition,\nwe propose a \"parsing-execution-filling\" strategy to alleviate the\nhallucination dilemma of the chain of thought by decoupling logic and numerical\ncomputation in each step. Extensive experiments show that our method can\neffectively leverage decomposed evidence and questions and outperforms the\nstrong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,\nour model outperforms human performance for the first time on the TabFact\ndataset.", "published": "2023-01-31 17:51:45", "link": "http://arxiv.org/abs/2301.13808v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Multi-Document Summarization Models Synthesize?", "abstract": "Multi-document summarization entails producing concise synopses of\ncollections of inputs. For some applications, the synopsis should accurately\nsynthesize inputs with respect to a key aspect, e.g., a synopsis of film\nreviews written about a particular movie should reflect the average critic\nconsensus. As a more consequential example, narrative summaries that accompany\nbiomedical systematic reviews of clinical trial results should accurately\nsummarize the potentially conflicting results from individual trials. In this\npaper we ask: To what extent do modern multi-document summarization models\nimplicitly perform this sort of synthesis? We run experiments over opinion and\nevidence synthesis datasets using a suite of summarization models, from\nfine-tuned transformers to GPT-4. We find that existing models partially\nperform synthesis, but imperfectly: even the best performing models are\nover-sensitive to changes in input ordering and under-sensitive to changes in\ninput compositions (e.g., ratio of positive to negative reviews). We propose a\nsimple, general, effective method for improving model synthesis capabilities by\ngenerating an explicitly diverse set of candidate outputs, and then selecting\nfrom these the string best aligned with the expected aggregate measure for the\ninputs, or abstaining when the model produces no good candidate.", "published": "2023-01-31 18:40:46", "link": "http://arxiv.org/abs/2301.13844v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Identification with BOS and EOS Label Combinations", "abstract": "The sentence is a fundamental unit in many NLP applications. Sentence\nsegmentation is widely used as the first preprocessing task, where an input\ntext is split into consecutive sentences considering the end of the sentence\n(EOS) as their boundaries. This task formulation relies on a strong assumption\nthat the input text consists only of sentences, or what we call the sentential\nunits (SUs). However, real-world texts often contain non-sentential units\n(NSUs) such as metadata, sentence fragments, nonlinguistic markers, etc. which\nare unreasonable or undesirable to be treated as a part of an SU. To tackle\nthis issue, we formulate a novel task of sentence identification, where the\ngoal is to identify SUs while excluding NSUs in a given text. To conduct\nsentence identification, we propose a simple yet effective method which\ncombines the beginning of the sentence (BOS) and EOS labels to determine the\nmost probable SUs and NSUs based on dynamic programming. To evaluate this task,\nwe design an automatic, language-independent procedure to convert the Universal\nDependencies corpora into sentence identification benchmarks. Finally, our\nexperiments on the sentence identification task demonstrate that our proposed\nmethod generally outperforms sentence segmentation baselines which only utilize\nEOS labels.", "published": "2023-01-31 01:03:07", "link": "http://arxiv.org/abs/2301.13352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Open-Domain Dialogue Evaluation with a Causal Inference Model", "abstract": "Effective evaluation methods remain a significant challenge for research on\nopen-domain conversational dialogue systems. Explicit satisfaction ratings can\nbe elicited from users, but users often do not provide ratings when asked, and\nthose they give can be highly subjective. Post-hoc ratings by experts are an\nalternative, but these can be both expensive and complex to collect. Here, we\nexplore the creation of automated methods for predicting both expert and user\nratings of open-domain dialogues. We compare four different approaches. First,\nwe train a baseline model using an end-to-end transformer to predict ratings\ndirectly from the raw dialogue text. The other three methods are variants of a\ntwo-stage approach in which we first extract interpretable features at the turn\nlevel that capture, among other aspects, user dialogue behaviors indicating\ncontradiction, repetition, disinterest, compliments, or criticism. We project\nthese features to the dialogue level and train a dialogue-level MLP regression\nmodel, a dialogue-level LSTM, and a novel causal inference model called\ncounterfactual-LSTM (CF-LSTM) to predict ratings. The proposed CF-LSTM is a\nsequential model over turn-level features which predicts ratings using multiple\nregressors depending on hypotheses derived from the turn-level features. As a\ncausal inference model, CF-LSTM aims to learn the underlying causes of a\nspecific event, such as a low rating. We also bin the user ratings and perform\nclassification experiments with all four models. In evaluation experiments on\nconversational data from the Alexa Prize SocialBot, we show that the CF-LSTM\nachieves the best performance for predicting dialogue ratings and\nclassification.", "published": "2023-01-31 02:31:42", "link": "http://arxiv.org/abs/2301.13372v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TopoBERT: Plug and Play Toponym Recognition Module Harnessing Fine-tuned\n  BERT", "abstract": "Extracting precise geographical information from textual contents is crucial\nin a plethora of applications. For example, during hazardous events, a robust\nand unbiased toponym extraction framework can provide an avenue to tie the\nlocation concerned to the topic discussed by news media posts and pinpoint\nhumanitarian help requests or damage reports from social media. Early studies\nhave leveraged rule-based, gazetteer-based, deep learning, and hybrid\napproaches to address this problem. However, the performance of existing tools\nis deficient in supporting operations like emergency rescue, which relies on\nfine-grained, accurate geographic information. The emerging pretrained language\nmodels can better capture the underlying characteristics of text information,\nincluding place names, offering a promising pathway to optimize toponym\nrecognition to underpin practical applications. In this paper, TopoBERT, a\ntoponym recognition module based on a one dimensional Convolutional Neural\nNetwork (CNN1D) and Bidirectional Encoder Representation from Transformers\n(BERT), is proposed and fine-tuned. Three datasets (CoNLL2003-Train,\nWikipedia3000, WNUT2017) are leveraged to tune the hyperparameters, discover\nthe best training strategy, and train the model. Another two datasets\n(CoNLL2003-Test and Harvey2017) are used to evaluate the performance. Three\ndistinguished classifiers, linear, multi-layer perceptron, and CNN1D, are\nbenchmarked to determine the optimal model architecture. TopoBERT achieves\nstate-of-the-art performance (f1-score=0.865) compared to the other five\nbaseline models and can be applied to diverse toponym recognition tasks without\nadditional training.", "published": "2023-01-31 13:44:34", "link": "http://arxiv.org/abs/2301.13631v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Sentiment and Hate Speech Analysis of Facebook Data by\n  Employing Multilingual Transformer Models", "abstract": "In recent years, there has been a heightened consensus within academia and in\nthe public discourse that Social Media Platforms (SMPs), amplify the spread of\nhateful and negative sentiment content. Researchers have identified how hateful\ncontent, political propaganda, and targeted messaging contributed to real-world\nharms including insurrections against democratically elected governments,\ngenocide, and breakdown of social cohesion due to heightened negative discourse\ntowards certain communities in parts of the world. To counter these issues,\nSMPs have created semi-automated systems that can help identify toxic speech.\nIn this paper we analyse the statistical distribution of hateful and negative\nsentiment contents within a representative Facebook dataset (n= 604,703)\nscrapped through 648 public Facebook pages which identify themselves as\nproponents (and followers) of far-right Hindutva actors. These pages were\nidentified manually using keyword searches on Facebook and on CrowdTangleand\nclassified as far-right Hindutva pages based on page names, page descriptions,\nand discourses shared on these pages. We employ state-of-the-art, open-source\nXLM-T multilingual transformer-based language models to perform sentiment and\nhate speech analysis of the textual contents shared on these pages over a\nperiod of 5.5 years. The result shows the statistical distributions of the\npredicted sentiment and the hate speech labels; top actors, and top page\ncategories. We further discuss the benchmark performances and limitations of\nthese pre-trained language models.", "published": "2023-01-31 14:37:04", "link": "http://arxiv.org/abs/2301.13668v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Power of External Memory in Increasing Predictive Model Capacity", "abstract": "One way of introducing sparsity into deep networks is by attaching an\nexternal table of parameters that is sparsely looked up at different layers of\nthe network. By storing the bulk of the parameters in the external table, one\ncan increase the capacity of the model without necessarily increasing the\ninference time. Two crucial questions in this setting are then: what is the\nlookup function for accessing the table and how are the contents of the table\nconsumed? Prominent methods for accessing the table include 1) using\nwords/wordpieces token-ids as table indices, 2) LSH hashing the token vector in\neach layer into a table of buckets, and 3) learnable softmax style routing to a\ntable entry. The ways to consume the contents include adding/concatenating to\ninput representation, and using the contents as expert networks that specialize\nto different inputs. In this work, we conduct rigorous experimental evaluations\nof existing ideas and their combinations. We also introduce a new method,\nalternating updates, that enables access to an increased token dimension\nwithout increasing the computation time, and demonstrate its effectiveness in\nlanguage modeling.", "published": "2023-01-31 00:29:39", "link": "http://arxiv.org/abs/2302.00003v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "In-Context Retrieval-Augmented Language Models", "abstract": "Retrieval-Augmented Language Modeling (RALM) methods, which condition a\nlanguage model (LM) on relevant documents from a grounding corpus during\ngeneration, were shown to significantly improve language modeling performance.\nIn addition, they can mitigate the problem of factually inaccurate text\ngeneration and provide natural source attribution mechanism. Existing RALM\napproaches focus on modifying the LM architecture in order to facilitate the\nincorporation of external information, significantly complicating deployment.\nThis paper considers a simple alternative, which we dub In-Context RALM:\nleaving the LM architecture unchanged and prepending grounding documents to the\ninput, without any further training of the LM. We show that In-Context RALM\nthat builds on off-the-shelf general purpose retrievers provides surprisingly\nlarge LM gains across model sizes and diverse corpora. We also demonstrate that\nthe document retrieval and ranking mechanism can be specialized to the RALM\nsetting to further boost performance. We conclude that In-Context RALM has\nconsiderable potential to increase the prevalence of LM grounding, particularly\nin settings where a pretrained LM must be used without modification or even via\nAPI access.", "published": "2023-01-31 20:26:16", "link": "http://arxiv.org/abs/2302.00083v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "abstract": "Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.", "published": "2023-01-31 20:48:57", "link": "http://arxiv.org/abs/2302.00093v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Detecting Harmful Agendas in News Articles", "abstract": "Manipulated news online is a growing problem which necessitates the use of\nautomated systems to curtail its spread. We argue that while misinformation and\ndisinformation detection have been studied, there has been a lack of investment\nin the important open challenge of detecting harmful agendas in news articles;\nidentifying harmful agendas is critical to flag news campaigns with the\ngreatest potential for real world harm. Moreover, due to real concerns around\ncensorship, harmful agenda detectors must be interpretable to be effective. In\nthis work, we propose this new task and release a dataset, NewsAgendas, of\nannotated news articles for agenda identification. We show how interpretable\nsystems can be effective on this task and demonstrate that they can perform\ncomparably to black-box models.", "published": "2023-01-31 21:08:58", "link": "http://arxiv.org/abs/2302.00102v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine Translation Impact in E-commerce Multilingual Search", "abstract": "Previous work suggests that performance of cross-lingual information\nretrieval correlates highly with the quality of Machine Translation. However,\nthere may be a threshold beyond which improving query translation quality\nyields little or no benefit to further improve the retrieval performance. This\nthreshold may depend upon multiple factors including the source and target\nlanguages, the existing MT system quality and the search pipeline. In order to\nidentify the benefit of improving an MT system for a given search pipeline, we\ninvestigate the sensitivity of retrieval quality to the presence of different\nlevels of MT quality using experimental datasets collected from actual traffic.\nWe systematically improve the performance of our MT systems quality on language\npairs as measured by MT evaluation metrics including Bleu and Chrf to determine\ntheir impact on search precision metrics and extract signals that help to guide\nthe improvement strategies. Using this information we develop techniques to\ncompare query translations for multiple language pairs and identify the most\npromising language pairs to invest and improve.", "published": "2023-01-31 21:59:35", "link": "http://arxiv.org/abs/2302.00119v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Universal Topological Regularities of Syntactic Structures: Decoupling\n  Efficiency from Optimization", "abstract": "Human syntactic structures are usually represented as graphs. Much research\nhas focused on the mapping between such graphs and linguistic sequences, but\nless attention has been paid to the shapes of the graphs themselves: their\ntopologies. This study investigates how the topologies of syntactic graphs\nreveal traces of the processes that led to their emergence. I report a new\nuniversal regularity in syntactic structures: Their topology is communicatively\nefficient above chance. The pattern holds, without exception, for all 124\nlanguages studied, across linguistic families and modalities (spoken, written,\nand signed). This pattern can arise from a process optimizing for communicative\nefficiency or, alternatively, by construction, as a by-effect of a sublinear\npreferential attachment process reflecting language production mechanisms known\nfrom psycholinguistics. This dual explanation shows how communicative\nefficiency, per se, does not require optimization. Among the two options,\nefficiency without optimization offers the better explanation for the new\npattern.", "published": "2023-01-31 22:35:11", "link": "http://arxiv.org/abs/2302.00129v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "The Flan Collection: Designing Data and Methods for Effective\n  Instruction Tuning", "abstract": "We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.", "published": "2023-01-31 15:03:44", "link": "http://arxiv.org/abs/2301.13688v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "UPop: Unified and Progressive Pruning for Compressing Vision-Language\n  Transformers", "abstract": "Real-world data contains a vast amount of multimodal information, among which\nvision and language are the two most representative modalities. Moreover,\nincreasingly heavier models, \\textit{e}.\\textit{g}., Transformers, have\nattracted the attention of researchers to model compression. However, how to\ncompress multimodal models, especially vison-language Transformers, is still\nunder-explored. This paper proposes the \\textbf{U}nified and\n\\textbf{P}r\\textbf{o}gressive \\textbf{P}runing (\\textbf{\\emph{UPop}}) as a\nuniversal vison-language Transformer compression framework, which incorporates\n1) unifiedly searching multimodal subnets in a continuous optimization space\nfrom the original model, which enables automatic assignment of pruning ratios\namong compressible modalities and structures; 2) progressively searching and\nretraining the subnet, which maintains convergence between the search and\nretrain to attain higher compression ratios. Experiments on various tasks,\ndatasets, and model architectures demonstrate the effectiveness and versatility\nof the proposed UPop framework. The code is available at\nhttps://github.com/sdc17/UPop.", "published": "2023-01-31 16:18:52", "link": "http://arxiv.org/abs/2301.13741v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Execution-based Code Generation using Deep Reinforcement Learning", "abstract": "The utilization of programming language (PL) models, pre-trained on\nlarge-scale code corpora, as a means of automating software engineering\nprocesses has demonstrated considerable potential in streamlining various code\ngeneration tasks such as code completion, code translation, and program\nsynthesis. However, current approaches mainly rely on supervised fine-tuning\nobjectives borrowed from text generation, neglecting unique sequence-level\ncharacteristics of code, including but not limited to compilability as well as\nsyntactic and functional correctness. To address this limitation, we propose\nPPOCoder, a new framework for code generation that synergistically combines\npre-trained PL models with Proximal Policy Optimization (PPO) which is a widely\nused deep reinforcement learning technique. By utilizing non-differentiable\nfeedback from code execution and structure alignment, PPOCoder seamlessly\nintegrates external code-specific knowledge into the model optimization\nprocess. It's important to note that PPOCoder is a task-agnostic and\nmodel-agnostic framework that can be used across different code generation\ntasks and PLs. Extensive experiments on three code generation tasks demonstrate\nthe effectiveness of our proposed approach compared to SOTA methods, achieving\nsignificant improvements in compilation success rates and functional\ncorrectness across different PLs.", "published": "2023-01-31 18:02:26", "link": "http://arxiv.org/abs/2301.13816v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "Grounding Language Models to Images for Multimodal Inputs and Outputs", "abstract": "We propose an efficient method to ground pretrained text-only language models\nto the visual domain, enabling them to process arbitrarily interleaved\nimage-and-text data, and generate text interleaved with retrieved images. Our\nmethod leverages the abilities of language models learnt from large scale\ntext-only pretraining, such as in-context learning and free-form text\ngeneration. We keep the language model frozen, and finetune input and output\nlinear layers to enable cross-modality interactions. This allows our model to\nprocess arbitrarily interleaved image-and-text inputs, and generate free-form\ntext interleaved with retrieved images. We achieve strong zero-shot performance\non grounded tasks such as contextual image retrieval and multimodal dialogue,\nand showcase compelling interactive abilities. Our approach works with any\noff-the-shelf language model and paves the way towards an effective, general\nsolution for leveraging pretrained language models in visually grounded\nsettings.", "published": "2023-01-31 18:33:44", "link": "http://arxiv.org/abs/2301.13823v4", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image\n  Diffusion Models", "abstract": "Recent text-to-image generative models have demonstrated an unparalleled\nability to generate diverse and creative imagery guided by a target text\nprompt. While revolutionary, current state-of-the-art diffusion models may\nstill fail in generating images that fully convey the semantics in the given\ntext prompt. We analyze the publicly available Stable Diffusion model and\nassess the existence of catastrophic neglect, where the model fails to generate\none or more of the subjects from the input prompt. Moreover, we find that in\nsome cases the model also fails to correctly bind attributes (e.g., colors) to\ntheir corresponding subjects. To help mitigate these failure cases, we\nintroduce the concept of Generative Semantic Nursing (GSN), where we seek to\nintervene in the generative process on the fly during inference time to improve\nthe faithfulness of the generated images. Using an attention-based formulation\nof GSN, dubbed Attend-and-Excite, we guide the model to refine the\ncross-attention units to attend to all subject tokens in the text prompt and\nstrengthen - or excite - their activations, encouraging the model to generate\nall subjects described in the text prompt. We compare our approach to\nalternative approaches and demonstrate that it conveys the desired concepts\nmore faithfully across a range of text prompts.", "published": "2023-01-31 18:10:38", "link": "http://arxiv.org/abs/2301.13826v2", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Benchmarking Large Language Models for News Summarization", "abstract": "Large language models (LLMs) have shown promise for automatic summarization\nbut the reasons behind their successes are poorly understood. By conducting a\nhuman evaluation on ten LLMs across different pretraining methods, prompts, and\nmodel scales, we make two important observations. First, we find instruction\ntuning, and not model size, is the key to the LLM's zero-shot summarization\ncapability. Second, existing studies have been limited by low-quality\nreferences, leading to underestimates of human performance and lower few-shot\nand finetuning performance. To better evaluate LLMs, we perform human\nevaluation over high-quality summaries we collect from freelance writers.\nDespite major stylistic differences such as the amount of paraphrasing, we find\nthat LMM summaries are judged to be on par with human written summaries.", "published": "2023-01-31 18:46:19", "link": "http://arxiv.org/abs/2301.13848v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mathematical Capabilities of ChatGPT", "abstract": "We investigate the mathematical capabilities of two iterations of ChatGPT\n(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on\npublicly available datasets, as well as hand-crafted ones, using a novel\nmethodology. In contrast to formal mathematics, where large databases of formal\nproofs are available (e.g., the Lean Mathematical Library), current datasets of\nnatural-language mathematics, used to benchmark language models, either cover\nonly elementary mathematics or are very small. We address this by publicly\nreleasing two new datasets: GHOSTS and miniGHOSTS. These are the first\nnatural-language datasets curated by working researchers in mathematics that\n(1) aim to cover graduate-level mathematics, (2) provide a holistic overview of\nthe mathematical capabilities of language models, and (3) distinguish multiple\ndimensions of mathematical reasoning. These datasets also test whether ChatGPT\nand GPT-4 can be helpful assistants to professional mathematicians by emulating\nuse cases that arise in the daily professional activities of mathematicians. We\nbenchmark the models on a range of fine-grained performance metrics. For\nadvanced mathematics, this is the most detailed evaluation effort to date. We\nfind that ChatGPT can be used most successfully as a mathematical assistant for\nquerying facts, acting as a mathematical search engine and knowledge base\ninterface. GPT-4 can additionally be used for undergraduate-level mathematics\nbut fails on graduate-level difficulty. Contrary to many positive reports in\nthe media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of\nselection bias), their overall mathematical performance is well below the level\nof a graduate student. Hence, if your goal is to use ChatGPT to pass a\ngraduate-level math exam, you would be better off copying from your average\npeer!", "published": "2023-01-31 18:59:03", "link": "http://arxiv.org/abs/2301.13867v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PADL: Language-Directed Physics-Based Character Control", "abstract": "Developing systems that can synthesize natural and life-like motions for\nsimulated characters has long been a focus for computer animation. But in order\nfor these systems to be useful for downstream applications, they need not only\nproduce high-quality motions, but must also provide an accessible and versatile\ninterface through which users can direct a character's behaviors. Natural\nlanguage provides a simple-to-use and expressive medium for specifying a user's\nintent. Recent breakthroughs in natural language processing (NLP) have\ndemonstrated effective use of language-based interfaces for applications such\nas image generation and program synthesis. In this work, we present PADL, which\nleverages recent innovations in NLP in order to take steps towards developing\nlanguage-directed controllers for physics-based character animation. PADL\nallows users to issue natural language commands for specifying both high-level\ntasks and low-level skills that a character should perform. We present an\nadversarial imitation learning approach for training policies to map high-level\nlanguage commands to low-level controls that enable a character to perform the\ndesired task and skill specified by a user's commands. Furthermore, we propose\na multi-task aggregation method that leverages a language-based multiple-choice\nquestion-answering approach to determine high-level task objectives from\nlanguage commands. We show that our framework can be applied to effectively\ndirect a simulated humanoid character to perform a diverse array of complex\nmotor skills.", "published": "2023-01-31 18:59:22", "link": "http://arxiv.org/abs/2301.13868v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GR"], "primary_category": "cs.LG"}
{"title": "ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View\n  Semantic Consistency", "abstract": "Recently, great success has been made in learning visual representations from\ntext supervision, facilitating the emergence of text-supervised semantic\nsegmentation. However, existing works focus on pixel grouping and cross-modal\nsemantic alignment, while ignoring the correspondence among multiple augmented\nviews of the same image. To overcome such limitation, we propose\nmulti-\\textbf{View} \\textbf{Co}nsistent learning (ViewCo) for text-supervised\nsemantic segmentation. Specifically, we first propose text-to-views consistency\nmodeling to learn correspondence for multiple views of the same input image.\nAdditionally, we propose cross-view segmentation consistency modeling to\naddress the ambiguity issue of text supervision by contrasting the segment\nfeatures of Siamese visual encoders. The text-to-views consistency benefits the\ndense assignment of the visual features by encouraging different crops to align\nwith the same text, while the cross-view segmentation consistency modeling\nprovides additional self-supervision, overcoming the limitation of ambiguous\ntext supervision for segmentation masks. Trained with large-scale image-text\ndata, our model can directly segment objects of arbitrary categories in a\nzero-shot manner. Extensive experiments show that ViewCo outperforms\nstate-of-the-art methods on average by up to 2.9\\%, 1.6\\%, and 2.4\\% mIoU on\nPASCAL VOC2012, PASCAL Context, and COCO, respectively.", "published": "2023-01-31 01:57:52", "link": "http://arxiv.org/abs/2302.10307v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Neural Target Speech Extraction: An Overview", "abstract": "Humans can listen to a target speaker even in challenging acoustic conditions\nthat have noise, reverberation, and interfering speakers. This phenomenon is\nknown as the cocktail-party effect. For decades, researchers have focused on\napproaching the listening ability of humans. One critical issue is handling\ninterfering speakers because the target and non-target speech signals share\nsimilar characteristics, complicating their discrimination. Target\nspeech/speaker extraction (TSE) isolates the speech signal of a target speaker\nfrom a mixture of several speakers with or without noises and reverberations\nusing clues that identify the speaker in the mixture. Such clues might be a\nspatial clue indicating the direction of the target speaker, a video of the\nspeaker's lips, or a pre-recorded enrollment utterance from which their voice\ncharacteristics can be derived. TSE is an emerging field of research that has\nreceived increased attention in recent years because it offers a practical\napproach to the cocktail-party problem and involves such aspects of signal\nprocessing as audio, visual, array processing, and deep learning. This paper\nfocuses on recent neural-based approaches and presents an in-depth overview of\nTSE. We guide readers through the different major approaches, emphasizing the\nsimilarities among frameworks and discussing potential future directions.", "published": "2023-01-31 00:26:52", "link": "http://arxiv.org/abs/2301.13341v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with\n  Natural Language Style Prompt", "abstract": "Expressive text-to-speech (TTS) aims to synthesize different speaking style\nspeech according to human's demands. Nowadays, there are two common ways to\ncontrol speaking styles: (1) Pre-defining a group of speaking style and using\ncategorical index to denote different speaking style. However, there are\nlimitations in the diversity of expressiveness, as these models can only\ngenerate the pre-defined styles. (2) Using reference speech as style input,\nwhich results in a problem that the extracted style information is not\nintuitive or interpretable. In this study, we attempt to use natural language\nas style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone\nin full of sad mood with some helpless feeling\". Considering that there is no\nexisting TTS corpus which is proper to benchmark this novel task, we first\nconstruct a speech corpus, whose speech samples are annotated with not only\ncontent transcriptions but also style descriptions in natural language. Then we\npropose an expressive TTS model, named as InstructTTS, which is novel in the\nsense of following aspects: (1) We fully take the advantage of self-supervised\nlearning and cross-modal metric learning, and propose a novel three-stage\ntraining procedure to obtain a robust sentence embedding model, which can\neffectively capture semantic information from the style prompts and control the\nspeaking style in the generated speech. (2) We propose to model acoustic\nfeatures in discrete latent space and train a novel discrete diffusion\nprobabilistic model to generate vector-quantized (VQ) acoustic tokens rather\nthan the commonly-used mel spectrogram. (3) We jointly apply mutual information\n(MI) estimation and minimization during acoustic model training to minimize\nstyle-speaker and style-content MI, avoiding possible content and speaker\ninformation leakage from the style prompt.", "published": "2023-01-31 14:26:52", "link": "http://arxiv.org/abs/2301.13662v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automated Time-frequency Domain Audio Crossfades using Graph Cuts", "abstract": "The problem of transitioning smoothly from one audio clip to another arises\nin many music consumption scenarios, especially as music consumption has moved\nfrom professionally curated and live-streamed radios to personal playback\ndevices and services. we present the first steps toward a new method of\nautomatically transitioning from one audio clip to another by discretizing the\nfrequency spectrum into bins and then finding transition times for each bin. We\nphrase the problem as one of graph flow optimization; specifically\nmin-cut/max-flow.", "published": "2023-01-31 03:05:48", "link": "http://arxiv.org/abs/2301.13380v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Comparative Analysis of Different Pitch and Metrical Grid Encoding\n  Methods in the Task of Sequential Music Generation", "abstract": "Pitch and meter are two fundamental music features for symbolic music\ngeneration tasks, where researchers usually choose different encoding methods\ndepending on specific goals. However, the advantages and drawbacks of different\nencoding methods have not been frequently discussed. This paper presents a\nintegrated analysis of the influence of two low-level feature, pitch and meter,\non the performance of a token-based sequential music generation model. First,\nthe commonly used MIDI number encoding and a less used class-octave encoding\nare compared. Second, an dense intra-bar metric grid is imposed to the encoded\nsequence as auxiliary features. Different complexity and resolutions of the\nmetric grid are compared. For complexity, the single token approach and the\nmultiple token approach are compared; for grid resolution, 0 (ablation), 1\n(bar-level), 4 (downbeat-level) 12, (8th-triplet-level) up to 64\n(64th-note-grid-level) are compared; for duration resolution, 4, 8, 12 and 16\nsubdivisions per beat are compared. All different encodings are tested on\nseparately trained Transformer-XL models for a melody generation task.\nRegarding distribution similarity of several objective evaluation metrics to\nthe test dataset, results suggest that the class-octave encoding significantly\noutperforms the taken-for-granted MIDI encoding on pitch-related metrics; finer\ngrids and multiple-token grids improve the rhythmic quality, but also suffer\nfrom over-fitting at early training stage. Results display a general phenomenon\nof over-fitting from two aspects, the pitch embedding space and the test loss\nof the single-token grid encoding. From a practical perspective, we both\ndemonstrate the feasibility and raise the concern of easy over-fitting problem\nof using smaller networks and lower embedding dimensions on the generation\ntask. The findings can also contribute to futural models in terms of feature\nengineering.", "published": "2023-01-31 03:19:50", "link": "http://arxiv.org/abs/2301.13383v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Analysis of Classification Approaches for Hit Song Prediction using\n  Engineered Metadata Features with Lyrics and Audio Features", "abstract": "Hit song prediction, one of the emerging fields in music information\nretrieval (MIR), remains a considerable challenge. Being able to understand\nwhat makes a given song a hit is clearly beneficial to the whole music\nindustry. Previous approaches to hit song prediction have focused on using\naudio features of a record. This study aims to improve the prediction result of\nthe top 10 hits among Billboard Hot 100 songs using more alternative metadata,\nincluding song audio features provided by Spotify, song lyrics, and novel\nmetadata-based features (title topic, popularity continuity and genre class).\nFive machine learning approaches are applied, including: k-nearest neighbours,\nNaive Bayes, Random Forest, Logistic Regression and Multilayer Perceptron. Our\nresults show that Random Forest (RF) and Logistic Regression (LR) with all\nfeatures (including novel features, song audio features and lyrics features)\noutperforms other models, achieving 89.1% and 87.2% accuracy, and 0.91 and 0.93\nAUC, respectively. Our findings also demonstrate the utility of our novel music\nmetadata features, which contributed most to the models' discriminative\nperformance.", "published": "2023-01-31 09:48:53", "link": "http://arxiv.org/abs/2301.13507v1", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
