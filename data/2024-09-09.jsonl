{"title": "Critical Dynamics of Random Surfaces", "abstract": "Conformal field theories with central charge $c\\le1$ on random surfaces have\nbeen extensively studied in the past. Here, this discussion is extended from\ntheir equilibrium distribution to their critical dynamics. This is motivated by\nthe conjecture that these models describe the time evolution of certain social\nnetworks that are self-driven to a critical point. The time evolution of the\nsurface area is shown to follow a Cox Ingersol Ross process. Planar surfaces\nshrink, while higher genus surfaces grow until the cosmological constant stops\ntheir growth. Two different equilibrium states are distinguished, dominated by\n(i) planar surfaces, and (ii) ``foamy'' surfaces, whose genus diverges. Time\nvariations of the order parameter are analyzed and are found to have\ngeneralized hyperbolic distributions. In state (i), those have power law tails\nwith a tail index close to 4. Analogies between the time evolution of the order\nparameter and a multifractal random walk are also pointed out.", "published": "2024-09-09 12:12:16", "link": "http://arxiv.org/abs/2409.05547v2", "categories": ["hep-th", "cond-mat.stat-mech", "q-fin.ST"], "primary_category": "hep-th"}
{"title": "UPCS: Unbiased Persona Construction for Dialogue Generation", "abstract": "Narrative systems, such as dialogue and storytelling systems, often utilize\npersona profiles to enhance personalized interactions. Existing persona\nprofiles frequently exhibit biases, posing risks to system integrity and\nfairness. To address this, we introduce the UPCS framework, which categorizes\ncharacter descriptions into eight dimensions, including bias mitigation\nstrategies. Experimental results demonstrate UPCS's superiority in accuracy,\ndiversity, bias elimination, and user satisfaction, marking a significant\nadvancement in persona construction for reliable narrative systems.", "published": "2024-09-09 00:40:47", "link": "http://arxiv.org/abs/2409.05257v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RexUniNLU: Recursive Method with Explicit Schema Instructor for\n  Universal NLU", "abstract": "Information Extraction (IE) and Text Classification (CLS) serve as the\nfundamental pillars of NLU, with both disciplines relying on analyzing input\nsequences to categorize outputs into pre-established schemas. However, there is\nno existing encoder-based model that can unify IE and CLS tasks from this\nperspective. To fully explore the foundation shared within NLU tasks, we have\nproposed a Recursive Method with Explicit Schema Instructor for Universal NLU.\nSpecifically, we firstly redefine the true universal information extraction\n(UIE) with a formal formulation that covers almost all extraction schemas,\nincluding quadruples and quintuples which remain unsolved for previous UIE\nmodels. Then, we expands the formulation to all CLS and multi-modal NLU tasks.\nBased on that, we introduce RexUniNLU, an universal NLU solution that employs\nexplicit schema constraints for IE and CLS, which encompasses all IE and CLS\ntasks and prevent incorrect connections between schema and input sequence. To\navoid interference between different schemas, we reset the position ids and\nattention mask matrices. Extensive experiments are conducted on IE, CLS in both\nEnglish and Chinese, and multi-modality, revealing the effectiveness and\nsuperiority. Our codes are publicly released.", "published": "2024-09-09 01:59:29", "link": "http://arxiv.org/abs/2409.05275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diagnostic Reasoning in Natural Language: Computational Model and\n  Application", "abstract": "Diagnostic reasoning is a key component of expert work in many domains. It is\na hard, time-consuming activity that requires expertise, and AI research has\ninvestigated the ways automated systems can support this process. Yet, due to\nthe complexity of natural language, the applications of AI for diagnostic\nreasoning to language-related tasks are lacking. To close this gap, we\ninvestigate diagnostic abductive reasoning (DAR) in the context of\nlanguage-grounded tasks (NL-DAR). We propose a novel modeling framework for\nNL-DAR based on Pearl's structural causal models and instantiate it in a\ncomprehensive study of scientific paper assessment in the biomedical domain. We\nuse the resulting dataset to investigate the human decision-making process in\nNL-DAR and determine the potential of LLMs to support structured\ndecision-making over text. Our framework, open resources and tools lay the\ngroundwork for the empirical study of collaborative diagnostic reasoning in the\nage of LLMs, in the scholarly domain and beyond.", "published": "2024-09-09 06:55:37", "link": "http://arxiv.org/abs/2409.05367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Application Specific Compression of Deep Learning Models", "abstract": "Large Deep Learning models are compressed and deployed for specific\napplications. However, current Deep Learning model compression methods do not\nutilize the information about the target application. As a result, the\ncompressed models are application agnostic. Our goal is to customize the model\ncompression process to create a compressed model that will perform better for\nthe target application. Our method, Application Specific Compression (ASC),\nidentifies and prunes components of the large Deep Learning model that are\nredundant specifically for the given target application. The intuition of our\nwork is to prune the parts of the network that do not contribute significantly\nto updating the data representation for the given application. We have\nexperimented with the BERT family of models for three applications: Extractive\nQA, Natural Language Inference, and Paraphrase Identification. We observe that\ncustomized compressed models created using ASC method perform better than\nexisting model compression methods and off-the-shelf compressed models.", "published": "2024-09-09 06:55:38", "link": "http://arxiv.org/abs/2409.05368v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STLM Engineering Report: Dropout", "abstract": "In this work we explore the relevance of dropout for modern language models,\nparticularly in the context of models on the scale of <100M parameters. We\nexplore it's relevance firstly in the regime of improving the sample efficiency\nof models given small, high quality datasets, and secondly in the regime of\nimproving the quality of its fit on larger datasets where models may underfit.\nWe find that concordant with conventional wisdom, dropout remains effective in\nthe overfitting scenario, and that furthermore it may have some relevance for\nimproving the fit of models even in the case of excess data, as suggested by\nprevious research. In the process we find that the existing explanation for the\nmechanism behind this performance gain is not applicable in the case of\nlanguage modelling.", "published": "2024-09-09 08:24:29", "link": "http://arxiv.org/abs/2409.05423v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Representational Analysis of Binding in Language Models", "abstract": "Entity tracking is essential for complex reasoning. To perform in-context\nentity tracking, language models (LMs) must bind an entity to its attribute\n(e.g., bind a container to its content) to recall attribute for a given entity.\nFor example, given a context mentioning ``The coffee is in Box Z, the stone is\nin Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later,\nLMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs,\nexisting research introduces a Binding ID mechanism and states that LMs use a\nabstract concept called Binding ID (BI) to internally mark entity-attribute\npairs. However, they have not captured the Ordering ID (OI) from entity\nactivations that directly determines the binding behaviour. In this work, we\nprovide a novel view of the BI mechanism by localizing OI and proving the\ncausality between OI and binding behaviour. Specifically, by leveraging\ndimension reduction methods (e.g., PCA), we discover that there exists a\nlow-rank subspace in the activations of LMs, that primarily encodes the order\n(i.e., OI) of entity and attribute. Moreover, we also discover the causal\neffect of OI on binding that when editing representations along the OI encoding\ndirection, LMs tend to bind a given entity to other attributes accordingly. For\nexample, by patching activations along the OI encoding direction we can make\nthe LM to infer ``Box Z contains the stone'' and ``Box Z contains the map''.", "published": "2024-09-09 09:04:56", "link": "http://arxiv.org/abs/2409.05448v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QiBERT -- Classifying Online Conversations Messages with BERT as a\n  Feature", "abstract": "Recent developments in online communication and their usage in everyday life\nhave caused an explosion in the amount of a new genre of text data, short text.\nThus, the need to classify this type of text based on its content has a\nsignificant implication in many areas. Online debates are no exception, once\nthese provide access to information about opinions, positions and preferences\nof its users. This paper aims to use data obtained from online social\nconversations in Portuguese schools (short text) to observe behavioural trends\nand to see if students remain engaged in the discussion when stimulated. This\nproject used the state of the art (SoA) Machine Learning (ML) algorithms and\nmethods, through BERT based models to classify if utterances are in or out of\nthe debate subject. Using SBERT embeddings as a feature, with supervised\nlearning, the proposed model achieved results above 0.95 average accuracy for\nclassifying online messages. Such improvements can help social scientists\nbetter understand human communication, behaviour, discussion and persuasion.", "published": "2024-09-09 11:38:06", "link": "http://arxiv.org/abs/2409.05530v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spatially-Aware Speaker for Vision-and-Language Navigation Instruction\n  Generation", "abstract": "Embodied AI aims to develop robots that can \\textit{understand} and execute\nhuman language instructions, as well as communicate in natural languages. On\nthis front, we study the task of generating highly detailed navigational\ninstructions for the embodied robots to follow. Although recent studies have\ndemonstrated significant leaps in the generation of step-by-step instructions\nfrom sequences of images, the generated instructions lack variety in terms of\ntheir referral to objects and landmarks. Existing speaker models learn\nstrategies to evade the evaluation metrics and obtain higher scores even for\nlow-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker),\nan instruction generator or \\textit{Speaker} model that utilises both\nstructural and semantic knowledge of the environment to produce richer\ninstructions. For training, we employ a reward learning method in an\nadversarial setting to avoid systematic bias introduced by language evaluation\nmetrics. Empirically, our method outperforms existing instruction generation\nmodels, evaluated using standard metrics. Our code is available at\n\\url{https://github.com/gmuraleekrishna/SAS}.", "published": "2024-09-09 13:12:11", "link": "http://arxiv.org/abs/2409.05583v1", "categories": ["cs.CL", "I.2.7; I.2.10; I.2.9"], "primary_category": "cs.CL"}
{"title": "WinoPron: Revisiting English Winogender Schemas for Consistency,\n  Coverage, and Grammatical Case", "abstract": "While measuring bias and robustness in coreference resolution are important\ngoals, such measurements are only as good as the tools we use to measure them.\nWinogender Schemas (Rudinger et al., 2018) are an influential dataset proposed\nto evaluate gender bias in coreference resolution, but a closer look reveals\nissues with the data that compromise its use for reliable evaluation, including\ntreating different pronominal forms as equivalent, violations of template\nconstraints, and typographical errors. We identify these issues and fix them,\ncontributing a new dataset: WinoPron. Using WinoPron, we evaluate two\nstate-of-the-art supervised coreference resolution systems, SpanBERT, and five\nsizes of FLAN-T5, and demonstrate that accusative pronouns are harder to\nresolve for all models. We also propose a new method to evaluate pronominal\nbias in coreference resolution that goes beyond the binary. With this method,\nwe also show that bias characteristics vary not just across pronoun sets (e.g.,\nhe vs. she), but also across surface forms of those sets (e.g., him vs. his).", "published": "2024-09-09 14:19:21", "link": "http://arxiv.org/abs/2409.05653v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Democratizing Multilingual Large Language Models For Medicine\n  Through A Two-Stage Instruction Fine-tuning Approach", "abstract": "Open-source, multilingual medical large language models (LLMs) have the\npotential to serve linguistically diverse populations across different regions.\nAdapting generic LLMs for healthcare often requires continual pretraining, but\nthis approach is computationally expensive and sometimes impractical.\nInstruction fine-tuning on a specific task may not always guarantee optimal\nperformance due to the lack of broader domain knowledge that the model needs to\nunderstand and reason effectively in diverse scenarios. To address these\nchallenges, we introduce two multilingual instruction fine-tuning datasets,\nMMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in\nsix languages. We propose a two-stage training paradigm: the first stage\ninjects general medical knowledge using MMed-IFT, while the second stage\nfine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method\nachieves competitive results on both English and multilingual benchmarks,\nstriking a balance between computational efficiency and performance. We plan to\nmake our dataset and model weights public at\n\\url{https://github.com/SpassMed/Med-Llama3} in the future.", "published": "2024-09-09 15:42:19", "link": "http://arxiv.org/abs/2409.05732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct", "abstract": "The development of Multimodal Large Language Models (MLLMs) has seen\nsignificant advancements with increasing demands in various fields (e.g.,\nmultimodal agents, embodied intelligence). While model-driven approaches\nattempt to enhance MLLMs capabilities through diverse architectures, the gains\nhave become increasingly marginal. Conversely, data-driven methods, which scale\nup image-text instruction data, are more effective but face limited data\ndiversity and complexity challenges. The absence of high-quality data\nconstitutes a significant development barrier for MLLMs. To address the data\nquality bottleneck, we propose MMEvol, a novel multimodal instruction data\nevolution framework. This framework iteratively improve data quality through a\nrefined combination of fine-grained perception, cognitive reasoning, and\ninteraction evolution, generating a more complex and diverse image-text\ninstruction dataset that empowers MLLMs with enhanced capabilities. Beginning\nwith an initial set of instructions, SEED-163K, we utilize MMEvol to\nsystematically broaden the diversity of instruction types, extend visual\nreasoning steps to improve cognitive reasoning abilities, and thoroughly\nexplore fine-grained information within images to enhance visual understanding\nand robustness. To comprehensively evaluate the effectiveness of our approach,\nwe conduct extensive qualitative analysis and quantitative experiments across\n13 vision-language tasks. Compared to baseline models trained with the initial\nseed data, the results demonstrate that our method achieves an average accuracy\nimprovement of 3.1 percentage points. Furthermore, our approach reaches\nstate-of-the-art (SOTA) performance in nine tasks using significantly less data\ncompared to state-of-the-art models.", "published": "2024-09-09 17:44:00", "link": "http://arxiv.org/abs/2409.05840v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mathematical Formalized Problem Solving and Theorem Proving in Different\n  Fields in Lean 4", "abstract": "Formalizing mathematical proofs using computerized verification languages\nlike Lean 4 has the potential to significantly impact the field of mathematics,\nit offers prominent capabilities for advancing mathematical reasoning. However,\nexisting efforts are largely limited to creating formalized versions of proofs\nfrom extensive online mathematical corpora, struggling to keep pace with the\nrapidly evolving nature of mathematics. To bridge the gap between traditional\nand computerized proof techniques, this paper explores the use of Large\nLanguage Models (LLMs) to generate formal proof steps and complete formalized\nproofs. By converting natural language (NL) mathematical proofs into formalized\nversions, this work introduces the basic structure and tactics of the Lean 4\nlanguage. The goal is to determine how AI can be leveraged to assist the\nmathematical formalization process and improve its performance. Several\nexamples are provided that demonstrate solving problems using both traditional\nand Lean 4-based approaches. Ultimately, this paper presents an explanation of\nthe foundations of Lean 4 and comparative analyses of the mathematical\nformalization process using traditional and AI-augmented techniques. The\nfindings indicate that AI- powered tools have significant potential to\naccelerate and enhance the formalization of mathematical proofs, paving the way\nfor more efficient and reliable theorem-proving for AI for Math in the future.", "published": "2024-09-09 18:21:28", "link": "http://arxiv.org/abs/2409.05977v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransformerRanker: A Tool for Efficiently Finding the Best-Suited\n  Language Models for Downstream Classification Tasks", "abstract": "Classification tasks in NLP are typically addressed by selecting a\npre-trained language model (PLM) from a model hub, and fine-tuning it for the\ntask at hand. However, given the very large number of PLMs that are currently\navailable, a practical challenge is to determine which of them will perform\nbest for a specific downstream task. With this paper, we introduce\nTransformerRanker, a lightweight library that efficiently ranks PLMs for\nclassification tasks without the need for computationally costly fine-tuning.\nOur library implements current approaches for transferability estimation\n(LogME, H-Score, kNN), in combination with layer aggregation options, which we\nempirically showed to yield state-of-the-art rankings of PLMs (Garbas et al.,\n2024). We designed the interface to be lightweight and easy to use, allowing\nusers to directly connect to the HuggingFace Transformers and Dataset\nlibraries. Users need only select a downstream classification task and a list\nof PLMs to create a ranking of likely best-suited PLMs for their task. We make\nTransformerRanker available as a pip-installable open-source library\nhttps://github.com/flairNLP/transformer-ranker.", "published": "2024-09-09 18:47:00", "link": "http://arxiv.org/abs/2409.05997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying the sources of ideological bias in GPT models through\n  linguistic variation in output", "abstract": "Extant work shows that generative AI models such as GPT-3.5 and 4 perpetuate\nsocial stereotypes and biases. One concerning but less explored source of bias\nis ideology. Do GPT models take ideological stances on politically sensitive\ntopics? In this article, we provide an original approach to identifying\nideological bias in generative models, showing that bias can stem from both the\ntraining data and the filtering algorithm. We leverage linguistic variation in\ncountries with contrasting political attitudes to evaluate bias in average GPT\nresponses to sensitive political topics in those languages. First, we find that\nGPT output is more conservative in languages that map well onto conservative\nsocieties (i.e., Polish), and more liberal in languages used uniquely in\nliberal societies (i.e., Swedish). This result provides strong evidence of\ntraining data bias in GPT models. Second, differences across languages observed\nin GPT-3.5 persist in GPT-4, even though GPT-4 is significantly more liberal\ndue to OpenAI's filtering policy. Our main takeaway is that generative model\ntraining must focus on high-quality, curated datasets to reduce bias, even if\nit entails a compromise in training data size. Filtering responses after\ntraining only introduces new biases and does not remove the underlying training\nbiases.", "published": "2024-09-09 20:11:08", "link": "http://arxiv.org/abs/2409.06043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information\n  in Task-Oriented Dialog", "abstract": "We introduce ClarQ-LLM, an evaluation framework consisting of bilingual\nEnglish-Chinese conversation tasks, conversational agents and evaluation\nmetrics, designed to serve as a strong benchmark for assessing agents' ability\nto ask clarification questions in task-oriented dialogues. The benchmark\nincludes 31 different task types, each with 10 unique dialogue scenarios\nbetween information seeker and provider agents. The scenarios require the\nseeker to ask questions to resolve uncertainty and gather necessary information\nto complete tasks. Unlike traditional benchmarks that evaluate agents based on\nfixed dialogue content, ClarQ-LLM includes a provider conversational agent to\nreplicate the original human provider in the benchmark. This allows both\ncurrent and future seeker agents to test their ability to complete information\ngathering tasks through dialogue by directly interacting with our provider\nagent. In tests, LLAMA3.1 405B seeker agent managed a maximum success rate of\nonly 60.05\\%, showing that ClarQ-LLM presents a strong challenge for future\nresearch.", "published": "2024-09-09 22:29:35", "link": "http://arxiv.org/abs/2409.06097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LegiLM: A Fine-Tuned Legal Language Model for Data Compliance", "abstract": "Ensuring compliance with international data protection standards for privacy\nand data security is a crucial but complex task, often requiring substantial\nlegal expertise. This paper introduces LegiLM, a novel legal language model\nspecifically tailored for consulting on data or information compliance. LegiLM\nleverages a pre-trained GDPR Fines dataset and has been fine-tuned to\nautomatically assess whether particular actions or events breach data security\nand privacy regulations. By incorporating a specialized dataset that includes\nglobal data protection laws, meticulously annotated policy documents, and\nrelevant privacy policies, LegiLM is optimized for addressing data compliance\nchallenges. The model integrates advanced legal reasoning methods and\ninformation retrieval enhancements to enhance accuracy and reliability in\npractical legal consulting scenarios. Our evaluation using a custom benchmark\ndataset demonstrates that LegiLM excels in detecting data regulation breaches,\noffering sound legal justifications, and recommending necessary compliance\nmodifications, setting a new benchmark for AI-driven legal compliance\nsolutions. Our resources are publicly available at\nhttps://github.com/DAOLegalAI/LegiLM", "published": "2024-09-09 02:06:52", "link": "http://arxiv.org/abs/2409.13721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Relationship between Truth and Political Bias in Language Models", "abstract": "Language model alignment research often attempts to ensure that models are\nnot only helpful and harmless, but also truthful and unbiased. However,\noptimizing these objectives simultaneously can obscure how improving one aspect\nmight impact the others. In this work, we focus on analyzing the relationship\nbetween two concepts essential in both language model alignment and political\nscience: truthfulness and political bias. We train reward models on various\npopular truthfulness datasets and subsequently evaluate their political bias.\nOur findings reveal that optimizing reward models for truthfulness on these\ndatasets tends to result in a left-leaning political bias. We also find that\nexisting open-source reward models (i.e., those trained on standard human\npreference datasets) already show a similar bias and that the bias is larger\nfor larger models. These results raise important questions about the datasets\nused to represent truthfulness, potential limitations of aligning models to be\nboth truthful and politically unbiased, and what language models capture about\nthe relationship between truth and politics.", "published": "2024-09-09 02:28:53", "link": "http://arxiv.org/abs/2409.05283v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seek and Solve Reasoning for Table Question Answering", "abstract": "The complexities of table structures and question logic make table-based\nquestion answering (TQA) tasks challenging for Large Language Models (LLMs),\noften requiring task simplification before solving. This paper reveals that the\nreasoning process during task simplification may be more valuable than the\nsimplified tasks themselves and aims to improve TQA performance by leveraging\nLLMs' reasoning capabilities. We propose a Seek-and-Solve pipeline that\ninstructs the LLM to first seek relevant information and then answer questions,\nintegrating these two stages at the reasoning level into a coherent\nSeek-and-Solve Chain of Thought (SS-CoT). Additionally, we distill a\nsingle-step TQA-solving prompt from this pipeline, using demonstrations with\nSS-CoT paths to guide the LLM in solving complex TQA tasks under In-Context\nLearning settings. Our experiments show that our approaches result in improved\nperformance and reliability while being efficient. Our findings emphasize the\nimportance of eliciting LLMs' reasoning capabilities to handle complex TQA\ntasks effectively.", "published": "2024-09-09 02:41:00", "link": "http://arxiv.org/abs/2409.05286v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models", "abstract": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability.", "published": "2024-09-09 07:32:30", "link": "http://arxiv.org/abs/2409.05385v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking and Building Zero-Shot Hindi Retrieval Model with\n  Hindi-BEIR and NLLB-E5", "abstract": "Given the large number of Hindi speakers worldwide, there is a pressing need\nfor robust and efficient information retrieval systems for Hindi. Despite\nongoing research, comprehensive benchmarks for evaluating retrieval models in\nHindi are lacking. To address this gap, we introduce the Hindi-BEIR benchmark,\ncomprising 15 datasets across seven distinct tasks. We evaluate\nstate-of-the-art multilingual retrieval models on the Hindi-BEIR benchmark,\nidentifying task and domain-specific challenges that impact Hindi retrieval\nperformance. Building on the insights from these results, we introduce NLLB-E5,\na multilingual retrieval model that leverages a zero-shot approach to support\nHindi without the need for Hindi training data. We believe our contributions,\nwhich include the release of the Hindi-BEIR benchmark and the NLLB-E5 model,\nwill prove to be a valuable resource for researchers and promote advancements\nin multilingual retrieval models.", "published": "2024-09-09 07:57:43", "link": "http://arxiv.org/abs/2409.05401v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Elsevier Arena: Human Evaluation of Chemistry/Biology/Health\n  Foundational Large Language Models", "abstract": "arXiv admin comment: This version has been removed by arXiv administrators as\nthe submitter did not have the rights to agree to the license at the time of\nsubmission", "published": "2024-09-09 10:30:00", "link": "http://arxiv.org/abs/2409.05486v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation", "abstract": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.", "published": "2024-09-09 13:20:31", "link": "http://arxiv.org/abs/2409.05591v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ExDDI: Explaining Drug-Drug Interaction Predictions with Natural\n  Language", "abstract": "Predicting unknown drug-drug interactions (DDIs) is crucial for improving\nmedication safety. Previous efforts in DDI prediction have typically focused on\nbinary classification or predicting DDI categories, with the absence of\nexplanatory insights that could enhance trust in these predictions. In this\nwork, we propose to generate natural language explanations for DDI predictions,\nenabling the model to reveal the underlying pharmacodynamics and\npharmacokinetics mechanisms simultaneously as making the prediction. To do\nthis, we have collected DDI explanations from DDInter and DrugBank and\ndeveloped various models for extensive experiments and analysis. Our models can\nprovide accurate explanations for unknown DDIs between known drugs. This paper\ncontributes new tools to the field of DDI prediction and lays a solid\nfoundation for further research on generating explanations for DDI predictions.", "published": "2024-09-09 13:23:14", "link": "http://arxiv.org/abs/2409.05592v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Longer is (Not Necessarily) Stronger: Punctuated Long-Sequence Training\n  for Enhanced Speech Recognition and Translation", "abstract": "This paper presents a new method for training sequence-to-sequence models for\nspeech recognition and translation tasks. Instead of the traditional approach\nof training models on short segments containing only lowercase or partial\npunctuation and capitalization (PnC) sentences, we propose training on longer\nutterances that include complete sentences with proper punctuation and\ncapitalization. We achieve this by using the FastConformer architecture which\nallows training 1 Billion parameter models with sequences up to 60 seconds long\nwith full attention. However, while training with PnC enhances the overall\nperformance, we observed that accuracy plateaus when training on sequences\nlonger than 40 seconds across various evaluation settings. Our proposed method\nsignificantly improves punctuation and capitalization accuracy, showing a 25%\nrelative word error rate (WER) improvement on the Earnings-21 and Earnings-22\nbenchmarks. Additionally, training on longer audio segments increases the\noverall model accuracy across speech recognition and translation benchmarks.\nThe model weights and training code are open-sourced though NVIDIA NeMo.", "published": "2024-09-09 13:35:52", "link": "http://arxiv.org/abs/2409.05601v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models", "abstract": "Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties.", "published": "2024-09-09 16:33:16", "link": "http://arxiv.org/abs/2409.05771v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PDAF: A Phonetic Debiasing Attention Framework For Speaker Verification", "abstract": "Speaker verification systems are crucial for authenticating identity through\nvoice. Traditionally, these systems focus on comparing feature vectors,\noverlooking the speech's content. However, this paper challenges this by\nhighlighting the importance of phonetic dominance, a measure of the frequency\nor duration of phonemes, as a crucial cue in speaker verification. A novel\nPhoneme Debiasing Attention Framework (PDAF) is introduced, integrating with\nexisting attention frameworks to mitigate biases caused by phonetic dominance.\nPDAF adjusts the weighting for each phoneme and influences feature extraction,\nallowing for a more nuanced analysis of speech. This approach paves the way for\nmore accurate and reliable identity authentication through voice. Furthermore,\nby employing various weighting strategies, we evaluate the influence of\nphonetic features on the efficacy of the speaker verification system.", "published": "2024-09-09 17:03:38", "link": "http://arxiv.org/abs/2409.05799v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "SVFit: Parameter-Efficient Fine-Tuning of Large Pre-Trained Models Using\n  Singular Values", "abstract": "Large pre-trained models (LPMs) have demonstrated exceptional performance in\ndiverse natural language processing and computer vision tasks. However, fully\nfine-tuning these models poses substantial memory challenges, particularly in\nresource-constrained environments. Parameter-efficient fine-tuning (PEFT)\nmethods, such as LoRA, mitigate this issue by adjusting only a small subset of\nparameters. Nevertheless, these methods typically employ random initialization\nfor low-rank matrices, which can lead to inefficiencies in gradient descent and\ndiminished generalizability due to suboptimal starting points. To address these\nlimitations, we propose SVFit, a novel PEFT approach that leverages singular\nvalue decomposition (SVD) to initialize low-rank matrices using critical\nsingular values as trainable parameters. Specifically, SVFit performs SVD on\nthe pre-trained weight matrix to obtain the best rank-r approximation matrix,\nemphasizing the most critical singular values that capture over 99% of the\nmatrix's information. These top-r singular values are then used as trainable\nparameters to scale the fundamental subspaces of the matrix, facilitating rapid\ndomain adaptation. Extensive experiments across various pre-trained models in\nnatural language understanding, text-to-image generation, and image\nclassification tasks reveal that SVFit outperforms LoRA while requiring 16\ntimes fewer trainable parameters.", "published": "2024-09-09 08:44:53", "link": "http://arxiv.org/abs/2409.05926v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Small Claims Court for the NLP: Judging Legal Text Classification\n  Strategies With Small Datasets", "abstract": "Recent advances in language modelling has significantly decreased the need of\nlabelled data in text classification tasks. Transformer-based models,\npre-trained on unlabeled data, can outmatch the performance of models trained\nfrom scratch for each task. However, the amount of labelled data need to\nfine-tune such type of model is still considerably high for domains requiring\nexpert-level annotators, like the legal domain. This paper investigates the\nbest strategies for optimizing the use of a small labeled dataset and large\namounts of unlabeled data and perform a classification task in the legal area\nwith 50 predefined topics. More specifically, we use the records of demands to\na Brazilian Public Prosecutor's Office aiming to assign the descriptions in one\nof the subjects, which currently demands deep legal knowledge for manual\nfilling. The task of optimizing the performance of classifiers in this scenario\nis especially challenging, given the low amount of resources available\nregarding the Portuguese language, especially in the legal domain. Our results\ndemonstrate that classic supervised models such as logistic regression and SVM\nand the ensembles random forest and gradient boosting achieve better\nperformance along with embeddings extracted with word2vec when compared to BERT\nlanguage model. The latter demonstrates superior performance in association\nwith the architecture of the model itself as a classifier, having surpassed all\nprevious models in that regard. The best result was obtained with Unsupervised\nData Augmentation (UDA), which jointly uses BERT, data augmentation, and\nstrategies of semi-supervised learning, with an accuracy of 80.7% in the\naforementioned task.", "published": "2024-09-09 18:10:05", "link": "http://arxiv.org/abs/2409.05972v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MessIRve: A Large-Scale Spanish Information Retrieval Dataset", "abstract": "Information retrieval (IR) is the task of finding relevant documents in\nresponse to a user query. Although Spanish is the second most spoken native\nlanguage, current IR benchmarks lack Spanish data, hindering the development of\ninformation access tools for Spanish speakers. We introduce MessIRve, a\nlarge-scale Spanish IR dataset with around 730 thousand queries from Google's\nautocomplete API and relevant documents sourced from Wikipedia. MessIRve's\nqueries reflect diverse Spanish-speaking regions, unlike other datasets that\nare translated from English or do not consider dialectal variations. The large\nsize of the dataset allows it to cover a wide variety of topics, unlike smaller\ndatasets. We provide a comprehensive description of the dataset, comparisons\nwith existing datasets, and baseline evaluations of prominent IR models. Our\ncontributions aim to advance Spanish IR research and improve information access\nfor Spanish speakers.", "published": "2024-09-09 18:45:04", "link": "http://arxiv.org/abs/2409.05994v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DetoxBench: Benchmarking Large Language Models for Multitask Fraud &\n  Abuse Detection", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks. However, their practical application in\nhigh-stake domains, such as fraud and abuse detection, remains an area that\nrequires further exploration. The existing applications often narrowly focus on\nspecific tasks like toxicity or hate speech detection. In this paper, we\npresent a comprehensive benchmark suite designed to assess the performance of\nLLMs in identifying and mitigating fraudulent and abusive language across\nvarious real-world scenarios. Our benchmark encompasses a diverse set of tasks,\nincluding detecting spam emails, hate speech, misogynistic language, and more.\nWe evaluated several state-of-the-art LLMs, including models from Anthropic,\nMistral AI, and the AI21 family, to provide a comprehensive assessment of their\ncapabilities in this critical domain. The results indicate that while LLMs\nexhibit proficient baseline performance in individual fraud and abuse detection\ntasks, their performance varies considerably across tasks, particularly\nstruggling with tasks that demand nuanced pragmatic reasoning, such as\nidentifying diverse forms of misogynistic language. These findings have\nimportant implications for the responsible development and deployment of LLMs\nin high-risk applications. Our benchmark suite can serve as a tool for\nresearchers and practitioners to systematically evaluate LLMs for multi-task\nfraud detection and drive the creation of more robust, trustworthy, and\nethically-aligned systems for fraud and abuse detection.", "published": "2024-09-09 21:12:03", "link": "http://arxiv.org/abs/2409.06072v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Doppelg\u00e4nger's Watch: A Split Objective Approach to Large Language\n  Models", "abstract": "In this paper, we investigate the problem of \"generation supervision\" in\nlarge language models, and present a novel bicameral architecture to separate\nsupervision signals from their core capability, helpfulness. Doppelg\\\"anger, a\nnew module parallel to the underlying language model, supervises the generation\nof each token, and learns to concurrently predict the supervision score(s) of\nthe sequences up to and including each token. In this work, we present the\ntheoretical findings, and leave the report on experimental results to a\nforthcoming publication.", "published": "2024-09-09 23:22:27", "link": "http://arxiv.org/abs/2409.06107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Estimating the Completeness of Discrete Speech Units", "abstract": "Representing speech with discrete units has been widely used in speech codec\nand speech generation. However, there are several unverified claims about\nself-supervised discrete units, such as disentangling phonetic and speaker\ninformation with k-means, or assuming information loss after k-means. In this\nwork, we take an information-theoretic perspective to answer how much\ninformation is present (information completeness) and how much information is\naccessible (information accessibility), before and after residual vector\nquantization. We show a lower bound for information completeness and estimate\ncompleteness on discretized HuBERT representations after residual vector\nquantization. We find that speaker information is sufficiently present in\nHuBERT discrete units, and that phonetic information is sufficiently present in\nthe residual, showing that vector quantization does not achieve\ndisentanglement. Our results offer a comprehensive assessment on the choice of\ndiscrete units, and suggest that a lot more information in the residual should\nbe mined rather than discarded.", "published": "2024-09-09 23:31:56", "link": "http://arxiv.org/abs/2409.06109v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Classification performance and reproducibility of GPT-4 omni for\n  information extraction from veterinary electronic health records", "abstract": "Large language models (LLMs) can extract information from veterinary\nelectronic health records (EHRs), but performance differences between models,\nthe effect of temperature settings, and the influence of text ambiguity have\nnot been previously evaluated. This study addresses these gaps by comparing the\nperformance of GPT-4 omni (GPT-4o) and GPT-3.5 Turbo under different conditions\nand investigating the relationship between human interobserver agreement and\nLLM errors. The LLMs and five humans were tasked with identifying six clinical\nsigns associated with Feline chronic enteropathy in 250 EHRs from a veterinary\nreferral hospital. At temperature 0, the performance of GPT-4o compared to the\nmajority opinion of human respondents, achieved 96.9% sensitivity\n(interquartile range [IQR] 92.9-99.3%), 97.6% specificity (IQR 96.5-98.5%),\n80.7% positive predictive value (IQR 70.8-84.6%), 99.5% negative predictive\nvalue (IQR 99.0-99.9%), 84.4% F1 score (IQR 77.3-90.4%), and 96.3% balanced\naccuracy (IQR 95.0-97.9%). The performance of GPT-4o was significantly better\nthan that of its predecessor, GPT-3.5 Turbo, particularly with respect to\nsensitivity where GPT-3.5 Turbo only achieved 81.7% (IQR 78.9-84.8%). Adjusting\nthe temperature for GPT-4o did not significantly impact classification\nperformance. GPT-4o demonstrated greater reproducibility than human pairs\nregardless of temperature, with an average Cohen's kappa of 0.98 (IQR\n0.98-0.99) at temperature 0 compared to 0.8 (IQR 0.78-0.81) for humans. Most\nGPT-4o errors occurred in instances where humans disagreed (35/43 errors,\n81.4%), suggesting that these errors were more likely caused by ambiguity of\nthe EHR than explicit model faults. Using GPT-4o to automate information\nextraction from veterinary EHRs is a viable alternative to manual extraction.", "published": "2024-09-09 21:55:15", "link": "http://arxiv.org/abs/2409.13727v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech\n  Corpus for Scaling Indian TTS", "abstract": "Recent advancements in text-to-speech (TTS) synthesis show that large-scale\nmodels trained with extensive web data produce highly natural-sounding output.\nHowever, such data is scarce for Indian languages due to the lack of\nhigh-quality, manually subtitled data on platforms like LibriVox or YouTube. To\naddress this gap, we enhance existing large-scale ASR datasets containing\nnatural conversations collected in low-quality environments to generate\nhigh-quality TTS training data. Our pipeline leverages the cross-lingual\ngeneralization of denoising and speech enhancement models trained on English\nand applied to Indian languages. This results in IndicVoices-R (IV-R), the\nlargest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704\nhours of high-quality speech from 10,496 speakers across 22 Indian languages.\nIV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,\nand IndicTTS. We also introduce the IV-R Benchmark, the first to assess\nzero-shot, few-shot, and many-shot speaker generalization capabilities of TTS\nmodels on Indian voices, ensuring diversity in age, gender, and style. We\ndemonstrate that fine-tuning an English pre-trained model on a combined dataset\nof high-quality IndicTTS and our IV-R dataset results in better zero-shot\nspeaker generalization compared to fine-tuning on the IndicTTS dataset alone.\nFurther, our evaluation reveals limited zero-shot generalization for Indian\nvoices in TTS models trained on prior datasets, which we improve by fine-tuning\nthe model on our data containing diverse set of speakers across language\nfamilies. We open-source all data and code, releasing the first TTS model for\nall 22 official Indian languages.", "published": "2024-09-09 06:28:47", "link": "http://arxiv.org/abs/2409.05356v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Harmonic Reasoning in Large Language Models", "abstract": "Large Language Models (LLMs) are becoming very popular and are used for many\ndifferent purposes, including creative tasks in the arts. However, these models\nsometimes have trouble with specific reasoning tasks, especially those that\ninvolve logical thinking and counting. This paper looks at how well LLMs\nunderstand and reason when dealing with musical tasks like figuring out notes\nfrom intervals and identifying chords and scales. We tested GPT-3.5 and GPT-4o\nto see how they handle these tasks. Our results show that while LLMs do well\nwith note intervals, they struggle with more complicated tasks like recognizing\nchords and scales. This points out clear limits in current LLM abilities and\nshows where we need to make them better, which could help improve how they\nthink and work in both artistic and other complex areas. We also provide an\nautomatically generated benchmark data set for the described tasks.", "published": "2024-09-09 11:28:02", "link": "http://arxiv.org/abs/2409.05521v1", "categories": ["cs.CL", "cs.AI", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Evaluation of real-time transcriptions using end-to-end ASR models", "abstract": "Automatic Speech Recognition (ASR) or Speech-to-text (STT) has greatly\nevolved in the last few years. Traditional architectures based on pipelines\nhave been replaced by joint end-to-end (E2E) architectures that simplify and\nstreamline the model training process. In addition, new AI training methods,\nsuch as weak-supervised learning have reduced the need for high-quality audio\ndatasets for model training. However, despite all these advancements, little to\nno research has been done on real-time transcription. In real-time scenarios,\nthe audio is not pre-recorded, and the input audio must be fragmented to be\nprocessed by the ASR systems. To achieve real-time requirements, these\nfragments must be as short as possible to reduce latency. However, audio cannot\nbe split at any point as dividing an utterance into two separate fragments will\ngenerate an incorrect transcription. Also, shorter fragments provide less\ncontext for the ASR model. For this reason, it is necessary to design and test\ndifferent splitting algorithms to optimize the quality and delay of the\nresulting transcription. In this paper, three audio splitting algorithms are\nevaluated with different ASR models to determine their impact on both the\nquality of the transcription and the end-to-end delay. The algorithms are\nfragmentation at fixed intervals, voice activity detection (VAD), and\nfragmentation with feedback. The results are compared to the performance of the\nsame model, without audio fragmentation, to determine the effects of this\ndivision. The results show that VAD fragmentation provides the best quality\nwith the highest delay, whereas fragmentation at fixed intervals provides the\nlowest quality and the lowest delay. The newly proposed feedback algorithm\nexchanges a 2-4% increase in WER for a reduction of 1.5-2s delay, respectively,\nto the VAD splitting.", "published": "2024-09-09 14:41:57", "link": "http://arxiv.org/abs/2409.05674v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Referring Expression Generation in Visually Grounded Dialogue with\n  Discourse-aware Comprehension Guiding", "abstract": "We propose an approach to referring expression generation (REG) in visually\ngrounded dialogue that is meant to produce referring expressions (REs) that are\nboth discriminative and discourse-appropriate. Our method constitutes a\ntwo-stage process. First, we model REG as a text- and image-conditioned\nnext-token prediction task. REs are autoregressively generated based on their\npreceding linguistic context and a visual representation of the referent.\nSecond, we propose the use of discourse-aware comprehension guiding as part of\na generate-and-rerank strategy through which candidate REs generated with our\nREG model are reranked based on their discourse-dependent discriminatory power.\nResults from our human evaluation indicate that our proposed two-stage approach\nis effective in producing discriminative REs, with higher performance in terms\nof text-image retrieval accuracy for reranked REs compared to those generated\nusing greedy decoding.", "published": "2024-09-09 15:33:07", "link": "http://arxiv.org/abs/2409.05721v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs", "abstract": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.", "published": "2024-09-09 17:11:51", "link": "http://arxiv.org/abs/2409.05806v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Pretraining Data Using Perplexity Correlations", "abstract": "Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier. We have now also updated this paper to\ninclude results from preregistered experiments with new pretraining data on an\naggregation of 22 benchmarks up to the 1.4B scale, showing increasing\nimprovements of our method over others with more scale. A pip package with full\ndocumentation can be found here:\nhttps://github.com/TristanThrush/perplexity-correlations.", "published": "2024-09-09 17:23:29", "link": "http://arxiv.org/abs/2409.05816v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Assessing SPARQL capabilities of Large Language Models", "abstract": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)\noffers significant synergistic potential for knowledge-driven applications. One\npossible integration is the interpretation and generation of formal languages,\nsuch as those used in the Semantic Web, with SPARQL being a core technology for\naccessing KGs. In this paper, we focus on measuring out-of-the box capabilities\nof LLMs to work with SPARQL and more specifically with SPARQL SELECT queries\napplying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for\nautomated execution and evaluation with several LLMs. The tasks assess\ncapabilities along the dimensions of syntax, semantic read, semantic create,\nand the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,\nand Claude models. Our findings indicate that working with SPARQL SELECT\nqueries is still challenging for LLMs and heavily depends on the specific LLM\nas well as the complexity of the task. While fixing basic syntax errors seems\nto pose no problems for the best of the current LLMs evaluated, creating\nsemantically correct SPARQL SELECT queries is difficult in several cases.", "published": "2024-09-09 08:29:39", "link": "http://arxiv.org/abs/2409.05925v2", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Improved Visually Prompted Keyword Localisation in Real Low-Resource\n  Settings", "abstract": "Given an image query, visually prompted keyword localisation (VPKL) aims to\nfind occurrences of the depicted word in a speech collection. This can be\nuseful when transcriptions are not available for a low-resource language (e.g.\nif it is unwritten). Previous work showed that VPKL can be performed with a\nvisually grounded speech model trained on paired images and unlabelled speech.\nBut all experiments were done on English. Moreover, transcriptions were used to\nget positive and negative pairs for the contrastive loss. This paper introduces\na few-shot learning scheme to mine pairs automatically without transcriptions.\nOn English, this results in only a small drop in performance. We also - for the\nfirst time - consider VPKL on a real low-resource language, Yoruba. While\nscores are reasonable, here we see a bigger drop in performance compared to\nusing ground truth pairs because the mining is less accurate in Yoruba.", "published": "2024-09-09 19:12:03", "link": "http://arxiv.org/abs/2409.06013v1", "categories": ["cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Investigating Causal Cues: Strengthening Spoofed Audio Detection with\n  Human-Discernible Linguistic Features", "abstract": "Several types of spoofed audio, such as mimicry, replay attacks, and\ndeepfakes, have created societal challenges to information integrity. Recently,\nresearchers have worked with sociolinguistics experts to label spoofed audio\nsamples with Expert Defined Linguistic Features (EDLFs) that can be discerned\nby the human ear: pitch, pause, word-initial and word-final release bursts of\nconsonant stops, audible intake or outtake of breath, and overall audio\nquality. It is established that there is an improvement in several deepfake\ndetection algorithms when they augmented the traditional and common features of\naudio data with these EDLFs. In this paper, using a hybrid dataset comprised of\nmultiple types of spoofed audio augmented with sociolinguistic annotations, we\ninvestigate causal discovery and inferences between the discernible linguistic\nfeatures and the label in the audio clips, comparing the findings of the causal\nmodels with the expert ground truth validation labeling process. Our findings\nsuggest that the causal models indicate the utility of incorporating linguistic\nfeatures to help discern spoofed audio, as well as the overall need and\nopportunity to incorporate human knowledge into models and techniques for\nstrengthening AI models. The causal discovery and inference can be used as a\nfoundation of training humans to discern spoofed audio as well as automating\nEDLFs labeling for the purpose of performance improvement of the common\nAI-based spoofed audio detectors.", "published": "2024-09-09 19:47:57", "link": "http://arxiv.org/abs/2409.06033v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated\n  Learning", "abstract": "Previous studies on federated learning (FL) often encounter performance\ndegradation due to data heterogeneity among different clients. In light of the\nrecent advances in multimodal large language models (MLLMs), such as GPT-4v and\nLLaVA, which demonstrate their exceptional proficiency in multimodal tasks,\nsuch as image captioning and multimodal question answering. We introduce a\nnovel federated learning framework, named Multimodal Large Language Model\nAssisted Federated Learning (MLLM-LLaVA-FL), which employs powerful MLLMs at\nthe server end to address the heterogeneous and long-tailed challenges. Owing\nto the advanced cross-modality representation capabilities and the extensive\nopen-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing\nthe extensive, yet previously underexploited, open-source data accessible from\nwebsites and powerful server-side computational resources. Hence, the\nMLLM-LLaVA-FL not only enhances the performance but also avoids increasing the\nrisk of privacy leakage and the computational burden on local devices,\ndistinguishing it from prior methodologies. Our framework has three key stages.\nInitially, we conduct global visual-text pretraining of the model. This\npretraining is facilitated by utilizing the extensive open-source data\navailable online, with the assistance of MLLMs. Subsequently, the pretrained\nmodel is distributed among various clients for local training. Finally, once\nthe locally trained models are transmitted back to the server, a global\nalignment is carried out under the supervision of MLLMs to further enhance the\nperformance. Experimental evaluations on established benchmarks, show that our\nframework delivers promising performance in the typical scenarios with data\nheterogeneity and long-tail distribution across different clients in FL.", "published": "2024-09-09 21:04:16", "link": "http://arxiv.org/abs/2409.06067v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Logically Consistent Language Models via Neuro-Symbolic Integration", "abstract": "Large language models (LLMs) are a promising venue for natural language\nunderstanding and generation. However, current LLMs are far from reliable: they\nare prone to generating non-factual information and, more crucially, to\ncontradicting themselves when prompted to reason about relations between\nentities of the world. These problems are currently addressed with large scale\nfine-tuning or by delegating reasoning to external tools. In this work, we\nstrive for a middle ground and introduce a loss based on neuro-symbolic\nreasoning that teaches an LLM to be logically consistent with an external set\nof facts and rules and improves self-consistency even when the LLM is\nfine-tuned on a limited set of facts. Our approach also allows to easily\ncombine multiple logical constraints at once in a principled way, delivering\nLLMs that are more consistent w.r.t. all constraints and improve over several\nbaselines w.r.t. a given constraint. Moreover, our method allows LLMs to\nextrapolate to unseen but semantically similar factual knowledge, represented\nin unseen datasets, more systematically.", "published": "2024-09-09 10:52:57", "link": "http://arxiv.org/abs/2409.13724v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identity-related Speech Suppression in Generative AI Content Moderation", "abstract": "Automated content moderation has long been used to help identify and filter\nundesired user-generated content online. Generative AI systems now use such\nfilters to keep undesired generated content from being created by or shown to\nusers. From classrooms to Hollywood, as generative AI is increasingly used for\ncreative or expressive text generation, whose stories will these technologies\nallow to be told, and whose will they suppress? In this paper, we define and\nintroduce measures of speech suppression, focusing on speech related to\ndifferent identity groups incorrectly filtered by a range of content moderation\nAPIs. Using both short-form, user-generated datasets traditional in content\nmoderation and longer generative AI-focused data, including two datasets we\nintroduce in this work, we create a benchmark for measurement of speech\nsuppression for nine identity groups. Across one traditional and four\ngenerative AI-focused automated content moderation services tested, we find\nthat identity-related speech is more likely to be incorrectly suppressed than\nother speech. We find differences in identity-related speech suppression for\ntraditional versus generative AI data, with APIs performing better on\ngenerative AI data but worse on longer text instances, and by identity, with\nidentity-specific reasons for incorrect flagging behavior. Overall, we find\nthat on traditional short-form data incorrectly suppressed speech is likely to\nbe political, while for generative AI creative data it is likely to be\ntelevision violence.", "published": "2024-09-09 14:34:51", "link": "http://arxiv.org/abs/2409.13725v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding\n  Asian-European Non-verbal Cultural Characteristics and their Influences on\n  Engagement", "abstract": "Non-verbal behavior is a central challenge in understanding the dynamics of a\nconversation and the affective states between interlocutors arising from the\ninteraction. Although psychological research has demonstrated that non-verbal\nbehaviors vary across cultures, limited computational analysis has been\nconducted to clarify these differences and assess their impact on engagement\nrecognition. To gain a greater understanding of engagement and non-verbal\nbehaviors among a wide range of cultures and language spheres, in this study we\nconduct a multilingual computational analysis of non-verbal features and\ninvestigate their role in engagement and engagement prediction. To achieve this\ngoal, we first expanded the NoXi dataset, which contains interaction data from\nparticipants living in France, Germany, and the United Kingdom, by collecting\nsession data of dyadic conversations in Japanese and Chinese, resulting in the\nenhanced dataset NoXi+J. Next, we extracted multimodal non-verbal features,\nincluding speech acoustics, facial expressions, backchanneling and gestures,\nvia various pattern recognition techniques and algorithms. Then, we conducted a\nstatistical analysis of listening behaviors and backchannel patterns to\nidentify culturally dependent and independent features in each language and\ncommon features among multiple languages. These features were also correlated\nwith the engagement shown by the interlocutors. Finally, we analyzed the\ninfluence of cultural differences in the input features of LSTM models trained\nto predict engagement for five language datasets. A SHAP analysis combined with\ntransfer learning confirmed a considerable correlation between the importance\nof input features for a language set and the significant cultural\ncharacteristics analyzed.", "published": "2024-09-09 18:37:34", "link": "http://arxiv.org/abs/2409.13726v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rule Extrapolation in Language Models: A Study of Compositional\n  Generalization on OOD Prompts", "abstract": "LLMs show remarkable emergent abilities, such as inferring concepts from\npresumably out-of-distribution prompts, known as in-context learning. Though\nthis success is often attributed to the Transformer architecture, our\nsystematic understanding is limited. In complex real-world data sets, even\ndefining what is out-of-distribution is not obvious. To better understand the\nOOD behaviour of autoregressive LLMs, we focus on formal languages, which are\ndefined by the intersection of rules. We define a new scenario of OOD\ncompositional generalization, termed rule extrapolation. Rule extrapolation\ndescribes OOD scenarios, where the prompt violates at least one rule. We\nevaluate rule extrapolation in formal languages with varying complexity in\nlinear and recurrent architectures, the Transformer, and state space models to\nunderstand the architectures' influence on rule extrapolation. We also lay the\nfirst stones of a normative theory of rule extrapolation, inspired by the\nSolomonoff prior in algorithmic information theory.", "published": "2024-09-09 22:36:35", "link": "http://arxiv.org/abs/2409.13728v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Revisiting the Solution of Meta KDD Cup 2024: CRAG", "abstract": "This paper presents the solution of our team APEX in the Meta KDD CUP 2024:\nCRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the\nlimitations of existing QA benchmarks in evaluating the diverse and dynamic\nchallenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a\nmore comprehensive assessment of RAG performance and contributes to advancing\nresearch in this field. We propose a routing-based domain and dynamic adaptive\nRAG pipeline, which performs specific processing for the diverse and dynamic\nnature of the question in all three stages: retrieval, augmentation, and\ngeneration. Our method achieved superior performance on CRAG and ranked 2nd for\nTask 2&3 on the final competition leaderboard. Our implementation is available\nat this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.", "published": "2024-09-09 07:28:14", "link": "http://arxiv.org/abs/2409.15337v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Efficient Training of Self-Supervised Speech Foundation Models on a\n  Compute Budget", "abstract": "Despite their impressive success, training foundation models remains\ncomputationally costly. This paper investigates how to efficiently train speech\nfoundation models with self-supervised learning (SSL) under a limited compute\nbudget. We examine critical factors in SSL that impact the budget, including\nmodel architecture, model size, and data size. Our goal is to make analytical\nsteps toward understanding the training dynamics of speech foundation models.\nWe benchmark SSL objectives in an entirely comparable setting and find that\nother factors contribute more significantly to the success of SSL. Our results\nshow that slimmer model architectures outperform common small architectures\nunder the same compute and parameter budget. We demonstrate that the size of\nthe pre-training data remains crucial, even with data augmentation during SSL\ntraining, as performance suffers when iterating over limited data. Finally, we\nidentify a trade-off between model size and data size, highlighting an optimal\nmodel size for a given compute budget.", "published": "2024-09-09 10:36:42", "link": "http://arxiv.org/abs/2409.16295v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram\n  Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis", "abstract": "The world is currently experiencing an outbreak of mpox, which has been\ndeclared a Public Health Emergency of International Concern by WHO. No prior\nwork related to social media mining has focused on the development of a dataset\nof Instagram posts about the mpox outbreak. The work presented in this paper\naims to address this research gap and makes two scientific contributions to\nthis field. First, it presents a multilingual dataset of 60,127 Instagram posts\nabout mpox, published between July 23, 2022, and September 5, 2024. The\ndataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram\nposts about mpox in 52 languages. For each of these posts, the Post ID, Post\nDescription, Date of publication, language, and translated version of the post\n(translation to English was performed using the Google Translate API) are\npresented as separate attributes in the dataset. After developing this dataset,\nsentiment analysis, hate speech detection, and anxiety or stress detection were\nperformed. This process included classifying each post into (i) one of the\nsentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or\nneutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no\nanxiety/stress detected. These results are presented as separate attributes in\nthe dataset. Second, this paper presents the results of performing sentiment\nanalysis, hate speech analysis, and anxiety or stress analysis. The variation\nof the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and\nneutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and\n50.64%, respectively. In terms of hate speech detection, 95.75% of the posts\ndid not contain hate and the remaining 4.25% of the posts contained hate.\nFinally, 72.05% of the posts did not indicate any anxiety/stress, and the\nremaining 27.95% of the posts represented some form of anxiety/stress.", "published": "2024-09-09 03:00:53", "link": "http://arxiv.org/abs/2409.05292v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.SI", "I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6"], "primary_category": "cs.LG"}
{"title": "SciAgents: Automating scientific discovery through multi-agent\n  intelligent graph reasoning", "abstract": "A key challenge in artificial intelligence is the creation of systems capable\nof autonomously advancing scientific understanding by exploring novel domains,\nidentifying complex patterns, and uncovering previously unseen connections in\nvast scientific data. In this work, we present SciAgents, an approach that\nleverages three core concepts: (1) the use of large-scale ontological knowledge\ngraphs to organize and interconnect diverse scientific concepts, (2) a suite of\nlarge language models (LLMs) and data retrieval tools, and (3) multi-agent\nsystems with in-situ learning capabilities. Applied to biologically inspired\nmaterials, SciAgents reveals hidden interdisciplinary relationships that were\npreviously considered unrelated, achieving a scale, precision, and exploratory\npower that surpasses traditional human-driven research methods. The framework\nautonomously generates and refines research hypotheses, elucidating underlying\nmechanisms, design principles, and unexpected material properties. By\nintegrating these capabilities in a modular fashion, the intelligent system\nyields material discoveries, critique and improve existing hypotheses, retrieve\nup-to-date data about existing research, and highlights their strengths and\nlimitations. Our case studies demonstrate scalable capabilities to combine\ngenerative AI, ontological representations, and multi-agent modeling,\nharnessing a `swarm of intelligence' similar to biological systems. This\nprovides new avenues for materials discovery and accelerates the development of\nadvanced materials by unlocking Nature's design principles.", "published": "2024-09-09 12:25:10", "link": "http://arxiv.org/abs/2409.05556v1", "categories": ["cs.AI", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "RIRAG: Regulatory Information Retrieval and Answer Generation", "abstract": "Regulatory documents, issued by governmental regulatory bodies, establish\nrules, guidelines, and standards that organizations must adhere to for legal\ncompliance. These documents, characterized by their length, complexity and\nfrequent updates, are challenging to interpret, requiring significant\nallocation of time and expertise on the part of organizations to ensure ongoing\ncompliance. Regulatory Natural Language Processing (RegNLP) is a\nmultidisciplinary field aimed at simplifying access to and interpretation of\nregulatory rules and obligations. We introduce a task of generating\nquestion-passages pairs, where questions are automatically created and paired\nwith relevant regulatory passages, facilitating the development of regulatory\nquestion-answering systems. We create the ObliQA dataset, containing 27,869\nquestions derived from the collection of Abu Dhabi Global Markets (ADGM)\nfinancial regulation documents, design a baseline Regulatory Information\nRetrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, a\nnovel evaluation metric that tests whether generated answers accurately capture\nall relevant obligations while avoiding contradictions.", "published": "2024-09-09 14:44:19", "link": "http://arxiv.org/abs/2409.05677v2", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.ET", "cs.IR"], "primary_category": "cs.CL"}
{"title": "OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System", "abstract": "Knowledge representation has been a central aim of AI since its inception.\nSymbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can\nboth represent knowledge. KGs provide highly accurate and explicit knowledge\nrepresentation, but face scalability issue; while LLMs offer expansive coverage\nof knowledge, but incur significant training costs and struggle with precise\nand reliable knowledge manipulation. To this end, we introduce OneEdit, a\nneural-symbolic prototype system for collaborative knowledge editing using\nnatural language, which facilitates easy-to-use knowledge management with KG\nand LLM. OneEdit consists of three modules: 1) The Interpreter serves for user\ninteraction with natural language; 2) The Controller manages editing requests\nfrom various users, leveraging the KG with rollbacks to handle knowledge\nconflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the\nknowledge from the Controller to edit KG and LLM. We conduct experiments on two\nnew datasets with KGs which demonstrate that OneEdit can achieve superior\nperformance.", "published": "2024-09-09 16:46:47", "link": "http://arxiv.org/abs/2409.07497v1", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Unveiling Induction Heads: Provable Training Dynamics and Feature\n  Learning in Transformers", "abstract": "In-context learning (ICL) is a cornerstone of large language model (LLM)\nfunctionality, yet its theoretical foundations remain elusive due to the\ncomplexity of transformer architectures. In particular, most existing work only\ntheoretically explains how the attention mechanism facilitates ICL under\ncertain data models. It remains unclear how the other building blocks of the\ntransformer contribute to ICL. To address this question, we study how a\ntwo-attention-layer transformer is trained to perform ICL on $n$-gram Markov\nchain data, where each token in the Markov chain statistically depends on the\nprevious $n$ tokens. We analyze a sophisticated transformer model featuring\nrelative positional embedding, multi-head softmax attention, and a feed-forward\nlayer with normalization. We prove that the gradient flow with respect to a\ncross-entropy ICL loss converges to a limiting model that performs a\ngeneralized version of the induction head mechanism with a learned feature,\nresulting from the congruous contribution of all the building blocks. In the\nlimiting model, the first attention layer acts as a $\\mathit{copier}$, copying\npast tokens within a given window to each position, and the feed-forward\nnetwork with normalization acts as a $\\mathit{selector}$ that generates a\nfeature vector by only looking at informationally relevant parents from the\nwindow. Finally, the second attention layer is a $\\mathit{classifier}$ that\ncompares these features with the feature at the output position, and uses the\nresulting similarity scores to generate the desired output. Our theory is\nfurther validated by experiments.", "published": "2024-09-09 18:10:26", "link": "http://arxiv.org/abs/2409.10559v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "NTT Multi-Speaker ASR System for the DASR Task of CHiME-8 Challenge", "abstract": "We present a distant automatic speech recognition (DASR) system developed for\nthe CHiME-8 DASR track. It consists of a diarization first pipeline. For\ndiarization, we use end-to-end diarization with vector clustering (EEND-VC)\nfollowed by target speaker voice activity detection (TS-VAD) refinement. To\ndeal with various numbers of speakers, we developed a new multi-channel speaker\ncounting approach. We then apply guided source separation (GSS) with several\nimprovements to the baseline system. Finally, we perform ASR using a\ncombination of systems built from strong pre-trained models. Our proposed\nsystem achieves a macro tcpWER of 21.3 % on the dev set, which is a 57 %\nrelative improvement over the baseline.", "published": "2024-09-09 12:21:42", "link": "http://arxiv.org/abs/2409.05554v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Leveraging Content and Acoustic Representations for Speech Emotion\n  Recognition", "abstract": "Speech emotion recognition (SER), the task of identifying the expression of\nemotion from spoken content, is challenging due to the difficulty in extracting\nrepresentations that capture emotional attributes from speech. The scarcity of\nlabeled datasets further complicates the challenge where large models are prone\nto over-fitting. In this paper, we propose CARE (Content and Acoustic\nRepresentations of Emotions), where we design a dual encoding scheme which\nemphasizes semantic and acoustic factors of speech. While the semantic encoder\nis trained using distillation from utterance-level text representations, the\nacoustic encoder is trained to predict low-level frame-wise features of the\nspeech signal. The proposed dual encoding scheme is a base-sized model trained\nonly on unsupervised raw speech. With a simple light-weight classification\nmodel trained on the downstream task, we show that the CARE embeddings provide\neffective emotion recognition on a variety of datasets. We compare the proposal\nwith several other self-supervised models as well as recent large-language\nmodel based approaches. In these evaluations, the proposed CARE is shown to be\nthe best performing model based on average performance across 8 diverse\ndatasets. We also conduct several ablation studies to analyze the importance of\nvarious design choices.", "published": "2024-09-09 12:46:32", "link": "http://arxiv.org/abs/2409.05566v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An investigation of modularity for noise robustness in conformer-based\n  ASR", "abstract": "Whilst state of the art automatic speech recognition (ASR) can perform well,\nit still degrades when exposed to acoustic environments that differ from those\nused when training the model. Unfamiliar environments for a given model may\nwell be known a-priori, but yield comparatively small amounts of adaptation\ndata. In this experimental study, we investigate to what extent recent\nformalisations of modularity can aid adaptation of ASR to new acoustic\nenvironments. Using a conformer based model and fixed routing, we confirm that\nenvironment awareness can indeed lead to improved performance in known\nenvironments. However, at least on the (CHIME) datasets in the study, it is\ndifficult for a classifier module to distinguish different noisy environments,\na simpler distinction between noisy and clean speech being the optimal\nconfiguration. The results have clear implications for deploying large models\nin particular environments with or without a-priori knowledge of the\nenvironmental noise.", "published": "2024-09-09 13:19:23", "link": "http://arxiv.org/abs/2409.05589v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AS-Speech: Adaptive Style For Speech Synthesis", "abstract": "In recent years, there has been significant progress in Text-to-Speech (TTS)\nsynthesis technology, enabling the high-quality synthesis of voices in common\nscenarios. In unseen situations, adaptive TTS requires a strong generalization\ncapability to speaker style characteristics. However, the existing adaptive\nmethods can only extract and integrate coarse-grained timbre or mixed rhythm\nattributes separately. In this paper, we propose AS-Speech, an adaptive style\nmethodology that integrates the speaker timbre characteristics and rhythmic\nattributes into a unified framework for text-to-speech synthesis. Specifically,\nAS-Speech can accurately simulate style characteristics through fine-grained\ntext-based timbre features and global rhythm information, and achieve\nhigh-fidelity speech synthesis through the diffusion model. Experiments show\nthat the proposed model produces voices with higher naturalness and similarity\nin terms of timbre and rhythm compared to a series of adaptive TTS models.", "published": "2024-09-09 15:41:38", "link": "http://arxiv.org/abs/2409.05730v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec", "abstract": "We present BigCodec, a low-bitrate neural speech codec. While recent neural\nspeech codecs have shown impressive progress, their performance significantly\ndeteriorates at low bitrates (around 1 kbps). Although a low bitrate inherently\nrestricts performance, other factors, such as model capacity, also hinder\nfurther improvements. To address this problem, we scale up the model size to\n159M parameters that is more than 10 times larger than popular codecs with\nabout 10M parameters. Besides, we integrate sequential models into traditional\nconvolutional architectures to better capture temporal dependency and adopt\nlow-dimensional vector quantization to ensure a high code utilization.\nComprehensive objective and subjective evaluations show that BigCodec, with a\nbitrate of 1.04 kbps, significantly outperforms several existing low-bitrate\ncodecs. Furthermore, BigCodec achieves objective performance comparable to\npopular codecs operating at 4-6 times higher bitrates, and even delivers better\nsubjective perceptual quality than the ground truth.", "published": "2024-09-09 07:18:07", "link": "http://arxiv.org/abs/2409.05377v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Findings of the 2024 Mandarin Stuttering Event Detection and Automatic\n  Speech Recognition Challenge", "abstract": "The StutteringSpeech Challenge focuses on advancing speech technologies for\npeople who stutter, specifically targeting Stuttering Event Detection (SED) and\nAutomatic Speech Recognition (ASR) in Mandarin. The challenge comprises three\ntracks: (1) SED, which aims to develop systems for detection of stuttering\nevents; (2) ASR, which focuses on creating robust systems for recognizing\nstuttered speech; and (3) Research track for innovative approaches utilizing\nthe provided dataset. We utilizes an open-source Mandarin stuttering dataset\nAS-70, which has been split into new training and test sets for the challenge.\nThis paper presents the dataset, details the challenge tracks, and analyzes the\nperformance of the top systems, highlighting improvements in detection accuracy\nand reductions in recognition error rates. Our findings underscore the\npotential of specialized models and augmentation strategies in developing\nstuttered speech technologies.", "published": "2024-09-09 08:30:26", "link": "http://arxiv.org/abs/2409.05430v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transferable Selective Virtual Sensing Active Noise Control Technique\n  Based on Metric Learning", "abstract": "Virtual sensing (VS) technology enables active noise control (ANC) systems to\nattenuate noise at virtual locations distant from the physical error\nmicrophones. Appropriate auxiliary filters (AF) can significantly enhance the\neffectiveness of VS approaches. The selection of appropriate AF for various\ntypes of noise can be automatically achieved using convolutional neural\nnetworks (CNNs). However, training the CNN model for different ANC systems is\noften labour-intensive and time-consuming. To tackle this problem, we propose a\nnovel method, Transferable Selective VS, by integrating metric-learning\ntechnology into CNN-based VS approaches. The Transferable Selective VS method\nallows a pre-trained CNN to be applied directly to new ANC systems without\nrequiring retraining, and it can handle unseen noise types. Numerical\nsimulations demonstrate the effectiveness of the proposed method in attenuating\nsudden-varying broadband noises and real-world noises.", "published": "2024-09-09 09:56:22", "link": "http://arxiv.org/abs/2409.05470v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "A Toolkit for Joint Speaker Diarization and Identification with\n  Application to Speaker-Attributed ASR", "abstract": "We present a modular toolkit to perform joint speaker diarization and speaker\nidentification. The toolkit can leverage on multiple models and algorithms\nwhich are defined in a configuration file. Such flexibility allows our system\nto work properly in various conditions (e.g., multiple registered speakers'\nsets, acoustic conditions and languages) and across application domains (e.g.\nmedia monitoring, institutional, speech analytics). In this demonstration we\nshow a practical use-case in which speaker-related information is used jointly\nwith automatic speech recognition engines to generate speaker-attributed\ntranscriptions. To achieve that, we employ a user-friendly web-based interface\nto process audio and video inputs with the chosen configuration.", "published": "2024-09-09 16:05:40", "link": "http://arxiv.org/abs/2409.05750v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Vector Quantized Diffusion Model Based Speech Bandwidth Extension", "abstract": "Recent advancements in neural audio codec (NAC) unlock new potential in audio\nsignal processing. Studies have increasingly explored leveraging the latent\nfeatures of NAC for various speech signal processing tasks. This paper\nintroduces the first approach to speech bandwidth extension (BWE) that utilizes\nthe discrete features obtained from NAC. By restoring high-frequency details\nwithin highly compressed discrete tokens, this approach enhances speech\nintelligibility and naturalness. Based on Vector Quantized Diffusion, the\nproposed framework combines the strengths of advanced NAC, diffusion models,\nand Mamba-2 to reconstruct high-frequency speech components. Extensive\nexperiments demonstrate that this method exhibits superior performance across\nboth log-spectral distance and ViSQOL, significantly improving speech quality.", "published": "2024-09-09 16:46:54", "link": "http://arxiv.org/abs/2409.05784v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continuous Learning of Transformer-based Audio Deepfake Detection", "abstract": "This paper proposes a novel framework for audio deepfake detection with two\nmain objectives: i) attaining the highest possible accuracy on available fake\ndata, and ii) effectively performing continuous learning on new fake data in a\nfew-shot learning manner. Specifically, we conduct a large audio deepfake\ncollection using various deep audio generation methods. The data is further\nenhanced with additional augmentation methods to increase variations amidst\ncompressions, far-field recordings, noise, and other distortions. We then adopt\nthe Audio Spectrogram Transformer for the audio deepfake detection model.\nAccordingly, the proposed method achieves promising performance on various\nbenchmark datasets. Furthermore, we present a continuous learning plugin module\nto update the trained model most effectively with the fewest possible labeled\ndata points of the new fake type. The proposed method outperforms the\nconventional direct fine-tuning approach with much fewer labeled data points.", "published": "2024-09-09 08:28:09", "link": "http://arxiv.org/abs/2409.05924v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Musical Chords: A Novel Java Algorithm and App Utility to Enumerate\n  Chord-Progressions Adhering to Music Theory Guidelines", "abstract": "A song's backbone is its chord progressions, a series of chords that improve\nthe harmony and add to the overall composition. For individuals ranging from\nbeginners to creative artists, comprehending and implementing music theory\ngrammar for their own compositions can stifle the music creation process and\ncause song-writer's block. The existing Chord Progression approaches in the\nmarketplace are limited on producing only pre-selected progressions and often\nfail to conform to music theory guidelines or provide APIs for other musicians\nto build on. Because four-chord and eight-chord progressions are yet to be\nenumerated, Machine learning use-cases that train on chord progressions are\nlimited, and mobile applications don't provide users with unique or unexplored\nprogressions. To address these limitations, a novel Java Algorithm and\nautomated music theory chord progression and variations generator App has been\ndeveloped. This App offers a piano user interface, that applies music theory to\ngenerate all possible four-chord and eight-chord progressions and produces\nthree alternate variations of the generated progressions selected by the user.\nThe Algorithm elucidates 3,297 Total 4-Chord Progressions and 405,216 Total\n8-Chord Progressions. Within the 4-Chord Progression pool, there are 1,533\nMajor 4-chord Progressions and 1,764 Minor 4-Chord Progressions. Within the\n8-chord Progression pool, there are 182,094 Major Progressions and 223,122\nMinor Progressions. This innovative approach provides musicians with a\ncomprehensive and customizable tool for their music creation, allowing them to\ndevelop their signature sounds.", "published": "2024-09-09 19:26:45", "link": "http://arxiv.org/abs/2409.06024v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Retrieval Augmented Correction of Named Entity Speech Recognition Errors", "abstract": "In recent years, end-to-end automatic speech recognition (ASR) systems have\nproven themselves remarkably accurate and performant, but these systems still\nhave a significant error rate for entity names which appear infrequently in\ntheir training data. In parallel to the rise of end-to-end ASR systems, large\nlanguage models (LLMs) have proven to be a versatile tool for various natural\nlanguage processing (NLP) tasks. In NLP tasks where a database of relevant\nknowledge is available, retrieval augmented generation (RAG) has achieved\nimpressive results when used with LLMs. In this work, we propose a RAG-like\ntechnique for correcting speech recognition entity name errors. Our approach\nuses a vector database to index a set of relevant entities. At runtime,\ndatabase queries are generated from possibly errorful textual ASR hypotheses,\nand the entities retrieved using these queries are fed, along with the ASR\nhypotheses, to an LLM which has been adapted to correct ASR errors. Overall,\nour best system achieves 33%-39% relative word error rate reductions on\nsynthetic test sets focused on voice assistant queries of rare music entities\nwithout regressing on the STOP test set, a publicly available voice assistant\ntest set covering many domains.", "published": "2024-09-09 20:52:25", "link": "http://arxiv.org/abs/2409.06062v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Speaker Diarization: Current Databases, Approaches and\n  Challenges", "abstract": "Nowadays, the large amount of audio-visual content available has fostered the\nneed to develop new robust automatic speaker diarization systems to analyse and\ncharacterise it. This kind of system helps to reduce the cost of doing this\nprocess manually and allows the use of the speaker information for different\napplications, as a huge quantity of information is present, for example, images\nof faces, or audio recordings. Therefore, this paper aims to address a critical\narea in the field of speaker diarization systems, the integration of\naudio-visual content of different domains. This paper seeks to push beyond\ncurrent state-of-the-art practices by developing a robust audio-visual speaker\ndiarization framework adaptable to various data domains, including TV\nscenarios, meetings, and daily activities. Unlike most of the existing\naudio-visual speaker diarization systems, this framework will also include the\nproposal of an approach to lead the precise assignment of specific identities\nin TV scenarios where celebrities appear. In addition, in this work, we have\nconducted an extensive compilation of the current state-of-the-art approaches\nand the existing databases for developing audio-visual speaker diarization.", "published": "2024-09-09 14:29:22", "link": "http://arxiv.org/abs/2409.05659v1", "categories": ["cs.SD", "cs.MM", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "SongCreator: Lyrics-based Universal Song Generation", "abstract": "Music is an integral part of human culture, embodying human intelligence and\ncreativity, of which songs compose an essential part. While various aspects of\nsong generation have been explored by previous works, such as singing voice,\nvocal composition and instrumental arrangement, etc., generating songs with\nboth vocals and accompaniment given lyrics remains a significant challenge,\nhindering the application of music generation models in the real world. In this\nlight, we propose SongCreator, a song-generation system designed to tackle this\nchallenge. The model features two novel designs: a meticulously designed\ndual-sequence language model (DSLM) to capture the information of vocals and\naccompaniment for song generation, and a series of attention mask strategies\nfor DSLM, which allows our model to understand, generate and edit songs, making\nit suitable for various songrelated generation tasks by utilizing specific\nattention masks. Extensive experiments demonstrate the effectiveness of\nSongCreator by achieving state-of-the-art or competitive performances on all\neight tasks. Notably, it surpasses previous works by a large margin in\nlyrics-to-song and lyrics-to-vocals. Additionally, it is able to independently\ncontrol the acoustic conditions of the vocals and accompaniment in the\ngenerated song through different audio prompts, exhibiting its potential\napplicability. Our samples are available at\nhttps://thuhcsi.github.io/SongCreator/.", "published": "2024-09-09 19:37:07", "link": "http://arxiv.org/abs/2409.06029v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer", "abstract": "Music timbre transfer is a challenging task that involves modifying the\ntimbral characteristics of an audio signal while preserving its melodic\nstructure. In this paper, we propose a novel method based on dual diffusion\nbridges, trained using the CocoChorales Dataset, which consists of unpaired\nmonophonic single-instrument audio data. Each diffusion model is trained on a\nspecific instrument with a Gaussian prior. During inference, a model is\ndesignated as the source model to map the input audio to its corresponding\nGaussian prior, and another model is designated as the target model to\nreconstruct the target audio from this Gaussian prior, thereby facilitating\ntimbre transfer. We compare our approach against existing unsupervised timbre\ntransfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental\nresults demonstrate that our method achieves both better Fr\\'echet Audio\nDistance (FAD) and melody preservation, as reflected by lower pitch distances\n(DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise\nlevel from the Gaussian prior, $\\sigma$, can be adjusted to control the degree\nof melody preservation and amount of timbre transferred.", "published": "2024-09-09 22:16:48", "link": "http://arxiv.org/abs/2409.06096v4", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
