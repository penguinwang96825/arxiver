{"title": "Massively Parallel Cross-Lingual Learning in Low-Resource Target\n  Language Translation", "abstract": "We work on translation from rich-resource languages to low-resource\nlanguages. The main challenges we identify are the lack of low-resource\nlanguage data, effective methods for cross-lingual transfer, and the\nvariable-binding problem that is common in neural systems. We build a\ntranslation system that addresses these challenges using eight European\nlanguage families as our test ground. Firstly, we add the source and the target\nfamily labels and study intra-family and inter-family influences for effective\ncross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for\nEnglish-Swedish translation using eight families compared to the single-family\nmulti-source multi-target baseline. Moreover, we find that training on two\nneighboring families closest to the low-resource language is often enough.\nSecondly, we construct an ablation study and find that reasonably good results\ncan be achieved even with considerably less target data. Thirdly, we address\nthe variable-binding problem by building an order-preserving named entity\ntranslation model. We obtain 60.6% accuracy in qualitative evaluation where our\ntranslations are akin to human translations in a preliminary study.", "published": "2018-04-21 02:18:36", "link": "http://arxiv.org/abs/1804.07878v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Extraction with Generative Adversarial Imitation Learning", "abstract": "We propose a new method for event extraction (EE) task based on an imitation\nlearning framework, specifically, inverse reinforcement learning (IRL) via\ngenerative adversarial network (GAN). The GAN estimates proper rewards\naccording to the difference between the actions committed by the expert (or\nground truth) and the agent among complicated states in the environment. EE\ntask benefits from these dynamic rewards because instances and labels yield to\nvarious extents of difficulty and the gains are expected to be diverse -- e.g.,\nan ambiguous but correctly detected trigger or argument should receive high\ngains -- while the traditional RL models usually neglect such differences and\npay equal attention on all instances. Moreover, our experiments also\ndemonstrate that the proposed framework outperforms state-of-the-art methods,\nwithout explicit feature engineering.", "published": "2018-04-21 02:43:00", "link": "http://arxiv.org/abs/1804.07881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stochastic Answer Networks for Natural Language Inference", "abstract": "We propose a stochastic answer network (SAN) to explore multi-step inference\nstrategies in Natural Language Inference. Rather than directly predicting the\nresults given the inputs, the model maintains a state and iteratively refines\nits predictions. Our experiments show that SAN achieves the state-of-the-art\nresults on three benchmarks: Stanford Natural Language Inference (SNLI)\ndataset, MultiGenre Natural Language Inference (MultiNLI) dataset and Quora\nQuestion Pairs dataset.", "published": "2018-04-21 04:31:59", "link": "http://arxiv.org/abs/1804.07888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-aware Image Caption Generation", "abstract": "Current image captioning approaches generate descriptions which lack specific\ninformation, such as named entities that are involved in the images. In this\npaper we propose a new task which aims to generate informative image captions,\ngiven images and hashtags as input. We propose a simple but effective approach\nto tackle this problem. We first train a convolutional neural networks - long\nshort term memory networks (CNN-LSTM) model to generate a template caption\nbased on the input image. Then we use a knowledge graph based collective\ninference algorithm to fill in the template with specific named entities\nretrieved via the hashtags. Experiments on a new benchmark dataset collected\nfrom Flickr show that our model generates news-style image descriptions with\nmuch richer information. Our model outperforms unimodal baselines significantly\nwith various evaluation metrics.", "published": "2018-04-21 04:40:10", "link": "http://arxiv.org/abs/1804.07889v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taylor's law for Human Linguistic Sequences", "abstract": "Taylor's law describes the fluctuation characteristics underlying a system in\nwhich the variance of an event within a time span grows by a power law with\nrespect to the mean. Although Taylor's law has been applied in many natural and\nsocial systems, its application for language has been scarce. This article\ndescribes a new quantification of Taylor's law in natural language and reports\nan analysis of over 1100 texts across 14 languages. The Taylor exponents of\nwritten natural language texts were found to exhibit almost the same value. The\nexponent was also compared for other language-related data, such as the\nchild-directed speech, music, and programming language code. The results show\nhow the Taylor exponent serves to quantify the fundamental structural\ncomplexity underlying linguistic time series. The article also shows the\napplicability of these findings in evaluating language models.", "published": "2018-04-21 05:24:10", "link": "http://arxiv.org/abs/1804.07893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Natural Language Generation with Denoising Autoencoders", "abstract": "Generating text from structured data is important for various tasks such as\nquestion answering and dialog systems. We show that in at least one domain,\nwithout any supervision and only based on unlabeled text, we are able to build\na Natural Language Generation (NLG) system with higher performance than\nsupervised approaches. In our approach, we interpret the structured data as a\ncorrupt representation of the desired output and use a denoising auto-encoder\nto reconstruct the sentence. We show how to introduce noise into training\nexamples that do not contain structured data, and that the resulting denoising\nauto-encoder generalizes to generate correct sentences when given structured\ndata.", "published": "2018-04-21 06:16:57", "link": "http://arxiv.org/abs/1804.07899v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task Learning for Universal Sentence Embeddings: A Thorough\n  Evaluation using Transfer and Auxiliary Tasks", "abstract": "Learning distributed sentence representations is one of the key challenges in\nnatural language processing. Previous work demonstrated that a recurrent neural\nnetwork (RNNs) based sentence encoder trained on a large collection of\nannotated natural language inference data, is efficient in the transfer\nlearning to facilitate other related tasks. In this paper, we show that joint\nlearning of multiple tasks results in better generalizable sentence\nrepresentations by conducting extensive experiments and analysis comparing the\nmulti-task and single-task learned sentence encoders. The quantitative analysis\nusing auxiliary tasks show that multi-task learning helps to embed better\nsemantic information in the sentence representations compared to single-task\nlearning. In addition, we compare multi-task sentence encoders with\ncontextualized word representations and show that combining both of them can\nfurther boost the performance of transfer learning.", "published": "2018-04-21 07:53:07", "link": "http://arxiv.org/abs/1804.07911v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Stable and Effective Learning Strategy for Trainable Greedy Decoding", "abstract": "Beam search is a widely used approximate search strategy for neural network\ndecoders, and it generally outperforms simple greedy decoding on tasks like\nmachine translation. However, this improvement comes at substantial\ncomputational cost. In this paper, we propose a flexible new method that allows\nus to reap nearly the full benefits of beam search with nearly no additional\ncomputational cost. The method revolves around a small neural network actor\nthat is trained to observe and manipulate the hidden state of a\npreviously-trained decoder. To train this actor network, we introduce the use\nof a pseudo-parallel corpus built using the output of beam search on a base\nmodel, ranked by a target quality metric like BLEU. Our method is inspired by\nearlier work on this problem, but requires no reinforcement learning, and can\nbe trained reliably on a range of models. Experiments on three parallel corpora\nand three architectures show that the method yields substantial improvements in\ntranslation quality and speed over each base system.", "published": "2018-04-21 08:14:58", "link": "http://arxiv.org/abs/1804.07915v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading\n  Comprehension", "abstract": "We propose DuoRC, a novel dataset for Reading Comprehension (RC) that\nmotivates several new challenges for neural approaches in language\nunderstanding beyond those offered by existing RC datasets. DuoRC contains\n186,089 unique question-answer pairs created from a collection of 7680 pairs of\nmovie plots where each pair in the collection reflects two versions of the same\nmovie - one from Wikipedia and the other from IMDb - written by two different\nauthors. We asked crowdsourced workers to create questions from one version of\nthe plot and a different set of workers to extract or synthesize answers from\nthe other version. This unique characteristic of DuoRC where questions and\nanswers are created from different versions of a document narrating the same\nunderlying story, ensures by design, that there is very little lexical overlap\nbetween the questions created from one version and the segments containing the\nanswer in the other version. Further, since the two versions have different\nlevels of plot detail, narration style, vocabulary, etc., answering questions\nfrom the second version requires deeper language understanding and\nincorporating external background knowledge. Additionally, the narrative style\nof passages arising from movie plots (as opposed to typical descriptive\npassages in existing datasets) exhibits the need to perform complex reasoning\nover events across multiple sentences. Indeed, we observe that state-of-the-art\nneural RC models which have achieved near human performance on the SQuAD\ndataset, even when coupled with traditional NLP techniques to address the\nchallenges presented in DuoRC exhibit very poor performance (F1 score of 37.42%\non DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research\navenues wherein DuoRC could complement other RC datasets to explore novel\nneural approaches for studying language understanding.", "published": "2018-04-21 09:43:06", "link": "http://arxiv.org/abs/1804.07927v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Stock Question Answering", "abstract": "We study the problem of stock related question answering (StockQA):\nautomatically generating answers to stock related questions, just like\nprofessional stock analysts providing action recommendations to stocks upon\nuser's requests. StockQA is quite different from previous QA tasks since (1)\nthe answers in StockQA are natural language sentences (rather than entities or\nvalues) and due to the dynamic nature of StockQA, it is scarcely possible to\nget reasonable answers in an extractive way from the training data; and (2)\nStockQA requires properly analyzing the relationship between keywords in QA\npair and the numerical features of a stock. We propose to address the problem\nwith a memory-augmented encoder-decoder architecture, and integrate different\nmechanisms of number understanding and generation, which is a critical\ncomponent of StockQA.\n  We build a large-scale dataset containing over 180K StockQA instances, based\non which various technique combinations are extensively studied and compared.\nExperimental results show that a hybrid word-character model with separate\ncharacter components for number processing, achieves the best performance. By\nanalyzing the results, we found that 44.8% of answers generated by our best\nmodel still suffer from the generic answer problem, which can be alleviated by\na straightforward hybrid retrieval-generation model.", "published": "2018-04-21 10:59:06", "link": "http://arxiv.org/abs/1804.07942v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated essay scoring with string kernels and word embeddings", "abstract": "In this work, we present an approach based on combining string kernels and\nword embeddings for automatic essay scoring. String kernels capture the\nsimilarity among strings based on counting common character n-grams, which are\na low-level yet powerful type of feature, demonstrating state-of-the-art\nresults in various text classification tasks such as Arabic dialect\nidentification or native language identification. To our best knowledge, we are\nthe first to apply string kernels to automatically score essays. We are also\nthe first to combine them with a high-level semantic feature representation,\nnamely the bag-of-super-word-embeddings. We report the best performance on the\nAutomated Student Assessment Prize data set, in both in-domain and cross-domain\nsettings, surpassing recent state-of-the-art deep learning approaches.", "published": "2018-04-21 12:26:29", "link": "http://arxiv.org/abs/1804.07954v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faster Shift-Reduce Constituent Parsing with a Non-Binary, Bottom-Up\n  Strategy", "abstract": "An increasingly wide range of artificial intelligence applications rely on\nsyntactic information to process and extract meaning from natural language text\nor speech, with constituent trees being one of the most widely used syntactic\nformalisms. To produce these phrase-structure representations from sentences in\nnatural language, shift-reduce constituent parsers have become one of the most\nefficient approaches. Increasing their accuracy and speed is still one of the\nmain objectives pursued by the research community so that artificial\nintelligence applications that make use of parsing outputs, such as machine\ntranslation or voice assistant services, can improve their performance. With\nthis goal in mind, we propose in this article a novel non-binary shift-reduce\nalgorithm for constituent parsing. Our parser follows a classical bottom-up\nstrategy but, unlike others, it straightforwardly creates non-binary branchings\nwith just one Reduce transition, instead of requiring prior binarization or a\nsequence of binary transitions, allowing its direct application to any language\nwithout the need of further resources such as percolation tables. As a result,\nit uses fewer transitions per sentence than existing transition-based\nconstituent parsers, becoming the fastest such system and, as a consequence,\nspeeding up downstream applications. Using static oracle training and greedy\nsearch, the accuracy of this novel approach is on par with state-of-the-art\ntransition-based constituent parsers and outperforms all top-down and bottom-up\ngreedy shift-reduce systems on the Wall Street Journal section from the English\nPenn Treebank and the Penn Chinese Treebank. Additionally, we develop a dynamic\noracle for training the proposed transition-based algorithm, achieving further\nimprovements in both benchmarks and obtaining the best accuracy to date on the\nPenn Chinese Treebank among greedy shift-reduce parsers.", "published": "2018-04-21 13:01:55", "link": "http://arxiv.org/abs/1804.07961v3", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Eval all, trust a few, do wrong to none: Comparing sentence generation\n  models", "abstract": "In this paper, we study recent neural generative models for text generation\nrelated to variational autoencoders. Previous works have employed various\ntechniques to control the prior distribution of the latent codes in these\nmodels, which is important for sampling performance, but little attention has\nbeen paid to reconstruction error. In our study, we follow a rigorous\nevaluation protocol using a large set of previously used and novel automatic\nand human evaluation metrics, applied to both generated samples and\nreconstructions. We hope that it will become the new evaluation standard when\ncomparing neural generative models for text.", "published": "2018-04-21 14:29:39", "link": "http://arxiv.org/abs/1804.07972v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural-Davidsonian Semantic Proto-role Labeling", "abstract": "We present a model for semantic proto-role labeling (SPRL) using an adapted\nbidirectional LSTM encoding strategy that we call \"Neural-Davidsonian\":\npredicate-argument structure is represented as pairs of hidden states\ncorresponding to predicate and argument head tokens of the input sequence. We\ndemonstrate: (1) state-of-the-art results in SPRL, and (2) that our network\nnaturally shares parameters between attributes, allowing for learning new\nattribute types with limited added supervision.", "published": "2018-04-21 14:48:56", "link": "http://arxiv.org/abs/1804.07976v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Meta-Embeddings for Improved Sentence Representations", "abstract": "While one of the first steps in many NLP systems is selecting what\npre-trained word embeddings to use, we argue that such a step is better left\nfor neural networks to figure out by themselves. To that end, we introduce\ndynamic meta-embeddings, a simple yet effective method for the supervised\nlearning of embedding ensembles, which leads to state-of-the-art performance\nwithin the same model class on a variety of tasks. We subsequently show how the\ntechnique can be used to shed new light on the usage of word embeddings in NLP\nsystems.", "published": "2018-04-21 15:32:32", "link": "http://arxiv.org/abs/1804.07983v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Natural Language Adversarial Examples", "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples,\nperturbations to correctly classified examples which can cause the model to\nmisclassify. In the image domain, these perturbations are often virtually\nindistinguishable to human perception, causing humans and state-of-the-art\nmodels to disagree. However, in the natural language domain, small\nperturbations are clearly perceptible, and the replacement of a single word can\ndrastically alter the semantics of the document. Given these challenges, we use\na black-box population-based optimization algorithm to generate semantically\nand syntactically similar adversarial examples that fool well-trained sentiment\nanalysis and textual entailment models with success rates of 97% and 70%,\nrespectively. We additionally demonstrate that 92.3% of the successful\nsentiment analysis adversarial examples are classified to their original label\nby 20 human annotators, and that the examples are perceptibly quite similar.\nFinally, we discuss an attempt to use adversarial training as a defense, but\nfail to yield improvement, demonstrating the strength and diversity of our\nadversarial examples. We hope our findings encourage researchers to pursue\nimproving the robustness of DNNs in the natural language domain.", "published": "2018-04-21 17:02:20", "link": "http://arxiv.org/abs/1804.07998v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Entity Typing through Increased Discourse Context and\n  Adaptive Classification Thresholds", "abstract": "Fine-grained entity typing is the task of assigning fine-grained semantic\ntypes to entity mentions. We propose a neural architecture which learns a\ndistributional semantic representation that leverages a greater amount of\nsemantic context -- both document and sentence level information -- than prior\nwork. We find that additional context improves performance, with further\nimprovements gained by utilizing adaptive classification thresholds.\nExperiments show that our approach without reliance on hand-crafted features\nachieves the state-of-the-art results on three benchmark datasets.", "published": "2018-04-21 17:21:28", "link": "http://arxiv.org/abs/1804.08000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Stance Detection and Fact Checking in a Unified Corpus", "abstract": "A reasonable approach for fact checking a claim involves retrieving\npotentially relevant documents from different sources (e.g., news websites,\nsocial media, etc.), determining the stance of each document with respect to\nthe claim, and finally making a prediction about the claim's factuality by\naggregating the strength of the stances, while taking the reliability of the\nsource into account. Moreover, a fact checking system should be able to explain\nits decision by providing relevant extracts (rationales) from the documents.\nYet, this setup is not directly supported by existing datasets, which treat\nfact checking, document retrieval, source credibility, stance detection and\nrationale extraction as independent tasks. In this paper, we support the\ninterdependencies between these tasks as annotations in the same corpus. We\nimplement this setup on an Arabic fact checking corpus, the first of its kind.", "published": "2018-04-21 19:18:22", "link": "http://arxiv.org/abs/1804.08012v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Semantic Parsing", "abstract": "We introduce the task of cross-lingual semantic parsing: mapping content\nprovided in a source language into a meaning representation based on a target\nlanguage. We present: (1) a meaning representation designed to allow systems to\ntarget varying levels of structural complexity (shallow to deep analysis), (2)\nan evaluation metric to measure the similarity between system output and\nreference meaning representations, (3) an end-to-end model with a novel copy\nmechanism that supports intrasentential coreference, and (4) an evaluation\ndataset where experiments show our model outperforms strong baselines by at\nleast 1.18 F1 score.", "published": "2018-04-21 22:15:36", "link": "http://arxiv.org/abs/1804.08037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-lingual Common Semantic Space Construction via Cluster-consistent\n  Word Embedding", "abstract": "We construct a multilingual common semantic space based on distributional\nsemantics, where words from multiple languages are projected into a shared\nspace to enable knowledge and resource transfer across languages. Beyond word\nalignment, we introduce multiple cluster-level alignments and enforce the word\nclusters to be consistently distributed across multiple languages. We exploit\nthree signals for clustering: (1) neighbor words in the monolingual word\nembedding space; (2) character-level information; and (3) linguistic properties\n(e.g., apposition, locative suffix) derived from linguistic structure knowledge\nbases available for thousands of languages. We introduce a new\ncluster-consistent correlational neural network to construct the common\nsemantic space by aligning words as well as clusters. Intrinsic evaluation on\nmonolingual and multilingual QVEC tasks shows our approach achieves\nsignificantly higher correlation with linguistic features than state-of-the-art\nmulti-lingual embedding learning methods do. Using low-resource language name\ntagging as a case study for extrinsic evaluation, our approach achieves up to\n24.5\\% absolute F-score gain over the state of the art.", "published": "2018-04-21 01:48:38", "link": "http://arxiv.org/abs/1804.07875v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing", "abstract": "Building a semantic parser quickly in a new domain is a fundamental challenge\nfor conversational interfaces, as current semantic parsers require expensive\nsupervision and lack the ability to generalize to new domains. In this paper,\nwe introduce a zero-shot approach to semantic parsing that can parse utterances\nin unseen domains while only being trained on examples in other source domains.\nFirst, we map an utterance to an abstract, domain-independent, logical form\nthat represents the structure of the logical form, but contains slots instead\nof KB constants. Then, we replace slots with KB constants via lexical alignment\nscores and global inference. Our model reaches an average accuracy of 53.4% on\n7 domains in the Overnight dataset, substantially better than other zero-shot\nbaselines, and performs as good as a parser trained on over 30% of the target\ndomain examples.", "published": "2018-04-21 08:27:14", "link": "http://arxiv.org/abs/1804.07918v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extrofitting: Enriching Word Representation and its Vector Space with\n  Semantic Lexicons", "abstract": "We propose post-processing method for enriching not only word representation\nbut also its vector space using semantic lexicons, which we call extrofitting.\nThe method consists of 3 steps as follows: (i) Expanding 1 or more dimension(s)\non all the word vectors, filling with their representative value. (ii)\nTransferring semantic knowledge by averaging each representative values of\nsynonyms and filling them in the expanded dimension(s). These two steps make\nrepresentations of the synonyms close together. (iii) Projecting the vector\nspace using Linear Discriminant Analysis, which eliminates the expanded\ndimension(s) with semantic knowledge. When experimenting with GloVe, we find\nthat our method outperforms Faruqui's retrofitting on some of word similarity\ntask. We also report further analysis on our method in respect to word vector\ndimensions, vocabulary size as well as other well-known pretrained word vectors\n(e.g., Word2Vec, Fasttext).", "published": "2018-04-21 11:17:26", "link": "http://arxiv.org/abs/1804.07946v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expert Finding in Community Question Answering: A Review", "abstract": "The rapid development recently of Community Question Answering (CQA)\nsatisfies users quest for professional and personal knowledge about anything.\nIn CQA, one central issue is to find users with expertise and willingness to\nanswer the given questions. Expert finding in CQA often exhibits very different\nchallenges compared to traditional methods. Sparse data and new features\nviolate fundamental assumptions of traditional recommendation systems. This\npaper focuses on reviewing and categorizing the current progress on expert\nfinding in CQA. We classify all the existing solutions into four different\ncategories: matrix factorization based models (MF-based models), gradient\nboosting tree based models (GBT-based models), deep learning based models\n(DL-based models) and ranking based models (R-based models). We find that\nMF-based models outperform other categories of models in the field of expert\nfinding in CQA. Moreover, we use innovative diagrams to clarify several\nimportant concepts of ensemble learning, and find that ensemble models with\nseveral specific single models can further boosting the performance. Further,\nwe compare the performance of different models on different types of matching\ntasks, including text vs. text, graph vs. text, audio vs. text and video vs.\ntext. The results can help the model selection of expert finding in practice.\nFinally, we explore some potential future issues in expert finding research in\nCQA.", "published": "2018-04-21 12:37:45", "link": "http://arxiv.org/abs/1804.07958v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Variational Inference In Pachinko Allocation Machines", "abstract": "The Pachinko Allocation Machine (PAM) is a deep topic model that allows\nrepresenting rich correlation structures among topics by a directed acyclic\ngraph over topics. Because of the flexibility of the model, however,\napproximate inference is very difficult. Perhaps for this reason, only a small\nnumber of potential PAM architectures have been explored in the literature. In\nthis paper we present an efficient and flexible amortized variational inference\nmethod for PAM, using a deep inference network to parameterize the approximate\nposterior distribution in a manner similar to the variational autoencoder. Our\ninference method produces more coherent topics than state-of-art inference\nmethods for PAM while being an order of magnitude faster, which allows\nexploration of a wider range of PAM architectures than have previously been\nstudied.", "published": "2018-04-21 11:12:25", "link": "http://arxiv.org/abs/1804.07944v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
