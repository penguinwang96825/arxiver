{"title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval", "abstract": "Neural retrieval methods using transformer-based pre-trained language models\nhave advanced multilingual and cross-lingual retrieval. However, their\neffectiveness for low-resource, morphologically rich languages such as Amharic\nremains underexplored due to data scarcity and suboptimal tokenization. We\naddress this gap by introducing Amharic-specific dense retrieval models based\non pre-trained Amharic BERT and RoBERTa backbones. Our proposed\nRoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative\nimprovement in MRR@10 and a 9.86% gain in Recall@10 over the strongest\nmultilingual baseline, Arctic Embed 2.0 (568M parameters). More compact\nvariants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while\nbeing over 13x smaller. Additionally, we train a ColBERT-based late interaction\nretrieval model that achieves the highest MRR@10 score (0.843) among all\nevaluated models. We benchmark our proposed models against both sparse and\ndense retrieval baselines to systematically assess retrieval effectiveness in\nAmharic. Our analysis highlights key challenges in low-resource settings and\nunderscores the importance of language-specific adaptation. To foster future\nresearch in low-resource IR, we publicly release our dataset, codebase, and\ntrained models at https://github.com/kidist-amde/amharic-ir-benchmarks.", "published": "2025-05-25 23:06:20", "link": "http://arxiv.org/abs/2505.19356v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50 (Primary), 68T05 (Secondary)", "H.3.3; H.3.1; I.2.7"], "primary_category": "cs.IR"}
{"title": "Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales", "abstract": "Large language models (LLMs) obtain state of the art zero shot relevance\nranking performance on a variety of information retrieval tasks. The two most\ncommon prompts to elicit LLM relevance judgments are pointwise scoring (a.k.a.\nrelevance generation), where the LLM sees a single query-document pair and\noutputs a single relevance score, and listwise ranking (a.k.a. permutation\ngeneration), where the LLM sees a query and a list of documents and outputs a\npermutation, sorting the documents in decreasing order of relevance. The\ncurrent research community consensus is that listwise ranking yields superior\nperformance, and significant research effort has been devoted to crafting LLM\nlistwise ranking algorithms. The underlying hypothesis is that LLMs are better\nat making relative relevance judgments than absolute ones. In tension with this\nhypothesis, we find that the gap between pointwise scoring and listwise ranking\nshrinks when pointwise scoring is implemented using a sufficiently large\nordinal relevance label space, becoming statistically insignificant for many\nLLM-benchmark dataset combinations (where ``significant'' means ``95\\%\nconfidence that listwise ranking improves NDCG@10''). Our evaluations span four\nLLMs, eight benchmark datasets from the BEIR and TREC-DL suites, and two\nproprietary datasets with relevance labels collected after the training cut-off\nof all LLMs evaluated.", "published": "2025-05-25 21:41:35", "link": "http://arxiv.org/abs/2505.19334v1", "categories": ["cs.LG", "cs.IR", "H.3.3; I.2.7; H.3.1"], "primary_category": "cs.LG"}
{"title": "DocMMIR: A Framework for Document Multi-modal Information Retrieval", "abstract": "The rapid advancement of unsupervised representation learning and large-scale\npre-trained vision-language models has significantly improved cross-modal\nretrieval tasks. However, existing multi-modal information retrieval (MMIR)\nstudies lack a comprehensive exploration of document-level retrieval and suffer\nfrom the absence of cross-domain datasets at this granularity. To address this\nlimitation, we introduce DocMMIR, a novel multi-modal document retrieval\nframework designed explicitly to unify diverse document formats and domains,\nincluding Wikipedia articles, scientific papers (arXiv), and presentation\nslides, within a comprehensive retrieval scenario. We construct a large-scale\ncross-domain multimodal benchmark, comprising 450K samples, which\nsystematically integrates textual and visual information. Our comprehensive\nexperimental analysis reveals substantial limitations in current\nstate-of-the-art MLLMs (CLIP, BLIP2, SigLIP-2, ALIGN) when applied to our\ntasks, with only CLIP demonstrating reasonable zero-shot performance.\nFurthermore, we conduct a systematic investigation of training strategies,\nincluding cross-modal fusion methods and loss functions, and develop a tailored\napproach to train CLIP on our benchmark. This results in a +31% improvement in\nMRR@10 compared to the zero-shot baseline. All our data and code are released\nin https://github.com/J1mL1/DocMMIR.", "published": "2025-05-25 20:58:58", "link": "http://arxiv.org/abs/2505.19312v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Aligning Web Query Generation with Ranking Objectives via Direct Preference Optimization", "abstract": "Neural retrieval models excel in Web search, but their training requires\nsubstantial amounts of labeled query-document pairs, which are costly to\nobtain. With the widespread availability of Web document collections like\nClueWeb22, synthetic queries generated by large language models offer a\nscalable alternative. Still, synthetic training queries often vary in quality,\nwhich leads to suboptimal downstream retrieval performance. Existing methods\ntypically filter out noisy query-document pairs based on signals from an\nexternal re-ranker. In contrast, we propose a framework that leverages Direct\nPreference Optimization (DPO) to integrate ranking signals into the query\ngeneration process, aiming to directly optimize the model towards generating\nhigh-quality queries that maximize downstream retrieval effectiveness.\nExperiments show higher ranker-assessed relevance between query-document pairs\nafter DPO, leading to stronger downstream performance on the MS~MARCO benchmark\nwhen compared to baseline models trained with synthetic data.", "published": "2025-05-25 20:34:12", "link": "http://arxiv.org/abs/2505.19307v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "RankLLM: A Python Package for Reranking with LLMs", "abstract": "The adoption of large language models (LLMs) as rerankers in multi-stage\nretrieval systems has gained significant traction in academia and industry.\nThese models refine a candidate list of retrieved documents, often through\ncarefully designed prompts, and are typically used in applications built on\nretrieval-augmented generation (RAG). This paper introduces RankLLM, an\nopen-source Python package for reranking that is modular, highly configurable,\nand supports both proprietary and open-source LLMs in customized reranking\nworkflows. To improve usability, RankLLM features optional integration with\nPyserini for retrieval and provides integrated evaluation for multi-stage\npipelines. Additionally, RankLLM includes a module for detailed analysis of\ninput prompts and LLM responses, addressing reliability concerns with LLM APIs\nand non-deterministic behavior in Mixture-of-Experts (MoE) models. This paper\npresents the architecture of RankLLM, along with a detailed step-by-step guide\nand sample code. We reproduce results from RankGPT, LRL, RankVicuna,\nRankZephyr, and other recent models. RankLLM integrates with common inference\nframeworks and a wide range of LLMs. This compatibility allows for quick\nreproduction of reported results, helping to speed up both research and\nreal-world applications. The complete repository is available at rankllm.ai,\nand the package can be installed via PyPI.", "published": "2025-05-25 19:29:27", "link": "http://arxiv.org/abs/2505.19284v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Conventional Contrastive Learning Often Falls Short: Improving Dense Retrieval with Cross-Encoder Listwise Distillation and Synthetic Data", "abstract": "We investigate improving the retrieval effectiveness of embedding models\nthrough the lens of corpus-specific fine-tuning. Prior work has shown that\nfine-tuning with queries generated using a dataset's retrieval corpus can boost\nretrieval effectiveness for the dataset. However, we find that surprisingly,\nfine-tuning using the conventional InfoNCE contrastive loss often reduces\neffectiveness in state-of-the-art models. To overcome this, we revisit\ncross-encoder listwise distillation and demonstrate that, unlike using\ncontrastive learning alone, listwise distillation can help more consistently\nimprove retrieval effectiveness across multiple datasets. Additionally, we show\nthat synthesizing more training data using diverse query types (such as claims,\nkeywords, and questions) yields greater effectiveness than using any single\nquery type alone, regardless of the query type used in evaluation. Our findings\nfurther indicate that synthetic queries offer comparable utility to\nhuman-written queries for training. We use our approach to train an embedding\nmodel that achieves state-of-the-art effectiveness among BERT embedding models.\nWe release our model and both query generation and training code to facilitate\nfurther research.", "published": "2025-05-25 19:06:19", "link": "http://arxiv.org/abs/2505.19274v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research", "abstract": "Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.", "published": "2025-05-25 18:16:13", "link": "http://arxiv.org/abs/2505.19253v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval", "abstract": "Although Multi-Vector Retrieval (MVR) has achieved the state of the art on\nmany information retrieval (IR) tasks, its performance highly depends on how to\ndecompose queries into smaller pieces, say phrases or tokens. However,\noptimizing query decomposition for MVR performance is not end-to-end\ndifferentiable. Even worse, jointly solving this problem and training the\ndownstream retrieval-based systems, say RAG systems could be highly\ninefficient. To overcome these challenges, we propose Performance-Oriented\nQuery Decomposer (POQD), a novel query decomposition framework for MVR. POQD\nleverages one LLM for query decomposition and searches the optimal prompt with\nan LLM-based optimizer. We further propose an end-to-end training algorithm to\nalternatively optimize the prompt for query decomposition and the downstream\nmodels. This algorithm can achieve superior MVR performance at a reasonable\ntraining cost as our theoretical analysis suggests. POQD can be integrated\nseamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented\nGeneration (RAG) systems. Extensive empirical studies on representative\nRAG-based QA tasks show that POQD outperforms existing query decomposition\nstrategies in both retrieval performance and end-to-end QA accuracy. POQD is\navailable at https://github.com/PKU-SDS-lab/POQD-ICML25.", "published": "2025-05-25 15:31:52", "link": "http://arxiv.org/abs/2505.19189v1", "categories": ["cs.IR", "cs.DB"], "primary_category": "cs.IR"}
{"title": "DLF: Enhancing Explicit-Implicit Interaction via Dynamic Low-Order-Aware Fusion for CTR Prediction", "abstract": "Click-through rate (CTR) prediction is a critical task in online advertising\nand recommender systems, relying on effective modeling of feature interactions.\nExplicit interactions capture predefined relationships, such as inner products,\nbut often suffer from data sparsity, while implicit interactions excel at\nlearning complex patterns through non-linear transformations but lack inductive\nbiases for efficient low-order modeling. Existing two-stream architectures\nintegrate these paradigms but face challenges such as limited information\nsharing, gradient imbalance, and difficulty preserving low-order signals in\nsparse CTR data. We propose a novel framework, Dynamic Low-Order-Aware Fusion\n(DLF), which addresses these limitations through two key components: a\nResidual-Aware Low-Order Interaction Network (RLI) and a Network-Aware\nAttention Fusion Module (NAF). RLI explicitly preserves low-order signals while\nmitigating redundancy from residual connections, and NAF dynamically integrates\nexplicit and implicit representations at each layer, enhancing information\nsharing and alleviating gradient imbalance. Together, these innovations balance\nlow-order and high-order interactions, improving model expressiveness.\nExtensive experiments on public datasets demonstrate that DLF achieves\nstate-of-the-art performance in CTR prediction, addressing key limitations of\nexisting models. The implementation is publicly available at\nhttps://github.com/USTC-StarTeam/DLF.", "published": "2025-05-25 15:05:00", "link": "http://arxiv.org/abs/2505.19182v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations", "abstract": "In the domain of sponsored search advertising, the focus of Keyphrase\nrecommendation has largely been on exact match types, which pose issues such as\nhigh management expenses, limited targeting scope, and evolving search query\npatterns. Alternatives like Broad match types can alleviate certain drawbacks\nof exact matches but present challenges like poor targeting accuracy and\nminimal supervisory signals owing to limited advertiser usage. This research\ndefines the criteria for an ideal broad match, emphasizing on both efficiency\nand effectiveness, ensuring that a significant portion of matched queries are\nrelevant. We propose BroadGen, an innovative framework that recommends\nefficient and effective broad match keyphrases by utilizing historical search\nquery data. Additionally, we demonstrate that BroadGen, through token\ncorrespondence modeling, maintains better query stability over time. BroadGen's\ncapabilities allow it to serve daily, millions of sellers at eBay with over 2.3\nbillion items.", "published": "2025-05-25 14:25:52", "link": "http://arxiv.org/abs/2505.19164v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Semantic-enhanced Co-attention Prompt Learning for Non-overlapping Cross-Domain Recommendation", "abstract": "Non-overlapping Cross-domain Sequential Recommendation (NCSR) is the task\nthat focuses on domain knowledge transfer without overlapping entities.\nCompared with traditional Cross-domain Sequential Recommendation (CSR), NCSR\nposes several challenges: 1) NCSR methods often rely on explicit item IDs,\noverlooking semantic information among entities. 2) Existing CSR mainly relies\non domain alignment for knowledge transfer, risking semantic loss during\nalignment. 3) Most previous studies do not consider the many-to-one\ncharacteristic, which is challenging because of the utilization of multiple\nsource domains. Given the above challenges, we introduce the prompt learning\ntechnique for Many-to-one Non-overlapping Cross-domain Sequential\nRecommendation (MNCSR) and propose a Text-enhanced Co-attention Prompt Learning\nParadigm (TCPLP). Specifically, we capture semantic meanings by representing\nitems through text rather than IDs, leveraging natural language universality to\nfacilitate cross-domain knowledge transfer. Unlike prior works that need to\nconduct domain alignment, we directly learn transferable domain information,\nwhere two types of prompts, i.e., domain-shared and domain-specific prompts,\nare devised, with a co-attention-based network for prompt encoding. Then, we\ndevelop a two-stage learning strategy, i.e., pre-train & prompt-tuning\nparadigm, for domain knowledge pre-learning and transferring, respectively. We\nconduct extensive experiments on three datasets and the experimental results\ndemonstrate the superiority of our TCPLP. Our source codes have been publicly\nreleased.", "published": "2025-05-25 10:45:19", "link": "http://arxiv.org/abs/2505.19085v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "HGCL: Hierarchical Graph Contrastive Learning for User-Item Recommendation", "abstract": "Graph Contrastive Learning (GCL), which fuses graph neural networks with\ncontrastive learning, has evolved as a pivotal tool in user-item\nrecommendations. While promising, existing GCL methods often lack explicit\nmodeling of hierarchical item structures, which represent item similarities\nacross varying resolutions. Such hierarchical item structures are ubiquitous in\nvarious items (e.g., online products and local businesses), and reflect their\ninherent organizational properties that serve as critical signals for enhancing\nrecommendation accuracy. In this paper, we propose Hierarchical Graph\nContrastive Learning (HGCL), a novel GCL method that incorporates hierarchical\nitem structures for user-item recommendations. First, HGCL pre-trains a GCL\nmodule using cross-layer contrastive learning to obtain user and item\nrepresentations. Second, HGCL employs a representation compression and\nclustering method to construct a two-hierarchy user-item bipartite graph.\nUltimately, HGCL fine-tunes user and item representations by learning on the\nhierarchical graph, and then provides recommendations based on user-item\ninteraction scores. Experiments on three widely adopted benchmark datasets\nranging from 70K to 382K nodes confirm the superior performance of HGCL over\nexisting baseline models, highlighting the contribution of hierarchical item\nstructures in enhancing GCL methods for recommendation tasks.", "published": "2025-05-25 07:56:56", "link": "http://arxiv.org/abs/2505.19020v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Lightweight Embeddings with Graph Rewiring for Collaborative Filtering", "abstract": "As recommendation services scale rapidly and their deployment now commonly\ninvolves resource-constrained edge devices, GNN-based recommender systems face\nsignificant challenges, including high embedding storage costs and runtime\nlatency from graph propagations. Our previous work, LEGCF, effectively reduced\nembedding storage costs but struggled to maintain recommendation performance\nunder stricter storage limits. Additionally, LEGCF did not address the\nextensive runtime computation costs associated with graph propagation, which\ninvolves heavy multiplication and accumulation operations (MACs). These\nchallenges consequently hinder effective training and inference on\nresource-constrained edge devices. To address these limitations, we propose\nLightweight Embeddings with Rewired Graph for Graph Collaborative Filtering\n(LERG), an improved extension of LEGCF. LERG retains LEGCFs compositional\ncodebook structure but introduces quantization techniques to reduce the storage\ncost, enabling the inclusion of more meta-embeddings within the same storage.\nTo optimize graph propagation, we pretrain the quantized compositional\nembedding table using the full interaction graph on resource-rich servers,\nafter which a fine-tuning stage is engaged to identify and prune\nlow-contribution entities via a gradient-free binary integer programming\napproach, constructing a rewired graph that excludes these entities (i.e.,\nuser/item nodes) from propagating signals. The quantized compositional\nembedding table with selective embedding participation and sparse rewired graph\nare transferred to edge devices which significantly reduce computation memory\nand inference time. Experiments on three public benchmark datasets, including\nan industry-scale dataset, demonstrate that LERG achieves superior\nrecommendation performance while dramatically reducing storage and computation\ncosts for graph-based recommendation services.", "published": "2025-05-25 06:39:20", "link": "http://arxiv.org/abs/2505.18999v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE", "abstract": "We revisit the efficacy of simple, real-valued embedding models for knowledge\ngraph completion and introduce RelatE, an interpretable and modular method that\nefficiently integrates dual representations for entities and relations. RelatE\nemploys a real-valued phase-modulus decomposition, leveraging sinusoidal phase\nalignments to encode relational patterns such as symmetry, inversion, and\ncomposition. In contrast to recent approaches based on complex-valued\nembeddings or deep neural architectures, RelatE preserves architectural\nsimplicity while achieving competitive or superior performance on standard\nbenchmarks. Empirically, RelatE outperforms prior methods across several\ndatasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,\nsurpassing all baselines. Additionally, RelatE offers significant efficiency\ngains, reducing training time by 24%, inference latency by 31%, and peak GPU\nmemory usage by 22% compared to RotatE. Perturbation studies demonstrate\nimproved robustness, with MRR degradation reduced by up to 61% relative to\nTransE and by up to 19% compared to RotatE under structural edits such as edge\nremovals and relation swaps. Formal analysis further establishes the model's\nfull expressiveness and its capacity to represent essential first-order logical\ninference patterns. These results position RelatE as a scalable and\ninterpretable alternative to more complex architectures for knowledge graph\ncompletion.", "published": "2025-05-25 04:36:52", "link": "http://arxiv.org/abs/2505.18971v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Weaver: Interweaving SQL and LLM for Table Reasoning", "abstract": "Querying tables with unstructured data is challenging due to the presence of\ntext (or image), either embedded in the table or in external paragraphs, which\ntraditional SQL struggles to process, especially for tasks requiring semantic\nreasoning. While Large Language Models (LLMs) excel at understanding context,\nthey face limitations with long input sequences. Existing approaches that\ncombine SQL and LLMs typically rely on rigid, predefined work-flows, limiting\ntheir adaptability to complex queries. To address these issues, we introduce\nWeaver , a modular pipeline that dynamically integrates SQL and LLMs for\ntable-based question answering (TableQA). Weaver generates a flexible,\nstep-by-step plan that combines SQL for structured data retrieval with LLMs for\nsemantic processing. By decomposing complex queries into manageable subtasks,\nWeaver improves accuracy and generalization. Our experiments show that Weaver\nconsistently outperforms state-of-the-art methods across four TableQA datasets,\nreducing both API calls and error rates.", "published": "2025-05-25 03:27:37", "link": "http://arxiv.org/abs/2505.18961v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection", "abstract": "Graph Anomaly Detection (GAD) in heterogeneous networks presents unique\nchallenges due to node and edge heterogeneity. Existing Graph Neural Network\n(GNN) methods primarily focus on homogeneous GAD and thus fail to address three\nkey issues: (C1) Capturing abnormal signal and rich semantics across diverse\nmeta-paths; (C2) Retaining high-frequency content in HIN dimension alignment;\nand (C3) Learning effectively from difficult anomaly samples with class\nimbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based\non a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse\ndomains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter,\nwhich captures anomalous information via applying dedicated Chi-Square filters\nto each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns\nfeatures while preserving high-frequency information and incorporates\nheterogeneous messages by a unified Chi-Square Filter; and (3)\nContribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies\nto address class imbalance. Extensive experiments on public and industrial\ndatasets show that ChiGAD outperforms state-of-the-art models on multiple\nmetrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD\ndatasets, validating the effectiveness of Chi-Square filters. Our code is\navailable at https://github.com/HsipingLi/ChiGAD.", "published": "2025-05-25 01:58:02", "link": "http://arxiv.org/abs/2505.18934v1", "categories": ["cs.LG", "cs.AI", "cs.IR", "cs.SI"], "primary_category": "cs.LG"}
{"title": "On the Secrecy of RIS-aided THz Wireless System subject to $\u03b1-\u03bc$ fading with Pointing Errors", "abstract": "The study examines the secrecy outage probability (SOP) and intercept\nprobability (IP) of a reflecting intelligent surface (RIS)-enabled THz wireless\nnetwork experiencing $\\alpha-\\mu$ fading with pointing errors. Specifically,\nthe base station (BS) sends information to a legitimate user $\\ell$ via the RIS\nwhile an eavesdropper $e$ tries to overhear the conversation. Furthermore,\nreceive nodes are equipped with a single antenna, and the RIS phase shifts were\nselected to boost the SNR at node $\\ell$. Elementary functions are used to\naccurately approximate the statistical features of channel gain in BS-$\\ell$\nand BS-$e$ links, leading to SOP and IP approximate and asymptotic expressions.\nMonte Carlo simulation validates all analytical findings for different system\nparameters' values.", "published": "2025-05-25 23:14:09", "link": "http://arxiv.org/abs/2505.19357v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Agentic Information Theory: Ergodicity and Intrinsic Semantics of Information Processes", "abstract": "We develop information theory for the temporal behavior of memoryful agents\nmoving through complex -- structured, stochastic -- environments. We introduce\ninformation processes -- stochastic processes produced by cognitive agents in\nreal-time as they interact with and interpret incoming stimuli. We provide\nbasic results on the ergodicity and semantics of the resulting time series of\nShannon information measures that monitor an agent's adapting view of\nuncertainty and structural correlation in its environment.", "published": "2025-05-25 19:12:05", "link": "http://arxiv.org/abs/2505.19275v1", "categories": ["cond-mat.stat-mech", "cs.IT", "cs.MA", "math.IT", "nlin.AO"], "primary_category": "cond-mat.stat-mech"}
{"title": "RIS-Assisted Survivable Fronthaul Design in Cell-Free Massive MIMO System", "abstract": "This paper investigates the application of reconfigurable intelligent\nsurfaces (RISs) to improve fronthaul link survivability in cell-free massive\nMIMO (CF mMIMO) systems. To enhance the fronthaul survivability, two\ncomplementary mechanisms are considered. Firstly, RIS is set to provide\nreliable line-of-sight (LOS) connectivity and enhance the mmWave backup link.\nSecondly, a resource-sharing scheme that leverages redundant cable capacity\nthrough neighboring master access points (APs) to guarantee availability is\nconsidered. We formulate the redundant capacity minimization problem as a\nRIS-assisted multi-user MIMO rate control optimization problem, developing a\nnovel solution that combines a modified weighted minimum mean square error\n(WMMSE) algorithm for precoding design with Riemannian gradient descent for RIS\nphase shift optimization. Our numerical evaluations show that RIS reduces the\nrequired redundant capacity by 65.6% compared to the no RIS case to reach a 99%\nsurvivability. The results show that the most substantial gains of RIS occur\nduring complete outages of the direct disconnected master AP-CPU channel. These\nresults demonstrate RIS's potential to significantly enhance fronthaul\nreliability while minimizing infrastructure costs in next-generation wireless\nnetworks.", "published": "2025-05-25 14:00:28", "link": "http://arxiv.org/abs/2505.19152v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "High Throughput QC-LDPC Decoder With Optimized Schedule Policy in Layered Decoding", "abstract": "In this study, a scheduling policy of layered decoding for quasi-cycle (QC)\nlow-density parity-check (LDPC) codes with high throughput and good performance\nis designed. The influence of scheduling on the delay of the decoder's hardware\nimplementation and on the decoding performance are considered simultaneously.\nSpecifically, we analyze the idle time required under various scheduling\nsequences within a pipelined decoding architecture and formulate the problem as\na traveling salesman problem (TSP) aiming at minimizing idle time. Furthermore,\nconsidering that different scheduling sequences can affect decoding\nperformance, we refine the graph used to solve the TSP based on scheduling\ncharacteristics that promote improved decoding outcomes. Simulation results\ndemonstrate that the identified scheduling sequence achieves a low number of\nhardware delays while maintaining excellent decoding performance for 5G New\nRadio (NR) LDPC codes.", "published": "2025-05-25 08:27:54", "link": "http://arxiv.org/abs/2505.19027v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Making Teams and Influencing Agents: Efficiently Coordinating Decision Trees for Interpretable Multi-Agent Reinforcement Learning", "abstract": "Poor interpretability hinders the practical applicability of multi-agent\nreinforcement learning (MARL) policies. Deploying interpretable surrogates of\nuninterpretable policies enhances the safety and verifiability of MARL for\nreal-world applications. However, if these surrogates are to interact directly\nwith the environment within human supervisory frameworks, they must be both\nperformant and computationally efficient. Prior work on interpretable MARL has\neither sacrificed performance for computational efficiency or computational\nefficiency for performance. To address this issue, we propose HYDRAVIPER, a\ndecision tree-based interpretable MARL algorithm. HYDRAVIPER coordinates\ntraining between agents based on expected team performance, and adaptively\nallocates budgets for environment interaction to improve computational\nefficiency. Experiments on standard benchmark environments for multi-agent\ncoordination and traffic signal control show that HYDRAVIPER matches the\nperformance of state-of-the-art methods using a fraction of the runtime, and\nthat it maintains a Pareto frontier of performance for different interaction\nbudgets.", "published": "2025-05-25 21:05:48", "link": "http://arxiv.org/abs/2505.19316v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control", "abstract": "Traditional Identity and Access Management (IAM) systems, primarily designed\nfor human users or static machine identities via protocols such as OAuth,\nOpenID Connect (OIDC), and SAML, prove fundamentally inadequate for the\ndynamic, interdependent, and often ephemeral nature of AI agents operating at\nscale within Multi Agent Systems (MAS), a computational system composed of\nmultiple interacting intelligent agents that work collectively.\n  This paper posits the imperative for a novel Agentic AI IAM framework: We\ndeconstruct the limitations of existing protocols when applied to MAS,\nillustrating with concrete examples why their coarse-grained controls,\nsingle-entity focus, and lack of context-awareness falter. We then propose a\ncomprehensive framework built upon rich, verifiable Agent Identities (IDs),\nleveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs),\nthat encapsulate an agents capabilities, provenance, behavioral scope, and\nsecurity posture.\n  Our framework includes an Agent Naming Service (ANS) for secure and\ncapability-aware discovery, dynamic fine-grained access control mechanisms, and\ncritically, a unified global session management and policy enforcement layer\nfor real-time control and consistent revocation across heterogeneous agent\ncommunication protocols. We also explore how Zero-Knowledge Proofs (ZKPs)\nenable privacy-preserving attribute disclosure and verifiable policy\ncompliance.\n  We outline the architecture, operational lifecycle, innovative contributions,\nand security considerations of this new IAM paradigm, aiming to establish the\nfoundational trust, accountability, and security necessary for the burgeoning\nfield of agentic AI and the complex ecosystems they will inhabit.", "published": "2025-05-25 20:21:55", "link": "http://arxiv.org/abs/2505.19301v1", "categories": ["cs.CR", "cs.AI", "cs.MA"], "primary_category": "cs.CR"}
{"title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "abstract": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration face critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm, learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization.", "published": "2025-05-25 17:15:55", "link": "http://arxiv.org/abs/2505.19234v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding", "abstract": "Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial\nintelligence and robotics, requiring the computation of collision-free paths\nfor multiple agents navigating from their start locations to designated goals.\nAs autonomous systems become increasingly prevalent in warehouses, urban\ntransportation, and other complex environments, MAPF has evolved from a\ntheoretical challenge to a critical enabler of real-world multi-robot\ncoordination. This comprehensive survey bridges the long-standing divide\nbetween classical algorithmic approaches and emerging learning-based methods in\nMAPF research. We present a unified framework that encompasses search-based\nmethods (including Conflict-Based Search, Priority-Based Search, and Large\nNeighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP\nformulations), and data-driven techniques (reinforcement learning, supervised\nlearning, and hybrid strategies). Through systematic analysis of experimental\npractices across 200+ papers, we uncover significant disparities in evaluation\nmethodologies, with classical methods typically tested on larger-scale\ninstances (up to 200 by 200 grids with 1000+ agents) compared to learning-based\napproaches (predominantly 10-100 agents). We provide a comprehensive taxonomy\nof evaluation metrics, environment types, and baseline selections, highlighting\nthe need for standardized benchmarking protocols. Finally, we outline promising\nfuture directions including mixed-motive MAPF with game-theoretic\nconsiderations, language-grounded planning with large language models, and\nneural solver architectures that combine the rigor of classical methods with\nthe flexibility of deep learning. This survey serves as both a comprehensive\nreference for researchers and a practical guide for deploying MAPF solutions in\nincreasingly complex real-world applications.", "published": "2025-05-25 16:28:06", "link": "http://arxiv.org/abs/2505.19219v1", "categories": ["cs.AI", "cs.LG", "cs.MA", "math.CO"], "primary_category": "cs.AI"}
{"title": "OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization", "abstract": "Hyperparameter optimization (HPO) is a critical yet challenging aspect of\nmachine learning model development, significantly impacting model performance\nand generalization. Traditional HPO methods often struggle with high\ndimensionality, complex interdependencies, and computational expense. This\npaper introduces OptiMindTune, a novel multi-agent framework designed to\nintelligently and efficiently optimize hyperparameters. OptiMindTune leverages\nthe collaborative intelligence of three specialized AI agents -- a Recommender\nAgent, an Evaluator Agent, and a Decision Agent -- each powered by Google's\nGemini models. These agents address distinct facets of the HPO problem, from\nmodel selection and hyperparameter suggestion to robust evaluation and\nstrategic decision-making. By fostering dynamic interactions and knowledge\nsharing, OptiMindTune aims to converge to optimal hyperparameter configurations\nmore rapidly and robustly than existing single-agent or monolithic approaches.\nOur framework integrates principles from advanced large language models, and\nadaptive search to achieve scalable and intelligent AutoML. We posit that this\nmulti-agent paradigm offers a promising avenue for tackling the increasing\ncomplexity of modern machine learning model tuning.", "published": "2025-05-25 16:05:41", "link": "http://arxiv.org/abs/2505.19205v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management", "abstract": "Motivated by dynamic parameter optimization in finite, but large action\n(configurations) spaces, this work studies the nonstochastic multi-armed bandit\n(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We\npropose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can\nuse state-of-the-art existing \"flat\" algorithms, but additionally clusters\nsimilar configurations to exploit local structures and adapt to changing\nenvironments. We prove that in the worst-case scenario, such clustering\napproach cannot hurt too much and ABoB guarantees a standard worst-case regret\nbound of $O\\left(k^{\\frac{1}{2}}T^{\\frac{1}{2}}\\right)$, where $T$ is the\nnumber of rounds and $k$ is the number of arms, matching the traditional flat\napproach. However, under favorable conditions related to the algorithm\nproperties, clusters properties, and certain Lipschitz conditions, the regret\nbound can be improved to $O\\left(k^{\\frac{1}{4}}T^{\\frac{1}{2}}\\right)$.\nSimulations and experiments on a real storage system demonstrate that ABoB,\nusing standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and\nfaster convergence than the flat method, up to 50% improvement in known\nprevious setups, nonstochastic and stochastic, as well as in our settings.", "published": "2025-05-25 09:30:47", "link": "http://arxiv.org/abs/2505.19061v1", "categories": ["cs.LG", "cs.MA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SANNet: A Semantic-Aware Agentic AI Networking Framework for Multi-Agent Cross-Layer Coordination", "abstract": "Agentic AI networking (AgentNet) is a novel AI-native networking paradigm\nthat relies on a large number of specialized AI agents to collaborate and\ncoordinate for autonomous decision-making, dynamic environmental adaptation,\nand complex goal achievement. It has the potential to facilitate real-time\nnetwork management alongside capabilities for self-configuration,\nself-optimization, and self-adaptation across diverse and complex networking\nenvironments, laying the foundation for fully autonomous networking systems in\nthe future. Despite its promise, AgentNet is still in the early stage of\ndevelopment, and there still lacks an effective networking framework to support\nautomatic goal discovery and multi-agent self-orchestration and task\nassignment. This paper proposes SANNet, a novel semantic-aware agentic AI\nnetworking architecture that can infer the semantic goal of the user and\nautomatically assign agents associated with different layers of a mobile system\nto fulfill the inferred goal. Motivated by the fact that one of the major\nchallenges in AgentNet is that different agents may have different and even\nconflicting objectives when collaborating for certain goals, we introduce a\ndynamic weighting-based conflict-resolving mechanism to address this issue. We\nprove that SANNet can provide theoretical guarantee in both conflict-resolving\nand model generalization performance for multi-agent collaboration in dynamic\nenvironment. We develop a hardware prototype of SANNet based on the open RAN\nand 5GS core platform. Our experimental results show that SANNet can\nsignificantly improve the performance of multi-agent networking systems, even\nwhen agents with conflicting objectives are selected to collaborate for the\nsame goal.", "published": "2025-05-25 02:45:18", "link": "http://arxiv.org/abs/2505.18946v1", "categories": ["cs.AI", "cs.MA", "cs.NI"], "primary_category": "cs.AI"}
{"title": "IsoGeometric Suitable Coupling Methods for Partitioned Multiphysics Simulation with Application to Fluid-Structure Interaction", "abstract": "This paper presents spline-based coupling methods for partitioned\nmultiphysics simulations, specifically designed for isogeometric analysis (IGA)\nbased solvers. Traditional vertex-based coupling approaches face significant\nchallenges when applied to IGA solvers, including geometric accuracy issues,\ninterpolation errors, and substantial communication overhead. The methodology\ndraws on the IGA mathematical framework to deliver coupling solutions that\npreserve high-order continuity and exact geometric representation of splines.\nWe develop two complementary strategies: (1) a spline-vertex coupling method\nenabling efficient interaction between IGA and conventional solvers, and (2) a\nfully isogeometric coupling approach maximizing accuracy for IGA-to-IGA\ncommunication.\n  Both theoretical analysis and extensive numerical experiments demonstrate\nthat our spline-based methods significantly reduce communication overhead\ncompared to traditional approaches while enhancing geometric accuracy through\nexact boundary representation and maintaining higher-order solution continuity\nacross coupled interfaces. We quantitatively confirm communication efficiency\nbenefits through systematic measurements of transfer times and data volumes\nacross various mesh refinement levels. Our benchmark studies demonstrate\ngeometric fidelity advantages while highlighting how splines naturally preserve\nsolution derivatives across interfaces without requiring additional\ncomputation. This work provides efficient coupling strategies tailored to\nIGA-based solvers and establishes a practical bridge between IGA and\ntraditional discretization methods, enabling broader adoption of IGA in\nestablished simulation workflows.", "published": "2025-05-25 22:26:28", "link": "http://arxiv.org/abs/2505.19346v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Rapid Development of Efficient Participant-Specific Computational Models of the Wrist", "abstract": "While computational modeling may help to develop new treatment options for\nhand and wrist injuries, at present, few models exist. The time and expertise\nrequired to develop and use these models is considerable. Moreover, most do not\nallow for variation of material properties, instead relying on literature\nreported averages. We have developed a novel automated workflow combining\nnon-linear morphing techniques with various algorithmic techniques to create\nparticipant-specific finite element models. Using this workflow, three\nparticipant-specific models were created from our existing four-dimensional\ncomputed tomography (4DCT) data. These were then used to perform two analyses\nto demonstrate the usefulness of the models to investigate clinical questions,\nnamely optimization of ligament properties to participant-specific kinematics,\nand Monte Carlo (MC) analysis of the impacts of ligament injury on joint\ncontact pressure, as an analogue for joint injury that may lead to\nosteoarthritis. Participant-specific models can be created in 2 hours and\nindividual simulations performed in 45 seconds. This work lays the groundwork\nfor future patient-specific modeling of the hand and wrist.", "published": "2025-05-25 19:26:00", "link": "http://arxiv.org/abs/2505.19282v1", "categories": ["q-bio.QM", "cs.CE", "cs.NA", "math.NA"], "primary_category": "q-bio.QM"}
{"title": "Asymptotic numerical hypocoercivity of the space-time discontinuous Galerkin method for Kolmogorov equation", "abstract": "We are concerned with discretisations of the classical Kolmogorov equation by\na standard space-time discontinuous Galerkin method. Kolmogorov equation serves\nas simple, yet rich enough in the present context, model problem for a wide\nrange of kinetic-type equations: although it involves diffusion in one of the\ntwo spatial dimensions only, the combined nature of the first order\ntransport/drift term and the degenerate diffusion are sufficient to `propagate\ndissipation' across the spatial domain in its entirety. This is a manifestation\nof the celebrated concept of hypocoercivity, a term coined and studied\nextensively by Villani in [27]. We show that the standard, classical,\nspace-time discontinuous Galerkin method, admits a corresponding hypocoercivity\nproperty at the discrete level, asymptotically for large times. To the best of\nour knowledge, this is the first result of this kind for any standard Galerkin\nscheme. This property is shown by proving one part of a discrete inf-sup-type\nstability result for the method in a family of norms dictated by a modified\nscalar product motivated by the theory in [27]. This family of norms contains\nthe full gradient of the numerical solution, thereby allowing for a full\nspectral gap/Poincar\\'e-type inequality at the discrete level, thus, showcasing\na subtle, discretisation-parameter-dependent, numerical hypocoercivity\nproperty. Further, we show that the space-time discontinuous Galerkin method is\ninf-sup stable in the family of norms containing the full gradient of the\nnumerical solution, which may be a result of independent interest.", "published": "2025-05-25 16:36:09", "link": "http://arxiv.org/abs/2505.19222v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Higher order stray field computation on tensor product domains", "abstract": "We present an extension of the tensor grid method for stray field computation\non rectangular domains that incorporates higher-order basis functions. Both the\nmagnetization and the resulting magnetic field are represented using\nhigher-order B-spline bases, which allow for increased accuracy and smoothness.\nThe method employs a super-potential formulation, which circumvents the need to\nconvolve with a singular kernel. The field is represented with high accuracy as\na functional Tucker tensor, leveraging separable expansions on the tensor\nproduct domain and trained via a multilinear extension of the extreme learning\nmachine methodology. Unlike conventional grid-based methods, the proposed\nmesh-free approach allows for continuous field evaluation. Numerical\nexperiments confirm the accuracy and efficiency of the proposed method,\ndemonstrating exponential convergence of the energy and linear computational\nscaling with respect to the multilinear expansion rank.", "published": "2025-05-25 15:01:05", "link": "http://arxiv.org/abs/2505.19180v1", "categories": ["physics.comp-ph", "cs.NA", "math.NA", "65Z05"], "primary_category": "physics.comp-ph"}
{"title": "Efficient isogeometric Boundary Element simulation of elastic domains containing thin inclusions", "abstract": "This paper is concerned with the Boundary Element simulation of elastic\ndomains that contain thin inclusions that have elastic material properties,\nwhich are different to the domain. With thin inclusions we mean inclusions with\nextreme aspect ratios, i.e. where one dimension is much smaller than the other\nones. Examples of this are reinforcements in civil/mechanical engineering or\nconcrete linings in underground construction. The fact that an inclusion has an\nextreme aspect ratio poses a challenge to the numerical integration of the\narising singular integrals and novel approaches are presented to deal with it.\nSeveral examples demonstrate the efficiency and accuracy of the proposed\nmethods and show that the results are in good agreement with analytical and\nother numerical solutions.", "published": "2025-05-25 14:39:02", "link": "http://arxiv.org/abs/2505.19170v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Efficient and robust solvers for a cell-by-cell dual-poroelasticity problem", "abstract": "This paper presents a scalable and robust solver for a cell-by-cell\ndual-poroelasticity model, describing the mechanical interactions between\nbrains cells embedded in extracellular space. Explicitly representing the\ncomplex cellular shapes, the proposed approach models both intracellular and\nextracellular spaces as distinct poroelastic media, separated by a permeable\ncell membrane which allows hydrostatic and osmotic pressure-driven fluid\nexchange. The solver, which employs a three-field (displacement, total\npressure, and fluid pressure) formulation, leverages the framework of\nnorm-equivalent preconditioning and appropriately fitted norms to ensure\nrobustness across all material parameters of the model. Scalability for large\nand complex geometries is achieved through efficient Algebraic Multigrid (AMG)\napproximations of the preconditioners' individual blocks. Furthermore, we\naccommodate diverse boundary conditions, including full Dirichlet boundary\nconditions for displacement, which we handle efficiently using the\nSherman-Morrison-Woodbury formula. Numerical experiments demonstrate the\npreconditioners' robustness and performance across various parameters relevant\nto realistic scenarios, and a large scale example of cellular swelling on a\ndense reconstruction of the mouse visual cortex highlights the method's\npotential for investigating complex physiological processes like cellular\nvolume regulation in detailed biological structures.", "published": "2025-05-25 14:14:45", "link": "http://arxiv.org/abs/2505.19157v1", "categories": ["math.NA", "cs.NA", "65F10, 35B35"], "primary_category": "math.NA"}
{"title": "State-based nested iteration solution of optimal control problems with PDE constraints", "abstract": "We consider an abstract framework for the numerical solution of optimal\ncontrol problems (OCPs) subject to partial differential equations (PDEs).\nExamples include not only the distributed control of elliptic PDEs such as the\nPoisson equation discussed in this paper in detail but also parabolic and\nhyperbolic equations. The approach covers the standard $L^2$ setting as well as\nthe more recent energy regularization, also including state and control\nconstraints. We discretize OCPs subject to parabolic or hyperbolic PDEs by\nmeans of space-time finite elements similar as in the elliptic case. We discuss\nregularization and finite element error estimates, and derive an optimal\nrelation between the regularization parameter and the finite element mesh size\nin order to balance the accuracy, and the energy costs for the corresponding\ncontrol. Finally, we also discuss the efficient solution of the resulting\nsystems of algebraic equations, and their use in a state-based nested iteration\nprocedure that allows us to compute finite element approximations to the state\nand the control in asymptotically optimal complexity. The numerical results\nillustrate the theoretical findings quantitatively.", "published": "2025-05-25 09:37:37", "link": "http://arxiv.org/abs/2505.19062v1", "categories": ["math.NA", "cs.NA", "49J20, 49M05, 35J05, 65M60, 65N22, 65F10"], "primary_category": "math.NA"}
{"title": "Weak Physics Informed Neural Networks for Geometry Compatible Hyperbolic Conservation Laws on Manifolds", "abstract": "Physics-informed neural networks (PINNs), owing to their mesh-free nature,\noffer a powerful approach for solving high-dimensional partial differential\nequations (PDEs) in complex geometries, including irregular domains. This\ncapability effectively circumvents the challenges of mesh generation that\ntraditional numerical methods face in high-dimensional or geometrically\nintricate settings. While recent studies have extended PINNs to manifolds, the\ntheoretical foundations remain scarce. Existing theoretical analyses of PINNs\nin Euclidean space often rely on smoothness assumptions for the solutions.\nHowever, recent empirical evidence indicates that PINNs may struggle to\napproximate solutions with low regularity, such as those arising from nonlinear\nhyperbolic equations. In this paper, we develop a framework for PINNs tailored\nto the efficient approximation of weak solutions, particularly nonlinear\nhyperbolic equations defined on manifolds. We introduce a novel weak PINN\n(wPINN) formulation on manifolds that leverages the well-posedness theory to\napproximate entropy solutions of geometry-compatible hyperbolic conservation\nlaws on manifolds. Employing tools from approximation theory, we establish a\nconvergence analysis of the algorithm, including an analysis of approximation\nerrors for time-dependent entropy solutions. This analysis provides insight\ninto the accumulation of approximation errors over long time horizons. Notably,\nthe network complexity depends only on the intrinsic dimension, independent of\nthe ambient space dimension. Our results match the minimax rate in the\nd-dimensional Euclidean space, demonstrating that PINNs can alleviate the curse\nof dimensionality in the context of low-dimensional manifolds. Finally, we\nvalidate the performance of the proposed wPINN framework through numerical\nexperiments, confirming its ability to efficiently approximate entropy\nsolutions on manifolds.", "published": "2025-05-25 08:36:56", "link": "http://arxiv.org/abs/2505.19036v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Error estimates for the interpolation and approximation of gradients and vector fields on protected Delaunay meshes in $\\mathbb{R}^d$", "abstract": "One frequently needs to interpolate or approximate gradients on simplicial\nmeshes. Unfortunately, there are very few explicit mathematical results\ngoverning the interpolation or approximation of vector-valued functions on\nDelaunay meshes in more than two dimensions. Most of the existing results are\ntailored towards interpolation with piecewise linear polynomials. In contrast,\ninterpolation with piecewise high-order polynomials is not well understood. In\nparticular, the results in this area are sometimes difficult to immediately\ninterpret, or to specialize to the Delaunay setting. In order to address this\nissue, we derive explicit error estimates for high-order, piecewise polynomial\ngradient interpolation and approximation on protected Delaunay meshes. In\naddition, we generalize our analysis beyond gradients, and obtain error\nestimates for sufficiently-smooth vector fields. Throughout the paper, we show\nthat the quality of interpolation and approximation often depends (in part) on\nthe minimum thickness of simplices in the mesh. Fortunately, the minimum\nthickness can be precisely controlled on protected Delaunay meshes in\n$\\mathbb{R}^d$.", "published": "2025-05-25 05:50:37", "link": "http://arxiv.org/abs/2505.18987v1", "categories": ["math.NA", "cs.NA", "65N50, 65N15, 65N30"], "primary_category": "math.NA"}
{"title": "Comparative analysis of financial data differentiation techniques using LSTM neural network", "abstract": "We compare traditional approach of computing logarithmic returns with the\nfractional differencing method and its tempered extension as methods of data\npreparation before their usage in advanced machine learning models.\nDifferencing parameters are estimated using multiple techniques. The empirical\ninvestigation is conducted on data from four major stock indices covering the\nmost recent 10-year period. The set of explanatory variables is additionally\nextended with technical indicators. The effectiveness of the differencing\nmethods is evaluated using both forecast error metrics and risk-adjusted return\ntrading performance metrics. The findings suggest that fractional\ndifferentiation methods provide a suitable data transformation technique,\nimproving the predictive model forecasting performance. Furthermore, the\ngenerated predictions appeared to be effective in constructing profitable\ntrading strategies for both individual assets and a portfolio of stock indices.\nThese results underline the importance of appropriate data transformation\ntechniques in financial time series forecasting, supporting the application of\nmemory-preserving techniques.", "published": "2025-05-25 17:49:10", "link": "http://arxiv.org/abs/2505.19243v1", "categories": ["q-fin.ST", "econ.EM", "q-fin.CP", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "A General Theory of Risk Sharing", "abstract": "We introduce a new paradigm for risk sharing that generalizes earlier models\nbased on discrete agents and extends them to allow for sharing risk within a\ncontinuum of agents. Agents are represented by points of a measure space and\nhave potentially heterogeneous risk preferences modeled by risk measures. The\nexistence of risk minimizing allocations is proved when constrained to satisfy\neconomically convincing conditions. In the unconstrained case, we derive the\ndual representation of the value function using a Strassen-type theorem for the\nweak-star topology. These results are illustrated by explicit formulas when\nrisk preferences are within the family of entropic and expected shortfall risk\nmeasures.", "published": "2025-05-25 19:17:53", "link": "http://arxiv.org/abs/2505.19276v1", "categories": ["q-fin.RM", "econ.TH", "q-fin.MF", "91B05, 91G70"], "primary_category": "q-fin.RM"}
{"title": "Adaptive Diffusion Guidance via Stochastic Optimal Control", "abstract": "Guidance is a cornerstone of modern diffusion models, playing a pivotal role\nin conditional generation and enhancing the quality of unconditional samples.\nHowever, current approaches to guidance scheduling--determining the appropriate\nguidance weight--are largely heuristic and lack a solid theoretical foundation.\nThis work addresses these limitations on two fronts. First, we provide a\ntheoretical formalization that precisely characterizes the relationship between\nguidance strength and classifier confidence. Second, building on this insight,\nwe introduce a stochastic optimal control framework that casts guidance\nscheduling as an adaptive optimization problem. In this formulation, guidance\nstrength is not fixed but dynamically selected based on time, the current\nsample, and the conditioning class, either independently or in combination. By\nsolving the resulting control problem, we establish a principled foundation for\nmore effective guidance in diffusion models.", "published": "2025-05-25 23:34:10", "link": "http://arxiv.org/abs/2505.19367v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "PIGPVAE: Physics-Informed Gaussian Process Variational Autoencoders", "abstract": "Recent advances in generative AI offer promising solutions for synthetic data\ngeneration but often rely on large datasets for effective training. To address\nthis limitation, we propose a novel generative model that learns from limited\ndata by incorporating physical constraints to enhance performance.\nSpecifically, we extend the VAE architecture by incorporating physical models\nin the generative process, enabling it to capture underlying dynamics more\neffectively. While physical models provide valuable insights, they struggle to\ncapture complex temporal dependencies present in real-world data. To bridge\nthis gap, we introduce a discrepancy term to account for unmodeled dynamics,\nrepresented within a latent Gaussian Process VAE (GPVAE). Furthermore, we apply\nregularization to ensure the generated data aligns closely with observed data,\nenhancing both the diversity and accuracy of the synthetic samples. The\nproposed method is applied to indoor temperature data, achieving\nstate-of-the-art performance. Additionally, we demonstrate that PIGPVAE can\nproduce realistic samples beyond the observed distribution, highlighting its\nrobustness and usefulness under distribution shifts.", "published": "2025-05-25 21:12:01", "link": "http://arxiv.org/abs/2505.19320v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search", "abstract": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines.", "published": "2025-05-25 16:13:46", "link": "http://arxiv.org/abs/2505.19209v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Federated Learning: From Theory to Practice", "abstract": "This book offers a hands-on introduction to building and understanding\nfederated learning (FL) systems. FL enables multiple devices -- such as\nsmartphones, sensors, or local computers -- to collaboratively train machine\nlearning (ML) models, while keeping their data private and local. It is a\npowerful solution when data cannot or should not be centralized due to privacy,\nregulatory, or technical reasons. The book is designed for students, engineers,\nand researchers who want to learn how to design scalable, privacy preserving FL\nsystems. Our main focus is on personalization: enabling each device to train\nits own model while still benefiting from collaboration with relevant devices.\nThis is achieved by leveraging similarities between (the learning tasks\nassociated with) devices that are encoded by the weighted edges (or links) of a\nfederated learning network (FL network). The key idea is to represent\nreal-world FL systems as networks of devices, where nodes correspond to device\nand edges represent communication links and data similarities between them. The\ntraining of personalized models for these devices can be naturally framed as a\ndistributed optimization problem. This optimization problem is referred to as\ngeneralized total variation minimization (GTVMin) and ensures that devices with\nsimilar learning tasks learn similar model parameters. Our approach is both\nmathematically principled and practically motivated. While we introduce some\nadvanced ideas from optimization theory and graph-based learning, we aim to\nkeep the book accessible. Readers are guided through the core ideas step by\nstep, with intuitive explanations.", "published": "2025-05-25 15:05:21", "link": "http://arxiv.org/abs/2505.19183v1", "categories": ["cs.LG", "stat.ML", "F.1.1; I.2.11; I.5.3"], "primary_category": "cs.LG"}
{"title": "Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference", "abstract": "Uncertainty quantification (UQ) in scientific machine learning is\nincreasingly critical as neural networks are widely adopted to tackle complex\nproblems across diverse scientific disciplines. For physics-informed neural\nnetworks (PINNs), a prominent model in scientific machine learning, uncertainty\nis typically quantified using Bayesian or dropout methods. However, both\napproaches suffer from a fundamental limitation: the prior distribution or\ndropout rate required to construct honest confidence sets cannot be determined\nwithout additional information. In this paper, we propose a novel method within\nthe framework of extended fiducial inference (EFI) to provide rigorous\nuncertainty quantification for PINNs. The proposed method leverages a\nnarrow-neck hyper-network to learn the parameters of the PINN and quantify\ntheir uncertainty based on imputed random errors in the observations. This\napproach overcomes the limitations of Bayesian and dropout methods, enabling\nthe construction of honest confidence sets based solely on observed data. This\nadvancement represents a significant breakthrough for PINNs, greatly enhancing\ntheir reliability, interpretability, and applicability to real-world scientific\nand engineering challenges. Moreover, it establishes a new theoretical\nframework for EFI, extending its application to large-scale models, eliminating\nthe need for sparse hyper-networks, and significantly improving the\nautomaticity and robustness of statistical inference.", "published": "2025-05-25 13:18:13", "link": "http://arxiv.org/abs/2505.19136v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Incentivizing High-Quality Human Annotations with Golden Questions", "abstract": "Human-annotated data plays a vital role in training large language models\n(LLMs), such as supervised fine-tuning and human preference alignment. However,\nit is not guaranteed that paid human annotators produce high-quality data. In\nthis paper, we study how to incentivize human annotators to do so. We start\nfrom a principal-agent model to model the dynamics between the company (the\nprincipal) and the annotator (the agent), where the principal can only monitor\nthe annotation quality by examining $n$ samples. We investigate the maximum\nlikelihood estimators (MLE) and the corresponding hypothesis testing to\nincentivize annotators: the agent is given a bonus if the MLE passes the test.\nBy analyzing the variance of the outcome, we show that the strategic behavior\nof the agent makes the hypothesis testing very different from traditional ones:\nUnlike the exponential rate proved by the large deviation theory, the\nprincipal-agent model's hypothesis testing rate is of $\\Theta(1/\\sqrt{n \\log\nn})$. Our theory implies two criteria for the \\emph{golden questions} to\nmonitor the performance of the annotators: they should be of (1) high certainty\nand (2) similar format to normal ones. In that light, we select a set of golden\nquestions in human preference data. By doing incentive-compatible experiments,\nwe find out that the annotators' behavior is better revealed by those golden\nquestions, compared to traditional survey techniques such as instructed\nmanipulation checks.", "published": "2025-05-25 13:11:55", "link": "http://arxiv.org/abs/2505.19134v1", "categories": ["cs.GT", "cs.LG", "stat.ML"], "primary_category": "cs.GT"}
{"title": "Statistical inference for Linear Stochastic Approximation with Markovian Noise", "abstract": "In this paper we derive non-asymptotic Berry-Esseen bounds for Polyak-Ruppert\naveraged iterates of the Linear Stochastic Approximation (LSA) algorithm driven\nby the Markovian noise. Our analysis yields $\\mathcal{O}(n^{-1/4})$ convergence\nrates to the Gaussian limit in the Kolmogorov distance. We further establish\nthe non-asymptotic validity of a multiplier block bootstrap procedure for\nconstructing the confidence intervals, guaranteeing consistent inference under\nMarkovian sampling. Our work provides the first non-asymptotic guarantees on\nthe rate of convergence of bootstrap-based confidence intervals for stochastic\napproximation with Markov noise. Moreover, we recover the classical rate of\norder $\\mathcal{O}(n^{-1/8})$ up to logarithmic factors for estimating the\nasymptotic variance of the iterates of the LSA algorithm.", "published": "2025-05-25 11:43:28", "link": "http://arxiv.org/abs/2505.19102v1", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.TH", "60F05, 62L20, 62E20"], "primary_category": "stat.ML"}
{"title": "Towards Robust Influence Functions with Flat Validation Minima", "abstract": "The Influence Function (IF) is a widely used technique for assessing the\nimpact of individual training samples on model predictions. However, existing\nIF methods often fail to provide reliable influence estimates in deep neural\nnetworks, particularly when applied to noisy training data. This issue does not\nstem from inaccuracies in parameter change estimation, which has been the\nprimary focus of prior research, but rather from deficiencies in loss change\nestimation, specifically due to the sharpness of validation risk. In this work,\nwe establish a theoretical connection between influence estimation error,\nvalidation set risk, and its sharpness, underscoring the importance of flat\nvalidation minima for accurate influence estimation. Furthermore, we introduce\na novel estimation form of Influence Function specifically designed for flat\nvalidation minima. Experimental results across various tasks validate the\nsuperiority of our approach.", "published": "2025-05-25 11:20:28", "link": "http://arxiv.org/abs/2505.19097v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random", "abstract": "Model-based clustering integrated with variable selection is a powerful tool\nfor uncovering latent structures within complex data. However, its\neffectiveness is often hindered by challenges such as identifying relevant\nvariables that define heterogeneous subgroups and handling data that are\nmissing not at random, a prevalent issue in fields like transcriptomics. While\nseveral notable methods have been proposed to address these problems, they\ntypically tackle each issue in isolation, thereby limiting their flexibility\nand adaptability. This paper introduces a unified framework designed to address\nthese challenges simultaneously. Our approach incorporates a data-driven\npenalty matrix into penalized clustering to enable more flexible variable\nselection, along with a mechanism that explicitly models the relationship\nbetween missingness and latent class membership. We demonstrate that, under\ncertain regularity conditions, the proposed framework achieves both asymptotic\nconsistency and selection consistency, even in the presence of missing data.\nThis unified strategy significantly enhances the capability and efficiency of\nmodel-based clustering, advancing methodologies for identifying informative\nvariables that define homogeneous subgroups in the presence of complex missing\ndata patterns. The performance of the framework, including its computational\nefficiency, is evaluated through simulations and demonstrated using both\nsynthetic and real-world transcriptomic datasets.", "published": "2025-05-25 11:08:43", "link": "http://arxiv.org/abs/2505.19093v1", "categories": ["stat.ME", "cs.LG", "math.ST", "stat.AP", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations", "abstract": "Recent advances in lightweight time series forecasting models suggest the\ninherent simplicity of time series forecasting tasks. In this paper, we present\nCMoS, a super-lightweight time series forecasting model. Instead of learning\nthe embedding of the shapes, CMoS directly models the spatial correlations\nbetween different time series chunks. Additionally, we introduce a Correlation\nMixing technique that enables the model to capture diverse spatial correlations\nwith minimal parameters, and an optional Periodicity Injection technique to\nensure faster convergence. Despite utilizing as low as 1% of the lightweight\nmodel DLinear's parameters count, experimental results demonstrate that CMoS\noutperforms existing state-of-the-art models across multiple datasets.\nFurthermore, the learned weights of CMoS exhibit great interpretability,\nproviding practitioners with valuable insights into temporal structures within\nspecific application scenarios.", "published": "2025-05-25 11:01:53", "link": "http://arxiv.org/abs/2505.19090v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes", "abstract": "We analyze the generalization gap (gap between the training and test errors)\nwhen training a potentially over-parametrized model using a Markovian\nstochastic training algorithm, initialized from some distribution $\\theta_0\n\\sim p_0$. We focus on Langevin dynamics with a positive temperature\n$\\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal\nstep size, perturbed with $\\beta^{-1}$-variances Gaussian noise, and lightly\nregularized or bounded. There, we bound the generalization gap, at any time\nduring training, by $\\sqrt{(\\beta\\mathbb{E} L (\\theta_0) + \\log(1/\\delta))/N}$\nwith probability $1-\\delta$ over the dataset, where $N$ is the sample size, and\n$\\mathbb{E} L (\\theta_0) =O(1)$ with standard initialization scaling. In\ncontrast to previous guarantees, we have no dependence on either training time\nor reliance on mixing, nor a dependence on dimensionality, gradient norms, or\nany other properties of the loss or model. This guarantee follows from a\ngeneral analysis of any Markov process-based training that has a Gibbs-style\nstationary distribution. The proof is surprisingly simple, once we observe that\nthe marginal distribution divergence from initialization remains bounded, as\nimplied by a generalized second law of thermodynamics.", "published": "2025-05-25 10:49:09", "link": "http://arxiv.org/abs/2505.19087v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Distributionally Robust Deep Q-Learning", "abstract": "We propose a novel distributionally robust $Q$-learning algorithm for the\nnon-tabular case accounting for continuous state spaces where the state\ntransition of the underlying Markov decision process is subject to model\nuncertainty. The uncertainty is taken into account by considering the\nworst-case transition from a ball around a reference probability measure. To\ndetermine the optimal policy under the worst-case state transition, we solve\nthe associated non-linear Bellman equation by dualising and regularising the\nBellman operator with the Sinkhorn distance, which is then parameterized with\ndeep neural networks. This approach allows us to modify the Deep Q-Network\nalgorithm to optimise for the worst case state transition.\n  We illustrate the tractability and effectiveness of our approach through\nseveral applications, including a portfolio optimisation task based on\nS\\&{P}~500 data.", "published": "2025-05-25 09:22:06", "link": "http://arxiv.org/abs/2505.19058v1", "categories": ["cs.LG", "math.OC", "q-fin.PM", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Structured Reinforcement Learning for Combinatorial Decision-Making", "abstract": "Reinforcement learning (RL) is increasingly applied to real-world problems\ninvolving complex and structured decisions, such as routing, scheduling, and\nassortment planning. These settings challenge standard RL algorithms, which\nstruggle to scale, generalize, and exploit structure in the presence of\ncombinatorial action spaces. We propose Structured Reinforcement Learning\n(SRL), a novel actor-critic framework that embeds combinatorial optimization\nlayers into the actor neural network. We enable end-to-end learning of the\nactor via Fenchel-Young losses and provide a geometric interpretation of SRL as\na primal-dual algorithm in the dual of the moment polytope. Across six\nenvironments with exogenous and endogenous uncertainty, SRL matches or\nsurpasses the performance of unstructured RL and imitation learning on static\ntasks and improves over these baselines by up to 92% on dynamic problems, with\nimproved stability and convergence speed.", "published": "2025-05-25 09:17:10", "link": "http://arxiv.org/abs/2505.19053v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "When Models Don't Collapse: On the Consistency of Iterative MLE", "abstract": "The widespread use of generative models has created a feedback loop, in which\neach generation of models is trained on data partially produced by its\npredecessors. This process has raised concerns about \\emph{model collapse}: A\ncritical degradation in performance caused by repeated training on synthetic\ndata. However, different analyses in the literature have reached different\nconclusions as to the severity of model collapse. As such, it remains unclear\nhow concerning this phenomenon is, and under which assumptions it can be\navoided. To address this, we theoretically study model collapse for maximum\nlikelihood estimation (MLE), in a natural setting where synthetic data is\ngradually added to the original data set. Under standard assumptions (similar\nto those long used for proving asymptotic consistency and normality of MLE), we\nestablish non-asymptotic bounds showing that collapse can be avoided even as\nthe fraction of real data vanishes. On the other hand, we prove that some\nassumptions (beyond MLE consistency) are indeed necessary: Without them, model\ncollapse can occur arbitrarily quickly, even when the original data is still\npresent in the training set. To the best of our knowledge, these are the first\nrigorous examples of iterative generative modeling with accumulating data that\nrapidly leads to model collapse.", "published": "2025-05-25 08:50:46", "link": "http://arxiv.org/abs/2505.19046v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Bayesian sparse modeling for interpretable prediction of hydroxide ion conductivity in anion-conductive polymer membranes", "abstract": "Anion-conductive polymer membranes have attracted considerable attention as\nsolid electrolytes for alkaline fuel cells and electrolysis cells. Their\nhydroxide ion conductivity varies depending on factors such as the type and\ndistribution of quaternary ammonium groups, as well as the structure and\nconnectivity of hydrophilic and hydrophobic domains. In particular, the size\nand connectivity of hydrophilic domains significantly influence the mobility of\nhydroxide ions; however, this relationship has remained largely qualitative. In\nthis study, we calculated the number of key constituent elements in the\nhydrophilic and hydrophobic units based on the copolymer composition, and\ninvestigated their relationship with hydroxide ion conductivity by using\nBayesian sparse modeling. As a result, we successfully identified\ncomposition-derived features that are critical for accurately predicting\nhydroxide ion conductivity.", "published": "2025-05-25 08:46:32", "link": "http://arxiv.org/abs/2505.19044v1", "categories": ["cond-mat.soft", "stat.AP", "stat.ML"], "primary_category": "cond-mat.soft"}
{"title": "Offline Clustering of Linear Bandits: Unlocking the Power of Clusters in Data-Limited Environments", "abstract": "Contextual linear multi-armed bandits are a learning framework for making a\nsequence of decisions, e.g., advertising recommendations for a sequence of\narriving users. Recent works have shown that clustering these users based on\nthe similarity of their learned preferences can significantly accelerate the\nlearning. However, prior work has primarily focused on the online setting,\nwhich requires continually collecting user data, ignoring the offline data\nwidely available in many applications. To tackle these limitations, we study\nthe offline clustering of bandits (Off-ClusBand) problem, which studies how to\nuse the offline dataset to learn cluster properties and improve decision-making\nacross multiple users. The key challenge in Off-ClusBand arises from data\ninsufficiency for users: unlike the online case, in the offline case, we have a\nfixed, limited dataset to work from and thus must determine whether we have\nenough data to confidently cluster users together. To address this challenge,\nwe propose two algorithms: Off-C$^2$LUB, which we analytically show performs\nwell for arbitrary amounts of user data, and Off-CLUB, which is prone to bias\nwhen data is limited but, given sufficient data, matches a theoretical lower\nbound that we derive for the offline clustered MAB problem. We experimentally\nvalidate these results on both real and synthetic datasets.", "published": "2025-05-25 08:43:40", "link": "http://arxiv.org/abs/2505.19043v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Optimal Conformal Prediction under Epistemic Uncertainty", "abstract": "Conformal prediction (CP) is a popular frequentist framework for representing\nuncertainty by providing prediction sets that guarantee coverage of the true\nlabel with a user-adjustable probability. In most applications, CP operates on\nconfidence scores coming from a standard (first-order) probabilistic predictor\n(e.g., softmax outputs). Second-order predictors, such as credal set predictors\nor Bayesian models, are also widely used for uncertainty quantification and are\nknown for their ability to represent both aleatoric and epistemic uncertainty.\nDespite their popularity, there is still an open question on ``how they can be\nincorporated into CP''. In this paper, we discuss the desiderata for CP when\nvalid second-order predictions are available. We then introduce Bernoulli\nprediction sets (BPS), which produce the smallest prediction sets that ensure\nconditional coverage in this setting. When given first-order predictions, BPS\nreduces to the well-known adaptive prediction sets (APS). Furthermore, when the\nvalidity assumption on the second-order predictions is compromised, we apply\nconformal risk control to obtain a marginal coverage guarantee while still\naccounting for epistemic uncertainty.", "published": "2025-05-25 08:32:44", "link": "http://arxiv.org/abs/2505.19033v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Learn Beneficial Noise as Graph Augmentation", "abstract": "Although graph contrastive learning (GCL) has been widely investigated, it is\nstill a challenge to generate effective and stable graph augmentations.\nExisting methods often apply heuristic augmentation like random edge dropping,\nwhich may disrupt important graph structures and result in unstable GCL\nperformance. In this paper, we propose Positive-incentive Noise driven Graph\nData Augmentation (PiNGDA), where positive-incentive noise (pi-noise)\nscientifically analyzes the beneficial effect of noise under the information\ntheory. To bridge the standard GCL and pi-noise framework, we design a Gaussian\nauxiliary variable to convert the loss function to information entropy. We\nprove that the standard GCL with pre-defined augmentations is equivalent to\nestimate the beneficial noise via the point estimation. Following our analysis,\nPiNGDA is derived from learning the beneficial noise on both topology and\nattributes through a trainable noise generator for graph augmentations, instead\nof the simple estimation. Since the generator learns how to produce beneficial\nperturbations on graph topology and node attributes, PiNGDA is more reliable\ncompared with the existing methods. Extensive experimental results validate the\neffectiveness and stability of PiNGDA.", "published": "2025-05-25 08:20:34", "link": "http://arxiv.org/abs/2505.19024v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Faithful Group Shapley Value", "abstract": "Data Shapley is an important tool for data valuation, which quantifies the\ncontribution of individual data points to machine learning models. In practice,\ngroup-level data valuation is desirable when data providers contribute data in\nbatch. However, we identify that existing group-level extensions of Data\nShapley are vulnerable to shell company attacks, where strategic group\nsplitting can unfairly inflate valuations. We propose Faithful Group Shapley\nValue (FGSV) that uniquely defends against such attacks. Building on original\nmathematical insights, we develop a provably fast and accurate approximation\nalgorithm for computing FGSV. Empirical experiments demonstrate that our\nalgorithm significantly outperforms state-of-the-art methods in computational\nefficiency and approximation accuracy, while ensuring faithful group-level\nvaluation.", "published": "2025-05-25 07:32:12", "link": "http://arxiv.org/abs/2505.19013v1", "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Semi-pessimistic Reinforcement Learning", "abstract": "Offline reinforcement learning (RL) aims to learn an optimal policy from\npre-collected data. However, it faces challenges of distributional shift, where\nthe learned policy may encounter unseen scenarios not covered in the offline\ndata. Additionally, numerous applications suffer from a scarcity of labeled\nreward data. Relying on labeled data alone often leads to a narrow state-action\ndistribution, further amplifying the distributional shift, and resulting in\nsuboptimal policy learning. To address these issues, we first recognize that\nthe volume of unlabeled data is typically substantially larger than that of\nlabeled data. We then propose a semi-pessimistic RL method to effectively\nleverage abundant unlabeled data. Our approach offers several advantages. It\nconsiderably simplifies the learning process, as it seeks a lower bound of the\nreward function, rather than that of the Q-function or state transition\nfunction. It is highly flexible, and can be integrated with a range of\nmodel-free and model-based RL algorithms. It enjoys the guaranteed improvement\nwhen utilizing vast unlabeled data, but requires much less restrictive\nconditions. We compare our method with a number of alternative solutions, both\nanalytically and numerically, and demonstrate its clear competitiveness. We\nfurther illustrate with an application to adaptive deep brain stimulation for\nParkinson's disease.", "published": "2025-05-25 06:47:36", "link": "http://arxiv.org/abs/2505.19002v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Automatic and Structure-Aware Sparsification of Hybrid Neural ODEs", "abstract": "Hybrid neural ordinary differential equations (neural ODEs) integrate\nmechanistic models with neural ODEs, offering strong inductive bias and\nflexibility, and are particularly advantageous in data-scarce healthcare\nsettings. However, excessive latent states and interactions from mechanistic\nmodels can lead to training inefficiency and over-fitting, limiting practical\neffectiveness of hybrid neural ODEs. In response, we propose a new hybrid\npipeline for automatic state selection and structure optimization in\nmechanistic neural ODEs, combining domain-informed graph modifications with\ndata-driven regularization to sparsify the model for improving predictive\nperformance and stability while retaining mechanistic plausibility. Experiments\non synthetic and real-world data show improved predictive performance and\nrobustness with desired sparsity, establishing an effective solution for hybrid\nmodel reduction in healthcare applications.", "published": "2025-05-25 06:36:30", "link": "http://arxiv.org/abs/2505.18996v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ALPCAHUS: Subspace Clustering for Heteroscedastic Data", "abstract": "Principal component analysis (PCA) is a key tool in the field of data\ndimensionality reduction. Various methods have been proposed to extend PCA to\nthe union of subspace (UoS) setting for clustering data that come from multiple\nsubspaces like K-Subspaces (KSS). However, some applications involve\nheterogeneous data that vary in quality due to noise characteristics associated\nwith each data sample. Heteroscedastic methods aim to deal with such mixed data\nquality. This paper develops a heteroscedastic-focused subspace clustering\nmethod, named ALPCAHUS, that can estimate the sample-wise noise variances and\nuse this information to improve the estimate of the subspace bases associated\nwith the low-rank structure of the data. This clustering algorithm builds on\nK-Subspaces (KSS) principles by extending the recently proposed heteroscedastic\nPCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS\nsetting. Simulations and real-data experiments show the effectiveness of\naccounting for data heteroscedasticity compared to existing clustering\nalgorithms. Code available at https://github.com/javiersc1/ALPCAHUS.", "published": "2025-05-25 00:56:08", "link": "http://arxiv.org/abs/2505.18918v1", "categories": ["stat.ML", "cs.LG", "eess.SP"], "primary_category": "stat.ML"}
{"title": "On the Role of Label Noise in the Feature Learning Process", "abstract": "Deep learning with noisy labels presents significant challenges. In this\nwork, we theoretically characterize the role of label noise from a feature\nlearning perspective. Specifically, we consider a signal-noise data\ndistribution, where each sample comprises a label-dependent signal and\nlabel-independent noise, and rigorously analyze the training dynamics of a\ntwo-layer convolutional neural network under this data setup, along with the\npresence of label noise. Our analysis identifies two key stages. In Stage I,\nthe model perfectly fits all the clean samples (i.e., samples without label\nnoise) while ignoring the noisy ones (i.e., samples with noisy labels). During\nthis stage, the model learns the signal from the clean samples, which\ngeneralizes well on unseen data. In Stage II, as the training loss converges,\nthe gradient in the direction of noise surpasses that of the signal, leading to\noverfitting on noisy samples. Eventually, the model memorizes the noise present\nin the noisy samples and degrades its generalization ability. Furthermore, our\nanalysis provides a theoretical basis for two widely used techniques for\ntackling label noise: early stopping and sample selection. Experiments on both\nsynthetic and real-world setups validate our theory.", "published": "2025-05-25 00:13:28", "link": "http://arxiv.org/abs/2505.18909v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline", "abstract": "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from\na mixture of multiple speakers by leveraging speaker-specific cues, typically\nprovided as auxiliary audio (a.k.a. cue audio). Although recent advancements in\nTSE have primarily employed discriminative models that offer high perceptual\nquality, these models often introduce unwanted artifacts, reduce naturalness,\nand are sensitive to discrepancies between training and testing environments.\nOn the other hand, generative models for TSE lag in perceptual quality and\nintelligibility. To address these challenges, we present SoloSpeech, a novel\ncascaded generative pipeline that integrates compression, extraction,\nreconstruction, and correction processes. SoloSpeech features a\nspeaker-embedding-free target extractor that utilizes conditional information\nfrom the cue audio's latent space, aligning it with the mixture audio's latent\nspace to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset,\nSoloSpeech achieves the new state-of-the-art intelligibility and quality in\ntarget speech extraction and speech separation tasks while demonstrating\nexceptional generalization on out-of-domain data and real-world scenarios.", "published": "2025-05-25 21:00:48", "link": "http://arxiv.org/abs/2505.19314v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Reliable Large Audio Language Model", "abstract": "Recent advancements in large audio language models (LALMs) have demonstrated\nimpressive results and promising prospects in universal understanding and\nreasoning across speech, music, and general sound. However, these models still\nlack the ability to recognize their knowledge boundaries and refuse to answer\nquestions they don't know proactively. While there have been successful\nattempts to enhance the reliability of LLMs, reliable LALMs remain largely\nunexplored. In this paper, we systematically investigate various approaches\ntowards reliable LALMs, including training-free methods such as multi-modal\nchain-of-thought (MCoT), and training-based methods such as supervised\nfine-tuning (SFT). Besides, we identify the limitations of previous evaluation\nmetrics and propose a new metric, the Reliability Gain Index (RGI), to assess\nthe effectiveness of different reliable methods. Our findings suggest that both\ntraining-free and training-based methods enhance the reliability of LALMs to\ndifferent extents. Moreover, we find that awareness of reliability is a \"meta\nability\", which can be transferred across different audio modalities, although\nsignificant structural and content differences exist among sound, music, and\nspeech.", "published": "2025-05-25 20:00:31", "link": "http://arxiv.org/abs/2505.19294v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Eta-WavLM: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation", "abstract": "Self-supervised learning (SSL) has reduced the reliance on expensive labeling\nin speech technologies by learning meaningful representations from unannotated\ndata. Since most SSL-based downstream tasks prioritize content information in\nspeech, ideal representations should disentangle content from unwanted\nvariations like speaker characteristics in the SSL representations. However,\nremoving speaker information often degrades other speech components, and\nexisting methods either fail to fully disentangle speaker identity or require\nresource-intensive models. In this paper, we propose a novel disentanglement\nmethod that linearly decomposes SSL representations into speaker-specific and\nspeaker-independent components, effectively generating speaker disentangled\nrepresentations. Comprehensive experiments show that our approach achieves\nspeaker independence and as such, when applied to content-driven tasks such as\nvoice conversion, our representations yield significant improvements over\nstate-of-the-art methods.", "published": "2025-05-25 19:05:26", "link": "http://arxiv.org/abs/2505.19273v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpeakStream: Streaming Text-to-Speech with Interleaved Data", "abstract": "The latency bottleneck of traditional text-to-speech (TTS) systems\nfundamentally hinders the potential of streaming large language models (LLMs)\nin conversational AI. These TTS systems, typically trained and inferenced on\ncomplete utterances, introduce unacceptable delays, even with optimized\ninference speeds, when coupled with streaming LLM outputs. This is particularly\nproblematic for creating responsive conversational agents where low first-token\nlatency is critical. In this paper, we present SpeakStream, a streaming TTS\nsystem that generates audio incrementally from streaming text using a\ndecoder-only architecture. SpeakStream is trained using a next-step prediction\nloss on interleaved text-speech data. During inference, it generates speech\nincrementally while absorbing streaming input text, making it particularly\nsuitable for cascaded conversational AI agents where an LLM streams text to a\nTTS system. Our experiments demonstrate that SpeakStream achieves\nstate-of-the-art latency results in terms of first-token latency while\nmaintaining the quality of non-streaming TTS systems.", "published": "2025-05-25 16:11:10", "link": "http://arxiv.org/abs/2505.19206v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EnvSDD: Benchmarking Environmental Sound Deepfake Detection", "abstract": "Audio generation systems now create very realistic soundscapes that can\nenhance media production, but also pose potential risks. Several studies have\nexamined deepfakes in speech or singing voice. However, environmental sounds\nhave different characteristics, which may make methods for detecting speech and\nsinging deepfakes less effective for real-world sounds. In addition, existing\ndatasets for environmental sound deepfake detection are limited in scale and\naudio types. To address this gap, we introduce EnvSDD, the first large-scale\ncurated dataset designed for this task, consisting of 45.25 hours of real and\n316.74 hours of fake audio. The test set includes diverse conditions to\nevaluate the generalizability, such as unseen generation models and unseen\ndatasets. We also propose an audio deepfake detection system, based on a\npre-trained audio foundation model. Results on EnvSDD show that our proposed\nsystem outperforms the state-of-the-art systems from speech and singing\ndomains.", "published": "2025-05-25 16:02:56", "link": "http://arxiv.org/abs/2505.19203v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BR-ASR: Efficient and Scalable Bias Retrieval Framework for Contextual Biasing ASR in Speech LLM", "abstract": "While speech large language models (SpeechLLMs) have advanced standard\nautomatic speech recognition (ASR), contextual biasing for named entities and\nrare words remains challenging, especially at scale. To address this, we\npropose BR-ASR: a Bias Retrieval framework for large-scale contextual biasing\n(up to 200k entries) via two innovations: (1) speech-and-bias contrastive\nlearning to retrieve semantically relevant candidates; (2) dynamic curriculum\nlearning that mitigates homophone confusion which negatively impacts the final\nperformance. The is a general framework that allows seamless integration of the\nretrieved candidates into diverse ASR systems without fine-tuning. Experiments\non LibriSpeech test-clean/-other achieve state-of-the-art (SOTA) biased word\nerror rates (B-WER) of 2.8%/7.1% with 2000 bias words, delivering 45% relative\nimprovement over prior methods. BR-ASR also demonstrates high scalability: when\nexpanding the bias list to 200k where traditional methods generally fail, it\ninduces only 0.3 / 2.9% absolute WER / B-WER degradation with a 99.99% pruning\nrate and only 20ms latency per query on test-other.", "published": "2025-05-25 14:57:57", "link": "http://arxiv.org/abs/2505.19179v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "CloneShield: A Framework for Universal Perturbation Against Zero-Shot Voice Cloning", "abstract": "Recent breakthroughs in text-to-speech (TTS) voice cloning have raised\nserious privacy concerns, allowing highly accurate vocal identity replication\nfrom just a few seconds of reference audio, while retaining the speaker's vocal\nauthenticity. In this paper, we introduce CloneShield, a universal time-domain\nadversarial perturbation framework specifically designed to defend against\nzero-shot voice cloning. Our method provides protection that is robust across\nspeakers and utterances, without requiring any prior knowledge of the\nsynthesized text. We formulate perturbation generation as a multi-objective\noptimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to\nensure the robust protection across diverse utterances. To preserve natural\nauditory perception for users, we decompose the adversarial perturbation via\nMel-spectrogram representations and fine-tune it for each sample. This design\nensures imperceptibility while maintaining strong degradation effects on\nzero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS\nsystems, five benchmark datasets and evaluations from 60 human listeners\ndemonstrate that our method preserves near-original audio quality in protected\ninputs (PESQ = 3.90, SRS = 0.93) while substantially degrading both speaker\nsimilarity and speech quality in cloned samples (PESQ = 1.07, SRS = 0.08).", "published": "2025-05-25 12:22:00", "link": "http://arxiv.org/abs/2505.19119v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WHISTRESS: Enriching Transcriptions with Sentence Stress Detection", "abstract": "Spoken language conveys meaning not only through words but also through\nintonation, emotion, and emphasis. Sentence stress, the emphasis placed on\nspecific words within a sentence, is crucial for conveying speaker intent and\nhas been extensively studied in linguistics. In this work, we introduce\nWHISTRESS, an alignment-free approach for enhancing transcription systems with\nsentence stress detection. To support this task, we propose TINYSTRESS-15K, a\nscalable, synthetic training data for the task of sentence stress detection\nwhich resulted from a fully automated dataset creation process. We train\nWHISTRESS on TINYSTRESS-15K and evaluate it against several competitive\nbaselines. Our results show that WHISTRESS outperforms existing methods while\nrequiring no additional input priors during training or inference. Notably,\ndespite being trained on synthetic data, WHISTRESS demonstrates strong\nzero-shot generalization across diverse benchmarks. Project page:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/whistress.", "published": "2025-05-25 11:45:08", "link": "http://arxiv.org/abs/2505.19103v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models", "abstract": "We introduce Speech-IFeval, an evaluation framework designed to assess\ninstruction-following capabilities and quantify catastrophic forgetting in\nspeech-aware language models (SLMs). Recent SLMs integrate speech perception\nwith large language models (LLMs), often degrading textual capabilities due to\nspeech-centric training. Existing benchmarks conflate speech perception with\ninstruction-following, hindering evaluation of these distinct skills. To\naddress this gap, we provide a benchmark for diagnosing the\ninstruction-following abilities of SLMs. Our findings show that most SLMs\nstruggle with even basic instructions, performing far worse than text-based\nLLMs. Additionally, these models are highly sensitive to prompt variations,\noften yielding inconsistent and unreliable outputs. We highlight core\nchallenges and provide insights to guide future research, emphasizing the need\nfor evaluation beyond task-level metrics.", "published": "2025-05-25 08:37:55", "link": "http://arxiv.org/abs/2505.19037v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Self-supervised learning method using multiple sampling strategies for general-purpose audio representation", "abstract": "We propose a self-supervised learning method using multiple sampling\nstrategies to obtain general-purpose audio representation. Multiple sampling\nstrategies are used in the proposed method to construct contrastive losses from\ndifferent perspectives and learn representations based on them. In this study,\nin addition to the widely used clip-level sampling strategy, we introduce two\nnew strategies, a frame-level strategy and a task-specific strategy. The\nproposed multiple strategies improve the performance of frame-level\nclassification and other tasks like pitch detection, which are not the focus of\nthe conventional single clip-level sampling strategy. We pre-trained the method\non a subset of Audioset and applied it to a downstream task with frozen\nweights. The proposed method improved clip classification, sound event\ndetection, and pitch detection performance by 25%, 20%, and 3.6%.", "published": "2025-05-25 05:36:26", "link": "http://arxiv.org/abs/2505.18984v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Serial-OE: Anomalous sound detection based on serial method with outlier exposure capable of using small amounts of anomalous data for training", "abstract": "We introduce Serial-OE, a new approach to anomalous sound detection (ASD)\nthat leverages small amounts of anomalous data to improve the performance.\nConventional ASD methods rely primarily on the modeling of normal data, due to\nthe cost of collecting anomalous data from various possible types of equipment\nbreakdowns. Our method improves upon existing ASD systems by implementing an\noutlier exposure framework that utilizes normal and pseudo-anomalous data for\ntraining, with the capability to also use small amounts of real anomalous data.\nA comprehensive evaluation using the DCASE2020 Task2 dataset shows that our\nmethod outperforms state-of-the-art ASD models. We also investigate the impact\non performance of using a small amount of anomalous data during training, of\nusing data without machine ID information, and of using contaminated training\ndata. Our experimental results reveal the potential of using a very limited\namount of anomalous data during training to address the limitations of existing\nmethods using only normal data for training due to the scarcity of anomalous\ndata. This study contributes to the field by presenting a method that can be\ndynamically adapted to include anomalous data during the operational phase of\nan ASD system, paving the way for more accurate ASD.", "published": "2025-05-25 05:28:58", "link": "http://arxiv.org/abs/2505.18982v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Anomalous Sound Detection through Pseudo-anomalous Set Selection and Pseudo-label Utilization under Unlabeled Conditions", "abstract": "This paper addresses performance degradation in anomalous sound detection\n(ASD) when neither sufficiently similar machine data nor operational state\nlabels are available. We present an integrated pipeline that combines three\ncomplementary components derived from prior work and extends them to the\nunlabeled ASD setting. First, we adapt an anomaly score based selector to\ncurate external audio data resembling the normal sounds of the target machine.\nSecond, we utilize triplet learning to assign pseudo-labels to unlabeled data,\nenabling finer classification of operational sounds and detection of subtle\nanomalies. Third, we employ iterative training to refine both the\npseudo-anomalous set selection and pseudo-label assignment, progressively\nimproving detection accuracy. Experiments on the DCASE2022-2024 Task 2 datasets\ndemonstrate that, in unlabeled settings, our approach achieves an average AUC\nincrease of over 6.6 points compared to conventional methods. In labeled\nsettings, incorporating external data from the pseudo-anomalous set further\nboosts performance. These results highlight the practicality and robustness of\nour methods in scenarios with scarce machine data and labels, facilitating ASD\ndeployment across diverse industrial settings with minimal annotation effort.", "published": "2025-05-25 05:20:54", "link": "http://arxiv.org/abs/2505.18980v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis", "abstract": "This paper explores multi-modal controllable Text-to-Speech Synthesis (TTS)\nwhere the voice can be generated from face image, and the characteristics of\noutput speech (e.g., pace, noise level, distance, tone, place) can be\ncontrollable with natural text description. Specifically, we aim to mitigate\nthe following three challenges in face-driven TTS systems. 1) To overcome the\nlimited audio quality of audio-visual speech corpora, we propose a training\nmethod that additionally utilizes high-quality audio-only speech corpora. 2) To\ngenerate voices not only from real human faces but also from artistic\nportraits, we propose augmenting the input face image with stylization. 3) To\nconsider one-to-many possibilities in face-to-voice mapping and ensure\nconsistent voice generation at the same time, we propose to first employ\nsampling-based decoding and then use prompting with generated speech samples.\nExperimental results validate the proposed model's effectiveness in face-driven\nvoice synthesis.", "published": "2025-05-25 04:43:17", "link": "http://arxiv.org/abs/2505.18972v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Physics-Informed Deep Learning for Nonlinear Friction Model of Bow-string Interaction", "abstract": "This study investigates the use of an unsupervised, physics-informed deep\nlearning framework to model a one-degree-of-freedom mass-spring system\nsubjected to a nonlinear friction bow force and governed by a set of ordinary\ndifferential equations. Specifically, it examines the application of\nPhysics-Informed Neural Networks (PINNs) and Physics-Informed Deep Operator\nNetworks (PI-DeepONets). Our findings demonstrate that PINNs successfully\naddress the problem across different bow force scenarios, while PI-DeepONets\nperform well under low bow forces but encounter difficulties at higher forces.\nAdditionally, we analyze the Hessian eigenvalue density and visualize the loss\nlandscape. Overall, the presence of large Hessian eigenvalues and sharp minima\nindicates highly ill-conditioned optimization. These results underscore the\npromise of physics-informed deep learning for nonlinear modelling in musical\nacoustics, while also revealing the limitations of relying solely on\nphysics-based approaches to capture complex nonlinearities. We demonstrate that\nPI-DeepONets, with their ability to generalize across varying parameters, are\nwell-suited for sound synthesis. Furthermore, we demonstrate that the\nlimitations of PI-DeepONets under higher forces can be mitigated by integrating\nobservation data within a hybrid supervised-unsupervised framework. This\nsuggests that a hybrid supervised-unsupervised DeepONets framework could be a\npromising direction for future practical applications.", "published": "2025-05-25 02:53:27", "link": "http://arxiv.org/abs/2505.18950v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards a Spatiotemporal Fusion Approach to Precipitation Nowcasting", "abstract": "With the increasing availability of meteorological data from various sensors,\nnumerical models and reanalysis products, the need for efficient data\nintegration methods has become paramount for improving weather forecasts and\nhydrometeorological studies. In this work, we propose a data fusion approach\nfor precipitation nowcasting by integrating data from meteorological and rain\ngauge stations in Rio de Janeiro metropolitan area with ERA5 reanalysis data\nand GFS numerical weather prediction. We employ the spatiotemporal deep\nlearning architecture called STConvS2S, leveraging a structured dataset\ncovering a 9 x 11 grid. The study spans from January 2011 to October 2024, and\nwe evaluate the impact of integrating three surface station systems. Among the\ntested configurations, the fusion-based model achieves an F1-score of 0.2033\nfor forecasting heavy precipitation events (greater than 25 mm/h) at a one-hour\nlead time. Additionally, we present an ablation study to assess the\ncontribution of each station network and propose a refined inference strategy\nfor precipitation nowcasting, integrating the GFS numerical weather prediction\n(NWP) data with in-situ observations.", "published": "2025-05-25 18:27:19", "link": "http://arxiv.org/abs/2505.19258v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Design of a Wearable Parallel Electrical Impedance Imaging System for Healthcare", "abstract": "A wireless wearable electrical impedance tomography (EIT) system has been\ndeveloped using the AD5933 for real-time lung respiration imaging. The system\nuses a current injection method tailored to the human body's impedance\ncharacteristics. It applies a voltage excitation and measures the resulting\ncurrent as the voltage passes through the body. Additionally, measures are\ntaken to suppress the effects of parasitic capacitance, which can lead to\nsignal oscillations and leakage currents. Additionally, to improve data\nacquisition speed, five AD5933 units are used for parallel measurements, with\nmultiple measures taken to ensure high synchronization during the parallel\nacquisition process. The results demonstrate conductivity images generated from\nthe EIT system, with data collected from a water tank, human lungs during\nrespiration, and a human calf at rest, confirming that this portable EIT system\ncan measure biological tissues with high precision and low cost.", "published": "2025-05-25 13:46:18", "link": "http://arxiv.org/abs/2505.19146v1", "categories": ["physics.med-ph", "eess.SP"], "primary_category": "physics.med-ph"}
{"title": "Movable-Element STARS-Assisted Near-Field Wideband Communications", "abstract": "A novel movable-element simultaneously transmitting and reflecting surface\n(ME-STARS)-assisted near-field wideband communication framework is proposed. In\nparticular, the position of each STARS element can be adjusted to combat the\nsignificant wideband beam squint issue in the near field instead of using\ncostly true-time delay components. Four practical ME-STARS element movement\nmodes are proposed, namely region-based (RB), horizontal-based (HB),\nvertical-based (VB), and diagonal-based (DB) modes. Based on this, a near-field\nwideband multi-user downlink communication scenario is considered, where a sum\nrate maximization problem is formulated by jointly optimizing the base station\n(BS) precoding, ME-STARS beamforming, and element positions. To solve this\nintractable problem, a two-layer algorithm is developed. For the inner layer,\nthe block coordinate descent optimization framework is utilized to solve the BS\nprecoding and ME-STARS beamforming in an iterative manner. For the outer layer,\nthe particle swarm optimization-based heuristic search method is employed to\ndetermine the desired element positions. Numerical results show that:1) the\nME-STARSs can effectively address the beam squint for near-field wideband\ncommunications compared to conventional STARSs with fixed element positions; 2)\nthe RB mode achieves the most efficient beam squint effect mitigation, while\nthe DB mode achieves the best trade-off between performance gain and hardware\noverhead; and 3) an increase in the number of ME-STARS elements or BS\nsubcarriers substantially improves the system performance.", "published": "2025-05-25 08:57:29", "link": "http://arxiv.org/abs/2505.19048v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Derivative-Free Position Optimization Approach for Movable Antenna Multi-User Communication Systems", "abstract": "Movable antennas (MAs) have emerged as a disruptive technology in wireless\ncommunications for enhancing spatial degrees of freedom through continuous\nantenna repositioning within predefined regions, thereby creating favorable\nchannel propagation conditions. In this paper, we study the problem of position\noptimization for MA-enabled multi-user MISO systems, where a base station (BS),\nequipped with multiple MAs, communicates with multiple users each equipped with\na single fixed-position antenna (FPA). To circumvent the difficulty of\nacquiring the channel state information (CSI) from the transmitter to the\nreceiver over the entire movable region, we propose a derivative-free approach\nfor MA position optimization. The basic idea is to treat position optimization\nas a closed-box optimization problem and calculate the gradient of the unknown\nobjective function using zeroth-order (ZO) gradient approximation techniques.\nSpecifically, the proposed method does not need to explicitly estimate the\nglobal CSI. Instead, it adaptively refines its next movement based on previous\nmeasurements such that it eventually converges to an optimum or stationary\nsolution. Simulation results show that the proposed derivative-free approach is\nable to achieve higher sample and computational efficiencies than the CSI\nestimation-based position optimization approach, particularly for challenging\nscenarios where the number of multi-path components (MPCs) is large or the\nnumber of pilot signals is limited.", "published": "2025-05-25 07:31:19", "link": "http://arxiv.org/abs/2505.19012v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization", "abstract": "Hyperparameter optimization (HPO) is a critical yet challenging aspect of\nmachine learning model development, significantly impacting model performance\nand generalization. Traditional HPO methods often struggle with high\ndimensionality, complex interdependencies, and computational expense. This\npaper introduces OptiMindTune, a novel multi-agent framework designed to\nintelligently and efficiently optimize hyperparameters. OptiMindTune leverages\nthe collaborative intelligence of three specialized AI agents -- a Recommender\nAgent, an Evaluator Agent, and a Decision Agent -- each powered by Google's\nGemini models. These agents address distinct facets of the HPO problem, from\nmodel selection and hyperparameter suggestion to robust evaluation and\nstrategic decision-making. By fostering dynamic interactions and knowledge\nsharing, OptiMindTune aims to converge to optimal hyperparameter configurations\nmore rapidly and robustly than existing single-agent or monolithic approaches.\nOur framework integrates principles from advanced large language models, and\nadaptive search to achieve scalable and intelligent AutoML. We posit that this\nmulti-agent paradigm offers a promising avenue for tackling the increasing\ncomplexity of modern machine learning model tuning.", "published": "2025-05-25 16:05:41", "link": "http://arxiv.org/abs/2505.19205v2", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Collaborative Agentic AI Needs Interoperability Across Ecosystems", "abstract": "Collaborative agentic AI is projected to transform entire industries by\nenabling AI-powered agents to autonomously perceive, plan, and act within\ndigital environments. Yet, current solutions in this field are all built in\nisolation, and we are rapidly heading toward a landscape of fragmented,\nincompatible ecosystems. In this position paper, we argue that\ninteroperability, achieved by the adoption of minimal standards, is essential\nto ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To\nthis end, we devise a minimal architectural foundation for collaborative\nagentic AI, named Web of Agents, which is composed of four components:\nagent-to-agent messaging, interaction interoperability, state management, and\nagent discovery. Web of Agents adopts existing standards and reuses existing\ninfrastructure where possible. With Web of Agents, we take the first but\ncritical step toward interoperable agentic systems and offer a pragmatic path\nforward before ecosystem fragmentation becomes the norm.", "published": "2025-05-25 14:25:08", "link": "http://arxiv.org/abs/2505.21550v1", "categories": ["cs.NI", "cs.AI", "cs.MA"], "primary_category": "cs.NI"}
