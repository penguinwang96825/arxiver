{"title": "Modelling Verbal Morphology in Nen", "abstract": "Nen verbal morphology is remarkably complex; a transitive verb can take up to\n1,740 unique forms. The combined effect of having a large combinatoric space\nand a low-resource setting amplifies the need for NLP tools. Nen morphology\nutilises distributed exponence - a non-trivial means of mapping form to\nmeaning. In this paper, we attempt to model Nen verbal morphology using\nstate-of-the-art machine learning models for morphological reinflection. We\nexplore and categorise the types of errors these systems generate. Our results\nshow sensitivity to training data composition; different distributions of verb\ntype yield different accuracies (patterning with E-complexity). We also\ndemonstrate the types of patterns that can be inferred from the training data\nthrough the case study of syncretism.", "published": "2020-11-30 01:22:05", "link": "http://arxiv.org/abs/2011.14489v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Curriculum Learning for Low-Resource Neural Machine Translation", "abstract": "Large amounts of data has made neural machine translation (NMT) a big success\nin recent years. But it is still a challenge if we train these models on\nsmall-scale corpora. In this case, the way of using data appears to be more\nimportant. Here, we investigate the effective use of training data for\nlow-resource NMT. In particular, we propose a dynamic curriculum learning (DCL)\nmethod to reorder training samples in training. Unlike previous work, we do not\nuse a static scoring function for reordering. Instead, the order of training\nsamples is dynamically determined in two ways - loss decline and model\ncompetence. This eases training by highlighting easy samples that the current\nmodel has enough competence to learn. We test our DCL method in a\nTransformer-based system. Experimental results show that DCL outperforms\nseveral strong baselines on three low-resource machine translation benchmarks\nand different sized data of WMT' 16 En-De.", "published": "2020-11-30 08:13:41", "link": "http://arxiv.org/abs/2011.14608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UWB @ DIACR-Ita: Lexical Semantic Change Detection with CCA and\n  Orthogonal Transformation", "abstract": "In this paper, we describe our method for detection of lexical semantic\nchange (i.e., word sense changes over time) for the DIACR-Ita shared task,\nwhere we ranked $1^{st}$. We examine semantic differences between specific\nwords in two Italian corpora, chosen from different time periods. Our method is\nfully unsupervised and language independent. It consists of preparing a\nsemantic vector space for each corpus, earlier and later. Then we compute a\nlinear transformation between earlier and later spaces, using CCA and\nOrthogonal Transformation. Finally, we measure the cosines between the\ntransformed vectors.", "published": "2020-11-30 10:41:50", "link": "http://arxiv.org/abs/2011.14678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple and Effective Approach to Robust Unsupervised Bilingual\n  Dictionary Induction", "abstract": "Unsupervised Bilingual Dictionary Induction methods based on the\ninitialization and the self-learning have achieved great success in similar\nlanguage pairs, e.g., English-Spanish. But they still fail and have an accuracy\nof 0% in many distant language pairs, e.g., English-Japanese. In this work, we\nshow that this failure results from the gap between the actual initialization\nperformance and the minimum initialization performance for the self-learning to\nsucceed. We propose Iterative Dimension Reduction to bridge this gap. Our\nexperiments show that this simple method does not hamper the performance of\nsimilar language pairs and achieves an accuracy of 13.64~55.53% between English\nand four distant languages, i.e., Chinese, Japanese, Vietnamese and Thai.", "published": "2020-11-30 15:11:51", "link": "http://arxiv.org/abs/2011.14874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation of Novels in the Age of Transformer", "abstract": "In this chapter we build a machine translation (MT) system tailored to the\nliterary domain, specifically to novels, based on the state-of-the-art\narchitecture in neural MT (NMT), the Transformer (Vaswani et al., 2017), for\nthe translation direction English-to-Catalan. Subsequently, we assess to what\nextent such a system can be useful by evaluating its translations, by comparing\nthis MT system against three other systems (two domain-specific systems under\nthe recurrent and phrase-based paradigms and a popular generic on-line system)\non three evaluations. The first evaluation is automatic and uses the\nmost-widely used automatic evaluation metric, BLEU. The two remaining\nevaluations are manual and they assess, respectively, preference and amount of\npost-editing required to make the translation error-free. As expected, the\ndomain-specific Transformer-based system outperformed the three other systems\nin all the three evaluations conducted, in all cases by a large margin.", "published": "2020-11-30 16:51:08", "link": "http://arxiv.org/abs/2011.14979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UWB at SemEval-2020 Task 1: Lexical Semantic Change Detection", "abstract": "In this paper, we describe our method for the detection of lexical semantic\nchange, i.e., word sense changes over time. We examine semantic differences\nbetween specific words in two corpora, chosen from different time periods, for\nEnglish, German, Latin, and Swedish. Our method was created for the SemEval\n2020 Task 1: \\textit{Unsupervised Lexical Semantic Change Detection.} We ranked\n$1^{st}$ in Sub-task 1: binary change detection, and $4^{th}$ in Sub-task 2:\nranked change detection. Our method is fully unsupervised and language\nindependent. It consists of preparing a semantic vector space for each corpus,\nearlier and later; computing a linear transformation between earlier and later\nspaces, using Canonical Correlation Analysis and Orthogonal Transformation; and\nmeasuring the cosines between the transformed vector for the target word from\nthe earlier corpus and the vector for the target word in the later corpus.", "published": "2020-11-30 10:47:45", "link": "http://arxiv.org/abs/2012.00004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Systematically Exploring Redundancy Reduction in Summarizing Long\n  Documents", "abstract": "Our analysis of large summarization datasets indicates that redundancy is a\nvery serious problem when summarizing long documents. Yet, redundancy reduction\nhas not been thoroughly investigated in neural summarization. In this work, we\nsystematically explore and compare different ways to deal with redundancy when\nsummarizing long documents. Specifically, we organize the existing methods into\ncategories based on when and how the redundancy is considered. Then, in the\ncontext of these categories, we propose three additional methods balancing\nnon-redundancy and importance in a general and flexible way. In a series of\nexperiments, we show that our proposed methods achieve the state-of-the-art\nwith respect to ROUGE scores on two scientific paper datasets, Pubmed and\narXiv, while reducing redundancy significantly.", "published": "2020-11-30 19:07:27", "link": "http://arxiv.org/abs/2012.00052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta learning to classify intent and slot labels with noisy few shot\n  examples", "abstract": "Recently deep learning has dominated many machine learning areas, including\nspoken language understanding (SLU). However, deep learning models are\nnotorious for being data-hungry, and the heavily optimized models are usually\nsensitive to the quality of the training examples provided and the consistency\nbetween training and inference conditions. To improve the performance of SLU\nmodels on tasks with noisy and low training resources, we propose a new SLU\nbenchmarking task: few-shot robust SLU, where SLU comprises two core problems,\nintent classification (IC) and slot labeling (SL). We establish the task by\ndefining few-shot splits on three public IC/SL datasets, ATIS, SNIPS, and TOP,\nand adding two types of natural noises (adaptation example missing/replacing\nand modality mismatch) to the splits. We further propose a novel noise-robust\nfew-shot SLU model based on prototypical networks. We show the model\nconsistently outperforms the conventional fine-tuning baseline and another\npopular meta-learning method, Model-Agnostic Meta-Learning (MAML), in terms of\nachieving better IC accuracy and SL F1, and yielding smaller performance\nvariation when noises are present.", "published": "2020-11-30 18:53:30", "link": "http://arxiv.org/abs/2012.07516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake News Detection in Social Media using Graph Neural Networks and NLP\n  Techniques: A COVID-19 Use-case", "abstract": "The paper presents our solutions for the MediaEval 2020 task namely FakeNews:\nCorona Virus and 5G Conspiracy Multimedia Twitter-Data-Based Analysis. The task\naims to analyze tweets related to COVID-19 and 5G conspiracy theories to detect\nmisinformation spreaders. The task is composed of two sub-tasks namely (i)\ntext-based, and (ii) structure-based fake news detection. For the first task,\nwe propose six different solutions relying on Bag of Words (BoW) and BERT\nembedding. Three of the methods aim at binary classification task by\ndifferentiating in 5G conspiracy and the rest of the COVID-19 related tweets\nwhile the rest of them treat the task as ternary classification problem. In the\nternary classification task, our BoW and BERT based methods obtained an\nF1-score of .606% and .566% on the development set, respectively. On the binary\nclassification, the BoW and BERT based solutions obtained an average F1-score\nof .666% and .693%, respectively. On the other hand, for structure-based fake\nnews detection, we rely on Graph Neural Networks (GNNs) achieving an average\nROC of .95% on the development set.", "published": "2020-11-30 16:41:04", "link": "http://arxiv.org/abs/2012.07517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Procode: the Swiss Multilingual Solution for Automatic Coding and\n  Recoding of Occupations and Economic Activities", "abstract": "Objective. Epidemiological studies require data that are in alignment with\nthe classifications established for occupations or economic activities. The\nclassifications usually include hundreds of codes and titles. Manual coding of\nraw data may result in misclassification and be time consuming. The goal was to\ndevelop and test a web-tool, named Procode, for coding of free-texts against\nclassifications and recoding between different classifications. Methods. Three\ntext classifiers, i.e. Complement Naive Bayes (CNB), Support Vector Machine\n(SVM) and Random Forest Classifier (RFC), were investigated using a k-fold\ncross-validation. 30 000 free-texts with manually assigned classification codes\nof French classification of occupations (PCS) and French classification of\nactivities (NAF) were available. For recoding, Procode integrated a workflow\nthat converts codes of one classification to another according to existing\ncrosswalks. Since this is a straightforward operation, only the recoding time\nwas measured. Results. Among the three investigated text classifiers, CNB\nresulted in the best performance, where the classifier predicted accurately\n57-81% and 63-83% classification codes for PCS and NAF, respectively. SVM lead\nto somewhat lower results (by 1-2%), while RFC coded accurately up to 30% of\nthe data. The coding operation required one minute per 10 000 records, while\nthe recoding was faster, i.e. 5-10 seconds. Conclusion. The algorithm\nintegrated in Procode showed satisfactory performance, since the tool had to\nassign the right code by choosing between 500-700 different choices. Based on\nthe results, the authors decided to implement CNB in Procode. In future, if\nanother classifier shows a superior performance, an update will include the\nrequired modifications.", "published": "2020-11-30 07:46:21", "link": "http://arxiv.org/abs/2012.07521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Review on Recent Methods and Challenges of Video\n  Description", "abstract": "Video description involves the generation of the natural language description\nof actions, events, and objects in the video. There are various applications of\nvideo description by filling the gap between languages and vision for visually\nimpaired people, generating automatic title suggestion based on content,\nbrowsing of the video based on the content and video-guided machine translation\n[86] etc.In the past decade, several works had been done in this field in terms\nof approaches/methods for video description, evaluation metrics,and datasets.\nFor analyzing the progress in the video description task, a comprehensive\nsurvey is needed that covers all the phases of video description approaches\nwith a special focus on recent deep learning approaches. In this work, we\nreport a comprehensive survey on the phases of video description approaches,\nthe dataset for video description, evaluation metrics, open competitions for\nmotivating the research on the video description, open challenges in this\nfield, and future research directions. In this survey, we cover the\nstate-of-the-art approaches proposed for each and every dataset with their pros\nand cons. For the growth of this research domain,the availability of numerous\nbenchmark dataset is a basic need. Further, we categorize all the dataset into\ntwo classes: open domain dataset and domain-specific dataset. From our survey,\nwe observe that the work in this field is in fast-paced development since the\ntask of video description falls in the intersection of computer vision and\nnatural language processing. But still, the work in the video description is\nfar from saturation stage due to various challenges like the redundancy due to\nsimilar frames which affect the quality of visual features, the availability of\ndataset containing more diverse content and availability of an effective\nevaluation metric.", "published": "2020-11-30 13:08:45", "link": "http://arxiv.org/abs/2011.14752v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Transformer-Transducers for Code-Switched Speech Recognition", "abstract": "We live in a world where 60% of the population can speak two or more\nlanguages fluently. Members of these communities constantly switch between\nlanguages when having a conversation. As automatic speech recognition (ASR)\nsystems are being deployed to the real-world, there is a need for practical\nsystems that can handle multiple languages both within an utterance or across\nutterances. In this paper, we present an end-to-end ASR system using a\ntransformer-transducer model architecture for code-switched speech recognition.\nWe propose three modifications over the vanilla model in order to handle\nvarious aspects of code-switching. First, we introduce two auxiliary loss\nfunctions to handle the low-resource scenario of code-switching. Second, we\npropose a novel mask-based training strategy with language ID information to\nimprove the label encoder training towards intra-sentential code-switching.\nFinally, we propose a multi-label/multi-audio encoder structure to leverage the\nvast monolingual speech corpora towards code-switching. We demonstrate the\nefficacy of our proposed approaches on the SEAME dataset, a public\nMandarin-English code-switching corpus, achieving a mixed error rate of 18.5%\nand 26.3% on test_man and test_sge sets respectively.", "published": "2020-11-30 17:27:41", "link": "http://arxiv.org/abs/2011.15023v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework\n  of Vision-and-Language BERTs", "abstract": "Large-scale pretraining and task-specific fine-tuning is now the standard\nmethodology for many tasks in computer vision and natural language processing.\nRecently, a multitude of methods have been proposed for pretraining vision and\nlanguage BERTs to tackle challenges at the intersection of these two key areas\nof AI. These models can be categorised into either single-stream or dual-stream\nencoders. We study the differences between these two categories, and show how\nthey can be unified under a single theoretical framework. We then conduct\ncontrolled experiments to discern the empirical differences between five V&L\nBERTs. Our experiments show that training data and hyperparameters are\nresponsible for most of the differences between the reported results, but they\nalso reveal that the embedding layer plays a crucial role in these massive\nmodels.", "published": "2020-11-30 18:55:24", "link": "http://arxiv.org/abs/2011.15124v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Detection of Alzheimer's Disease from Speech and Text", "abstract": "Reliable detection of the prodromal stages of Alzheimer's disease (AD)\nremains difficult even today because, unlike other neurocognitive impairments,\nthere is no definitive diagnosis of AD in vivo. In this context, existing\nresearch has shown that patients often develop language impairment even in mild\nAD conditions. We propose a multimodal deep learning method that utilizes\nspeech and the corresponding transcript simultaneously to detect AD. For audio\nsignals, the proposed audio-based network, a convolutional neural network (CNN)\nbased model, predicts the diagnosis for multiple speech segments, which are\ncombined for the final prediction. Similarly, we use contextual embedding\nextracted from BERT concatenated with a CNN-generated embedding for classifying\nthe transcript. The individual predictions of the two models are then combined\nto make the final classification. We also perform experiments to analyze the\nmodel performance when Automated Speech Recognition (ASR) system generated\ntranscripts are used instead of manual transcription in the text-based model.\nThe proposed method achieves 85.3% 10-fold cross-validation accuracy when\ntrained and evaluated on the Dementiabank Pitt corpus.", "published": "2020-11-30 21:18:17", "link": "http://arxiv.org/abs/2012.00096v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Blind signal decomposition of various word embeddings based on join and\n  individual variance explained", "abstract": "In recent years, natural language processing (NLP) has become one of the most\nimportant areas with various applications in human's life. As the most\nfundamental task, the field of word embedding still requires more attention and\nresearch. Currently, existing works about word embedding are focusing on\nproposing novel embedding algorithms and dimension reduction techniques on\nwell-trained word embeddings. In this paper, we propose to use a novel joint\nsignal separation method - JIVE to jointly decompose various trained word\nembeddings into joint and individual components. Through this decomposition\nframework, we can easily investigate the similarity and difference among\ndifferent word embeddings. We conducted extensive empirical study on word2vec,\nFastText and GLoVE trained on different corpus and with different dimensions.\nWe compared the performance of different decomposed components based on\nsentiment analysis on Twitter and Stanford sentiment treebank. We found that by\nmapping different word embeddings into the joint component, sentiment\nperformance can be greatly improved for the original word embeddings with lower\nperformance. Moreover, we found that by concatenating different components\ntogether, the same model can achieve better performance. These findings provide\ngreat insights into the word embeddings and our work offer a new of generating\nword embeddings by fusing.", "published": "2020-11-30 01:36:29", "link": "http://arxiv.org/abs/2011.14496v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "CovidExplorer: A Multi-faceted AI-based Search and Visualization Engine\n  for COVID-19 Information", "abstract": "The entire world is engulfed in the fight against the COVID-19 pandemic,\nleading to a significant surge in research experiments, government policies,\nand social media discussions. A multi-modal information access and data\nvisualization platform can play a critical role in supporting research aimed at\nunderstanding and developing preventive measures for the pandemic. In this\npaper, we present a multi-faceted AI-based search and visualization engine,\nCovidExplorer. Our system aims to help researchers understand current\nstate-of-the-art COVID-19 research, identify research articles relevant to\ntheir domain, and visualize real-time trends and statistics of COVID-19 cases.\nIn contrast to other existing systems, CovidExplorer also brings in\nIndia-specific topical discussions on social media to study different aspects\nof COVID-19. The system, demo video, and the datasets are available at\nhttp://covidexplorer.in.", "published": "2020-11-30 08:42:13", "link": "http://arxiv.org/abs/2011.14618v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Language-Driven Region Pointer Advancement for Controllable Image\n  Captioning", "abstract": "Controllable Image Captioning is a recent sub-field in the multi-modal task\nof Image Captioning wherein constraints are placed on which regions in an image\nshould be described in the generated natural language caption. This puts a\nstronger focus on producing more detailed descriptions, and opens the door for\nmore end-user control over results. A vital component of the Controllable Image\nCaptioning architecture is the mechanism that decides the timing of attending\nto each region through the advancement of a region pointer. In this paper, we\npropose a novel method for predicting the timing of region pointer advancement\nby treating the advancement step as a natural part of the language structure\nvia a NEXT-token, motivated by a strong correlation to the sentence structure\nin the training data. We find that our timing agrees with the ground-truth\ntiming in the Flickr30k Entities test data with a precision of 86.55% and a\nrecall of 97.92%. Our model implementing this technique improves the\nstate-of-the-art on standard captioning metrics while additionally\ndemonstrating a considerably larger effective vocabulary size.", "published": "2020-11-30 15:34:59", "link": "http://arxiv.org/abs/2011.14901v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.NE", "68T07, 68T45, 68T50", "I.2.7; I.2.10; I.5.1"], "primary_category": "cs.CL"}
{"title": "A Framework for Authorial Clustering of Shorter Texts in Latent Semantic\n  Spaces", "abstract": "Authorial clustering involves the grouping of documents written by the same\nauthor or team of authors without any prior positive examples of an author's\nwriting style or thematic preferences. For authorial clustering on shorter\ntexts (paragraph-length texts that are typically shorter than conventional\ndocuments), the document representation is particularly important: very\nhigh-dimensional feature spaces lead to data sparsity and suffer from serious\nconsequences like the curse of dimensionality, while feature selection may lead\nto information loss. We propose a high-level framework which utilizes a compact\ndata representation in a latent feature space derived with non-parametric topic\nmodeling. Authorial clusters are identified thereafter in two scenarios: (a)\nfully unsupervised and (b) semi-supervised where a small number of shorter\ntexts are known to belong to the same author (must-link constraints) or not\n(cannot-link constraints). We report on experiments with 120 collections in\nthree languages and two genres and show that the topic-based latent feature\nspace provides a promising level of performance while reducing the\ndimensionality by a factor of 1500 compared to state-of-the-arts. We also\ndemonstrate that, while prior knowledge on the precise number of authors (i.e.\nauthorial clusters) does not contribute much to additional quality, little\nknowledge on constraints in authorial clusters memberships leads to clear\nperformance improvements in front of this difficult task. Thorough\nexperimentation with standard metrics indicates that there still remains an\nample room for improvement for authorial clustering, especially with shorter\ntexts", "published": "2020-11-30 17:39:44", "link": "http://arxiv.org/abs/2011.15038v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Facilitating the Communication of Politeness through Fine-Grained\n  Paraphrasing", "abstract": "Aided by technology, people are increasingly able to communicate across\ngeographical, cultural, and language barriers. This ability also results in new\nchallenges, as interlocutors need to adapt their communication approaches to\nincreasingly diverse circumstances. In this work, we take the first steps\ntowards automatically assisting people in adjusting their language to a\nspecific communication circumstance.\n  As a case study, we focus on facilitating the accurate transmission of\npragmatic intentions and introduce a methodology for suggesting paraphrases\nthat achieve the intended level of politeness under a given communication\ncircumstance. We demonstrate the feasibility of this approach by evaluating our\nmethod in two realistic communication scenarios and show that it can reduce the\npotential for misalignment between the speaker's intentions and the listener's\nperceptions in both cases.", "published": "2020-11-30 19:00:00", "link": "http://arxiv.org/abs/2012.00012v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Extreme Model Compression for On-device Natural Language Understanding", "abstract": "In this paper, we propose and experiment with techniques for extreme\ncompression of neural natural language understanding (NLU) models, making them\nsuitable for execution on resource-constrained devices. We propose a\ntask-aware, end-to-end compression approach that performs word-embedding\ncompression jointly with NLU task learning. We show our results on a\nlarge-scale, commercial NLU system trained on a varied set of intents with huge\nvocabulary sizes. Our approach outperforms a range of baselines and achieves a\ncompression rate of 97.4% with less than 3.7% degradation in predictive\nperformance. Our analysis indicates that the signal from the downstream task is\nimportant for effective compression with minimal degradation in performance.", "published": "2020-11-30 21:47:48", "link": "http://arxiv.org/abs/2012.00124v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving accuracy of rare words for RNN-Transducer through unigram\n  shallow fusion", "abstract": "End-to-end automatic speech recognition (ASR) systems, such as recurrent\nneural network transducer (RNN-T), have become popular, but rare word remains a\nchallenge. In this paper, we propose a simple, yet effective method called\nunigram shallow fusion (USF) to improve rare words for RNN-T. In USF, we\nextract rare words from RNN-T training data based on unigram count, and apply a\nfixed reward when the word is encountered during decoding. We show that this\nsimple method can improve performance on rare words by 3.7% WER relative\nwithout degradation on general test set, and the improvement from USF is\nadditive to any additional language model based rescoring. Then, we show that\nthe same USF does not work on conventional hybrid system. Finally, we reason\nthat USF works by fixing errors in probability estimates of words due to\nViterbi search used during decoding with subword-based RNN-T.", "published": "2020-11-30 22:06:02", "link": "http://arxiv.org/abs/2012.00133v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Accurate and Scalable Matching of Translators to Displaced Persons for\n  Overcoming Language Barriers", "abstract": "Residents of developing countries are disproportionately susceptible to\ndisplacement as a result of humanitarian crises. During such crises, language\nbarriers impede aid workers in providing services to those displaced. To build\nresilience, such services must be flexible and robust to a host of possible\nlanguages. \\textit{Tarjimly} aims to overcome the barriers by providing a\nplatform capable of matching bilingual volunteers to displaced persons or aid\nworkers in need of translating. However, Tarjimly's large pool of translators\ncomes with the challenge of selecting the right translator per request. In this\npaper, we describe a machine learning system that matches translator requests\nto volunteers at scale. We demonstrate that a simple logistic regression,\noperating on easily computable features, can accurately predict and rank\ntranslator response. In deployment, this lightweight system matches 82\\% of\nrequests with a median response time of 59 seconds, allowing aid workers to\naccelerate their services supporting displaced persons.", "published": "2020-11-30 22:50:00", "link": "http://arxiv.org/abs/2012.02595v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Look who's not talking", "abstract": "The objective of this work is speaker diarisation of speech recordings 'in\nthe wild'. The ability to determine speech segments is a crucial part of\ndiarisation systems, accounting for a large proportion of errors. In this\npaper, we present a simple but effective solution for speech activity detection\nbased on the speaker embeddings. In particular, we discover that the norm of\nthe speaker embedding is an extremely effective indicator of speech activity.\nThe method does not require an independent model for speech activity detection,\ntherefore allows speaker diarisation to be performed using a unified\nrepresentation for both speaker modelling and speech activity detection. We\nperform a number of experiments on in-house and public datasets, in which our\nmethod outperforms popular baselines.", "published": "2020-11-30 15:25:21", "link": "http://arxiv.org/abs/2011.14885v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolutive Transfer Function Invariant SDR training criteria for\n  Multi-Channel Reverberant Speech Separation", "abstract": "Time-domain training criteria have proven to be very effective for the\nseparation of single-channel non-reverberant speech mixtures. Likewise,\nmask-based beamforming has shown impressive performance in multi-channel\nreverberant speech enhancement and source separation. Here, we propose to\ncombine neural network supported multi-channel source separation with a\ntime-domain training objective function. For the objective we propose to use a\nconvolutive transfer function invariant Signal-to-Distortion Ratio (CI-SDR)\nbased loss. While this is a well-known evaluation metric (BSS Eval), it has not\nbeen used as a training objective before. To show the effectiveness, we\ndemonstrate the performance on LibriSpeech based reverberant mixtures. On this\ntask, the proposed system approaches the error rate obtained on single-source\nnon-reverberant input, i.e., LibriSpeech test_clean, with a difference of only\n1.2 percentage points, thus outperforming a conventional permutation invariant\ntraining based system and alternative objectives like Scale Invariant\nSignal-to-Distortion Ratio by a large margin.", "published": "2020-11-30 17:08:19", "link": "http://arxiv.org/abs/2011.15003v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A proposal and evaluation of new timbre visualisation methods for audio\n  sample browsers", "abstract": "Searching through vast libraries of sound samples can be a daunting and\ntime-consuming task. Modern audio sample browsers use mappings between acoustic\nproperties and visual attributes to visually differentiate displayed items.\nThere are few studies focused on how well these mappings help users search for\na specific sample. We propose new methods for generating textural labels and\npositioning samples based on perceptual representations of timbre. We perform a\nseries of studies to evaluate the benefits of using shape, color or texture as\nlabels in a known-item search task. We describe the motivation and\nimplementation of the study, and present an in-depth analysis of results. We\nfind that shape significantly improves task performance, while color and\ntexture have little effect. We also compare results between in-person and\nonline participants and propose research directions for further studies.", "published": "2020-11-30 18:30:26", "link": "http://arxiv.org/abs/2011.15096v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.2; H.5.5"], "primary_category": "cs.HC"}
{"title": "Detecting expressions with multimodal transformers", "abstract": "Developing machine learning algorithms to understand person-to-person\nengagement can result in natural user experiences for communal devices such as\nAmazon Alexa. Among other cues such as voice activity and gaze, a person's\naudio-visual expression that includes tone of the voice and facial expression\nserves as an implicit signal of engagement between parties in a dialog. This\nstudy investigates deep-learning algorithms for audio-visual detection of\nuser's expression. We first implement an audio-visual baseline model with\nrecurrent layers that shows competitive results compared to current state of\nthe art. Next, we propose the transformer architecture with encoder layers that\nbetter integrate audio-visual features for expressions tracking. Performance on\nthe Aff-Wild2 database shows that the proposed methods perform better than\nbaseline architecture with recurrent layers with absolute gains approximately\n2% for arousal and valence descriptors. Further, multimodal architectures show\nsignificant improvements over models trained on single modalities with gains of\nup to 3.6%. Ablation studies show the significance of the visual modality for\nthe expression detection on the Aff-Wild2 database.", "published": "2020-11-30 19:31:03", "link": "http://arxiv.org/abs/2012.00063v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
