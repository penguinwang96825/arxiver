{"title": "Contrastive Feedback Mechanism for Simultaneous Speech Translation", "abstract": "Recent advances in simultaneous speech translation (SST) focus on the\ndecision policies that enable the use of offline-trained ST models for\nsimultaneous inference. These decision policies not only control the\nquality-latency trade-off in SST but also mitigate the impact of unstable\npredictions on translation quality by delaying translation for more context or\ndiscarding these predictions through stable hypothesis detection. However,\nthese policies often overlook the potential benefits of utilizing unstable\npredictions. We introduce the contrastive feedback mechanism (CFM) for SST, a\nnovel method that leverages these unstable predictions as feedback to improve\ntranslation quality. CFM guides the system to eliminate undesired model\nbehaviors from these predictions through a contrastive objective. The\nexperiments on 3 state-of-the-art decision policies across 8 languages in the\nMuST-C v1.0 dataset show that CFM effectively improves the performance of SST.", "published": "2024-07-30 03:50:10", "link": "http://arxiv.org/abs/2407.20524v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large\n  Language Models over Factual Knowledge", "abstract": "While large language models (LLMs) have demonstrated impressive capabilities\nacross various natural language processing tasks by acquiring rich factual\nknowledge from their broad training data, their ability to synthesize and\nlogically reason with this knowledge in complex ways remains underexplored. In\nthis work, we present a systematic evaluation of state-of-the-art LLMs' complex\nlogical reasoning abilities through a novel benchmark of automatically\ngenerated complex reasoning questions over general domain and biomedical\nknowledge graphs. Our extensive experiments, employing diverse in-context\nlearning techniques, reveal that LLMs excel at reasoning over general world\nknowledge but face significant challenges with specialized domain-specific\nknowledge. We find that prompting with explicit Chain-of-Thought demonstrations\ncan substantially improve LLM performance on complex logical reasoning tasks\nwith diverse logical operations. Interestingly, our controlled evaluations\nuncover an asymmetry where LLMs display proficiency at set union operations,\nbut struggle considerably with set intersections - a key building block of\nlogical reasoning. To foster further work, we will publicly release our\nevaluation benchmark and code.", "published": "2024-07-30 05:40:32", "link": "http://arxiv.org/abs/2407.20564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knesset-DictaBERT: A Hebrew Language Model for Parliamentary Proceedings", "abstract": "We present Knesset-DictaBERT, a large Hebrew language model fine-tuned on the\nKnesset Corpus, which comprises Israeli parliamentary proceedings. The model is\nbased on the DictaBERT architecture and demonstrates significant improvements\nin understanding parliamentary language according to the MLM task. We provide a\ndetailed evaluation of the model's performance, showing improvements in\nperplexity and accuracy over the baseline DictaBERT model.", "published": "2024-07-30 06:29:01", "link": "http://arxiv.org/abs/2407.20581v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Enhancing Agricultural Machinery Management through Advanced LLM\n  Integration", "abstract": "The integration of artificial intelligence into agricultural practices,\nspecifically through Consultation on Intelligent Agricultural Machinery\nManagement (CIAMM), has the potential to revolutionize efficiency and\nsustainability in farming. This paper introduces a novel approach that\nleverages large language models (LLMs), particularly GPT-4, combined with\nmulti-round prompt engineering to enhance decision-making processes in\nagricultural machinery management. We systematically developed and refined\nprompts to guide the LLMs in generating precise and contextually relevant\noutputs. Our approach was evaluated using a manually curated dataset from\nvarious online sources, and performance was assessed with accuracy and GPT-4\nScores. Comparative experiments were conducted using LLama-2-70B, ChatGPT, and\nGPT-4 models, alongside baseline and state-of-the-art methods such as Chain of\nThought (CoT) and Thought of Thought (ThoT). The results demonstrate that our\nmethod significantly outperforms these approaches, achieving higher accuracy\nand relevance in generated responses. This paper highlights the potential of\nadvanced prompt engineering techniques in improving the robustness and\napplicability of AI in agricultural contexts.", "published": "2024-07-30 06:49:55", "link": "http://arxiv.org/abs/2407.20588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArabicNLU 2024: The First Arabic Natural Language Understanding Shared\n  Task", "abstract": "This paper presents an overview of the Arabic Natural Language Understanding\n(ArabicNLU 2024) shared task, focusing on two subtasks: Word Sense\nDisambiguation (WSD) and Location Mention Disambiguation (LMD). The task aimed\nto evaluate the ability of automated systems to resolve word ambiguity and\nidentify locations mentioned in Arabic text. We provided participants with\nnovel datasets, including a sense-annotated corpus for WSD, called SALMA with\napproximately 34k annotated tokens, and the IDRISI-DA dataset with 3,893\nannotations and 763 unique location mentions. These are challenging tasks. Out\nof the 38 registered teams, only three teams participated in the final\nevaluation phase, with the highest accuracy being 77.8% for WSD and the highest\nMRR@1 being 95.0% for LMD. The shared task not only facilitated the evaluation\nand comparison of different techniques, but also provided valuable insights and\nresources for the continued advancement of Arabic NLU technologies.", "published": "2024-07-30 08:57:01", "link": "http://arxiv.org/abs/2407.20663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Safe-for-Work Classifier for Malaysian Language Text: Enhancing\n  Alignment in LLM-Ops Framework", "abstract": "As large language models (LLMs) become increasingly integrated into\noperational workflows (LLM-Ops), there is a pressing need for effective\nguardrails to ensure safe and aligned interactions, including the ability to\ndetect potentially unsafe or inappropriate content across languages. However,\nexisting safe-for-work classifiers are primarily focused on English text. To\naddress this gap for the Malaysian language, we present a novel safe-for-work\ntext classifier tailored specifically for Malaysian language content. By\ncurating and annotating a first-of-its-kind dataset of Malaysian text spanning\nmultiple content categories, we trained a classification model capable of\nidentifying potentially unsafe material using state-of-the-art natural language\nprocessing techniques. This work represents an important step in enabling safer\ninteractions and content filtering to mitigate potential risks and ensure\nresponsible deployment of LLMs. To maximize accessibility and promote further\nresearch towards enhancing alignment in LLM-Ops for the Malaysian context, the\nmodel is publicly released at\nhttps://huggingface.co/malaysia-ai/malaysian-sfw-classifier.", "published": "2024-07-30 10:51:51", "link": "http://arxiv.org/abs/2407.20729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meltemi: The first open Large Language Model for Greek", "abstract": "We describe the development and capabilities of Meltemi 7B, the first open\nLarge Language Model for the Greek language. Meltemi 7B has 7 billion\nparameters and is trained on a 40 billion token Greek corpus. For the\ndevelopment of Meltemi 7B, we adapt Mistral, by continuous pretraining on the\nGreek Corpus. Meltemi 7B contains up-to-date information up to September 2023.\nFurthermore, we have translated and curated a Greek instruction corpus, which\nhas been used for the instruction-tuning of a chat model, named Meltemi 7B\nInstruct. Special care has been given to the alignment and the removal of toxic\ncontent for the Meltemi 7B Instruct. The developed models are evaluated on a\nbroad set of collected evaluation corpora, and examples of prompts and\nresponses are presented. Both Meltemi 7B and Meltemi 7B Instruct are available\nat https://huggingface.co/ilsp under the Apache 2.0 license.", "published": "2024-07-30 11:22:52", "link": "http://arxiv.org/abs/2407.20743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Semantic Similarity Understanding in Arabic NLP with Nested\n  Embedding Learning", "abstract": "This work presents a novel framework for training Arabic nested embedding\nmodels through Matryoshka Embedding Learning, leveraging multilingual,\nArabic-specific, and English-based models, to highlight the power of nested\nembeddings models in various Arabic NLP downstream tasks. Our innovative\ncontribution includes the translation of various sentence similarity datasets\ninto Arabic, enabling a comprehensive evaluation framework to compare these\nmodels across different dimensions. We trained several nested embedding models\non the Arabic Natural Language Inference triplet dataset and assessed their\nperformance using multiple evaluation metrics, including Pearson and Spearman\ncorrelations for cosine similarity, Manhattan distance, Euclidean distance, and\ndot product similarity. The results demonstrate the superior performance of the\nMatryoshka embedding models, particularly in capturing semantic nuances unique\nto the Arabic language. Results demonstrated that Arabic Matryoshka embedding\nmodels have superior performance in capturing semantic nuances unique to the\nArabic language, significantly outperforming traditional models by up to\n20-25\\% across various similarity metrics. These results underscore the\neffectiveness of language-specific training and highlight the potential of\nMatryoshka models in enhancing semantic textual similarity tasks for Arabic\nNLP.", "published": "2024-07-30 19:03:03", "link": "http://arxiv.org/abs/2407.21139v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event-Arguments Extraction Corpus and Modeling using BERT for Arabic", "abstract": "Event-argument extraction is a challenging task, particularly in Arabic due\nto sparse linguistic resources. To fill this gap, we introduce the \\hadath\ncorpus ($550$k tokens) as an extension of Wojood, enriched with event-argument\nannotations. We used three types of event arguments: $agent$, $location$, and\n$date$, which we annotated as relation types. Our inter-annotator agreement\nevaluation resulted in $82.23\\%$ $Kappa$ score and $87.2\\%$ $F_1$-score.\nAdditionally, we propose a novel method for event relation extraction using\nBERT, in which we treat the task as text entailment. This method achieves an\n$F_1$-score of $94.01\\%$. To further evaluate the generalization of our\nproposed method, we collected and annotated another out-of-domain corpus (about\n$80$k tokens) called \\testNLI and used it as a second test set, on which our\napproach achieved promising results ($83.59\\%$ $F_1$-score). Last but not\nleast, we propose an end-to-end system for event-arguments extraction. This\nsystem is implemented as part of SinaTools, and both corpora are publicly\navailable at {\\small \\url{https://sina.birzeit.edu/wojood}}", "published": "2024-07-30 19:32:22", "link": "http://arxiv.org/abs/2407.21153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs for Enhanced Agricultural Meteorological Recommendations", "abstract": "Agricultural meteorological recommendations are crucial for enhancing crop\nproductivity and sustainability by providing farmers with actionable insights\nbased on weather forecasts, soil conditions, and crop-specific data. This paper\npresents a novel approach that leverages large language models (LLMs) and\nprompt engineering to improve the accuracy and relevance of these\nrecommendations. We designed a multi-round prompt framework to iteratively\nrefine recommendations using updated data and feedback, implemented on ChatGPT,\nClaude2, and GPT-4. Our method was evaluated against baseline models and a\nChain-of-Thought (CoT) approach using manually collected datasets. The results\ndemonstrate significant improvements in accuracy and contextual relevance, with\nour approach achieving up to 90\\% accuracy and high GPT-4 scores. Additional\nvalidation through real-world pilot studies further confirmed the practical\nbenefits of our method, highlighting its potential to transform agricultural\npractices and decision-making.", "published": "2024-07-30 18:10:49", "link": "http://arxiv.org/abs/2408.04640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder", "abstract": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.", "published": "2024-07-30 01:13:42", "link": "http://arxiv.org/abs/2407.20485v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training", "abstract": "The remarkable success of Large Language Models (LLMs) relies heavily on\ntheir substantial scale, which poses significant challenges during model\ndeployment in terms of latency and memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often suffer from considerable performance degradation\non complex language understanding tasks, raising concerns about the feasibility\nof pruning in LLMs. To address this issue, we propose Adaptive Sparse Trainer\n(AST), a novel and efficient retraining framework tailored for semi-structured\nsparse models. AST enables models to learn optimal masks during the weight\nupdate process without incurring additional computational overhead.\nFurthermore, we demonstrate that incorporating knowledge distillation\nsignificantly improves retraining efficiency and enhances model performance\nunder fixed computational constraints. Additionally, a supplementary set of\nwell-initialized parameters is integrated to further augment the model's\nefficacy. AST achieves state-of-the-art performance with minimal training cost.\nWhen applied to the LLaMA2-7B model, AST reduces the perplexity and zero-shot\naccuracy gap between dense and 2:4 semi-structured sparse models to 0.6 and\n1.16%, respectively, utilizing less than 0.4% of the pretraining tokens and GPU\nhours. Our work demonstrates the feasibility of deploying semi-structured\nsparse LLMs and offers a promising alternative for achieving highly compressed\nmodels when combined with existing quantization techniques.", "published": "2024-07-30 06:33:44", "link": "http://arxiv.org/abs/2407.20584v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Harvesting Textual and Structured Data from the HAL Publication\n  Repository", "abstract": "HAL (\\textit{Hyper Articles en Ligne}) is the French national publication\nrepository, used by most higher education and research organizations for their\nopen science policy. Although it is a rich repository of academic documents,\nits potential for advanced research has not been fully explored. We present\nHALvest, a unique dataset that bridges the gap between citation networks and\nthe full text of HAL-submitted articles to help with authorship attribution and\nverification. This first iteration consists of approximately 700,000 documents,\nspanning 56 languages across 13 identified domains. We transform articles'\nmetadata into a citation network, producing a heterogeneous graph. This graph\nincludes uniquely identified authors on HAL, as well as all open-access\ndocuments and their references. Finally, we mine 14.5 million high-quality\nsequence pairs from HALvest for contrastive learning purposes. By providing\ndifferent views of HAL, suited for modern machine learning, we aim to assist\npractitioners in better analyzing and interpreting research dynamics.", "published": "2024-07-30 07:14:04", "link": "http://arxiv.org/abs/2407.20595v2", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Questionnaires for Everyone: Streamlining Cross-Cultural Questionnaire\n  Adaptation with GPT-Based Translation Quality Evaluation", "abstract": "Adapting questionnaires to new languages is a resource-intensive process\noften requiring the hiring of multiple independent translators, which limits\nthe ability of researchers to conduct cross-cultural research and effectively\ncreates inequalities in research and society. This work presents a prototype\ntool that can expedite the questionnaire translation process. The tool\nincorporates forward-backward translation using DeepL alongside GPT-4-generated\ntranslation quality evaluations and improvement suggestions. We conducted two\nonline studies in which participants translated questionnaires from English to\neither German (Study 1; n=10) or Portuguese (Study 2; n=20) using our\nprototype. To evaluate the quality of the translations created using the tool,\nevaluation scores between conventionally translated and tool-supported versions\nwere compared. Our results indicate that integrating LLM-generated translation\nquality evaluations and suggestions for improvement can help users\nindependently attain results similar to those provided by conventional,\nnon-NLP-supported translation methods. This is the first step towards more\nequitable questionnaire-based research, powered by AI.", "published": "2024-07-30 07:34:40", "link": "http://arxiv.org/abs/2407.20608v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain\n  Study in Italian", "abstract": "Addressing the challenge of limited annotated data in specialized fields and\nlow-resource languages is crucial for the effective use of Language Models\n(LMs). While most Large Language Models (LLMs) are trained on general-purpose\nEnglish corpora, there is a notable gap in models specifically tailored for\nItalian, particularly for technical and bureaucratic jargon. This paper\nexplores the feasibility of employing smaller, domain-specific encoder LMs\nalongside prompting techniques to enhance performance in these specialized\ncontexts. Our study concentrates on the Italian bureaucratic and legal\nlanguage, experimenting with both general-purpose and further pre-trained\nencoder-only models. We evaluated the models on downstream tasks such as\ndocument classification and entity typing and conducted intrinsic evaluations\nusing Pseudo-Log-Likelihood. The results indicate that while further\npre-trained models may show diminished robustness in general knowledge, they\nexhibit superior adaptability for domain-specific tasks, even in a zero-shot\nsetting. Furthermore, the application of calibration techniques and in-domain\nverbalizers significantly enhances the efficacy of encoder models. These\ndomain-specialized models prove to be particularly advantageous in scenarios\nwhere in-domain resources or expertise are scarce. In conclusion, our findings\noffer new insights into the use of Italian models in specialized contexts,\nwhich may have a significant impact on both research and industrial\napplications in the digital transformation era.", "published": "2024-07-30 08:50:16", "link": "http://arxiv.org/abs/2407.20654v1", "categories": ["cs.CL", "cs.AI", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Label-Guided Prompt for Multi-label Few-shot Aspect Category Detection", "abstract": "Multi-label few-shot aspect category detection aims at identifying multiple\naspect categories from sentences with a limited number of training instances.\nThe representation of sentences and categories is a key issue in this task.\nMost of current methods extract keywords for the sentence representations and\nthe category representations. Sentences often contain many category-independent\nwords, which leads to suboptimal performance of keyword-based methods. Instead\nof directly extracting keywords, we propose a label-guided prompt method to\nrepresent sentences and categories. To be specific, we design label-specific\nprompts to represent sentences by combining crucial contextual and semantic\ninformation. Further, the label is introduced into a prompt to obtain category\ndescriptions by utilizing a large language model. This kind of category\ndescriptions contain the characteristics of the aspect categories, guiding the\nconstruction of discriminative category prototypes. Experimental results on two\npublic datasets show that our method outperforms current state-of-the-art\nmethods with a 3.86% - 4.75% improvement in the Macro-F1 score.", "published": "2024-07-30 09:11:17", "link": "http://arxiv.org/abs/2407.20673v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CultureVo: The Serious Game of Utilizing Gen AI for Enhancing Cultural\n  Intelligence", "abstract": "CultureVo, Inc. has developed the Integrated Culture Learning Suite (ICLS) to\ndeliver foundational knowledge of world cultures through a combination of\ninteractive lessons and gamified experiences. This paper explores how\nGenerative AI powered by open source Large Langauge Models are utilized within\nthe ICLS to enhance cultural intelligence. The suite employs Generative AI\ntechniques to automate the assessment of learner knowledge, analyze behavioral\npatterns, and manage interactions with non-player characters using real time\nlearner assessment. Additionally, ICLS provides contextual hint and recommend\ncourse content by assessing learner proficiency, while Generative AI\nfacilitates the automated creation and validation of educational content.", "published": "2024-07-30 09:26:43", "link": "http://arxiv.org/abs/2407.20685v2", "categories": ["cs.ET", "cs.CL"], "primary_category": "cs.ET"}
{"title": "SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision\n  Language Models", "abstract": "Vision-Language Models (VLMs) have recently emerged, demonstrating remarkable\nvision-understanding capabilities. However, training these models requires\nlarge-scale datasets, which brings challenges related to efficiency,\neffectiveness, quality, and privacy of web data. In this paper, we introduce\nSynthVLM, a novel data synthesis and curation method for generating\nimage-caption pairs. Unlike traditional methods, where captions are generated\nfrom images, SynthVLM utilizes advanced diffusion models and high-quality\ncaptions to automatically synthesize and select high-resolution images from\ntext descriptions, thereby creating precisely aligned image-text pairs. To\ndemonstrate the power of SynthVLM, we introduce SynthVLM-100K, a high-quality\ndataset consisting of 100,000 curated and synthesized image-caption pairs. In\nboth model and human evaluations, SynthVLM-100K outperforms traditional\nreal-world datasets. Leveraging this dataset, we develop a new family of\nmultimodal large language models (MLLMs), SynthVLM-7B and SynthVLM-13B, which\nachieve state-of-the-art (SOTA) performance on various vision\nquestion-answering (VQA) tasks. Notably, our models outperform LLaVA across\nmost metrics with only 18\\% pretrain data. Furthermore, SynthVLM-7B and\nSynthVLM-13B attain SOTA performance on the MMLU benchmark, demonstrating that\nthe high-quality SynthVLM-100K dataset preserves language abilities. To\nfacilitate future research, our dataset and the complete data generating and\ncurating methods are open-sourced at\nhttps://github.com/starriver030515/SynthVLM.", "published": "2024-07-30 11:57:40", "link": "http://arxiv.org/abs/2407.20756v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Faithful and Plausible Natural Language Explanations for Image\n  Classification: A Pipeline Approach", "abstract": "Existing explanation methods for image classification struggle to provide\nfaithful and plausible explanations. This paper addresses this issue by\nproposing a post-hoc natural language explanation method that can be applied to\nany CNN-based classifier without altering its training process or affecting\npredictive performance. By analysing influential neurons and the corresponding\nactivation maps, the method generates a faithful description of the\nclassifier's decision process in the form of a structured meaning\nrepresentation, which is then converted into text by a language model. Through\nthis pipeline approach, the generated explanations are grounded in the neural\nnetwork architecture, providing accurate insight into the classification\nprocess while remaining accessible to non-experts. Experimental results show\nthat the NLEs constructed by our method are significantly more plausible and\nfaithful. In particular, user interventions in the neural network structure\n(masking of neurons) are three times more effective than the baselines.", "published": "2024-07-30 15:17:15", "link": "http://arxiv.org/abs/2407.20899v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enabling Contextual Soft Moderation on Social Media through Contrastive\n  Textual Deviation", "abstract": "Automated soft moderation systems are unable to ascertain if a post supports\nor refutes a false claim, resulting in a large number of contextual false\npositives. This limits their effectiveness, for example undermining trust in\nhealth experts by adding warnings to their posts or resorting to vague warnings\ninstead of granular fact-checks, which result in desensitizing users. In this\npaper, we propose to incorporate stance detection into existing automated\nsoft-moderation pipelines, with the goal of ruling out contextual false\npositives and providing more precise recommendations for social media content\nthat should receive warnings. We develop a textual deviation task called\nContrastive Textual Deviation (CTD) and show that it outperforms existing\nstance detection approaches when applied to soft moderation.We then integrate\nCTD into the stateof-the-art system for automated soft moderation Lambretta,\nshowing that our approach can reduce contextual false positives from 20% to\n2.1%, providing another important building block towards deploying reliable\nautomated soft moderation tools on social media.", "published": "2024-07-30 15:37:05", "link": "http://arxiv.org/abs/2407.20910v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models\n  for Hateful Meme Detection", "abstract": "Recent advances show that two-stream approaches have achieved outstanding\nperformance in hateful meme detection. However, hateful memes constantly evolve\nas new memes emerge by fusing progressive cultural ideas, making existing\nmethods obsolete or ineffective. In this work, we explore the potential of\nLarge Multimodal Models (LMMs) for hateful meme detection. To this end, we\npropose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)\nPrompting, by integrating the evolution attribute and in-context information of\nmemes. Specifically, Evolver simulates the evolving and expressing process of\nmemes and reasons through LMMs in a step-by-step manner. First, an evolutionary\npair mining module retrieves the top-k most similar memes in the external\ncurated meme set with the input meme. Second, an evolutionary information\nextractor is designed to summarize the semantic regularities between the paired\nmemes for prompting. Finally, a contextual relevance amplifier enhances the\nin-context hatefulness information to boost the search for evolutionary\nprocesses. Extensive experiments on public FHM, MAMI, and HarM datasets show\nthat CoE prompting can be incorporated into existing LMMs to improve their\nperformance. More encouragingly, it can serve as an interpretive tool to\npromote the understanding of the evolution of social memes. [Homepage]\n(https://github.com/inFaaa/Evolver)", "published": "2024-07-30 17:51:44", "link": "http://arxiv.org/abs/2407.21004v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ThinK: Thinner Key Cache by Query-Driven Pruning", "abstract": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.", "published": "2024-07-30 17:59:08", "link": "http://arxiv.org/abs/2407.21018v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decomposed Prompting to Answer Questions on a Course Discussion Board", "abstract": "We propose and evaluate a question-answering system that uses decomposed\nprompting to classify and answer student questions on a course discussion\nboard. Our system uses a large language model (LLM) to classify questions into\none of four types: conceptual, homework, logistics, and not answerable. This\nenables us to employ a different strategy for answering questions that fall\nunder different types. Using a variant of GPT-3, we achieve $81\\%$\nclassification accuracy. We discuss our system's performance on answering\nconceptual questions from a machine learning course and various failure modes.", "published": "2024-07-30 20:24:44", "link": "http://arxiv.org/abs/2407.21170v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Advancing Vietnamese Visual Question Answering with Transformer and\n  Convolutional Integration", "abstract": "Visual Question Answering (VQA) has recently emerged as a potential research\ndomain, captivating the interest of many in the field of artificial\nintelligence and computer vision. Despite the prevalence of approaches in\nEnglish, there is a notable lack of systems specifically developed for certain\nlanguages, particularly Vietnamese. This study aims to bridge this gap by\nconducting comprehensive experiments on the Vietnamese Visual Question\nAnswering (ViVQA) dataset, demonstrating the effectiveness of our proposed\nmodel. In response to community interest, we have developed a model that\nenhances image representation capabilities, thereby improving overall\nperformance in the ViVQA system. Specifically, our model integrates the\nBootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)\nand the convolutional neural network EfficientNet to extract and process both\nlocal and global features from images. This integration leverages the strengths\nof transformer-based architectures for capturing comprehensive contextual\ninformation and convolutional networks for detailed local features. By freezing\nthe parameters of these pre-trained models, we significantly reduce the\ncomputational cost and training time, while maintaining high performance. This\napproach significantly improves image representation and enhances the\nperformance of existing VQA systems. We then leverage a multi-modal fusion\nmodule based on a general-purpose multi-modal foundation model (BEiT-3) to fuse\nthe information between visual and textual features. Our experimental findings\ndemonstrate that our model surpasses competing baselines, achieving promising\nperformance. This is particularly evident in its accuracy of $71.04\\%$ on the\ntest set of the ViVQA dataset, marking a significant advancement in our\nresearch area. The code is available at https://github.com/nngocson2002/ViVQA.", "published": "2024-07-30 22:32:50", "link": "http://arxiv.org/abs/2407.21229v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Affective Computing in the Era of Large Language Models: A Survey from\n  the NLP Perspective", "abstract": "Affective Computing (AC), integrating computer science, psychology, and\ncognitive science knowledge, aims to enable machines to recognize, interpret,\nand simulate human emotions.To create more value, AC can be applied to diverse\nscenarios, including social media, finance, healthcare, education, etc.\nAffective Computing (AC) includes two mainstream tasks, i.e., Affective\nUnderstanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained\nLanguage Models (PLMs) for AU tasks has succeeded considerably. However, these\nmodels lack generalization ability, requiring specialized models for specific\ntasks. Additionally, traditional PLMs face challenges in AG, particularly in\ngenerating diverse and emotionally rich responses. The emergence of Large\nLanguage Models (LLMs), such as the ChatGPT series and LLaMA models, brings new\nopportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess\ncapabilities of in-context learning, common sense reasoning, and advanced\nsequence generation, which present unprecedented opportunities for AU. To\nprovide a comprehensive overview of AC in the LLMs era from an NLP perspective,\nwe summarize the development of LLMs research in this field, aiming to offer\nnew insights. Specifically, we first summarize the traditional tasks related to\nAC and introduce the preliminary study based on LLMs. Subsequently, we outline\nthe relevant techniques of popular LLMs to improve AC tasks, including\nInstruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss\nfull parameter fine-tuning and parameter-efficient methods such as LoRA,\nP-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot,\nFew-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To\nclearly understand the performance of LLMs on different Affective Computing\ntasks, we further summarize the existing benchmarks and evaluation methods.", "published": "2024-07-30 08:12:04", "link": "http://arxiv.org/abs/2408.04638v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural\n  Language", "abstract": "This paper presents a conversational pipeline for crafting domain knowledge\nfor complex neuro-symbolic models through natural language prompts. It\nleverages large language models to generate declarative programs in the\nDomiKnowS framework. The programs in this framework express concepts and their\nrelationships as a graph in addition to logical constraints between them. The\ngraph, later, can be connected to trainable neural models according to those\nspecifications. Our proposed pipeline utilizes techniques like dynamic\nin-context demonstration retrieval, model refinement based on feedback from a\nsymbolic parser, visualization, and user interaction to generate the tasks'\nstructure and formal knowledge representation. This approach empowers domain\nexperts, even those not well-versed in ML/AI, to formally declare their\nknowledge to be incorporated in customized neural models in the DomiKnowS\nframework.", "published": "2024-07-30 03:10:30", "link": "http://arxiv.org/abs/2407.20513v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Machine Unlearning in Generative AI: A Survey", "abstract": "Generative AI technologies have been deployed in many places, such as\n(multimodal) large language models and vision generative models. Their\nremarkable performance should be attributed to massive training data and\nemergent reasoning abilities. However, the models would memorize and generate\nsensitive, biased, or dangerous information originated from the training data\nespecially those from web crawl. New machine unlearning (MU) techniques are\nbeing developed to reduce or eliminate undesirable knowledge and its effects\nfrom the models, because those that were designed for traditional\nclassification tasks could not be applied for Generative AI. We offer a\ncomprehensive survey on many things about MU in Generative AI, such as a new\nproblem formulation, evaluation methods, and a structured discussion on the\nadvantages and limitations of different kinds of MU techniques. It also\npresents several critical challenges and promising directions in MU research. A\ncurated list of readings can be found:\nhttps://github.com/franciscoliu/GenAI-MU-Reading.", "published": "2024-07-30 03:26:09", "link": "http://arxiv.org/abs/2407.20516v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Survey of Design Paradigms for Social Robots", "abstract": "The demand for social robots in fields like healthcare, education, and\nentertainment increases due to their emotional adaptation features. These\nrobots leverage multimodal communication, incorporating speech, facial\nexpressions, and gestures to enhance user engagement and emotional support. The\nunderstanding of design paradigms of social robots is obstructed by the\ncomplexity of the system and the necessity to tune it to a specific task. This\narticle provides a structured review of social robot design paradigms,\ncategorizing them into cognitive architectures, role design models, linguistic\nmodels, communication flow, activity system models, and integrated design\nmodels. By breaking down the articles on social robot design and application\nbased on these paradigms, we highlight the strengths and areas for improvement\nin current approaches. We further propose our original integrated design model\nthat combines the most important aspects of the design of social robots. Our\napproach shows the importance of integrating operational, communicational, and\nemotional dimensions to create more adaptive and empathetic interactions\nbetween robots and humans.", "published": "2024-07-30 05:22:31", "link": "http://arxiv.org/abs/2407.20556v1", "categories": ["cs.RO", "cs.CL", "cs.CY"], "primary_category": "cs.RO"}
{"title": "Comparison of Large Language Models for Generating Contextually Relevant\n  Questions", "abstract": "This study explores the effectiveness of Large Language Models (LLMs) for\nAutomatic Question Generation in educational settings. Three LLMs are compared\nin their ability to create questions from university slide text without\nfine-tuning. Questions were obtained in a two-step pipeline: first, answer\nphrases were extracted from slides using Llama 2-Chat 13B; then, the three\nmodels generated questions for each answer. To analyze whether the questions\nwould be suitable in educational applications for students, a survey was\nconducted with 46 students who evaluated a total of 246 questions across five\nmetrics: clarity, relevance, difficulty, slide relation, and question-answer\nalignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan\nT5 XXL by a small margin, particularly in terms of clarity and question-answer\nalignment. GPT-3.5 especially excels at tailoring questions to match the input\nanswers. The contribution of this research is the analysis of the capacity of\nLLMs for Automatic Question Generation in education.", "published": "2024-07-30 06:23:59", "link": "http://arxiv.org/abs/2407.20578v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "K.3"], "primary_category": "cs.CL"}
{"title": "Decoding Linguistic Representations of Human Brain", "abstract": "Language, as an information medium created by advanced organisms, has always\nbeen a concern of neuroscience regarding how it is represented in the brain.\nDecoding linguistic representations in the evoked brain has shown\ngroundbreaking achievements, thanks to the rapid improvement of neuroimaging,\nmedical technology, life sciences and artificial intelligence. In this work, we\npresent a taxonomy of brain-to-language decoding of both textual and speech\nformats. This work integrates two types of research: neuroscience focusing on\nlanguage understanding and deep learning-based brain decoding. Generating\ndiscernible language information from brain activity could not only help those\nwith limited articulation, especially amyotrophic lateral sclerosis (ALS)\npatients but also open up a new way for the next generation's brain-computer\ninterface (BCI). This article will help brain scientists and deep-learning\nresearchers to gain a bird's eye view of fine-grained language perception, and\nthus facilitate their further investigation and research of neural process and\nlanguage decoding.", "published": "2024-07-30 07:55:44", "link": "http://arxiv.org/abs/2407.20622v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks", "abstract": "Recent vision-language foundation models, such as CLIP, have demonstrated\nsuperior capabilities in learning representations that can be transferable\nacross diverse range of downstream tasks and domains. With the emergence of\nsuch powerful models, it has become crucial to effectively leverage their\ncapabilities in tackling challenging vision tasks. On the other hand, only a\nfew works have focused on devising adversarial examples that transfer well to\nboth unknown domains and model architectures. In this paper, we propose a novel\ntransfer attack method called PDCL-Attack, which leverages the CLIP model to\nenhance the transferability of adversarial perturbations generated by a\ngenerative model-based attack framework. Specifically, we formulate an\neffective prompt-driven feature guidance by harnessing the semantic\nrepresentation power of text, particularly from the ground-truth class labels\nof input images. To the best of our knowledge, we are the first to introduce\nprompt learning to enhance the transferable generative attacks. Extensive\nexperiments conducted across various cross-domain and cross-model settings\nempirically validate our approach, demonstrating its superiority over\nstate-of-the-art methods.", "published": "2024-07-30 08:52:16", "link": "http://arxiv.org/abs/2407.20657v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Industrial-Grade Smart Troubleshooting through Causal Technical Language\n  Processing: a Proof of Concept", "abstract": "This paper describes the development of a causal diagnosis approach for\ntroubleshooting an industrial environment on the basis of the technical\nlanguage expressed in Return on Experience records. The proposed method\nleverages the vectorized linguistic knowledge contained in the distributed\nrepresentation of a Large Language Model, and the causal associations entailed\nby the embedded failure modes and mechanisms of the industrial assets. The\npaper presents the elementary but essential concepts of the solution, which is\nconceived as a causality-aware retrieval augmented generation system, and\nillustrates them experimentally on a real-world Predictive Maintenance setting.\nFinally, it discusses avenues of improvement for the maturity of the utilized\ncausal technology to meet the robustness challenges of increasingly complex\nscenarios in the industry.", "published": "2024-07-30 09:53:55", "link": "http://arxiv.org/abs/2407.20700v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.AI"}
{"title": "JaColBERTv2.5: Optimising Multi-Vector Retrievers to Create\n  State-of-the-Art Japanese Retrievers with Constrained Resources", "abstract": "Neural Information Retrieval has advanced rapidly in high-resource languages,\nbut progress in lower-resource ones such as Japanese has been hindered by data\nscarcity, among other challenges. Consequently, multilingual models have\ndominated Japanese retrieval, despite their computational inefficiencies and\ninability to capture linguistic nuances. While recent multi-vector monolingual\nmodels like JaColBERT have narrowed this gap, they still lag behind\nmultilingual methods in large-scale evaluations. This work addresses the\nsuboptimal training methods of multi-vector retrievers in lower-resource\nsettings, focusing on Japanese. We systematically evaluate and improve key\naspects of the inference and training settings of JaColBERT, and more broadly,\nmulti-vector models. We further enhance performance through a novel checkpoint\nmerging step, showcasing it to be an effective way of combining the benefits of\nfine-tuning with the generalization capabilities of the original checkpoint.\nBuilding on our analysis, we introduce a novel training recipe, resulting in\nthe JaColBERTv2.5 model. JaColBERTv2.5, with only 110 million parameters and\ntrained in under 15 hours on 4 A100 GPUs, significantly outperforms all\nexisting methods across all common benchmarks, reaching an average score of\n0.754, significantly above the previous best of 0.720. To support future\nresearch, we make our final models, intermediate checkpoints and all data used\npublicly available.", "published": "2024-07-30 11:42:19", "link": "http://arxiv.org/abs/2407.20750v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Effective Black Box Testing of Sentiment Analysis Classification\n  Networks", "abstract": "Transformer-based neural networks have demonstrated remarkable performance in\nnatural language processing tasks such as sentiment analysis. Nevertheless, the\nissue of ensuring the dependability of these complicated architectures through\ncomprehensive testing is still open. This paper presents a collection of\ncoverage criteria specifically designed to assess test suites created for\ntransformer-based sentiment analysis networks. Our approach utilizes input\nspace partitioning, a black-box method, by considering emotionally relevant\nlinguistic features such as verbs, adjectives, adverbs, and nouns. In order to\neffectively produce test cases that encompass a wide range of emotional\nelements, we utilize the k-projection coverage metric. This metric minimizes\nthe complexity of the problem by examining subsets of k features at the same\ntime, hence reducing dimensionality. Large language models are employed to\ngenerate sentences that display specific combinations of emotional features.\nThe findings from experiments obtained from a sentiment analysis dataset\nillustrate that our criteria and generated tests have led to an average\nincrease of 16\\% in test coverage. In addition, there is a corresponding\naverage decrease of 6.5\\% in model accuracy, showing the ability to identify\nvulnerabilities. Our work provides a foundation for improving the dependability\nof transformer-based sentiment analysis systems through comprehensive test\nevaluation.", "published": "2024-07-30 14:58:11", "link": "http://arxiv.org/abs/2407.20884v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Automated Review Generation Method Based on Large Language Models", "abstract": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations.", "published": "2024-07-30 15:26:36", "link": "http://arxiv.org/abs/2407.20906v4", "categories": ["cs.CL", "cs.AI", "physics.data-an"], "primary_category": "cs.CL"}
{"title": "Accelerating Large Language Model Inference with Self-Supervised Early\n  Exits", "abstract": "This paper presents a novel technique for accelerating inference in large,\npre-trained language models (LLMs) by introducing early exits during inference.\nThe computational demands of these models, used across a wide range of\napplications, can be substantial. By capitalizing on the inherent variability\nin token complexity, our approach enables selective acceleration of the\ninference process. Specifically, we propose the integration of early exit\n''heads'' atop existing transformer layers, which facilitate conditional\nterminations based on a confidence metric. These heads are trained in a\nself-supervised manner using the model's own predictions as training data,\nthereby eliminating the need for additional annotated data. The confidence\nmetric, established using a calibration set, ensures a desired level of\naccuracy while enabling early termination when confidence exceeds a\npredetermined threshold. Notably, our method preserves the original accuracy\nand reduces computational time on certain tasks, leveraging the existing\nknowledge of pre-trained LLMs without requiring extensive retraining. This\nlightweight, modular modification has the potential to greatly enhance the\npractical usability of LLMs, particularly in applications like real-time\nlanguage processing in resource-constrained environments.", "published": "2024-07-30 07:58:28", "link": "http://arxiv.org/abs/2407.21082v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Entropy, Thermodynamics and the Geometrization of the Language Model", "abstract": "In this paper, we discuss how pure mathematics and theoretical physics can be\napplied to the study of language models. Using set theory and analysis, we\nformulate mathematically rigorous definitions of language models, and introduce\nthe concept of the moduli space of distributions for a language model. We\nformulate a generalized distributional hypothesis using functional analysis and\ntopology. We define the entropy function associated with a language model and\nshow how it allows us to understand many interesting phenomena in languages. We\nargue that the zero points of the entropy function and the points where the\nentropy is close to 0 are the key obstacles for an LLM to approximate an\nintelligent language model, which explains why good LLMs need billions of\nparameters. Using the entropy function, we formulate a conjecture about AGI.\n  Then, we show how thermodynamics gives us an immediate interpretation to\nlanguage models. In particular we will define the concepts of partition\nfunction, internal energy and free energy for a language model, which offer\ninsights into how language models work. Based on these results, we introduce a\ngeneral concept of the geometrization of language models and define what is\ncalled the Boltzmann manifold. While the current LLMs are the special cases of\nthe Boltzmann manifold.", "published": "2024-07-30 17:11:15", "link": "http://arxiv.org/abs/2407.21092v1", "categories": ["cs.CL", "cond-mat.stat-mech", "hep-th", "math.DG", "68T01", "I.2.7"], "primary_category": "cs.CL"}
{"title": "GenRec: Generative Sequential Recommendation with Large Language Models", "abstract": "Sequential recommendation is a task to capture hidden user preferences from\nhistorical user item interaction data and recommend next items for the user.\nSignificant progress has been made in this domain by leveraging classification\nbased learning methods. Inspired by the recent paradigm of 'pretrain, prompt\nand predict' in NLP, we consider sequential recommendation as a sequence to\nsequence generation task and propose a novel model named Generative\nRecommendation (GenRec). Unlike classification based models that learn explicit\nuser and item representations, GenRec utilizes the sequence modeling capability\nof Transformer and adopts the masked item prediction objective to effectively\nlearn the hidden bidirectional sequential patterns. Different from existing\ngenerative sequential recommendation models, GenRec does not rely on manually\ndesigned hard prompts. The input to GenRec is textual user item sequence and\nthe output is top ranked next items. Moreover, GenRec is lightweight and\nrequires only a few hours to train effectively in low-resource settings, making\nit highly applicable to real-world scenarios and helping to democratize large\nlanguage models in the sequential recommendation domain. Our extensive\nexperiments have demonstrated that GenRec generalizes on various public\nreal-world datasets and achieves state-of-the-art results. Our experiments also\nvalidate the effectiveness of the the proposed masked item prediction objective\nthat improves the model performance by a large margin.", "published": "2024-07-30 20:58:36", "link": "http://arxiv.org/abs/2407.21191v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Adaptive Pre-training Data Detection for Large Language Models via\n  Surprising Tokens", "abstract": "While large language models (LLMs) are extensively used, there are raising\nconcerns regarding privacy, security, and copyright due to their opaque\ntraining data, which brings the problem of detecting pre-training data on the\ntable. Current solutions to this problem leverage techniques explored in\nmachine learning privacy such as Membership Inference Attacks (MIAs), which\nheavily depend on LLMs' capability of verbatim memorization. However, this\nreliance presents challenges, especially given the vast amount of training data\nand the restricted number of effective training epochs. In this paper, we\npropose an adaptive pre-training data detection method which alleviates this\nreliance and effectively amplify the identification. Our method adaptively\nlocates \\textit{surprising tokens} of the input. A token is surprising to a LLM\nif the prediction on the token is \"certain but wrong\", which refers to low\nShannon entropy of the probability distribution and low probability of the\nground truth token at the same time. By using the prediction probability of\nsurprising tokens to measure \\textit{surprising}, the detection method is\nachieved based on the simple hypothesis that seeing seen data is less\nsurprising for the model compared with seeing unseen data. The method can be\napplied without any access to the the pre-training data corpus or additional\ntraining like reference models. Our approach exhibits a consistent enhancement\ncompared to existing methods in diverse experiments conducted on various\nbenchmarks and models, achieving a maximum improvement of 29.5\\%. We also\nintroduce a new benchmark Dolma-Book developed upon a novel framework, which\nemploys book data collected both before and after model training to provide\nfurther evaluation.", "published": "2024-07-30 23:43:59", "link": "http://arxiv.org/abs/2407.21248v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstractive summarization from Audio Transcription", "abstract": "Currently, large language models are gaining popularity, their achievements\nare used in many areas, ranging from text translation to generating answers to\nqueries. However, the main problem with these new machine learning algorithms\nis that training such models requires large computing resources that only large\nIT companies have. To avoid this problem, a number of methods (LoRA,\nquantization) have been proposed so that existing models can be effectively\nfine-tuned for specific tasks. In this paper, we propose an E2E (end to end)\naudio summarization model using these techniques. In addition, this paper\nexamines the effectiveness of these approaches to the problem under\nconsideration and draws conclusions about the applicability of these methods.", "published": "2024-07-30 16:38:38", "link": "http://arxiv.org/abs/2408.04639v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lyrics Transcription for Humans: A Readability-Aware Benchmark", "abstract": "Writing down lyrics for human consumption involves not only accurately\ncapturing word sequences, but also incorporating punctuation and formatting for\nclarity and to convey contextual information. This includes song structure,\nemotional emphasis, and contrast between lead and background vocals. While\nautomatic lyrics transcription (ALT) systems have advanced beyond producing\nunstructured strings of words and are able to draw on wider context, ALT\nbenchmarks have not kept pace and continue to focus exclusively on words. To\naddress this gap, we introduce Jam-ALT, a comprehensive lyrics transcription\nbenchmark. The benchmark features a complete revision of the JamendoLyrics\ndataset, in adherence to industry standards for lyrics transcription and\nformatting, along with evaluation metrics designed to capture and assess the\nlyric-specific nuances, laying the foundation for improving the readability of\nlyrics. We apply the benchmark to recent transcription systems and present\nadditional error analysis, as well as an experimental comparison with a\nclassical music dataset.", "published": "2024-07-30 14:20:09", "link": "http://arxiv.org/abs/2408.06370v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Effects of a Prompt Engineering Intervention on Undergraduate Students'\n  AI Self-Efficacy, AI Knowledge and Prompt Engineering Ability: A Mixed\n  Methods Study", "abstract": "Prompt engineering is critical for effective interaction with large language\nmodels (LLMs) such as ChatGPT. However, efforts to teach this skill to students\nhave been limited. This study designed and implemented a prompt engineering\nintervention, examining its influence on undergraduate students' AI\nself-efficacy, AI knowledge, and proficiency in creating effective prompts. The\nintervention involved 27 students who participated in a 100-minute workshop\nconducted during their history course at a university in Hong Kong. During the\nworkshop, students were introduced to prompt engineering strategies, which they\napplied to plan the course's final essay task. Multiple data sources were\ncollected, including students' responses to pre- and post-workshop\nquestionnaires, pre- and post-workshop prompt libraries, and written\nreflections. The study's findings revealed that students demonstrated a higher\nlevel of AI self-efficacy, an enhanced understanding of AI concepts, and\nimproved prompt engineering skills because of the intervention. These findings\nhave implications for AI literacy education, as they highlight the importance\nof prompt engineering training for specific higher education use cases. This is\na significant shift from students haphazardly and intuitively learning to\nengineer prompts. Through prompt engineering education, educators can faciitate\nstudents' effective navigation and leverage of LLMs to support their\ncoursework.", "published": "2024-07-30 15:05:24", "link": "http://arxiv.org/abs/2408.07302v1", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "From Feature Importance to Natural Language Explanations Using LLMs with\n  RAG", "abstract": "As machine learning becomes increasingly integral to autonomous\ndecision-making processes involving human interaction, the necessity of\ncomprehending the model's outputs through conversational means increases. Most\nrecently, foundation models are being explored for their potential as post hoc\nexplainers, providing a pathway to elucidate the decision-making mechanisms of\npredictive models. In this work, we introduce traceable question-answering,\nleveraging an external knowledge repository to inform the responses of Large\nLanguage Models (LLMs) to user queries within a scene understanding task. This\nknowledge repository comprises contextual details regarding the model's output,\ncontaining high-level features, feature importance, and alternative\nprobabilities. We employ subtractive counterfactual reasoning to compute\nfeature importance, a method that entails analysing output variations resulting\nfrom decomposing semantic features. Furthermore, to maintain a seamless\nconversational flow, we integrate four key characteristics - social, causal,\nselective, and contrastive - drawn from social science research on human\nexplanations into a single-shot prompt, guiding the response generation\nprocess. Our evaluation demonstrates that explanations generated by the LLMs\nencompassed these elements, indicating its potential to bridge the gap between\ncomplex model outputs and natural language expressions.", "published": "2024-07-30 17:27:20", "link": "http://arxiv.org/abs/2407.20990v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "$T\\bar{a}laGen:$ A System for Automatic $T\\bar{a}la$ Identification and\n  Generation", "abstract": "In Hindustani classical music, the tabla plays an important role as a\nrhythmic backbone and accompaniment. In applications like computer-based music\nanalysis, learning singing, and learning musical instruments, tabla stroke\ntranscription, $t\\bar{a}la$ identification, and generation are crucial. This\npaper proposes a comprehensive system aimed at addressing these challenges. For\ntabla stroke transcription, we propose a novel approach based on model-agnostic\nmeta-learning (MAML) that facilitates the accurate identification of tabla\nstrokes using minimal data. Leveraging these transcriptions, the system\nintroduces two novel $t\\bar{a}la$ identification methods based on the sequence\nanalysis of tabla strokes. \\par Furthermore, the paper proposes a framework for\n$t\\bar{a}la$ generation to bridge traditional and modern learning methods. This\nframework utilizes finite state transducers (FST) and linear time-invariant\n(LTI) filters to generate $t\\bar{a}las$ with real-time tempo control through\nuser interaction, enhancing practice sessions and musical education.\nExperimental evaluations on tabla solo and concert datasets demonstrate the\nsystem's exceptional performance on real-world data and its ability to\noutperform existing methods. Additionally, the proposed $t\\bar{a}la$\nidentification methods surpass state-of-the-art techniques. The contributions\nof this paper include a combined approach to tabla stroke transcription,\ninnovative $t\\bar{a}la$ identification techniques, and a robust framework for\n$t\\bar{a}la$ generation that handles the rhythmic complexities of Hindustani\nmusic.", "published": "2024-07-30 16:15:50", "link": "http://arxiv.org/abs/2407.20935v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SuperCodec: A Neural Speech Codec with Selective Back-Projection Network", "abstract": "Neural speech coding is a rapidly developing topic, where state-of-the-art\napproaches now exhibit superior compression performance than conventional\nmethods. Despite significant progress, existing methods still have limitations\nin preserving and reconstructing fine details for optimal reconstruction,\nespecially at low bitrates. In this study, we introduce SuperCodec, a neural\nspeech codec that achieves state-of-the-art performance at low bitrates. It\nemploys a novel back projection method with selective feature fusion for\naugmented representation. Specifically, we propose to use Selective Up-sampling\nBack Projection (SUBP) and Selective Down-sampling Back Projection (SDBP)\nmodules to replace the standard up- and down-sampling layers at the encoder and\ndecoder, respectively. Experimental results show that our method outperforms\nthe existing neural speech codecs operating at various bitrates. Specifically,\nour proposed method can achieve higher quality reconstructed speech at 1 kbps\nthan Lyra V2 at 3.2 kbps and Encodec at 6 kbps.", "published": "2024-07-30 04:12:17", "link": "http://arxiv.org/abs/2407.20530v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Abusive Speech Detection in Indic Languages Using Acoustic Features", "abstract": "Abusive content in online social networks is a well-known problem that can\ncause serious psychological harm and incite hatred. The ability to upload audio\ndata increases the importance of developing methods to detect abusive content\nin speech recordings. However, simply transferring the mechanisms from written\nabuse detection would ignore relevant information such as emotion and tone. In\naddition, many current algorithms require training in the specific language for\nwhich they are being used. This paper proposes to use acoustic and prosodic\nfeatures to classify abusive content. We used the ADIMA data set, which\ncontains recordings from ten Indic languages, and trained different models in\nmultilingual and cross-lingual settings. Our results show that it is possible\nto classify abusive and non-abusive content using only acoustic and prosodic\nfeatures. The most important and influential features are discussed.", "published": "2024-07-30 13:13:38", "link": "http://arxiv.org/abs/2407.20808v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PiCoGen: Generate Piano Covers with a Two-stage Approach", "abstract": "Cover song generation stands out as a popular way of music making in the\nmusic-creative community. In this study, we introduce Piano Cover Generation\n(PiCoGen), a two-stage approach for automatic cover song generation that\ntranscribes the melody line and chord progression of a song given its audio\nrecording, and then uses the resulting lead sheet as the condition to generate\na piano cover in the symbolic domain. This approach is advantageous in that it\ndoes not required paired data of covers and their original songs for training.\nCompared to an existing approach that demands such paired data, our evaluation\nshows that PiCoGen demonstrates competitive or even superior performance across\nsongs of different musical genres.", "published": "2024-07-30 14:58:02", "link": "http://arxiv.org/abs/2407.20883v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Self-Supervised Models for Automatic Whispered Speech\n  Recognition", "abstract": "In automatic speech recognition, any factor that alters the acoustic\nproperties of speech can pose a challenge to the system's performance. This\npaper presents a novel approach for automatic whispered speech recognition in\nthe Irish dialect using the self-supervised WavLM model. Conventional automatic\nspeech recognition systems often fail to accurately recognise whispered speech\ndue to its distinct acoustic properties and the scarcity of relevant training\ndata. To address this challenge, we utilized a pre-trained WavLM model,\nfine-tuned with a combination of whispered and normal speech data from the\nwTIMIT and CHAINS datasets, which include the English language in Singaporean\nand Irish dialects, respectively. Our baseline evaluation with the OpenAI\nWhisper model highlighted its limitations, achieving a Word Error Rate (WER) of\n18.8% and a Character Error Rate (CER) of 4.24% on whispered speech. In\ncontrast, the proposed WavLM-based system significantly improved performance,\nachieving a WER of 9.22% and a CER of 2.59%. These results demonstrate the\nefficacy of our approach in recognising whispered speech and underscore the\nimportance of tailored acoustic modeling for robust automatic speech\nrecognition systems. This study provides valuable insights into developing\neffective automatic speech recognition solutions for challenging speech\naffected by whisper and dialect. The source codes for this paper are freely\navailable.", "published": "2024-07-30 21:45:37", "link": "http://arxiv.org/abs/2407.21211v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DeepSpeech models show Human-like Performance and Processing of Cochlear\n  Implant Inputs", "abstract": "Cochlear implants(CIs) are arguably the most successful neural implant,\nhaving restored hearing to over one million people worldwide. While CI research\nhas focused on modeling the cochlear activations in response to low-level\nacoustic features, we hypothesize that the success of these implants is due in\nlarge part to the role of the upstream network in extracting useful features\nfrom a degraded signal and learned statistics of language to resolve the\nsignal. In this work, we use the deep neural network (DNN) DeepSpeech2, as a\nparadigm to investigate how natural input and cochlear implant-based inputs are\nprocessed over time. We generate naturalistic and cochlear implant-like inputs\nfrom spoken sentences and test the similarity of model performance to human\nperformance on analogous phoneme recognition tests. Our model reproduces error\npatterns in reaction time and phoneme confusion patterns under noise conditions\nin normal hearing and CI participant studies. We then use interpretability\ntechniques to determine where and when confusions arise when processing\nnaturalistic and CI-like inputs. We find that dynamics over time in each layer\nare affected by context as well as input type. Dynamics of all phonemes diverge\nduring confusion and comprehension within the same time window, which is\ntemporally shifted backward in each layer of the network. There is a modulation\nof this signal during processing of CI which resembles changes in human EEG\nsignals in the auditory stream. This reduction likely relates to the reduction\nof encoded phoneme identity. These findings suggest that we have a viable model\nin which to explore the loss of speech-related information in time and that we\ncan use it to find population-level encoding signals to target when optimizing\ncochlear implant inputs to improve encoding of essential speech-related\ninformation and improve perception.", "published": "2024-07-30 04:32:27", "link": "http://arxiv.org/abs/2407.20535v1", "categories": ["cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
{"title": "EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos", "abstract": "We introduce EgoSonics, a method to generate semantically meaningful and\nsynchronized audio tracks conditioned on silent egocentric videos. Generating\naudio for silent egocentric videos could open new applications in virtual\nreality, assistive technologies, or for augmenting existing datasets. Existing\nwork has been limited to domains like speech, music, or impact sounds and\ncannot capture the broad range of audio frequencies found in egocentric videos.\nEgoSonics addresses these limitations by building on the strengths of latent\ndiffusion models for conditioned audio synthesis. We first encode and process\npaired audio-video data to make them suitable for generation. The encoded data\nis then used to train a model that can generate an audio track that captures\nthe semantics of the input video. Our proposed SyncroNet builds on top of\nControlNet to provide control signals that enables generation of temporally\nsynchronized audio. Extensive evaluations and a comprehensive user study show\nthat our model outperforms existing work in audio quality, and in our proposed\nsynchronization evaluation method. Furthermore, we demonstrate downstream\napplications of our model in improving video summarization.", "published": "2024-07-30 06:57:00", "link": "http://arxiv.org/abs/2407.20592v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Integrating audiological datasets via federated merging of Auditory\n  Profiles", "abstract": "Audiological datasets contain valuable knowledge about hearing loss in\npatients, which can be uncovered using data-driven, federated learning\ntechniques. Our previous approach summarized patient information from one\naudiological dataset into distinct Auditory Profiles (APs). To obtain a better\nestimate of the audiological patient population, however, patient patterns must\nbe analyzed across multiple, separated datasets, and finally, be integrated\ninto a combined set of APs.\n  This study aimed at extending the existing profile generation pipeline with\nan AP merging step, enabling the combination of APs from different datasets\nbased on their similarity across audiological measures. The 13 previously\ngenerated APs (NA=595) were merged with 31 newly generated APs from a second\ndataset (NB=1272) using a similarity score derived from the overlapping\ndensities of common features across the two datasets. To ensure clinical\napplicability, random forest models were created for various scenarios,\nencompassing different combinations of audiological measures.\n  A new set with 13 combined APs is proposed, providing separable profiles,\nwhich still capture detailed patient information from various test outcome\ncombinations. The classification performance across these profiles is\nsatisfactory. The best performance was achieved using a combination of loudness\nscaling, audiogram and speech test information, while single measures performed\nworst.\n  The enhanced profile generation pipeline demonstrates the feasibility of\ncombining APs across datasets, which should generalize to all datasets and\ncould lead to an interpretable global profile set in the future. The\nclassification models maintain clinical applicability.", "published": "2024-07-30 12:08:44", "link": "http://arxiv.org/abs/2407.20765v2", "categories": ["physics.med-ph", "cs.SD", "eess.AS", "physics.data-an"], "primary_category": "physics.med-ph"}
{"title": "Emotion-driven Piano Music Generation via Two-stage Disentanglement and\n  Functional Representation", "abstract": "Managing the emotional aspect remains a challenge in automatic music\ngeneration. Prior works aim to learn various emotions at once, leading to\ninadequate modeling. This paper explores the disentanglement of emotions in\npiano performance generation through a two-stage framework. The first stage\nfocuses on valence modeling of lead sheet, and the second stage addresses\narousal modeling by introducing performance-level attributes. To further\ncapture features that shape valence, an aspect less explored by previous\napproaches, we introduce a novel functional representation of symbolic music.\nThis representation aims to capture the emotional impact of major-minor\ntonality, as well as the interactions among notes, chords, and key signatures.\nObjective and subjective experiments validate the effectiveness of our\nframework in both emotional valence and arousal modeling. We further leverage\nour framework in a novel application of emotional controls, showing a broad\npotential in emotion-driven music generation.", "published": "2024-07-30 16:29:28", "link": "http://arxiv.org/abs/2407.20955v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions", "abstract": "Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.", "published": "2024-07-30 16:43:24", "link": "http://arxiv.org/abs/2407.20962v3", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Contrasting Deep Learning Models for Direct Respiratory Insufficiency\n  Detection Versus Blood Oxygen Saturation Estimation", "abstract": "We contrast high effectiveness of state of the art deep learning\narchitectures designed for general audio classification tasks, refined for\nrespiratory insufficiency (RI) detection and blood oxygen saturation (SpO$_2$)\nestimation and classification through automated audio analysis. Recently,\nmultiple deep learning architectures have been proposed to detect RI in COVID\npatients through audio analysis, achieving accuracy above 95% and F1-score\nabove 0.93. RI is a condition associated with low SpO$_2$ levels, commonly\ndefined as the threshold SpO$_2$ <92%. While SpO$_2$ serves as a crucial\ndeterminant of RI, a medical doctor's diagnosis typically relies on multiple\nfactors. These include respiratory frequency, heart rate, SpO$_2$ levels, among\nothers. Here we study pretrained audio neural networks (CNN6, CNN10 and CNN14)\nand the Masked Autoencoder (Audio-MAE) for RI detection, where these models\nachieve near perfect accuracy, surpassing previous results. Yet, for the\nregression task of estimating SpO$_2$ levels, the models achieve root mean\nsquare error values exceeding the accepted clinical range of 3.5% for finger\noximeters. Additionally, Pearson correlation coefficients fail to surpass 0.3.\nAs deep learning models perform better in classification than regression, we\ntransform SpO$_2$-regression into a SpO$_2$-threshold binary classification\nproblem, with a threshold of 92%. However, this task still yields an F1-score\nbelow 0.65. Thus, audio analysis offers valuable insights into a patient's RI\nstatus, but does not provide accurate information about actual SpO$_2$ levels,\nindicating a separation of domains in which voice and speech biomarkers may and\nmay not be useful in medical diagnostics under current technologies.", "published": "2024-07-30 17:26:16", "link": "http://arxiv.org/abs/2407.20989v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Computational music analysis from first principles", "abstract": "We use coupled hidden Markov models to automatically annotate the 371 Bach\nchorales in the Riemenschneider edition, a corpus containing approximately\n100,000 notes and 20,000 chords. We give three separate analyses that achieve\nprogressively greater accuracy at the cost of making increasingly strong\nassumptions about musical syntax. Although our method makes almost no use of\nhuman input, we are able to identify both chords and keys with an accuracy of\n85% or greater when compared to an expert human analysis, resulting in\nannotations accurate enough to be used for a range of music-theoretical\npurposes, while also being free of subjective human judgments. Our work bears\non longstanding debates about the objective reality of the structures\npostulated by standard Western harmonic theory, as well as on specific\nquestions about the nature of Western harmonic syntax.", "published": "2024-07-30 18:44:40", "link": "http://arxiv.org/abs/2407.21130v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal\n  Image Captioning", "abstract": "Multimodal machine learning models that combine visual and textual data are\nincreasingly being deployed in critical applications, raising significant\nsafety and security concerns due to their vulnerability to adversarial attacks.\nThis paper presents an effective strategy to enhance the robustness of\nmultimodal image captioning models against such attacks. By leveraging the Fast\nGradient Sign Method (FGSM) to generate adversarial examples and incorporating\nadversarial training techniques, we demonstrate improved model robustness on\ntwo benchmark datasets: Flickr8k and COCO. Our findings indicate that\nselectively training only the text decoder of the multimodal architecture shows\nperformance comparable to full adversarial training while offering increased\ncomputational efficiency. This targeted approach suggests a balance between\nrobustness and training costs, facilitating the ethical deployment of\nmultimodal AI systems across various domains.", "published": "2024-07-30 20:28:31", "link": "http://arxiv.org/abs/2407.21174v1", "categories": ["cs.CV", "cs.AI", "eess.AS", "I.2.7"], "primary_category": "cs.CV"}
