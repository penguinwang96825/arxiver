{"title": "Linking-Enhanced Pre-Training for Table Semantic Parsing", "abstract": "Recently pre-training models have significantly improved the performance of\nvarious NLP tasks by leveraging large-scale text corpora to improve the\ncontextual representation ability of the neural network. The large pre-training\nlanguage model has also been applied in the area of table semantic parsing.\nHowever, existing pre-training approaches have not carefully explored explicit\ninteraction relationships between a question and the corresponding database\nschema, which is a key ingredient for uncovering their semantic and structural\ncorrespondence. Furthermore, the question-aware representation learning in the\nschema grounding context has received less attention in pre-training\nobjective.To alleviate these issues, this paper designs two novel pre-training\nobjectives to impose the desired inductive bias into the learned\nrepresentations for table pre-training. We further propose a schema-aware\ncurriculum learning approach to mitigate the impact of noise and learn\neffectively from the pre-training data in an easy-to-hard manner. We evaluate\nour pre-trained framework by fine-tuning it on two benchmarks, Spider and\nSQUALL. The results demonstrate the effectiveness of our pre-training objective\nand curriculum compared to a variety of baselines.", "published": "2021-11-18 02:51:04", "link": "http://arxiv.org/abs/2111.09486v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How much do language models copy from their training data? Evaluating\n  linguistic novelty in text generation using RAVEN", "abstract": "Current language models can generate high-quality text. Are they simply\ncopying text they have seen before, or have they learned generalizable\nlinguistic abstractions? To tease apart these possibilities, we introduce\nRAVEN, a suite of analyses for assessing the novelty of generated text,\nfocusing on sequential structure (n-grams) and syntactic structure. We apply\nthese analyses to four neural language models (an LSTM, a Transformer,\nTransformer-XL, and GPT-2). For local structure - e.g., individual dependencies\n- model-generated text is substantially less novel than our baseline of\nhuman-generated text from each model's test set. For larger-scale structure -\ne.g., overall sentence structure - model-generated text is as novel or even\nmore novel than the human-generated baseline, but models still sometimes copy\nsubstantially, in some cases duplicating passages over 1,000 words long from\nthe training set. We also perform extensive manual analysis showing that\nGPT-2's novel text is usually well-formed morphologically and syntactically but\nhas reasonably frequent semantic issues (e.g., being self-contradictory).", "published": "2021-11-18 04:07:09", "link": "http://arxiv.org/abs/2111.09509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in\n  Summarization", "abstract": "In the summarization domain, a key requirement for summaries is to be\nfactually consistent with the input document. Previous work has found that\nnatural language inference (NLI) models do not perform competitively when\napplied to inconsistency detection. In this work, we revisit the use of NLI for\ninconsistency detection, finding that past work suffered from a mismatch in\ninput granularity between NLI datasets (sentence-level), and inconsistency\ndetection (document level). We provide a highly effective and light-weight\nmethod called SummaCConv that enables NLI models to be successfully used for\nthis task by segmenting documents into sentence units and aggregating scores\nbetween pairs of sentences. On our newly introduced benchmark called SummaC\n(Summary Consistency) consisting of six large inconsistency detection datasets,\nSummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%,\na 5% point improvement compared to prior work. We make the models and datasets\navailable: https://github.com/tingofurro/summac", "published": "2021-11-18 05:02:31", "link": "http://arxiv.org/abs/2111.09525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Expansion and Retargeting of Arabic Offensive Language\n  Training", "abstract": "Rampant use of offensive language on social media led to recent efforts on\nautomatic identification of such language. Though offensive language has\ngeneral characteristics, attacks on specific entities may exhibit distinct\nphenomena such as malicious alterations in the spelling of names. In this\npaper, we present a method for identifying entity specific offensive language.\nWe employ two key insights, namely that replies on Twitter often imply\nopposition and some accounts are persistent in their offensiveness towards\nspecific targets. Using our methodology, we are able to collect thousands of\ntargeted offensive tweets. We show the efficacy of the approach on Arabic\ntweets with 13% and 79% relative F1-measure improvement in entity specific\noffensive language detection when using deep-learning based and support vector\nmachine based classifiers respectively. Further, expanding the training set\nwith automatically identified offensive tweets directed at multiple entities\ncan improve F1-measure by 48%.", "published": "2021-11-18 08:25:09", "link": "http://arxiv.org/abs/2111.09574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeking Common but Distinguishing Difference, A Joint Aspect-based\n  Sentiment Analysis Model", "abstract": "Aspect-based sentiment analysis (ABSA) task consists of three typical\nsubtasks: aspect term extraction, opinion term extraction, and sentiment\npolarity classification. These three subtasks are usually performed jointly to\nsave resources and reduce the error propagation in the pipeline. However, most\nof the existing joint models only focus on the benefits of encoder sharing\nbetween subtasks but ignore the difference. Therefore, we propose a joint ABSA\nmodel, which not only enjoys the benefits of encoder sharing but also focuses\non the difference to improve the effectiveness of the model. In detail, we\nintroduce a dual-encoder design, in which a pair encoder especially focuses on\ncandidate aspect-opinion pair classification, and the original encoder keeps\nattention on sequence labeling. Empirical results show that our proposed model\nshows robustness and significantly outperforms the previous state-of-the-art on\nfour benchmark datasets.", "published": "2021-11-18 11:24:48", "link": "http://arxiv.org/abs/2111.09634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the Sentiment Analysis of Dravidian Languages in Code-Mixed\n  Text", "abstract": "We present the results of the Dravidian-CodeMix shared task held at FIRE\n2021, a track on sentiment analysis for Dravidian Languages in Code-Mixed Text.\nWe describe the task, its organization, and the submitted systems. This shared\ntask is the continuation of last year's Dravidian-CodeMix shared task held at\nFIRE 2020. This year's tasks included code-mixing at the intra-token and\ninter-token levels. Additionally, apart from Tamil and Malayalam, Kannada was\nalso introduced. We received 22 systems for Tamil-English, 15 systems for\nMalayalam-English, and 15 for Kannada-English. The top system for\nTamil-English, Malayalam-English and Kannada-English scored weighted average\nF1-score of 0.711, 0.804, and 0.630, respectively. In summary, the quality and\nquantity of the submission show that there is great interest in Dravidian\nlanguages in code-mixed setting and state of the art in this domain still needs\nmore improvement.", "published": "2021-11-18 17:27:32", "link": "http://arxiv.org/abs/2111.09811v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pegasus@Dravidian-CodeMix-HASOC2021: Analyzing Social Media Content for\n  Detection of Offensive Text", "abstract": "To tackle the conundrum of detecting offensive comments/posts which are\nconsiderably informal, unstructured, miswritten and code-mixed, we introduce\ntwo inventive methods in this research paper. Offensive comments/posts on the\nsocial media platforms, can affect an individual, a group or underage alike. In\norder to classify comments/posts in two popular Dravidian languages, Tamil and\nMalayalam, as a part of the HASOC - DravidianCodeMix FIRE 2021 shared task, we\nemploy two Transformer-based prototypes which successfully stood in the top 8\nfor all the tasks. The codes for our approach can be viewed and utilized.", "published": "2021-11-18 18:03:36", "link": "http://arxiv.org/abs/2111.09836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoBERTuito: a pre-trained language model for social media text in\n  Spanish", "abstract": "Since BERT appeared, Transformer language models and transfer learning have\nbecome state-of-the-art for Natural Language Understanding tasks. Recently,\nsome works geared towards pre-training specially-crafted models for particular\ndomains, such as scientific papers, medical documents, user-generated texts,\namong others. These domain-specific models have been shown to improve\nperformance significantly in most tasks. However, for languages other than\nEnglish such models are not widely available.\n  In this work, we present RoBERTuito, a pre-trained language model for\nuser-generated text in Spanish, trained on over 500 million tweets. Experiments\non a benchmark of tasks involving user-generated text showed that RoBERTuito\noutperformed other pre-trained language models in Spanish. In addition to this,\nour model achieves top results for some English-Spanish tasks of the Linguistic\nCode-Switching Evaluation benchmark (LinCE) and has also competitive\nperformance against monolingual models in English tasks. To facilitate further\nresearch, we make RoBERTuito publicly available at the HuggingFace model hub\ntogether with the dataset used to pre-train it.", "published": "2021-11-18 00:10:25", "link": "http://arxiv.org/abs/2111.09453v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\n  Gradient-Disentangled Embedding Sharing", "abstract": "This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.", "published": "2021-11-18 06:48:00", "link": "http://arxiv.org/abs/2111.09543v4", "categories": ["cs.CL", "cs.LG", "cs.CL, cs.GL", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "LAnoBERT: System Log Anomaly Detection based on BERT Masked Language\n  Model", "abstract": "The system log generated in a computer system refers to large-scale data that\nare collected simultaneously and used as the basic data for determining errors,\nintrusion and abnormal behaviors. The aim of system log anomaly detection is to\npromptly identify anomalies while minimizing human intervention, which is a\ncritical problem in the industry. Previous studies performed anomaly detection\nthrough algorithms after converting various forms of log data into a\nstandardized template using a parser. Particularly, a template corresponding to\na specific event should be defined in advance for all the log data using which\nthe information within the log key may get lost. In this study, we propose\nLAnoBERT, a parser free system log anomaly detection method that uses the BERT\nmodel, exhibiting excellent natural language processing performance. The\nproposed method, LAnoBERT, learns the model through masked language modeling,\nwhich is a BERT-based pre-training method, and proceeds with unsupervised\nlearning-based anomaly detection using the masked language modeling loss\nfunction per log key during the test process. In addition, we also propose an\nefficient inference process to establish a practically applicable pipeline to\nthe actual system. Experiments on three well-known log datasets, i.e., HDFS,\nBGL, and Thunderbird, show that not only did LAnoBERT yield a higher anomaly\ndetection performance compared to unsupervised learning-based benchmark models,\nbut also it resulted in a comparable performance with supervised learning-based\nbenchmark models.", "published": "2021-11-18 07:46:35", "link": "http://arxiv.org/abs/2111.09564v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Emotionally Stable is ALBERT? Testing Robustness with Stochastic\n  Weight Averaging on a Sentiment Analysis Task", "abstract": "Despite their success, modern language models are fragile. Even small changes\nin their training pipeline can lead to unexpected results. We study this\nphenomenon by examining the robustness of ALBERT (arXiv:1909.11942) in\ncombination with Stochastic Weight Averaging (SWA) (arXiv:1803.05407) -- a\ncheap way of ensembling -- on a sentiment analysis task (SST-2). In particular,\nwe analyze SWA's stability via CheckList criteria (arXiv:2005.04118), examining\nthe agreement on errors made by models differing only in their random seed. We\nhypothesize that SWA is more stable because it ensembles model snapshots taken\nalong the gradient descent trajectory. We quantify stability by comparing the\nmodels' mistakes with Fleiss' Kappa (Fleiss, 1971) and overlap ratio scores. We\nfind that SWA reduces error rates in general; yet the models still suffer from\ntheir own distinct biases (according to CheckList).", "published": "2021-11-18 10:39:01", "link": "http://arxiv.org/abs/2111.09612v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "To Augment or Not to Augment? A Comparative Study on Text Augmentation\n  Techniques for Low-Resource NLP", "abstract": "Data-hungry deep neural networks have established themselves as the standard\nfor many NLP tasks including the traditional sequence tagging ones. Despite\ntheir state-of-the-art performance on high-resource languages, they still fall\nbehind of their statistical counter-parts in low-resource scenarios. One\nmethodology to counter attack this problem is text augmentation, i.e.,\ngenerating new synthetic training data points from existing data. Although NLP\nhas recently witnessed a load of textual augmentation techniques, the field\nstill lacks a systematic performance analysis on a diverse set of languages and\nsequence tagging tasks. To fill this gap, we investigate three categories of\ntext augmentation methodologies which perform changes on the syntax (e.g.,\ncropping sub-sentences), token (e.g., random word insertion) and character\n(e.g., character swapping) levels. We systematically compare them on\npart-of-speech tagging, dependency parsing and semantic role labeling for a\ndiverse set of language families using various models including the\narchitectures that rely on pretrained multilingual contextualized language\nmodels such as mBERT. Augmentation most significantly improves dependency\nparsing, followed by part-of-speech tagging and semantic role labeling. We find\nthe experimented techniques to be effective on morphologically rich languages\nin general rather than analytic languages such as Vietnamese. Our results\nsuggest that the augmentation techniques can further improve over strong\nbaselines based on mBERT. We identify the character-level methods as the most\nconsistent performers, while synonym replacement and syntactic augmenters\nprovide inconsistent improvements. Finally, we discuss that the results most\nheavily depend on the task, language pair, and the model type.", "published": "2021-11-18 10:52:48", "link": "http://arxiv.org/abs/2111.09618v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic-TinyBERT: Boost TinyBERT's Inference Efficiency by Dynamic\n  Sequence Length", "abstract": "Limited computational budgets often prevent transformers from being used in\nproduction and from having their high accuracy utilized. TinyBERT addresses the\ncomputational efficiency by self-distilling BERT into a smaller transformer\nrepresentation having fewer layers and smaller internal embedding. However,\nTinyBERT's performance drops when we reduce the number of layers by 50%, and\ndrops even more abruptly when we reduce the number of layers by 75% for\nadvanced NLP tasks such as span question answering. Additionally, a separate\nmodel must be trained for each inference scenario with its distinct\ncomputational budget. In this work we present Dynamic-TinyBERT, a TinyBERT\nmodel that utilizes sequence-length reduction and Hyperparameter Optimization\nfor enhanced inference efficiency per any computational budget.\nDynamic-TinyBERT is trained only once, performing on-par with BERT and\nachieving an accuracy-speedup trade-off superior to any other efficient\napproaches (up to 3.3x with <1% loss-drop). Upon publication, the code to\nreproduce our work will be open-sourced.", "published": "2021-11-18 11:58:19", "link": "http://arxiv.org/abs/2111.09645v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli\n  Sampling", "abstract": "Transformer-based models are widely used in natural language processing\n(NLP). Central to the transformer model is the self-attention mechanism, which\ncaptures the interactions of token pairs in the input sequences and depends\nquadratically on the sequence length. Training such models on longer sequences\nis expensive. In this paper, we show that a Bernoulli sampling attention\nmechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic\ncomplexity of such models to linear. We bypass the quadratic cost by\nconsidering self-attention as a sum of individual tokens associated with\nBernoulli random variables that can, in principle, be sampled at once by a\nsingle hash (although in practice, this number may be a small constant). This\nleads to an efficient sampling scheme to estimate self-attention which relies\non specific modifications of LSH (to enable deployment on GPU architectures).\nWe evaluate our algorithm on the GLUE benchmark with standard 512 sequence\nlength where we see favorable performance relative to a standard pretrained\nTransformer. On the Long Range Arena (LRA) benchmark, for evaluating\nperformance on long sequences, our method achieves results consistent with\nsoftmax self-attention but with sizable speed-ups and memory savings and often\noutperforms other efficient self-attention methods. Our code is available at\nhttps://github.com/mlpen/YOSO", "published": "2021-11-18 14:24:34", "link": "http://arxiv.org/abs/2111.09714v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Detecting Cross-Language Plagiarism using Open Knowledge Graphs", "abstract": "Identifying cross-language plagiarism is challenging, especially for distant\nlanguage pairs and sense-for-sense translations. We introduce the new\nmultilingual retrieval model Cross-Language Ontology-Based Similarity Analysis\n(CL-OSA) for this task. CL-OSA represents documents as entity vectors obtained\nfrom the open knowledge graph Wikidata. Opposed to other methods, CL-OSA does\nnot require computationally expensive machine translation, nor pre-training\nusing comparable or parallel corpora. It reliably disambiguates homonyms and\nscales to allow its application to Web-scale document collections. We show that\nCL-OSA outperforms state-of-the-art methods for retrieving candidate documents\nfrom five large, topically diverse test corpora that include distant language\npairs like Japanese-English. For identifying cross-language plagiarism at the\ncharacter level, CL-OSA primarily improves the detection of sense-for-sense\ntranslations. For these challenging cases, CL-OSA's performance in terms of the\nwell-established PlagDet score exceeds that of the best competitor by more than\nfactor two. The code and data of our study are openly available.", "published": "2021-11-18 15:23:27", "link": "http://arxiv.org/abs/2111.09749v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Supporting Undotted Arabic with Pre-trained Language Models", "abstract": "We observe a recent behaviour on social media, in which users intentionally\nremove consonantal dots from Arabic letters, in order to bypass\ncontent-classification algorithms. Content classification is typically done by\nfine-tuning pre-trained language models, which have been recently employed by\nmany natural-language-processing applications. In this work we study the effect\nof applying pre-trained Arabic language models on \"undotted\" Arabic texts. We\nsuggest several ways of supporting undotted texts with pre-trained models,\nwithout additional training, and measure their performance on two Arabic\nnatural-language-processing downstream tasks. The results are encouraging; in\none of the tasks our method shows nearly perfect performance.", "published": "2021-11-18 16:47:56", "link": "http://arxiv.org/abs/2111.09791v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quality and Cost Trade-offs in Passage Re-ranking Task", "abstract": "Deep learning models named transformers achieved state-of-the-art results in\na vast majority of NLP tasks at the cost of increased computational complexity\nand high memory consumption. Using the transformer model in real-time inference\nbecomes a major challenge when implemented in production, because it requires\nexpensive computational resources. The more executions of a transformer are\nneeded the lower the overall throughput is, and switching to the smaller\nencoders leads to the decrease of accuracy. Our paper is devoted to the problem\nof how to choose the right architecture for the ranking step of the information\nretrieval pipeline, so that the number of required calls of transformer encoder\nis minimal with the maximum achievable quality of ranking. We investigated\nseveral late-interaction models such as Colbert and Poly-encoder architectures\nalong with their modifications. Also, we took care of the memory footprint of\nthe search index and tried to apply the learning-to-hash method to binarize the\noutput vectors from the transformer encoders. The results of the evaluation are\nprovided using TREC 2019-2021 and MS Marco dev datasets.", "published": "2021-11-18 19:47:45", "link": "http://arxiv.org/abs/2111.09927v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "How to Build Robust FAQ Chatbot with Controllable Question Generator?", "abstract": "Many unanswerable adversarial questions fool the question-answer (QA) system\nwith some plausible answers. Building a robust, frequently asked questions\n(FAQ) chatbot needs a large amount of diverse adversarial examples. Recent\nquestion generation methods are ineffective at generating many high-quality and\ndiverse adversarial question-answer pairs from unstructured text. We propose\nthe diversity controllable semantically valid adversarial attacker (DCSA), a\nhigh-quality, diverse, controllable method to generate standard and adversarial\nsamples with a semantic graph. The fluent and semantically generated QA pairs\nfool our passage retrieval model successfully. After that, we conduct a study\non the robustness and generalization of the QA model with generated QA pairs\namong different domains. We find that the generated data set improves the\ngeneralizability of the QA model to the new target domain and the robustness of\nthe QA model to detect unanswerable adversarial questions.", "published": "2021-11-18 12:54:07", "link": "http://arxiv.org/abs/2112.03007v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How News Evolves? Modeling News Text and Coverage using Graphs and\n  Hawkes Process", "abstract": "Monitoring news content automatically is an important problem. The news\ncontent, unlike traditional text, has a temporal component. However, few works\nhave explored the combination of natural language processing and dynamic system\nmodels. One reason is that it is challenging to mathematically model the\nnuances of natural language. In this paper, we discuss how we built a novel\ndataset of news articles collected over time. Then, we present a method of\nconverting news text collected over time to a sequence of directed\nmulti-graphs, which represent semantic triples (Subject -> Predicate}\n->Object). We model the dynamics of specific topological changes in these\ngraphs using a set of multivariate count series, which we fit the discrete-time\nHawkes process. With our real-world data, we show that the multivariate time\nseries contain both dynamic information of how many articles/words were\npublished each day and semantic information of the content of the articles.\nThis yields novel insights into how news events are covered. We show with the\nexperiment that our approach can be used to infer from a sequence of news\narticles if the articles were published by major or entertainment news outlets.", "published": "2021-11-18 10:36:40", "link": "http://arxiv.org/abs/2112.03008v2", "categories": ["cs.CL", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "DawDreamer: Bridging the Gap Between Digital Audio Workstations and\n  Python Interfaces", "abstract": "Audio production techniques which previously only existed in GUI-constrained\ndigital audio workstations, livecoding environments, or C++ APIs are now\naccessible with our new Python module called DawDreamer. DawDreamer therefore\nbridges the gap between real sound engineers and coders imitating them with\noffline batch-processing. Like contemporary modules in this domain, DawDreamer\ncan create directed acyclic graphs of audio processors such as VSTs which\ngenerate or manipulate audio streams. DawDreamer can also dynamically compile\nand execute code from Faust, a powerful signal processing language which can be\ndeployed to many platforms and microcontrollers. We discuss DawDreamer's unique\nfeatures in detail and potential applications across music information\nretrieval including source separation, transcription, and audio effect\nparameter inference. We provide fully cross-platform PyPI installers, a Linux\nDockerfile, and an example Jupyter notebook.", "published": "2021-11-18 20:07:39", "link": "http://arxiv.org/abs/2111.09931v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Conformer-based ASR Frontend for Joint Acoustic Echo Cancellation,\n  Speech Enhancement and Speech Separation", "abstract": "We present a frontend for improving robustness of automatic speech\nrecognition (ASR), that jointly implements three modules within a single model:\nacoustic echo cancellation, speech enhancement, and speech separation. This is\nachieved by using a contextual enhancement neural network that can optionally\nmake use of different types of side inputs: (1) a reference signal of the\nplayback audio, which is necessary for echo cancellation; (2) a noise context,\nwhich is useful for speech enhancement; and (3) an embedding vector\nrepresenting the voice characteristic of the target speaker of interest, which\nis not only critical in speech separation, but also helpful for echo\ncancellation and speech enhancement. We present detailed evaluations to show\nthat the joint model performs almost as well as the task-specific models, and\nsignificantly reduces word error rate in noisy conditions even when using a\nlarge-scale state-of-the-art ASR model. Compared to the noisy baseline, the\njoint model reduces the word error rate in low signal-to-noise ratio conditions\nby at least 71% on our echo cancellation dataset, 10% on our noisy dataset, and\n26% on our multi-speaker dataset. Compared to task-specific models, the joint\nmodel performs within 10% on our echo cancellation dataset, 2% on the noisy\ndataset, and 3% on the multi-speaker dataset.", "published": "2021-11-18 20:15:35", "link": "http://arxiv.org/abs/2111.09935v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Measuring Fairness in Speech Recognition: Casual Conversations\n  Dataset Transcriptions", "abstract": "It is well known that many machine learning systems demonstrate bias towards\nspecific groups of individuals. This problem has been studied extensively in\nthe Facial Recognition area, but much less so in Automatic Speech Recognition\n(ASR). This paper presents initial Speech Recognition results on \"Casual\nConversations\" -- a publicly released 846 hour corpus designed to help\nresearchers evaluate their computer vision and audio models for accuracy across\na diverse set of metadata, including age, gender, and skin tone. The entire\ncorpus has been manually transcribed, allowing for detailed ASR evaluations\nacross these metadata. Multiple ASR models are evaluated, including models\ntrained on LibriSpeech, 14,000 hour transcribed, and over 2 million hour\nuntranscribed social media videos. Significant differences in word error rate\nacross gender and skin tone are observed at times for all models. We are\nreleasing human transcripts from the Casual Conversations dataset to encourage\nthe community to develop a variety of techniques to reduce these statistical\nbiases.", "published": "2021-11-18 23:54:05", "link": "http://arxiv.org/abs/2111.09983v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Intelligibility-Oriented Audio-Visual Speech Enhancement", "abstract": "Existing deep learning (DL) based speech enhancement approaches are generally\noptimised to minimise the distance between clean and enhanced speech features.\nThese often result in improved speech quality however they suffer from a lack\nof generalisation and may not deliver the required speech intelligibility in\nreal noisy situations. In an attempt to address these challenges, researchers\nhave explored intelligibility-oriented (I-O) loss functions and integration of\naudio-visual (AV) information for more robust speech enhancement (SE). In this\npaper, we introduce DL based I-O SE algorithms exploiting AV information, which\nis a novel and previously unexplored research direction. Specifically, we\npresent a fully convolutional AV SE model that uses a modified short-time\nobjective intelligibility (STOI) metric as a training cost function. To the\nbest of our knowledge, this is the first work that exploits the integration of\nAV modalities with an I-O based loss function for SE. Comparative experimental\nresults demonstrate that our proposed I-O AV SE framework outperforms\naudio-only (AO) and AV models trained with conventional distance-based loss\nfunctions, in terms of standard objective evaluation measures when dealing with\nunseen speakers and noises.", "published": "2021-11-18 11:47:37", "link": "http://arxiv.org/abs/2111.09642v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transformer-S2A: Robust and Efficient Speech-to-Animation", "abstract": "We propose a novel robust and efficient Speech-to-Animation (S2A) approach\nfor synchronized facial animation generation in human-computer interaction.\nCompared with conventional approaches, the proposed approach utilizes phonetic\nposteriorgrams (PPGs) of spoken phonemes as input to ensure the cross-language\nand cross-speaker ability, and introduces corresponding prosody features (i.e.\npitch and energy) to further enhance the expression of generated animation.\nMixture-of-experts (MOE)-based Transformer is employed to better model\ncontextual information while provide significant optimization on computation\nefficiency. Experiments demonstrate the effectiveness of the proposed approach\non both objective and subjective evaluation with 17x inference speedup compared\nwith the state-of-the-art approach.", "published": "2021-11-18 16:09:39", "link": "http://arxiv.org/abs/2111.09771v3", "categories": ["cs.MM", "cs.GR", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
