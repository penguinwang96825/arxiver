{"title": "Semi Supervised Preposition-Sense Disambiguation using Multilingual Data", "abstract": "Prepositions are very common and very ambiguous, and understanding their\nsense is critical for understanding the meaning of the sentence. Supervised\ncorpora for the preposition-sense disambiguation task are small, suggesting a\nsemi-supervised approach to the task. We show that signals from unannotated\nmultilingual data can be used to improve supervised preposition-sense\ndisambiguation. Our approach pre-trains an LSTM encoder for predicting the\ntranslation of a preposition, and then incorporates the pre-trained encoder as\na component in a supervised classification system, and fine-tunes it for the\ntask. The multilingual signals consistently improve results on two\npreposition-sense datasets.", "published": "2016-11-27 09:53:36", "link": "http://arxiv.org/abs/1611.08813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The polysemy of the words that children learn over time", "abstract": "Here we study polysemy as a potential learning bias in vocabulary learning in\nchildren. Words of low polysemy could be preferred as they reduce the\ndisambiguation effort for the listener. However, such preference could be a\nside-effect of another bias: the preference of children for nouns in\ncombination with the lower polysemy of nouns with respect to other\npart-of-speech categories. Our results show that mean polysemy in children\nincreases over time in two phases, i.e. a fast growth till the 31st month\nfollowed by a slower tendency towards adult speech. In contrast, this evolution\nis not found in adults interacting with children. This suggests that children\nhave a preference for non-polysemous words in their early stages of vocabulary\nacquisition. Interestingly, the evolutionary pattern described above weakens\nwhen controlling for syntactic category (noun, verb, adjective or adverb) but\nit does not disappear completely, suggesting that it could result from\nacombination of a standalone bias for low polysemy and a preference for nouns.", "published": "2016-11-27 08:32:19", "link": "http://arxiv.org/abs/1611.08807v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "A theory of interpretive clustering in free recall", "abstract": "A stochastic model of short-term verbal memory is proposed, in which the\npsychological state of the subject is encoded as the instantaneous position of\na particle diffusing over a semantic graph with a probabilistic structure. The\nmodel is particularly suitable for studying the dependence of free-recall\nobservables on semantic properties of the words to be recalled. Besides\npredicting some well-known experimental features (contiguity effect, forward\nasymmetry, word-length effect), a novel prediction is obtained on the\nrelationship between the contiguity effect and the syllabic length of words;\nshorter words, by way of their wider semantic range, are predicted to be\ncharacterized by stronger forward contiguity. A fresh analysis of archival data\nallows to confirm this prediction.", "published": "2016-11-27 22:42:13", "link": "http://arxiv.org/abs/1611.08928v2", "categories": ["q-bio.NC", "cs.CL", "91E10"], "primary_category": "q-bio.NC"}
{"title": "Invariant Representations for Noisy Speech Recognition", "abstract": "Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training.", "published": "2016-11-27 22:20:51", "link": "http://arxiv.org/abs/1612.01928v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "cs.CL"}
