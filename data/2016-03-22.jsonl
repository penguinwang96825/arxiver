{"title": "Semi-supervised Word Sense Disambiguation with Neural Models", "abstract": "Determining the intended sense of words in text - word sense disambiguation\n(WSD) - is a long standing problem in natural language processing. Recently,\nresearchers have shown promising results using word vectors extracted from a\nneural network language model as features in WSD algorithms. However, a simple\naverage or concatenation of word vectors for each word in a text loses the\nsequential and syntactic information of the text. In this paper, we study WSD\nwith a sequence learning neural net, LSTM, to better capture the sequential and\nsyntactic patterns of the text. To alleviate the lack of training data in\nall-words WSD, we employ the same LSTM in a semi-supervised label propagation\nclassifier. We demonstrate state-of-the-art results, especially on verbs.", "published": "2016-03-22 22:15:10", "link": "http://arxiv.org/abs/1603.07012v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Executable Semantic Parsers for Natural Language Understanding", "abstract": "For building question answering systems and natural language interfaces,\nsemantic parsing has emerged as an important and powerful paradigm. Semantic\nparsers map natural language into logical forms, the classic representation for\nmany important linguistic phenomena. The modern twist is that we are interested\nin learning semantic parsers from data, which introduces a new layer of\nstatistical and computational issues. This article lays out the components of a\nstatistical semantic parser, highlighting the key challenges. We will see that\nsemantic parsing is a rich fusion of the logical and the statistical world, and\nthat this fusion will play an integral role in the future of natural language\nunderstanding systems.", "published": "2016-03-22 05:07:16", "link": "http://arxiv.org/abs/1603.06677v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Latent Predictor Networks for Code Generation", "abstract": "Many language generation tasks require the production of text conditioned on\nboth structured and unstructured inputs. We present a novel neural network\narchitecture which generates an output sequence conditioned on an arbitrary\nnumber of input functions. Crucially, our approach allows both the choice of\nconditioning context and the granularity of generation, for example characters\nor tokens, to be marginalised, thus permitting scalable and effective training.\nUsing this framework, we address the problem of generating programming code\nfrom a mixed natural language and structured specification. We create two new\ndata sets for this paradigm derived from the collectible trading card games\nMagic the Gathering and Hearthstone. On these, and a third preexisting corpus,\nwe demonstrate that marginalising multiple predictors allows our model to\noutperform strong benchmarks.", "published": "2016-03-22 11:41:51", "link": "http://arxiv.org/abs/1603.06744v2", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Multi-domain machine translation enhancements by parallel data\n  extraction from comparable corpora", "abstract": "Parallel texts are a relatively rare language resource, however, they\nconstitute a very useful research material with a wide range of applications.\nThis study presents and analyses new methodologies we developed for obtaining\nsuch data from previously built comparable corpora. The methodologies are\nautomatic and unsupervised which makes them good for large scale research. The\ntask is highly practical as non-parallel multilingual data occur much more\nfrequently than parallel corpora and accessing them is easy, although parallel\nsentences are a considerably more useful resource. In this study, we propose a\nmethod of automatic web crawling in order to build topic-aligned comparable\ncorpora, e.g. based on the Wikipedia or Euronews.com. We also developed new\nmethods of obtaining parallel sentences from comparable data and proposed\nmethods of filtration of corpora capable of selecting inconsistent or only\npartially equivalent translations. Our methods are easily scalable to other\nlanguages. Evaluation of the quality of the created corpora was performed by\nanalysing the impact of their use on statistical machine translation systems.\nExperiments were presented on the basis of the Polish-English language pair for\ntexts from different domains, i.e. lectures, phrasebooks, film dialogues,\nEuropean Parliament proceedings and texts contained medicines leaflets. We also\ntested a second method of creating parallel corpora based on data from\ncomparable corpora which allows for automatically expanding the existing corpus\nof sentences about a given domain on the basis of analogies found between them.\nIt does not require, therefore, having past parallel resources in order to\ntrain a classifier.", "published": "2016-03-22 13:34:28", "link": "http://arxiv.org/abs/1603.06785v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment\n  Analysis", "abstract": "In aspect-based sentiment analysis, extracting aspect terms along with the\nopinions being expressed from user-generated content is one of the most\nimportant subtasks. Previous studies have shown that exploiting connections\nbetween aspect and opinion terms is promising for this task. In this paper, we\npropose a novel joint model that integrates recursive neural networks and\nconditional random fields into a unified framework for explicit aspect and\nopinion terms co-extraction. The proposed model learns high-level\ndiscriminative features and double propagate information between aspect and\nopinion terms, simultaneously. Moreover, it is flexible to incorporate\nhand-crafted features into the proposed model to further boost its information\nextraction performance. Experimental results on the SemEval Challenge 2014\ndataset show the superiority of our proposed model over several baseline\nmethods as well as the winning systems of the challenge.", "published": "2016-03-22 05:59:00", "link": "http://arxiv.org/abs/1603.06679v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M\n  Factoid Question-Answer Corpus", "abstract": "Over the past decade, large-scale supervised learning corpora have enabled\nmachine learning researchers to make substantial advances. However, to this\ndate, there are no large-scale question-answer corpora available. In this paper\nwe present the 30M Factoid Question-Answer Corpus, an enormous question answer\npair corpus produced by applying a novel neural network architecture on the\nknowledge base Freebase to transduce facts into natural language questions. The\nproduced question answer pairs are evaluated both by human evaluators and using\nautomatic evaluation metrics, including well-established machine translation\nand sentence similarity metrics. Across all evaluation criteria the\nquestion-generation model outperforms the competing template-based baseline.\nFurthermore, when presented to human evaluators, the generated questions appear\ncomparable in quality to real human-generated questions.", "published": "2016-03-22 14:25:16", "link": "http://arxiv.org/abs/1603.06807v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "H.3.4; I.5.1; I.2.6; I.2.7"], "primary_category": "cs.CL"}
