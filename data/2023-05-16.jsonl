{"title": "Pre-Training to Learn in Context", "abstract": "In-context learning, where pre-trained language models learn to perform tasks\nfrom task examples and instructions in their contexts, has attracted much\nattention in the NLP community. However, the ability of in-context learning is\nnot fully exploited because language models are not explicitly trained to learn\nin context. To this end, we propose PICL (Pre-training for In-Context\nLearning), a framework to enhance the language models' in-context learning\nability by pre-training the model on a large collection of \"intrinsic tasks\" in\nthe general plain-text corpus using the simple language modeling objective.\nPICL encourages the model to infer and perform tasks by conditioning on the\ncontexts while maintaining task generalization of pre-trained models. We\nevaluate the in-context learning performance of the model trained with PICL on\nseven widely-used text classification datasets and the Super-NaturalInstrctions\nbenchmark, which contains 100+ NLP tasks formulated to text generation. Our\nexperiments show that PICL is more effective and task-generalizable than a\nrange of baselines, outperforming larger language models with nearly 4x\nparameters. The code is publicly available at https://github.com/thu-coai/PICL.", "published": "2023-05-16 03:38:06", "link": "http://arxiv.org/abs/2305.09137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Translation: Improving Domain Robustness of Neural Machine\n  Translation with Intermediate Sequences", "abstract": "Previous studies show that intermediate supervision signals benefit various\nNatural Language Processing tasks. However, it is not clear whether there exist\nintermediate signals that benefit Neural Machine Translation (NMT). Borrowing\ntechniques from Statistical Machine Translation, we propose intermediate\nsignals which are intermediate sequences from the \"source-like\" structure to\nthe \"target-like\" structure. Such intermediate sequences introduce an inductive\nbias that reflects a domain-agnostic principle of translation, which reduces\nspurious correlations that are harmful to out-of-domain generalisation.\nFurthermore, we introduce a full-permutation multi-task learning to alleviate\nthe spurious causal relations from intermediate sequences to the target, which\nresults from exposure bias. The Minimum Bayes Risk decoding algorithm is used\nto pick the best candidate translation from all permutations to further improve\nthe performance. Experiments show that the introduced intermediate signals can\neffectively improve the domain robustness of NMT and reduces the amount of\nhallucinations on out-of-domain translation. Further analysis shows that our\nmethods are especially promising in low-resource scenarios.", "published": "2023-05-16 04:15:25", "link": "http://arxiv.org/abs/2305.09154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Easy-to-Hard Learning for Information Extraction", "abstract": "Information extraction (IE) systems aim to automatically extract structured\ninformation, such as named entities, relations between entities, and events,\nfrom unstructured texts. While most existing work addresses a particular IE\ntask, universally modeling various IE tasks with one model has achieved great\nsuccess recently. Despite their success, they employ a one-stage learning\nstrategy, i.e., directly learning to extract the target structure given the\ninput text, which contradicts the human learning process. In this paper, we\npropose a unified easy-to-hard learning framework consisting of three stages,\ni.e., the easy stage, the hard stage, and the main stage, for IE by mimicking\nthe human learning process. By breaking down the learning process into multiple\nstages, our framework facilitates the model to acquire general IE task\nknowledge and improve its generalization ability. Extensive experiments across\nfour IE tasks demonstrate the effectiveness of our framework. We achieve new\nstate-of-the-art results on 13 out of 17 datasets. Our code is available at\n\\url{https://github.com/DAMO-NLP-SG/IE-E2H}.", "published": "2023-05-16 06:04:14", "link": "http://arxiv.org/abs/2305.09193v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Speech Dialogue Translation Mediating Speakers of Different\n  Languages", "abstract": "We present a new task, speech dialogue translation mediating speakers of\ndifferent languages. We construct the SpeechBSD dataset for the task and\nconduct baseline experiments. Furthermore, we consider context to be an\nimportant aspect that needs to be addressed in this task and propose two ways\nof utilizing context, namely monolingual context and bilingual context. We\nconduct cascaded speech translation experiments using Whisper and mBART, and\nshow that bilingual context performs better in our settings.", "published": "2023-05-16 06:37:55", "link": "http://arxiv.org/abs/2305.09210v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xPQA: Cross-Lingual Product Question Answering across 12 Languages", "abstract": "Product Question Answering (PQA) systems are key in e-commerce applications\nto provide responses to customers' questions as they shop for products. While\nexisting work on PQA focuses mainly on English, in practice there is need to\nsupport multiple customer languages while leveraging product information\navailable in English. To study this practical industrial task, we present xPQA,\na large-scale annotated cross-lingual PQA dataset in 12 languages across 9\nbranches, and report results in (1) candidate ranking, to select the best\nEnglish candidate containing the information to answer a non-English question;\nand (2) answer generation, to generate a natural-sounding non-English answer\nbased on the selected English candidate. We evaluate various approaches\ninvolving machine translation at runtime or offline, leveraging multilingual\npre-trained LMs, and including or excluding xPQA training data. We find that\n(1) In-domain data is essential as cross-lingual rankers trained on other\ndomains perform poorly on the PQA task; (2) Candidate ranking often prefers\nruntime-translation approaches while answer generation prefers multilingual\napproaches; (3) Translating offline to augment multilingual models helps\ncandidate ranking mainly on languages with non-Latin scripts; and helps answer\ngeneration mainly on languages with Latin scripts. Still, there remains a\nsignificant performance gap between the English and the cross-lingual test\nsets.", "published": "2023-05-16 07:56:19", "link": "http://arxiv.org/abs/2305.09249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ContrastNet: A Contrastive Learning Framework for Few-Shot Text\n  Classification", "abstract": "Few-shot text classification has recently been promoted by the meta-learning\nparadigm which aims to identify target classes with knowledge transferred from\nsource classes with sets of small tasks named episodes. Despite their success,\nexisting works building their meta-learner based on Prototypical Networks are\nunsatisfactory in learning discriminative text representations between similar\nclasses, which may lead to contradictions during label prediction. In addition,\nthe tasklevel and instance-level overfitting problems in few-shot text\nclassification caused by a few training examples are not sufficiently tackled.\nIn this work, we propose a contrastive learning framework named ContrastNet to\ntackle both discriminative representation and overfitting problems in few-shot\ntext classification. ContrastNet learns to pull closer text representations\nbelonging to the same class and push away text representations belonging to\ndifferent classes, while simultaneously introducing unsupervised contrastive\nregularization at both task-level and instance-level to prevent overfitting.\nExperiments on 8 few-shot text classification datasets show that ContrastNet\noutperforms the current state-of-the-art models.", "published": "2023-05-16 08:22:17", "link": "http://arxiv.org/abs/2305.09269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Origins of Bias in NLP through the Lens of the Jim Code", "abstract": "In this paper, we trace the biases in current natural language processing\n(NLP) models back to their origins in racism, sexism, and homophobia over the\nlast 500 years. We review literature from critical race theory, gender studies,\ndata ethics, and digital humanities studies, and summarize the origins of bias\nin NLP models from these social science perspective. We show how the causes of\nthe biases in the NLP pipeline are rooted in social issues. Finally, we argue\nthat the only way to fix the bias and unfairness in NLP is by addressing the\nsocial problems that caused them in the first place and by incorporating social\nsciences and social scientists in efforts to mitigate bias in NLP models. We\nprovide actionable recommendations for the NLP research community to do so.", "published": "2023-05-16 08:37:13", "link": "http://arxiv.org/abs/2305.09281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Word Dilution as Text Data Augmentation in Low-Resource\n  Regime", "abstract": "Data augmentation is widely used in text classification, especially in the\nlow-resource regime where a few examples for each class are available during\ntraining. Despite the success, generating data augmentations as hard positive\nexamples that may increase their effectiveness is under-explored. This paper\nproposes an Adversarial Word Dilution (AWD) method that can generate hard\npositive examples as text data augmentations to train the low-resource text\nclassification model efficiently. Our idea of augmenting the text data is to\ndilute the embedding of strong positive words by weighted mixing with\nunknown-word embedding, making the augmented inputs hard to be recognized as\npositive by the classification model. We adversarially learn the dilution\nweights through a constrained min-max optimization process with the guidance of\nthe labels. Empirical studies on three benchmark datasets show that AWD can\ngenerate more effective data augmentations and outperform the state-of-the-art\ntext data augmentation methods. The additional analysis demonstrates that the\ndata augmentations generated by AWD are interpretable and can flexibly extend\nto new examples without further training.", "published": "2023-05-16 08:46:11", "link": "http://arxiv.org/abs/2305.09287v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of Layer Normalization for Zero-shot Neural Machine\n  Translation", "abstract": "This paper studies the impact of layer normalization (LayerNorm) on zero-shot\ntranslation (ZST). Recent efforts for ZST often utilize the Transformer\narchitecture as the backbone, with LayerNorm at the input of layers (PreNorm)\nset as the default. However, Xu et al. (2019) has revealed that PreNorm carries\nthe risk of overfitting the training data. Based on this, we hypothesize that\nPreNorm may overfit supervised directions and thus have low generalizability\nfor ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST\ndirections, we demonstrate that the original Transformer setting of LayerNorm\nafter residual connections (PostNorm) consistently outperforms PreNorm by up to\n12.3 BLEU points. We then study the performance disparities by analyzing the\ndifferences in off-target rates and structural variations between PreNorm and\nPostNorm. This study highlights the need for careful consideration of the\nLayerNorm setting for ZST.", "published": "2023-05-16 09:37:08", "link": "http://arxiv.org/abs/2305.09312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MsPrompt: Multi-step Prompt Learning for Debiasing Few-shot Event\n  Detection", "abstract": "Event detection (ED) is aimed to identify the key trigger words in\nunstructured text and predict the event types accordingly. Traditional ED\nmodels are too data-hungry to accommodate real applications with scarce labeled\ndata. Besides, typical ED models are facing the context-bypassing and disabled\ngeneralization issues caused by the trigger bias stemming from ED datasets.\nTherefore, we focus on the true few-shot paradigm to satisfy the low-resource\nscenarios. In particular, we propose a multi-step prompt learning model\n(MsPrompt) for debiasing few-shot event detection, that consists of the\nfollowing three components: an under-sampling module targeting to construct a\nnovel training set that accommodates the true few-shot setting, a multi-step\nprompt module equipped with a knowledge-enhanced ontology to leverage the event\nsemantics and latent prior knowledge in the PLMs sufficiently for tackling the\ncontext-bypassing problem, and a prototypical module compensating for the\nweakness of classifying events with sparse data and boost the generalization\nperformance. Experiments on two public datasets ACE-2005 and FewEvent show that\nMsPrompt can outperform the state-of-the-art models, especially in the strict\nlow-resource scenarios reporting 11.43% improvement in terms of weighted\nF1-score against the best-performing baseline and achieving an outstanding\ndebiasing performance.", "published": "2023-05-16 10:19:12", "link": "http://arxiv.org/abs/2305.09335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing and Interpreting Causal Knowledge Graphs from News", "abstract": "Many financial jobs rely on news to learn about causal events in the past and\npresent, to make informed decisions and predictions about the future. With the\never-increasing amount of news available online, there is a need to automate\nthe extraction of causal events from unstructured texts. In this work, we\npropose a methodology to construct causal knowledge graphs (KGs) from news\nusing two steps: (1) Extraction of Causal Relations, and (2) Argument\nClustering and Representation into KG. We aim to build graphs that emphasize on\nrecall, precision and interpretability. For extraction, although many earlier\nworks already construct causal KGs from text, most adopt rudimentary\npattern-based methods. We close this gap by using the latest BERT-based\nextraction models alongside pattern-based ones. As a result, we achieved a high\nrecall, while still maintaining a high precision. For clustering, we utilized a\ntopic modelling approach to cluster our arguments, so as to increase the\nconnectivity of our graph. As a result, instead of 15,686 disconnected\nsubgraphs, we were able to obtain 1 connected graph that enables users to infer\nmore causal relationships from. Our final KG effectively captures and conveys\ncausal relationships, validated through experiments, multiple use cases and\nuser feedback.", "published": "2023-05-16 11:33:32", "link": "http://arxiv.org/abs/2305.09359v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation\n  Understanding", "abstract": "Addressing the issues of who saying what to whom in multi-party conversations\n(MPCs) has recently attracted a lot of research attention. However, existing\nmethods on MPC understanding typically embed interlocutors and utterances into\nsequential information flows, or utilize only the superficial of inherent graph\nstructures in MPCs. To this end, we present a plug-and-play and lightweight\nmethod named graph-induced fine-tuning (GIFT) which can adapt various\nTransformer-based pre-trained language models (PLMs) for universal MPC\nunderstanding. In detail, the full and equivalent connections among utterances\nin regular Transformer ignore the sparse but distinctive dependency of an\nutterance on another in MPCs. To distinguish different relationships between\nutterances, four types of edges are designed to integrate graph-induced signals\ninto attention mechanisms to refine PLMs originally designed for processing\nsequential texts. We evaluate GIFT by implementing it into three PLMs, and test\nthe performance on three downstream tasks including addressee recognition,\nspeaker identification and response selection. Experimental results show that\nGIFT can significantly improve the performance of three PLMs on three\ndownstream tasks and two benchmarks with only 4 additional parameters per\nencoding layer, achieving new state-of-the-art performance on MPC\nunderstanding.", "published": "2023-05-16 11:35:24", "link": "http://arxiv.org/abs/2305.09360v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop\n  Fact Verification", "abstract": "The success of deep learning models on multi-hop fact verification has\nprompted researchers to understand the behavior behind their veracity. One\npossible way is erasure search: obtaining the rationale by entirely removing a\nsubset of input without compromising the veracity prediction. Although\nextensively explored, existing approaches fall within the scope of the\nsingle-granular (tokens or sentences) explanation, which inevitably leads to\nexplanation redundancy and inconsistency. To address such issues, this paper\nexplores the viability of multi-granular rationale extraction with consistency\nand faithfulness for explainable multi-hop fact verification. In particular,\ngiven a pretrained veracity prediction model, both the token-level explainer\nand sentence-level explainer are trained simultaneously to obtain\nmulti-granular rationales via differentiable masking. Meanwhile, three\ndiagnostic properties (fidelity, consistency, salience) are introduced and\napplied to the training process, to ensure that the extracted rationales\nsatisfy faithfulness and consistency. Experimental results on three multi-hop\nfact verification datasets show that the proposed approach outperforms some\nstate-of-the-art baselines.", "published": "2023-05-16 12:31:53", "link": "http://arxiv.org/abs/2305.09400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fuzzy Temporal Protoforms for the Quantitative Description of Processes\n  in Natural Language", "abstract": "In this paper, we propose a series of fuzzy temporal protoforms in the\nframework of the automatic generation of quantitative and qualitative natural\nlanguage descriptions of processes. The model includes temporal and causal\ninformation from processes and attributes, quantifies attributes in time during\nthe process life-span and recalls causal relations and temporal distances\nbetween events, among other features. Through integrating process mining\ntechniques and fuzzy sets within the usual Data-to-Text architecture, our\nframework is able to extract relevant quantitative temporal as well as\nstructural information from a process and describe it in natural language\ninvolving uncertain terms. A real use-case in the cardiology domain is\npresented, showing the potential of our model for providing natural language\nexplanations addressed to domain experts.", "published": "2023-05-16 14:59:38", "link": "http://arxiv.org/abs/2305.09506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional Generative Framework for Cross-domain Aspect-based\n  Sentiment Analysis", "abstract": "Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various\nfine-grained sentiment analysis tasks on a target domain by transferring\nknowledge from a source domain. Since labeled data only exists in the source\ndomain, a model is expected to bridge the domain gap for tackling cross-domain\nABSA. Though domain adaptation methods have proven to be effective, most of\nthem are based on a discriminative model, which needs to be specifically\ndesigned for different ABSA tasks. To offer a more general solution, we propose\na unified bidirectional generative framework to tackle various cross-domain\nABSA tasks. Specifically, our framework trains a generative model in both\ntext-to-label and label-to-text directions. The former transforms each task\ninto a unified format to learn domain-agnostic features, and the latter\ngenerates natural sentences from noisy labels for data augmentation, with which\na more accurate model can be trained. To investigate the effectiveness and\ngenerality of our framework, we conduct extensive experiments on four\ncross-domain ABSA tasks and present new state-of-the-art results on all tasks.\nOur data and code are publicly available at\n\\url{https://github.com/DAMO-NLP-SG/BGCA}.", "published": "2023-05-16 15:02:23", "link": "http://arxiv.org/abs/2305.09509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation", "abstract": "Diffusion models have gained significant attention in the realm of image\ngeneration due to their exceptional performance. Their success has been\nrecently expanded to text generation via generating all tokens within a\nsequence concurrently. However, natural language exhibits a far more pronounced\nsequential dependency in comparison to images, and the majority of existing\nlanguage models are trained with a left-to-right auto-regressive approach. To\naccount for the inherent sequential characteristic of natural language, we\nintroduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that\nthe generation of tokens on the right depends on the generated ones on the\nleft, a mechanism achieved through employing a dynamic number of denoising\nsteps that vary based on token position. This results in tokens on the left\nundergoing fewer denoising steps than those on the right, thereby enabling them\nto generate earlier and subsequently influence the generation of tokens on the\nright. In a series of experiments on various text generation tasks, including\ntext summarization, machine translation, and common sense generation,\nAR-Diffusion clearly demonstrated its superiority over existing diffusion\nlanguage models and that it can be $100\\times\\sim600\\times$ faster when\nachieving comparable results. Our code is available at\nhttps://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.", "published": "2023-05-16 15:10:22", "link": "http://arxiv.org/abs/2305.09515v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DLUE: Benchmarking Document Language Understanding", "abstract": "Understanding documents is central to many real-world tasks but remains a\nchallenging topic. Unfortunately, there is no well-established consensus on how\nto comprehensively evaluate document understanding abilities, which\nsignificantly hinders the fair comparison and measuring the progress of the\nfield. To benchmark document understanding researches, this paper summarizes\nfour representative abilities, i.e., document classification, document\nstructural analysis, document information extraction, and document\ntranscription. Under the new evaluation framework, we propose \\textbf{Document\nLanguage Understanding Evaluation} -- \\textbf{DLUE}, a new task suite which\ncovers a wide-range of tasks in various forms, domains and document genres. We\nalso systematically evaluate six well-established transformer models on DLUE,\nand find that due to the lengthy content, complicated underlying structure and\ndispersed knowledge, document understanding is still far from being solved, and\ncurrently there is no neural architecture that dominates all tasks, raising\nrequirements for a universal document understanding architecture.", "published": "2023-05-16 15:16:24", "link": "http://arxiv.org/abs/2305.09520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaSRL++: A Uniform Scheme for Modelling Deeper Semantics", "abstract": "Despite enormous progress in Natural Language Processing (NLP), our field is\nstill lacking a common deep semantic representation scheme. As a result, the\nproblem of meaning and understanding is typically sidestepped through more\nsimple, approximative methods. This paper argues that in order to arrive at\nsuch a scheme, we also need a common modelling scheme. It therefore introduces\nMetaSRL++, a uniform, language- and modality-independent modelling scheme based\non Semantic Graphs, as a step towards a common representation scheme; as well\nas a method for defining the concepts and entities that are used in these\ngraphs. Our output is twofold. First, we illustrate MetaSRL++ through concrete\nexamples. Secondly, we discuss how it relates to existing work in the field.", "published": "2023-05-16 15:26:52", "link": "http://arxiv.org/abs/2305.09534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Dimensions of Self-Presentation in Twitter Bios and their\n  Links to Misinformation Sharing", "abstract": "Social media platforms provide users with a profile description field,\ncommonly known as a ``bio,\" where they can present themselves to the world. A\ngrowing literature shows that text in these bios can improve our understanding\nof online self-presentation and behavior, but existing work relies exclusively\non keyword-based approaches to do so. We here propose and evaluate a suite of\n\\hl{simple, effective, and theoretically motivated} approaches to embed bios in\nspaces that capture salient dimensions of social meaning, such as age and\npartisanship. We \\hl{evaluate our methods on four tasks, showing that the\nstrongest one out-performs several practical baselines.} We then show the\nutility of our method in helping understand associations between\nself-presentation and the sharing of URLs from low-quality news sites on\nTwitter\\hl{, with a particular focus on explore the interactions between age\nand partisanship, and exploring the effects of self-presentations of\nreligiosity}. Our work provides new tools to help computational social\nscientists make use of information in bios, and provides new insights into how\nmisinformation sharing may be perceived on Twitter.", "published": "2023-05-16 15:45:59", "link": "http://arxiv.org/abs/2305.09548v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Sentence Transformers for the Aviation Domain", "abstract": "Learning effective sentence representations is crucial for many Natural\nLanguage Processing (NLP) tasks, including semantic search, semantic textual\nsimilarity (STS), and clustering. While multiple transformer models have been\ndeveloped for sentence embedding learning, these models may not perform\noptimally when dealing with specialized domains like aviation, which has unique\ncharacteristics such as technical jargon, abbreviations, and unconventional\ngrammar. Furthermore, the absence of labeled datasets makes it difficult to\ntrain models specifically for the aviation domain. To address these challenges,\nwe propose a novel approach for adapting sentence transformers for the aviation\ndomain. Our method is a two-stage process consisting of pre-training followed\nby fine-tuning. During pre-training, we use Transformers and Sequential\nDenoising AutoEncoder (TSDAE) with aviation text data as input to improve the\ninitial model performance. Subsequently, we fine-tune our models using a\nNatural Language Inference (NLI) dataset in the Sentence Bidirectional Encoder\nRepresentations from Transformers (SBERT) architecture to mitigate overfitting\nissues. Experimental results on several downstream tasks show that our adapted\nsentence transformers significantly outperform general-purpose transformers,\ndemonstrating the effectiveness of our approach in capturing the nuances of the\naviation domain. Overall, our work highlights the importance of domain-specific\nadaptation in developing high-quality NLP solutions for specialized industries\nlike aviation.", "published": "2023-05-16 15:53:24", "link": "http://arxiv.org/abs/2305.09556v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Event Extraction with Denoised Structure-to-Text Augmentation", "abstract": "Event extraction aims to recognize pre-defined event triggers and arguments\nfrom texts, which suffer from the lack of high-quality annotations. In most NLP\napplications, involving a large scale of synthetic training data is a practical\nand effective approach to alleviate the problem of data scarcity. However, when\napplying to the task of event extraction, recent data augmentation methods\noften neglect the problem of grammatical incorrectness, structure misalignment,\nand semantic drifting, leading to unsatisfactory performances. In order to\nsolve these problems, we propose a denoised structure-to-text augmentation\nframework for event extraction DAEE, which generates additional training data\nthrough the knowledge-based structure-to-text generation model and selects the\neffective subset from the generated data iteratively with a deep reinforcement\nlearning agent. Experimental results on several datasets demonstrate that the\nproposed method generates more diverse text representations for event\nextraction and achieves comparable results with the state-of-the-art.", "published": "2023-05-16 16:52:07", "link": "http://arxiv.org/abs/2305.09598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StructGPT: A General Framework for Large Language Model to Reason over\n  Structured Data", "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of\nlarge language models~(LLMs) over structured data in a unified way. Inspired by\nthe study on tool augmentation for LLMs, we develop an \\emph{Iterative\nReading-then-Reasoning~(IRR)} approach for solving question answering tasks\nbased on structured data, called \\textbf{StructGPT}. In our approach, we\nconstruct the specialized function to collect relevant evidence from structured\ndata (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on\nthe collected information (\\ie \\emph{reasoning}). Specially, we propose an\n\\emph{invoking-linearization-generation} procedure to support LLMs in reasoning\non the structured data with the help of the external interfaces. By iterating\nthis procedures with provided interfaces, our approach can gradually approach\nthe target answer to a given query. Extensive experiments conducted on three\ntypes of structured data demonstrate the effectiveness of our approach, which\ncan significantly boost the performance of ChatGPT and achieve comparable\nperformance against the full-data supervised-tuning baselines. Our codes and\ndata are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.", "published": "2023-05-16 17:45:23", "link": "http://arxiv.org/abs/2305.09645v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks\n  for Patient-Level Representation Learning", "abstract": "Leveraging knowledge from electronic health records (EHRs) to predict a\npatient's condition is essential to the effective delivery of appropriate care.\nClinical notes of patient EHRs contain valuable information from healthcare\nprofessionals, but have been underused due to their difficult contents and\ncomplex hierarchies. Recently, hypergraph-based methods have been proposed for\ndocument classifications. Directly adopting existing hypergraph methods on\nclinical notes cannot sufficiently utilize the hierarchy information of the\npatient, which can degrade clinical semantic information by (1) frequent\nneutral words and (2) hierarchies with imbalanced distribution. Thus, we\npropose a taxonomy-aware multi-level hypergraph neural network (TM-HGNN), where\nmulti-level hypergraphs assemble useful neutral words with rare keywords via\nnote and taxonomy level hyperedges to retain the clinical semantic information.\nThe constructed patient hypergraphs are fed into hierarchical message passing\nlayers for learning more balanced multi-level knowledge at the note and\ntaxonomy levels. We validate the effectiveness of TM-HGNN by conducting\nextensive experiments with MIMIC-III dataset on benchmark in-hospital-mortality\nprediction.", "published": "2023-05-16 19:08:18", "link": "http://arxiv.org/abs/2305.09756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mirages: On Anthropomorphism in Dialogue Systems", "abstract": "Automated dialogue or conversational systems are anthropomorphised by\ndevelopers and personified by users. While a degree of anthropomorphism may be\ninevitable due to the choice of medium, conscious and unconscious design\nchoices can guide users to personify such systems to varying degrees.\nEncouraging users to relate to automated systems as if they were human can lead\nto high risk scenarios caused by over-reliance on their outputs. As a result,\nnatural language processing researchers have investigated the factors that\ninduce personification and develop resources to mitigate such effects. However,\nthese efforts are fragmented, and many aspects of anthropomorphism have yet to\nbe explored. In this paper, we discuss the linguistic factors that contribute\nto the anthropomorphism of dialogue systems and the harms that can arise,\nincluding reinforcing gender stereotypes and notions of acceptable language. We\nrecommend that future efforts towards developing dialogue systems take\nparticular care in their design, development, release, and description; and\nattend to the many linguistic cues that can elicit personification by users.", "published": "2023-05-16 20:50:46", "link": "http://arxiv.org/abs/2305.09800v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weight-Inherited Distillation for Task-Agnostic BERT Compression", "abstract": "Knowledge Distillation (KD) is a predominant approach for BERT compression.\nPrevious KD-based methods focus on designing extra alignment losses for the\nstudent model to mimic the behavior of the teacher model. These methods\ntransfer the knowledge in an indirect way. In this paper, we propose a novel\nWeight-Inherited Distillation (WID), which directly transfers knowledge from\nthe teacher. WID does not require any additional alignment loss and trains a\ncompact student by inheriting the weights, showing a new perspective of\nknowledge distillation. Specifically, we design the row compactors and column\ncompactors as mappings and then compress the weights via structural\nre-parameterization. Experimental results on the GLUE and SQuAD benchmarks show\nthat WID outperforms previous state-of-the-art KD-based baselines. Further\nanalysis indicates that WID can also learn the attention patterns from the\nteacher model without any alignment loss on attention distributions. The code\nis available at https://github.com/wutaiqiang/WID-NAACL2024.", "published": "2023-05-16 01:51:22", "link": "http://arxiv.org/abs/2305.09098v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism\n  of Language Models", "abstract": "Memory is one of the most essential cognitive functions serving as a\nrepository of world knowledge and episodes of activities. In recent years,\nlarge-scale pre-trained language models have shown remarkable memorizing\nability. On the contrary, vanilla neural networks without pre-training have\nbeen long observed suffering from the catastrophic forgetting problem. To\ninvestigate such a retentive-forgetful contradiction and understand the memory\nmechanism of language models, we conduct thorough experiments by controlling\nthe target knowledge types, the learning strategies and the learning schedules.\nWe find that: 1) Vanilla language models are forgetful; 2) Pre-training leads\nto retentive language models; 3) Knowledge relevance and diversification\nsignificantly influence the memory formation. These conclusions are useful for\nunderstanding the abilities of pre-trained language models and shed light on\ndesigning and evaluating new learning and inference algorithms of language\nmodels.", "published": "2023-05-16 03:50:38", "link": "http://arxiv.org/abs/2305.09144v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dual-Alignment Pre-training for Cross-lingual Sentence Embedding", "abstract": "Recent studies have shown that dual encoder models trained with the\nsentence-level translation ranking task are effective methods for cross-lingual\nsentence embedding. However, our research indicates that token-level alignment\nis also crucial in multilingual scenarios, which has not been fully explored\npreviously. Based on our findings, we propose a dual-alignment pre-training\n(DAP) framework for cross-lingual sentence embedding that incorporates both\nsentence-level and token-level alignment. To achieve this, we introduce a novel\nrepresentation translation learning (RTL) task, where the model learns to use\none-side contextualized token representation to reconstruct its translation\ncounterpart. This reconstruction objective encourages the model to embed\ntranslation information into the token representation. Compared to other\ntoken-level alignment methods such as translation language modeling, RTL is\nmore suitable for dual encoder architectures and is computationally efficient.\nExtensive experiments on three sentence-level cross-lingual benchmarks\ndemonstrate that our approach can significantly improve sentence embedding. Our\ncode is available at https://github.com/ChillingDream/DAP.", "published": "2023-05-16 03:53:30", "link": "http://arxiv.org/abs/2305.09148v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Unifying Multi-Lingual and Cross-Lingual Summarization", "abstract": "To adapt text summarization to the multilingual world, previous work proposes\nmulti-lingual summarization (MLS) and cross-lingual summarization (CLS).\nHowever, these two tasks have been studied separately due to the different\ndefinitions, which limits the compatible and systematic research on both of\nthem. In this paper, we aim to unify MLS and CLS into a more general setting,\ni.e., many-to-many summarization (M2MS), where a single model could process\ndocuments in any language and generate their summaries also in any language. As\nthe first step towards M2MS, we conduct preliminary studies to show that M2MS\ncan better transfer task knowledge across different languages than MLS and CLS.\nFurthermore, we propose Pisces, a pre-trained M2MS model that learns language\nmodeling, cross-lingual ability and summarization ability via three-stage\npre-training. Experimental results indicate that our Pisces significantly\noutperforms the state-of-the-art baselines, especially in the zero-shot\ndirections, where there is no training data from the source-language documents\nto the target-language summaries.", "published": "2023-05-16 06:53:21", "link": "http://arxiv.org/abs/2305.09220v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low\n  Training Data Instruction Tuning", "abstract": "Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.", "published": "2023-05-16 07:52:57", "link": "http://arxiv.org/abs/2305.09246v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "HyHTM: Hyperbolic Geometry based Hierarchical Topic Models", "abstract": "Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies\nin a collection of documents. However, traditional HTMs often produce\nhierarchies where lowerlevel topics are unrelated and not specific enough to\ntheir higher-level topics. Additionally, these methods can be computationally\nexpensive. We present HyHTM - a Hyperbolic geometry based Hierarchical Topic\nModels - that addresses these limitations by incorporating hierarchical\ninformation from hyperbolic geometry to explicitly model hierarchies in topic\nmodels. Experimental results with four baselines show that HyHTM can better\nattend to parent-child relationships among topics. HyHTM produces coherent\ntopic hierarchies that specialise in granularity from generic higher-level\ntopics to specific lowerlevel topics. Further, our model is significantly\nfaster and leaves a much smaller memory footprint than our best-performing\nbaseline.We have made the source code for our algorithm publicly accessible.", "published": "2023-05-16 08:06:11", "link": "http://arxiv.org/abs/2305.09258v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "UniS-MMC: Multimodal Classification via Unimodality-supervised\n  Multimodal Contrastive Learning", "abstract": "Multimodal learning aims to imitate human beings to acquire complementary\ninformation from multiple modalities for various downstream tasks. However,\ntraditional aggregation-based multimodal fusion methods ignore the\ninter-modality relationship, treat each modality equally, suffer sensor noise,\nand thus reduce multimodal learning performance. In this work, we propose a\nnovel multimodal contrastive method to explore more reliable multimodal\nrepresentations under the weak supervision of unimodal predicting.\nSpecifically, we first capture task-related unimodal representations and the\nunimodal predictions from the introduced unimodal predicting task. Then the\nunimodal representations are aligned with the more effective one by the\ndesigned multimodal contrastive method under the supervision of the unimodal\npredictions. Experimental results with fused features on two image-text\nclassification benchmarks UPMC-Food-101 and N24News show that our proposed\nUnimodality-Supervised MultiModal Contrastive UniS-MMC learning method\noutperforms current state-of-the-art multimodal methods. The detailed ablation\nstudy and analysis further demonstrate the advantage of our proposed method.", "published": "2023-05-16 09:18:38", "link": "http://arxiv.org/abs/2305.09299v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Keyphrase Extraction from Long Scientific Documents using\n  Graph Embeddings", "abstract": "In this study, we investigate using graph neural network (GNN)\nrepresentations to enhance contextualized representations of pre-trained\nlanguage models (PLMs) for keyphrase extraction from lengthy documents. We show\nthat augmenting a PLM with graph embeddings provides a more comprehensive\nsemantic understanding of words in a document, particularly for long documents.\nWe construct a co-occurrence graph of the text and embed it using a graph\nconvolutional network (GCN) trained on the task of edge prediction. We propose\na graph-enhanced sequence tagging architecture that augments contextualized PLM\nembeddings with graph representations. Evaluating on benchmark datasets, we\ndemonstrate that enhancing PLMs with graph embeddings outperforms\nstate-of-the-art models on long documents, showing significant improvements in\nF1 scores across all the datasets. Our study highlights the potential of GNN\nrepresentations as a complementary approach to improve PLM performance for\nkeyphrase extraction from long documents.", "published": "2023-05-16 09:44:38", "link": "http://arxiv.org/abs/2305.09316v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural\n  Topic Modeling", "abstract": "Most existing topic models rely on bag-of-words (BOW) representation, which\nlimits their ability to capture word order information and leads to challenges\nwith out-of-vocabulary (OOV) words in new documents. Contextualized word\nembeddings, however, show superiority in word sense disambiguation and\neffectively address the OOV issue. In this work, we introduce a novel neural\ntopic model called the Contextlized Word Topic Model (CWTM), which integrates\ncontextualized word embeddings from BERT. The model is capable of learning the\ntopic vector of a document without BOW information. In addition, it can also\nderive the topic vectors for individual words within a document based on their\ncontextualized word embeddings. Experiments across various datasets show that\nCWTM generates more coherent and meaningful topics compared to existing topic\nmodels, while also accommodating unseen words in newly encountered documents.", "published": "2023-05-16 10:07:33", "link": "http://arxiv.org/abs/2305.09329v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Preliminary Analysis on the Code Generation Capabilities of GPT-3.5\n  and Bard AI Models for Java Functions", "abstract": "This paper evaluates the capability of two state-of-the-art artificial\nintelligence (AI) models, GPT-3.5 and Bard, in generating Java code given a\nfunction description. We sourced the descriptions from CodingBat.com, a popular\nonline platform that provides practice problems to learn programming. We\ncompared the Java code generated by both models based on correctness, verified\nthrough the platform's own test cases. The results indicate clear differences\nin the capabilities of the two models. GPT-3.5 demonstrated superior\nperformance, generating correct code for approximately 90.6% of the function\ndescriptions, whereas Bard produced correct code for 53.1% of the functions.\nWhile both models exhibited strengths and weaknesses, these findings suggest\npotential avenues for the development and refinement of more advanced\nAI-assisted code generation tools. The study underlines the potential of AI in\nautomating and supporting aspects of software development, although further\nresearch is required to fully realize this potential.", "published": "2023-05-16 12:44:39", "link": "http://arxiv.org/abs/2305.09402v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "About Evaluation of F1 Score for RECENT Relation Extraction System", "abstract": "This document contains a discussion of the F1 score evaluation used in the\narticle 'Relation Classification with Entity Type Restriction' by Shengfei Lyu,\nHuanhuan Chen published on Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021. The authors created a system named RECENT and\nclaim it achieves (then) a new state-of-the-art result 75.2 (previous 74.8) on\nthe TACRED dataset, while after correcting errors and reevaluation the final\nresult is 65.16", "published": "2023-05-16 12:55:43", "link": "http://arxiv.org/abs/2305.09410v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Built-in Autoregressive Search Engines", "abstract": "Document retrieval is a key stage of standard Web search engines. Existing\ndual-encoder dense retrievers obtain representations for questions and\ndocuments independently, allowing for only shallow interactions between them.\nTo overcome this limitation, recent autoregressive search engines replace the\ndual-encoder architecture by directly generating identifiers for relevant\ndocuments in the candidate pool. However, the training cost of such\nautoregressive search engines rises sharply as the number of candidate\ndocuments increases. In this paper, we find that large language models (LLMs)\ncan follow human instructions to directly generate URLs for document retrieval.\n  Surprisingly, when providing a few {Query-URL} pairs as in-context\ndemonstrations, LLMs can generate Web URLs where nearly 90\\% of the\ncorresponding documents contain correct answers to open-domain questions. In\nthis way, LLMs can be thought of as built-in search engines, since they have\nnot been explicitly trained to map questions to document identifiers.\nExperiments demonstrate that our method can consistently achieve better\nretrieval performance than existing retrieval approaches by a significant\nmargin on three open-domain question answering benchmarks, under both zero and\nfew-shot settings. The code for this work can be found at\n\\url{https://github.com/Ziems/llm-url}.", "published": "2023-05-16 17:04:48", "link": "http://arxiv.org/abs/2305.09612v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Tailoring Instructions to Student's Learning Levels Boosts Knowledge\n  Distillation", "abstract": "It has been commonly observed that a teacher model with superior performance\ndoes not necessarily result in a stronger student, highlighting a discrepancy\nbetween current teacher training practices and effective knowledge transfer. In\norder to enhance the guidance of the teacher training process, we introduce the\nconcept of distillation influence to determine the impact of distillation from\neach training sample on the student's generalization ability. In this paper, we\npropose Learning Good Teacher Matters (LGTM), an efficient training technique\nfor incorporating distillation influence into the teacher's learning process.\nBy prioritizing samples that are likely to enhance the student's generalization\nability, our LGTM outperforms 10 common knowledge distillation baselines on 6\ntext classification tasks in the GLUE benchmark.", "published": "2023-05-16 17:50:09", "link": "http://arxiv.org/abs/2305.09651v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SatLM: Satisfiability-Aided Language Models Using Declarative Prompting", "abstract": "Prior work has combined chain-of-thought prompting in large language models\n(LLMs) with programmatic representations to perform effective and transparent\nreasoning. While such an approach works well for tasks that only require\nforward reasoning (e.g., straightforward arithmetic), it is less effective for\nconstraint solving problems that require more sophisticated planning and\nsearch. In this paper, we propose a new satisfiability-aided language modeling\n(SatLM) approach for improving the reasoning capabilities of LLMs. We use an\nLLM to generate a declarative task specification rather than an imperative\nprogram and leverage an off-the-shelf automated theorem prover to derive the\nfinal answer. This approach has two key advantages. The declarative\nspecification is closer to the problem description than the reasoning steps\nare, so the LLM can parse it out of the description more accurately.\nFurthermore, by offloading the actual reasoning task to an automated theorem\nprover, our approach can guarantee the correctness of the answer with respect\nto the parsed specification and avoid planning errors in the solving process.\nWe evaluate SATLM on 8 different datasets and show that it consistently\noutperforms program-aided LMs in the imperative paradigm. In particular, SATLM\noutperforms program-aided LMs by 23% on a challenging subset of the GSM\narithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and\nBoardgameQA, surpassing previous models that are trained on the respective\ntraining sets.", "published": "2023-05-16 17:55:51", "link": "http://arxiv.org/abs/2305.09656v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task\n  Recognition and Task Learning", "abstract": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks\nwith only a few demonstrations, but its mechanisms are not yet well-understood.\nSome works suggest that LLMs only recall already learned concepts from\npre-training, while others hint that ICL performs implicit learning over\ndemonstrations. We characterize two ways through which ICL leverages\ndemonstrations. Task recognition (TR) captures the extent to which LLMs can\nrecognize a task through demonstrations -- even without ground-truth labels --\nand apply their pre-trained priors, whereas task learning (TL) is the ability\nto capture new input-label mappings unseen in pre-training. Using a wide range\nof classification datasets and three LLM families (GPT-3, LLaMA and OPT), we\ndesign controlled experiments to disentangle the roles of TR and TL in ICL. We\nshow that (1) models can achieve non-trivial performance with only TR, and TR\ndoes not further improve with larger models or more demonstrations; (2) LLMs\nacquire TL as the model scales, and TL's performance consistently improves with\nmore demonstrations in context. Our findings unravel two different forces\nbehind ICL and we advocate for discriminating them in future ICL research due\nto their distinct nature.", "published": "2023-05-16 18:05:19", "link": "http://arxiv.org/abs/2305.09731v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In\n  Zero Shot", "abstract": "Multimedia content, such as advertisements and story videos, exhibit a rich\nblend of creativity and multiple modalities. They incorporate elements like\ntext, visuals, audio, and storytelling techniques, employing devices like\nemotions, symbolism, and slogans to convey meaning. There is a dearth of large\nannotated training datasets in the multimedia domain hindering the development\nof supervised learning models with satisfactory performance for real-world\napplications. On the other hand, the rise of large language models (LLMs) has\nwitnessed remarkable zero-shot performance in various natural language\nprocessing (NLP) tasks, such as emotion classification, question-answering, and\ntopic classification. To leverage such advanced techniques to bridge this\nperformance gap in multimedia understanding, we propose verbalizing long videos\nto generate their descriptions in natural language, followed by performing\nvideo-understanding tasks on the generated story as opposed to the original\nvideo. Through extensive experiments on fifteen video-understanding tasks, we\ndemonstrate that our method, despite being zero-shot, achieves significantly\nbetter results than supervised baselines for video understanding. Furthermore,\nto alleviate a lack of story understanding benchmarks, we publicly release the\nfirst dataset on a crucial task in computational social science on persuasion\nstrategy identification.", "published": "2023-05-16 19:13:11", "link": "http://arxiv.org/abs/2305.09758v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned\n  Language Models", "abstract": "Learning vectors that capture the meaning of concepts remains a fundamental\nchallenge. Somewhat surprisingly, perhaps, pre-trained language models have\nthus far only enabled modest improvements to the quality of such concept\nembeddings. Current strategies for using language models typically represent a\nconcept by averaging the contextualised representations of its mentions in some\ncorpus. This is potentially sub-optimal for at least two reasons. First,\ncontextualised word vectors have an unusual geometry, which hampers downstream\ntasks. Second, concept embeddings should capture the semantic properties of\nconcepts, whereas contextualised word vectors are also affected by other\nfactors. To address these issues, we propose two contrastive learning\nstrategies, based on the view that whenever two sentences reveal similar\nproperties, the corresponding contextualised vectors should also be similar.\nOne strategy is fully unsupervised, estimating the properties which are\nexpressed in a sentence from the neighbourhood structure of the contextualised\nword embeddings. The second strategy instead relies on a distant supervision\nsignal from ConceptNet. Our experimental results show that the resulting\nvectors substantially outperform existing concept embeddings in predicting the\nsemantic properties of concepts, with the ConceptNet-based strategy achieving\nthe best results. These findings are furthermore confirmed in a clustering task\nand in the downstream task of ontology completion.", "published": "2023-05-16 20:17:02", "link": "http://arxiv.org/abs/2305.09785v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Dataset Transferability in Active Learning for Transformers", "abstract": "Active learning (AL) aims to reduce labeling costs by querying the examples\nmost beneficial for model learning. While the effectiveness of AL for\nfine-tuning transformer-based pre-trained language models (PLMs) has been\ndemonstrated, it is less clear to what extent the AL gains obtained with one\nmodel transfer to others. We consider the problem of transferability of\nactively acquired datasets in text classification and investigate whether AL\ngains persist when a dataset built using AL coupled with a specific PLM is used\nto train a different PLM. We link the AL dataset transferability to the\nsimilarity of instances queried by the different PLMs and show that AL methods\nwith similar acquisition sequences produce highly transferable datasets\nregardless of the models used. Additionally, we show that the similarity of\nacquisition sequences is influenced more by the choice of the AL method than\nthe choice of the model.", "published": "2023-05-16 21:10:54", "link": "http://arxiv.org/abs/2305.09807v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation\n  Detection in Online Communities", "abstract": "Detecting norm violations in online communities is critical to maintaining\nhealthy and safe spaces for online discussions. Existing machine learning\napproaches often struggle to adapt to the diverse rules and interpretations\nacross different communities due to the inherent challenges of fine-tuning\nmodels for such context-specific tasks. In this paper, we introduce\nContext-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a\nnovel method that employs prompt-based learning to detect norm violations\nacross various types of rules. CPL-NoViD outperforms the baseline by\nincorporating context through natural language prompts and demonstrates\nimproved performance across different rule types. Significantly, it not only\nexcels in cross-rule-type and cross-community norm violation detection but also\nexhibits adaptability in few-shot learning scenarios. Most notably, it\nestablishes a new state-of-the-art in norm violation detection, surpassing\nexisting benchmarks. Our work highlights the potential of prompt-based learning\nfor context-sensitive norm violation detection and paves the way for future\nresearch on more adaptable, context-aware models to better support online\ncommunity moderators.", "published": "2023-05-16 23:27:59", "link": "http://arxiv.org/abs/2305.09846v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Pre-training with Unified Modality Masking for\n  Visual Document Understanding", "abstract": "This paper presents GenDoc, a general sequence-to-sequence document\nunderstanding model pre-trained with unified masking across three modalities:\ntext, image, and layout. The proposed model utilizes an encoder-decoder\narchitecture, which allows for increased adaptability to a wide range of\ndownstream tasks with diverse output formats, in contrast to the encoder-only\nmodels commonly employed in document understanding. In addition to the\ntraditional text infilling task used in previous encoder-decoder models, our\npre-training extends to include tasks of masked image token prediction and\nmasked layout prediction. We also design modality-specific instruction and\nadopt both disentangled attention and the mixture-of-modality-experts strategy\nto effectively capture the information leveraged by each modality. Evaluation\nof the proposed model through extensive experiments on several downstream tasks\nin document understanding demonstrates its ability to achieve superior or\ncompetitive performance compared to state-of-the-art approaches. Our analysis\nfurther suggests that GenDoc is more robust than the encoder-only models in\nscenarios where the OCR quality is imperfect.", "published": "2023-05-16 15:25:19", "link": "http://arxiv.org/abs/2305.10448v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is a Video worth $n\\times n$ Images? A Highly Efficient Approach to\n  Transformer-based Video Question Answering", "abstract": "Conventional Transformer-based Video Question Answering (VideoQA) approaches\ngenerally encode frames independently through one or more image encoders\nfollowed by interaction between frames and question. However, such schema would\nincur significant memory use and inevitably slow down the training and\ninference speed. In this work, we present a highly efficient approach for\nVideoQA based on existing vision-language pre-trained models where we\nconcatenate video frames to a $n\\times n$ matrix and then convert it to one\nimage. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$\nwhile maintaining the temporal structure of the original video. Experimental\nresults on MSRVTT and TrafficQA show that our proposed approach achieves\nstate-of-the-art performance with nearly $4\\times$ faster speed and only 30%\nmemory use. We show that by integrating our approach into VideoQA systems we\ncan achieve comparable, even superior, performance with a significant speed up\nfor training and inference. We believe the proposed approach can facilitate\nVideoQA-related research by reducing the computational requirements for those\nwho have limited access to budgets and resources. Our code will be made\npublicly available for research use.", "published": "2023-05-16 02:12:57", "link": "http://arxiv.org/abs/2305.09107v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Adversarial Speaker Disentanglement Using Unannotated External Data for\n  Self-supervised Representation Based Voice Conversion", "abstract": "Nowadays, recognition-synthesis-based methods have been quite popular with\nvoice conversion (VC). By introducing linguistics features with good\ndisentangling characters extracted from an automatic speech recognition (ASR)\nmodel, the VC performance achieved considerable breakthroughs. Recently,\nself-supervised learning (SSL) methods trained with a large-scale unannotated\nspeech corpus have been applied to downstream tasks focusing on the content\ninformation, which is suitable for VC tasks. However, a huge amount of speaker\ninformation in SSL representations degrades timbre similarity and the quality\nof converted speech significantly. To address this problem, we proposed a\nhigh-similarity any-to-one voice conversion method with the input of SSL\nrepresentations. We incorporated adversarial training mechanisms in the\nsynthesis module using external unannotated corpora. Two auxiliary\ndiscriminators were trained to distinguish whether a sequence of\nmel-spectrograms has been converted by the acoustic model and whether a\nsequence of content embeddings contains speaker information from external\ncorpora. Experimental results show that our proposed method achieves comparable\nsimilarity and higher naturalness than the supervised method, which needs a\nhuge amount of annotated corpora for training and is applicable to improve\nsimilarity for VC methods with other SSL representations as input.", "published": "2023-05-16 04:52:29", "link": "http://arxiv.org/abs/2305.09167v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Weighted M\u00f6bius Score: A Unified Framework for Feature Attribution", "abstract": "Feature attribution aims to explain the reasoning behind a black-box model's\nprediction by identifying the impact of each feature on the prediction. Recent\nwork has extended feature attribution to interactions between multiple\nfeatures. However, the lack of a unified framework has led to a proliferation\nof methods that are often not directly comparable. This paper introduces a\nparameterized attribution framework -- the Weighted M\\\"obius Score -- and (i)\nshows that many different attribution methods for both individual features and\nfeature interactions are special cases and (ii) identifies some new methods. By\nstudying the vector space of attribution methods, our framework utilizes\nstandard linear algebra tools and provides interpretations in various fields,\nincluding cooperative game theory and causal mediation analysis. We empirically\ndemonstrate the framework's versatility and effectiveness by applying these\nattribution methods to feature interactions in sentiment analysis and\nchain-of-thought prompting.", "published": "2023-05-16 06:27:27", "link": "http://arxiv.org/abs/2305.09204v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hybrid and Collaborative Passage Reranking", "abstract": "In passage retrieval system, the initial passage retrieval results may be\nunsatisfactory, which can be refined by a reranking scheme. Existing solutions\nto passage reranking focus on enriching the interaction between query and each\npassage separately, neglecting the context among the top-ranked passages in the\ninitial retrieval list. To tackle this problem, we propose a Hybrid and\nCollaborative Passage Reranking (HybRank) method, which leverages the\nsubstantial similarity measurements of upstream retrievers for passage\ncollaboration and incorporates the lexical and semantic properties of sparse\nand dense retrievers for reranking. Besides, built on off-the-shelf retriever\nfeatures, HybRank is a plug-in reranker capable of enhancing arbitrary passage\nlists including previously reranked ones. Extensive experiments demonstrate the\nstable improvements of performance over prevalent retrieval and reranking\nmethods, and verify the effectiveness of the core components of HybRank.", "published": "2023-05-16 09:38:52", "link": "http://arxiv.org/abs/2305.09313v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Multi-modal Visual Understanding with Prompts for Semantic Information\n  Disentanglement of Image", "abstract": "Multi-modal visual understanding of images with prompts involves using\nvarious visual and textual cues to enhance the semantic understanding of\nimages. This approach combines both vision and language processing to generate\nmore accurate predictions and recognition of images. By utilizing prompt-based\ntechniques, models can learn to focus on certain features of an image to\nextract useful information for downstream tasks. Additionally, multi-modal\nunderstanding can improve upon single modality models by providing more robust\nrepresentations of images. Overall, the combination of visual and textual\ninformation is a promising area of research for advancing image recognition and\nunderstanding. In this paper we will try an amount of prompt design methods and\npropose a new method for better extraction of semantic information", "published": "2023-05-16 10:15:44", "link": "http://arxiv.org/abs/2305.09333v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with\n  Transformers", "abstract": "Message Passing Interface (MPI) plays a crucial role in distributed memory\nparallelization across multiple nodes. However, parallelizing MPI code\nmanually, and specifically, performing domain decomposition, is a challenging,\nerror-prone task. In this paper, we address this problem by developing\nMPI-RICAL, a novel data-driven, programming-assistance tool that assists\nprogrammers in writing domain decomposition based distributed memory\nparallelization code. Specifically, we train a supervised language model to\nsuggest MPI functions and their proper locations in the code on the fly. We\nalso introduce MPICodeCorpus, the first publicly available corpus of MPI-based\nparallel programs that is created by mining more than 15,000 open-source\nrepositories on GitHub. Experimental results have been done on MPICodeCorpus\nand more importantly, on a compiled benchmark of MPI-based parallel programs\nfor numerical computations that represent real-world scientific applications.\nMPI-RICAL achieves F1 scores between 0.87-0.91 on these programs, demonstrating\nits accuracy in suggesting correct MPI functions at appropriate code\nlocations.. The source code used in this work, as well as other relevant\nsources, are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/MPI-rical", "published": "2023-05-16 13:50:24", "link": "http://arxiv.org/abs/2305.09438v3", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "Life of PII -- A PII Obfuscation Transformer", "abstract": "Protecting sensitive information is crucial in today's world of Large\nLanguage Models (LLMs) and data-driven services. One common method used to\npreserve privacy is by using data perturbation techniques to reduce\noverreaching utility of (sensitive) Personal Identifiable Information (PII)\ndata while maintaining its statistical and semantic properties. Data\nperturbation methods often result in significant information loss, making them\nimpractical for use. In this paper, we propose 'Life of PII', a novel\nObfuscation Transformer framework for transforming PII into faux-PII while\npreserving the original information, intent, and context as much as possible.\nOur approach includes an API to interface with the given document, a\nconfiguration-based obfuscator, and a model based on the Transformer\narchitecture, which has shown high context preservation and performance in\nnatural language processing tasks and LLMs.\n  Our Transformer-based approach learns mapping between the original PII and\nits transformed faux-PII representation, which we call \"obfuscated\" data. Our\nexperiments demonstrate that our method, called Life of PII, outperforms\ntraditional data perturbation techniques in terms of both utility preservation\nand privacy protection. We show that our approach can effectively reduce\nutility loss while preserving the original information, offering greater\nflexibility in the trade-off between privacy protection and data utility. Our\nwork provides a solution for protecting PII in various real-world applications.", "published": "2023-05-16 15:48:36", "link": "http://arxiv.org/abs/2305.09550v2", "categories": ["cs.CL", "cs.IR", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "UOR: Universal Backdoor Attacks on Pre-trained Language Models", "abstract": "Backdoors implanted in pre-trained language models (PLMs) can be transferred\nto various downstream tasks, which exposes a severe security threat. However,\nmost existing backdoor attacks against PLMs are un-targeted and task-specific.\nFew targeted and task-agnostic methods use manually pre-defined triggers and\noutput representations, which prevent the attacks from being more effective and\ngeneral. In this paper, we first summarize the requirements that a more\nthreatening backdoor attack against PLMs should satisfy, and then propose a new\nbackdoor attack method called UOR, which breaks the bottleneck of the previous\napproach by turning manual selection into automatic optimization. Specifically,\nwe define poisoned supervised contrastive learning which can automatically\nlearn the more uniform and universal output representations of triggers for\nvarious PLMs. Moreover, we use gradient search to select appropriate trigger\nwords which can be adaptive to different PLMs and vocabularies. Experiments\nshow that our method can achieve better attack performance on various text\nclassification tasks compared to manual methods. Further, we tested our method\non PLMs with different architectures, different usage paradigms, and more\ndifficult tasks, which demonstrated the universality of our method.", "published": "2023-05-16 16:11:48", "link": "http://arxiv.org/abs/2305.09574v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Towards Expert-Level Medical Question Answering with Large Language\n  Models", "abstract": "Recent artificial intelligence (AI) systems have reached milestones in \"grand\nchallenges\" ranging from Go to protein-folding. The capability to retrieve\nmedical knowledge, reason over it, and answer medical questions comparably to\nphysicians has long been viewed as one such grand challenge.\n  Large language models (LLMs) have catalyzed significant progress in medical\nquestion answering; Med-PaLM was the first model to exceed a \"passing\" score in\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\non the MedQA dataset. However, this and other prior work suggested significant\nroom for improvement, especially when models' answers were compared to\nclinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\nfinetuning, and prompting strategies including a novel ensemble refinement\napproach.\n  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art. We also observed performance\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\nclinical topics datasets.\n  We performed detailed human evaluations on long-form questions along multiple\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\n< 0.001). We also observed significant improvements compared to Med-PaLM on\nevery evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form\n\"adversarial\" questions to probe LLM limitations.\n  While further studies are necessary to validate the efficacy of these models\nin real-world settings, these results highlight rapid progress towards\nphysician-level performance in medical question answering.", "published": "2023-05-16 17:11:29", "link": "http://arxiv.org/abs/2305.09617v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for\n  Opinion Prediction", "abstract": "Large language models (LLMs) that produce human-like responses have begun to\nrevolutionize research practices in the social sciences. We develop a novel\nmethodological framework that fine-tunes LLMs with repeated cross-sectional\nsurveys to incorporate the meaning of survey questions, individual beliefs, and\ntemporal contexts for opinion prediction. We introduce two new emerging\napplications of the AI-augmented survey: retrodiction (i.e., predict year-level\nmissing responses) and unasked opinion prediction (i.e., predict entirely\nmissing responses). Among 3,110 binarized opinions from 68,846 Americans in the\nGeneral Social Survey from 1972 to 2021, our models based on Alpaca-7b excel in\nretrodiction (AUC = 0.86 for personal opinion prediction, $\\rho$ = 0.98 for\npublic opinion prediction). These remarkable prediction capabilities allow us\nto fill in missing trends with high confidence and pinpoint when public\nattitudes changed, such as the rising support for same-sex marriage. On the\nother hand, our fine-tuned Alpaca-7b models show modest success in unasked\nopinion prediction (AUC = 0.73, $\\rho$ = 0.67). We discuss practical\nconstraints and ethical concerns regarding individual autonomy and privacy when\nusing LLMs for opinion prediction. Our study demonstrates that LLMs and surveys\ncan mutually enhance each other's capabilities: LLMs can broaden survey\npotential, while surveys can improve the alignment of LLMs.", "published": "2023-05-16 17:13:07", "link": "http://arxiv.org/abs/2305.09620v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Interpreter Understands Your Meaning: End-to-end Spoken Language\n  Understanding Aided by Speech Translation", "abstract": "End-to-end spoken language understanding (SLU) remains elusive even with\ncurrent large pretrained language models on text and speech, especially in\nmultilingual cases. Machine translation has been established as a powerful\npretraining objective on text as it enables the model to capture high-level\nsemantics of the input utterance and associations between different languages,\nwhich is desired for speech models that work on lower-level acoustic frames.\nMotivated particularly by the task of cross-lingual SLU, we demonstrate that\nthe task of speech translation (ST) is a good means of pretraining speech\nmodels for end-to-end SLU on both intra- and cross-lingual scenarios.\n  By introducing ST, our models reach higher performance over baselines on\nmonolingual and multilingual intent classification as well as spoken question\nanswering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the\neffectiveness of our methods, we also create new benchmark datasets from both\nsynthetic and real sources, for speech summarization and low-resource/zero-shot\ntransfer from English to French or Spanish. We further show the value of\npreserving knowledge for the ST pretraining task for better downstream\nperformance, possibly using Bayesian transfer regularizers.", "published": "2023-05-16 17:53:03", "link": "http://arxiv.org/abs/2305.09652v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generative Table Pre-training Empowers Models for Tabular Prediction", "abstract": "Recently, the topic of table pre-training has attracted considerable research\ninterest. However, how to employ table pre-training to boost the performance of\ntabular prediction remains an open challenge. In this paper, we propose TapTap,\nthe first attempt that leverages table pre-training to empower models for\ntabular prediction. After pre-training on a large corpus of real-world tabular\ndata, TapTap can generate high-quality synthetic tables to support various\napplications on tabular data, including privacy protection, low resource\nregime, missing value imputation, and imbalanced classification. Extensive\nexperiments on 12 datasets demonstrate that TapTap outperforms a total of 16\nbaselines in different scenarios. Meanwhile, it can be easily combined with\nvarious backbone models, including LightGBM, Multilayer Perceptron (MLP) and\nTransformer. Moreover, with the aid of table pre-training, models trained using\nsynthetic data generated by TapTap can even compete with models using the\noriginal dataset on half of the experimental datasets, marking a milestone in\nthe development of synthetic tabular data generation. The codes are available\nat https://github.com/ZhangTP1996/TapTap.", "published": "2023-05-16 06:37:38", "link": "http://arxiv.org/abs/2305.09696v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Application-Agnostic Language Modeling for On-Device ASR", "abstract": "On-device automatic speech recognition systems face several challenges\ncompared to server-based systems. They have to meet stricter constraints in\nterms of speed, disk size and memory while maintaining the same accuracy. Often\nthey have to serve several applications with different distributions at once,\nsuch as communicating with a virtual assistant and speech-to-text. The simplest\nsolution to serve multiple applications is to build application-specific\n(language) models, but this leads to an increase in memory. Therefore, we\nexplore different data- and architecture-driven language modeling approaches to\nbuild a single application-agnostic model. We propose two novel feed-forward\narchitectures that find an optimal trade off between different on-device\nconstraints. In comparison to the application-specific solution, one of our\nnovel approaches reduces the disk size by half, while maintaining speed and\naccuracy of the original model.", "published": "2023-05-16 19:31:18", "link": "http://arxiv.org/abs/2305.09764v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to\n  Support Human-AI Scientific Writing", "abstract": "Despite a surge collection of XAI methods, users still struggle to obtain\nrequired AI explanations. Previous research suggests chatbots as dynamic\nsolutions, but the effective design of conversational XAI agents for practical\nhuman needs remains under-explored. This paper focuses on Conversational XAI\nfor AI-assisted scientific writing tasks. Drawing from human linguistic\ntheories and formative studies, we identify four design rationales:\n\"multifaceted\", \"controllability\", \"mix-initiative\", \"context-aware\ndrill-down\". We incorporate them into an interactive prototype, ConvXAI, which\nfacilitates heterogeneous AI explanations for scientific writing through\ndialogue. In two studies with 21 users, ConvXAI outperforms a GUI-based\nbaseline on improving human-perceived understanding and writing improvement.\nThe paper further discusses the practical human usage patterns in interacting\nwith ConvXAI for scientific co-writing.", "published": "2023-05-16 19:48:49", "link": "http://arxiv.org/abs/2305.09770v6", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SpecInfer: Accelerating Generative Large Language Model Serving with\n  Tree-based Speculative Inference and Verification", "abstract": "This paper introduces SpecInfer, a system that accelerates generative large\nlanguage model (LLM) serving with tree-based speculative inference and\nverification. The key idea behind SpecInfer is leveraging small speculative\nmodels to predict the LLM's outputs; the predictions are organized as a token\ntree, whose nodes each represent a candidate token sequence. The correctness of\nall candidate token sequences represented by a token tree is verified against\nthe LLM in parallel using a novel tree-based parallel decoding mechanism.\nSpecInfer uses an LLM as a token tree verifier instead of an incremental\ndecoder, which significantly reduces the end-to-end latency and computational\nrequirement for serving generative LLMs while provably preserving model\nquality. Our evaluation shows that SpecInfer outperforms existing LLM serving\nsystems by 1.5-2.8x for distributed LLM inference and by 2.6-3.5x for\noffloading-based LLM inference, while preserving the same generative\nperformance. SpecInfer is publicly available at\nhttps://github.com/flexflow/FlexFlow/", "published": "2023-05-16 20:12:59", "link": "http://arxiv.org/abs/2305.09781v4", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Ways of Words: The Impact of Word Choice on Information Engagement\n  and Decision Making", "abstract": "Little research has explored how information engagement (IE), the degree to\nwhich individuals interact with and use information in a manner that manifests\ncognitively, behaviorally, and affectively. This study explored the impact of\nphrasing, specifically word choice, on IE and decision making. Synthesizing two\ntheoretical models, User Engagement Theory UET and Information Behavior Theory\nIBT, a theoretical framework illustrating the impact of and relationships among\nthe three IE dimensions of perception, participation, and perseverance was\ndeveloped and hypotheses generated. The framework was empirically validated in\na large-scale user study measuring how word choice impacts the dimensions of\nIE. The findings provide evidence that IE differs from other forms of\nengagement in that it is driven and fostered by the expression of the\ninformation itself, regardless of the information system used to view, interact\nwith, and use the information. The findings suggest that phrasing can have a\nsignificant effect on the interpretation of and interaction with digital\ninformation, indicating the importance of expression of information, in\nparticular word choice, on decision making and IE. The research contributes to\nthe literature by identifying methods for assessment and improvement of IE and\ndecision making with digital text.", "published": "2023-05-16 20:46:36", "link": "http://arxiv.org/abs/2305.09798v1", "categories": ["cs.CL", "cs.HC", "cs.SY", "eess.SY", "stat.AP", "28-08", "H.5.2; H.1.2"], "primary_category": "cs.CL"}
{"title": "Understanding of Normal and Abnormal Hearts by Phase Space Analysis and\n  Convolutional Neural Networks", "abstract": "Cardiac diseases are one of the leading mortality factors in modern,\nindustrialized societies, which cause high expenses in public health systems.\nDue to high costs, developing analytical methods to improve cardiac diagnostics\nis essential. The heart's electric activity was first modeled using a set of\nnonlinear differential equations. Following this, variations of cardiac spectra\noriginating from deterministic dynamics are investigated. Analyzing a normal\nhuman heart's power spectra offers His-Purkinje network, which possesses a\nfractal-like structure. Phase space trajectories are extracted from the time\nseries electrocardiogram (ECG) graph with third-order derivate Taylor Series.\nHere in this study, phase space analysis and Convolutional Neural Networks\n(CNNs) method are applied to 44 records via the MIT-BIH database recorded with\nMLII. In order to increase accuracy, a straight line is drawn between the\nhighest Q-R distance in the phase space images of the records. Binary CNN\nclassification is used to determine healthy or unhealthy hearts. With a 90.90%\naccuracy rate, this model could classify records according to their heart\nstatus.", "published": "2023-05-16 19:52:40", "link": "http://arxiv.org/abs/2305.10450v1", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG", "cs.NE", "eess.SP", "68", "I.5.1"], "primary_category": "eess.IV"}
{"title": "TG-Critic: A Timbre-Guided Model for Reference-Independent Singing\n  Evaluation", "abstract": "Automatic singing evaluation independent of reference melody is a challenging\ntask due to its subjective and multi-dimensional nature. As an essential\nattribute of singing voices, vocal timbre has a non-negligible effect and\ninfluence on human perception of singing quality. However, no research has been\ndone to include timbre information explicitly in singing evaluation models. In\nthis paper, a data-driven model TG-Critic is proposed to introduce timbre\nembeddings as one of the model inputs to guide the evaluation of singing\nquality. The trunk structure of TG-Critic is designed as a multi-scale network\nto summarize the contextual information from constant-Q transform features in a\nhigh-resolution way. Furthermore, an automatic annotation method is designed to\nconstruct a large three-class singing evaluation dataset with low human-effort.\nThe experimental results show that the proposed model outperforms the existing\nstate-of-the-art models in most cases.", "published": "2023-05-16 03:15:17", "link": "http://arxiv.org/abs/2305.09127v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Cross-Modal Global Interaction and Local Alignment for Audio-Visual\n  Speech Recognition", "abstract": "Audio-visual speech recognition (AVSR) research has gained a great success\nrecently by improving the noise-robustness of audio-only automatic speech\nrecognition (ASR) with noise-invariant visual information. However, most\nexisting AVSR approaches simply fuse the audio and visual features by\nconcatenation, without explicit interactions to capture the deep correlations\nbetween them, which results in sub-optimal multimodal representations for\ndownstream speech recognition task. In this paper, we propose a cross-modal\nglobal interaction and local alignment (GILA) approach for AVSR, which captures\nthe deep audio-visual (A-V) correlations from both global and local\nperspectives. Specifically, we design a global interaction model to capture the\nA-V complementary relationship on modality level, as well as a local alignment\napproach to model the A-V temporal consistency on frame level. Such a holistic\nview of cross-modal correlations enable better multimodal representations for\nAVSR. Experiments on public benchmarks LRS3 and LRS2 show that our GILA\noutperforms the supervised learning state-of-the-art.", "published": "2023-05-16 06:41:25", "link": "http://arxiv.org/abs/2305.09212v1", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pink-Eggs Dataset V1: A Step Toward Invasive Species Management Using\n  Deep Learning Embedded Solutions", "abstract": "We introduce a novel dataset consisting of images depicting pink eggs that\nhave been identified as Pomacea canaliculata eggs, accompanied by corresponding\nbounding box annotations. The purpose of this dataset is to aid researchers in\nthe analysis of the spread of Pomacea canaliculata species by utilizing deep\nlearning techniques, as well as supporting other investigative pursuits that\nrequire visual data pertaining to the eggs of Pomacea canaliculata. It is worth\nnoting, however, that the identity of the eggs in question is not definitively\nestablished, as other species within the same taxonomic family have been\nobserved to lay similar-looking eggs in regions of the Americas. Therefore, a\ncrucial prerequisite to any decision regarding the elimination of these eggs\nwould be to establish with certainty whether they are exclusively attributable\nto invasive Pomacea canaliculata or if other species are also involved. The\ndataset is available at https://www.kaggle.com/datasets/deeshenzhen/pinkeggs", "published": "2023-05-16 09:21:56", "link": "http://arxiv.org/abs/2305.09302v1", "categories": ["cs.CV", "cs.AI", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Low-complexity deep learning frameworks for acoustic scene\n  classification using teacher-student scheme and multiple spectrograms", "abstract": "In this technical report, a low-complexity deep learning system for acoustic\nscene classification (ASC) is presented. The proposed system comprises two main\nphases: (Phase I) Training a teacher network; and (Phase II) training a student\nnetwork using distilled knowledge from the teacher. In the first phase, the\nteacher, which presents a large footprint model, is trained. After training the\nteacher, the embeddings, which are the feature map of the second last layer of\nthe teacher, are extracted. In the second phase, the student network, which\npresents a low complexity model, is trained with the embeddings extracted from\nthe teacher. Our experiments conducted on DCASE 2023 Task 1 Development dataset\nhave fulfilled the requirement of low-complexity and achieved the best\nclassification accuracy of 57.4%, improving DCASE baseline by 14.5%.", "published": "2023-05-16 14:21:45", "link": "http://arxiv.org/abs/2305.09463v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discrete Diffusion Probabilistic Models for Symbolic Music Generation", "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have made great strides in\ngenerating high-quality samples in both discrete and continuous domains.\nHowever, Discrete DDPMs (D3PMs) have yet to be applied to the domain of\nSymbolic Music. This work presents the direct generation of Polyphonic Symbolic\nMusic using D3PMs. Our model exhibits state-of-the-art sample quality,\naccording to current quantitative evaluation metrics, and allows for flexible\ninfilling at the note level. We further show, that our models are accessible to\npost-hoc classifier guidance, widening the scope of possible applications.\nHowever, we also cast a critical view on quantitative evaluation of music\nsample quality via statistical metrics, and present a simple algorithm that can\nconfound our metrics with completely spurious, non-musical samples.", "published": "2023-05-16 14:43:38", "link": "http://arxiv.org/abs/2305.09489v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust and lightweight audio fingerprint for Automatic Content\n  Recognition", "abstract": "This research paper presents a novel audio fingerprinting system for\nAutomatic Content Recognition (ACR). By using signal processing techniques and\nstatistical transformations, our proposed method generates compact fingerprints\nof audio segments that are robust to noise degradations present in real-world\naudio. The system is designed to be highly scalable, with the ability to\nidentify thousands of hours of content using fingerprints generated from\nmillions of TVs. The fingerprint's high temporal correlation and utilization of\nexisting GPU-compatible Approximate Nearest Neighbour (ANN) search algorithms\nmake this possible. Furthermore, the fingerprint generation can run on\nlow-power devices with limited compute, making it accessible to a wide range of\napplications. Experimental results show improvements in our proposed system\ncompared to a min-hash based audio fingerprint on all evaluated metrics,\nincluding accuracy on proprietary ACR datasets, retrieval speed, memory usage,\nand robustness to various noises. For similar retrieval accuracy, our system is\n30x faster and uses 6x fewer fingerprints than the min-hash method.", "published": "2023-05-16 15:55:03", "link": "http://arxiv.org/abs/2305.09559v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundStorm: Efficient Parallel Audio Generation", "abstract": "We present SoundStorm, a model for efficient, non-autoregressive audio\ngeneration. SoundStorm receives as input the semantic tokens of AudioLM, and\nrelies on bidirectional attention and confidence-based parallel decoding to\ngenerate the tokens of a neural audio codec. Compared to the autoregressive\ngeneration approach of AudioLM, our model produces audio of the same quality\nand with higher consistency in voice and acoustic conditions, while being two\norders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5\nseconds on a TPU-v4. We demonstrate the ability of our model to scale audio\ngeneration to longer sequences by synthesizing high-quality, natural dialogue\nsegments, given a transcript annotated with speaker turns and a short prompt\nwith the speakers' voices.", "published": "2023-05-16 17:41:25", "link": "http://arxiv.org/abs/2305.09636v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
