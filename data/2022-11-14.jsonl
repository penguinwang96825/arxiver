{"title": "Controllable Citation Sentence Generation with Language Models", "abstract": "Citation generation aims to generate a citation sentence that refers to a\nchosen paper in the context of a manuscript. However, a rigid citation\ngeneration process is at odds with an author's desire to control specific\nattributes, such as 1) the citation intent, e.g., either introducing background\ninformation or comparing results, and 2) keywords that should appear in the\ncitation text. To provide these degrees of controllability during citation\ngeneration, we propose to integrate the manuscript context, the context of the\nreferenced paper, and the desired control attributes into a structured template\nand use it to fine-tune a language model (LM) via next-token prediction. We\nthen utilize Proximal Policy Optimization to directly optimize the LM in favor\nof a high score of our proposed controllability metric. The proposed workflow\nharmoniously combines citation attribute suggestion and conditional citation\ngeneration into one LM, allowing for better user control.", "published": "2022-11-14 01:54:08", "link": "http://arxiv.org/abs/2211.07066v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Generative Question Answering for Event Argument\n  Extraction", "abstract": "Event argument extraction has long been studied as a sequential prediction\nproblem with extractive-based methods, tackling each argument in isolation.\nAlthough recent work proposes generation-based methods to capture\ncross-argument dependency, they require generating and post-processing a\ncomplicated target sequence (template). Motivated by these observations and\nrecent pretrained language models' capabilities of learning from\ndemonstrations. We propose a retrieval-augmented generative QA model (R-GQA)\nfor event argument extraction. It retrieves the most similar QA pair and\naugments it as prompt to the current example's context, then decodes the\narguments as answers. Our approach outperforms substantially prior methods\nacross various settings (i.e. fully supervised, domain transfer, and fewshot\nlearning). Finally, we propose a clustering-based sampling strategy (JointEnc)\nand conduct a thorough analysis of how different strategies influence the\nfew-shot learning performance. The implementations are available at https://\ngithub.com/xinyadu/RGQA", "published": "2022-11-14 02:00:32", "link": "http://arxiv.org/abs/2211.07067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Easy Guided Decoding in Providing Suggestions for Interactive Machine\n  Translation", "abstract": "Machine translation technology has made great progress in recent years, but\nit cannot guarantee error free results. Human translators perform post editing\non machine translations to correct errors in the scene of computer aided\ntranslation. In favor of expediting the post editing process, many works have\ninvestigated machine translation in interactive modes, in which machines can\nautomatically refine the rest of translations constrained by human's edits.\nTranslation Suggestion (TS), as an interactive mode to assist human\ntranslators, requires machines to generate alternatives for specific incorrect\nwords or phrases selected by human translators. In this paper, we utilize the\nparameterized objective function of neural machine translation (NMT) and\npropose a novel constrained decoding algorithm, namely Prefix Suffix Guided\nDecoding (PSGD), to deal with the TS problem without additional training.\nCompared to the state of the art lexically constrained decoding method, PSGD\nimproves translation quality by an average of $10.87$ BLEU and $8.62$ BLEU on\nthe WeTS and the WMT 2022 Translation Suggestion datasets, respectively, and\nreduces decoding time overhead by an average of 63.4% tested on the WMT\ntranslation datasets. Furthermore, on both of the TS benchmark datasets, it is\nsuperior to other supervised learning systems trained with TS annotated data.", "published": "2022-11-14 03:40:02", "link": "http://arxiv.org/abs/2211.07093v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discharge Summary Hospital Course Summarisation of In Patient Electronic\n  Health Record Text with Clinical Concept Guided Deep Pre-Trained Transformer\n  Models", "abstract": "Brief Hospital Course (BHC) summaries are succinct summaries of an entire\nhospital encounter, embedded within discharge summaries, written by senior\nclinicians responsible for the overall care of a patient. Methods to\nautomatically produce summaries from inpatient documentation would be\ninvaluable in reducing clinician manual burden of summarising documents under\nhigh time-pressure to admit and discharge patients. Automatically producing\nthese summaries from the inpatient course, is a complex, multi-document\nsummarisation task, as source notes are written from various perspectives (e.g.\nnursing, doctor, radiology), during the course of the hospitalisation. We\ndemonstrate a range of methods for BHC summarisation demonstrating the\nperformance of deep learning summarisation models across extractive and\nabstractive summarisation scenarios. We also test a novel ensemble extractive\nand abstractive summarisation model that incorporates a medical concept\nontology (SNOMED) as a clinical guidance signal and shows superior performance\nin 2 real-world clinical data sets.", "published": "2022-11-14 05:39:45", "link": "http://arxiv.org/abs/2211.07126v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Omission in Dialogue Summarization", "abstract": "Dialogue summarization aims to condense the lengthy dialogue into a concise\nsummary, and has recently achieved significant progress. However, the result of\nexisting methods is still far from satisfactory. Previous works indicated that\nomission is a major factor in affecting the quality of summarization, but few\nof them have further explored the omission problem, such as how omission\naffects summarization results and how to detect omission, which is critical for\nreducing omission and improving summarization quality. Moreover, analyzing and\ndetecting omission relies on summarization datasets with omission labels (i.e.,\nwhich dialogue utterances are omitted in the summarization), which are not\navailable in the current literature. In this paper, we propose the OLDS\ndataset, which provides high-quality Omission Labels for Dialogue\nSummarization. By analyzing this dataset, we find that a large improvement in\nsummarization quality can be achieved by providing ground-truth omission labels\nfor the summarization model to recover omission information, which demonstrates\nthe importance of omission detection for omission mitigation in dialogue\nsummarization. Therefore, we formulate an omission detection task and\ndemonstrate our proposed dataset can support the training and evaluation of\nthis task well. We also call for research action on omission detection based on\nour proposed datasets. Our dataset and codes are publicly available.", "published": "2022-11-14 06:56:59", "link": "http://arxiv.org/abs/2211.07145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text\n  Generation via Concentrating Attention", "abstract": "Recently, powerful Transformer architectures have proven superior in\ngenerating high-quality sentences. Nevertheless, these models tend to produce\ndull high-frequency phrases, severely hurting the diversity and novelty of\ngenerated text. In this work, we dig into the intrinsic mechanism of this\nproblem and found that sparser attention values in Transformer could improve\ndiversity. To understand such a phenomenon, we first conduct both empirical and\ntheoretical analysis and then attribute it to representation degeneration\ncaused by the attentive mixture of the hidden states during training. We term\nthis process the Trap of Mediocrity. To escape from such a trap, we introduce a\nnovel attention regularization loss to control the sharpness of the attention\ndistribution, which is transparent to model structures and can be easily\nimplemented within 20 lines of python code. We prove that this method could be\nmathematically regarded as learning a Bayesian approximation of posterior\nattention. Experiments show that our method improved the diversity and novelty\nof the generated text while maintaining comparable quality on a variety of\nconditional and unconditional generation tasks.", "published": "2022-11-14 07:53:16", "link": "http://arxiv.org/abs/2211.07164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Adversarial Training with Robust Early-Bird Tickets", "abstract": "Adversarial training is one of the most powerful methods to improve the\nrobustness of pre-trained language models (PLMs). However, this approach is\ntypically more expensive than traditional fine-tuning because of the necessity\nto generate adversarial examples via gradient descent. Delving into the\noptimization process of adversarial training, we find that robust connectivity\npatterns emerge in the early training phase (typically $0.15\\sim0.3$ epochs),\nfar before parameters converge. Inspired by this finding, we dig out robust\nearly-bird tickets (i.e., subnetworks) to develop an efficient adversarial\ntraining method: (1) searching for robust tickets with structured sparsity in\nthe early stage; (2) fine-tuning robust tickets in the remaining time. To\nextract the robust tickets as early as possible, we design a ticket convergence\nmetric to automatically terminate the searching process. Experiments show that\nthe proposed efficient adversarial training method can achieve up to $7\\times\n\\sim 13 \\times$ training speedups while maintaining comparable or even better\nrobustness compared to the most competitive state-of-the-art adversarial\ntraining methods.", "published": "2022-11-14 10:44:25", "link": "http://arxiv.org/abs/2211.07263v3", "categories": ["cs.CL", "68-06", "I.2"], "primary_category": "cs.CL"}
{"title": "MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference,\n  Temporal, Causal, and Subevent Relation Extraction", "abstract": "The diverse relationships among real-world events, including coreference,\ntemporal, causal, and subevent relations, are fundamental to understanding\nnatural languages. However, two drawbacks of existing datasets limit event\nrelation extraction (ERE) tasks: (1) Small scale. Due to the annotation\ncomplexity, the data scale of existing datasets is limited, which cannot well\ntrain and evaluate data-hungry models. (2) Absence of unified annotation.\nDifferent types of event relations naturally interact with each other, but\nexisting datasets only cover limited relation types at once, which prevents\nmodels from taking full advantage of relation interactions. To address these\nissues, we construct a unified large-scale human-annotated ERE dataset\nMAVEN-ERE with improved annotation schemes. It contains 103,193 event\ncoreference chains, 1,216,217 temporal relations, 57,992 causal relations, and\n15,841 subevent relations, which is larger than existing datasets of all the\nERE tasks by at least an order of magnitude. Experiments show that ERE on\nMAVEN-ERE is quite challenging, and considering relation interactions with\njoint learning can improve performances. The dataset and source codes can be\nobtained from https://github.com/THU-KEG/MAVEN-ERE.", "published": "2022-11-14 13:34:49", "link": "http://arxiv.org/abs/2211.07342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Parsing as Tagging", "abstract": "There have been many proposals to reduce constituency parsing to tagging in\nthe literature. To better understand what these approaches have in common, we\ncast several existing proposals into a unifying pipeline consisting of three\nsteps: linearization, learning, and decoding. In particular, we show how to\nreduce tetratagging, a state-of-the-art constituency tagger, to shift--reduce\nparsing by performing a right-corner transformation on the grammar and making a\nspecific independence assumption. Furthermore, we empirically evaluate our\ntaxonomy of tagging pipelines with different choices of linearizers, learners,\nand decoders. Based on the results in English and a set of 8 typologically\ndiverse languages, we conclude that the linearization of the derivation tree\nand its alignment with the input sequence is the most critical factor in\nachieving accurate taggers.", "published": "2022-11-14 13:37:07", "link": "http://arxiv.org/abs/2211.07344v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Debiasing Inevitably Degrade the Model Performance", "abstract": "Gender bias in language models has attracted sufficient attention because it\nthreatens social justice. However, most of the current debiasing methods\ndegraded the model's performance on other tasks while the degradation mechanism\nis still mysterious. We propose a theoretical framework explaining the three\ncandidate mechanisms of the language model's gender bias. We use our\ntheoretical framework to explain why the current debiasing methods cause\nperformance degradation. We also discover a pathway through which debiasing\nwill not degrade the model performance. We further develop a\ncausality-detection fine-tuning approach to correct gender bias. The numerical\nexperiment demonstrates that our method is able to lead to double dividends:\npartially mitigating gender bias while avoiding performance degradation.", "published": "2022-11-14 13:46:13", "link": "http://arxiv.org/abs/2211.07350v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrated Interpretation: Confidence Estimation in Semantic Parsing", "abstract": "Sequence generation models are increasingly being used to translate natural\nlanguage into programs, i.e. to perform executable semantic parsing. The fact\nthat semantic parsing aims to predict programs that can lead to executed\nactions in the real world motivates developing safe systems. This in turn makes\nmeasuring calibration -- a central component to safety -- particularly\nimportant. We investigate the calibration of popular generation models across\nfour popular semantic parsing datasets, finding that it varies across models\nand datasets. We then analyze factors associated with calibration error and\nrelease new confidence-based challenge splits of two parsing datasets. To\nfacilitate the inclusion of calibration in semantic parsing evaluations, we\nrelease a library for computing calibration metrics.", "published": "2022-11-14 15:17:55", "link": "http://arxiv.org/abs/2211.07443v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust Numerical Question Answering: Diagnosing Numerical\n  Capabilities of NLP Systems", "abstract": "Numerical Question Answering is the task of answering questions that require\nnumerical capabilities. Previous works introduce general adversarial attacks to\nNumerical Question Answering, while not systematically exploring numerical\ncapabilities specific to the topic. In this paper, we propose to conduct\nnumerical capability diagnosis on a series of Numerical Question Answering\nsystems and datasets. A series of numerical capabilities are highlighted, and\ncorresponding dataset perturbations are designed. Empirical results indicate\nthat existing systems are severely challenged by these perturbations. E.g.,\nGraph2Tree experienced a 53.83% absolute accuracy drop against the ``Extra''\nperturbation on ASDiv-a, and BART experienced 13.80% accuracy drop against the\n``Language'' perturbation on the numerical subset of DROP. As a counteracting\napproach, we also investigate the effectiveness of applying perturbations as\ndata augmentation to relieve systems' lack of robust numerical capabilities.\nWith experiment analysis and empirical studies, it is demonstrated that\nNumerical Question Answering with robust numerical capabilities is still to a\nlarge extent an open question. We discuss future directions of Numerical\nQuestion Answering and summarize guidelines on future dataset collection and\nsystem design.", "published": "2022-11-14 15:32:12", "link": "http://arxiv.org/abs/2211.07455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the Covid-19 MLIA Machine Translation Task", "abstract": "This work presents the results of the machine translation (MT) task from the\nCovid-19 MLIA @ Eval initiative, a community effort to improve the generation\nof MT systems focused on the current Covid-19 crisis. Nine teams took part in\nthis event, which was divided in two rounds and involved seven different\nlanguage pairs. Two different scenarios were considered: one in which only the\nprovided data was allowed, and a second one in which the use of external\nresources was allowed. Overall, best approaches were based on multilingual\nmodels and transfer learning, with an emphasis on the importance of applying a\ncleaning process to the training data.", "published": "2022-11-14 15:47:53", "link": "http://arxiv.org/abs/2211.07465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cracking Double-Blind Review: Authorship Attribution with Deep Learning", "abstract": "Double-blind peer review is considered a pillar of academic research because\nit is perceived to ensure a fair, unbiased, and fact-centered scientific\ndiscussion. Yet, experienced researchers can often correctly guess from which\nresearch group an anonymous submission originates, biasing the peer-review\nprocess. In this work, we present a transformer-based, neural-network\narchitecture that only uses the text content and the author names in the\nbibliography to attribute an anonymous manuscript to an author. To train and\nevaluate our method, we created the largest authorship identification dataset\nto date. It leverages all research papers publicly available on arXiv amounting\nto over 2 million manuscripts. In arXiv-subsets with up to 2,000 different\nauthors, our method achieves an unprecedented authorship attribution accuracy,\nwhere up to 73% of papers are attributed correctly. We present a scaling\nanalysis to highlight the applicability of the proposed method to even larger\ndatasets when sufficient compute capabilities are more widely available to the\nacademic community. Furthermore, we analyze the attribution accuracy in\nsettings where the goal is to identify all authors of an anonymous manuscript.\nThanks to our method, we are not only able to predict the author of an\nanonymous work, but we also provide empirical evidence of the key aspects that\nmake a paper attributable. We have open-sourced the necessary tools to\nreproduce our experiments.", "published": "2022-11-14 15:50:24", "link": "http://arxiv.org/abs/2211.07467v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CST5: Data Augmentation for Code-Switched Semantic Parsing", "abstract": "Extending semantic parsers to code-switched input has been a challenging\nproblem, primarily due to a lack of supervised training data. In this work, we\nintroduce CST5, a new data augmentation technique that finetunes a T5 model\nusing a small seed set ($\\approx$100 utterances) to generate code-switched\nutterances from English utterances. We show that CST5 generates high quality\ncode-switched data, both intrinsically (per human evaluation) and extrinsically\nby comparing baseline models which are trained without data augmentation to\nmodels which are trained with augmented data. Empirically we observe that using\nCST5, one can achieve the same semantic parsing performance by using up to 20x\nless labeled data. To aid further research in this area, we are also releasing\n(a) Hinglish-TOP, the largest human annotated code-switched semantic parsing\ndataset to date, containing 10k human annotated Hindi-English (Hinglish)\ncode-switched utterances, and (b) Over 170K CST5 generated code-switched\nutterances from the TOPv2 dataset. Human evaluation shows that both the human\nannotated data as well as the CST5 generated data is of good quality.", "published": "2022-11-14 16:45:30", "link": "http://arxiv.org/abs/2211.07514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous\n  Questions in VQA", "abstract": "Natural language is ambiguous. Resolving ambiguous questions is key to\nsuccessfully answering them. Focusing on questions about images, we create a\ndataset of ambiguous examples. We annotate these, grouping answers by the\nunderlying question they address and rephrasing the question for each group to\nreduce ambiguity. Our analysis reveals a linguistically-aligned ontology of\nreasons for ambiguity in visual questions. We then develop an English\nquestion-generation model which we demonstrate via automatic and human\nevaluation produces less ambiguous questions. We further show that the question\ngeneration objective we use allows the model to integrate answer group\ninformation without any direct supervision.", "published": "2022-11-14 16:45:42", "link": "http://arxiv.org/abs/2211.07516v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Answer Multilingual and Code-Mixed Questions", "abstract": "Question-answering (QA) that comes naturally to humans is a critical\ncomponent in seamless human-computer interaction. It has emerged as one of the\nmost convenient and natural methods to interact with the web and is especially\ndesirable in voice-controlled environments. Despite being one of the oldest\nresearch areas, the current QA system faces the critical challenge of handling\nmultilingual queries. To build an Artificial Intelligent (AI) agent that can\nserve multilingual end users, a QA system is required to be language versatile\nand tailored to suit the multilingual environment. Recent advances in QA models\nhave enabled surpassing human performance primarily due to the availability of\na sizable amount of high-quality datasets. However, the majority of such\nannotated datasets are expensive to create and are only confined to the English\nlanguage, making it challenging to acknowledge progress in foreign languages.\nTherefore, to measure a similar improvement in the multilingual QA system, it\nis necessary to invest in high-quality multilingual evaluation benchmarks. In\nthis dissertation, we focus on advancing QA techniques for handling end-user\nqueries in multilingual environments. This dissertation consists of two parts.\nIn the first part, we explore multilingualism and a new dimension of\nmultilingualism referred to as code-mixing. Second, we propose a technique to\nsolve the task of multi-hop question generation by exploiting multiple\ndocuments. Experiments show our models achieve state-of-the-art performance on\nanswer extraction, ranking, and generation tasks on multiple domains of MQA,\nVQA, and language generation. The proposed techniques are generic and can be\nwidely used in various domains and languages to advance QA systems.", "published": "2022-11-14 16:49:58", "link": "http://arxiv.org/abs/2211.07522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "High-Resource Methodological Bias in Low-Resource Investigations", "abstract": "The central bottleneck for low-resource NLP is typically regarded to be the\nquantity of accessible data, overlooking the contribution of data quality. This\nis particularly seen in the development and evaluation of low-resource systems\nvia down sampling of high-resource language data. In this work we investigate\nthe validity of this approach, and we specifically focus on two well-known NLP\ntasks for our empirical investigations: POS-tagging and machine translation. We\nshow that down sampling from a high-resource language results in datasets with\ndifferent properties than the low-resource datasets, impacting the model\nperformance for both POS-tagging and machine translation. Based on these\nresults we conclude that naive down sampling of datasets results in a biased\nview of how well these systems work in a low-resource scenario.", "published": "2022-11-14 17:04:38", "link": "http://arxiv.org/abs/2211.07534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Imagination is All You Need! Curved Contrastive Learning for Abstract\n  Sequence Modeling Utilized on Long Short-Term Dialogue Planning", "abstract": "Inspired by the curvature of space-time (Einstein, 1921), we introduce Curved\nContrastive Learning (CCL), a novel representation learning technique for\nlearning the relative turn distance between utterance pairs in multi-turn\ndialogues. The resulting bi-encoder models can guide transformers as a response\nranking model towards a goal in a zero-shot fashion by projecting the goal\nutterance and the corresponding reply candidates into a latent space. Here the\ncosine similarity indicates the distance/reachability of a candidate utterance\ntoward the corresponding goal. Furthermore, we explore how these\nforward-entailing language representations can be utilized for assessing the\nlikelihood of sequences by the entailment strength i.e. through the cosine\nsimilarity of its individual members (encoded separately) as an emergent\nproperty in the curved space. These non-local properties allow us to imagine\nthe likelihood of future patterns in dialogues, specifically by\nordering/identifying future goal utterances that are multiple turns away, given\na dialogue context. As part of our analysis, we investigate characteristics\nthat make conversations (un)plannable and find strong evidence of planning\ncapability over multiple turns (in 61.56% over 3 turns) in conversations from\nthe DailyDialog (Li et al., 2017) dataset. Finally, we show how we achieve\nhigher efficiency in sequence modeling tasks compared to previous work thanks\nto our relativistic approach, where only the last utterance needs to be encoded\nand computed during inference.", "published": "2022-11-14 18:16:48", "link": "http://arxiv.org/abs/2211.07591v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UGIF: UI Grounded Instruction Following", "abstract": "Smartphone users often find it difficult to navigate myriad menus to perform\ncommon tasks such as \"How to block calls from unknown numbers?\". Currently,\nhelp documents with step-by-step instructions are manually written to aid the\nuser. The user experience can be further enhanced by grounding the instructions\nin the help document to the UI and overlaying a tutorial on the phone UI. To\nbuild such tutorials, several natural language processing components including\nretrieval, parsing, and grounding are necessary, but there isn't any relevant\ndataset for such a task. Thus, we introduce UGIF-DataSet, a multi-lingual,\nmulti-modal UI grounded dataset for step-by-step task completion on the\nsmartphone containing 4,184 tasks across 8 languages. As an initial approach to\nthis problem, we propose retrieving the relevant instruction steps based on the\nuser's query and parsing the steps using Large Language Models (LLMs) to\ngenerate macros that can be executed on-device. The instruction steps are often\navailable only in English, so the challenge includes cross-modal, cross-lingual\nretrieval of English how-to pages from user queries in many languages and\nmapping English instruction steps to UI in a potentially different language. We\ncompare the performance of different LLMs including PaLM and GPT-3 and find\nthat the end-to-end task completion rate is 48% for English UI but the\nperformance drops to 32% for other languages. We analyze the common failure\nmodes of existing models on this task and point out areas for improvement.", "published": "2022-11-14 18:36:19", "link": "http://arxiv.org/abs/2211.07615v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Similarity Models for Depression Severity Estimation", "abstract": "Depressive disorders constitute a severe public health issue worldwide.\nHowever, public health systems have limited capacity for case detection and\ndiagnosis. In this regard, the widespread use of social media has opened up a\nway to access public information on a large scale. Computational methods can\nserve as support tools for rapid screening by exploiting this user-generated\nsocial media content. This paper presents an efficient semantic pipeline to\nstudy depression severity in individuals based on their social media writings.\nWe select test user sentences for producing semantic rankings over an index of\nrepresentative training sentences corresponding to depressive symptoms and\nseverity levels. Then, we use the sentences from those results as evidence for\npredicting users' symptom severity. For that, we explore different aggregation\nmethods to answer one of four Beck Depression Inventory (BDI) options per\nsymptom. We evaluate our methods on two Reddit-based benchmarks, achieving 30\\%\nimprovement over state of the art in terms of measuring depression severity.", "published": "2022-11-14 18:47:26", "link": "http://arxiv.org/abs/2211.07624v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Agnostic Code-Mixing Data Augmentation by Predicting Linguistic\n  Patterns", "abstract": "In this work, we focus on intrasentential code-mixing and propose several\ndifferent Synthetic Code-Mixing (SCM) data augmentation methods that outperform\nthe baseline on downstream sentiment analysis tasks across various amounts of\nlabeled gold data. Most importantly, our proposed methods demonstrate that\nstrategically replacing parts of sentences in the matrix language with a\nconstant mask significantly improves classification accuracy, motivating\nfurther linguistic insights into the phenomenon of code-mixing. We test our\ndata augmentation method in a variety of low-resource and cross-lingual\nsettings, reaching up to a relative improvement of 7.73% on the extremely\nscarce English-Malayalam dataset. We conclude that the code-switch pattern in\ncode-mixing sentences is also important for the model to learn. Finally, we\npropose a language-agnostic SCM algorithm that is cheap yet extremely helpful\nfor low-resource languages.", "published": "2022-11-14 18:50:16", "link": "http://arxiv.org/abs/2211.07628v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speaking Multiple Languages Affects the Moral Bias of Language Models", "abstract": "Pre-trained multilingual language models (PMLMs) are commonly used when\ndealing with data from multiple languages and cross-lingual transfer. However,\nPMLMs are trained on varying amounts of data for each language. In practice\nthis means their performance is often much better on English than many other\nlanguages. We explore to what extent this also applies to moral norms. Do the\nmodels capture moral norms from English and impose them on other languages? Do\nthe models exhibit random and thus potentially harmful beliefs in certain\nlanguages? Both these issues could negatively impact cross-lingual transfer and\npotentially lead to harmful outcomes. In this paper, we (1) apply the\nMoralDirection framework to multilingual models, comparing results in German,\nCzech, Arabic, Chinese, and English, (2) analyse model behaviour on filtered\nparallel subtitles corpora, and (3) apply the models to a Moral Foundations\nQuestionnaire, comparing with human responses from different countries. Our\nexperiments demonstrate that, indeed, PMLMs encode differing moral biases, but\nthese do not necessarily correspond to cultural differences or commonalities in\nhuman opinions. We release our code and models.", "published": "2022-11-14 20:08:54", "link": "http://arxiv.org/abs/2211.07733v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Aspect-Based Sentiment Analysis with Contrastive Learning and\n  Expressive Structure", "abstract": "Generative models have demonstrated impressive results on Aspect-based\nSentiment Analysis (ABSA) tasks, particularly for the emerging task of\nextracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these\nmodels struggle with implicit sentiment expressions, which are commonly\nobserved in opinionated content such as online reviews. In this work, we\nintroduce GEN-SCL-NAT, which consists of two techniques for improved structured\ngeneration for ACOS quadruple extraction. First, we propose GEN-SCL, a\nsupervised contrastive learning objective that aids quadruple prediction by\nencouraging the model to produce input representations that are discriminable\nacross key input attributes, such as sentiment polarity and the existence of\nimplicit opinions and aspects. Second, we introduce GEN-NAT, a new structured\ngeneration format that better adapts autoregressive encoder-decoder models to\nextract quadruples in a generative fashion. Experimental results show that\nGEN-SCL-NAT achieves top performance across three ACOS datasets, averaging\n1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset.\nAdditionally, we see significant gains on implicit aspect and opinion splits\nthat have been shown as challenging for existing ACOS approaches.", "published": "2022-11-14 20:47:02", "link": "http://arxiv.org/abs/2211.07743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALBERT with Knowledge Graph Encoder Utilizing Semantic Similarity for\n  Commonsense Question Answering", "abstract": "Recently, pre-trained language representation models such as bidirectional\nencoder representations from transformers (BERT) have been performing well in\ncommonsense question answering (CSQA). However, there is a problem that the\nmodels do not directly use explicit information of knowledge sources existing\noutside. To augment this, additional methods such as knowledge-aware graph\nnetwork (KagNet) and multi-hop graph relation network (MHGRN) have been\nproposed. In this study, we propose to use the latest pre-trained language\nmodel a lite bidirectional encoder representations from transformers (ALBERT)\nwith knowledge graph information extraction technique. We also propose to\napplying the novel method, schema graph expansion to recent language models.\nThen, we analyze the effect of applying knowledge graph-based knowledge\nextraction techniques to recent pre-trained language models and confirm that\nschema graph expansion is effective in some extent. Furthermore, we show that\nour proposed model can achieve better performance than existing KagNet and\nMHGRN models in CommonsenseQA dataset.", "published": "2022-11-14 01:39:26", "link": "http://arxiv.org/abs/2211.07065v1", "categories": ["cs.CL", "cs.AI", "68T07 (Secondary), 68T30, 68T50 (Primary)", "I.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Sentiment recognition of Italian elderly through domain adaptation on\n  cross-corpus speech dataset", "abstract": "The aim of this work is to define a speech emotion recognition (SER) model\nable to recognize positive, neutral and negative emotions in natural\nconversations of Italian elderly people. Several datasets for SER are available\nin the literature. However most of them are in English or Chinese, have been\nrecorded while actors and actresses pronounce short phrases and thus are not\nrelated to natural conversation. Moreover only few speeches among all the\ndatabases are related to elderly people. Therefore, in this work, a\nmulti-language and multi-age corpus is considered merging a dataset in English,\nthat includes also elderly people, with a dataset in Italian. A general model,\ntrained on young and adult English actors and actresses is proposed, based on\nXGBoost. Then two strategies of domain adaptation are proposed to adapt the\nmodel either to elderly people and to Italian speakers. The results suggest\nthat this approach increases the classification performance, underlining also\nthat new datasets should be collected.", "published": "2022-11-14 12:39:41", "link": "http://arxiv.org/abs/2211.07307v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Replacing Language Model for Style Transfer", "abstract": "We introduce replacing language model (RLM), a sequence-to-sequence language\nmodeling framework for text style transfer (TST). Our method autoregressively\nreplaces each token of the source sentence with a text span that has a similar\nmeaning but in the target style. The new span is generated via a\nnon-autoregressive masked language model, which can better preserve the\nlocal-contextual meaning of the replaced token. This RLM generation scheme\ngathers the flexibility of autoregressive models and the accuracy of\nnon-autoregressive models, which bridges the gap between sentence-level and\nword-level style transfer methods. To control the generation style more\nprecisely, we conduct a token-level style-content disentanglement on the hidden\nrepresentations of RLM. Empirical results on real-world text datasets\ndemonstrate the effectiveness of RLM compared with other TST baselines. The\ncode is at https://github.com/Linear95/RLM.", "published": "2022-11-14 13:35:55", "link": "http://arxiv.org/abs/2211.07343v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models", "abstract": "Transformer-based pre-trained language models have demonstrated superior\nperformance on various natural language processing tasks. However, it remains\nunclear how the skills required to handle these tasks distribute among model\nparameters. In this paper, we find that after prompt tuning for specific tasks,\nthe activations of some neurons within pre-trained Transformers are highly\npredictive of the task labels. We dub these neurons skill neurons and confirm\nthey encode task-specific skills by finding that: (1) Skill neurons are crucial\nfor handling tasks. Performances of pre-trained Transformers on a task\nsignificantly drop when corresponding skill neurons are perturbed. (2) Skill\nneurons are task-specific. Similar tasks tend to have similar distributions of\nskill neurons. Furthermore, we demonstrate the skill neurons are most likely\ngenerated in pre-training rather than fine-tuning by showing that the skill\nneurons found with prompt tuning are also crucial for other fine-tuning methods\nfreezing neuron weights, such as the adapter-based tuning and BitFit. We also\nexplore the applications of skill neurons, including accelerating Transformers\nwith network pruning and building better transferability indicators. These\nfindings may promote further research on understanding Transformers. The source\ncode can be obtained from https://github.com/THU-KEG/Skill-Neuron.", "published": "2022-11-14 13:43:46", "link": "http://arxiv.org/abs/2211.07349v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot\n  Domain Adaptation of KeyBERT", "abstract": "Keyword extraction has been an important topic for modern natural language\nprocessing. With its applications ranging from ontology generation, fact\nverification in summarized text, and recommendation systems. While it has had\nsignificant data-intensive applications, it is often hampered when the data set\nis small. Downstream training for keyword extractors is a lengthy process and\nrequires a significant amount of data. Recently, Few-shot Learning (FSL) and\nZero-Shot Learning (ZSL) have been proposed to tackle this problem. Therefore,\nwe propose AdaptKeyBERT, a pipeline for training keyword extractors with LLM\nbases by incorporating the concept of regularized attention into a pre-training\nphase for downstream domain adaptation. As we believe our work has implications\nto be utilized in the pipeline of FSL/ZSL and keyword extraction, we\nopen-source our code as well as provide the fine-tuning library of the same\nname AdaptKeyBERT at https://github.com/AmanPriyanshu/AdaptKeyBERT.", "published": "2022-11-14 16:29:03", "link": "http://arxiv.org/abs/2211.07499v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Hard Examples also Harder to Explain? A Study with Human and\n  Model-Generated Explanations", "abstract": "Recent work on explainable NLP has shown that few-shot prompting can enable\nlarge pretrained language models (LLMs) to generate grammatical and factual\nnatural language explanations for data labels. In this work, we study the\nconnection between explainability and sample hardness by investigating the\nfollowing research question - \"Are LLMs and humans equally good at explaining\ndata labels for both easy and hard samples?\" We answer this question by first\ncollecting human-written explanations in the form of generalizable commonsense\nrules on the task of Winograd Schema Challenge (Winogrande dataset). We compare\nthese explanations with those generated by GPT-3 while varying the hardness of\nthe test samples as well as the in-context samples. We observe that (1) GPT-3\nexplanations are as grammatical as human explanations regardless of the\nhardness of the test samples, (2) for easy examples, GPT-3 generates highly\nsupportive explanations but human explanations are more generalizable, and (3)\nfor hard examples, human explanations are significantly better than GPT-3\nexplanations both in terms of label-supportiveness and generalizability\njudgements. We also find that hardness of the in-context examples impacts the\nquality of GPT-3 explanations. Finally, we show that the supportiveness and\ngeneralizability aspects of human explanations are also impacted by sample\nhardness, although by a much smaller margin than models. Supporting code and\ndata are available at https://github.com/swarnaHub/ExplanationHardness", "published": "2022-11-14 16:46:14", "link": "http://arxiv.org/abs/2211.07517v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards a Mathematics Formalisation Assistant using Large Language\n  Models", "abstract": "Mathematics formalisation is the task of writing mathematics (i.e.,\ndefinitions, theorem statements, proofs) in natural language, as found in books\nand papers, into a formal language that can then be checked for correctness by\na program. It is a thriving activity today, however formalisation remains\ncumbersome. In this paper, we explore the abilities of a large language model\n(Codex) to help with formalisation in the Lean theorem prover. We find that\nwith careful input-dependent prompt selection and postprocessing, Codex is able\nto formalise short mathematical statements at undergrad level with nearly 75\\%\naccuracy for $120$ theorem statements. For proofs quantitative analysis is\ninfeasible and we undertake a detailed case study. We choose a diverse set of\n$13$ theorems at undergrad level with proofs that fit in two-three paragraphs.\nWe show that with a new prompting strategy Codex can formalise these proofs in\nnatural language with at least one out of twelve Codex completion being easy to\nrepair into a complete proof. This is surprising as essentially no aligned data\nexists for formalised mathematics, particularly for proofs. These results\nsuggest that large language models are a promising avenue towards fully or\npartially automating formalisation.", "published": "2022-11-14 16:52:32", "link": "http://arxiv.org/abs/2211.07524v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Abstractive Timeline Summarisation using Preference-based\n  Reinforcement Learning", "abstract": "This paper introduces a novel pipeline for summarising timelines of events\nreported by multiple news sources. Transformer-based models for abstractive\nsummarisation generate coherent and concise summaries of long documents but can\nfail to outperform established extractive methods on specialised tasks such as\ntimeline summarisation (TLS). While extractive summaries are more faithful to\ntheir sources, they may be less readable and contain redundant or unnecessary\ninformation. This paper proposes a preference-based reinforcement learning\n(PBRL) method for adapting pretrained abstractive summarisers to TLS, which can\novercome the drawbacks of extractive timeline summaries. We define a compound\nreward function that learns from keywords of interest and pairwise preference\nlabels, which we use to fine-tune a pretrained abstractive summariser via\noffline reinforcement learning. We carry out both automated and human\nevaluation on three datasets, finding that our method outperforms a comparable\nextractive TLS method on two of the three benchmark datasets, and participants\nprefer our method's summaries to those of both the extractive TLS method and\nthe pretrained abstractive model. The method does not require expensive\nreference summaries and needs only a small number of preferences to align the\ngenerated summaries with human preferences.", "published": "2022-11-14 18:24:13", "link": "http://arxiv.org/abs/2211.07596v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Follow the Wisdom of the Crowd: Effective Text Generation via Minimum\n  Bayes Risk Decoding", "abstract": "In open-ended natural-language generation, existing text decoding methods\ntypically struggle to produce text which is both diverse and high-quality.\nGreedy and beam search are known to suffer from text degeneration and\nlinguistic diversity issues, while temperature, top-k, and nucleus sampling\noften yield diverse but low-quality outputs. In this work, we present crowd\nsampling, a family of decoding methods based on Bayesian risk minimization, to\naddress this diversity-quality trade-off. Inspired by the principle of \"the\nwisdom of the crowd,\" crowd sampling seeks to select a candidate from a pool of\ncandidates that has the least expected risk (i.e., highest expected reward)\nunder a generative model according to a given utility function. Crowd sampling\ncan be seen as a generalization of numerous existing methods, including\nmajority voting, and in practice, it can be used as a drop-in replacement for\nexisting sampling methods. Extensive experiments show that crowd sampling\ndelivers improvements of 3-7 ROUGE and BLEU points across a wide range of\ntasks, including summarization, data-to-text, translation, and textual style\ntransfer, while achieving new state-of-the-art results on WebNLG and WMT'16.", "published": "2022-11-14 18:57:37", "link": "http://arxiv.org/abs/2211.07634v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Technological taxonomies for hypernym and hyponym retrieval in patent\n  texts", "abstract": "This paper presents an automatic approach to creating taxonomies of technical\nterms based on the Cooperative Patent Classification (CPC). The resulting\ntaxonomy contains about 170k nodes in 9 separate technological branches and is\nfreely available. We also show that a Text-to-Text Transfer Transformer (T5)\nmodel can be fine-tuned to generate hypernyms and hyponyms with relatively high\nprecision, confirming the manually assessed quality of the resource. The T5\nmodel opens the taxonomy to any new technological terms for which a hypernym\ncan be generated, thus making the resource updateable with new terms, an\nessential feature for the constantly evolving field of technological\nterminology.", "published": "2022-11-14 19:01:55", "link": "http://arxiv.org/abs/2212.06039v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Decomposition Improves Learning of Large Language Models on EHR\n  Data", "abstract": "Electronic health records (EHR) are widely believed to hold a profusion of\nactionable insights, encrypted in an irregular, semi-structured format, amidst\na loud noise background. To simplify learning patterns of health and disease,\nmedical codes in EHR can be decomposed into semantic units connected by\nhierarchical graphs. Building on earlier synergy between Bidirectional Encoder\nRepresentations from Transformers (BERT) and Graph Attention Networks (GAT), we\npresent H-BERT, which ingests complete graph tree expansions of hierarchical\nmedical codes as opposed to only ingesting the leaves and pushes patient-level\nlabels down to each visit. This methodology significantly improves prediction\nof patient membership in over 500 medical diagnosis classes as measured by\naggregated AUC and APS, and creates distinct representations of patients in\nclosely related but clinically distinct phenotypes.", "published": "2022-11-14 14:59:16", "link": "http://arxiv.org/abs/2212.06040v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPE: Symmetrical Prompt Enhancement for Fact Probing", "abstract": "Pretrained language models (PLMs) have been shown to accumulate factual\nknowledge during pretrainingng (Petroni et al., 2019). Recent works probe PLMs\nfor the extent of this knowledge through prompts either in discrete or\ncontinuous forms. However, these methods do not consider symmetry of the task:\nobject prediction and subject prediction. In this work, we propose Symmetrical\nPrompt Enhancement (SPE), a continuous prompt-based method for factual probing\nin PLMs that leverages the symmetry of the task by constructing symmetrical\nprompts for subject and object prediction. Our results on a popular factual\nprobing dataset, LAMA, show significant improvement of SPE over previous\nprobing methods.", "published": "2022-11-14 03:05:41", "link": "http://arxiv.org/abs/2211.07078v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Model Multimodal Semantic Alignment for Story Visualization", "abstract": "Story visualization aims to generate a sequence of images to narrate each\nsentence in a multi-sentence story, where the images should be realistic and\nkeep global consistency across dynamic scenes and characters. Current works\nface the problem of semantic misalignment because of their fixed architecture\nand diversity of input modalities. To address this problem, we explore the\nsemantic alignment between text and image representations by learning to match\ntheir semantic levels in the GAN-based generative model. More specifically, we\nintroduce dynamic interactions according to learning to dynamically explore\nvarious semantic depths and fuse the different-modal information at a matched\nsemantic level, which thus relieves the text-image semantic misalignment\nproblem. Extensive experiments on different datasets demonstrate the\nimprovements of our approach, neither using segmentation masks nor auxiliary\ncaptioning networks, on image quality and story consistency, compared with\nstate-of-the-art methods.", "published": "2022-11-14 11:41:44", "link": "http://arxiv.org/abs/2211.07289v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by\n  Integrating Multiple Targets", "abstract": "In this paper, we provide a new perspective on self-supervised speech models\nfrom how the training targets are obtained. We generalize the targets extractor\ninto Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE).\nBased on this, we propose a new multi-tasking learning framework for\nself-supervised learning, MT4SSL, which stands for Boosting Self-Supervised\nSpeech Representation Learning by Integrating Multiple Targets. MT4SSL uses the\nK-means algorithm as an Off-TE and a teacher network without gradients as an\nOn-TE, respectively. Our model outperforms previous SSL methods by nontrivial\nmargins on the LibriSpeech benchmark, and is comparable to or even better than\nthe best-performing models with fewer data. Furthermore, we find that using\nboth Off-TE and On-TE results in better convergence in the pre-training phase.\nWith both effectiveness and efficiency, we think doing multi-task learning on\nself-supervised speech models from our perspective is a promising trend.", "published": "2022-11-14 13:00:47", "link": "http://arxiv.org/abs/2211.07321v3", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-VQG: Generating Engaging Questions for Multiple Images", "abstract": "Generating engaging content has drawn much recent attention in the NLP\ncommunity. Asking questions is a natural way to respond to photos and promote\nawareness. However, most answers to questions in traditional question-answering\n(QA) datasets are factoids, which reduce individuals' willingness to answer.\nFurthermore, traditional visual question generation (VQG) confines the source\ndata for question generation to single images, resulting in a limited ability\nto comprehend time-series information of the underlying event. In this paper,\nwe propose generating engaging questions from multiple images. We present MVQG,\na new dataset, and establish a series of baselines, including both end-to-end\nand dual-stage architectures. Results show that building stories behind the\nimage sequence enables models to generate engaging questions, which confirms\nour assumption that people typically construct a picture of the event in their\nminds before asking questions. These results open up an exciting challenge for\nvisual-and-language models to implicitly construct a story behind a series of\nphotos to allow for creativity and experience sharing and hence draw attention\nto downstream applications.", "published": "2022-11-14 15:15:00", "link": "http://arxiv.org/abs/2211.07441v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EVA: Exploring the Limits of Masked Visual Representation Learning at\n  Scale", "abstract": "We launch EVA, a vision-centric foundation model to explore the limits of\nvisual representation at scale using only publicly accessible data. EVA is a\nvanilla ViT pre-trained to reconstruct the masked out image-text aligned vision\nfeatures conditioned on visible image patches. Via this pretext task, we can\nefficiently scale up EVA to one billion parameters, and sets new records on a\nbroad range of representative vision downstream tasks, such as image\nrecognition, video action recognition, object detection, instance segmentation\nand semantic segmentation without heavy supervised training. Moreover, we\nobserve quantitative changes in scaling EVA result in qualitative changes in\ntransfer learning performance that are not present in other models. For\ninstance, EVA takes a great leap in the challenging large vocabulary instance\nsegmentation task: our model achieves almost the same state-of-the-art\nperformance on LVISv1.0 dataset with over a thousand categories and COCO\ndataset with only eighty categories. Beyond a pure vision encoder, EVA can also\nserve as a vision-centric, multi-modal pivot to connect images and text. We\nfind initializing the vision tower of a giant CLIP from EVA can greatly\nstabilize the training and outperform the training from scratch counterpart\nwith much fewer samples and less compute, providing a new direction for scaling\nup and accelerating the costly training of multi-modal foundation models. To\nfacilitate future research, we release all the code and models at\nhttps://github.com/baaivision/EVA.", "published": "2022-11-14 18:59:52", "link": "http://arxiv.org/abs/2211.07636v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "QueryForm: A Simple Zero-shot Form Entity Query Framework", "abstract": "Zero-shot transfer learning for document understanding is a crucial yet\nunder-investigated scenario to help reduce the high cost involved in annotating\ndocument entities. We present a novel query-based framework, QueryForm, that\nextracts entity values from form-like documents in a zero-shot fashion.\nQueryForm contains a dual prompting mechanism that composes both the document\nschema and a specific entity type into a query, which is used to prompt a\nTransformer model to perform a single entity extraction task. Furthermore, we\npropose to leverage large-scale query-entity pairs generated from form-like\nwebpages with weak HTML annotations to pre-train QueryForm. By unifying\npre-training and fine-tuning into the same query-based framework, QueryForm\nenables models to learn from structured documents containing various entities\nand layouts, leading to better generalization to target document types without\nthe need for target-specific training data. QueryForm sets new state-of-the-art\naverage F1 score on both the XFUND (+4.6%~10.1%) and the Payment (+3.2%~9.5%)\nzero-shot benchmark, with a smaller model size and no additional image input.", "published": "2022-11-14 20:02:02", "link": "http://arxiv.org/abs/2211.07730v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Children's Speech Recognition by Fine-tuning Self-supervised\n  Adult Speech Representations", "abstract": "Children's speech recognition is a vital, yet largely overlooked domain when\nbuilding inclusive speech technologies. The major challenge impeding progress\nin this domain is the lack of adequate child speech corpora; however, recent\nadvances in self-supervised learning have created a new opportunity for\novercoming this problem of data scarcity. In this paper, we leverage\nself-supervised adult speech representations and use three well-known child\nspeech corpora to build models for children's speech recognition. We assess the\nperformance of fine-tuning on both native and non-native children's speech,\nexamine the effect of cross-domain child corpora, and investigate the minimum\namount of child speech required to fine-tune a model which outperforms a\nstate-of-the-art adult model. We also analyze speech recognition performance\nacross children's ages. Our results demonstrate that fine-tuning with\ncross-domain child corpora leads to relative improvements of up to 46.08% and\n45.53% for native and non-native child speech respectively, and absolute\nimprovements of 14.70% and 31.10%. We also show that with as little as 5 hours\nof transcribed children's speech, it is possible to fine-tune a children's\nspeech recognition system that outperforms a state-of-the-art adult model\nfine-tuned on 960 hours of adult speech.", "published": "2022-11-14 22:03:36", "link": "http://arxiv.org/abs/2211.07769v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach\n  for Speech Emotion Recognition", "abstract": "Speech emotion recognition (SER) plays a vital role in improving the\ninteractions between humans and machines by inferring human emotion and\naffective states from speech signals. Whereas recent works primarily focus on\nmining spatiotemporal information from hand-crafted features, we explore how to\nmodel the temporal patterns of speech emotions from dynamic temporal scales.\nTowards that goal, we introduce a novel temporal emotional modeling approach\nfor SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net),\nwhich learns multi-scale contextual affective representations from various time\nscales. Specifically, TIM-Net first employs temporal-aware blocks to learn\ntemporal affective representation, then integrates complementary information\nfrom the past and the future to enrich contextual representations, and finally,\nfuses multiple time scale features for better adaptation to the emotional\nvariation. Extensive experimental results on six benchmark SER datasets\ndemonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61%\nimprovements of the average UAR and WAR over the second-best on each corpus.\nThe source code is available at https://github.com/Jiaxin-Ye/TIM-Net_SER.", "published": "2022-11-14 13:35:01", "link": "http://arxiv.org/abs/2211.08233v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Knowledge Augmentation to Multi-tasking: Towards Human-like\n  Dialogue Systems", "abstract": "The goal of building dialogue agents that can converse with humans naturally\nhas been a long-standing dream of researchers since the early days of\nartificial intelligence. The well-known Turing Test proposed to judge the\nultimate validity of an artificial intelligence agent on the\nindistinguishability of its dialogues from humans'. It should come as no\nsurprise that human-level dialogue systems are very challenging to build. But,\nwhile early effort on rule-based systems found limited success, the emergence\nof deep learning enabled great advance on this topic.\n  In this thesis, we focus on methods that address the numerous issues that\nhave been imposing the gap between artificial conversational agents and\nhuman-level interlocutors. These methods were proposed and experimented with in\nways that were inspired by general state-of-the-art AI methodologies. But they\nalso targeted the characteristics that dialogue systems possess.", "published": "2022-11-14 17:27:07", "link": "http://arxiv.org/abs/2212.03279v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Hope Speech Detection on Social Media Platforms", "abstract": "Since personal computers became widely available in the consumer market, the\namount of harmful content on the internet has significantly expanded. In simple\nterms, harmful content is anything online which causes a person distress or\nharm. It may include hate speech, violent content, threats, non-hope speech,\netc. The online content must be positive, uplifting and supportive. Over the\npast few years, many studies have focused on solving this problem through hate\nspeech detection, but very few focused on identifying hope speech. This paper\ndiscusses various machine learning approaches to identify a sentence as Hope\nSpeech, Non-Hope Speech, or a Neutral sentence. The dataset used in the study\ncontains English YouTube comments and is released as a part of the shared task\n\"EACL-2021: Hope Speech Detection for Equality, Diversity, and Inclusion\".\nInitially, the dataset obtained from the shared task had three classes: Hope\nSpeech, non-Hope speech, and not in English; however, upon deeper inspection,\nwe discovered that dataset relabeling is required. A group of undergraduates\nwas hired to help perform the entire dataset's relabeling task. We experimented\nwith conventional machine learning models (such as Na\\\"ive Bayes, logistic\nregression and support vector machine) and pre-trained models (such as BERT) on\nrelabeled data. According to the experimental results, the relabeled data has\nachieved a better accuracy for Hope speech identification than the original\ndata set.", "published": "2022-11-14 10:58:22", "link": "http://arxiv.org/abs/2212.07424v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Analyzing the Role of Image for Visual-enhanced Relation Extraction", "abstract": "Multimodal relation extraction is an essential task for knowledge graph\nconstruction. In this paper, we take an in-depth empirical analysis that\nindicates the inaccurate information in the visual scene graph leads to poor\nmodal alignment weights, further degrading performance. Moreover, the visual\nshuffle experiments illustrate that the current approaches may not take full\nadvantage of visual information. Based on the above observation, we further\npropose a strong baseline with an implicit fine-grained multimodal alignment\nbased on Transformer for multimodal relation extraction. Experimental results\ndemonstrate the better performance of our method. Codes are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal.", "published": "2022-11-14 16:39:24", "link": "http://arxiv.org/abs/2211.07504v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards A Unified Conformer Structure: from ASR to ASV Task", "abstract": "Transformer has achieved extraordinary performance in Natural Language\nProcessing and Computer Vision tasks thanks to its powerful self-attention\nmechanism, and its variant Conformer has become a state-of-the-art architecture\nin the field of Automatic Speech Recognition (ASR). However, the main-stream\narchitecture for Automatic Speaker Verification (ASV) is convolutional Neural\nNetworks, and there is still much room for research on the Conformer based ASV.\nIn this paper, firstly, we modify the Conformer architecture from ASR to ASV\nwith very minor changes. Length-Scaled Attention (LSA) method and\nSharpness-Aware Minimizationis (SAM) are adopted to improve model\ngeneralization. Experiments conducted on VoxCeleb and CN-Celeb show that our\nConformer based ASV achieves competitive performance compared with the popular\nECAPA-TDNN. Secondly, inspired by the transfer learning strategy, ASV Conformer\nis natural to be initialized from the pretrained ASR model. Via parameter\ntransferring, self-attention mechanism could better focus on the relationship\nbetween sequence features, brings about 11% relative improvement in EER on test\nset of VoxCeleb and CN-Celeb, which reveals the potential of Conformer to unify\nASV and ASR task. Finally, we provide a runtime in ASV-Subtools to evaluate its\ninference speed in production scenario. Our code is released at\nhttps://github.com/Snowdar/asv-subtools/tree/master/doc/papers/conformer.md.", "published": "2022-11-14 08:44:53", "link": "http://arxiv.org/abs/2211.07201v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SNIPER Training: Single-Shot Sparse Training for Text-to-Speech", "abstract": "Text-to-speech (TTS) models have achieved remarkable naturalness in recent\nyears, yet like most deep neural models, they have more parameters than\nnecessary. Sparse TTS models can improve on dense models via pruning and extra\nretraining, or converge faster than dense models with some performance loss.\nThus, we propose training TTS models using decaying sparsity, i.e. a high\ninitial sparsity to accelerate training first, followed by a progressive rate\nreduction to obtain better eventual performance. This decremental approach\ndiffers from current methods of incrementing sparsity to a desired target,\nwhich costs significantly more time than dense training. We call our method\nSNIPER training: Single-shot Initialization Pruning Evolving-Rate training. Our\nexperiments on FastSpeech2 show that we were able to obtain better losses in\nthe first few training epochs with SNIPER, and that the final SNIPER-trained\nmodels outperformed constant-sparsity models and edged out dense models, with\nnegligible difference in training time.", "published": "2022-11-14 11:26:13", "link": "http://arxiv.org/abs/2211.07283v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Label Training for Text-Independent Speaker Identification", "abstract": "In this paper, we propose a novel strategy for text-independent speaker\nidentification system: Multi-Label Training (MLT). Instead of the commonly used\none-to-one correspondence between the speech and the speaker label, we divide\nall the speeches of each speaker into several subgroups, with each subgroup\nassigned a different set of labels. During the identification process, a\nspecific speaker is identified as long as the predicted label is the same as\none of his/her corresponding labels. We found that this method can force the\nmodel to distinguish the data more accurately, and somehow takes advantages of\nensemble learning, while avoiding the significant increase of computation and\nstorage burden. In the experiments, we found that not only in clean conditions,\nbut also in noisy conditions with speech enhancement, Multi-Label Training can\nstill achieve better identification performance than commom methods. It should\nbe noted that the proposed strategy can be easily applied to almost all current\ntext-independent speaker identification models to achieve further improvements.", "published": "2022-11-14 14:07:25", "link": "http://arxiv.org/abs/2211.07373v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Potential of Neural Speech Synthesis-based Data Augmentation for\n  Personalized Speech Enhancement", "abstract": "With the advances in deep learning, speech enhancement systems benefited from\nlarge neural network architectures and achieved state-of-the-art quality.\nHowever, speaker-agnostic methods are not always desirable, both in terms of\nquality and their complexity, when they are to be used in a\nresource-constrained environment. One promising way is personalized speech\nenhancement (PSE), which is a smaller and easier speech enhancement problem for\nsmall models to solve, because it focuses on a particular test-time user. To\nachieve the personalization goal, while dealing with the typical lack of\npersonal data, we investigate the effect of data augmentation based on neural\nspeech synthesis (NSS). In the proposed method, we show that the quality of the\nNSS system's synthetic data matters, and if they are good enough the augmented\ndataset can be used to improve the PSE system that outperforms the\nspeaker-agnostic baseline. The proposed PSE systems show significant complexity\nreduction while preserving the enhancement quality.", "published": "2022-11-14 16:20:41", "link": "http://arxiv.org/abs/2211.07493v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with\n  Emotion Annotations", "abstract": "Existing multi-instrumental datasets tend to be biased toward pop and\nclassical music. In addition, they generally lack high-level annotations such\nas emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game\nmusic dataset with multi-label emotion annotations. It includes 669 audio and\nMIDI files of music from Sega and MSX PC games in the 80s using YM2413, a\nprogrammable sound generator based on FM. The collected game music is arranged\nwith a subset of 15 monophonic instruments and one drum instrument. They were\nconverted from binary commands of the YM2413 sound chip. Each song was labeled\nwith 19 emotion tags by two annotators and validated by three verifiers to\nobtain refined tags. We provide the baseline models and results for emotion\nrecognition and emotion-conditioned symbolic music generation using YM2413-MDB.", "published": "2022-11-14 06:18:25", "link": "http://arxiv.org/abs/2211.07131v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "I.2.1; I.2.7"], "primary_category": "cs.SD"}
{"title": "Exploiting Device and Audio Data to Tag Music with User-Aware Listening\n  Contexts", "abstract": "As music has become more available especially on music streaming platforms,\npeople have started to have distinct preferences to fit to their varying\nlistening situations, also known as context. Hence, there has been a growing\ninterest in considering the user's situation when recommending music to users.\nPrevious works have proposed user-aware autotaggers to infer situation-related\ntags from music content and user's global listening preferences. However, in a\npractical music retrieval system, the autotagger could be only used by assuming\nthat the context class is explicitly provided by the user. In this work, for\ndesigning a fully automatised music retrieval system, we propose to\ndisambiguate the user's listening information from their stream data. Namely,\nwe propose a system which can generate a situational playlist for a user at a\ncertain time 1) by leveraging user-aware music autotaggers, and 2) by\nautomatically inferring the user's situation from stream data (e.g. device,\nnetwork) and user's general profile information (e.g. age). Experiments show\nthat such a context-aware personalized music retrieval system is feasible, but\nthe performance decreases in the case of new users, new tracks or when the\nnumber of context classes increases.", "published": "2022-11-14 10:08:12", "link": "http://arxiv.org/abs/2211.07250v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MedleyVox: An Evaluation Dataset for Multiple Singing Voices Separation", "abstract": "Separation of multiple singing voices into each voice is a rarely studied\narea in music source separation research. The absence of a benchmark dataset\nhas hindered its progress. In this paper, we present an evaluation dataset and\nprovide baseline studies for multiple singing voices separation. First, we\nintroduce MedleyVox, an evaluation dataset for multiple singing voices\nseparation. We specify the problem definition in this dataset by categorizing\nit into i) unison, ii) duet, iii) main vs. rest, and iv) N-singing separation.\nSecond, to overcome the absence of existing multi-singing datasets for a\ntraining purpose, we present a strategy for construction of multiple singing\nmixtures using various single-singing datasets. Third, we propose the improved\nsuper-resolution network (iSRNet), which greatly enhances initial estimates of\nseparation networks. Jointly trained with the Conv-TasNet and the multi-singing\nmixture construction strategy, the proposed iSRNet achieved comparable\nperformance to ideal time-frequency masks on duet and unison subsets of\nMedleyVox. Audio samples, the dataset, and codes are available on our website\n(https://github.com/jeonchangbin49/MedleyVox).", "published": "2022-11-14 12:27:35", "link": "http://arxiv.org/abs/2211.07302v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring the Impact of Noise and Degradations on Heart Sound\n  Classification Models", "abstract": "The development of data-driven heart sound classification models has been an\nactive area of research in recent years. To develop such data-driven models in\nthe first place, heart sound signals need to be captured using a signal\nacquisition device. However, it is almost impossible to capture noise-free\nheart sound signals due to the presence of internal and external noises in most\nsituations. Such noises and degradations in heart sound signals can potentially\nreduce the accuracy of data-driven classification models. Although different\ntechniques have been proposed in the literature to address the noise issue, how\nand to what extent different noise and degradations in heart sound signals\nimpact the accuracy of data-driven classification models remains unexplored. To\nanswer this question, we produced a synthetic heart sound dataset including\nnormal and abnormal heart sounds contaminated with a large variety of noise and\ndegradations. We used this dataset to investigate the impact of noise and\ndegradation in heart sound recordings on the performance of different\nclassification models. The results show different noises and degradations\naffect the performance of heart sound classification models to a different\nextent; some are more problematic for classification models, and others are\nless destructive. Comparing the findings of this study with the results of a\nsurvey we previously carried out with a group of clinicians shows noise and\ndegradations that are more detrimental to classification models are also more\ndisruptive to accurate auscultation. The findings of this study can be\nleveraged to develop targeted heart sound quality enhancement approaches -\nwhich adapt the type and aggressiveness of quality enhancement based on the\ncharacteristics of noise and degradation in heart sound signals.", "published": "2022-11-14 15:18:31", "link": "http://arxiv.org/abs/2211.07445v1", "categories": ["eess.AS", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "The Birds Need Attention Too: Analysing usage of Self Attention in\n  identifying bird calls in soundscapes", "abstract": "Birds are vital parts of ecosystems across the world and are an excellent\nmeasure of the quality of life on earth. Many bird species are endangered while\nothers are already extinct. Ecological efforts in understanding and monitoring\nbird populations are important to conserve their habitat and species, but this\nmostly relies on manual methods in rough terrains. Recent advances in Machine\nLearning and Deep Learning have made automatic bird recognition in diverse\nenvironments possible. Birdcall recognition till now has been performed using\nconvolutional neural networks. In this work, we try and understand how\nself-attention can aid in this endeavor. With that we build an pre-trained\nAttention-based Spectrogram Transformer baseline for BirdCLEF 2022 and compare\nthe results against the pre-trained Convolution-based baseline. Our results\nshow that the transformer models outperformed the convolutional model and we\nfurther validate our results by building baselines and analyzing the results\nfor the previous year BirdCLEF 2021 challenge. Source code available at\nhttps://github.com/ck090/BirdCLEF-22", "published": "2022-11-14 19:48:45", "link": "http://arxiv.org/abs/2211.07722v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Describing emotions with acoustic property prompts for speech emotion\n  recognition", "abstract": "Emotions lie on a broad continuum and treating emotions as a discrete number\nof classes limits the ability of a model to capture the nuances in the\ncontinuum. The challenge is how to describe the nuances of emotions and how to\nenable a model to learn the descriptions. In this work, we devise a method to\nautomatically create a description (or prompt) for a given audio by computing\nacoustic properties, such as pitch, loudness, speech rate, and articulation\nrate. We pair a prompt with its corresponding audio using 5 different emotion\ndatasets. We trained a neural network model using these audio-text pairs. Then,\nwe evaluate the model using one more dataset. We investigate how the model can\nlearn to associate the audio with the descriptions, resulting in performance\nimprovement of Speech Emotion Recognition and Speech Audio Retrieval. We expect\nour findings to motivate research describing the broad continuum of emotion", "published": "2022-11-14 20:29:37", "link": "http://arxiv.org/abs/2211.07737v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Unsupervised Uncertainty-Driven Speech Pseudo-Label Filtering and\n  Model Calibration", "abstract": "Pseudo-label (PL) filtering forms a crucial part of Self-Training (ST)\nmethods for unsupervised domain adaptation. Dropout-based Uncertainty-driven\nSelf-Training (DUST) proceeds by first training a teacher model on source\ndomain labeled data. Then, the teacher model is used to provide PLs for the\nunlabeled target domain data. Finally, we train a student on augmented labeled\nand pseudo-labeled data. The process is iterative, where the student becomes\nthe teacher for the next DUST iteration. A crucial step that precedes the\nstudent model training in each DUST iteration is filtering out noisy PLs that\ncould lead the student model astray. In DUST, we proposed a simple, effective,\nand theoretically sound PL filtering strategy based on the teacher model's\nuncertainty about its predictions on unlabeled speech utterances. We estimate\nthe model's uncertainty by computing disagreement amongst multiple samples\ndrawn from the teacher model during inference by injecting noise via dropout.\nIn this work, we show that DUST's PL filtering, as initially used, may fail\nunder severe source and target domain mismatch. We suggest several approaches\nto eliminate or alleviate this issue. Further, we bring insights from the\nresearch in neural network model calibration to DUST and show that a\nwell-calibrated model correlates strongly with a positive outcome of the DUST\nPL filtering step.", "published": "2022-11-14 23:20:36", "link": "http://arxiv.org/abs/2211.07795v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Is my automatic audio captioning system so bad? spider-max: a metric to\n  consider several caption candidates", "abstract": "Automatic Audio Captioning (AAC) is the task that aims to describe an audio\nsignal using natural language. AAC systems take as input an audio signal and\noutput a free-form text sentence, called a caption. Evaluating such systems is\nnot trivial, since there are many ways to express the same idea. For this\nreason, several complementary metrics, such as BLEU, CIDEr, SPICE and SPIDEr,\nare used to compare a single automatic caption to one or several captions of\nreference, produced by a human annotator. Nevertheless, an automatic system can\nproduce several caption candidates, either using some randomness in the\nsentence generation process, or by considering the various competing\nhypothesized captions during decoding with beam-search, for instance. If we\nconsider an end-user of an AAC system, presenting several captions instead of a\nsingle one seems relevant to provide some diversity, similarly to information\nretrieval systems. In this work, we explore the possibility to consider several\npredicted captions in the evaluation process instead of one. For this purpose,\nwe propose SPIDEr-max, a metric that takes the maximum SPIDEr value among the\nscores of several caption candidates. To advocate for our metric, we report\nexperiments on Clotho v2.1 and AudioCaps, with a transformed-based system. On\nAudioCaps for example, this system reached a SPIDEr-max value (with 5\ncandidates) close to the SPIDEr human score of reference.", "published": "2022-11-14 19:16:45", "link": "http://arxiv.org/abs/2211.08983v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
