{"title": "An Empirical Study of Incorporating Pseudo Data into Grammatical Error\n  Correction", "abstract": "The incorporation of pseudo data in the training of grammatical error\ncorrection models has been one of the main factors in improving the performance\nof such models. However, consensus is lacking on experimental configurations,\nnamely, choosing how the pseudo data should be generated or used. In this\nstudy, these choices are investigated through extensive experiments, and\nstate-of-the-art performance is achieved on the CoNLL-2014 test set\n($F_{0.5}=65.0$) and the official test set of the BEA-2019 shared task\n($F_{0.5}=70.2$) without making any modifications to the model architecture.", "published": "2019-09-02 01:33:08", "link": "http://arxiv.org/abs/1909.00502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rotate King to get Queen: Word Relationships as Orthogonal\n  Transformations in Embedding Space", "abstract": "A notable property of word embeddings is that word relationships can exist as\nlinear substructures in the embedding space. For example, $\\textit{gender}$\ncorresponds to $\\vec{\\textit{woman}} - \\vec{\\textit{man}}$ and\n$\\vec{\\textit{queen}} - \\vec{\\textit{king}}$. This, in turn, allows word\nanalogies to be solved arithmetically: $\\vec{\\textit{king}} -\n\\vec{\\textit{man}} + \\vec{\\textit{woman}} \\approx \\vec{\\textit{queen}}$. This\nproperty is notable because it suggests that models trained on word embeddings\ncan easily learn such relationships as geometric translations. However, there\nis no evidence that models $\\textit{exclusively}$ represent relationships in\nthis manner. We document an alternative way in which downstream models might\nlearn these relationships: orthogonal and linear transformations. For example,\ngiven a translation vector for $\\textit{gender}$, we can find an orthogonal\nmatrix $R$, representing a rotation and reflection, such that\n$R(\\vec{\\textit{king}}) \\approx \\vec{\\textit{queen}}$ and\n$R(\\vec{\\textit{man}}) \\approx \\vec{\\textit{woman}}$. Analogical reasoning\nusing orthogonal transformations is almost as accurate as using vector\narithmetic; using linear transformations is more accurate than both. Our\nfindings suggest that these transformations can be as good a representation of\nword relationships as translation vectors.", "published": "2019-09-02 01:36:33", "link": "http://arxiv.org/abs/1909.00504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Contextual are Contextualized Word Representations? Comparing the\n  Geometry of BERT, ELMo, and GPT-2 Embeddings", "abstract": "Replacing static word embeddings with contextualized word representations has\nyielded significant improvements on many NLP tasks. However, just how\ncontextual are the contextualized representations produced by models such as\nELMo and BERT? Are there infinitely many context-specific representations for\neach word, or are words essentially assigned one of a finite number of\nword-sense representations? For one, we find that the contextualized\nrepresentations of all words are not isotropic in any layer of the\ncontextualizing model. While representations of the same word in different\ncontexts still have a greater cosine similarity than those of two different\nwords, this self-similarity is much lower in upper layers. This suggests that\nupper layers of contextualizing models produce more context-specific\nrepresentations, much like how upper layers of LSTMs produce more task-specific\nrepresentations. In all layers of ELMo, BERT, and GPT-2, on average, less than\n5% of the variance in a word's contextualized representations can be explained\nby a static embedding for that word, providing some justification for the\nsuccess of contextualized representations.", "published": "2019-09-02 01:51:46", "link": "http://arxiv.org/abs/1909.00512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond The Wall Street Journal: Anchoring and Comparing Discourse\n  Signals across Genres", "abstract": "Recent research on discourse relations has found that they are cued not only\nby discourse markers (DMs) but also by other textual signals and that signaling\ninformation is indicative of genres. While several corpora exist with discourse\nrelation signaling information such as the Penn Discourse Treebank (PDTB,\nPrasad et al. 2008) and the Rhetorical Structure Theory Signalling Corpus\n(RST-SC, Das and Taboada 2018), they both annotate the Wall Street Journal\n(WSJ) section of the Penn Treebank (PTB, Marcus et al. 1993), which is limited\nto the news domain. Thus, this paper adapts the signal identification and\nanchoring scheme (Liu and Zeldes, 2019) to three more genres, examines the\ndistribution of signaling devices across relations and genres, and provides a\ntaxonomy of indicative signals found in this dataset.", "published": "2019-09-02 02:27:19", "link": "http://arxiv.org/abs/1909.00516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All Roads Lead to UD: Converting Stanford and Penn Parses to English\n  Universal Dependencies with Multilayer Annotations", "abstract": "We describe and evaluate different approaches to the conversion of gold\nstandard corpus data from Stanford Typed Dependencies (SD) and Penn-style\nconstituent trees to the latest English Universal Dependencies representation\n(UD 2.2). Our results indicate that pure SD to UD conversion is highly accurate\nacross multiple genres, resulting in around 1.5% errors, but can be improved\nfurther to fewer than 0.5% errors given access to annotations beyond the pure\nsyntax tree, such as entity types and coreference resolution, which are\nnecessary for correct generation of several UD relations. We show that\nconstituent-based conversion using CoreNLP (with automatic NER) performs\nsubstantially worse in all genres, including when using gold constituent trees,\nprimarily due to underspecification of phrasal grammatical functions.", "published": "2019-09-02 03:14:17", "link": "http://arxiv.org/abs/1909.00522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Context-aware Neural Machine Translation with Target-side\n  Context", "abstract": "In recent years, several studies on neural machine translation (NMT) have\nattempted to use document-level context by using a multi-encoder and two\nattention mechanisms to read the current and previous sentences to incorporate\nthe context of the previous sentences. These studies concluded that the\ntarget-side context is less useful than the source-side context. However, we\nconsidered that the reason why the target-side context is less useful lies in\nthe architecture used to model these contexts.\n  Therefore, in this study, we investigate how the target-side context can\nimprove context-aware neural machine translation. We propose a weight sharing\nmethod wherein NMT saves decoder states and calculates an attention vector\nusing the saved states when translating a current sentence. Our experiments\nshow that the target-side context is also useful if we plug it into NMT as the\ndecoder state when translating a previous sentence.", "published": "2019-09-02 04:04:18", "link": "http://arxiv.org/abs/1909.00531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classification Betters Regression in Query-based Multi-document\n  Summarisation Techniques for Question Answering: Macquarie University at\n  BioASQ7b", "abstract": "Task B Phase B of the 2019 BioASQ challenge focuses on biomedical question\nanswering. Macquarie University's participation applies query-based\nmulti-document extractive summarisation techniques to generate a multi-sentence\nanswer given the question and the set of relevant snippets. In past\nparticipation we explored the use of regression approaches using deep learning\narchitectures and a simple policy gradient architecture. For the 2019 challenge\nwe experiment with the use of classification approaches with and without\nreinforcement learning. In addition, we conduct a correlation analysis between\nvarious ROUGE metrics and the BioASQ human evaluation scores.", "published": "2019-09-02 04:39:26", "link": "http://arxiv.org/abs/1909.00542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phrase-Level Class based Language Model for Mandarin Smart Speaker Query\n  Recognition", "abstract": "The success of speech assistants requires precise recognition of a number of\nentities on particular contexts. A common solution is to train a class-based\nn-gram language model and then expand the classes into specific words or\nphrases. However, when the class has a huge list, e.g., more than 20 million\nsongs, a fully expansion will cause memory explosion. Worse still, the list\nitems in the class need to be updated frequently, which requires a dynamic\nmodel updating technique. In this work, we propose to train pruned language\nmodels for the word classes to replace the slots in the root n-gram. We further\npropose to use a novel technique, named Difference Language Model (DLM), to\ncorrect the bias from the pruned language models. Once the decoding graph is\nbuilt, we only need to recalculate the DLM when the entities in word classes\nare updated. Results show that the proposed method consistently and\nsignificantly outperforms the conventional approaches on all datasets, esp. for\nlarge lists, which the conventional approaches cannot handle.", "published": "2019-09-02 05:55:36", "link": "http://arxiv.org/abs/1909.00556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Context Modeling with a Query-Guided Capsule Network for\n  Document-level Translation", "abstract": "Context modeling is essential to generate coherent and consistent translation\nfor Document-level Neural Machine Translations. The widely used method for\ndocument-level translation usually compresses the context information into a\nrepresentation via hierarchical attention networks. However, this method\nneither considers the relationship between context words nor distinguishes the\nroles of context words. To address this problem, we propose a query-guided\ncapsule networks to cluster context information into different perspectives\nfrom which the target translation may concern. Experiment results show that our\nmethod can significantly outperform strong baselines on multiple data sets of\ndifferent domains.", "published": "2019-09-02 06:55:16", "link": "http://arxiv.org/abs/1909.00564v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sketch-Based System for Semantic Parsing", "abstract": "This paper presents our semantic parsing system for the evaluation task of\nopen domain semantic parsing in NLPCC 2019. Many previous works formulate\nsemantic parsing as a sequence-to-sequence(seq2seq) problem. Instead, we treat\nthe task as a sketch-based problem in a coarse-to-fine(coarse2fine) fashion.\nThe sketch is a high-level structure of the logical form exclusive of low-level\ndetails such as entities and predicates. In this way, we are able to optimize\neach part individually. Specifically, we decompose the process into three\nstages: the sketch classification determines the high-level structure while the\nentity labeling and the matching network fill in missing details. Moreover, we\nadopt the seq2seq method to evaluate logical form candidates from an overall\nperspective. The co-occurrence relationship between predicates and entities\ncontribute to the reranking as well. Our submitted system achieves the exactly\nmatching accuracy of 82.53% on full test set and 47.83% on hard test subset,\nwhich is the 3rd place in NLPCC 2019 Shared Task 2. After optimizations for\nparameters, network structure and sampling, the accuracy reaches 84.47% on full\ntest set and 63.08% on hard test subset(Our code and data are available at\nhttps://github.com/zechagl/NLPCC2019-Semantic-Parsing).", "published": "2019-09-02 07:16:57", "link": "http://arxiv.org/abs/1909.00574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SumQE: a BERT-based Summary Quality Estimation Model", "abstract": "We propose SumQE, a novel Quality Estimation model for summarization based on\nBERT. The model addresses linguistic quality aspects that are only indirectly\ncaptured by content-based approaches to summary evaluation, without involving\ncomparison with human references. SumQE achieves very high correlations with\nhuman ratings, outperforming simpler models addressing these linguistic\naspects. Predictions of the SumQE model can be used for system development, and\nto inform users of the quality of automatically produced summaries and other\ntypes of generated text.", "published": "2019-09-02 07:30:53", "link": "http://arxiv.org/abs/1909.00578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subword Language Model for Query Auto-Completion", "abstract": "Current neural query auto-completion (QAC) systems rely on character-level\nlanguage models, but they slow down when queries are long. We present how to\nutilize subword language models for the fast and accurate generation of query\ncompletion candidates. Representing queries with subwords shorten a decoding\nlength significantly. To deal with issues coming from introducing subword\nlanguage model, we develop a retrace algorithm and a reranking method by\napproximate marginalization. As a result, our model achieves up to 2.5 times\nfaster while maintaining a similar quality of generated results compared to the\ncharacter-level baseline. Also, we propose a new evaluation metric, mean\nrecoverable length (MRL), measuring how many upcoming characters the model\ncould complete correctly. It provides more explicit meaning and eliminates the\nneed for prefix length sampling for existing rank-based metrics. Moreover, we\nperformed a comprehensive analysis with ablation study to figure out the\nimportance of each component.", "published": "2019-09-02 08:40:41", "link": "http://arxiv.org/abs/1909.00599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Medcial Terminology Knowledge Bases via Pre-trained Language\n  Model and Graph Convolutional Network", "abstract": "Enriching existing medical terminology knowledge bases (KBs) is an important\nand never-ending work for clinical research because new terminology alias may\nbe continually added and standard terminologies may be newly renamed. In this\npaper, we propose a novel automatic terminology enriching approach to\nsupplement a set of terminologies to KBs. Specifically, terminology and entity\ncharacters are first fed into pre-trained language model to obtain semantic\nembedding. The pre-trained model is used again to initialize the terminology\nand entity representations, then they are further embedded through graph\nconvolutional network to gain structure embedding. Afterwards, both semantic\nand structure embeddings are combined to measure the relevancy between the\nterminology and the entity. Finally, the optimal alignment is achieved based on\nthe order of relevancy between the terminology and all the entities in the KB.\nExperimental results on clinical indicator terminology KB, collected from 38\ntop-class hospitals of Shanghai Hospital Development Center, show that our\nproposed approach outperforms baseline methods and can effectively enrich the\nKB.", "published": "2019-09-02 09:16:06", "link": "http://arxiv.org/abs/1909.00615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Story-oriented Image Selection and Placement", "abstract": "Multimodal contents have become commonplace on the Internet today, manifested\nas news articles, social media posts, and personal or business blog posts.\nAmong the various kinds of media (images, videos, graphics, icons, audio) used\nin such multimodal stories, images are the most popular. The selection of\nimages from a collection - either author's personal photo album, or web\nrepositories - and their meticulous placement within a text, builds a succinct\nmultimodal commentary for digital consumption. In this paper we present a\nsystem that automates the process of selecting relevant images for a story and\nplacing them at contextual paragraphs within the story for a multimodal\nnarration. We leverage automatic object recognition, user-provided tags, and\ncommonsense knowledge, and use an unsupervised combinatorial optimization to\nsolve the selection and placement problems seamlessly as a single unit.", "published": "2019-09-02 12:41:45", "link": "http://arxiv.org/abs/1909.00692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimally Supervised Learning of Affective Events Using Discourse\n  Relations", "abstract": "Recognizing affective events that trigger positive or negative sentiment has\na wide range of natural language processing applications but remains a\nchallenging problem mainly because the polarity of an event is not necessarily\npredictable from its constituent words. In this paper, we propose to propagate\naffective polarity using discourse relations. Our method is simple and only\nrequires a very small seed lexicon and a large raw corpus. Our experiments\nusing Japanese data show that our method learns affective events effectively\nwithout manually labeled data. It also improves supervised learning results\nwhen labeled data are small.", "published": "2019-09-02 12:46:26", "link": "http://arxiv.org/abs/1909.00694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-Level Content Planning and Style Specification for Neural Text\n  Generation", "abstract": "Building effective text generation systems requires three critical\ncomponents: content selection, text planning, and surface realization, and\ntraditionally they are tackled as separate problems. Recent all-in-one style\nneural generation models have made impressive progress, yet they often produce\noutputs that are incoherent and unfaithful to the input. To address these\nissues, we present an end-to-end trained two-step generation model, where a\nsentence-level content planner first decides on the keyphrases to cover as well\nas a desired language style, followed by a surface realization decoder that\ngenerates relevant and coherent text. For experiments, we consider three tasks\nfrom domains with diverse topics and varying language styles: persuasive\nargument construction from Reddit, paragraph generation for normal and simple\nversions of Wikipedia, and abstract generation for scientific articles.\nAutomatic evaluation shows that our system can significantly outperform\ncompetitive comparisons. Human judges further rate our system generated text as\nmore fluent and correct, compared to the generations by its variants that do\nnot consider language style.", "published": "2019-09-02 14:29:36", "link": "http://arxiv.org/abs/1909.00734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Editing-Based SQL Query Generation for Cross-Domain Context-Dependent\n  Questions", "abstract": "We focus on the cross-domain context-dependent text-to-SQL generation task.\nBased on the observation that adjacent natural language questions are often\nlinguistically dependent and their corresponding SQL queries tend to overlap,\nwe utilize the interaction history by editing the previous predicted query to\nimprove the generation quality. Our editing mechanism views SQL as sequences\nand reuses generation results at the token level in a simple manner. It is\nflexible to change individual tokens and robust to error propagation.\nFurthermore, to deal with complex table structures in different domains, we\nemploy an utterance-table encoder and a table-aware decoder to incorporate the\ncontext of the user utterance and the table schema. We evaluate our approach on\nthe SParC dataset and demonstrate the benefit of editing compared with the\nstate-of-the-art baselines which generate SQL from scratch. Our code is\navailable at https://github.com/ryanzhumich/sparc_atis_pytorch.", "published": "2019-09-02 16:24:57", "link": "http://arxiv.org/abs/1909.00786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Relationship between Multi-Party Linguistic\n  Entrainment, Team Characteristics, and the Perception of Team Social Outcomes", "abstract": "Multi-party linguistic entrainment refers to the phenomenon that speakers\ntend to speak more similarly during conversation. We first developed new\nmeasures of multi-party entrainment on features describing linguistic style,\nand then examined the relationship between entrainment and team characteristics\nin terms of gender composition, team size, and diversity. Next, we predicted\nthe perception of team social outcomes using multi-party linguistic entrainment\nand team characteristics with a hierarchical regression model. We found that\nteams with greater gender diversity had higher minimum convergence than teams\nwith less gender diversity. Entrainment contributed significantly to predicting\nperceived team social outcomes both alone and controlling for team\ncharacteristics.", "published": "2019-09-02 21:04:39", "link": "http://arxiv.org/abs/1909.00867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Personality Traits Using Overlap Dynamics in Multiparty\n  Dialogue", "abstract": "Research on human spoken language has shown that speech plays an important\nrole in identifying speaker personality traits. In this work, we propose an\napproach for identifying speaker personality traits using overlap dynamics in\nmultiparty spoken dialogues. We first define a set of novel features\nrepresenting the overlap dynamics of each speaker. We then investigate the\nimpact of speaker personality traits on these features using ANOVA tests. We\nfind that features of overlap dynamics significantly vary for speakers with\ndifferent levels of both Extraversion and Conscientiousness. Finally, we find\nthat classifiers using only overlap dynamics features outperform random\nguessing in identifying Extraversion and Agreeableness, and that the\nimprovements are statistically significant.", "published": "2019-09-02 21:50:52", "link": "http://arxiv.org/abs/1909.00876v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Understanding The Effect Of Loss function On Then Performance Of\n  Knowledge Graph Embedding", "abstract": "Knowledge graphs (KGs) represent world's facts in structured forms. KG\ncompletion exploits the existing facts in a KG to discover new ones.\nTranslation-based embedding model (TransE) is a prominent formulation to do KG\ncompletion. Despite the efficiency of TransE in memory and time, it suffers\nfrom several limitations in encoding relation patterns such as symmetric,\nreflexive etc. To resolve this problem, most of the attempts have circled\naround the revision of the score function of TransE i.e., proposing a more\ncomplicated score function such as Trans(A, D, G, H, R, etc) to mitigate the\nlimitations. In this paper, we tackle this problem from a different\nperspective. We show that existing theories corresponding to the limitations of\nTransE are inaccurate because they ignore the effect of loss function.\nAccordingly, we pose theoretical investigations of the main limitations of\nTransE in the light of loss function. To the best of our knowledge, this has\nnot been investigated so far comprehensively. We show that by a proper\nselection of the loss function for training the TransE model, the main\nlimitations of the model are mitigated. This is explained by setting\nupper-bound for the scores of positive samples, showing the region of truth\n(i.e., the region that a triple is considered positive by the model). Our\ntheoretical proofs with experimental results fill the gap between the\ncapability of translation-based class of embedding models and the loss\nfunction. The theories emphasise the importance of the selection of the loss\nfunctions for training the models. Our experimental evaluations on different\nloss functions used for training the models justify our theoretical proofs and\nconfirm the importance of the loss functions on the performance.", "published": "2019-09-02 03:10:14", "link": "http://arxiv.org/abs/1909.00519v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Answering questions by learning to rank -- Learning to rank by answering\n  questions", "abstract": "Answering multiple-choice questions in a setting in which no supporting\ndocuments are explicitly provided continues to stand as a core problem in\nnatural language processing. The contribution of this article is two-fold.\nFirst, it describes a method which can be used to semantically rank documents\nextracted from Wikipedia or similar natural language corpora. Second, we\npropose a model employing the semantic ranking that holds the first place in\ntwo of the most popular leaderboards for answering multiple-choice questions:\nARC Easy and Challenge. To achieve this, we introduce a self-attention based\nneural network that latently learns to rank documents by their importance\nrelated to a given question, whilst optimizing the objective of predicting the\ncorrect answer. These documents are considered relevant contexts for the\nunderlying question. We have published the ranked documents so that they can be\nused off-the-shelf to improve downstream decision models.", "published": "2019-09-02 08:36:32", "link": "http://arxiv.org/abs/1909.00596v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Method to Learn Embedding of a Probabilistic Medical Knowledge Graph:\n  Algorithm Development", "abstract": "This paper proposes an algorithm named as PrTransH to learn embedding vectors\nfrom real world EMR data based medical knowledge. The unique challenge in\nembedding medical knowledge graph from real world EMR data is that the\nuncertainty of knowledge triplets blurs the border between \"correct triplet\"\nand \"wrong triplet\", changing the fundamental assumption of many existing\nalgorithms. To address the challenge, some enhancements are made to existing\nTransH algorithm, including: 1) involve probability of medical knowledge\ntriplet into training objective; 2) replace the margin-based ranking loss with\nunified loss calculation considering both valid and corrupted triplets; 3)\naugment training data set with medical background knowledge. Verifications on\nreal world EMR data based medical knowledge graph prove that PrTransH\noutperforms TransH in link prediction task. To the best of our survey, this\npaper is the first one to learn and verify knowledge embedding on probabilistic\nknowledge graphs.", "published": "2019-09-02 11:28:35", "link": "http://arxiv.org/abs/1909.00672v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence\n  Generation", "abstract": "Existing approaches to dialogue state tracking rely on pre-defined ontologies\nconsisting of a set of all possible slot types and values. Though such\napproaches exhibit promising performance on single-domain benchmarks, they\nsuffer from computational complexity that increases proportionally to the\nnumber of pre-defined slots that need tracking. This issue becomes more severe\nwhen it comes to multi-domain dialogues which include larger numbers of slots.\nIn this paper, we investigate how to approach DST using a generation framework\nwithout the pre-defined ontology list. Given each turn of user utterance and\nsystem response, we directly generate a sequence of belief states by applying a\nhierarchical encoder-decoder structure. In this way, the computational\ncomplexity of our model will be a constant regardless of the number of\npre-defined slots. Experiments on both the multi-domain and the single domain\ndialogue state tracking dataset show that our model not only scales easily with\nthe increasing number of pre-defined domains and slots but also reaches the\nstate-of-the-art performance.", "published": "2019-09-02 15:00:08", "link": "http://arxiv.org/abs/1909.00754v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The CL-SciSumm Shared Task 2018: Results and Key Insights", "abstract": "This overview describes the official results of the CL-SciSumm Shared Task\n2018 -- the first medium-scale shared task on scientific document summarization\nin the computational linguistics (CL) domain. This year, the dataset comprised\n60 annotated sets of citing and reference papers from the open access research\npapers in the CL domain. The Shared Task was organized as a part of the 41st\nAnnual Conference of the Special Interest Group in Information Retrieval\n(SIGIR), held in Ann Arbor, USA in July 2018. We compare the participating\nsystems in terms of two evaluation metrics. The annotated dataset and\nevaluation scripts can be accessed and used by the community from:\n\\url{https://github.com/WING-NUS/scisumm-corpus}.", "published": "2019-09-02 15:23:55", "link": "http://arxiv.org/abs/1909.00764v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "It's All in the Name: Mitigating Gender Bias with Name-Based\n  Counterfactual Data Substitution", "abstract": "This paper treats gender bias latent in word embeddings. Previous mitigation\nattempts rely on the operationalisation of gender bias as a projection over a\nlinear subspace. An alternative approach is Counterfactual Data Augmentation\n(CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by\nswapping all inherently-gendered words in the copy. We perform an empirical\ncomparison of these approaches on the English Gigaword and Wikipedia, and find\nthat whilst both successfully reduce direct bias and perform well in tasks\nwhich quantify embedding quality, CDA variants outperform projection-based\nmethods at the task of drawing non-biased gender analogies by an average of 19%\nacross both corpora. We propose two improvements to CDA: Counterfactual Data\nSubstitution (CDS), a variant of CDA in which potentially biased text is\nrandomly substituted to avoid duplication, and the Names Intervention, a novel\nname-pairing technique that vastly increases the number of words being treated.\nCDA/S with the Names Intervention is the only approach which is able to\nmitigate indirect gender bias: following debiasing, previously biased words are\nsignificantly less clustered according to gender (cluster purity is reduced by\n49%), thus improving on the state-of-the-art for bias mitigation.", "published": "2019-09-02 21:33:03", "link": "http://arxiv.org/abs/1909.00871v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Joint Event and Temporal Relation Extraction with Shared Representations\n  and Structured Prediction", "abstract": "We propose a joint event and temporal relation extraction model with shared\nrepresentation learning and structured prediction. The proposed method has two\nadvantages over existing work. First, it improves event representation by\nallowing the event and relation modules to share the same contextualized\nembeddings and neural representation learner. Second, it avoids error\npropagation in the conventional pipeline systems by leveraging structured\ninference and learning methods to assign both the event labels and the temporal\nrelation labels jointly. Experiments show that the proposed method can improve\nboth event extraction and temporal relation extraction over state-of-the-art\nsystems, with the end-to-end F1 improved by 10% and 6.8% on two benchmark\ndatasets respectively.", "published": "2019-09-02 18:00:43", "link": "http://arxiv.org/abs/1909.05360v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Commonsense Knowledge Mining from Pretrained Models", "abstract": "Inferring commonsense knowledge is a key challenge in natural language\nprocessing, but due to the sparsity of training data, previous work has shown\nthat supervised methods for commonsense knowledge mining underperform when\nevaluated on novel data. In this work, we develop a method for generating\ncommonsense knowledge using a large, pre-trained bidirectional language model.\nBy transforming relational triples into masked sentences, we can use this model\nto rank a triple's validity by the estimated pointwise mutual information\nbetween the two entities. Since we do not update the weights of the\nbidirectional model, our approach is not biased by the coverage of any one\ncommonsense knowledge base. Though this method performs worse on a test set\nthan models explicitly trained on a corresponding training set, it outperforms\nthese methods when mining commonsense knowledge from new sources, suggesting\nthat unsupervised techniques may generalize better than current supervised\napproaches.", "published": "2019-09-02 01:41:00", "link": "http://arxiv.org/abs/1909.00505v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent\n  Neural Network Machine Translation", "abstract": "Reduction of training time is an important issue in many tasks like patent\ntranslation involving neural networks. Data parallelism and model parallelism\nare two common approaches for reducing training time using multiple graphics\nprocessing units (GPUs) on one machine. In this paper, we propose a hybrid\ndata-model parallel approach for sequence-to-sequence (Seq2Seq) recurrent\nneural network (RNN) machine translation. We apply a model parallel approach to\nthe RNN encoder-decoder part of the Seq2Seq model and a data parallel approach\nto the attention-softmax part of the model. We achieved a speed-up of 4.13 to\n4.20 times when using 4 GPUs compared with the training speed when using 1 GPU\nwithout affecting machine translation accuracy as measured in terms of BLEU\nscores.", "published": "2019-09-02 06:41:34", "link": "http://arxiv.org/abs/1909.00562v2", "categories": ["cs.DC", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.DC"}
{"title": "A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text", "abstract": "When trained effectively, the Variational Autoencoder (VAE) is both a\npowerful language model and an effective representation learning framework. In\npractice, however, VAEs are trained with the evidence lower bound (ELBO) as a\nsurrogate objective to the intractable marginal data likelihood. This approach\nto training yields unstable results, frequently leading to a disastrous local\noptimum known as posterior collapse. In this paper, we investigate a simple fix\nfor posterior collapse which yields surprisingly effective results. The\ncombination of two known heuristics, previously considered only in isolation,\nsubstantially improves held-out likelihood, reconstruction, and latent\nrepresentation learning when compared with previous state-of-the-art methods.\nMore interestingly, while our experiments demonstrate superiority on these\nprinciple evaluations, our method obtains a worse ELBO. We use these results to\nargue that the typical surrogate objective for VAEs may not be sufficient or\nnecessarily appropriate for balancing the goals of representation learning and\ndata distribution modeling.", "published": "2019-09-02 21:08:00", "link": "http://arxiv.org/abs/1909.00868v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Modeling Long-Range Context for Concurrent Dialogue Acts Recognition", "abstract": "In dialogues, an utterance is a chain of consecutive sentences produced by\none speaker which ranges from a short sentence to a thousand-word post. When\nstudying dialogues at the utterance level, it is not uncommon that an utterance\nwould serve multiple functions. For instance, \"Thank you. It works great.\"\nexpresses both gratitude and positive feedback in the same utterance. Multiple\ndialogue acts (DA) for one utterance breeds complex dependencies across\ndialogue turns. Therefore, DA recognition challenges a model's predictive power\nover long utterances and complex DA context. We term this problem Concurrent\nDialogue Acts (CDA) recognition. Previous work on DA recognition either assumes\none DA per utterance or fails to realize the sequential nature of dialogues. In\nthis paper, we present an adapted Convolutional Recurrent Neural Network (CRNN)\nwhich models the interactions between utterances of long-range context. Our\nmodel significantly outperforms existing work on CDA recognition on a tech\nforum dataset.", "published": "2019-09-02 03:12:19", "link": "http://arxiv.org/abs/1909.00521v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Avaya Conversational Intelligence: A Real-Time System for Spoken\n  Language Understanding in Human-Human Call Center Conversations", "abstract": "Avaya Conversational Intelligence(ACI) is an end-to-end, cloud-based solution\nfor real-time Spoken Language Understanding for call centers. It combines large\nvocabulary, real-time speech recognition, transcript refinement, and entity and\nintent recognition in order to convert live audio into a rich, actionable\nstream of structured events. These events can be further leveraged with a\nbusiness rules engine, thus serving as a foundation for real-time supervision\nand assistance applications. After the ingestion, calls are enriched with\nunsupervised keyword extraction, abstractive summarization, and\nbusiness-defined attributes, enabling offline use cases, such as business\nintelligence, topic mining, full-text search, quality assurance, and agent\ntraining. ACI comes with a pretrained, configurable library of hundreds of\nintents and a robust intent training environment that allows for efficient,\ncost-effective creation and customization of customer-specific intents.", "published": "2019-09-02 22:57:10", "link": "http://arxiv.org/abs/1909.02851v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
