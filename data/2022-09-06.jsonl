{"title": "Few-Shot Document-Level Event Argument Extraction", "abstract": "Event argument extraction (EAE) has been well studied at the sentence level\nbut under-explored at the document level. In this paper, we study to capture\nevent arguments that actually spread across sentences in documents. Prior works\nusually assume full access to rich document supervision, ignoring the fact that\nthe available argument annotation is usually limited. To fill this gap, we\npresent FewDocAE, a Few-Shot Document-Level Event Argument Extraction\nbenchmark, based on the existing document-level event extraction dataset. We\nfirst define the new problem and reconstruct the corpus by a novel N -Way-D-Doc\nsampling instead of the traditional N -Way-K-Shot strategy. Then we adjust the\ncurrent document-level neural models into the few-shot setting to provide\nbaseline results under in- and cross-domain settings. Since the argument\nextraction depends on the context from multiple sentences and the learning\nprocess is limited to very few examples, we find this novel task to be very\nchallenging with substantively low performance. Considering FewDocAE is closely\nrelated to practical use under low-resource regimes, we hope this benchmark\nencourages more research in this direction. Our data and codes will be\navailable online.", "published": "2022-09-06 03:57:23", "link": "http://arxiv.org/abs/2209.02203v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reference Resolution and Context Change in Multimodal Situated Dialogue\n  for Exploring Data Visualizations", "abstract": "Reference resolution, which aims to identify entities being referred to by a\nspeaker, is more complex in real world settings: new referents may be created\nby processes the agents engage in and/or be salient only because they belong to\nthe shared physical setting. Our focus is on resolving references to\nvisualizations on a large screen display in multimodal dialogue; crucially,\nreference resolution is directly involved in the process of creating new\nvisualizations. We describe our annotations for user references to\nvisualizations appearing on a large screen via language and hand gesture and\nalso new entity establishment, which results from executing the user request to\ncreate a new visualization. We also describe our reference resolution pipeline\nwhich relies on an information-state architecture to maintain dialogue context.\nWe report results on detecting and resolving references, effectiveness of\ncontextual information on the model, and under-specified requests for creating\nvisualizations. We also experiment with conventional CRF and deep learning /\ntransformer models (BiLSTM-CRF and BERT-CRF) for tagging references in user\nutterance text. Our results show that transfer learning significantly boost\nperformance of the deep learning methods, although CRF still out-performs them,\nsuggesting that conventional methods may generalize better for low resource\ndata.", "published": "2022-09-06 04:43:28", "link": "http://arxiv.org/abs/2209.02215v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "External Knowledge Selection with Weighted Negative Sampling in\n  Knowledge-grounded Task-oriented Dialogue Systems", "abstract": "Constructing a robust dialogue system on spoken conversations bring more\nchallenge than written conversation. In this respect, DSTC10-Track2-Task2 is\nproposed, which aims to build a task-oriented dialogue (TOD) system\nincorporating unstructured external knowledge on a spoken conversation,\nextending DSTC9-Track1. This paper introduces our system containing four\nadvanced methods: data construction, weighted negative sampling, post-training,\nand style transfer. We first automatically construct a large training data\nbecause DSTC10-Track2 does not release the official training set. For the\nknowledge selection task, we propose weighted negative sampling to train the\nmodel more fine-grained manner. We also employ post-training and style transfer\nfor the response generation task to generate an appropriate response with a\nsimilar style to the target response. In the experiment, we investigate the\neffect of weighted negative sampling, post-training, and style transfer. Our\nmodel ranked 7 out of 16 teams in the objective evaluation and 6 in human\nevaluation.", "published": "2022-09-06 06:55:42", "link": "http://arxiv.org/abs/2209.02251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Layer or Representation Space: What makes BERT-based Evaluation Metrics\n  Robust?", "abstract": "The evaluation of recent embedding-based evaluation metrics for text\ngeneration is primarily based on measuring their correlation with human\nevaluations on standard benchmarks. However, these benchmarks are mostly from\nsimilar domains to those used for pretraining word embeddings. This raises\nconcerns about the (lack of) generalization of embedding-based metrics to new\nand noisy domains that contain a different vocabulary than the pretraining\ndata. In this paper, we examine the robustness of BERTScore, one of the most\npopular embedding-based metrics for text generation. We show that (a) an\nembedding-based metric that has the highest correlation with human evaluations\non a standard benchmark can have the lowest correlation if the amount of input\nnoise or unknown tokens increases, (b) taking embeddings from the first layer\nof pretrained models improves the robustness of all metrics, and (c) the\nhighest robustness is achieved when using character-level embeddings, instead\nof token-based embeddings, from the first layer of the pretrained model.", "published": "2022-09-06 09:10:54", "link": "http://arxiv.org/abs/2209.02317v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Monolingual alignment of word senses and definitions in lexicographical\n  resources", "abstract": "The focus of this thesis is broadly on the alignment of lexicographical data,\nparticularly dictionaries. In order to tackle some of the challenges in this\nfield, two main tasks of word sense alignment and translation inference are\naddressed. The first task aims to find an optimal alignment given the sense\ndefinitions of a headword in two different monolingual dictionaries. This is a\nchallenging task, especially due to differences in sense granularity, coverage\nand description in two resources. After describing the characteristics of\nvarious lexical semantic resources, we introduce a benchmark containing 17\ndatasets of 15 languages where monolingual word senses and definitions are\nmanually annotated across different resources by experts. In the creation of\nthe benchmark, lexicographers' knowledge is incorporated through the\nannotations where a semantic relation, namely exact, narrower, broader, related\nor none, is selected for each sense pair. This benchmark can be used for\nevaluation purposes of word-sense alignment systems. The performance of a few\nalignment techniques based on textual and non-textual semantic similarity\ndetection and semantic relation induction is evaluated using the benchmark.\nFinally, we extend this work to translation inference where translation pairs\nare induced to generate bilingual lexicons in an unsupervised way using various\napproaches based on graph analysis. This task is of particular interest for the\ncreation of lexicographical resources for less-resourced and under-represented\nlanguages and also, assists in increasing coverage of the existing resources.\nFrom a practical point of view, the techniques and methods that are developed\nin this thesis are implemented within a tool that can facilitate the alignment\ntask.", "published": "2022-09-06 13:09:52", "link": "http://arxiv.org/abs/2209.02465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning of Lexical Semantic Families for Argumentative\n  Discourse Units Identification", "abstract": "Argument mining tasks require an informed range of low to high complexity\nlinguistic phenomena and commonsense knowledge. Previous work has shown that\npre-trained language models are highly effective at encoding syntactic and\nsemantic linguistic phenomena when applied with transfer learning techniques\nand built on different pre-training objectives. It remains an issue of how much\nthe existing pre-trained language models encompass the complexity of argument\nmining tasks. We rely on experimentation to shed light on how language models\nobtained from different lexical semantic families leverage the performance of\nthe identification of argumentative discourse units task. Experimental results\nshow that transfer learning techniques are beneficial to the task and that\ncurrent methods may be insufficient to leverage commonsense knowledge from\ndifferent lexical semantic families.", "published": "2022-09-06 13:38:47", "link": "http://arxiv.org/abs/2209.02495v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Mama Always Had a Way of Explaining Things So I Could Understand'': A\n  Dialogue Corpus for Learning to Construct Explanations", "abstract": "As AI is more and more pervasive in everyday life, humans have an increasing\ndemand to understand its behavior and decisions. Most research on explainable\nAI builds on the premise that there is one ideal explanation to be found. In\nfact, however, everyday explanations are co-constructed in a dialogue between\nthe person explaining (the explainer) and the specific person being explained\nto (the explainee). In this paper, we introduce a first corpus of dialogical\nexplanations to enable NLP research on how humans explain as well as on how AI\ncan learn to imitate this process. The corpus consists of 65 transcribed\nEnglish dialogues from the Wired video series \\emph{5 Levels}, explaining 13\ntopics to five explainees of different proficiency. All 1550 dialogue turns\nhave been manually labeled by five independent professionals for the topic\ndiscussed as well as for the dialogue act and the explanation move performed.\nWe analyze linguistic patterns of explainers and explainees, and we explore\ndifferences across proficiency levels. BERT-based baseline results indicate\nthat sequence information helps predicting topics, acts, and moves effectively", "published": "2022-09-06 14:00:22", "link": "http://arxiv.org/abs/2209.02508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OneEE: A One-Stage Framework for Fast Overlapping and Nested Event\n  Extraction", "abstract": "Event extraction (EE) is an essential task of information extraction, which\naims to extract structured event information from unstructured text. Most prior\nwork focuses on extracting flat events while neglecting overlapped or nested\nones. A few models for overlapped and nested EE includes several successive\nstages to extract event triggers and arguments,which suffer from error\npropagation. Therefore, we design a simple yet effective tagging scheme and\nmodel to formulate EE as word-word relation recognition, called OneEE. The\nrelations between trigger or argument words are simultaneously recognized in\none stage with parallel grid tagging, thus yielding a very fast event\nextraction speed. The model is equipped with an adaptive event fusion module to\ngenerate event-aware representations and a distance-aware predictor to\nintegrate relative distance information for word-word relation recognition,\nwhich are empirically demonstrated to be effective mechanisms. Experiments on 3\noverlapped and nested EE benchmarks, namely FewFC, Genia11, and Genia13, show\nthat OneEE achieves the state-of-the-art (SOTA) results. Moreover, the\ninference speed of OneEE is faster than those of baselines in the same\ncondition, and can be further substantially improved since it supports parallel\ninference.", "published": "2022-09-06 17:59:55", "link": "http://arxiv.org/abs/2209.02693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Increasing Adverse Drug Events extraction robustness on social media:\n  case study on negation and speculation", "abstract": "In the last decade, an increasing number of users have started reporting\nAdverse Drug Events (ADE) on social media platforms, blogs, and health forums.\nGiven the large volume of reports, pharmacovigilance has focused on ways to use\nNatural Language Processing (NLP) techniques to rapidly examine these large\ncollections of text, detecting mentions of drug-related adverse reactions to\ntrigger medical investigations. However, despite the growing interest in the\ntask and the advances in NLP, the robustness of these models in face of\nlinguistic phenomena such as negations and speculations is an open research\nquestion. Negations and speculations are pervasive phenomena in natural\nlanguage, and can severely hamper the ability of an automated system to\ndiscriminate between factual and nonfactual statements in text. In this paper\nwe take into consideration four state-of-the-art systems for ADE detection on\nsocial media texts. We introduce SNAX, a benchmark to test their performance\nagainst samples containing negated and speculated ADEs, showing their fragility\nagainst these phenomena. We then introduce two possible strategies to increase\nthe robustness of these models, showing that both of them bring significant\nincreases in performance, lowering the number of spurious entities predicted by\nthe models by 60% for negation and 80% for speculations.", "published": "2022-09-06 20:38:42", "link": "http://arxiv.org/abs/2209.02812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Bidirectional Unsupervised Translation Through Multilingual\n  Finetuning and Back-Translation", "abstract": "We propose a two-stage approach for training a single NMT model to translate\nunseen languages both to and from English. For the first stage, we initialize\nan encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform\nmultilingual fine-tuning on parallel data in 40 languages to English. We find\nthis model can generalize to zero-shot translations on unseen languages. For\nthe second stage, we leverage this generalization ability to generate synthetic\nparallel data from monolingual datasets, then bidirectionally train with\nsuccessive rounds of back-translation.\n  Our approach, which we EcXTra (English-centric Crosslingual (X) Transfer), is\nconceptually simple, only using a standard cross-entropy objective throughout.\nIt is also data-driven, sequentially leveraging auxiliary parallel data and\nmonolingual data. We evaluate unsupervised NMT results for 7 low-resource\nlanguages, and find that each round of back-translation training further\nrefines bidirectional performance. Our final single EcXTra-trained model\nachieves competitive translation performance in all translation directions,\nnotably establishing a new state-of-the-art for English-to-Kazakh (22.9 > 10.4\nBLEU). Our code is available at https://github.com/manestay/EcXTra .", "published": "2022-09-06 21:20:41", "link": "http://arxiv.org/abs/2209.02821v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASR2K: Speech Recognition for Around 2000 Languages without Audio", "abstract": "Most recent speech recognition models rely on large supervised datasets,\nwhich are unavailable for many low-resource languages. In this work, we present\na speech recognition pipeline that does not require any audio for the target\nlanguage. The only assumption is that we have access to raw text datasets or a\nset of n-gram statistics. Our speech pipeline consists of three components:\nacoustic, pronunciation, and language models. Unlike the standard pipeline, our\nacoustic and pronunciation models use multilingual models without any\nsupervision. The language model is built using n-gram statistics or the raw\ntext dataset. We build speech recognition for 1909 languages by combining it\nwith Crubadan: a large endangered languages n-gram database. Furthermore, we\ntest our approach on 129 languages across two datasets: Common Voice and CMU\nWilderness dataset. We achieve 50% CER and 74% WER on the Wilderness dataset\nwith Crubadan statistics only and improve them to 45% CER and 69% WER when\nusing 10000 raw text utterances.", "published": "2022-09-06 22:48:29", "link": "http://arxiv.org/abs/2209.02842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Aspect-level Sentiment Classification via Explicit Utilization\n  of Aspect-to-Document Sentiment Composition", "abstract": "As aspect-level sentiment labels are expensive and labor-intensive to\nacquire, zero-shot aspect-level sentiment classification is proposed to learn\nclassifiers applicable to new domains without using any annotated aspect-level\ndata. In contrast, document-level sentiment data with ratings are more easily\naccessible. In this work, we achieve zero-shot aspect-level sentiment\nclassification by only using document-level reviews. Our key intuition is that\nthe sentiment representation of a document is composed of the sentiment\nrepresentations of all the aspects of that document. Based on this, we propose\nthe AF-DSC method to explicitly model such sentiment composition in reviews.\nAF-DSC first learns sentiment representations for all potential aspects and\nthen aggregates aspect-level sentiments into a document-level one to perform\ndocument-level sentiment classification. In this way, we obtain the\naspect-level sentiment classifier as the by-product of the document-level\nsentiment classifier. Experimental results on aspect-level sentiment\nclassification benchmarks demonstrate the effectiveness of explicit utilization\nof sentiment composition in document-level sentiment classification. Our model\nwith only 30k training data outperforms previous work utilizing millions of\ndata.", "published": "2022-09-06 08:02:55", "link": "http://arxiv.org/abs/2209.02276v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jeopardy: An Invertible Functional Programming Language", "abstract": "Algorithms are ways of mapping problems to solutions. An algorithm is\ninvertible precisely when this mapping is injective, such that the initial\nproblem can be uniquely inferred from its solution.\n  While invertible algorithms can be described in general-purpose languages, no\nguarantees are generally made by such languages as regards invertibility, so\nensuring invertibility requires additional (and often non-trivial) proof. On\nthe other hand, while reversible programming languages guarantee that their\nprograms are invertible by restricting the permissible operations to those\nwhich are locally invertible, writing programs in the reversible style can be\ncumbersome, and may differ significantly from conventional implementations even\nwhen the implemented algorithm is, in fact, invertible.\n  In this paper we introduce Jeopardy, a functional programming language that\nguarantees program invertibility without imposing local reversibility. In\nparticular, Jeopardy allows the limited use of uninvertible -- and even\nnondeterministic! -- operations, provided that they are used in a way that can\nbe statically determined to be invertible. To this end, we outline an\n\\emph{implicitly available arguments analysis} and three further approaches\nthat can give a partial static guarantee to the (generally difficult) problem\nof guaranteeing invertibility.", "published": "2022-09-06 11:38:42", "link": "http://arxiv.org/abs/2209.02422v3", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "Comparing Methods for Extractive Summarization of Call Centre Dialogue", "abstract": "This paper provides results of evaluating some text summarisation techniques\nfor the purpose of producing call summaries for contact centre solutions. We\nspecifically focus on extractive summarisation methods, as they do not require\nany labelled data and are fairly quick and easy to implement for production\nuse. We experimentally compare several such methods by using them to produce\nsummaries of calls, and evaluating these summaries objectively (using ROUGE-L)\nand subjectively (by aggregating the judgements of several annotators). We\nfound that TopicSum and Lead-N outperform the other summarisation methods,\nwhilst BERTSum received comparatively lower scores in both subjective and\nobjective evaluations. The results demonstrate that even such simple\nheuristics-based methods like Lead-N ca n produce meaningful and useful\nsummaries of call centre dialogues.", "published": "2022-09-06 13:16:02", "link": "http://arxiv.org/abs/2209.02472v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reconstructing Action-Conditioned Human-Object Interactions Using\n  Commonsense Knowledge Priors", "abstract": "We present a method for inferring diverse 3D models of human-object\ninteractions from images. Reasoning about how humans interact with objects in\ncomplex scenes from a single 2D image is a challenging task given ambiguities\narising from the loss of information through projection. In addition, modeling\n3D interactions requires the generalization ability towards diverse object\ncategories and interaction types. We propose an action-conditioned modeling of\ninteractions that allows us to infer diverse 3D arrangements of humans and\nobjects without supervision on contact regions or 3D scene geometry. Our method\nextracts high-level commonsense knowledge from large language models (such as\nGPT-3), and applies them to perform 3D reasoning of human-object interactions.\nOur key insight is priors extracted from large language models can help in\nreasoning about human-object contacts from textural prompts only. We\nquantitatively evaluate the inferred 3D models on a large human-object\ninteraction dataset and show how our method leads to better 3D reconstructions.\nWe further qualitatively evaluate the effectiveness of our method on real\nimages and demonstrate its generalizability towards interaction types and\nobject categories.", "published": "2022-09-06 13:32:55", "link": "http://arxiv.org/abs/2209.02485v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Analyzing Transformers in Embedding Space", "abstract": "Understanding Transformer-based models has attracted significant attention,\nas they lie at the heart of recent technological advances across machine\nlearning. While most interpretability methods rely on running models over\ninputs, recent work has shown that a zero-pass approach, where parameters are\ninterpreted directly without a forward/backward pass is feasible for some\nTransformer parameters, and for two-layer attention networks. In this work, we\npresent a theoretical analysis where all parameters of a trained Transformer\nare interpreted by projecting them into the embedding space, that is, the space\nof vocabulary items they operate on. We derive a simple theoretical framework\nto support our arguments and provide ample evidence for its validity. First, an\nempirical analysis showing that parameters of both pretrained and fine-tuned\nmodels can be interpreted in embedding space. Second, we present two\napplications of our framework: (a) aligning the parameters of different models\nthat share a vocabulary, and (b) constructing a classifier without training by\n``translating'' the parameters of a fine-tuned classifier to parameters of a\ndifferent model that was only pretrained. Overall, our findings open the door\nto interpretation methods that, at least in part, abstract away from model\nspecifics and operate in the embedding space only.", "published": "2022-09-06 14:36:57", "link": "http://arxiv.org/abs/2209.02535v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Black Boxes to Conversations: Incorporating XAI in a Conversational\n  Agent", "abstract": "The goal of Explainable AI (XAI) is to design methods to provide insights\ninto the reasoning process of black-box models, such as deep neural networks,\nin order to explain them to humans. Social science research states that such\nexplanations should be conversational, similar to human-to-human explanations.\nIn this work, we show how to incorporate XAI in a conversational agent, using a\nstandard design for the agent comprising natural language understanding and\ngeneration components. We build upon an XAI question bank, which we extend by\nquality-controlled paraphrases, to understand the user's information needs. We\nfurther systematically survey the literature for suitable explanation methods\nthat provide the information to answer those questions, and present a\ncomprehensive list of suggestions. Our work is the first step towards truly\nnatural conversations about machine learning models with an explanation agent.\nThe comprehensive list of XAI questions and the corresponding explanation\nmethods may support other researchers in providing the necessary information to\naddress users' demands. To facilitate future work, we release our source code\nand data.", "published": "2022-09-06 15:01:06", "link": "http://arxiv.org/abs/2209.02552v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "\"Es geht um Respekt, nicht um Technologie\": Erkenntnisse aus einem\n  Interessensgruppen-\u00fcbergreifenden Workshop zu genderfairer Sprache und\n  Sprachtechnologie", "abstract": "With the increasing attention non-binary people receive in Western societies,\nstrategies of gender-fair language have started to move away from binary (only\nfemale/male) concepts of gender. Nevertheless, hardly any approaches to take\nthese identities into account into machine translation models exist so far. A\nlack of understanding of the socio-technical implications of such technologies\nrisks further reproducing linguistic mechanisms of oppression and mislabelling.\nIn this paper, we describe the methods and results of a workshop on gender-fair\nlanguage and language technologies, which was led and organised by ten\nresearchers from TU Wien, St. P\\\"olten UAS, FH Campus Wien and the University\nof Vienna and took place in Vienna in autumn 2021. A wide range of interest\ngroups and their representatives were invited to ensure that the topic could be\ndealt with holistically. Accordingly, we aimed to include translators, machine\ntranslation experts and non-binary individuals (as \"community experts\") on an\nequal footing. Our analysis shows that gender in machine translation requires a\nhigh degree of context sensitivity, that developers of such technologies need\nto position themselves cautiously in a process still under social negotiation,\nand that flexible approaches seem most adequate at present. We then illustrate\nsteps that follow from our results for the field of gender-fair language\ntechnologies so that technological developments can adequately line up with\nsocial advancements.\n  ----\n  Mit zunehmender gesamtgesellschaftlicher Wahrnehmung nicht-bin\\\"arer Personen\nhaben sich in den letzten Jahren auch Konzepte von genderfairer Sprache von der\nbisher verwendeten Binarit\\\"at (weiblich/m\\\"annlich) entfernt. Trotzdem gibt es\nbislang nur wenige Ans\\\"atze dazu, diese Identit\\\"aten in maschineller\n\\\"Ubersetzung abzubilden. Ein fehlendes Verst\\\"andnis unterschiedlicher\nsozio-technischer Implikationen derartiger Technologien birgt in sich die\nGefahr, fehlerhafte Ansprachen und Bezeichnungen sowie sprachliche\nUnterdr\\\"uckungsmechanismen zu reproduzieren. In diesem Beitrag beschreiben wir\ndie Methoden und Ergebnisse eines Workshops zu genderfairer Sprache in\ntechnologischen Zusammenh\\\"angen, der im Herbst 2021 in Wien stattgefunden hat.\nZehn Forscher*innen der TU Wien, FH St. P\\\"olten, FH Campus Wien und\nUniversit\\\"at Wien organisierten und leiteten den Workshop. Dabei wurden\nunterschiedlichste Interessensgruppen und deren Vertreter*innen breit gestreut\neingeladen, um sicherzustellen, dass das Thema holistisch behandelt werden\nkann. Dementsprechend setzten wir uns zum Ziel,\nMachine-Translation-Entwickler*innen, \\\"Ubersetzer*innen, und nicht-bin\\\"are\nPrivatpersonen (als \"Lebenswelt-Expert*innen\") gleichberechtigt einzubinden.\nUnsere Analyse zeigt, dass Geschlecht in maschineller \\\"Ubersetzung eine\nma\\ss{}geblich kontextsensible Herangehensweise erfordert, die Entwicklung von\nSprachtechnologien sich vorsichtig in einem sich noch in Aushandlung\nbefindlichen gesellschaftlichen Prozess positionieren muss, und flexible\nAns\\\"atze derzeit am ad\\\"aquatesten erscheinen. Wir zeigen auf, welche\nn\\\"achsten Schritte im Bereich genderfairer Technologien notwendig sind, damit\ntechnische mit sozialen Entwicklungen mithalten k\\\"onnen.", "published": "2022-09-06 19:36:42", "link": "http://arxiv.org/abs/2209.02793v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Entity Aware Syntax Tree Based Data Augmentation for Natural Language\n  Understanding", "abstract": "Understanding the intention of the users and recognizing the semantic\nentities from their sentences, aka natural language understanding (NLU), is the\nupstream task of many natural language processing tasks. One of the main\nchallenges is to collect a sufficient amount of annotated data to train a\nmodel. Existing research about text augmentation does not abundantly consider\nentity and thus performs badly for NLU tasks. To solve this problem, we propose\na novel NLP data augmentation technique, Entity Aware Data Augmentation (EADA),\nwhich applies a tree structure, Entity Aware Syntax Tree (EAST), to represent\nsentences combined with attention on the entity. Our EADA technique\nautomatically constructs an EAST from a small amount of annotated data, and\nthen generates a large number of training instances for intent detection and\nslot filling. Experimental results on four datasets showed that the proposed\ntechnique significantly outperforms the existing data augmentation methods in\nterms of both accuracy and generalization ability.", "published": "2022-09-06 07:34:10", "link": "http://arxiv.org/abs/2209.02267v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Depression Symptoms Modelling from Social Media Text: A Semi-supervised\n  Learning Approach", "abstract": "A fundamental component of user-level social media language based clinical\ndepression modelling is depression symptoms detection (DSD). Unfortunately,\nthere does not exist any DSD dataset that reflects both the clinical insights\nand the distribution of depression symptoms from the samples of self-disclosed\ndepressed population. In our work, we describe a Semi-supervised Learning (SSL)\nframework which uses an initial supervised learning model that leverages 1) a\nstate-of-the-art large mental health forum text pre-trained language model\nfurther fine-tuned on a clinician annotated DSD dataset, 2) a Zero-Shot\nlearning model for DSD, and couples them together to harvest depression\nsymptoms related samples from our large self-curated Depression Tweets\nRepository (DTR). Our clinician annotated dataset is the largest of its kind.\nFurthermore, DTR is created from the samples of tweets in self-disclosed\ndepressed users Twitter timeline from two datasets, including one of the\nlargest benchmark datasets for user-level depression detection from Twitter.\nThis further helps preserve the depression symptoms distribution of\nself-disclosed Twitter users tweets. Subsequently, we iteratively retrain our\ninitial DSD model with the harvested data. We discuss the stopping criteria and\nlimitations of this SSL process, and elaborate the underlying constructs which\nplay a vital role in the overall SSL process. We show that we can produce a\nfinal dataset which is the largest of its kind. Furthermore, a DSD and a\nDepression Post Detection (DPD) model trained on it achieves significantly\nbetter accuracy than their initial version.", "published": "2022-09-06 18:41:57", "link": "http://arxiv.org/abs/2209.02765v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Read it to me: An emotionally aware Speech Narration Application", "abstract": "In this work we try to perform emotional style transfer on audios. In\nparticular, MelGAN-VC architecture is explored for various emotion-pair\ntransfers. The generated audio is then classified using an LSTM-based emotion\nclassifier for audio. We find that \"sad\" audio is generated well as compared to\n\"happy\" or \"anger\" as people have similar expressions of sadness.", "published": "2022-09-06 19:13:09", "link": "http://arxiv.org/abs/2209.02785v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The BLue Amazon Brain (BLAB): A Modular Architecture of Services about\n  the Brazilian Maritime Territory", "abstract": "We describe the first steps in the development of an artificial agent focused\non the Brazilian maritime territory, a large region within the South Atlantic\nalso known as the Blue Amazon. The \"BLue Amazon Brain\" (BLAB) integrates a\nnumber of services aimed at disseminating information about this region and its\nimportance, functioning as a tool for environmental awareness. The main service\nprovided by BLAB is a conversational facility that deals with complex questions\nabout the Blue Amazon, called BLAB-Chat; its central component is a controller\nthat manages several task-oriented natural language processing modules (e.g.,\nquestion answering and summarizer systems). These modules have access to an\ninternal data lake as well as to third-party databases. A news reporter\n(BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the\nBLAB service architecture. In this paper, we describe our current version of\nBLAB's architecture (interface, backend, web services, NLP modules, and\nresources) and comment on the challenges we have faced so far, such as the lack\nof training data and the scattered state of domain information. Solving these\nissues presents a considerable challenge in the development of artificial\nintelligence for technical domains.", "published": "2022-09-06 18:32:08", "link": "http://arxiv.org/abs/2209.07928v1", "categories": ["cs.AI", "cs.CL", "cs.SY", "eess.SY"], "primary_category": "cs.AI"}
{"title": "Prospectively accelerated dynamic speech MRI at 3 Tesla using a\n  self-navigated spiral based manifold regularized scheme", "abstract": "This work proposes a self-navigated variable density spiral(VDS) based\nmanifold regularization scheme to prospectively improve dynamic speech MRI at\n3T. Short readout 1.3ms spirals were used to minimize off-resonance. A custom\n16-channel speech coil was used for improved parallel imaging of vocal tract.\nThe manifold model leveraged similarities between frames sharing similar speech\npostures without explicit motion binning. The self-navigating capability of VDS\nwas leveraged to learn the Laplacian matrix of the manifold. Reconstruction was\nposed as a SENSE-based non-local soft weighted temporal regularization scheme.\nOur approach was compared against view-sharing, low-rank, finite difference,\nextra-dimension-based sparsity reconstruction constraints. Under-sampling\nexperiments were conducted on five volunteers performing repetitive and\narbitrary speaking tasks at different speaking rates. Quantitative evaluation\nin terms of mean square error over moving edges were performed in a\nretrospectively under-sampled data. For prospective under-sampling, blinded\nimage quality evaluation in the categories of alias artifacts, spatial\nblurring, and temporal blurring were performed by three voice research experts.\nRegion of interest analysis at articulator boundaries were performed to assess\narticulatory motion. Our scheme provided improved reconstruction over the\nothers. With prospective under-sampling, a spatial resolution of 2.4mm2/pixel\nand a temporal resolution of 17.4 ms/frame for single slice imaging, and 52.2\nms/frame for 3-slice imaging were achieved. We demonstrated implicit motion\nbinning by analyzing the mechanics of the Laplacian matrix. Our method\ndemonstrated superior image quality scores in reducing spatial and temporal\nblurring. While it exhibited faint alias artifacts similar to temporal\nfinite-difference, it provided statistically significant improvements over\nremaining constraints.", "published": "2022-09-06 18:47:42", "link": "http://arxiv.org/abs/2209.02768v2", "categories": ["eess.IV", "eess.AS", "physics.med-ph"], "primary_category": "eess.IV"}
{"title": "The Role of Vocal Persona in Natural and Synthesized Speech", "abstract": "The inclusion of voice persona in synthesized voice can be significant in a\nbroad range of human-computer-interaction (HCI) applications, including\naugmentative and assistive communication (AAC), artistic performance, and\ndesign of virtual agents. We propose a framework to imbue compelling and\ncontextually-dependent expression within a synthesized voice by introducing the\nrole of the vocal persona within a synthesis system. In this framework, the\nresultant 'tone of voice' is defined as a point existing within a continuous,\ncontextually-dependent probability space that is traversable by the user of the\nvoice. We also present initial findings of a thematic analysis of 10 interviews\nwith vocal studies and performance experts to further understand the role of\nthe vocal persona within a natural communication ecology. The themes identified\nare then used to inform the design of the aforementioned framework.", "published": "2022-09-06 23:34:30", "link": "http://arxiv.org/abs/2209.02855v2", "categories": ["cs.SD", "cs.HC", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "cs.SD"}
