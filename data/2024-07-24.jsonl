{"title": "Estimation of bid-ask spreads in the presence of serial dependence", "abstract": "Starting from a basic model in which the dynamic of the transaction prices is\na geometric Brownian motion disrupted by a microstructure white noise,\ncorresponding to the random alternation of bids and asks, we propose\nmoment-based estimators along with their statistical properties. We then make\nthe model more realistic by considering serial dependence: we assume a\ngeometric fractional Brownian motion for the price, then an Ornstein-Uhlenbeck\nprocess for the microstructure noise. In these two cases of serial dependence,\nwe propose again consistent and asymptotically normal estimators. All our\nestimators are compared on simulated data with existing approaches, such as\nRoll, Corwin-Schultz, Abdi-Ranaldo, or Ardia-Guidotti-Kroencke estimators.", "published": "2024-07-24 16:26:20", "link": "http://arxiv.org/abs/2407.17401v3", "categories": ["q-fin.ST", "q-fin.MF", "q-fin.TR", "stat.AP", "stat.ME"], "primary_category": "q-fin.ST"}
{"title": "Train-Attention: Meta-Learning Where to Focus in Continual Knowledge\n  Learning", "abstract": "Previous studies on continual knowledge learning (CKL) in large language\nmodels (LLMs) have predominantly focused on approaches such as regularization,\narchitectural modifications, and rehearsal techniques to mitigate catastrophic\nforgetting. However, these methods naively inherit the inefficiencies of\nstandard training procedures, indiscriminately applying uniform weight across\nall tokens, which can lead to unnecessary parameter updates and increased\nforgetting. To address these shortcomings, we propose a novel CKL approach\ntermed Train-Attention-Augmented Language Model (TAALM), which enhances\nlearning efficiency by dynamically predicting and applying weights to tokens\nbased on their usefulness. This method employs a meta-learning framework that\noptimizes token importance predictions, facilitating targeted knowledge updates\nand minimizing forgetting. Also, we observe that existing benchmarks do not\nclearly exhibit the trade-off between learning and retaining, therefore we\npropose a new benchmark, \\textsc{LAMA-ckl}, to address this issue. Through\nexperiments conducted on both newly introduced and established CKL benchmarks,\nTAALM proves the state-of-the-art performance upon the baselines, and also\nshows synergistic compatibility when integrated with previous CKL approaches.", "published": "2024-07-24 01:04:34", "link": "http://arxiv.org/abs/2407.16920v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ScholarChemQA: Unveiling the Power of Language Models in Chemical\n  Research Question Answering", "abstract": "Question Answering (QA) effectively evaluates language models' reasoning and\nknowledge depth. While QA datasets are plentiful in areas like general domain\nand biomedicine, academic chemistry is less explored. Chemical QA plays a\ncrucial role in both education and research by effectively translating complex\nchemical information into readily understandable format. Addressing this gap,\nwe introduce ScholarChemQA, a large-scale QA dataset constructed from chemical\npapers. This dataset reflects typical real-world challenges, including an\nimbalanced data distribution and a substantial amount of unlabeled data that\ncan be potentially useful. Correspondingly, we introduce a QAMatch model,\nspecifically designed to effectively answer chemical questions by fully\nleveraging our collected data. We first address the issue of imbalanced label\ndistribution by re-weighting the instance-wise loss based on the inverse\nfrequency of each class, ensuring minority classes are not dominated by\nmajority ones during optimization. Next, we utilize the unlabeled data to\nenrich the learning process, generating a variety of augmentations based on a\nSoftMix operation and ensuring their predictions align with the same target,\ni.e., pseudo-labels. To ensure the quality of the pseudo-labels, we propose a\ncalibration procedure aimed at closely aligning the pseudo-label estimates of\nindividual samples with a desired ground truth distribution. Experiments show\nthat our QAMatch significantly outperforms the recent similar-scale baselines\nand Large Language Models (LLMs) not only on our ScholarChemQA dataset but also\non four benchmark datasets. We hope our benchmark and model can facilitate and\npromote more research on chemical QA.", "published": "2024-07-24 01:46:55", "link": "http://arxiv.org/abs/2407.16931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal\n  Intervention Perspective", "abstract": "This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.", "published": "2024-07-24 04:39:24", "link": "http://arxiv.org/abs/2407.16997v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling In-Context Learning: A Coordinate System to Understand Its\n  Working Mechanism", "abstract": "Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. The other attributes it to LLMs'\ninherent ability of task recognition, deeming label correctness and shot\nnumbers of demonstrations as not crucial. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether similar examples are presented in the demonstrations\n(perception) and whether LLMs can recognize the task (cognition). We propose\nthe peak inverse rank metric to detect the task recognition ability of LLMs and\nstudy LLMs' reactions to different definitions of similarity. Based on these,\nwe conduct extensive experiments to elucidate how ICL functions across each\nquadrant on multiple representative classification tasks. Finally, we extend\nour analyses to generation tasks, showing that our coordinate system can also\nbe used to interpret ICL for generation tasks effectively.", "published": "2024-07-24 05:26:52", "link": "http://arxiv.org/abs/2407.17011v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Language Models Evaluate Human Written Text? Case Study on Korean\n  Student Writing for Education", "abstract": "Large language model (LLM)-based evaluation pipelines have demonstrated their\ncapability to robustly evaluate machine-generated text. Extending this\nmethodology to assess human-written text could significantly benefit\neducational settings by providing direct feedback to enhance writing skills,\nalthough this application is not straightforward. In this paper, we investigate\nwhether LLMs can effectively assess human-written text for educational\npurposes. We collected 100 texts from 32 Korean students across 15 types of\nwriting and employed GPT-4-Turbo to evaluate them using grammaticality,\nfluency, coherence, consistency, and relevance as criteria. Our analyses\nindicate that LLM evaluators can reliably assess grammaticality and fluency, as\nwell as more objective types of writing, though they struggle with other\ncriteria and types of writing. We publicly release our dataset and feedback.", "published": "2024-07-24 06:02:57", "link": "http://arxiv.org/abs/2407.17022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAFETY-J: Evaluating Safety with Critique", "abstract": "The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at https://github.com/GAIR-NLP/Safety-J.", "published": "2024-07-24 08:04:00", "link": "http://arxiv.org/abs/2407.17075v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Alignment and Reassignment with Generalist Large Language Model\n  for Enhanced Cross-Domain Named Entity Recognition", "abstract": "Named entity recognition on the in-domain supervised and few-shot settings\nhave been extensively discussed in the NLP community and made significant\nprogress. However, cross-domain NER, a more common task in practical scenarios,\nstill poses a challenge for most NER methods. Previous research efforts in that\narea primarily focus on knowledge transfer such as correlate label information\nfrom source to target domains but few works pay attention to the problem of\nlabel conflict. In this study, we introduce a label alignment and reassignment\napproach, namely LAR, to address this issue for enhanced cross-domain named\nentity recognition, which includes two core procedures: label alignment between\nsource and target domains and label reassignment for type inference. The\nprocess of label reassignment can significantly be enhanced by integrating with\nan advanced large-scale language model such as ChatGPT. We conduct an extensive\nrange of experiments on NER datasets involving both supervised and zero-shot\nscenarios. Empirical experimental results demonstrate the validation of our\nmethod with remarkable performance under the supervised and zero-shot\nout-of-domain settings compared to SOTA methods.", "published": "2024-07-24 15:13:12", "link": "http://arxiv.org/abs/2407.17344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Large Language Models with Socratic Method for Conversational\n  Mathematics Teaching", "abstract": "With the introduction of large language models (LLMs), automatic math\nreasoning has seen tremendous success. However, current methods primarily focus\non providing solutions or using techniques like Chain-of-Thought to enhance\nproblem-solving accuracy. In this paper, we focus on improving the capability\nof mathematics teaching via a Socratic teaching-based LLM\n(\\texttt{SocraticLLM}), which guides learners toward profound thinking with\nclarity and self-discovery via conversation. We collect and release a\nhigh-quality mathematical teaching dataset, named \\texttt{SocraticMATH}, which\nprovides Socratic-style conversations of problems with extra knowledge. Also,\nwe propose a knowledge-enhanced LLM as a strong baseline to generate reliable\nresponses with review, guidance/heuristic, rectification, and summarization.\nExperimental results show the great advantages of \\texttt{SocraticLLM} by\ncomparing it with several strong generative models. The codes and datasets are\navailable on \\url{https://github.com/ECNU-ICALK/SocraticMath}.", "published": "2024-07-24 15:18:17", "link": "http://arxiv.org/abs/2407.17349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PERSONA: A Reproducible Testbed for Pluralistic Alignment", "abstract": "The rapid advancement of language models (LMs) necessitates robust alignment\nwith diverse user values. However, current preference optimization approaches\noften fail to capture the plurality of user opinions, instead reinforcing\nmajority viewpoints and marginalizing minority perspectives. We introduce\nPERSONA, a reproducible test bed designed to evaluate and improve pluralistic\nalignment of LMs. We procedurally generate diverse user profiles from US census\ndata, resulting in 1,586 synthetic personas with varied demographic and\nidiosyncratic attributes. We then generate a large-scale evaluation dataset\ncontaining 3,868 prompts and 317,200 feedback pairs obtained from our synthetic\npersonas. Leveraging this dataset, we systematically evaluate LM capabilities\nin role-playing diverse users, verified through human judges, and the\nestablishment of both a benchmark, PERSONA Bench, for pluralistic alignment\napproaches as well as an extensive dataset to create new and future benchmarks.\nThe full dataset and benchmarks are available here:\nhttps://www.synthlabs.ai/research/persona.", "published": "2024-07-24 16:11:39", "link": "http://arxiv.org/abs/2407.17387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$T^5Score$: A Methodology for Automatically Assessing the Quality of LLM\n  Generated Multi-Document Topic Sets", "abstract": "Using LLMs for Multi-Document Topic Extraction has recently gained popularity\ndue to their apparent high-quality outputs, expressiveness, and ease of use.\nHowever, most existing evaluation practices are not designed for LLM-generated\ntopics and result in low inter-annotator agreement scores, hindering the\nreliable use of LLMs for the task. To address this, we introduce $T^5Score$, an\nevaluation methodology that decomposes the quality of a topic set into\nquantifiable aspects, measurable through easy-to-perform annotation tasks. This\nframing enables a convenient, manual or automatic, evaluation procedure\nresulting in a strong inter-annotator agreement score. To substantiate our\nmethodology and claims, we perform extensive experimentation on multiple\ndatasets and report the results.", "published": "2024-07-24 16:14:15", "link": "http://arxiv.org/abs/2407.17390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative artificial intelligence in dentistry: Current approaches and\n  future challenges", "abstract": "Artificial intelligence (AI) has become a commodity for people because of the\nadvent of generative AI (GenAI) models that bridge the usability gap of AI by\nproviding a natural language interface to interact with complex models. These\nGenAI models range from text generation - such as two-way chat systems - to the\ngeneration of image or video from textual descriptions input by a user. These\nadvancements in AI have impacted Dentistry in multiple aspects. In dental\neducation, the student now has the opportunity to solve a plethora of questions\nby only prompting a GenAI model and have the answer in a matter of seconds.\nGenAI models can help us deliver better patient healthcare by helping\npractitioners gather knowledge quickly and efficiently. Finally, GenAI can also\nbe used in dental research, where the applications range from new drug\ndiscovery to assistance in academic writing. In this review, we first define\nGenAI models and describe their multiple generation modalities; then, we\nexplain and discuss their current and potential applications in Dentistry; and\nfinally, we describe the challenges these new technologies impose in our area.", "published": "2024-07-24 03:33:47", "link": "http://arxiv.org/abs/2407.17532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Papilusion at DAGPap24: Paper or Illusion? Detecting AI-generated\n  Scientific Papers", "abstract": "This paper presents Papilusion, an AI-generated scientific text detector\ndeveloped within the DAGPap24 shared task on detecting automatically generated\nscientific papers. We propose an ensemble-based approach and conduct ablation\nstudies to analyze the effect of the detector configurations on the\nperformance. Papilusion is ranked 6th on the leaderboard, and we improve our\nperformance after the competition ended, achieving 99.46 (+9.63) of the\nF1-score on the official test set.", "published": "2024-07-24 20:38:13", "link": "http://arxiv.org/abs/2407.17629v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IgnitionInnovators at \"Discharge Me!\": Chain-of-Thought Instruction\n  Finetuning Large Language Models for Discharge Summaries", "abstract": "This paper presents our proposed approach to the Discharge Me! shared task,\ncollocated with the 23th Workshop on Biomedical Natural Language Processing\n(BioNLP). In this work, we develop an LLM-based framework for solving the\nDischarge Summary Documentation (DSD) task, i.e., generating the two critical\ntarget sections `Brief Hospital Course' and `Discharge Instructions' in the\ndischarge summary. By streamlining the recent instruction-finetuning process on\nLLMs, we explore several prompting strategies for optimally adapting LLMs to\nspecific generation task of DSD. Experimental results show that providing a\nclear output structure, complimented by a set of comprehensive\nChain-of-Thoughts (CoT) questions, effectively improves the model's reasoning\ncapability, and thereby, enhancing the structural correctness and faithfulness\nof clinical information in the generated text. Source code is available at:\nhttps://github.com/antangrocket1312/Discharge_LLM", "published": "2024-07-24 21:02:53", "link": "http://arxiv.org/abs/2407.17636v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time Matters: Examine Temporal Effects on Biomedical Language Models", "abstract": "Time roots in applying language models for biomedical applications: models\nare trained on historical data and will be deployed for new or future data,\nwhich may vary from training data. While increasing biomedical tasks have\nemployed state-of-the-art language models, there are very few studies have\nexamined temporal effects on biomedical models when data usually shifts across\ndevelopment and deployment. This study fills the gap by statistically probing\nrelations between language model performance and data shifts across three\nbiomedical tasks. We deploy diverse metrics to evaluate model performance,\ndistance methods to measure data drifts, and statistical methods to quantify\ntemporal effects on biomedical language models. Our study shows that time\nmatters for deploying biomedical language models, while the degree of\nperformance degradation varies by biomedical tasks and statistical\nquantification approaches. We believe this study can establish a solid\nbenchmark to evaluate and assess temporal effects on deploying biomedical\nlanguage models.", "published": "2024-07-24 21:06:40", "link": "http://arxiv.org/abs/2407.17638v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework\n  for Medical Applications", "abstract": "Large Language Models (LLMs) have exhibited remarkable proficiency in natural\nlanguage understanding, prompting extensive exploration of their potential\napplications across diverse domains. In the medical domain, open-source LLMs\nhave demonstrated moderate efficacy following domain-specific fine-tuning;\nhowever, they remain substantially inferior to proprietary models such as GPT-4\nand GPT-3.5. These open-source models encounter limitations in the\ncomprehensiveness of domain-specific knowledge and exhibit a propensity for\n'hallucinations' during text generation. To mitigate these issues, researchers\nhave implemented the Retrieval-Augmented Generation (RAG) approach, which\naugments LLMs with background information from external knowledge bases while\npreserving the model's internal parameters. However, document noise can\nadversely affect performance, and the application of RAG in the medical field\nremains in its nascent stages. This study presents the Bailicai framework: a\nnovel integration of retrieval-augmented generation with large language models\noptimized for the medical domain. The Bailicai framework augments the\nperformance of LLMs in medicine through the implementation of four sub-modules.\nExperimental results demonstrate that the Bailicai approach surpasses existing\nmedical domain LLMs across multiple medical benchmarks and exceeds the\nperformance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates\nthe prevalent issue of hallucinations in medical applications of LLMs and\nameliorates the noise-related challenges associated with traditional RAG\ntechniques when processing irrelevant or pseudo-relevant documents.", "published": "2024-07-24 12:27:33", "link": "http://arxiv.org/abs/2407.21055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Early screening of potential breakthrough technologies with enhanced\n  interpretability: A patent-specific hierarchical attention network model", "abstract": "Despite the usefulness of machine learning approaches for the early screening\nof potential breakthrough technologies, their practicality is often hindered by\nopaque models. To address this, we propose an interpretable machine learning\napproach to predicting future citation counts from patent texts using a\npatent-specific hierarchical attention network (PatentHAN) model. Central to\nthis approach are (1) a patent-specific pre-trained language model, capturing\nthe meanings of technical words in patent claims, (2) a hierarchical network\nstructure, enabling detailed analysis at the claim level, and (3) a claim-wise\nself-attention mechanism, revealing pivotal claims during the screening\nprocess. A case study of 35,376 pharmaceutical patents demonstrates the\neffectiveness of our approach in early screening of potential breakthrough\ntechnologies while ensuring interpretability. Furthermore, we conduct\nadditional analyses using different language models and claim types to examine\nthe robustness of the approach. It is expected that the proposed approach will\nenhance expert-machine collaboration in identifying breakthrough technologies,\nproviding new insight derived from text mining into technological value.", "published": "2024-07-24 02:17:10", "link": "http://arxiv.org/abs/2407.16939v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias\n  Mitigation", "abstract": "Large language models (LLMs) often inherit biases from vast amounts of\ntraining corpora. Traditional debiasing methods, while effective to some\nextent, do not completely eliminate memorized biases and toxicity in LLMs. In\nthis paper, we study an unlearning-based approach to debiasing in LLMs by\nperforming gradient ascent on hate speech against minority groups, i.e.,\nminimizing the likelihood of biased or toxic content. Specifically, we propose\na mask language modeling unlearning technique, which unlearns the harmful part\nof the text. This method enables LLMs to selectively forget and disassociate\nfrom biased and harmful content. Experimental results demonstrate the\neffectiveness of our approach in diminishing bias while maintaining the\nlanguage modeling abilities. Surprisingly, the results also unveil an\nunexpected potential for cross-domain transfer unlearning: debiasing in one\nbias form (e.g. gender) may contribute to mitigating others (e.g. race and\nreligion).", "published": "2024-07-24 02:37:42", "link": "http://arxiv.org/abs/2407.16951v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models", "abstract": "Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. However, conflicting knowledge can be\npresent in the LM's parameters, termed intra-memory conflict, which can affect\na model's propensity to accept contextual knowledge. To study the effect of\nintra-memory conflict on an LM's ability to accept relevant context, we utilize\ntwo knowledge conflict measures and a novel dataset containing inherently\nconflicting data, DynamicQA. This dataset includes facts with a temporal\ndynamic nature where facts can change over time and disputable dynamic facts,\nwhich can change depending on the viewpoint. DynamicQA is the first to include\nreal-world knowledge conflicts and provide context to study the link between\nthe different types of knowledge conflicts. We also evaluate several measures\non their ability to reflect the presence of intra-memory conflict: semantic\nentropy and a novel coherent persuasion score. With our extensive experiments,\nwe verify that LMs exhibit a greater degree of intra-memory conflict with\ndynamic facts compared to facts that have a single truth value. Furthermore, we\nreveal that facts with intra-memory conflict are harder to update with context,\nsuggesting that retrieval-augmented generation will struggle with the most\ncommonly adapted facts.", "published": "2024-07-24 06:06:07", "link": "http://arxiv.org/abs/2407.17023v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "To Know or Not To Know? Analyzing Self-Consistency of Large Language\n  Models under Ambiguity", "abstract": "One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. This paper focuses\non entity type ambiguity, analyzing the proficiency and consistency of\nstate-of-the-art LLMs in applying factual knowledge when prompted with\nambiguous entities. To do so, we propose an evaluation protocol that\ndisentangles knowing from applying knowledge, and test state-of-the-art LLMs on\n49 ambiguous entities. Our experiments reveal that LLMs struggle with choosing\nthe correct entity reading, achieving an average accuracy of only 85%, and as\nlow as 75% with underspecified prompts. The results also reveal systematic\ndiscrepancies in LLM behavior, showing that while the models may possess\nknowledge, they struggle to apply it consistently, exhibit biases toward\npreferred readings, and display self-inconsistencies. This highlights the need\nto address entity ambiguity in the future for more trustworthy LLMs.", "published": "2024-07-24 09:48:48", "link": "http://arxiv.org/abs/2407.17125v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SDoH-GPT: Using Large Language Models to Extract Social Determinants of\n  Health (SDoH)", "abstract": "Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost.", "published": "2024-07-24 09:57:51", "link": "http://arxiv.org/abs/2407.17126v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SimCT: A Simple Consistency Test Protocol in LLMs Development Lifecycle", "abstract": "In this work, we report our efforts to advance the standard operation\nprocedure of developing Large Language Models (LLMs) or LLMs-based systems or\nservices in industry. We introduce the concept of Large Language Model\nDevelopment Lifecycle (LDLC) and then highlight the importance of consistency\ntest in ensuring the delivery quality. The principled solution of consistency\ntest, however, is usually overlooked by industrial practitioners and not urgent\nin academia, and current practical solutions are insufficiently rigours and\nlabor-intensive. We thus propose a simple yet effective consistency test\nprotocol, named SimCT. SimCT is mainly to proactively check the consistency\nacross different development stages of \"bare metal\" LLMs or associated services\nwithout accessing the model artifacts, in an attempt to expedite the delivery\nby reducing the back-and-forth alignment communications among multiple teams\ninvolved in different development stages.\n  Specifically, SimCT encompasses response-wise and model-wise tests. We\nimplement the protocol with LightGBM and Student's t-test for two components\nrespectively, and perform extensive experiments to substantiate the\neffectiveness of SimCT and the involved components.", "published": "2024-07-24 10:49:19", "link": "http://arxiv.org/abs/2407.17150v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for\n  Automatic Speech Recognition in Multilingual Oral History Archives", "abstract": "In this paper, we are comparing monolingual Wav2Vec 2.0 models with various\nmultilingual models to see whether we could improve speech recognition\nperformance on a unique oral history archive containing a lot of mixed-language\nsentences. Our main goal is to push forward research on this unique dataset,\nwhich is an extremely valuable part of our cultural heritage. Our results\nsuggest that monolingual speech recognition models are, in most cases, superior\nto multilingual models, even when processing the oral history archive full of\nmixed-language sentences from non-native speakers. We also performed the same\nexperiments on the public CommonVoice dataset to verify our results. We are\ncontributing to the research community by releasing our pre-trained models to\nthe public.", "published": "2024-07-24 11:03:47", "link": "http://arxiv.org/abs/2407.17160v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NarrationDep: Narratives on Social Media For Automatic Depression\n  Detection", "abstract": "Social media posts provide valuable insight into the narrative of users and\ntheir intentions, including providing an opportunity to automatically model\nwhether a social media user is depressed or not. The challenge lies in\nfaithfully modelling user narratives from their online social media posts,\nwhich could potentially be useful in several different applications. We have\ndeveloped a novel and effective model called \\texttt{NarrationDep}, which\nfocuses on detecting narratives associated with depression. By analyzing a\nuser's tweets, \\texttt{NarrationDep} accurately identifies crucial narratives.\n\\texttt{NarrationDep} is a deep learning framework that jointly models\nindividual user tweet representations and clusters of users' tweets. As a\nresult, \\texttt{NarrationDep} is characterized by a novel two-layer deep\nlearning model: the first layer models using social media text posts, and the\nsecond layer learns semantic representations of tweets associated with a\ncluster. To faithfully model these cluster representations, the second layer\nincorporates a novel component that hierarchically learns from users' posts.\nThe results demonstrate that our framework outperforms other comparative models\nincluding recently developed models on a variety of datasets.", "published": "2024-07-24 11:24:25", "link": "http://arxiv.org/abs/2407.17174v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN\n  prover", "abstract": "Recently, large language models have presented promising results in aiding\nformal mathematical reasoning. However, their performance is restricted due to\nthe scarcity of formal theorem-proving data, which requires additional effort\nto be extracted from raw formal language corpora. Meanwhile, a significant\namount of human-written formal language corpora remains underutilized. To\naddress this issue, we propose LEAN-GitHub, a dataset consisting of large-scale\nformal data extracted from almost all Lean 4 repositories on GitHub. After\nfine-tuning InternLM-math-plus on this dataset, our model achieved accuracies\nof 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F\ntest, surpassing state-of-the-art method at 52%. And it also achieves\nstate-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting\ndifferent fields/levels of math. These results demonstrate that our proposed\ndataset is beneficial for formal reasoning on a wide range of math topics. We\nopen-source our model at https://GitHub. com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/ datasets/InternLM/Lean-GitHub", "published": "2024-07-24 12:28:03", "link": "http://arxiv.org/abs/2407.17227v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Improving ICD coding using Chapter based Named Entities and Attentional\n  Models", "abstract": "Recent advancements in natural language processing (NLP) have led to\nautomation in various domains. However, clinical NLP often relies on benchmark\ndatasets that may not reflect real-world scenarios accurately. Automatic ICD\ncoding, a vital NLP task, typically uses outdated and imbalanced datasets like\nMIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4\nand 0.7 due to many false positives. Our research introduces an enhanced\napproach to ICD coding that improves F1 scores by using chapter-based named\nentities and attentional models. This method categorizes discharge summaries\ninto ICD-9 Chapters and develops attentional models with chapter-specific data,\neliminating the need to consider external data for code identification. For\ncategorization, we use Chapter-IV to de-bias and influence key entities and\nweights without neural networks, creating accurate thresholds and providing\ninterpretability for human validation. Post-validation, we develop attentional\nmodels for three frequent and three non-frequent codes from Chapter-IV using\nBidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with\nMulti-head Attention architectures. The average Micro-F1 scores of 0.79 and\n0.81 from these models demonstrate significant performance improvements in ICD\ncoding.", "published": "2024-07-24 12:34:23", "link": "http://arxiv.org/abs/2407.17230v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image\n  Relational Association Capabilities in Large Visual Language Models", "abstract": "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.", "published": "2024-07-24 15:59:01", "link": "http://arxiv.org/abs/2407.17379v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dependency Transformer Grammars: Integrating Dependency Structures into\n  Transformer Language Models", "abstract": "Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars.", "published": "2024-07-24 16:38:38", "link": "http://arxiv.org/abs/2407.17406v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FLRT: Fluent Student-Teacher Redteaming", "abstract": "Many publicly available language models have been safety tuned to reduce the\nlikelihood of toxic or liability-inducing text. To redteam or jailbreak these\nmodels for compliance with toxic requests, users and security analysts have\ndeveloped adversarial prompting techniques. One attack method is to apply\ndiscrete optimization techniques to the prompt. However, the resulting attack\nstrings are often gibberish text, easily filtered by defenders due to high\nmeasured perplexity, and may fail for unseen tasks and/or well-tuned models. In\nthis work, we improve existing algorithms (primarily GCG and BEAST) to develop\npowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our\ntechnique centers around a new distillation-based approach that encourages the\nvictim model to emulate a toxified finetune, either in terms of output\nprobabilities or internal activations. To encourage human-fluent attacks, we\nadd a multi-model perplexity penalty and a repetition penalty to the objective.\nWe also enhance optimizer strength by allowing token insertions, token swaps,\nand token deletions and by using longer attack sequences. The resulting process\nis able to reliably jailbreak the most difficult target models with prompts\nthat appear similar to human-written prompts. On Advbench we achieve attack\nsuccess rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while\nmaintaining model-measured perplexity $<33$; we achieve $95$% attack success\nfor Phi-3, though with higher perplexity. We also find a universally-optimized\nsingle fluent prompt that induces $>88$% compliance on previously unseen tasks\nacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box\nmodels.", "published": "2024-07-24 17:23:18", "link": "http://arxiv.org/abs/2407.17447v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models", "abstract": "Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Considering the balance between efficiency and\neffectiveness, CMR can be regarded as the optimal mixture ratio. Through\nextensive experiments, we ascertain the predictability of CMR, propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.", "published": "2024-07-24 17:59:02", "link": "http://arxiv.org/abs/2407.17467v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WildHallucinations: Evaluating Long-form Factuality in LLMs with\n  Real-World Entity Queries", "abstract": "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.", "published": "2024-07-24 17:59:05", "link": "http://arxiv.org/abs/2407.17468v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "I Could've Asked That: Reformulating Unanswerable Questions", "abstract": "When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.", "published": "2024-07-24 17:59:07", "link": "http://arxiv.org/abs/2407.17469v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Aligning Language Models with Textual Feedback", "abstract": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.", "published": "2024-07-24 03:32:05", "link": "http://arxiv.org/abs/2407.16970v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Voter-Based Stochastic Rejection-Method Framework for Asymptotically\n  Safe Language Model Outputs", "abstract": "This paper proposes a new method for preventing unsafe or otherwise low\nquality large language model (LLM) outputs, by leveraging the stochasticity of\nLLMs. We propose a system whereby LLM checkers vote on the acceptability of a\ngenerated output, regenerating it if a threshold of disapproval is reached,\nuntil sufficient checkers approve. We further propose estimators for cost and\nfailure rate, and based on those estimators and experimental data tailored to\nthe application, we propose an algorithm that achieves a desired failure rate\nat the least possible cost. We demonstrate that, under these models, failure\nrate decreases exponentially as a function of cost when voter count and\nthreshold are chosen according to the algorithm, and that the models reasonably\nestimate the actual performance of such a system in action, even with limited\ndata.", "published": "2024-07-24 04:27:55", "link": "http://arxiv.org/abs/2407.16994v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "High Efficiency Image Compression for Large Visual-Language Models", "abstract": "In recent years, large visual language models (LVLMs) have shown impressive\nperformance and promising generalization capability in multi-modal tasks, thus\nreplacing humans as receivers of visual information in various application\nscenarios. In this paper, we pioneer to propose a variable bitrate image\ncompression framework consisting of a pre-editing module and an end-to-end\ncodec to achieve promising rate-accuracy performance for different LVLMs. In\nparticular, instead of optimizing an adaptive pre-editing network towards a\nparticular task or several representative tasks, we propose a new optimization\nstrategy tailored for LVLMs, which is designed based on the representation and\ndiscrimination capability with token-level distortion and rank. The pre-editing\nmodule and the variable bitrate end-to-end image codec are jointly trained by\nthe losses based on semantic tokens of the large model, which introduce\nenhanced generalization capability for various data and tasks. {Experimental\nresults demonstrate that the proposed framework could efficiently achieve much\nbetter rate-accuracy performance compared to the state-of-the-art coding\nstandard, Versatile Video Coding.} Meanwhile, experiments with multi-modal\ntasks have revealed the robustness and generalization capability of the\nproposed framework.", "published": "2024-07-24 07:37:12", "link": "http://arxiv.org/abs/2407.17060v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "primary_category": "cs.CV"}
{"title": "A Survey Forest Diagram : Gain a Divergent Insight View on a Specific\n  Research Topic", "abstract": "With the exponential growth in the number of papers and the trend of AI\nresearch, the use of Generative AI for information retrieval and\nquestion-answering has become popular for conducting research surveys. However,\nnovice researchers unfamiliar with a particular field may not significantly\nimprove their efficiency in interacting with Generative AI because they have\nnot developed divergent thinking in that field. This study aims to develop an\nin-depth Survey Forest Diagram that guides novice researchers in divergent\nthinking about the research topic by indicating the citation clues among\nmultiple papers, to help expand the survey perspective for novice researchers.", "published": "2024-07-24 08:17:37", "link": "http://arxiv.org/abs/2407.17081v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot vs. Few-Shot Multi-Speaker TTS Using Pre-trained Czech\n  SpeechT5 Model", "abstract": "In this paper, we experimented with the SpeechT5 model pre-trained on\nlarge-scale datasets. We pre-trained the foundation model from scratch and\nfine-tuned it on a large-scale robust multi-speaker text-to-speech (TTS) task.\nWe tested the model capabilities in a zero- and few-shot scenario. Based on two\nlistening tests, we evaluated the synthetic audio quality and the similarity of\nhow synthetic voices resemble real voices. Our results showed that the SpeechT5\nmodel can generate a synthetic voice for any speaker using only one minute of\nthe target speaker's data. We successfully demonstrated the high quality and\nsimilarity of our synthetic voices on publicly known Czech politicians and\ncelebrities.", "published": "2024-07-24 11:14:06", "link": "http://arxiv.org/abs/2407.17167v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Editing -- a Summary", "abstract": "With the rise of video production and social media, speech editing has become\ncrucial for creators to address issues like mispronunciations, missing words,\nor stuttering in audio recordings. This paper explores text-based speech\nediting methods that modify audio via text transcripts without manual waveform\nediting. These approaches ensure edited audio is indistinguishable from the\noriginal by altering the mel-spectrogram. Recent advancements, such as\ncontext-aware prosody correction and advanced attention mechanisms, have\nimproved speech editing quality. This paper reviews state-of-the-art methods,\ncompares key metrics, and examines widely used datasets. The aim is to\nhighlight ongoing issues and inspire further research and innovation in speech\nediting.", "published": "2024-07-24 11:22:57", "link": "http://arxiv.org/abs/2407.17172v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?", "abstract": "In this study, we address the growing issue of misleading charts, a prevalent\nproblem that undermines the integrity of information dissemination. Misleading\ncharts can distort the viewer's perception of data, leading to\nmisinterpretations and decisions based on false information. The development of\neffective automatic detection methods for misleading charts is an urgent field\nof research. The recent advancement of multimodal Large Language Models (LLMs)\nhas introduced a promising direction for addressing this challenge. We explored\nthe capabilities of these models in analyzing complex charts and assessing the\nimpact of different prompting strategies on the models' analyses. We utilized a\ndataset of misleading charts collected from the internet by prior research and\ncrafted nine distinct prompts, ranging from simple to complex, to test the\nability of four different multimodal LLMs in detecting over 21 different chart\nissues. Through three experiments--from initial exploration to detailed\nanalysis--we progressively gained insights into how to effectively prompt LLMs\nto identify misleading charts and developed strategies to address the\nscalability challenges encountered as we expanded our detection range from the\ninitial five issues to 21 issues in the final experiment. Our findings reveal\nthat multimodal LLMs possess a strong capability for chart comprehension and\ncritical thinking in data interpretation. There is significant potential in\nemploying multimodal LLMs to counter misleading information by supporting\ncritical thinking and enhancing visualization literacy. This study demonstrates\nthe applicability of LLMs in addressing the pressing concern of misleading\ncharts.", "published": "2024-07-24 14:02:20", "link": "http://arxiv.org/abs/2407.17291v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.HC"}
{"title": "A Comprehensive Approach to Misspelling Correction with BERT and\n  Levenshtein Distance", "abstract": "Writing, as an omnipresent form of human communication, permeates nearly\nevery aspect of contemporary life. Consequently, inaccuracies or errors in\nwritten communication can lead to profound consequences, ranging from financial\nlosses to potentially life-threatening situations. Spelling mistakes, among the\nmost prevalent writing errors, are frequently encountered due to various\nfactors. This research aims to identify and rectify diverse spelling errors in\ntext using neural networks, specifically leveraging the Bidirectional Encoder\nRepresentations from Transformers (BERT) masked language model. To achieve this\ngoal, we compiled a comprehensive dataset encompassing both non-real-word and\nreal-word errors after categorizing different types of spelling mistakes.\nSubsequently, multiple pre-trained BERT models were employed. To ensure optimal\nperformance in correcting misspelling errors, we propose a combined approach\nutilizing the BERT masked language model and Levenshtein distance. The results\nfrom our evaluation data demonstrate that the system presented herein exhibits\nremarkable capabilities in identifying and rectifying spelling mistakes, often\nsurpassing existing systems tailored for the Persian language.", "published": "2024-07-24 16:07:11", "link": "http://arxiv.org/abs/2407.17383v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Anomaly Detection in Computational Workflows:\n  from Supervised Fine-Tuning to In-Context Learning", "abstract": "Anomaly detection in computational workflows is critical for ensuring system\nreliability and security. However, traditional rule-based methods struggle to\ndetect novel anomalies. This paper leverages large language models (LLMs) for\nworkflow anomaly detection by exploiting their ability to learn complex data\npatterns. Two approaches are investigated: 1) supervised fine-tuning (SFT),\nwhere pre-trained LLMs are fine-tuned on labeled data for sentence\nclassification to identify anomalies, and 2) in-context learning (ICL) where\nprompts containing task descriptions and examples guide LLMs in few-shot\nanomaly detection without fine-tuning. The paper evaluates the performance,\nefficiency, generalization of SFT models, and explores zero-shot and few-shot\nICL prompts and interpretability enhancement via chain-of-thought prompting.\nExperiments across multiple workflow datasets demonstrate the promising\npotential of LLMs for effective anomaly detection in complex executions.", "published": "2024-07-24 16:33:04", "link": "http://arxiv.org/abs/2407.17545v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Exploring Domain Robust Lightweight Reward Models based on Router\n  Mechanism", "abstract": "Recent advancements in large language models have heavily relied on the large\nreward model from reinforcement learning from human feedback for fine-tuning.\nHowever, the use of a single reward model across various domains may not always\nbe optimal, often requiring retraining from scratch when new domain data is\nintroduced. To address these challenges, we explore the utilization of small\nlanguage models operating in a domain-specific manner based on router\nmechanisms. Our three approaches are: 1) utilize mixture of experts to form a\nsingle reward model by modularizing an internal router and experts, 2)\nemploying external router to select the appropriate reward model from multiple\ndomain-specific models, and 3) the framework reduces parameter size by loading\nreward models and router adapters onto a single small language model using\nadapters. Experimental validation underscores the effectiveness of our\napproach, demonstrating performance comparable to baseline methods while also\nreducing the total parameter size.", "published": "2024-07-24 17:25:12", "link": "http://arxiv.org/abs/2407.17546v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Coupling Speech Encoders with Downstream Text Models", "abstract": "We present a modular approach to building cascade speech translation (AST)\nmodels that guarantees that the resulting model performs no worse than the\n1-best cascade baseline while preserving state-of-the-art speech recognition\n(ASR) and text translation (MT) performance for a given task. Our novel\ncontribution is the use of an ``exporter'' layer that is trained under L2-loss\nto ensure a strong match between ASR embeddings and the MT token embeddings for\nthe 1-best sequence. The ``exporter'' output embeddings are fed directly to the\nMT model in lieu of 1-best token embeddings, thus guaranteeing that the\nresulting model performs no worse than the 1-best cascade baseline, while\nallowing back-propagation gradient to flow from the MT model into the ASR\ncomponents. The matched-embeddings cascade architecture provide a significant\nimprovement over its 1-best counterpart in scenarios where incremental training\nof the MT model is not an option and yet we seek to improve quality by\nleveraging (speech, transcription, translated transcription) data provided with\nthe AST task. The gain disappears when the MT model is incrementally trained on\nthe parallel text data available with the AST task. The approach holds promise\nfor other scenarios that seek to couple ASR encoders and immutable text models,\nsuch at large language models (LLM).", "published": "2024-07-24 19:29:13", "link": "http://arxiv.org/abs/2407.17605v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Forecasting Credit Ratings: A Case Study where Traditional Methods\n  Outperform Generative LLMs", "abstract": "Large Language Models (LLMs) have been shown to perform well for many\ndownstream tasks. Transfer learning can enable LLMs to acquire skills that were\nnot targeted during pre-training. In financial contexts, LLMs can sometimes\nbeat well-established benchmarks. This paper investigates how well LLMs perform\nin the task of forecasting corporate credit ratings. We show that while LLMs\nare very good at encoding textual information, traditional methods are still\nvery competitive when it comes to encoding numeric and multimodal data. For our\ntask, current LLMs perform worse than a more traditional XGBoost architecture\nthat combines fundamental and macroeconomic data with high-density text-based\nembedding features.", "published": "2024-07-24 20:30:55", "link": "http://arxiv.org/abs/2407.17624v2", "categories": ["q-fin.RM", "cs.CL", "q-fin.GN"], "primary_category": "q-fin.RM"}
{"title": "What Matters in Explanations: Towards Explainable Fake Review Detection\n  Focusing on Transformers", "abstract": "Customers' reviews and feedback play crucial role on electronic\ncommerce~(E-commerce) platforms like Amazon, Zalando, and eBay in influencing\nother customers' purchasing decisions. However, there is a prevailing concern\nthat sellers often post fake or spam reviews to deceive potential customers and\nmanipulate their opinions about a product. Over the past decade, there has been\nconsiderable interest in using machine learning (ML) and deep learning (DL)\nmodels to identify such fraudulent reviews. Unfortunately, the decisions made\nby complex ML and DL models - which often function as \\emph{black-boxes} - can\nbe surprising and difficult for general users to comprehend. In this paper, we\npropose an explainable framework for detecting fake reviews with high precision\nin identifying fraudulent content with explanations and investigate what\ninformation matters most for explaining particular decisions by conducting\nempirical user evaluation. Initially, we develop fake review detection models\nusing DL and transformer models including XLNet and DistilBERT. We then\nintroduce layer-wise relevance propagation (LRP) technique for generating\nexplanations that can map the contributions of words toward the predicted\nclass. The experimental results on two benchmark fake review detection datasets\ndemonstrate that our predictive models achieve state-of-the-art performance and\noutperform several existing methods. Furthermore, the empirical user evaluation\nof the generated explanations concludes which important information needs to be\nconsidered in generating explanations in the context of fake review\nidentification.", "published": "2024-07-24 13:26:02", "link": "http://arxiv.org/abs/2407.21056v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Reporting and Analysing the Environmental Impact of Language Models on\n  the Example of Commonsense Question Answering with External Knowledge", "abstract": "Human-produced emissions are growing at an alarming rate, causing already\nobservable changes in the climate and environment in general. Each year global\ncarbon dioxide emissions hit a new record, and it is reported that 0.5% of\ntotal US greenhouse gas emissions are attributed to data centres as of 2021.\nThe release of ChatGPT in late 2022 sparked social interest in Large Language\nModels (LLMs), the new generation of Language Models with a large number of\nparameters and trained on massive amounts of data. Currently, numerous\ncompanies are releasing products featuring various LLMs, with many more models\nin development and awaiting release. Deep Learning research is a competitive\nfield, with only models that reach top performance attracting attention and\nbeing utilized. Hence, achieving better accuracy and results is often the first\npriority, while the model's efficiency and the environmental impact of the\nstudy are neglected. However, LLMs demand substantial computational resources\nand are very costly to train, both financially and environmentally. It becomes\nessential to raise awareness and promote conscious decisions about algorithmic\nand hardware choices. Providing information on training time, the approximate\ncarbon dioxide emissions and power consumption would assist future studies in\nmaking necessary adjustments and determining the compatibility of available\ncomputational resources with model requirements. In this study, we infused T5\nLLM with external knowledge and fine-tuned the model for Question-Answering\ntask. Furthermore, we calculated and reported the approximate environmental\nimpact for both steps. The findings demonstrate that the smaller models may not\nalways be sustainable options, and increased training does not always imply\nbetter performance. The most optimal outcome is achieved by carefully\nconsidering both performance and efficiency factors.", "published": "2024-07-24 16:16:16", "link": "http://arxiv.org/abs/2408.01453v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Sentiment Reasoning for Healthcare", "abstract": "Transparency in AI healthcare decision-making is crucial for building trust\namong AI and users. Incorporating reasoning capabilities enables Large Language\nModels (LLMs) to understand emotions in context, handle nuanced language, and\ninfer unstated sentiments. In this work, we introduce a new task -- Sentiment\nReasoning -- for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Sentiment Reasoning is an auxiliary\ntask in sentiment analysis where the model predicts both the sentiment label\nand generates the rationale behind it based on the input transcript. Our study\nconducted on both human transcripts and Automatic Speech Recognition (ASR)\ntranscripts shows that Sentiment Reasoning helps improve model transparency by\nproviding rationale for model prediction with quality semantically comparable\nto humans while also improving model performance (1% increase in both accuracy\nand macro-F1) via rationale-augmented fine-tuning. Also, no significant\ndifference in the semantic quality of generated rationales between human and\nASR transcripts. All code, data (English-translated and Vietnamese) and models\nare published online: https://github.com/leduckhai/MultiMed.", "published": "2024-07-24 12:07:54", "link": "http://arxiv.org/abs/2407.21054v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Based Ensemble Learning For Speech Classification", "abstract": "Speech classification has attracted increasing attention due to its wide\napplications, particularly in classifying physical and mental states. However,\nthese tasks are challenging due to the high variability in speech signals.\nEnsemble learning has shown promising results when multiple classifiers are\ncombined to improve performance. With recent advancements in hardware\ndevelopment, combining several models is not a limitation in deep learning\nresearch and applications. In this paper, we propose an uncertainty-based\nensemble learning approach for speech classification. Specifically, we train a\nset of base features on the same classifier and quantify the uncertainty of\ntheir predictions. The predictions are combined using variants of uncertainty\ncalculation to produce the final prediction. The visualization of the effect of\nuncertainty and its ensemble learning results show potential improvements in\nspeech classification tasks. The proposed method outperforms single models and\nconventional ensemble learning methods in terms of unweighted accuracy or\nweighted accuracy.", "published": "2024-07-24 05:13:06", "link": "http://arxiv.org/abs/2407.17009v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Comprehensive Review and Taxonomy of Audio-Visual Synchronization\n  Techniques for Realistic Speech Animation", "abstract": "In many applications, synchronizing audio with visuals is crucial, such as in\ncreating graphic animations for films or games, translating movie audio into\ndifferent languages, and developing metaverse applications. This review\nexplores various methodologies for achieving realistic facial animations from\naudio inputs, highlighting generative and adaptive models. Addressing\nchallenges like model training costs, dataset availability, and silent moment\ndistributions in audio data, it presents innovative solutions to enhance\nperformance and realism. The research also introduces a new taxonomy to\ncategorize audio-visual synchronization methods based on logistical aspects,\nadvancing the capabilities of virtual assistants, gaming, and interactive\ndigital media.", "published": "2024-07-24 17:05:46", "link": "http://arxiv.org/abs/2407.17430v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Automatic Detection and Annotation of Sperm Whale Codas", "abstract": "A key technology in sperm whale (Physeter macrocephalus) monitoring is the\nidentification of sperm whale communication signals, known as codas. In this\npaper we present the first automatic coda detector and annotator. The main\ninnovation in our detector is graph-based clustering, which utilizes the\nexpected similarity between the clicks that make up the coda. Results show\ndetection and accurate annotation at low signal-to-noise ratios, separation\nbetween codas and echolocation clicks, and discrimination between codas from\nsimultaneously emitting whales. Using this automatic annotator, insights into\nthe characterization of sperm whale communication are presented. The results\ninclude new types of coda signals, analyzes of the distribution of coda types\namong different whales and for different years, and evidence for\nsynchronization between communicating whales in terms of coda type and coda\ntransmission time. These results indicate a high degree of complexity in the\ncommunication system of this cetacean species. To ensure traceability, we share\nthe implementation code of our coda detector.", "published": "2024-07-24 09:29:03", "link": "http://arxiv.org/abs/2407.17119v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reduction of Nonlinear Distortion in Condenser Microphones Using a\n  Simple Post-Processing Technique", "abstract": "In this paper, we introduce a novel approach for effectively reducing\nnonlinear distortion in single back-plate condenser microphones, i.e., most\nMEMS microphones, studio recording condenser microphones, and laboratory\nmeasurement microphones. This simple post-processing technique can be easily\nintegrated on an external hardware such as an analog circuit, microcontroller,\naudio codec, DSP unit, or within the ASIC chip in a case of MEMS microphones.\nIt significantly reduces microphone distortion across its frequency and dynamic\nrange. It relies on a single parameter, which can be derived from either the\nmicrophone's physical parameters or a straightforward measurement presented in\nthis paper. An optimal estimate of this parameter achieves the best distortion\nreduction, whereas overestimating it never increases distortion beyond the\noriginal level. The technique was tested on a MEMS microphone. Our findings\nindicate that for harmonic excitation the proposed technique reduces the second\nharmonic by approximately 40 dB, leading to a significant reduction in the\nTotal Harmonic Distortion (THD). The efficiency of the distortion reduction\ntechnique for more complex signals is demonstrated through two-tone and\nmultitone experiments, where second-order intermodulation products are reduced\nby at least 20 dB.", "published": "2024-07-24 13:11:01", "link": "http://arxiv.org/abs/2407.17250v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Long-Term, Store-Front Robotics: Interactive Music for Robotic Arm,\n  Caxixi and Frame Drums", "abstract": "This paper presents an innovative exploration into the integration of\ninteractive robotic musicianship within a commercial retail environment,\nspecifically through a three-week-long in-store installation featuring a UR3\nrobotic arm, custom-built frame drums, and an adaptive music generation system.\nSituated in a prominent storefront in one of the world's largest cities, this\nproject aimed to enhance the shopping experience by creating dynamic, engaging\nmusical interactions that respond to the store's ambient soundscape. Key\ncontributions include the novel application of industrial robotics in artistic\nexpression, the deployment of interactive music to enrich retail ambiance, and\nthe demonstration of continuous robotic operation in a public setting over an\nextended period. Challenges such as system reliability, variation in musical\noutput, safety in interactive contexts, and brand alignment were addressed to\nensure the installation's success. The project not only showcased the technical\nfeasibility and artistic potential of robotic musicianship in retail spaces but\nalso offered insights into the practical implications of such integration,\nincluding system reliability, the dynamics of human-robot interaction, and the\nimpact on store operations. This exploration opens new avenues for enhancing\nconsumer retail experiences through the intersection of technology, music, and\ninteractive art, suggesting a future where robotic musicianship contributes\nmeaningfully to public and commercial spaces.", "published": "2024-07-24 02:44:45", "link": "http://arxiv.org/abs/2407.16956v1", "categories": ["cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Collaboration Between Robots, Interfaces and Humans: Practice-Based and\n  Audience Perspectives", "abstract": "This paper provides an analysis of a mixed-media experimental musical work\nthat explores the integration of human musical interaction with a newly\ndeveloped interface for the violin, manipulated by an improvising violinist,\ninteractive visuals, a robotic drummer and an improvised synthesised orchestra.\nWe first present a detailed technical overview of the systems involved\nincluding the design and functionality of each component. We then conduct a\npractice-based review examining the creative processes and artistic decisions\nunderpinning the work, focusing on the challenges and breakthroughs encountered\nduring its development. Through this introspective analysis, we uncover\ninsights into the collaborative dynamics between the human performer and\ntechnological agents, revealing the complexities of blending traditional\nmusical expressiveness with artificial intelligence and robotics. To gauge\npublic reception and interpretive perspectives, we conducted an online survey,\nsharing a video of the performance with a diverse audience. The feedback\ncollected from this survey offers valuable viewpoints on the accessibility,\nemotional impact, and perceived artistic value of the work. Respondents'\nreactions underscore the transformative potential of integrating advanced\ntechnologies in musical performance, while also highlighting areas for further\nexploration and refinement.", "published": "2024-07-24 03:14:05", "link": "http://arxiv.org/abs/2407.16966v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Improved symbolic drum style classification with grammar-based\n  hierarchical representations", "abstract": "Deep learning models have become a critical tool for analysis and\nclassification of musical data. These models operate either on the audio\nsignal, e.g. waveform or spectrogram, or on a symbolic representation, such as\nMIDI. In the latter, musical information is often reduced to basic features,\ni.e. durations, pitches and velocities. Most existing works then rely on\ngeneric tokenization strategies from classical natural language processing, or\nmatrix representations, e.g. piano roll. In this work, we evaluate how enriched\nrepresentations of symbolic data can impact deep models, i.e. Transformers and\nRNN, for music style classification. In particular, we examine representations\nthat explicitly incorporate musical information implicitly present in MIDI-like\nencodings, such as rhythmic organization, and show that they outperform generic\ntokenization strategies. We introduce a new tree-based representation of MIDI\ndata built upon a context-free musical grammar. We show that this grammar\nrepresentation accurately encodes high-level rhythmic information and\noutperforms existing encodings on the GrooveMIDI Dataset for drumming style\nclassification, while being more compact and parameter-efficient.", "published": "2024-07-24 07:32:26", "link": "http://arxiv.org/abs/2407.17536v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
