{"title": "PASTA: A Dataset for Modeling Participant States in Narratives", "abstract": "The events in a narrative are understood as a coherent whole via the\nunderlying states of their participants. Often, these participant states are\nnot explicitly mentioned, instead left to be inferred by the reader. A model\nthat understands narratives should likewise infer these implicit states, and\neven reason about the impact of changes to these states on the narrative. To\nfacilitate this goal, we introduce a new crowdsourced English-language,\nParticipant States dataset, PASTA. This dataset contains inferable participant\nstates; a counterfactual perturbation to each state; and the changes to the\nstory that would be necessary if the counterfactual were true. We introduce\nthree state-based reasoning tasks that test for the ability to infer when a\nstate is entailed by a story, to revise a story conditioned on a counterfactual\nstate, and to explain the most likely state change given a revised story.\nExperiments show that today's LLMs can reason about states to some degree, but\nthere is large room for improvement, especially in problems requiring access\nand ability to reason with diverse types of knowledge (e.g. physical,\nnumerical, factual).", "published": "2022-07-31 01:21:48", "link": "http://arxiv.org/abs/2208.00329v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Who in Code-Switching: A Case Study for Predicting Egyptian\n  Arabic-English Code-Switching Levels based on Character Profiles", "abstract": "Code-switching (CS) is a common linguistic phenomenon exhibited by\nmultilingual individuals, where they tend to alternate between languages within\none single conversation. CS is a complex phenomenon that not only encompasses\nlinguistic challenges, but also contains a great deal of complexity in terms of\nits dynamic behaviour across speakers. Given that the factors giving rise to CS\nvary from one country to the other, as well as from one person to the other, CS\nis found to be a speaker-dependant behaviour, where the frequency by which the\nforeign language is embedded differs across speakers. While several researchers\nhave looked into predicting CS behaviour from a linguistic point of view,\nresearch is still lacking in the task of predicting user CS behaviour from\nsociological and psychological perspectives. We provide an empirical user\nstudy, where we investigate the correlations between users' CS levels and\ncharacter traits. We conduct interviews with bilinguals and gather information\non their profiles, including their demographics, personality traits, and\ntraveling experiences. We then use machine learning (ML) to predict users' CS\nlevels based on their profiles, where we identify the main influential factors\nin the modeling process. We experiment with both classification as well as\nregression tasks. Our results show that the CS behaviour is affected by the\nrelation between speakers, travel experiences as well as Neuroticism and\nExtraversion personality traits.", "published": "2022-07-31 13:47:35", "link": "http://arxiv.org/abs/2208.00433v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mismatching-Aware Unsupervised Translation Quality Estimation For\n  Low-Resource Languages", "abstract": "Translation Quality Estimation (QE) is the task of predicting the quality of\nmachine translation (MT) output without any reference. This task has gained\nincreasing attention as an important component in the practical applications of\nMT. In this paper, we first propose XLMRScore, which is a cross-lingual\ncounterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric\ncan be used as a simple unsupervised QE method, nevertheless facing two issues:\nfirstly, the untranslated tokens leading to unexpectedly high translation\nscores, and secondly, the issue of mismatching errors between source and\nhypothesis tokens when applying the greedy matching in XLMRScore. To mitigate\nthese issues, we suggest replacing untranslated words with the unknown token\nand the cross-lingual alignment of the pre-trained model to represent aligned\nwords closer to each other, respectively. We evaluate the proposed method on\nfour low-resource language pairs of the WMT21 QE shared task, as well as a new\nEnglish$\\rightarrow$Persian (En-Fa) test dataset introduced in this paper.\nExperiments show that our method could get comparable results with the\nsupervised baseline for two zero-shot scenarios, i.e., with less than 0.01\ndifference in Pearson correlation, while outperforming unsupervised rivals in\nall the low-resource language pairs for above 8%, on average.", "published": "2022-07-31 16:23:23", "link": "http://arxiv.org/abs/2208.00463v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Distantly Supervised Relation Extraction by Natural Language\n  Inference", "abstract": "To reduce human annotations for relation extraction (RE) tasks, distantly\nsupervised approaches have been proposed, while struggling with low\nperformance. In this work, we propose a novel DSRE-NLI framework, which\nconsiders both distant supervision from existing knowledge bases and indirect\nsupervision from pretrained language models for other tasks. DSRE-NLI energizes\nan off-the-shelf natural language inference (NLI) engine with a semi-automatic\nrelation verbalization (SARV) mechanism to provide indirect supervision and\nfurther consolidates the distant annotations to benefit multi-classification RE\nmodels. The NLI-based indirect supervision acquires only one relation\nverbalization template from humans as a semantically general template for each\nrelationship, and then the template set is enriched by high-quality textual\npatterns automatically mined from the distantly annotated corpus. With two\nsimple and effective data consolidation strategies, the quality of training\ndata is substantially improved. Extensive experiments demonstrate that the\nproposed framework significantly improves the SOTA performance (up to 7.73\\% of\nF1) on distantly supervised RE benchmark datasets.", "published": "2022-07-31 02:48:34", "link": "http://arxiv.org/abs/2208.00346v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chinese grammatical error correction based on knowledge distillation", "abstract": "In view of the poor robustness of existing Chinese grammatical error\ncorrection models on attack test sets and large model parameters, this paper\nuses the method of knowledge distillation to compress model parameters and\nimprove the anti-attack ability of the model. In terms of data, the attack test\nset is constructed by integrating the disturbance into the standard evaluation\ndata set, and the model robustness is evaluated by the attack test set. The\nexperimental results show that the distilled small model can ensure the\nperformance and improve the training speed under the condition of reducing the\nnumber of model parameters, and achieve the optimal effect on the attack test\nset, and the robustness is significantly improved. Code is available at\nhttps://github.com/Richard88888/KD-CGEC.", "published": "2022-07-31 03:16:29", "link": "http://arxiv.org/abs/2208.00351v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Chatbots to Teach Languages", "abstract": "This paper reports on progress towards building an online language learning\ntool to provide learners with conversational experience by using dialog systems\nas conversation practice partners. Our system can adapt to users' language\nproficiency on the fly. We also provide automatic grammar error feedback to\nhelp users learn from their mistakes. According to our first adopters, our\nsystem is entertaining and useful. Furthermore, we will provide the learning\ntechnology community a large-scale conversation dataset on language learning\nand grammar correction. Our next step is to make our system more adaptive to\nuser profile information by using reinforcement learning algorithms.", "published": "2022-07-31 07:01:35", "link": "http://arxiv.org/abs/2208.00376v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Knowledge Bank for Pretrained Transformers", "abstract": "The ability of pretrained Transformers to remember factual knowledge is\nessential but still limited for existing models. Inspired by existing work that\nregards Feed-Forward Networks (FFNs) in Transformers as key-value memories, we\ndesign a Neural Knowledge Bank (NKB) and a knowledge injection strategy to\nintroduce extra factual knowledge for pretrained Transformers. The NKB is in\nthe form of additional knowledgeable memory slots to the FFN and the\nmemory-like architecture makes it highly interpretable and flexible. When\ninjecting extra knowledge with the Salient Span Masking (SSM) pretraining\nobjective, we fix the original pretrained model and train only the NKB. This\ntraining strategy makes sure the general language modeling ability of the\noriginal pretrained model is not influenced. By mounting the NKB onto the T5\nmodel, we verify its strong ability to store extra factual knowledge based on\nthree closed-book question answering datasets. Also, we prove that mounting the\nNKB will not degrade the general language modeling ability of T5 through two\nrepresentative tasks, summarization and machine translation. Further, we\nthoroughly analyze the interpretability of the NKB and reveal the meaning of\nits keys and values in a human-readable way. Finally, we show the flexibility\nof the NKB by directly modifying its value vectors to update the factual\nknowledge stored in it.", "published": "2022-07-31 09:14:34", "link": "http://arxiv.org/abs/2208.00399v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The impact of Twitter on political influence on the choice of a running\n  mate: Social Network Analysis and Semantic Analysis -- A Review", "abstract": "In this new era of social media, social networks are becoming increasingly\nimportant sources of user-generated content on the internet. These kinds of\ninformation resources, which include a lot of people's feelings, opinions,\nfeedback, and reviews, are very useful for big businesses, markets, politics,\njournalism, and many other fields. Politics is one of the most talked-about and\npopular topics on social media networks right now. Many politicians use\nmicro-blogging services like Twitter because they have a large number of\nfollowers and supporters on those networks. Politicians, political parties,\npolitical organizations, and foundations use social media networks to\ncommunicate with citizens ahead of time. Today, social media is used by\nhundreds of thousands of political groups and politicians. On these social\nmedia networks, every politician and political party has millions of followers,\nand politicians find new and innovative ways to urge individuals to participate\nin politics. Furthermore, social media assists politicians in various\ndecision-making processes by providing recommendations, such as developing\npolicies and strategies based on previous experiences, recommending and\nselecting suitable candidates for a particular constituency, recommending a\nsuitable person for a particular position in the party, and launching a\npolitical campaign based on citizen sentiments on various issues and\ncontroversies, among other things. This research is a review on the use of\nsocial network analysis (SNA) and semantic analysis (SA) on the Twitter\nplatform to study the supporters networks of political leaders because it can\nhelp in decision-making when predicting their political futures.", "published": "2022-07-31 17:44:57", "link": "http://arxiv.org/abs/2208.00479v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Building an Efficiency Pipeline: Commutativity and Cumulativeness of\n  Efficiency Operators for Transformers", "abstract": "There exists a wide variety of efficiency methods for natural language\nprocessing (NLP) tasks, such as pruning, distillation, dynamic inference,\nquantization, etc. We can consider an efficiency method as an operator applied\non a model. Naturally, we may construct a pipeline of multiple efficiency\nmethods, i.e., to apply multiple operators on the model sequentially. In this\npaper, we study the plausibility of this idea, and more importantly, the\ncommutativity and cumulativeness of efficiency operators. We make two\ninteresting observations: (1) Efficiency operators are commutative -- the order\nof efficiency methods within the pipeline has little impact on the final\nresults; (2) Efficiency operators are also cumulative -- the final results of\ncombining several efficiency methods can be estimated by combining the results\nof individual methods. These observations deepen our understanding of\nefficiency operators and provide useful guidelines for their real-world\napplications.", "published": "2022-07-31 18:01:06", "link": "http://arxiv.org/abs/2208.00483v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
