{"title": "Does Putting a Linguist in the Loop Improve NLU Data Collection?", "abstract": "Many crowdsourced NLP datasets contain systematic gaps and biases that are\nidentified only after data collection is complete. Identifying these issues\nfrom early data samples during crowdsourcing should make mitigation more\nefficient, especially when done iteratively. We take natural language inference\nas a test case and ask whether it is beneficial to put a linguist `in the loop'\nduring data collection to dynamically identify and address gaps in the data by\nintroducing novel constraints on the task. We directly compare three data\ncollection protocols: (i) a baseline protocol, (ii) a linguist-in-the-loop\nintervention with iteratively-updated constraints on the task, and (iii) an\nextension of linguist-in-the-loop that provides direct interaction between\nlinguists and crowdworkers via a chatroom. The datasets collected with linguist\ninvolvement are more reliably challenging than baseline, without loss of\nquality. But we see no evidence that using this data in training leads to\nbetter out-of-domain model performance, and the addition of a chat platform has\nno measurable effect on the resulting dataset. We suggest integrating expert\nanalysis \\textit{during} data collection so that the expert can dynamically\naddress gaps and biases in the dataset.", "published": "2021-04-15 00:31:10", "link": "http://arxiv.org/abs/2104.07179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese\n  Pre-trained Language Models", "abstract": "Chinese pre-trained language models usually process text as a sequence of\ncharacters, while ignoring more coarse granularity, e.g., words. In this work,\nwe propose a novel pre-training paradigm for Chinese -- Lattice-BERT, which\nexplicitly incorporates word representations along with characters, thus can\nmodel a sentence in a multi-granularity manner. Specifically, we construct a\nlattice graph from the characters and words in a sentence and feed all these\ntext units into transformers. We design a lattice position attention mechanism\nto exploit the lattice structures in self-attention layers. We further propose\na masked segment prediction task to push the model to learn from rich but\nredundant information inherent in lattices, while avoiding learning unexpected\ntricks. Experiments on 11 Chinese natural language understanding tasks show\nthat our model can bring an average increase of 1.5% under the 12-layer\nsetting, which achieves new state-of-the-art among base-size models on the CLUE\nbenchmarks. Further analysis shows that Lattice-BERT can harness the lattice\nstructures, and the improvement comes from the exploration of redundant\ninformation and multi-granularity representations. Our code will be available\nat https://github.com/alibaba/pretrained-language-models/LatticeBERT.", "published": "2021-04-15 02:36:49", "link": "http://arxiv.org/abs/2104.07204v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RefSum: Refactoring Neural Summarization", "abstract": "Although some recent works show potential complementarity among different\nstate-of-the-art systems, few works try to investigate this problem in text\nsummarization. Researchers in other areas commonly refer to the techniques of\nreranking or stacking to approach this problem. In this work, we highlight\nseveral limitations of previous methods, which motivates us to present a new\nframework Refactor that provides a unified view of text summarization and\nsummaries combination. Experimentally, we perform a comprehensive evaluation\nthat involves twenty-two base systems, four datasets, and three different\napplication scenarios. Besides new state-of-the-art results on CNN/DailyMail\ndataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses\nthe limitations of the traditional methods and the effectiveness of the\nRefactor model sheds light on insight for performance improvement. Our system\ncan be directly used by other researchers as an off-the-shelf tool to achieve\nfurther performance improvements. We open-source all the code and provide a\nconvenient interface to use it:\nhttps://github.com/yixinL7/Refactoring-Summarization. We have also made the\ndemo of this work available at:\nhttp://explainaboard.nlpedia.ai/leaderboard/task-summ/index.php.", "published": "2021-04-15 02:58:41", "link": "http://arxiv.org/abs/2104.07210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Sequence Segmentation as Determining the Leftmost Segments", "abstract": "Prior methods to text segmentation are mostly at token level. Despite the\nadequacy, this nature limits their full potential to capture the long-term\ndependencies among segments. In this work, we propose a novel framework that\nincrementally segments natural language sentences at segment level. For every\nstep in segmentation, it recognizes the leftmost segment of the remaining\nsequence. Implementations involve LSTM-minus technique to construct the phrase\nrepresentations and recurrent neural networks (RNN) to model the iterations of\ndetermining the leftmost segments. We have conducted extensive experiments on\nsyntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets,\ndemonstrating that our methods have significantly outperformed previous all\nbaselines and achieved new state-of-the-art results. Moreover, qualitative\nanalysis and the study on segmenting long-length sentences verify its\neffectiveness in modeling long-term dependencies.", "published": "2021-04-15 03:35:03", "link": "http://arxiv.org/abs/2104.07217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dual-Questioning Attention Network for Emotion-Cause Pair Extraction\n  with Context Awareness", "abstract": "Emotion-cause pair extraction (ECPE), an emerging task in sentiment analysis,\naims at extracting pairs of emotions and their corresponding causes in\ndocuments. This is a more challenging problem than emotion cause extraction\n(ECE), since it requires no emotion signals which are demonstrated as an\nimportant role in the ECE task. Existing work follows a two-stage pipeline\nwhich identifies emotions and causes at the first step and pairs them at the\nsecond step. However, error propagation across steps and pair combining without\ncontextual information limits the effectiveness. Therefore, we propose a\nDual-Questioning Attention Network to alleviate these limitations.\nSpecifically, we question candidate emotions and causes to the context\nindependently through attention networks for a contextual and semantical\nanswer. Also, we explore how weighted loss functions in controlling error\npropagation between steps. Empirical results show that our method performs\nbetter than baselines in terms of multiple evaluation metrics. The source code\ncan be obtained at https://github.com/QixuanSun/DQAN.", "published": "2021-04-15 03:47:04", "link": "http://arxiv.org/abs/2104.07221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Task-Oriented Semantic Parsing via Intrinsic Modeling", "abstract": "Task-oriented semantic parsing models typically have high resource\nrequirements: to support new ontologies (i.e., intents and slots),\npractitioners crowdsource thousands of samples for supervised fine-tuning.\nPartly, this is due to the structure of de facto copy-generate parsers; these\nmodels treat ontology labels as discrete entities, relying on parallel data to\nextrinsically derive their meaning. In our work, we instead exploit what we\nintrinsically know about ontology labels; for example, the fact that\nSL:TIME_ZONE has the categorical type \"slot\" and language-based span \"time\nzone\". Using this motivation, we build our approach with offline and online\nstages. During preprocessing, for each ontology label, we extract its intrinsic\nproperties into a component, and insert each component into an inventory as a\ncache of sorts. During training, we fine-tune a seq2seq, pre-trained\ntransformer to map utterances and inventories to frames, parse trees comprised\nof utterance and ontology tokens. Our formulation encourages the model to\nconsider ontology labels as a union of its intrinsic properties, therefore\nsubstantially bootstrapping learning in low-resource settings. Experiments show\nour model is highly sample efficient: using a low-resource benchmark derived\nfrom TOPv2, our inventory parser outperforms a copy-generate parser by +15 EM\nabsolute (44% relative) when fine-tuning on 10 samples from an unseen domain.", "published": "2021-04-15 04:01:02", "link": "http://arxiv.org/abs/2104.07224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Designing a Minimal Retrieve-and-Read System for Open-Domain Question\n  Answering", "abstract": "In open-domain question answering (QA), retrieve-and-read mechanism has the\ninherent benefit of interpretability and the easiness of adding, removing, or\nediting knowledge compared to the parametric approaches of closed-book QA\nmodels. However, it is also known to suffer from its large storage footprint\ndue to its document corpus and index. Here, we discuss several orthogonal\nstrategies to drastically reduce the footprint of a retrieve-and-read\nopen-domain QA system by up to 160x. Our results indicate that\nretrieve-and-read can be a viable option even in a highly constrained serving\nenvironment such as edge devices, as we show that it can achieve better\naccuracy than a purely parametric model with comparable docker-level system\nsize.", "published": "2021-04-15 05:24:04", "link": "http://arxiv.org/abs/2104.07242v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning\n  for Eye-Tracking Prediction", "abstract": "Eye movement data during reading is a useful source of information for\nunderstanding language comprehension processes. In this paper, we describe our\nsubmission to the CMCL 2021 shared task on predicting human reading patterns.\nOur model uses RoBERTa with a regression layer to predict 5 eye-tracking\nfeatures. We train the model in two stages: we first fine-tune on the Provo\ncorpus (another eye-tracking dataset), then fine-tune on the task data. We\ncompare different Transformer models and apply ensembling methods to improve\nthe performance. Our final submission achieves a MAE score of 3.929, ranking\n3rd place out of 13 teams that participated in this shared task.", "published": "2021-04-15 05:29:13", "link": "http://arxiv.org/abs/2104.07244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic\n  Parsing", "abstract": "An effective recipe for building seq2seq, non-autoregressive, task-oriented\nparsers to map utterances to semantic frames proceeds in three steps: encoding\nan utterance $x$, predicting a frame's length |y|, and decoding a |y|-sized\nframe with utterance and ontology tokens. Though empirically strong, these\nmodels are typically bottlenecked by length prediction, as even small\ninaccuracies change the syntactic and semantic characteristics of resulting\nframes. In our work, we propose span pointer networks, non-autoregressive\nparsers which shift the decoding task from text generation to span prediction;\nthat is, when imputing utterance spans into frame slots, our model produces\nendpoints (e.g., [i, j]) as opposed to text (e.g., \"6pm\"). This natural\nquantization of the output space reduces the variability of gold frames,\ntherefore improving length prediction and, ultimately, exact match.\nFurthermore, length prediction is now responsible for frame syntax and the\ndecoder is responsible for frame semantics, resulting in a coarse-to-fine\nmodel. We evaluate our approach on several task-oriented semantic parsing\ndatasets. Notably, we bridge the quality gap between non-autogressive and\nautoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020).\nFurthermore, due to our more consistent gold frames, we show strong\nimprovements in model generalization in both cross-domain and cross-lingual\ntransfer in low-resource settings. Finally, due to our diminished output\nvocabulary, we observe 70% reduction in latency and 83% reduction in memory at\nbeam size 5 compared to prior non-autoregressive parsers.", "published": "2021-04-15 07:02:35", "link": "http://arxiv.org/abs/2104.07275v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransferNet: An Effective and Transparent Framework for Multi-hop\n  Question Answering over Relation Graph", "abstract": "Multi-hop Question Answering (QA) is a challenging task because it requires\nprecise reasoning with entity relations at every step towards the answer. The\nrelations can be represented in terms of labels in knowledge graph (e.g.,\n\\textit{spouse}) or text in text corpus (e.g., \\textit{they have been married\nfor 26 years}). Existing models usually infer the answer by predicting the\nsequential relation path or aggregating the hidden graph features. The former\nis hard to optimize, and the latter lacks interpretability. In this paper, we\npropose TransferNet, an effective and transparent model for multi-hop QA, which\nsupports both label and text relations in a unified framework. TransferNet\njumps across entities at multiple steps. At each step, it attends to different\nparts of the question, computes activated scores for relations, and then\ntransfer the previous entity scores along activated relations in a\ndifferentiable way. We carry out extensive experiments on three datasets and\ndemonstrate that TransferNet surpasses the state-of-the-art models by a large\nmargin. In particular, on MetaQA, it achieves 100\\% accuracy in 2-hop and 3-hop\nquestions. By qualitative analysis, we show that TransferNet has transparent\nand interpretable intermediate results.", "published": "2021-04-15 08:23:05", "link": "http://arxiv.org/abs/2104.07302v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NT5?! Training T5 to Perform Numerical Reasoning", "abstract": "Numerical reasoning over text (NRoT) presents unique challenges that are not\nwell addressed by existing pre-training objectives. We explore five sequential\ntraining schedules that adapt a pre-trained T5 model for NRoT. Our final model\nis adapted from T5, but further pre-trained on three datasets designed to\nstrengthen skills necessary for NRoT and general reading comprehension before\nbeing fine-tuned on the Discrete Reasoning over Text (DROP) dataset. The\ntraining improves DROP's adjusted F1 performance (a numeracy-focused score)\nfrom 45.90 to 70.83. Our model closes in on GenBERT (72.4), a custom BERT-Base\nmodel using the same datasets with significantly more parameters. We show that\ntraining the T5 multitasking framework with multiple numerical reasoning\ndatasets of increasing difficulty, good performance on DROP can be achieved\nwithout manually engineering partitioned functionality between distributed and\nsymbol modules.", "published": "2021-04-15 08:34:44", "link": "http://arxiv.org/abs/2104.07307v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Sparse Transformer for Multilingual Translation", "abstract": "Multilingual machine translation has attracted much attention recently due to\nits support of knowledge transfer among languages and the low cost of training\nand deployment compared with numerous bilingual models. A known challenge of\nmultilingual models is the negative language interference. In order to enhance\nthe translation quality, deeper and wider architectures are applied to\nmultilingual modeling for larger model capacity, which suffers from the\nincreased inference cost at the same time. It has been pointed out in recent\nstudies that parameters shared among languages are the cause of interference\nwhile they may also enable positive transfer. Based on these insights, we\npropose an adaptive and sparse architecture for multilingual modeling, and\ntrain the model to learn shared and language-specific parameters to improve the\npositive transfer and mitigate the interference. The sparse architecture only\nactivates a sub-network which preserves inference efficiency, and the adaptive\ndesign selects different sub-networks based on the input languages. Our model\noutperforms strong baselines across multiple benchmarks. On the large-scale\nOPUS dataset with $100$ languages, we achieve $+2.1$, $+1.3$ and $+6.2$ BLEU\nimprovements in one-to-many, many-to-one and zero-shot tasks respectively\ncompared to standard Transformer without increasing the inference cost.", "published": "2021-04-15 10:31:07", "link": "http://arxiv.org/abs/2104.07358v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UIT-E10dot3 at SemEval-2021 Task 5: Toxic Spans Detection with Named\n  Entity Recognition and Question-Answering Approaches", "abstract": "The increment of toxic comments on online space is causing tremendous effects\non other vulnerable users. For this reason, considerable efforts are made to\ndeal with this, and SemEval-2021 Task 5: Toxic Spans Detection is one of those.\nThis task asks competitors to extract spans that have toxicity from the given\ntexts, and we have done several analyses to understand its structure before\ndoing experiments. We solve this task by two approaches, Named Entity\nRecognition with spaCy library and Question-Answering with RoBERTa combining\nwith ToxicBERT, and the former gains the highest F1-score of 66.99%.", "published": "2021-04-15 11:07:56", "link": "http://arxiv.org/abs/2104.07376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Terminology Extraction from Comparable E-Commerce Corpora", "abstract": "Bilingual terminologies are important machine translation resources in the\nfield of e-commerce, which are usually either manually translated or\nautomatically extracted from parallel data. The human translation is costly and\ne-commerce parallel corpora is very scarce. However, the comparable data in\ndifferent languages in the same commodity field is abundant. In this paper, we\npropose a novel framework of extracting e-commercial bilingual terminologies\nfrom comparable data. Benefiting from the cross-lingual pre-training in\ne-commerce, our framework can make full use of the deep semantic relationship\nbetween source-side terminology and target-side sentence to extract\ncorresponding target terminology. Experimental results on various language\npairs show that our approaches achieve significantly better performance than\nvarious strong baselines.", "published": "2021-04-15 11:59:33", "link": "http://arxiv.org/abs/2104.07398v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous Multi-Pivot Neural Machine Translation", "abstract": "Parallel corpora are indispensable for training neural machine translation\n(NMT) models, and parallel corpora for most language pairs do not exist or are\nscarce. In such cases, pivot language NMT can be helpful where a pivot language\nis used such that there exist parallel corpora between the source and pivot and\npivot and target languages. Naturally, the quality of pivot language\ntranslation is more inferior to what could be achieved with a direct parallel\ncorpus of a reasonable size for that pair. In a real-time simultaneous\ntranslation setting, the quality of pivot language translation deteriorates\neven further given that the model has to output translations the moment a few\nsource words become available. To solve this issue, we propose multi-pivot\ntranslation and apply it to a simultaneous translation setting involving pivot\nlanguages. Our approach involves simultaneously translating a source language\ninto multiple pivots, which are then simultaneously translated together into\nthe target language by leveraging multi-source NMT. Our experiments in a\nlow-resource setting using the N-way parallel UN corpus for Arabic to English\nNMT via French and Spanish as pivots reveals that in a simultaneous pivot NMT\nsetting, using two pivot languages can lead to an improvement of up to 5.8\nBLEU.", "published": "2021-04-15 12:19:52", "link": "http://arxiv.org/abs/2104.07410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution", "abstract": "Masked language models (MLMs) have contributed to drastic performance\nimprovements with regard to zero anaphora resolution (ZAR). To further improve\nthis approach, in this study, we made two proposals. The first is a new\npretraining task that trains MLMs on anaphoric relations with explicit\nsupervision, and the second proposal is a new finetuning method that remedies a\nnotorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese\nZAR demonstrated that our two proposals boost the state-of-the-art performance,\nand our detailed analysis provides new insights on the remaining challenges.", "published": "2021-04-15 12:43:57", "link": "http://arxiv.org/abs/2104.07425v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "First the worst: Finding better gender translations during beam search", "abstract": "Neural machine translation inference procedures like beam search generate the\nmost likely output under the model. This can exacerbate any demographic biases\nexhibited by the model. We focus on gender bias resulting from systematic\nerrors in grammatical gender translation, which can lead to human referents\nbeing misrepresented or misgendered.\n  Most approaches to this problem adjust the training data or the model. By\ncontrast, we experiment with simply adjusting the inference procedure. We\nexperiment with reranking nbest lists using gender features obtained\nautomatically from the source sentence, and applying gender constraints while\ndecoding to improve nbest list gender diversity. We find that a combination of\nthese techniques allows large gains in WinoMT accuracy without requiring\nadditional bilingual data or an additional NMT model.", "published": "2021-04-15 12:53:30", "link": "http://arxiv.org/abs/2104.07429v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Compositional Generalization in Pre-trained Models Using\n  Intermediate Representations", "abstract": "Sequence-to-sequence (seq2seq) models are prevalent in semantic parsing, but\nhave been found to struggle at out-of-distribution compositional\ngeneralization. While specialized model architectures and pre-training of\nseq2seq models have been proposed to address this issue, the former often comes\nat the cost of generality and the latter only shows limited success. In this\npaper, we study the impact of intermediate representations on compositional\ngeneralization in pre-trained seq2seq models, without changing the model\narchitecture at all, and identify key aspects for designing effective\nrepresentations. Instead of training to directly map natural language to an\nexecutable form, we map to a reversible or lossy intermediate representation\nthat has stronger structural correspondence with natural language. The\ncombination of our proposed intermediate representations and pre-trained models\nis surprisingly effective, where the best combinations obtain a new\nstate-of-the-art on CFQ (+14.8 accuracy points) and on the template-splits of\nthree text-to-SQL datasets (+15.0 to +19.4 accuracy points). This work\nhighlights that intermediate representations provide an important and\npotentially overlooked degree of freedom for improving the compositional\ngeneralization abilities of pre-trained seq2seq models.", "published": "2021-04-15 14:15:14", "link": "http://arxiv.org/abs/2104.07478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unmasking the Mask -- Evaluating Social Biases in Masked Language Models", "abstract": "Masked Language Models (MLMs) have shown superior performances in numerous\ndownstream NLP tasks when used as text encoders. Unfortunately, MLMs also\ndemonstrate significantly worrying levels of social biases. We show that the\npreviously proposed evaluation metrics for quantifying the social biases in\nMLMs are problematic due to following reasons: (1) prediction accuracy of the\nmasked tokens itself tend to be low in some MLMs, which raises questions\nregarding the reliability of the evaluation metrics that use the (pseudo)\nlikelihood of the predicted tokens, and (2) the correlation between the\nprediction accuracy of the mask and the performance in downstream NLP tasks is\nnot taken into consideration, and (3) high frequency words in the training data\nare masked more often, introducing noise due to this selection bias in the test\ncases. To overcome the above-mentioned disfluencies, we propose All Unmasked\nLikelihood (AUL), a bias evaluation measure that predicts all tokens in a test\ncase given the MLM embedding of the unmasked input. We find that AUL accurately\ndetects different types of biases in MLMs. We also propose AUL with attention\nweights (AULA) to evaluate tokens based on their importance in a sentence.\nHowever, unlike AUL and AULA, previously proposed bias evaluation measures for\nMLMs systematically overestimate the measured biases, and are heavily\ninfluenced by the unmasked tokens in the context.", "published": "2021-04-15 14:40:42", "link": "http://arxiv.org/abs/2104.07496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via\n  Multi-Task Training", "abstract": "Language grounding aims at linking the symbolic representation of language\n(e.g., words) into the rich perceptual knowledge of the outside world. The\ngeneral approach is to embed both textual and visual information into a common\nspace -the grounded space-confined by an explicit relationship between both\nmodalities. We argue that this approach sacrifices the abstract knowledge\nobtained from linguistic co-occurrence statistics in the process of acquiring\nperceptual information. The focus of this paper is to solve this issue by\nimplicitly grounding the word embeddings. Rather than learning two mappings\ninto a joint space, our approach integrates modalities by determining a\nreversible grounded mapping between the textual and the grounded space by means\nof multi-task learning. Evaluations on intrinsic and extrinsic tasks show that\nour embeddings are highly beneficial for both abstract and concrete words. They\nare strongly correlated with human judgments and outperform previous works on a\nwide range of benchmarks. Our grounded embeddings are publicly available here.", "published": "2021-04-15 14:49:11", "link": "http://arxiv.org/abs/2104.07500v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Understanding with Privacy-Preserving BERT", "abstract": "Privacy preservation remains a key challenge in data mining and Natural\nLanguage Understanding (NLU). Previous research shows that the input text or\neven text embeddings can leak private information. This concern motivates our\nresearch on effective privacy preservation approaches for pretrained Language\nModels (LMs). We investigate the privacy and utility implications of applying\ndx-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU\napplications. More importantly, we further propose privacy-adaptive LM\npretraining methods and show that our approach can boost the utility of BERT\ndramatically while retaining the same level of privacy protection. We also\nquantify the level of privacy preservation and provide guidance on privacy\nconfiguration. Our experiments and findings lay the groundwork for future\nexplorations of privacy-preserving NLU with pretrained LMs.", "published": "2021-04-15 15:01:28", "link": "http://arxiv.org/abs/2104.07504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence tagging for biomedical extractive question answering", "abstract": "Current studies in extractive question answering (EQA) have modeled the\nsingle-span extraction setting, where a single answer span is a label to\npredict for a given question-passage pair. This setting is natural for general\ndomain EQA as the majority of the questions in the general domain can be\nanswered with a single span. Following general domain EQA models, current\nbiomedical EQA (BioEQA) models utilize the single-span extraction setting with\npost-processing steps. In this article, we investigate the question\ndistribution across the general and biomedical domains and discover biomedical\nquestions are more likely to require list-type answers (multiple answers) than\nfactoid-type answers (single answer). This necessitates the models capable of\nproducing multiple answers for a question. Based on this preliminary study, we\npropose a sequence tagging approach for BioEQA, which is a multi-span\nextraction setting. Our approach directly tackles questions with a variable\nnumber of phrases as their answer and can learn to decide the number of answers\nfor a question from training data. Our experimental results on the BioASQ 7b\nand 8b list-type questions outperformed the best-performing existing models\nwithout requiring post-processing steps. Source codes and resources are freely\navailable for download at https://github.com/dmis-lab/SeqTagQA", "published": "2021-04-15 15:42:34", "link": "http://arxiv.org/abs/2104.07535v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Learning for Generation with Long Source Sequences", "abstract": "One of the challenges for current sequence to sequence (seq2seq) models is\nprocessing long sequences, such as those in summarization and document level\nmachine translation tasks. These tasks require the model to reason at the token\nlevel as well as the sentence and paragraph level. We design and study a new\nHierarchical Attention Transformer-based architecture (HAT) that outperforms\nstandard Transformers on several sequence to sequence tasks. Furthermore, our\nmodel achieves state-of-the-art ROUGE scores on four summarization tasks,\nincluding PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms\ndocument-level machine translation baseline on the WMT20 English to German\ntranslation task. We investigate what the hierarchical layers learn by\nvisualizing the hierarchical encoder-decoder attention. Finally, we study\nhierarchical learning on encoder-only pre-training and analyze its performance\non classification tasks.", "published": "2021-04-15 15:57:32", "link": "http://arxiv.org/abs/2104.07545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-lingual Semantic Parsing", "abstract": "Recent work in cross-lingual semantic parsing has successfully applied\nmachine translation to localize parsers to new languages. However, these\nadvances assume access to high-quality machine translation systems and word\nalignment tools. We remove these assumptions and study cross-lingual semantic\nparsing as a zero-shot problem, without parallel data (i.e., utterance-logical\nform pairs) for new languages. We propose a multi-task encoder-decoder model to\ntransfer parsing knowledge to additional languages using only English-logical\nform paired data and in-domain natural language corpora in each new language.\nOur model encourages language-agnostic encodings by jointly optimizing for\nlogical-form generation with auxiliary objectives designed for cross-lingual\nlatent representation alignment. Our parser performs significantly above\ntranslation-based baselines and, in some cases, competes with the supervised\nupper-bound.", "published": "2021-04-15 16:08:43", "link": "http://arxiv.org/abs/2104.07554v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic\n  Evaluation", "abstract": "QuestEval is a reference-less metric used in text-to-text tasks, that\ncompares the generated summaries directly to the source text, by automatically\nasking and answering questions. Its adaptation to Data-to-Text tasks is not\nstraightforward, as it requires multimodal Question Generation and Answering\nsystems on the considered tasks, which are seldom available. To this purpose,\nwe propose a method to build synthetic multimodal corpora enabling to train\nmultimodal components for a data-QuestEval metric. The resulting metric is\nreference-less and multimodal; it obtains state-of-the-art correlations with\nhuman judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's\ncode and models available for reproducibility purpose, as part of the QuestEval\nproject.", "published": "2021-04-15 16:10:46", "link": "http://arxiv.org/abs/2104.07555v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Automatic Evaluation in Sentence Simplification", "abstract": "Automatic evaluation remains an open research question in Natural Language\nGeneration. In the context of Sentence Simplification, this is particularly\nchallenging: the task requires by nature to replace complex words with simpler\nones that shares the same meaning. This limits the effectiveness of n-gram\nbased metrics like BLEU. Going hand in hand with the recent advances in NLG,\nnew metrics have been proposed, such as BERTScore for Machine Translation. In\nsummarization, the QuestEval metric proposes to automatically compare two texts\nby questioning them.\n  In this paper, we first propose a simple modification of QuestEval allowing\nit to tackle Sentence Simplification. We then extensively evaluate the\ncorrelations w.r.t. human judgement for several metrics including the recent\nBERTScore and QuestEval, and show that the latter obtain state-of-the-art\ncorrelations, outperforming standard metrics like BLEU and SARI. More\nimportantly, we also show that a large part of the correlations are actually\nspurious for all the metrics. To investigate this phenomenon further, we\nrelease a new corpus of evaluated simplifications, this time not generated by\nsystems but instead, written by humans. This allows us to remove the spurious\ncorrelations and draw very different conclusions from the original ones,\nresulting in a better understanding of these metrics. In particular, we raise\nconcerns about very low correlations for most of traditional metrics. Our\nresults show that the only significant measure of the Meaning Preservation is\nour adaptation of QuestEval.", "published": "2021-04-15 16:13:50", "link": "http://arxiv.org/abs/2104.07560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Deconfounding the Influence of Entity Demographics for Question\n  Answering Accuracy", "abstract": "The goal of question answering (QA) is to answer any question. However, major\nQA datasets have skewed distributions over gender, profession, and nationality.\nDespite that skew, model accuracy analysis reveals little evidence that\naccuracy is lower for people based on gender or nationality; instead, there is\nmore variation on professions (question topic). But QA's lack of representation\ncould itself hide evidence of bias, necessitating QA datasets that better\nrepresent global diversity.", "published": "2021-04-15 16:26:54", "link": "http://arxiv.org/abs/2104.07571v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Perturbations Reveal Representational Correlates of\n  Hierarchical Phrase Structure in Pretrained Language Models", "abstract": "While vector-based language representations from pretrained language models\nhave set a new standard for many NLP tasks, there is not yet a complete\naccounting of their inner workings. In particular, it is not entirely clear\nwhat aspects of sentence-level syntax are captured by these representations,\nnor how (if at all) they are built along the stacked layers of the network. In\nthis paper, we aim to address such questions with a general class of\ninterventional, input perturbation-based analyses of representations from\npretrained language models. Importing from computational and cognitive\nneuroscience the notion of representational invariance, we perform a series of\nprobes designed to test the sensitivity of these representations to several\nkinds of structure in sentences. Each probe involves swapping words in a\nsentence and comparing the representations from perturbed sentences against the\noriginal. We experiment with three different perturbations: (1) random\npermutations of n-grams of varying width, to test the scale at which a\nrepresentation is sensitive to word position; (2) swapping of two spans which\ndo or do not form a syntactic phrase, to test sensitivity to global phrase\nstructure; and (3) swapping of two adjacent words which do or do not break\napart a syntactic phrase, to test sensitivity to local phrase structure.\n  Results from these probes collectively suggest that Transformers build\nsensitivity to larger parts of the sentence along their layers, and that\nhierarchical phrase structure plays a role in this process. More broadly, our\nresults also indicate that structured input perturbations widens the scope of\nanalyses that can be performed on often-opaque deep learning systems, and can\nserve as a complement to existing tools (such as supervised linear probes) for\ninterpreting complex black-box models.", "published": "2021-04-15 16:30:31", "link": "http://arxiv.org/abs/2104.07578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SummVis: Interactive Visual Analysis of Models, Data, and Evaluation for\n  Text Summarization", "abstract": "Novel neural architectures, training strategies, and the availability of\nlarge-scale corpora haven been the driving force behind recent progress in\nabstractive text summarization. However, due to the black-box nature of neural\nmodels, uninformative evaluation metrics, and scarce tooling for model and data\nanalysis, the true performance and failure modes of summarization models remain\nlargely unknown. To address this limitation, we introduce SummVis, an\nopen-source tool for visualizing abstractive summaries that enables\nfine-grained analysis of the models, data, and evaluation metrics associated\nwith text summarization. Through its lexical and semantic visualizations, the\ntools offers an easy entry point for in-depth model prediction exploration\nacross important dimensions such as factual consistency or abstractiveness. The\ntool together with several pre-computed model outputs is available at\nhttps://github.com/robustness-gym/summvis.", "published": "2021-04-15 17:13:00", "link": "http://arxiv.org/abs/2104.07605v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Planning with Learned Entity Prompts for Abstractive Summarization", "abstract": "We introduce a simple but flexible mechanism to learn an intermediate plan to\nground the generation of abstractive summaries. Specifically, we prepend (or\nprompt) target summaries with entity chains -- ordered sequences of entities\nmentioned in the summary. Transformer-based sequence-to-sequence models are\nthen trained to generate the entity chain and then continue generating the\nsummary conditioned on the entity chain and the input. We experimented with\nboth pretraining and finetuning with this content planning objective. When\nevaluated on CNN/DailyMail, XSum, SAMSum and BillSum, we demonstrate\nempirically that the grounded generation with the planning objective improves\nentity specificity and planning in summaries for all datasets, and achieves\nstate-of-the-art performance on XSum and SAMSum in terms of Rouge. Moreover, we\ndemonstrate empirically that planning with entity chains provides a mechanism\nto control hallucinations in abstractive summaries. By prompting the decoder\nwith a modified content plan that drops hallucinated entities, we outperform\nstate-of-the-art approaches for faithfulness when evaluated automatically and\nby humans.", "published": "2021-04-15 17:16:03", "link": "http://arxiv.org/abs/2104.07606v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Coreference Resolution Models through Active Learning", "abstract": "Neural coreference resolution models trained on one dataset may not transfer\nto new, low-resource domains. Active learning mitigates this problem by\nsampling a small subset of data for annotators to label. While active learning\nis well-defined for classification tasks, its application to coreference\nresolution is neither well-defined nor fully understood. This paper explores\nhow to actively label coreference, examining sources of model uncertainty and\ndocument reading costs. We compare uncertainty sampling strategies and their\nadvantages through thorough error analysis. In both synthetic and human\nexperiments, labeling spans within the same document is more effective than\nannotating spans across documents. The findings contribute to a more realistic\ndevelopment of coreference resolution models.", "published": "2021-04-15 17:21:51", "link": "http://arxiv.org/abs/2104.07611v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SINA-BERT: A pre-trained Language Model for Analysis of Medical Texts in\n  Persian", "abstract": "We have released Sina-BERT, a language model pre-trained on BERT (Devlin et\nal., 2018) to address the lack of a high-quality Persian language model in the\nmedical domain. SINA-BERT utilizes pre-training on a large-scale corpus of\nmedical contents including formal and informal texts collected from a variety\nof online resources in order to improve the performance on health-care related\ntasks. We employ SINA-BERT to complete following representative tasks:\ncategorization of medical questions, medical sentiment analysis, and medical\nquestion retrieval. For each task, we have developed Persian annotated data\nsets for training and evaluation and learnt a representation for the data of\neach task especially complex and long medical questions. With the same\narchitecture being used across tasks, SINA-BERT outperforms BERT-based models\nthat were previously made available in the Persian language.", "published": "2021-04-15 17:22:27", "link": "http://arxiv.org/abs/2104.07613v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sometimes We Want Translationese", "abstract": "Rapid progress in Neural Machine Translation (NMT) systems over the last few\nyears has been driven primarily towards improving translation quality, and as a\nsecondary focus, improved robustness to input perturbations (e.g. spelling and\ngrammatical mistakes). While performance and robustness are important\nobjectives, by over-focusing on these, we risk overlooking other important\nproperties. In this paper, we draw attention to the fact that for some\napplications, faithfulness to the original (input) text is important to\npreserve, even if it means introducing unusual language patterns in the\n(output) translation. We propose a simple, novel way to quantify whether an NMT\nsystem exhibits robustness and faithfulness, focusing on the case of word-order\nperturbations. We explore a suite of functions to perturb the word order of\nsource sentences without deleting or injecting tokens, and measure the effects\non the target side in terms of both robustness and faithfulness. Across several\nexperimental conditions, we observe a strong tendency towards robustness rather\nthan faithfulness. These results allow us to better understand the trade-off\nbetween faithfulness and robustness in NMT, and opens up the possibility of\ndeveloping systems where users have more autonomy and control in selecting\nwhich property is best suited for their use case.", "published": "2021-04-15 17:39:47", "link": "http://arxiv.org/abs/2104.07623v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual alignment transfers to multilingual alignment for unsupervised\n  parallel text mining", "abstract": "This work presents methods for learning cross-lingual sentence\nrepresentations using paired or unpaired bilingual texts. We hypothesize that\nthe cross-lingual alignment strategy is transferable, and therefore a model\ntrained to align only two languages can encode multilingually more aligned\nrepresentations. We thus introduce dual-pivot transfer: training on one\nlanguage pair and evaluating on other pairs. To study this theory, we design\nunsupervised models trained on unpaired sentences and single-pair supervised\nmodels trained on bitexts, both based on the unsupervised language model XLM-R\nwith its parameters frozen. The experiments evaluate the models as universal\nsentence encoders on the task of unsupervised bitext mining on two datasets,\nwhere the unsupervised model reaches the state of the art of unsupervised\nretrieval, and the alternative single-pair supervised model approaches the\nperformance of multilingually supervised models. The results suggest that\nbilingual training techniques as proposed can be applied to get sentence\nrepresentations with multilingual alignment.", "published": "2021-04-15 17:51:22", "link": "http://arxiv.org/abs/2104.07642v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Multilingual BERT models robust? A Case Study on Adversarial Attacks\n  for Multilingual Question Answering", "abstract": "Recent approaches have exploited weaknesses in monolingual question answering\n(QA) models by adding adversarial statements to the passage. These attacks\ncaused a reduction in state-of-the-art performance by almost 50%. In this\npaper, we are the first to explore and successfully attack a multilingual QA\n(MLQA) system pre-trained on multilingual BERT using several attack strategies\nfor the adversarial statement reducing performance by as much as 85%. We show\nthat the model gives priority to English and the language of the question\nregardless of the other languages in the QA pair. Further, we also show that\nadding our attack strategies during training helps alleviate the attacks.", "published": "2021-04-15 17:55:09", "link": "http://arxiv.org/abs/2104.07646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Gender Translation Accuracy with Filtered Self-Training", "abstract": "Targeted evaluations have found that machine translation systems often output\nincorrect gender, even when the gender is clear from context. Furthermore,\nthese incorrectly gendered translations have the potential to reflect or\namplify social biases. We propose a gender-filtered self-training technique to\nimprove gender translation accuracy on unambiguously gendered inputs. This\napproach uses a source monolingual corpus and an initial model to generate\ngender-specific pseudo-parallel corpora which are then added to the training\ndata. We filter the gender-specific corpora on the source and target sides to\nensure that sentence pairs contain and correctly translate the specified\ngender. We evaluate our approach on translation from English into five\nlanguages, finding that our models improve gender translation accuracy without\nany cost to generic translation quality. In addition, we show the viability of\nour approach on several settings, including re-training from scratch,\nfine-tuning, controlling the balance of the training data, forward translation,\nand back-translation.", "published": "2021-04-15 18:05:29", "link": "http://arxiv.org/abs/2104.07695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling", "abstract": "Recent models have shown that incorporating syntactic knowledge into the\nsemantic role labelling (SRL) task leads to a significant improvement. In this\npaper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model,\nwhich encodes the syntactic structure using a novel way to input graph\nrelations as embeddings, directly into the self-attention mechanism of\nTransformer. This approach adds a soft bias towards attention patterns that\nfollow the syntactic structure but also allows the model to use this\ninformation to learn alternative patterns. We evaluate our model on both\nspan-based and dependency-based SRL datasets, and outperform previous\nalternative methods in both in-domain and out-of-domain settings, on CoNLL 2005\nand CoNLL 2009 datasets.", "published": "2021-04-15 18:14:18", "link": "http://arxiv.org/abs/2104.07704v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proteno: Text Normalization with Limited Data for Fast Deployment in\n  Text to Speech Systems", "abstract": "Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new\nlanguages is hard. We propose a novel architecture to facilitate it for\nmultiple languages while using data less than 3% of the size of the data used\nby the state of the art results on English. We treat TN as a sequence\nclassification problem and propose a granular tokenization mechanism that\nenables the system to learn majority of the classes and their normalizations\nfrom the training data itself. This is further combined with minimal precoded\nlinguistic knowledge for other classes. We publish the first results on TN for\nTTS in Spanish and Tamil and also demonstrate that the performance of the\napproach is comparable with the previous work done on English. All annotated\ndatasets used for experimentation will be released at\nhttps://github.com/amazon-research/proteno.", "published": "2021-04-15 21:14:28", "link": "http://arxiv.org/abs/2104.07777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COIL: Revisit Exact Lexical Match in Information Retrieval with\n  Contextualized Inverted List", "abstract": "Classical information retrieval systems such as BM25 rely on exact lexical\nmatch and carry out search efficiently with inverted list index. Recent neural\nIR models shifts towards soft semantic matching all query document terms, but\nthey lose the computation efficiency of exact match systems. This paper\npresents COIL, a contextualized exact match retrieval architecture that brings\nsemantic lexical matching. COIL scoring is based on overlapping query document\ntokens' contextualized representations. The new architecture stores\ncontextualized token representations in inverted lists, bringing together the\nefficiency of exact match and the representation power of deep language models.\nOur experimental results show COIL outperforms classical lexical retrievers and\nstate-of-the-art deep LM retrievers with similar or smaller latency.", "published": "2021-04-15 00:53:54", "link": "http://arxiv.org/abs/2104.07186v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "An Alignment-Agnostic Model for Chinese Text Error Correction", "abstract": "This paper investigates how to correct Chinese text errors with types of\nmistaken, missing and redundant characters, which is common for Chinese native\nspeakers. Most existing models based on detect-correct framework can correct\nmistaken characters errors, but they cannot deal with missing or redundant\ncharacters. The reason is that lengths of sentences before and after correction\nare not the same, leading to the inconsistence between model inputs and\noutputs. Although the Seq2Seq-based or sequence tagging methods provide\nsolutions to the problem and achieved relatively good results on English\ncontext, but they do not perform well in Chinese context according to our\nexperimental results. In our work, we propose a novel detect-correct framework\nwhich is alignment-agnostic, meaning that it can handle both text aligned and\nnon-aligned occasions, and it can also serve as a cold start model when there\nare no annotated data provided. Experimental results on three datasets\ndemonstrate that our method is effective and achieves the best performance\namong existing published models.", "published": "2021-04-15 01:17:34", "link": "http://arxiv.org/abs/2104.07190v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ultra-High Dimensional Sparse Representations with Binarization for\n  Efficient Text Retrieval", "abstract": "The semantic matching capabilities of neural information retrieval can\nameliorate synonymy and polysemy problems of symbolic approaches. However,\nneural models' dense representations are more suitable for re-ranking, due to\ntheir inefficiency. Sparse representations, either in symbolic or latent form,\nare more efficient with an inverted index. Taking the merits of the sparse and\ndense representations, we propose an ultra-high dimensional (UHD)\nrepresentation scheme equipped with directly controllable sparsity. UHD's large\ncapacity and minimal noise and interference among the dimensions allow for\nbinarized representations, which are highly efficient for storage and search.\nAlso proposed is a bucketing method, where the embeddings from multiple layers\nof BERT are selected/merged to represent diverse linguistic aspects. We test\nour models with MS MARCO and TREC CAR, showing that our models outperforms\nother sparse models", "published": "2021-04-15 02:00:01", "link": "http://arxiv.org/abs/2104.07198v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Sentence-Permuted Paragraph Generation", "abstract": "Generating paragraphs of diverse contents is important in many applications.\nExisting generation models produce similar contents from homogenized contexts\ndue to the fixed left-to-right sentence order. Our idea is permuting the\nsentence orders to improve the content diversity of multi-sentence paragraph.\nWe propose a novel framework PermGen whose objective is to maximize the\nexpected log-likelihood of output paragraph distributions with respect to all\npossible sentence orders. PermGen uses hierarchical positional embedding and\ndesigns new procedures for training, decoding, and candidate ranking in the\nsentence-permuted generation. Experiments on three paragraph generation\nbenchmarks demonstrate PermGen generates more diverse outputs with a higher\nquality than existing models.", "published": "2021-04-15 04:17:03", "link": "http://arxiv.org/abs/2104.07228v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Regularization for Long Named Entity Recognition", "abstract": "When performing named entity recognition (NER), entity length is variable and\ndependent on a specific domain or dataset. Pre-trained language models (PLMs)\nare used to solve NER tasks and tend to be biased toward dataset patterns such\nas length statistics, surface form, and skewed class distribution. These biases\nhinder the generalization ability of PLMs, which is necessary to address many\nunseen mentions in real-world situations. We propose a novel debiasing method\nRegLER to improve predictions for entities of varying lengths. To close the gap\nbetween evaluation and real-world situations, we evaluated PLMs on partitioned\nbenchmark datasets containing unseen mention sets. Here, RegLER shows\nsignificant improvement over long-named entities that can predict through\ndebiasing on conjunction or special characters within entities. Furthermore,\nthere is a severe class imbalance in most NER datasets, causing easy-negative\nexamples to dominate during training, such as \"The\". Our approach alleviates\nskewed class distribution by reducing the influence of easy-negative examples.\nExtensive experiments on the biomedical and general domains demonstrated the\ngeneralization capabilities of our method. To facilitate reproducibility and\nfuture work, we release our code.\"https://github.com/minstar/RegLER\"", "published": "2021-04-15 05:47:27", "link": "http://arxiv.org/abs/2104.07249v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emotion Dynamics Modeling via BERT", "abstract": "Emotion dynamics modeling is a significant task in emotion recognition in\nconversation. It aims to predict conversational emotions when building\nempathetic dialogue systems. Existing studies mainly develop models based on\nRecurrent Neural Networks (RNNs). They cannot benefit from the power of the\nrecently-developed pre-training strategies for better token representation\nlearning in conversations. More seriously, it is hard to distinguish the\ndependency of interlocutors and the emotional influence among interlocutors by\nsimply assembling the features on top of RNNs. In this paper, we develop a\nseries of BERT-based models to specifically capture the inter-interlocutor and\nintra-interlocutor dependencies of the conversational emotion dynamics.\nConcretely, we first substitute BERT for RNNs to enrich the token\nrepresentations. Then, a Flat-structured BERT (F-BERT) is applied to link up\nutterances in a conversation directly, and a Hierarchically-structured BERT\n(H-BERT) is employed to distinguish the interlocutors when linking up\nutterances. More importantly, a Spatial-Temporal-structured BERT, namely\nST-BERT, is proposed to further determine the emotional influence among\ninterlocutors. Finally, we conduct extensive experiments on two popular emotion\nrecognition in conversation benchmark datasets and demonstrate that our\nproposed models can attain around 5\\% and 10\\% improvement over the\nstate-of-the-art baselines, respectively.", "published": "2021-04-15 05:58:48", "link": "http://arxiv.org/abs/2104.07252v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Consistency Training with Virtual Adversarial Discrete Perturbation", "abstract": "Consistency training regularizes a model by enforcing predictions of original\nand perturbed inputs to be similar. Previous studies have proposed various\naugmentation methods for the perturbation but are limited in that they are\nagnostic to the training model. Thus, the perturbed samples may not aid in\nregularization due to their ease of classification from the model. In this\ncontext, we propose an augmentation method of adding a discrete noise that\nwould incur the highest divergence between predictions. This virtual\nadversarial discrete noise obtained by replacing a small portion of tokens\nwhile keeping original semantics as much as possible efficiently pushes a\ntraining model's decision boundary. Experimental results show that our proposed\nmethod outperforms other consistency training baselines with text editing,\nparaphrasing, or a continuous noise on semi-supervised text classification\ntasks and a robustness benchmark", "published": "2021-04-15 07:49:43", "link": "http://arxiv.org/abs/2104.07284v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERT based Transformers lead the way in Extraction of Health Information\n  from Social Media", "abstract": "This paper describes our submissions for the Social Media Mining for Health\n(SMM4H)2021 shared tasks. We participated in 2 tasks:(1) Classification,\nextraction and normalization of adverse drug effect (ADE) mentions in English\ntweets (Task-1) and (2) Classification of COVID-19 tweets containing\nsymptoms(Task-6). Our approach for the first task uses the language\nrepresentation model RoBERTa with a binary classification head. For the second\ntask, we use BERTweet, based on RoBERTa. Fine-tuning is performed on the\npre-trained models for both tasks. The models are placed on top of a custom\ndomain-specific processing pipeline. Our system ranked first among all the\nsubmissions for subtask-1(a) with an F1-score of 61%. For subtask-1(b), our\nsystem obtained an F1-score of 50% with improvements up to +8% F1 over the\nscore averaged across all submissions. The BERTweet model achieved an F1 score\nof 94% on SMM4H 2021 Task-6.", "published": "2021-04-15 10:50:21", "link": "http://arxiv.org/abs/2104.07367v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Node Co-occurrence based Graph Neural Networks for Knowledge Graph Link\n  Prediction", "abstract": "We introduce a novel embedding model, named NoGE, which aims to integrate\nco-occurrence among entities and relations into graph neural networks to\nimprove knowledge graph completion (i.e., link prediction). Given a knowledge\ngraph, NoGE constructs a single graph considering entities and relations as\nindividual nodes. NoGE then computes weights for edges among nodes based on the\nco-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion\nGraph Neural Networks (DualQGNN) and utilizes DualQGNN to update vector\nrepresentations for entity and relation nodes. NoGE then adopts a score\nfunction to produce the triple scores. Comprehensive experimental results show\nthat NoGE obtains state-of-the-art results on three new and difficult benchmark\ndatasets CoDEx for knowledge graph completion.", "published": "2021-04-15 11:51:52", "link": "http://arxiv.org/abs/2104.07396v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation", "abstract": "Machine learning has brought striking advances in multilingual natural\nlanguage processing capabilities over the past year. For example, the latest\ntechniques have improved the state-of-the-art performance on the XTREME\nmultilingual benchmark by more than 13 points. While a sizeable gap to\nhuman-level performance remains, improvements have been easier to achieve in\nsome tasks than in others. This paper analyzes the current state of\ncross-lingual transfer learning and summarizes some lessons learned. In order\nto catalyze meaningful progress, we extend XTREME to XTREME-R, which consists\nof an improved set of ten natural language understanding tasks, including\nchallenging language-agnostic retrieval tasks, and covers 50 typologically\ndiverse languages. In addition, we provide a massively multilingual diagnostic\nsuite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities\nthrough an interactive public leaderboard to gain a better understanding of\nsuch models. The leaderboard and code for XTREME-R will be made available at\nhttps://sites.research.google/xtreme and\nhttps://github.com/google-research/xtreme respectively.", "published": "2021-04-15 12:26:12", "link": "http://arxiv.org/abs/2104.07412v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effect of Post-processing on Contextualized Word Representations", "abstract": "Post-processing of static embedding has beenshown to improve their\nperformance on both lexical and sequence-level tasks. However, post-processing\nfor contextualized embeddings is an under-studied problem. In this work, we\nquestion the usefulness of post-processing for contextualized embeddings\nobtained from different layers of pre-trained language models. More\nspecifically, we standardize individual neuron activations using z-score,\nmin-max normalization, and by removing top principle components using the\nall-but-the-top method. Additionally, we apply unit length normalization to\nword representations. On a diverse set of pre-trained models, we show that\npost-processing unwraps vital information present in the representations for\nboth lexical tasks (such as word similarity and analogy)and sequence\nclassification tasks. Our findings raise interesting points in relation to\ntheresearch studies that use contextualized representations, and suggest\nz-score normalization as an essential step to consider when using them in an\napplication.", "published": "2021-04-15 13:40:42", "link": "http://arxiv.org/abs/2104.07456v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Label-Adaptive Stance Detection", "abstract": "Stance detection concerns the classification of a writer's viewpoint towards\na target. There are different task variants, e.g., stance of a tweet vs. a full\narticle, or stance with respect to a claim vs. an (implicit) topic. Moreover,\ntask definitions vary, which includes the label inventory, the data collection,\nand the annotation protocol. All these aspects hinder cross-domain studies, as\nthey require changes to standard domain adaptation approaches. In this paper,\nwe perform an in-depth analysis of 16 stance detection datasets, and we explore\nthe possibility for cross-domain learning from them. Moreover, we propose an\nend-to-end unsupervised framework for out-of-domain prediction of unseen,\nuser-defined labels. In particular, we combine domain adaptation techniques\nsuch as mixture of experts and domain-adversarial training with label\nembeddings, and we demonstrate sizable performance gains over strong baselines,\nboth (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for\nunseen targets. Finally, we perform an exhaustive analysis of the cross-domain\nresults, and we highlight the important factors influencing the model\nperformance.", "published": "2021-04-15 14:04:29", "link": "http://arxiv.org/abs/2104.07467v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying Gender Bias Towards Politicians in Cross-Lingual Language\n  Models", "abstract": "Recent research has demonstrated that large pre-trained language models\nreflect societal biases expressed in natural language. The present paper\nintroduces a simple method for probing language models to conduct a\nmultilingual study of gender bias towards politicians. We quantify the usage of\nadjectives and verbs generated by language models surrounding the names of\npoliticians as a function of their gender. To this end, we curate a dataset of\n250k politicians worldwide, including their names and gender. Our study is\nconducted in seven languages across six different language modeling\narchitectures. The results demonstrate that pre-trained language models' stance\ntowards politicians varies strongly across analyzed languages. We find that\nwhile some words such as dead, and designated are associated with both male and\nfemale politicians, a few specific words such as beautiful and divorced are\npredominantly associated with female politicians. Finally, and contrary to\nprevious findings, our study suggests that larger language models do not tend\nto be significantly more gender-biased than smaller ones.", "published": "2021-04-15 15:03:26", "link": "http://arxiv.org/abs/2104.07505v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Sample-Based Training Method for Distantly Supervised Relation\n  Extraction with Pre-Trained Transformers", "abstract": "Multiple instance learning (MIL) has become the standard learning paradigm\nfor distantly supervised relation extraction (DSRE). However, due to relation\nextraction being performed at bag level, MIL has significant hardware\nrequirements for training when coupled with large sentence encoders such as\ndeep transformer neural networks. In this paper, we propose a novel sampling\nmethod for DSRE that relaxes these hardware requirements. In the proposed\nmethod, we limit the number of sentences in a batch by randomly sampling\nsentences from the bags in the batch. However, this comes at the cost of losing\nvalid sentences from bags. To alleviate the issues caused by random sampling,\nwe use an ensemble of trained models for prediction. We demonstrate the\neffectiveness of our approach by using our proposed learning setting to\nfine-tuning BERT on the widely NYT dataset. Our approach significantly\noutperforms previous state-of-the-art methods in terms of AUC and P@N metrics.", "published": "2021-04-15 15:09:34", "link": "http://arxiv.org/abs/2104.07512v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Datasets with Pretrained Language Models", "abstract": "To obtain high-quality sentence embeddings from pretrained language models\n(PLMs), they must either be augmented with additional pretraining objectives or\nfinetuned on a large set of labeled text pairs. While the latter approach\ntypically outperforms the former, it requires great human effort to generate\nsuitable datasets of sufficient size. In this paper, we show how PLMs can be\nleveraged to obtain high-quality sentence embeddings without the need for\nlabeled data, finetuning or modifications to the pretraining objective: We\nutilize the generative abilities of large and high-performing PLMs to generate\nentire datasets of labeled text pairs from scratch, which we then use for\nfinetuning much smaller and more efficient models. Our fully unsupervised\napproach outperforms strong baselines on several semantic textual similarity\ndatasets.", "published": "2021-04-15 15:51:41", "link": "http://arxiv.org/abs/2104.07540v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reward Optimization for Neural Machine Translation with Learned Metrics", "abstract": "Neural machine translation (NMT) models are conventionally trained with\ntoken-level negative log-likelihood (NLL), which does not guarantee that the\ngenerated translations will be optimized for a selected sequence-level\nevaluation metric. Multiple approaches are proposed to train NMT with BLEU as\nthe reward, in order to directly improve the metric. However, it was reported\nthat the gain in BLEU does not translate to real quality improvement, limiting\nthe application in industry. Recently, it became clear to the community that\nBLEU has a low correlation with human judgment when dealing with\nstate-of-the-art models. This leads to the emerging of model-based evaluation\nmetrics. These new metrics are shown to have a much higher human correlation.\nIn this paper, we investigate whether it is beneficial to optimize NMT models\nwith the state-of-the-art model-based metric, BLEURT. We propose a\ncontrastive-margin loss for fast and stable reward optimization suitable for\nlarge NMT models. In experiments, we perform automatic and human evaluations to\ncompare models trained with smoothed BLEU and BLEURT to the baseline models.\nResults show that the reward optimization with BLEURT is able to increase the\nmetric scores by a large margin, in contrast to limited gain when training with\nsmoothed BLEU. The human evaluation shows that models trained with BLEURT\nimprove adequacy and coverage of translations. Code is available via\nhttps://github.com/naver-ai/MetricMT.", "published": "2021-04-15 15:53:31", "link": "http://arxiv.org/abs/2104.07541v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieval Augmentation Reduces Hallucination in Conversation", "abstract": "Despite showing increasingly human-like conversational abilities,\nstate-of-the-art dialogue models often suffer from factual incorrectness and\nhallucination of knowledge (Roller et al., 2020). In this work we explore the\nuse of neural-retrieval-in-the-loop architectures - recently shown to be\neffective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) -\nfor knowledge-grounded dialogue, a task that is arguably more challenging as it\nrequires querying based on complex multi-turn dialogue context and generating\nconversationally coherent responses. We study various types of architectures\nwith multiple components - retrievers, rankers, and encoder-decoders - with the\ngoal of maximizing knowledgeability while retaining conversational ability. We\ndemonstrate that our best models obtain state-of-the-art performance on two\nknowledge-grounded conversational tasks. The models exhibit open-domain\nconversational capabilities, generalize effectively to scenarios not within the\ntraining data, and, as verified by human evaluations, substantially reduce the\nwell-known problem of knowledge hallucination in state-of-the-art chatbots.", "published": "2021-04-15 16:24:43", "link": "http://arxiv.org/abs/2104.07567v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ExplaGraphs: An Explanation Graph Generation Task for Structured\n  Commonsense Reasoning", "abstract": "Recent commonsense-reasoning tasks are typically discriminative in nature,\nwhere a model answers a multiple-choice question for a certain context.\nDiscriminative tasks are limiting because they fail to adequately evaluate the\nmodel's ability to reason and explain predictions with underlying commonsense\nknowledge. They also allow such models to use reasoning shortcuts and not be\n\"right for the right reasons\". In this work, we present ExplaGraphs, a new\ngenerative and structured commonsense-reasoning task (and an associated\ndataset) of explanation graph generation for stance prediction. Specifically,\ngiven a belief and an argument, a model has to predict if the argument supports\nor counters the belief and also generate a commonsense-augmented graph that\nserves as non-trivial, complete, and unambiguous explanation for the predicted\nstance. We collect explanation graphs through a novel Create-Verify-And-Refine\ngraph collection framework that improves the graph quality (up to 90%) via\nmultiple rounds of verification and refinement. A significant 79% of our graphs\ncontain external commonsense nodes with diverse structures and reasoning\ndepths. Next, we propose a multi-level evaluation framework, consisting of\nautomatic metrics and human evaluation, that check for the structural and\nsemantic correctness of the generated graphs and their degree of match with\nground-truth graphs. Finally, we present several structured,\ncommonsense-augmented, and text generation models as strong starting points for\nthis explanation graph generation task, and observe that there is a large gap\nwith human performance, thereby encouraging future work for this new\nchallenging task. ExplaGraphs will be publicly available at\nhttps://explagraphs.github.io.", "published": "2021-04-15 17:51:36", "link": "http://arxiv.org/abs/2104.07644v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sublanguage: A Serious Issue Affects Pretrained Models in Legal Domain", "abstract": "Legal English is a sublanguage that is important for everyone but not for\neveryone to understand. Pretrained models have become best practices among\ncurrent deep learning approaches for different problems. It would be a waste or\neven a danger if these models were applied in practice without knowledge of the\nsublanguage of the law. In this paper, we raise the issue and propose a trivial\nsolution by introducing BERTLaw a legal sublanguage pretrained model. The\npaper's experiments demonstrate the superior effectiveness of the method\ncompared to the baseline pretrained model", "published": "2021-04-15 21:25:53", "link": "http://arxiv.org/abs/2104.07782v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multitasking Inhibits Semantic Drift", "abstract": "When intelligent agents communicate to accomplish shared goals, how do these\ngoals shape the agents' language? We study the dynamics of learning in latent\nlanguage policies (LLPs), in which instructor agents generate natural-language\nsubgoal descriptions and executor agents map these descriptions to low-level\nactions. LLPs can solve challenging long-horizon reinforcement learning\nproblems and provide a rich model for studying task-oriented language use. But\nprevious work has found that LLP training is prone to semantic drift (use of\nmessages in ways inconsistent with their original natural language meanings).\nHere, we demonstrate theoretically and empirically that multitask training is\nan effective counter to this problem: we prove that multitask training\neliminates semantic drift in a well-studied family of signaling games, and show\nthat multitask training of neural LLPs in a complex strategy game reduces drift\nand while improving sample efficiency.", "published": "2021-04-15 03:42:17", "link": "http://arxiv.org/abs/2104.07219v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tracking entities in technical procedures -- a new dataset and baselines", "abstract": "We introduce TechTrack, a new dataset for tracking entities in technical\nprocedures. The dataset, prepared by annotating open domain articles from\nWikiHow, consists of 1351 procedures, e.g., \"How to connect a printer\",\nidentifies more than 1200 unique entities with an average of 4.7 entities per\nprocedure. We evaluate the performance of state-of-the-art models on the\nentity-tracking task and find that they are well below the human annotation\nperformance. We describe how TechTrack can be used to take forward the research\non understanding procedures from temporal texts.", "published": "2021-04-15 11:16:41", "link": "http://arxiv.org/abs/2104.07378v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Time-Stamped Language Model: Teaching Language Models to Understand the\n  Flow of Events", "abstract": "Tracking entities throughout a procedure described in a text is challenging\ndue to the dynamic nature of the world described in the process. Firstly, we\npropose to formulate this task as a question answering problem. This enables us\nto use pre-trained transformer-based language models on other QA benchmarks by\nadapting those to the procedural text understanding. Secondly, since the\ntransformer-based language models cannot encode the flow of events by\nthemselves, we propose a Time-Stamped Language Model~(TSLM model) to encode\nevent information in LMs architecture by introducing the timestamp encoding.\nOur model evaluated on the Propara dataset shows improvements on the published\nstate-of-the-art results with a $3.1\\%$ increase in F1 score. Moreover, our\nmodel yields better results on the location prediction task on the NPN-Cooking\ndataset. This result indicates that our approach is effective for procedural\ntext understanding in general.", "published": "2021-04-15 17:50:41", "link": "http://arxiv.org/abs/2104.07635v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Effect of Efficient Messaging and Input Variability on Neural-Agent\n  Iterated Language Learning", "abstract": "Natural languages display a trade-off among different strategies to convey\nsyntactic structure, such as word order or inflection. This trade-off, however,\nhas not appeared in recent simulations of iterated language learning with\nneural network agents (Chaabouni et al., 2019b). We re-evaluate this result in\nlight of three factors that play an important role in comparable experiments\nfrom the Language Evolution field: (i) speaker bias towards efficient\nmessaging, (ii) non systematic input languages, and (iii) learning bottleneck.\nOur simulations show that neural agents mainly strive to maintain the utterance\ntype distribution observed during learning, instead of developing a more\nefficient or systematic language.", "published": "2021-04-15 17:50:42", "link": "http://arxiv.org/abs/2104.07637v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust Optimization for Multilingual Translation with Imbalanced Data", "abstract": "Multilingual models are parameter-efficient and especially effective in\nimproving low-resource languages by leveraging crosslingual transfer. Despite\nrecent advance in massive multilingual translation with ever-growing model and\ndata, how to effectively train multilingual models has not been well\nunderstood. In this paper, we show that a common situation in multilingual\ntraining, data imbalance among languages, poses optimization tension between\nhigh resource and low resource languages where the found multilingual solution\nis often sub-optimal for low resources. We show that common training method\nwhich upsamples low resources can not robustly optimize population loss with\nrisks of either underfitting high resource languages or overfitting low\nresource ones. Drawing on recent findings on the geometry of loss landscape and\nits effect on generalization, we propose a principled optimization algorithm,\nCurvature Aware Task Scaling (CATS), which adaptively rescales gradients from\ndifferent tasks with a meta objective of guiding multilingual training to\nlow-curvature neighborhoods with uniformly low loss for all languages. We ran\nexperiments on common benchmarks (TED, WMT and OPUS-100) with varying degrees\nof data imbalance. CATS effectively improved multilingual optimization and as a\nresult demonstrated consistent gains on low resources ($+0.8$ to $+2.2$ BLEU)\nwithout hurting high resources. In addition, CATS is robust to\noverparameterization and large batch size training, making it a promising\ntraining method for massive multilingual models that truly improve low resource\nlanguages.", "published": "2021-04-15 17:51:03", "link": "http://arxiv.org/abs/2104.07639v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization\n  for Relation Extraction", "abstract": "Recently, prompt-tuning has achieved promising results for specific few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces\n(i.e., templates) into the input and transform a classification task into a\nmasked language modeling problem. However, for relation extraction, determining\nan appropriate prompt template requires domain expertise, and it is cumbersome\nand time-consuming to obtain a suitable label word. Furthermore, there exists\nabundant semantic and prior knowledge among the relation labels that cannot be\nignored. To this end, we focus on incorporating knowledge among relation labels\ninto prompt-tuning for relation extraction and propose a Knowledge-aware\nPrompt-tuning approach with synergistic optimization (KnowPrompt).\nSpecifically, we inject latent knowledge contained in relation labels into\nprompt construction with learnable virtual type words and answer words. Then,\nwe synergistically optimize their representation with structured constraints.\nExtensive experimental results on five datasets with standard and low-resource\nsettings demonstrate the effectiveness of our approach. Our code and datasets\nare available in https://github.com/zjunlp/KnowPrompt for reproducibility.", "published": "2021-04-15 17:57:43", "link": "http://arxiv.org/abs/2104.07650v7", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to Train BERT with an Academic Budget", "abstract": "While large language models a la BERT are used ubiquitously in NLP,\npretraining them is considered a luxury that only a few well-funded industry\nlabs can afford. How can one train such models with a more modest budget? We\npresent a recipe for pretraining a masked language model in 24 hours using a\nsingle low-end deep learning server. We demonstrate that through a combination\nof software optimizations, design choices, and hyperparameter tuning, it is\npossible to produce models that are competitive with BERT-base on GLUE tasks at\na fraction of the original pretraining cost.", "published": "2021-04-15 18:17:12", "link": "http://arxiv.org/abs/2104.07705v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?", "abstract": "Large Transformers pretrained over clinical notes from Electronic Health\nRecords (EHR) have afforded substantial gains in performance on predictive\nclinical tasks. The cost of training such models (and the necessity of data\naccess to do so) coupled with their utility motivates parameter sharing, i.e.,\nthe release of pretrained models such as ClinicalBERT. While most efforts have\nused deidentified EHR, many researchers have access to large sets of sensitive,\nnon-deidentified EHR with which they might train a BERT model (or similar).\nWould it be safe to release the weights of such a model if they did? In this\nwork, we design a battery of approaches intended to recover Personal Health\nInformation (PHI) from a trained BERT. Specifically, we attempt to recover\npatient names and conditions with which they are associated. We find that\nsimple probing methods are not able to meaningfully extract sensitive\ninformation from BERT trained over the MIMIC-III corpus of EHR. However, more\nsophisticated \"attacks\" may succeed in doing so: To facilitate such research,\nwe make our experimental setup and baseline probing models available at\nhttps://github.com/elehman16/exposing_patient_data_release", "published": "2021-04-15 20:40:05", "link": "http://arxiv.org/abs/2104.07762v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparative Study of Learning Outcomes for Online Learning Platforms", "abstract": "Personalization and active learning are key aspects to successful learning.\nThese aspects are important to address in intelligent educational applications,\nas they help systems to adapt and close the gap between students with varying\nabilities, which becomes increasingly important in the context of online and\ndistance learning. We run a comparative head-to-head study of learning outcomes\nfor two popular online learning platforms: Platform A, which follows a\ntraditional model delivering content over a series of lecture videos and\nmultiple-choice quizzes, and Platform B, which creates a personalized learning\nenvironment and provides problem-solving exercises and personalized feedback.\nWe report on the results of our study using pre- and post-assessment quizzes\nwith participants taking courses on an introductory data science topic on two\nplatforms. We observe a statistically significant increase in the learning\noutcomes on Platform B, highlighting the impact of well-designed and\nwell-engineered technology supporting active learning and problem-based\nlearning in online education. Moreover, the results of the self-assessment\nquestionnaire, where participants reported on perceived learning gains, suggest\nthat participants using Platform B improve their metacognition.", "published": "2021-04-15 20:40:24", "link": "http://arxiv.org/abs/2104.07763v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "I.2.0; I.2.1; I.2.7; K.3.1; G.4"], "primary_category": "cs.CY"}
{"title": "Detect and Classify -- Joint Span Detection and Classification for\n  Health Outcomes", "abstract": "A health outcome is a measurement or an observation used to capture and\nassess the effect of a treatment. Automatic detection of health outcomes from\ntext would undoubtedly speed up access to evidence necessary in healthcare\ndecision making. Prior work on outcome detection has modelled this task as\neither (a) a sequence labelling task, where the goal is to detect which text\nspans describe health outcomes, or (b) a classification task, where the goal is\nto classify a text into a pre-defined set of categories depending on an outcome\nthat is mentioned somewhere in that text. However, this decoupling of span\ndetection and classification is problematic from a modelling perspective and\nignores global structural correspondences between sentence-level and word-level\ninformation present in a given text. To address this, we propose a method that\nuses both word-level and sentence-level information to simultaneously perform\noutcome span detection and outcome type classification. In addition to\ninjecting contextual information to hidden vectors, we use label attention to\nappropriately weight both word and sentence level information. Experimental\nresults on several benchmark datasets for health outcome detection show that\nour proposed method consistently outperforms decoupled methods, reporting\ncompetitive results.", "published": "2021-04-15 21:47:15", "link": "http://arxiv.org/abs/2104.07789v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Robust Neural Retrieval Models with Synthetic Pre-Training", "abstract": "Recent work has shown that commonly available machine reading comprehension\n(MRC) datasets can be used to train high-performance neural information\nretrieval (IR) systems. However, the evaluation of neural IR has so far been\nlimited to standard supervised learning settings, where they have outperformed\ntraditional term matching baselines. We conduct in-domain and out-of-domain\nevaluations of neural IR, and seek to improve its robustness across different\nscenarios, including zero-shot settings. We show that synthetic training\nexamples generated using a sequence-to-sequence generator can be effective\ntowards this goal: in our experiments, pre-training with synthetic examples\nimproves retrieval performance in both in-domain and out-of-domain evaluation\non five different test sets.", "published": "2021-04-15 22:12:01", "link": "http://arxiv.org/abs/2104.07800v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Detecting Polarized Topics Using Partisanship-aware Contextualized Topic\n  Embeddings", "abstract": "Growing polarization of the news media has been blamed for fanning\ndisagreement, controversy and even violence. Early identification of polarized\ntopics is thus an urgent matter that can help mitigate conflict. However,\naccurate measurement of topic-wise polarization is still an open research\nchallenge. To address this gap, we propose Partisanship-aware Contextualized\nTopic Embeddings (PaCTE), a method to automatically detect polarized topics\nfrom partisan news sources. Specifically, utilizing a language model that has\nbeen finetuned on recognizing partisanship of the news articles, we represent\nthe ideology of a news corpus on a topic by corpus-contextualized topic\nembedding and measure the polarization using cosine distance. We apply our\nmethod to a dataset of news articles about the COVID-19 pandemic. Extensive\nexperiments on different news sources and topics demonstrate the efficacy of\nour method to capture topical polarization, as indicated by its effectiveness\nof retrieving the most polarized topics.", "published": "2021-04-15 23:05:52", "link": "http://arxiv.org/abs/2104.07814v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Method to Reveal Speaker Identity in Distributed ASR Training, and How\n  to Counter It", "abstract": "End-to-end Automatic Speech Recognition (ASR) models are commonly trained\nover spoken utterances using optimization methods like Stochastic Gradient\nDescent (SGD). In distributed settings like Federated Learning, model training\nrequires transmission of gradients over a network. In this work, we design the\nfirst method for revealing the identity of the speaker of a training utterance\nwith access only to a gradient. We propose Hessian-Free Gradients Matching, an\ninput reconstruction technique that operates without second derivatives of the\nloss function (required in prior works), which can be expensive to compute. We\nshow the effectiveness of our method using the DeepSpeech model architecture,\ndemonstrating that it is possible to reveal the speaker's identity with 34%\ntop-1 accuracy (51% top-5 accuracy) on the LibriSpeech dataset. Further, we\nstudy the effect of two well-known techniques, Differentially Private SGD and\nDropout, on the success of our method. We show that a dropout rate of 0.2 can\nreduce the speaker identity accuracy to 0% top-1 (0.5% top-5).", "published": "2021-04-15 23:15:12", "link": "http://arxiv.org/abs/2104.07815v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gradient-based Adversarial Attacks against Text Transformers", "abstract": "We propose the first general-purpose gradient-based attack against\ntransformer models. Instead of searching for a single adversarial example, we\nsearch for a distribution of adversarial examples parameterized by a\ncontinuous-valued matrix, hence enabling gradient-based optimization. We\nempirically demonstrate that our white-box attack attains state-of-the-art\nattack performance on a variety of natural language tasks. Furthermore, we show\nthat a powerful black-box transfer attack, enabled by sampling from the\nadversarial distribution, matches or exceeds existing methods, while only\nrequiring hard-label outputs.", "published": "2021-04-15 17:43:43", "link": "http://arxiv.org/abs/2104.13733v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Recent Abstract Summarization Techniques", "abstract": "This paper surveys several recent abstract summarization methods: T5,\nPegasus, and ProphetNet. We implement the systems in two languages: English and\nIndonesian languages. We investigate the impact of pre-training models (one T5,\nthree Pegasuses, three ProphetNets) on several Wikipedia datasets in English\nand Indonesian language and compare the results to the Wikipedia systems'\nsummaries. The T5-Large, the Pegasus-XSum, and the ProphetNet-CNNDM provide the\nbest summarization. The most significant factors that influence ROUGE\nperformance are coverage, density, and compression. The higher the scores, the\nbetter the summary. Other factors that influence the ROUGE scores are the\npre-training goal, the dataset's characteristics, the dataset used for testing\nthe pre-trained model, and the cross-lingual function. Several suggestions to\nimprove this paper's limitation are: 1) assure that the dataset used for the\npre-training model must sufficiently large, contains adequate instances for\nhandling cross-lingual purpose; 2) Advanced process (finetuning) shall be\nreasonable. We recommend using the large dataset consists of comprehensive\ncoverage of topics from many languages before implementing advanced processes\nsuch as the train-infer-train procedure to the zero-shot translation in the\ntraining stage of the pre-training model.", "published": "2021-04-15 20:01:34", "link": "http://arxiv.org/abs/2105.00824v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68P20 (Primary) 68T07, 68T50 (Secondary)", "H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Integration of Pre-trained Networks with Continuous Token Interface for\n  End-to-End Spoken Language Understanding", "abstract": "Most End-to-End (E2E) SLU networks leverage the pre-trained ASR networks but\nstill lack the capability to understand the semantics of utterances, crucial\nfor the SLU task. To solve this, recently proposed studies use pre-trained NLU\nnetworks. However, it is not trivial to fully utilize both pre-trained\nnetworks; many solutions were proposed, such as Knowledge Distillation,\ncross-modal shared embedding, and network integration with Interface. We\npropose a simple and robust integration method for the E2E SLU network with\nnovel Interface, Continuous Token Interface (CTI), the junctional\nrepresentation of the ASR and NLU networks when both networks are pre-trained\nwith the same vocabulary. Because the only difference is the noise level, we\ndirectly feed the ASR network's output to the NLU network. Thus, we can train\nour SLU network in an E2E manner without additional modules, such as\nGumbel-Softmax. We evaluate our model using SLURP, a challenging SLU dataset\nand achieve state-of-the-art scores on both intent classification and slot\nfilling tasks. We also verify the NLU network, pre-trained with Masked Language\nModel, can utilize a noisy textual representation of CTI. Moreover, we show our\nmodel can be trained with multi-task learning from heterogeneous data even\nafter integration with CTI.", "published": "2021-04-15 05:59:28", "link": "http://arxiv.org/abs/2104.07253v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Role of Context in Detecting Previously Fact-Checked Claims", "abstract": "Recent years have seen the proliferation of disinformation and fake news\nonline. Traditional approaches to mitigate these issues is to use manual or\nautomatic fact-checking. Recently, another approach has emerged: checking\nwhether the input claim has previously been fact-checked, which can be done\nautomatically, and thus fast, while also offering credibility and\nexplainability, thanks to the human fact-checking and explanations in the\nassociated fact-checking article. Here, we focus on claims made in a political\ndebate and we study the impact of modeling the context of the claim: both on\nthe source side, i.e., in the debate, as well as on the target side, i.e., in\nthe fact-checking explanation document. We do this by modeling the local\ncontext, the global context, as well as by means of co-reference resolution,\nand multi-hop reasoning over the sentences of the document describing the\nfact-checked claim. The experimental results show that each of these represents\na valuable information source, but that modeling the source-side context is\nmost important, and can yield 10+ points of absolute improvement over a\nstate-of-the-art model.", "published": "2021-04-15 12:39:37", "link": "http://arxiv.org/abs/2104.07423v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.NE", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Ensemble of MRR and NDCG models for Visual Dialog", "abstract": "Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.", "published": "2021-04-15 15:09:32", "link": "http://arxiv.org/abs/2104.07511v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Investigating the Utility of Multimodal Conversational Technology and\n  Audiovisual Analytic Measures for the Assessment and Monitoring of\n  Amyotrophic Lateral Sclerosis at Scale", "abstract": "We propose a cloud-based multimodal dialog platform for the remote assessment\nand monitoring of Amyotrophic Lateral Sclerosis (ALS) at scale. This paper\npresents our vision, technology setup, and an initial investigation of the\nefficacy of the various acoustic and visual speech metrics automatically\nextracted by the platform. 82 healthy controls and 54 people with ALS (pALS)\nwere instructed to interact with the platform and completed a battery of\nspeaking tasks designed to probe the acoustic, articulatory, phonatory, and\nrespiratory aspects of their speech. We find that multiple acoustic (rate,\nduration, voicing) and visual (higher order statistics of the jaw and lip)\nspeech metrics show statistically significant differences between controls,\nbulbar symptomatic and bulbar pre-symptomatic patients. We report on the\nsensitivity and specificity of these metrics using five-fold cross-validation.\nWe further conducted a LASSO-LARS regression analysis to uncover the relative\ncontributions of various acoustic and visual features in predicting the\nseverity of patients' ALS (as measured by their self-reported ALSFRS-R scores).\nOur results provide encouraging evidence of the utility of automatically\nextracted audiovisual analytics for scalable remote patient assessment and\nmonitoring in ALS.", "published": "2021-04-15 08:43:07", "link": "http://arxiv.org/abs/2104.07310v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EnvGAN: Adversarial Synthesis of Environmental Sounds for Data\n  Augmentation", "abstract": "The research in Environmental Sound Classification (ESC) has been\nprogressively growing with the emergence of deep learning algorithms. However,\ndata scarcity poses a major hurdle for any huge advance in this domain. Data\naugmentation offers an excellent solution to this problem. While Generative\nAdversarial Networks (GANs) have been successful in generating synthetic speech\nand sounds of musical instruments, they have hardly been applied to the\ngeneration of environmental sounds. This paper presents EnvGAN, the first ever\napplication of GANs for the adversarial generation of environmental sounds. Our\nexperiments on three standard ESC datasets illustrate that the EnvGAN can\nsynthesize audio similar to the ones in the datasets. The suggested method of\naugmentation outshines most of the futuristic techniques for audio\naugmentation.", "published": "2021-04-15 09:26:19", "link": "http://arxiv.org/abs/2104.07326v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards end-to-end F0 voice conversion based on Dual-GAN with\n  convolutional wavelet kernels", "abstract": "This paper presents a end-to-end framework for the F0 transformation in the\ncontext of expressive voice conversion. A single neural network is proposed, in\nwhich a first module is used to learn F0 representation over different temporal\nscales and a second adversarial module is used to learn the transformation from\none emotion to another. The first module is composed of a convolution layer\nwith wavelet kernels so that the various temporal scales of F0 variations can\nbe efficiently encoded. The single decomposition/transformation network allows\nto learn in a end-to-end manner the F0 decomposition that are optimal with\nrespect to the transformation, directly from the raw F0 signal.", "published": "2021-04-15 07:42:59", "link": "http://arxiv.org/abs/2104.07283v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Continual Learning for Fake Audio Detection", "abstract": "Fake audio attack becomes a major threat to the speaker verification system.\nAlthough current detection approaches have achieved promising results on\ndataset-specific scenarios, they encounter difficulties on unseen spoofing\ndata. Fine-tuning and retraining from scratch have been applied to incorporate\nnew data. However, fine-tuning leads to performance degradation on previous\ndata. Retraining takes a lot of time and computation resources. Besides,\nprevious data are unavailable due to privacy in some situations. To solve the\nabove problems, this paper proposes detecting fake without forgetting, a\ncontinual-learning-based method, to make the model learn new spoofing attacks\nincrementally. A knowledge distillation loss is introduced to loss function to\npreserve the memory of original model. Supposing the distribution of genuine\nvoice is consistent among different scenarios, an extra embedding similarity\nloss is used as another constraint to further do a positive sample alignment.\nExperiments are conducted on the ASVspoof2019 dataset. The results show that\nour proposed method outperforms fine-tuning by the relative reduction of\naverage equal error rate up to 81.62%.", "published": "2021-04-15 07:57:05", "link": "http://arxiv.org/abs/2104.07286v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Attentive Speech Emotion Recognition", "abstract": "Speech Emotion Recognition (SER) task has known significant improvements over\nthe last years with the advent of Deep Neural Networks (DNNs). However, even\nthe most successful methods are still rather failing when adaptation to\nspecific speakers and scenarios is needed, inevitably leading to poorer\nperformances when compared to humans. In this paper, we present novel work\nbased on the idea of teaching the emotion recognition network about speaker\nidentity. Our system is a combination of two ACRNN classifiers respectively\ndedicated to speaker and emotion recognition. The first informs the latter\nthrough a Self Speaker Attention (SSA) mechanism that is shown to considerably\nhelp to focus on emotional information of the speech signal. Experiments on\nsocial attitudes database Att-HACK and IEMOCAP corpus demonstrate the\neffectiveness of the proposed method and achieve the state-of-the-art\nperformance in terms of unweighted average recall.", "published": "2021-04-15 07:59:37", "link": "http://arxiv.org/abs/2104.07288v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conditional independence for pretext task selection in Self-supervised\n  speech representation learning", "abstract": "Through solving pretext tasks, self-supervised learning (SSL) leverages\nunlabeled data to extract useful latent representations replacing traditional\ninput features in the downstream task. A common pretext task consists in\npretraining a SSL model on pseudo-labels derived from the original signal. This\ntechnique is particularly relevant for speech data where various meaningful\nsignal processing features may serve as pseudo-labels. However, the process of\nselecting pseudo-labels, for speech or other types of data, remains mostly\nunexplored and currently relies on observing the results on the final\ndownstream task. Nevertheless, this methodology is not sustainable at scale due\nto substantial computational (hence carbon) costs. Thus, this paper introduces\na practical and theoretical framework to select relevant pseudo-labels with\nrespect to a given downstream task. More precisely, we propose a functional\nestimator of the pseudo-label utility grounded in the conditional independence\ntheory, which does not require any training. The experiments conducted on\nspeaker recognition and automatic speech recognition validate our estimator,\nshowing a significant correlation between the performance observed on the\ndownstream task and the utility estimates obtained with our approach,\nfacilitating the prospection of relevant pseudo-labels for self-supervised\nspeech representation learning.", "published": "2021-04-15 11:32:59", "link": "http://arxiv.org/abs/2104.07388v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Cross-domain Speech Recognition with Unsupervised Character-level\n  Distribution Matching", "abstract": "End-to-end automatic speech recognition (ASR) can achieve promising\nperformance with large-scale training data. However, it is known that domain\nmismatch between training and testing data often leads to a degradation of\nrecognition accuracy. In this work, we focus on the unsupervised domain\nadaptation for ASR and propose CMatch, a Character-level distribution matching\nmethod to perform fine-grained adaptation between each character in two\ndomains. First, to obtain labels for the features belonging to each character,\nwe achieve frame-level label assignment using the Connectionist Temporal\nClassification (CTC) pseudo labels. Then, we match the character-level\ndistributions using Maximum Mean Discrepancy. We train our algorithm using the\nself-training technique. Experiments on the Libri-Adapt dataset show that our\nproposed approach achieves 14.39% and 16.50% relative Word Error Rate (WER)\nreduction on both cross-device and cross-environment ASR. We also\ncomprehensively analyze the different strategies for frame-level label\nassignment and Transformer adaptations.", "published": "2021-04-15 14:36:54", "link": "http://arxiv.org/abs/2104.07491v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spectrogram Inpainting for Interactive Generation of Instrument Sounds", "abstract": "Modern approaches to sound synthesis using deep neural networks are hard to\ncontrol, especially when fine-grained conditioning information is not\navailable, hindering their adoption by musicians.\n  In this paper, we cast the generation of individual instrumental notes as an\ninpainting-based task, introducing novel and unique ways to iteratively shape\nsounds. To this end, we propose a two-step approach: first, we adapt the\nVQ-VAE-2 image generation architecture to spectrograms in order to convert\nreal-valued spectrograms into compact discrete codemaps, we then implement\ntoken-masked Transformers for the inpainting-based generation of these\ncodemaps.\n  We apply the proposed architecture on the NSynth dataset on masked resampling\ntasks. Most crucially, we open-source an interactive web interface to transform\nsounds by inpainting, for artists and practitioners alike, opening up to new,\ncreative uses.", "published": "2021-04-15 15:17:31", "link": "http://arxiv.org/abs/2104.07519v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
