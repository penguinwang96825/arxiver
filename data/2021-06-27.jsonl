{"title": "Analyzing Research Trends in Inorganic Materials Literature Using NLP", "abstract": "In the field of inorganic materials science, there is a growing demand to\nextract knowledge such as physical properties and synthesis processes of\nmaterials by machine-reading a large number of papers. This is because\nmaterials researchers refer to many papers in order to come up with promising\nterms of experiments for material synthesis. However, there are only a few\nsystems that can extract material names and their properties. This study\nproposes a large-scale natural language processing (NLP) pipeline for\nextracting material names and properties from materials science literature to\nenable the search and retrieval of results in materials science. Therefore, we\npropose a label definition for extracting material names and properties and\naccordingly build a corpus containing 836 annotated paragraphs extracted from\n301 papers for training a named entity recognition (NER) model. Experimental\nresults demonstrate the utility of this NER model; it achieves successful\nextraction with a micro-F1 score of 78.1%. To demonstrate the efficacy of our\napproach, we present a thorough evaluation on a real-world automatically\nannotated corpus by applying our trained NER model to 12,895 materials science\npapers. We analyze the trend in materials science by visualizing the outputs of\nthe NLP pipeline. For example, the country-by-year analysis indicates that in\nrecent years, the number of papers on \"MoS2,\" a material used in perovskite\nsolar cells, has been increasing rapidly in China but decreasing in the United\nStates. Further, according to the conditions-by-year analysis, the processing\ntemperature of the catalyst material \"PEDOT:PSS\" is shifting below 200 degree,\nand the number of reports with a processing time exceeding 5 h is increasing\nslightly.", "published": "2021-06-27 06:29:10", "link": "http://arxiv.org/abs/2106.14157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persian Causality Corpus (PerCause) and the Causality Detection\n  Benchmark", "abstract": "Recognizing causal elements and causal relations in text is one of the\nchallenging issues in natural language processing; specifically, in low\nresource languages such as Persian. In this research we prepare a causality\nhuman annotated corpus for the Persian language which consists of 4446\nsentences and 5128 causal relations and three labels of cause, effect and\ncausal mark -- if possibl -- are specified for each relation. We have used this\ncorpus to train a system for detecting causal elements boundaries. Also, we\npresent a causality detection benchmark for three machine learning methods and\ntwo deep learning systems based on this corpus. Performance evaluations\nindicate that our best total result is obtained through CRF classifier which\nhas F-measure of 0.76 and the best accuracy obtained through Bi-LSTM-CRF deep\nlearning method with Accuracy equal to %91.4.", "published": "2021-06-27 07:54:48", "link": "http://arxiv.org/abs/2106.14165v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "KGRefiner: Knowledge Graph Refinement for Improving Accuracy of\n  Translational Link Prediction Methods", "abstract": "The Link Prediction is the task of predicting missing relations between\nentities of the knowledge graph. Recent work in link prediction has attempted\nto provide a model for increasing link prediction accuracy by using more layers\nin neural network architecture. In this paper, we propose a novel method of\nrefining the knowledge graph so that link prediction operation can be performed\nmore accurately using relatively fast translational models. Translational link\nprediction models, such as TransE, TransH, TransD, have less complexity than\ndeep learning approaches. Our method uses the hierarchy of relationships and\nentities in the knowledge graph to add the entity information as auxiliary\nnodes to the graph and connect them to the nodes which contain this information\nin their hierarchy. Our experiments show that our method can significantly\nincrease the performance of translational link prediction methods in H@10, MR,\nMRR.", "published": "2021-06-27 13:32:39", "link": "http://arxiv.org/abs/2106.14233v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Closer Look at How Fine-tuning Changes BERT", "abstract": "Given the prevalence of pre-trained contextualized representations in today's\nNLP, there have been many efforts to understand what information they contain,\nand why they seem to be universally successful. The most common approach to use\nthese representations involves fine-tuning them for an end task. Yet, how\nfine-tuning changes the underlying embedding space is less studied. In this\nwork, we study the English BERT family and use two probing techniques to\nanalyze how fine-tuning changes the space. We hypothesize that fine-tuning\naffects classification performance by increasing the distances between examples\nassociated with different labels. We confirm this hypothesis with carefully\ndesigned experiments on five different NLP tasks. Via these experiments, we\nalso discover an exception to the prevailing wisdom that \"fine-tuning always\nimproves performance\". Finally, by comparing the representations before and\nafter fine-tuning, we discover that fine-tuning does not introduce arbitrary\nchanges to representations; instead, it adjusts the representations to\ndownstream tasks while largely preserving the original spatial structure of the\ndata points.", "published": "2021-06-27 17:01:43", "link": "http://arxiv.org/abs/2106.14282v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Draw Me a Flower: Processing and Grounding Abstraction in Natural\n  Language", "abstract": "Abstraction is a core tenet of human cognition and communication. When\ncomposing natural language instructions, humans naturally evoke abstraction to\nconvey complex procedures in an efficient and concise way. Yet, interpreting\nand grounding abstraction expressed in NL has not yet been systematically\nstudied in NLP, with no accepted benchmarks specifically eliciting abstraction\nin NL. In this work, we set the foundation for a systematic study of processing\nand grounding abstraction in NLP. First, we deliver a novel abstraction\nelicitation method and present Hexagons, a 2D instruction-following game. Using\nHexagons we collected over 4k naturally-occurring visually-grounded\ninstructions rich with diverse types of abstractions. From these data, we\nderive an instruction-to-execution task and assess different types of neural\nmodels. Our results show that contemporary models and modeling practices are\nsubstantially inferior to human performance, and that models' performance is\ninversely correlated with the level of abstraction, showing less satisfying\nperformance on higher levels of abstraction. These findings are consistent\nacross models and setups, confirming that abstraction is a challenging\nphenomenon deserving further attention and study in NLP/AI research.", "published": "2021-06-27 21:11:16", "link": "http://arxiv.org/abs/2106.14321v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cascade Dual-Decoder Model for Joint Entity and Relation Extraction", "abstract": "In knowledge graph construction, a challenging issue is how to extract\ncomplex (e.g., overlapping) entities and relationships from a small amount of\nunstructured historical data. The traditional pipeline methods are to divide\nthe extraction into two separate subtasks, which misses the potential\ninteraction between the two subtasks and may lead to error propagation. In this\nwork, we propose an effective cascade dual-decoder method to extract\noverlapping relational triples, which includes a text-specific relation decoder\nand a relation-corresponded entity decoder. Our approach is straightforward and\nit includes a text-specific relation decoder and a relation-corresponded entity\ndecoder. The text-specific relation decoder detects relations from a sentence\nat the text level. That is, it does this according to the semantic information\nof the whole sentence. For each extracted relation, which is with trainable\nembedding, the relation-corresponded entity decoder detects the corresponding\nhead and tail entities using a span-based tagging scheme. In this way, the\noverlapping triple problem can be tackled naturally. We conducted experiments\non a real-world open-pit mine dataset and two public datasets to verify the\nmethod's generalizability. The experimental results demonstrate the\neffectiveness and competitiveness of our proposed method and achieve better F1\nscores under strict evaluation metrics. Our implementation is available at\nhttps://github.com/prastunlp/DualDec.", "published": "2021-06-27 07:42:05", "link": "http://arxiv.org/abs/2106.14163v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PeCoQ: A Dataset for Persian Complex Question Answering over Knowledge\n  Graph", "abstract": "Question answering systems may find the answers to users' questions from\neither unstructured texts or structured data such as knowledge graphs.\nAnswering questions using supervised learning approaches including deep\nlearning models need large training datasets. In recent years, some datasets\nhave been presented for the task of Question answering over knowledge graphs,\nwhich is the focus of this paper. Although many datasets in English were\nproposed, there have been a few question-answering datasets in Persian. This\npaper introduces \\textit{PeCoQ}, a dataset for Persian question answering. This\ndataset contains 10,000 complex questions and answers extracted from the\nPersian knowledge graph, FarsBase. For each question, the SPARQL query and two\nparaphrases that were written by linguists are provided as well. There are\ndifferent types of complexities in the dataset, such as multi-relation,\nmulti-entity, ordinal, and temporal constraints. In this paper, we discuss the\ndataset's characteristics and describe our methodology for building it.", "published": "2021-06-27 08:21:23", "link": "http://arxiv.org/abs/2106.14167v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WVOQ at SemEval-2021 Task 6: BART for Span Detection and Classification", "abstract": "A novel solution to span detection and classification is presented in which a\nBART EncoderDecoder model is used to transform textual input into a version\nwith XML-like marked up spans. This markup is subsequently translated to an\nidentification of the beginning and end of fragments and of their classes.\nDiscussed is how pre-training methodology both explains the relative success of\nthis method and its limitations. This paper reports on participation in task 6\nof SemEval-2021: Detection of Persuasion Techniques in Texts and Images.", "published": "2021-06-27 07:59:22", "link": "http://arxiv.org/abs/2107.05467v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Conceptual Blending with Large-scale Language and Vision Models", "abstract": "We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.", "published": "2021-06-27 02:48:39", "link": "http://arxiv.org/abs/2106.14127v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression", "abstract": "Symbolic regression is the task of identifying a mathematical expression that\nbest fits a provided dataset of input and output values. Due to the richness of\nthe space of mathematical expressions, symbolic regression is generally a\nchallenging problem. While conventional approaches based on genetic evolution\nalgorithms have been used for decades, deep learning-based methods are\nrelatively new and an active research area. In this work, we present\nSymbolicGPT, a novel transformer-based language model for symbolic regression.\nThis model exploits the advantages of probabilistic language models like GPT,\nincluding strength in performance and flexibility. Through comprehensive\nexperiments, we show that our model performs strongly compared to competing\nmodels with respect to the accuracy, running time, and data efficiency.", "published": "2021-06-27 03:26:35", "link": "http://arxiv.org/abs/2106.14131v1", "categories": ["cs.LG", "cs.CL", "cs.SC"], "primary_category": "cs.LG"}
{"title": "Power Law Graph Transformer for Machine Translation and Representation\n  Learning", "abstract": "We present the Power Law Graph Transformer, a transformer model with well\ndefined deductive and inductive tasks for prediction and representation\nlearning. The deductive task learns the dataset level (global) and instance\nlevel (local) graph structures in terms of learnable power law distribution\nparameters. The inductive task outputs the prediction probabilities using the\ndeductive task output, similar to a transductive model. We trained our model\nwith Turkish-English and Portuguese-English datasets from TED talk transcripts\nfor machine translation and compared the model performance and characteristics\nto a transformer model with scaled dot product attention trained on the same\nexperimental setup. We report BLEU scores of $17.79$ and $28.33$ on the\nTurkish-English and Portuguese-English translation tasks with our model,\nrespectively. We also show how a duality between a quantization set and\nN-dimensional manifold representation can be leveraged to transform between\nlocal and global deductive-inductive outputs using successive application of\nlinear and non-linear transformations end-to-end.", "published": "2021-06-27 15:59:37", "link": "http://arxiv.org/abs/2107.02039v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation", "abstract": "This paper presents a methodology for using LLVM-based tools to tune the\nDCA++ (dynamical clusterapproximation) application that targets the new ARM\nA64FX processor. The goal is to describethe changes required for the new\narchitecture and generate efficient single instruction/multiple data(SIMD)\ninstructions that target the new Scalable Vector Extension instruction set.\nDuring manualtuning, the authors used the LLVM tools to improve code\nparallelization by using OpenMP SIMD,refactored the code and applied\ntransformation that enabled SIMD optimizations, and ensured thatthe correct\nlibraries were used to achieve optimal performance. By applying these code\nchanges, codespeed was increased by 1.98X and 78 GFlops were achieved on the\nA64FX processor. The authorsaim to automatize parts of the efforts in the\nOpenMP Advisor tool, which is built on top of existingand newly introduced LLVM\ntooling.", "published": "2021-06-27 22:38:16", "link": "http://arxiv.org/abs/2106.14332v1", "categories": ["cs.DC", "cond-mat.mtrl-sci", "cs.AR", "cs.CL", "cs.SE"], "primary_category": "cs.DC"}
{"title": "Listen As You Wish: Audio based Event Detection via Text-to-Audio\n  Grounding in Smart Cities", "abstract": "With the development of internet of things technologies, tremendous sensor\naudio data has been produced, which poses great challenges to audio-based event\ndetection in smart cities. In this paper, we target a challenging audio-based\nevent detection task, namely, text-to-audio grounding. In addition to precisely\nlocalizing all of the desired on- and off-sets in the untrimmed audio, this\nchallenging new task requires extensive acoustic and linguistic comprehension\nas well as the reasoning for the crossmodal matching relations between the\naudio and query. The current approaches often treat the query as an entire one\nthrough a global query representation in order to address those issues. We\ncontend that this strategy has several drawbacks. Firstly, the interactions\nbetween the query and the audio are not fully utilized. Secondly, it has not\ndistinguished the importance of different keywords in a query. In addition,\nsince the audio clips are of arbitrary lengths, there exist many segments which\nare irrelevant to the query but have not been filtered out in the approach.\nThis further hinders the effective grounding of desired segments. Motivated by\nthe above concerns, a novel Cross-modal Graph Interaction (CGI) model is\nproposed to comprehensively model the relations between the words in a query\nthrough a novel language graph. To capture the fine-grained relevances between\nthe audio and query, a cross-modal attention module is introduced to generate\nsnippet-specific query representations and automatically assign higher weights\nto keywords with more important semantics. Furthermore, we develop a\ncross-gating module for the audio and query to weaken irrelevant parts and\nemphasize the important ones.", "published": "2021-06-27 03:54:36", "link": "http://arxiv.org/abs/2106.14136v3", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Use of Variational Inference in Music Emotion Recognition", "abstract": "This work was developed aiming to employ Statistical techniques to the field\nof Music Emotion Recognition, a well-recognized area within the Signal\nProcessing world, but hardly explored from the statistical point of view. Here,\nwe opened several possibilities within the field, applying modern Bayesian\nStatistics techniques and developing efficient algorithms, focusing on the\napplicability of the results obtained. Although the motivation for this project\nwas the development of a emotion-based music recommendation system, its main\ncontribution is a highly adaptable multivariate model that can be useful\ninterpreting any database where there is an interest in applying regularization\nin an efficient manner. Broadly speaking, we will explore what role a sound\ntheoretical statistical analysis can play in the modeling of an algorithm that\nis able to understand a well-known database and what can be gained with this\nkind of approach.", "published": "2021-06-27 21:41:08", "link": "http://arxiv.org/abs/2106.14323v2", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Multi-Modal Chorus Recognition for Improving Song Search", "abstract": "We discuss a novel task, Chorus Recognition, which could potentially benefit\ndownstream tasks such as song search and music summarization. Different from\nthe existing tasks such as music summarization or lyrics summarization relying\non single-modal information, this paper models chorus recognition as a\nmulti-modal one by utilizing both the lyrics and the tune information of songs.\nWe propose a multi-modal Chorus Recognition model that considers diverse\nfeatures. Besides, we also create and publish the first Chorus Recognition\ndataset containing 627 songs for public use. Our empirical study performed on\nthe dataset demonstrates that our approach outperforms several baselines in\nchorus recognition. In addition, our approach also helps to improve the\naccuracy of its downstream task - song search by more than 10.6%.", "published": "2021-06-27 16:51:05", "link": "http://arxiv.org/abs/2106.16153v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
