{"title": "Saliency-driven Word Alignment Interpretation for Neural Machine\n  Translation", "abstract": "Despite their original goal to jointly learn to align and translate, Neural\nMachine Translation (NMT) models, especially Transformer, are often perceived\nas not learning interpretable word alignments. In this paper, we show that NMT\nmodels do learn interpretable word alignments, which could only be revealed\nwith proper interpretation methods. We propose a series of such methods that\nare model-agnostic, are able to be applied either offline or online, and do not\nrequire parameter update or architectural change. We show that under the force\ndecoding setup, the alignments induced by our interpretation method are of\nbetter quality than fast-align for some systems, and when performing free\ndecoding, they agree well with the alignments induced by automatic alignment\ntools.", "published": "2019-06-25 00:43:32", "link": "http://arxiv.org/abs/1906.10282v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model-based annotation of coreference", "abstract": "Humans do not make inferences over texts, but over models of what texts are\nabout. When annotators are asked to annotate coreferent spans of text, it is\ntherefore a somewhat unnatural task. This paper presents an alternative in\nwhich we preprocess documents, linking entities to a knowledge base, and turn\nthe coreference annotation task -- in our case limited to pronouns -- into an\nannotation task where annotators are asked to assign pronouns to entities.\nModel-based annotation is shown to lead to faster annotation and higher\ninter-annotator agreement, and we argue that it also opens up for an\nalternative approach to coreference resolution. We present two new coreference\nbenchmark datasets, for English Wikipedia and English teacher-student\ndialogues, and evaluate state-of-the-art coreference resolvers on them.", "published": "2019-06-25 18:56:36", "link": "http://arxiv.org/abs/1906.10724v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Conversational Recommender in Travel", "abstract": "When traveling to a foreign country, we are often in dire need of an\nintelligent conversational agent to provide instant and informative responses\nto our various queries. However, to build such a travel agent is non-trivial.\nFirst of all, travel naturally involves several sub-tasks such as hotel\nreservation, restaurant recommendation and taxi booking etc, which invokes the\nneed for global topic control. Secondly, the agent should consider various\nconstraints like price or distance given by the user to recommend an\nappropriate venue. In this paper, we present a Deep Conversational Recommender\n(DCR) and apply to travel. It augments the sequence-to-sequence (seq2seq)\nmodels with a neural latent topic component to better guide response generation\nand make the training easier. To consider the various constraints for venue\nrecommendation, we leverage a graph convolutional network (GCN) based approach\nto capture the relationships between different venues and the match between\nvenue and dialog context. For response generation, we combine the topic-based\ncomponent with the idea of pointer networks, which allows us to effectively\nincorporate recommendation results. We perform extensive evaluation on a\nmulti-turn task-oriented dialog dataset in travel domain and the results show\nthat our method achieves superior performance as compared to a wide range of\nbaselines.", "published": "2019-06-25 04:39:26", "link": "http://arxiv.org/abs/1907.00710v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Acoustic Modeling for Automatic Lyrics-to-Audio Alignment", "abstract": "Automatic lyrics to polyphonic audio alignment is a challenging task not only\nbecause the vocals are corrupted by background music, but also there is a lack\nof annotated polyphonic corpus for effective acoustic modeling. In this work,\nwe propose (1) using additional speech and music-informed features and (2)\nadapting the acoustic models trained on a large amount of solo singing vocals\ntowards polyphonic music using a small amount of in-domain data. Incorporating\nadditional information such as voicing and auditory features together with\nconventional acoustic features aims to bring robustness against the increased\nspectro-temporal variations in singing vocals. By adapting the acoustic model\nusing a small amount of polyphonic audio data, we reduce the domain mismatch\nbetween training and testing data. We perform several alignment experiments and\npresent an in-depth alignment error analysis on acoustic features, and model\nadaptation techniques. The results demonstrate that the proposed strategy\nprovides a significant error reduction of word boundary alignment over\ncomparable existing systems, especially on more challenging polyphonic data\nwith long-duration musical interludes.", "published": "2019-06-25 08:11:20", "link": "http://arxiv.org/abs/1906.10369v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Newswire versus Social Media for Disaster Response and Recovery", "abstract": "In a disaster situation, first responders need to quickly acquire situational\nawareness and prioritize response based on the need, resources available and\nimpact. Can they do this based on digital media such as Twitter alone, or\nnewswire alone, or some combination of the two? We examine this question in the\ncontext of the 2015 Nepal Earthquakes. Because newswire articles are longer,\neffective summaries can be helpful in saving time yet giving key content. We\nevaluate the effectiveness of several unsupervised summarization techniques in\ncapturing key content. We propose a method to link tweets written by the public\nand newswire articles, so that we can compare their key characteristics:\ntimeliness, whether tweets appear earlier than their corresponding news\narticles, and content. A novel idea is to view relevant tweets as a summary of\nthe matching news article and evaluate these summaries. Whenever possible, we\npresent both quantitative and qualitative evaluations. One of our main findings\nis that tweets and newswire articles provide complementary perspectives that\nform a holistic view of the disaster situation.", "published": "2019-06-25 15:31:14", "link": "http://arxiv.org/abs/1906.10607v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Non-Parallel Sequence-to-Sequence Voice Conversion with Disentangled\n  Linguistic and Speaker Representations", "abstract": "This paper presents a method of sequence-to-sequence (seq2seq) voice\nconversion using non-parallel training data. In this method, disentangled\nlinguistic and speaker representations are extracted from acoustic features,\nand voice conversion is achieved by preserving the linguistic representations\nof source utterances while replacing the speaker representations with the\ntarget ones. Our model is built under the framework of encoder-decoder neural\nnetworks. A recognition encoder is designed to learn the disentangled\nlinguistic representations with two strategies. First, phoneme transcriptions\nof training data are introduced to provide the references for leaning\nlinguistic representations of audio signals. Second, an adversarial training\nstrategy is employed to further wipe out speaker information from the\nlinguistic representations. Meanwhile, speaker representations are extracted\nfrom audio signals by a speaker encoder. The model parameters are estimated by\ntwo-stage training, including a pretraining stage using a multi-speaker dataset\nand a fine-tuning stage using the dataset of a specific conversion pair. Since\nboth the recognition encoder and the decoder for recovering acoustic features\nare seq2seq neural networks, there are no constrains of frame alignment and\nframe-by-frame conversion in our proposed method. Experimental results showed\nthat our method obtained higher similarity and naturalness than the best\nnon-parallel voice conversion method in Voice Conversion Challenge 2018.\nBesides, the performance of our proposed method was closed to the\nstate-of-the-art parallel seq2seq voice conversion method.", "published": "2019-06-25 13:33:47", "link": "http://arxiv.org/abs/1906.10508v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Naver at ActivityNet Challenge 2019 -- Task B Active Speaker Detection\n  (AVA)", "abstract": "This report describes our submission to the ActivityNet Challenge at CVPR\n2019. We use a 3D convolutional neural network (CNN) based front-end and an\nensemble of temporal convolution and LSTM classifiers to predict whether a\nvisible person is speaking or not. Our results show significant improvements\nover the baseline on the AVA-ActiveSpeaker dataset.", "published": "2019-06-25 14:11:14", "link": "http://arxiv.org/abs/1906.10555v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DALI: a large Dataset of synchronized Audio, LyrIcs and notes,\n  automatically created using teacher-student machine learning paradigm", "abstract": "The goal of this paper is twofold. First, we introduce DALI, a large and rich\nmultimodal dataset containing 5358 audio tracks with their time-aligned vocal\nmelody notes and lyrics at four levels of granularity. The second goal is to\nexplain our methodology where dataset creation and learning models interact\nusing a teacher-student machine learning paradigm that benefits each other. We\nstart with a set of manual annotations of draft time-aligned lyrics and notes\nmade by non-expert users of Karaoke games. This set comes without audio.\nTherefore, we need to find the corresponding audio and adapt the annotations to\nit. To that end, we retrieve audio candidates from the Web. Each candidate is\nthen turned into a singing-voice probability over time using a teacher, a deep\nconvolutional neural network singing-voice detection system (SVD), trained on\ncleaned data. Comparing the time-aligned lyrics and the singing-voice\nprobability, we detect matches and update the time-alignment lyrics\naccordingly. From this, we obtain new audio sets. They are then used to train\nnew SVD students used to perform again the above comparison. The process could\nbe repeated iteratively. We show that this allows to progressively improve the\nperformances of our SVD and get better audio-matching and alignment.", "published": "2019-06-25 15:30:07", "link": "http://arxiv.org/abs/1906.10606v1", "categories": ["eess.AS", "cs.DB", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emotion Recognition Using Fusion of Audio and Video Features", "abstract": "In this paper we propose a fusion approach to continuous emotion recognition\nthat combines visual and auditory modalities in their representation spaces to\npredict the arousal and valence levels. The proposed approach employs a\npre-trained convolution neural network and transfer learning to extract\nfeatures from video frames that capture the emotional content. For the auditory\ncontent, a minimalistic set of parameters such as prosodic, excitation, vocal\ntract, and spectral descriptors are used as features. The fusion of these two\nmodalities is carried out at a feature level, before training a single support\nvector regressor (SVR) or at a prediction level, after training one SVR for\neach modality. The proposed approach also includes preprocessing and\npost-processing techniques which contribute favorably to improving the\nconcordance correlation coefficient (CCC). Experimental results for predicting\nspontaneous and natural emotions on the RECOLA dataset have shown that the\nproposed approach takes advantage of the complementary information of visual\nand auditory modalities and provides CCCs of 0.749 and 0.565 for arousal and\nvalence, respectively.", "published": "2019-06-25 16:13:41", "link": "http://arxiv.org/abs/1906.10623v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SeER: An Explainable Deep Learning MIDI-based Hybrid Song Recommender\n  System", "abstract": "State of the art music recommender systems mainly rely on either matrix\nfactorization-based collaborative filtering approaches or deep learning\narchitectures. Deep learning models usually use metadata for content-based\nfiltering or predict the next user interaction by learning from temporal\nsequences of user actions. Despite advances in deep learning for song\nrecommendation, none has taken advantage of the sequential nature of songs by\nlearning sequence models that are based on content. Aside from the importance\nof prediction accuracy, other significant aspects are important, such as\nexplainability and solving the cold start problem. In this work, we propose a\nhybrid deep learning model, called \"SeER\", that uses collaborative filtering\n(CF) and deep learning sequence models on the MIDI content of songs for\nrecommendation in order to provide more accurate personalized recommendations;\nsolve the item cold start problem; and generate a relevant explanation for a\nsong recommendation. Our evaluation experiments show promising results compared\nto state of the art baseline and hybrid song recommender systems in terms of\nranking evaluation. Moreover, based on proposed tests for offline validation,\nwe show that our personalized explanations capture properties that are in\naccordance with the user's preferences.", "published": "2019-06-25 18:23:37", "link": "http://arxiv.org/abs/1907.01640v2", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.IR"}
{"title": "LipReading with 3D-2D-CNN BLSTM-HMM and word-CTC models", "abstract": "In recent years, deep learning based machine lipreading has gained\nprominence. To this end, several architectures such as LipNet, LCANet and\nothers have been proposed which perform extremely well compared to traditional\nlipreading DNN-HMM hybrid systems trained on DCT features. In this work, we\npropose a simpler architecture of 3D-2D-CNN-BLSTM network with a bottleneck\nlayer. We also present analysis of two different approaches for lipreading on\nthis architecture. In the first approach, 3D-2D-CNN-BLSTM network is trained\nwith CTC loss on characters (ch-CTC). Then BLSTM-HMM model is trained on\nbottleneck lip features (extracted from 3D-2D-CNN-BLSTM ch-CTC network) in a\ntraditional ASR training pipeline. In the second approach, same 3D-2D-CNN-BLSTM\nnetwork is trained with CTC loss on word labels (w-CTC). The first approach\nshows that bottleneck features perform better compared to DCT features. Using\nthe second approach on Grid corpus' seen speaker test set, we report $1.3\\%$\nWER - a $55\\%$ improvement relative to LCANet. On unseen speaker test set we\nreport $8.6\\%$ WER which is $24.5\\%$ improvement relative to LipNet. We also\nverify the method on a second dataset of $81$ speakers which we collected.\nFinally, we also discuss the effect of feature duplication on BLSTM-HMM model\nperformance.", "published": "2019-06-25 14:52:54", "link": "http://arxiv.org/abs/1906.12170v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
