{"title": "Tell Me Why You Feel That Way: Processing Compositional Dependency for\n  Tree-LSTM Aspect Sentiment Triplet Extraction (TASTE)", "abstract": "Sentiment analysis has transitioned from classifying the sentiment of an\nentire sentence to providing the contextual information of what targets exist\nin a sentence, what sentiment the individual targets have, and what the causal\nwords responsible for that sentiment are. However, this has led to elaborate\nrequirements being placed on the datasets needed to train neural networks on\nthe joint triplet task of determining an entity, its sentiment, and the causal\nwords for that sentiment. Requiring this kind of data for training systems is\nproblematic, as they suffer from stacking subjective annotations and domain\nover-fitting leading to poor model generalisation when applied in new contexts.\nThese problems are also likely to be compounded as we attempt to jointly\ndetermine additional contextual elements in the future. To mitigate these\nproblems, we present a hybrid neural-symbolic method utilising a Dependency\nTree-LSTM's compositional sentiment parse structure and complementary symbolic\nrules to correctly extract target-sentiment-cause triplets from sentences\nwithout the need for triplet training data. We show that this method has the\npotential to perform in line with state-of-the-art approaches while also\nsimplifying the data required and providing a degree of interpretability\nthrough the Tree-LSTM.", "published": "2021-03-10 01:52:10", "link": "http://arxiv.org/abs/2103.05815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How does Truth Evolve into Fake News? An Empirical Study of Fake News\n  Evolution", "abstract": "Automatically identifying fake news from the Internet is a challenging\nproblem in deception detection tasks. Online news is modified constantly during\nits propagation, e.g., malicious users distort the original truth and make up\nfake news. However, the continuous evolution process would generate\nunprecedented fake news and cheat the original model. We present the Fake News\nEvolution (FNE) dataset: a new dataset tracking the fake news evolution\nprocess. Our dataset is composed of 950 paired data, each of which consists of\narticles representing the three significant phases of the evolution process,\nwhich are the truth, the fake news, and the evolved fake news. We observe the\nfeatures during the evolution and they are the disinformation techniques, text\nsimilarity, top 10 keywords, classification accuracy, parts of speech, and\nsentiment properties.", "published": "2021-03-10 09:01:34", "link": "http://arxiv.org/abs/2103.05944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Learning for Zero Shot Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) approaches employing monolingual data are\nshowing steady improvements in resource rich conditions. However, evaluations\nusing real-world low-resource languages still result in unsatisfactory\nperformance. This work proposes a novel zero-shot NMT modeling approach that\nlearns without the now-standard assumption of a pivot language sharing parallel\ndata with the zero-shot source and target languages. Our approach is based on\nthree stages: initialization from any pre-trained NMT model observing at least\nthe target language, augmentation of source sides leveraging target monolingual\ndata, and learning to optimize the initial model to the zero-shot pair, where\nthe latter two constitute a self-learning cycle. Empirical findings involving\nfour diverse (in terms of a language family, script and relatedness) zero-shot\npairs show the effectiveness of our approach with up to +5.93 BLEU improvement\nagainst a supervised bilingual baseline. Compared to unsupervised NMT,\nconsistent improvements are observed even in a domain-mismatch setting,\nattesting to the usability of our method.", "published": "2021-03-10 09:15:19", "link": "http://arxiv.org/abs/2103.05951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Result based Portable Framework for Spoken Language Understanding", "abstract": "Spoken language understanding (SLU), which is a core component of the\ntask-oriented dialogue system, has made substantial progress in the research of\nsingle-turn dialogue. However, the performance in multi-turn dialogue is still\nnot satisfactory in the sense that the existing multi-turn SLU methods have low\nportability and compatibility for other single-turn SLU models. Further,\nexisting multi-turn SLU methods do not exploit the historical predicted results\nwhen predicting the current utterance, which wastes helpful information. To gap\nthose shortcomings, in this paper, we propose a novel Result-based Portable\nFramework for SLU (RPFSLU). RPFSLU allows most existing single-turn SLU models\nto obtain the contextual information from multi-turn dialogues and takes full\nadvantage of predicted results in the dialogue history during the current\nprediction. Experimental results on the public dataset KVRET have shown that\nall SLU models in baselines acquire enhancement by RPFSLU on multi-turn SLU\ntasks.", "published": "2021-03-10 12:06:26", "link": "http://arxiv.org/abs/2103.06010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Team Phoenix at WASSA 2021: Emotion Analysis on News Stories with\n  Pre-Trained Language Models", "abstract": "Emotion is fundamental to humanity. The ability to perceive, understand and\nrespond to social interactions in a human-like manner is one of the most\ndesired capabilities in artificial agents, particularly in social-media bots.\nOver the past few years, computational understanding and detection of emotional\naspects in language have been vital in advancing human-computer interaction.\nThe WASSA Shared Task 2021 released a dataset of news-stories across two\ntracks, Track-1 for Empathy and Distress Prediction and Track-2 for\nMulti-Dimension Emotion prediction at the essay-level. We describe our system\nentry for the WASSA 2021 Shared Task (for both Track-1 and Track-2), where we\nleveraged the information from Pre-trained language models for Track-specific\nTasks. Our proposed models achieved an Average Pearson Score of 0.417 and a\nMacro-F1 Score of 0.502 in Track 1 and Track 2, respectively. In the Shared\nTask leaderboard, we secured 4th rank in Track 1 and 2nd rank in Track 2.", "published": "2021-03-10 14:00:54", "link": "http://arxiv.org/abs/2103.06057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-based Extraction of Cause-Effect Relations from Biomedical\n  Text", "abstract": "We propose a knowledge-based approach for extraction of Cause-Effect (CE)\nrelations from biomedical text. Our approach is a combination of an\nunsupervised machine learning technique to discover causal triggers and a set\nof high-precision linguistic rules to identify cause/effect arguments of these\ncausal triggers. We evaluate our approach using a corpus of 58,761\nLeukaemia-related PubMed abstracts consisting of 568,528 sentences. We could\nextract 152,655 CE triplets from this corpus where each triplet consists of a\ncause phrase, an effect phrase and a causal trigger. As compared to the\nexisting knowledge base - SemMedDB (Kilicoglu et al., 2012), the number of\nextractions are almost twice. Moreover, the proposed approach outperformed the\nexisting technique SemRep (Rindflesch and Fiszman, 2003) on a dataset of 500\nsentences.", "published": "2021-03-10 14:31:46", "link": "http://arxiv.org/abs/2103.06078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Techniques for Jointly Extracting Entities and Relations: A Survey", "abstract": "Relation Extraction is an important task in Information Extraction which\ndeals with identifying semantic relations between entity mentions.\nTraditionally, relation extraction is carried out after entity extraction in a\n\"pipeline\" fashion, so that relation extraction only focuses on determining\nwhether any semantic relation exists between a pair of extracted entity\nmentions. This leads to propagation of errors from entity extraction stage to\nrelation extraction stage. Also, entity extraction is carried out without any\nknowledge about the relations. Hence, it was observed that jointly performing\nentity and relation extraction is beneficial for both the tasks. In this paper,\nwe survey various techniques for jointly extracting entities and relations. We\ncategorize techniques based on the approach they adopt for joint extraction,\ni.e. whether they employ joint inference or joint modelling or both. We further\ndescribe some representative techniques for joint inference and joint\nmodelling. We also describe two standard datasets, evaluation techniques and\nperformance of the joint extraction approaches on these datasets. We present a\nbrief analysis of application of a general domain joint extraction approach to\na Biomedical dataset. This survey is useful for researchers as well as\npractitioners in the field of Information Extraction, by covering a broad\nlandscape of joint extraction techniques.", "published": "2021-03-10 15:18:24", "link": "http://arxiv.org/abs/2103.06118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying ARDS using the Hierarchical Attention Network with Sentence\n  Objectives Framework", "abstract": "Acute respiratory distress syndrome (ARDS) is a life-threatening condition\nthat is often undiagnosed or diagnosed late. ARDS is especially prominent in\nthose infected with COVID-19. We explore the automatic identification of ARDS\nindicators and confounding factors in free-text chest radiograph reports. We\npresent a new annotated corpus of chest radiograph reports and introduce the\nHierarchical Attention Network with Sentence Objectives (HANSO) text\nclassification framework. HANSO utilizes fine-grained annotations to improve\ndocument classification performance. HANSO can extract ARDS-related information\nwith high performance by leveraging relation annotations, even if the annotated\nspans are noisy. Using annotated chest radiograph images as a gold standard,\nHANSO identifies bilateral infiltrates, an indicator of ARDS, in chest\nradiograph reports with performance (0.87 F1) comparable to human annotations\n(0.84 F1). This algorithm could facilitate more efficient and expeditious\nidentification of ARDS by clinicians and researchers and contribute to the\ndevelopment of new therapies to improve patient care.", "published": "2021-03-10 21:50:11", "link": "http://arxiv.org/abs/2103.06352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReportAGE: Automatically extracting the exact age of Twitter users based\n  on self-reports in tweets", "abstract": "Advancing the utility of social media data for research applications requires\nmethods for automatically detecting demographic information about social media\nstudy populations, including users' age. The objective of this study was to\ndevelop and evaluate a method that automatically identifies the exact age of\nusers based on self-reports in their tweets. Our end-to-end automatic natural\nlanguage processing (NLP) pipeline, ReportAGE, includes query patterns to\nretrieve tweets that potentially mention an age, a classifier to distinguish\nretrieved tweets that self-report the user's exact age (\"age\" tweets) and those\nthat do not (\"no age\" tweets), and rule-based extraction to identify the age.\nTo develop and evaluate ReportAGE, we manually annotated 11,000 tweets that\nmatched the query patterns. Based on 1000 tweets that were annotated by all\nfive annotators, inter-annotator agreement (Fleiss' kappa) was 0.80 for\ndistinguishing \"age\" and \"no age\" tweets, and 0.95 for identifying the exact\nage among the \"age\" tweets on which the annotators agreed. A deep neural\nnetwork classifier, based on a RoBERTa-Large pretrained model, achieved the\nhighest F1-score of 0.914 (precision = 0.905, recall = 0.942) for the \"age\"\nclass. When the age extraction was evaluated using the classifier's\npredictions, it achieved an F1-score of 0.855 (precision = 0.805, recall =\n0.914) for the \"age\" class. When it was evaluated directly on the held-out test\nset, it achieved an F1-score of 0.931 (precision = 0.873, recall = 0.998) for\nthe \"age\" class. We deployed ReportAGE on more than 1.2 billion tweets posted\nby 245,927 users, and predicted ages for 132,637 (54%) of them. Scaling the\ndetection of exact age to this large number of users can advance the utility of\nsocial media data for research applications that do not align with the\npredefined age groupings of extant binary or multi-class classification\napproaches.", "published": "2021-03-10 22:00:19", "link": "http://arxiv.org/abs/2103.06357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Amharic News Text classification Dataset", "abstract": "In NLP, text classification is one of the primary problems we try to solve\nand its uses in language analyses are indisputable. The lack of labeled\ntraining data made it harder to do these tasks in low resource languages like\nAmharic. The task of collecting, labeling, annotating, and making valuable this\nkind of data will encourage junior researchers, schools, and machine learning\npractitioners to implement existing classification models in their language. In\nthis short paper, we aim to introduce the Amharic text classification dataset\nthat consists of more than 50k news articles that were categorized into 6\nclasses. This dataset is made available with easy baseline performances to\nencourage studies and better performance experiments.", "published": "2021-03-10 16:36:39", "link": "http://arxiv.org/abs/2103.05639v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretable bias mitigation for textual data: Reducing gender bias in\n  patient notes while maintaining classification performance", "abstract": "Medical systems in general, and patient treatment decisions and outcomes in\nparticular, are affected by bias based on gender and other demographic\nelements. As language models are increasingly applied to medicine, there is a\ngrowing interest in building algorithmic fairness into processes impacting\npatient care. Much of the work addressing this question has focused on biases\nencoded in language models -- statistical estimates of the relationships\nbetween concepts derived from distant reading of corpora. Building on this\nwork, we investigate how word choices made by healthcare practitioners and\nlanguage models interact with regards to bias. We identify and remove gendered\nlanguage from two clinical-note datasets and describe a new debiasing procedure\nusing BERT-based gender classifiers. We show minimal degradation in health\ncondition classification tasks for low- to medium-levels of bias removal via\ndata augmentation. Finally, we compare the bias semantically encoded in the\nlanguage models with the bias empirically observed in health records. This work\noutlines an interpretable approach for using data augmentation to identify and\nreduce the potential for bias in natural language processing pipelines.", "published": "2021-03-10 03:09:30", "link": "http://arxiv.org/abs/2103.05841v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review", "abstract": "Many specialized domains remain untouched by deep learning, as large labeled\ndatasets require expensive expert annotators. We address this bottleneck within\nthe legal domain by introducing the Contract Understanding Atticus Dataset\n(CUAD), a new dataset for legal contract review. CUAD was created with dozens\nof legal experts from The Atticus Project and consists of over 13,000\nannotations. The task is to highlight salient portions of a contract that are\nimportant for a human to review. We find that Transformer models have nascent\nperformance, but that this performance is strongly influenced by model design\nand training dataset size. Despite these promising results, there is still\nsubstantial room for improvement. As one of the only large, specialized NLP\nbenchmarks annotated by experts, CUAD can serve as a challenging research\nbenchmark for the broader NLP community.", "published": "2021-03-10 18:59:34", "link": "http://arxiv.org/abs/2103.06268v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hurdles to Progress in Long-form Question Answering", "abstract": "The task of long-form question answering (LFQA) involves retrieving documents\nrelevant to a given question and using them to generate a paragraph-length\nanswer. While many models have recently been proposed for LFQA, we show in this\npaper that the task formulation raises fundamental challenges regarding\nevaluation and dataset creation that currently preclude meaningful modeling\nprogress. To demonstrate these challenges, we first design a new system that\nrelies on sparse attention and contrastive retriever learning to achieve\nstate-of-the-art performance on the ELI5 LFQA dataset. While our system tops\nthe public leaderboard, a detailed analysis reveals several troubling trends:\n(1) our system's generated answers are not actually grounded in the documents\nthat it retrieves; (2) ELI5 contains significant train / validation overlap, as\nat least 81% of ELI5 validation questions occur in paraphrased form in the\ntraining set; (3) ROUGE-L is not an informative metric of generated answer\nquality and can be easily gamed; and (4) human evaluations used for other text\ngeneration tasks are unreliable for LFQA. We offer suggestions to mitigate each\nof these issues, which we hope will lead to more rigorous LFQA research and\nmeaningful progress in the future.", "published": "2021-03-10 20:32:30", "link": "http://arxiv.org/abs/2103.06332v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Pre-training for Program Understanding and Generation", "abstract": "Code summarization and generation empower conversion between programming\nlanguage (PL) and natural language (NL), while code translation avails the\nmigration of legacy code from one PL to another. This paper introduces PLBART,\na sequence-to-sequence model capable of performing a broad spectrum of program\nand language understanding and generation tasks. PLBART is pre-trained on an\nextensive collection of Java and Python functions and associated NL text via\ndenoising autoencoding. Experiments on code summarization in the English\nlanguage, code generation, and code translation in seven programming languages\nshow that PLBART outperforms or rivals state-of-the-art models. Moreover,\nexperiments on discriminative tasks, e.g., program repair, clone detection, and\nvulnerable code detection, demonstrate PLBART's effectiveness in program\nunderstanding. Furthermore, analysis reveals that PLBART learns program syntax,\nstyle (e.g., identifier naming convention), logical flow (e.g., if block inside\nan else block is equivalent to else if block) that are crucial to program\nsemantics and thus excels even with limited annotations.", "published": "2021-03-10 20:32:59", "link": "http://arxiv.org/abs/2103.06333v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "ELLA: Exploration through Learned Language Abstraction", "abstract": "Building agents capable of understanding language instructions is critical to\neffective and robust human-AI collaboration. Recent work focuses on training\nthese agents via reinforcement learning in environments with synthetic\nlanguage; however, instructions often define long-horizon, sparse-reward tasks,\nand learning policies requires many episodes of experience. We introduce ELLA:\nExploration through Learned Language Abstraction, a reward shaping approach\ngeared towards boosting sample efficiency in sparse reward environments by\ncorrelating high-level instructions with simpler low-level constituents. ELLA\nhas two key elements: 1) A termination classifier that identifies when agents\ncomplete low-level instructions, and 2) A relevance classifier that correlates\nlow-level instructions with success on high-level tasks. We learn the\ntermination classifier offline from pairs of instructions and terminal states.\nNotably, in departure from prior work in language and abstraction, we learn the\nrelevance classifier online, without relying on an explicit decomposition of\nhigh-level instructions to low-level instructions. On a suite of complex BabyAI\nenvironments with varying instruction complexities and reward sparsity, ELLA\nshows gains in sample efficiency relative to language-based shaping and\ntraditional RL methods.", "published": "2021-03-10 02:18:46", "link": "http://arxiv.org/abs/2103.05825v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End\n  Information Extraction", "abstract": "We address the challenge of extracting structured information from business\ndocuments without detailed annotations. We propose Deep Conditional\nProbabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional\ncomplex documents and use Recursive Neural Networks to create an end-to-end\nsystem for finding the most probable parse that represents the structured\ninformation to be extracted. This system is trained end-to-end with scanned\ndocuments as input and only relational-records as labels. The\nrelational-records are extracted from existing databases avoiding the cost of\nannotating documents by hand. We apply this approach to extract information\nfrom scanned invoices achieving state-of-the-art results despite using no\nhand-annotations.", "published": "2021-03-10 07:35:21", "link": "http://arxiv.org/abs/2103.05908v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variable-rate discrete representation learning", "abstract": "Semantically meaningful information content in perceptual signals is usually\nunevenly distributed. In speech signals for example, there are often many\nsilences, and the speed of pronunciation can vary considerably. In this work,\nwe propose slow autoencoders (SlowAEs) for unsupervised learning of high-level\nvariable-rate discrete representations of sequences, and apply them to speech.\nWe show that the resulting event-based representations automatically grow or\nshrink depending on the density of salient information in the input signals,\nwhile still allowing for faithful signal reconstruction. We develop run-length\nTransformers (RLTs) for event-based representation modelling and use them to\nconstruct language models in the speech domain, which are able to generate\ngrammatical and semantically coherent utterances and continuations.", "published": "2021-03-10 14:42:31", "link": "http://arxiv.org/abs/2103.06089v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Relational Weight Priors in Neural Networks for Abstract Pattern\n  Learning and Language Modelling", "abstract": "Deep neural networks have become the dominant approach in natural language\nprocessing (NLP). However, in recent years, it has become apparent that there\nare shortcomings in systematicity that limit the performance and data\nefficiency of deep learning in NLP. These shortcomings can be clearly shown in\nlower-level artificial tasks, mostly on synthetic data. Abstract patterns are\nthe best known examples of a hard problem for neural networks in terms of\ngeneralisation to unseen data. They are defined by relations between items,\nsuch as equality, rather than their values. It has been argued that these\nlow-level problems demonstrate the inability of neural networks to learn\nsystematically. In this study, we propose Embedded Relation Based Patterns\n(ERBP) as a novel way to create a relational inductive bias that encourages\nlearning equality and distance-based relations for abstract patterns. ERBP is\nbased on Relation Based Patterns (RBP), but modelled as a Bayesian prior on\nnetwork weights and implemented as a regularisation term in otherwise standard\nnetwork learning. ERBP is is easy to integrate into standard neural networks\nand does not affect their learning capacity. In our experiments, ERBP priors\nlead to almost perfect generalisation when learning abstract patterns from\nsynthetic noise-free sequences. ERBP also improves natural language models on\nthe word and character level and pitch prediction in melodies with RNN, GRU and\nLSTM networks. We also find improvements in in the more complex tasks of\nlearning of graph edit distance and compositional sentence entailment. ERBP\nconsistently improves over RBP and over standard networks, showing that it\nenables abstract pattern learning which contributes to performance in natural\nlanguage tasks.", "published": "2021-03-10 17:21:16", "link": "http://arxiv.org/abs/2103.06198v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What is Multimodality?", "abstract": "The last years have shown rapid developments in the field of multimodal\nmachine learning, combining e.g., vision, text or speech. In this position\npaper we explain how the field uses outdated definitions of multimodality that\nprove unfit for the machine learning era. We propose a new task-relative\ndefinition of (multi)modality in the context of multimodal machine learning\nthat focuses on representations and information that are relevant for a given\nmachine learning task. With our new definition of multimodality we aim to\nprovide a missing foundation for multimodal research, an important component of\nlanguage grounding and a crucial milestone towards NLU.", "published": "2021-03-10 19:14:07", "link": "http://arxiv.org/abs/2103.06304v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "68Txx", "I.2.0; I.2.7; I.2.10"], "primary_category": "cs.AI"}
{"title": "Majority Voting with Bidirectional Pre-translation For Bitext Retrieval", "abstract": "Obtaining high-quality parallel corpora is of paramount importance for\ntraining NMT systems. However, as many language pairs lack adequate\ngold-standard training data, a popular approach has been to mine so-called\n\"pseudo-parallel\" sentences from paired documents in two languages. In this\npaper, we outline some problems with current methods, propose computationally\neconomical solutions to those problems, and demonstrate success with novel\nmethods on the Tatoeba similarity search benchmark and on a downstream task,\nnamely NMT. We uncover the effect of resource-related factors (i.e. how much\nmonolingual/bilingual data is available for a given language) on the optimal\nchoice of bitext mining approach, and echo problems with the oft-used BUCC\ndataset that have been observed by others. We make the code and data used for\nour experiments publicly available.", "published": "2021-03-10 22:24:01", "link": "http://arxiv.org/abs/2103.06369v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Causal-aware Safe Policy Improvement for Task-oriented dialogue", "abstract": "The recent success of reinforcement learning's (RL) in solving complex tasks\nis most often attributed to its capacity to explore and exploit an environment\nwhere it has been trained. Sample efficiency is usually not an issue since\ncheap simulators are available to sample data on-policy. On the other hand,\ntask oriented dialogues are usually learnt from offline data collected using\nhuman demonstrations. Collecting diverse demonstrations and annotating them is\nexpensive. Unfortunately, use of RL methods trained on off-policy data are\nprone to issues of bias and generalization, which are further exacerbated by\nstochasticity in human response and non-markovian belief state of a dialogue\nmanagement system. To this end, we propose a batch RL framework for task\noriented dialogue policy learning: causal aware safe policy improvement\n(CASPI). This method gives guarantees on dialogue policy's performance and also\nlearns to shape rewards according to intentions behind human responses, rather\nthan just mimicking demonstration data; this couple with batch-RL helps overall\nwith sample efficiency of the framework. We demonstrate the effectiveness of\nthis framework on a dialogue-context-to-text Generation and end-to-end dialogue\ntask of the Multiwoz2.0 dataset. The proposed method outperforms the current\nstate of the art on these metrics, in both case. In the end-to-end case, our\nmethod trained only on 10\\% of the data was able to out perform current state\nin three out of four evaluation metrics.", "published": "2021-03-10 22:34:28", "link": "http://arxiv.org/abs/2103.06370v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-tuning of Pre-trained End-to-end Speech Recognition with Generative\n  Adversarial Networks", "abstract": "Adversarial training of end-to-end (E2E) ASR systems using generative\nadversarial networks (GAN) has recently been explored for low-resource ASR\ncorpora. GANs help to learn the true data representation through a two-player\nmin-max game. However, training an E2E ASR model using a large ASR corpus with\na GAN framework has never been explored, because it might take excessively long\ntime due to high-variance gradient updates and face convergence issues. In this\npaper, we introduce a novel framework for fine-tuning a pre-trained ASR model\nusing the GAN objective where the ASR model acts as a generator and a\ndiscriminator tries to distinguish the ASR output from the real data. Since the\nASR model is pre-trained, we hypothesize that the ASR model output (soft\ndistribution vectors) helps to get higher scores from the discriminator and\nmakes the task of the discriminator harder within our GAN framework, which in\nturn improves the performance of the ASR model in the fine-tuning stage. Here,\nthe pre-trained ASR model is fine-tuned adversarially against the discriminator\nusing an additional adversarial loss. Experiments on full LibriSpeech dataset\nshow that our proposed approach outperforms baselines and conventional\nGAN-based adversarial models.", "published": "2021-03-10 17:40:48", "link": "http://arxiv.org/abs/2103.13329v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Best of Both Worlds: Robust Accented Speech Recognition with Adversarial\n  Transfer Learning", "abstract": "Training deep neural networks for automatic speech recognition (ASR) requires\nlarge amounts of transcribed speech. This becomes a bottleneck for training\nrobust models for accented speech which typically contains high variability in\npronunciation and other semantics, since obtaining large amounts of annotated\naccented data is both tedious and costly. Often, we only have access to large\namounts of unannotated speech from different accents. In this work, we leverage\nthis unannotated data to provide semantic regularization to an ASR model that\nhas been trained only on one accent, to improve its performance for multiple\naccents. We propose Accent Pre-Training (Acc-PT), a semi-supervised training\nstrategy that combines transfer learning and adversarial training. Our approach\nimproves the performance of a state-of-the-art ASR model by 33% on average over\nthe baseline across multiple accents, training only on annotated samples from\none standard accent, and as little as 105 minutes of unannotated speech from a\ntarget accent.", "published": "2021-03-10 02:33:52", "link": "http://arxiv.org/abs/2103.05834v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Search Disaster Victims using Sound Source Localization", "abstract": "Sound Source Localization (SSL) are used to estimate the position of sound\nsources. Various methods have been used for detecting sound and its\nlocalization. This paper presents a system for stationary sound source\nlocalization by cubical microphone array consisting of eight microphones placed\non four vertical adjacent faces which is mounted on three wheel\nomni-directional drive for the inspection and monitoring of the disaster\nvictims in disaster areas. The proposed method localizes sound source on a 3D\nspace by grid search method using Generalized Cross Correlation Phase Transform\n(GCC-PHAT) which is robust when operating in real life scenario where there is\nlack of visibility. The computed azimuth and elevation angle of victimized\nhuman voice are fed to embedded omni-directional drive system which navigates\nthe vehicle automatically towards the stationary sound source.", "published": "2021-03-10 13:44:36", "link": "http://arxiv.org/abs/2103.06049v1", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Speaker Independent Dysarthric Speech Intelligibility\n  Assessment System", "abstract": "Dysarthria is a condition which hampers the ability of an individual to\ncontrol the muscles that play a major role in speech delivery. The loss of fine\ncontrol over muscles that assist the movement of lips, vocal chords, tongue and\ndiaphragm results in abnormal speech delivery. One can assess the severity\nlevel of dysarthria by analyzing the intelligibility of speech spoken by an\nindividual. Continuous intelligibility assessment helps speech language\npathologists not only study the impact of medication but also allows them to\nplan personalized therapy. It helps the clinicians immensely if the\nintelligibility assessment system is reliable, automatic, simple for (a)\npatients to undergo and (b) clinicians to interpret. Lack of availability of\ndysarthric data has resulted in development of speaker dependent automatic\nintelligibility assessment systems which requires patients to speak a large\nnumber of utterances. In this paper, we propose (a) a cost minimization\nprocedure to select an optimal (small) number of utterances that need to be\nspoken by the dysarthric patient, (b) four different speaker independent\nintelligibility assessment systems which require the patient to speak a small\nnumber of words, and (c) the assessment score is close to the perceptual score\nthat the Speech Language Pathologist (SLP) can relate to. The need for small\nnumber of utterances to be spoken by the patient and the score being relatable\nto the SLP benefits both the dysarthric patient and the clinician from\nusability perspective.", "published": "2021-03-10 16:15:32", "link": "http://arxiv.org/abs/2103.06157v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion\n  Recognition", "abstract": "In this manuscript, the topic of multi-corpus Speech Emotion Recognition\n(SER) is approached from a deep transfer learning perspective. A large corpus\nof emotional speech data, EmoSet, is assembled from a number of existing SER\ncorpora. In total, EmoSet contains 84181 audio recordings from 26 SER corpora\nwith a total duration of over 65 hours. The corpus is then utilised to create a\nnovel framework for multi-corpus speech emotion recognition, namely EmoNet. A\ncombination of a deep ResNet architecture and residual adapters is transferred\nfrom the field of multi-domain visual recognition to multi-corpus SER on\nEmoSet. Compared against two suitable baselines and more traditional training\nand transfer settings for the ResNet, the residual adapter approach enables\nparameter efficient training of a multi-domain SER model on all 26 corpora. A\nshared model with only $3.5$ times the number of parameters of a model trained\non a single database leads to increased performance for 21 of the 26 corpora in\nEmoSet. Measured by McNemar's test, these improvements are further significant\nfor ten datasets at $p<0.05$ while there are just two corpora that see only\nsignificant decreases across the residual adapter transfer experiments.\nFinally, we make our EmoNet framework publicly available for users and\ndevelopers at https://github.com/EIHW/EmoNet. EmoNet provides an extensive\ncommand line interface which is comprehensively documented and can be used in a\nvariety of multi-corpus transfer learning settings.", "published": "2021-03-10 19:12:37", "link": "http://arxiv.org/abs/2103.08310v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
