{"title": "Counting Tree-Like Multigraphs with a Given Number of Vertices and Multiple Edges", "abstract": "The enumeration of chemical graphs is an important topic in cheminformatics\nand bioinformatics, particularly in the discovery of novel drugs. These graphs\nare typically either tree-like multigraphs or composed of tree-like multigraphs\nconnected to a core structure. In both cases, the tree-like components play a\nsignificant role in determining the properties and activities of chemical\ncompounds. This paper introduces a method based on dynamic programming to\nefficiently count tree-like multigraphs with a given number $n$ of vertices and\n$\\Delta$ multiple edges. The idea of our method is to consider multigraphs as\nrooted multigraphs by selecting their unicentroid or bicentroid as the root,\nand define their canonical representation based on maximal subgraphs rooted at\nthe children of the root. This representation guarantees that our proposed\nmethod will not repeat a multigraph in the counting process. Finally, recursive\nrelations are derived based on the number of vertices and multiple edges in the\nmaximal subgraphs rooted at the children of roots. These relations lead to an\nalgorithm with a time complexity of $\\mathcal{O}(n^2(n + \\Delta (n + \\Delta^2\n\\cdot \\min\\{n, \\Delta\\})))$ and a space complexity of\n$\\mathcal{O}(n^2(\\Delta^3+1))$. Experimental results show that the proposed\nalgorithm efficiently counts the desired multigraphs with up to 170 vertices\nand 50 multiple edges in approximately 930 seconds, confirming its\neffectiveness and potential as a valuable tool for exploring the chemical graph\nspace in novel drug discovery.", "published": "2025-02-08 11:23:53", "link": "http://arxiv.org/abs/2502.05529v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Currency Arbitrage Optimization using Quantum Annealing, QAOA and Constraint Mapping", "abstract": "Currency arbitrage capitalizes on price discrepancies in currency exchange\nrates between markets to produce profits with minimal risk. By employing a\ncombinatorial optimization problem, one can ascertain optimal paths within\ndirected graphs, thereby facilitating the efficient identification of\nprofitable trading routes. This research investigates the methodologies of\nquantum annealing and gate-based quantum computing in relation to the currency\narbitrage problem. In this study, we implement the Quantum Approximate\nOptimization Algorithm (QAOA) utilizing Qiskit version 1.2. In order to\noptimize the parameters of QAOA, we perform simulations utilizing the\nAerSimulator and carry out experiments in simulation. Furthermore, we present\nan NchooseK-based methodology utilizing D-Wave's Ocean suite. This methodology\nenables a comparison of the effectiveness of quantum techniques in identifying\noptimal arbitrage paths. The results of our study enhance the existing\nliterature on the application of quantum computing in financial optimization\nchallenges, emphasizing both the prospective benefits and the present\nlimitations of these developing technologies in real-world scenarios.", "published": "2025-02-08 01:22:31", "link": "http://arxiv.org/abs/2502.15742v1", "categories": ["q-fin.CP", "math.OC", "q-fin.TR"], "primary_category": "q-fin.CP"}
{"title": "The Role of Prosody in Spoken Question Answering", "abstract": "Spoken language understanding research to date has generally carried a heavy\ntext perspective. Most datasets are derived from text, which is then\nsubsequently synthesized into speech, and most models typically rely on\nautomatic transcriptions of speech. This is to the detriment of\nprosody--additional information carried by the speech signal beyond the\nphonetics of the words themselves and difficult to recover from text alone. In\nthis work, we investigate the role of prosody in Spoken Question Answering. By\nisolating prosodic and lexical information on the SLUE-SQA-5 dataset, which\nconsists of natural speech, we demonstrate that models trained on prosodic\ninformation alone can perform reasonably well by utilizing prosodic cues.\nHowever, we find that when lexical information is available, models tend to\npredominantly rely on it. Our findings suggest that while prosodic cues provide\nvaluable supplementary information, more effective integration methods are\nrequired to ensure prosody contributes more significantly alongside lexical\nfeatures.", "published": "2025-02-08 00:11:55", "link": "http://arxiv.org/abs/2502.05389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Lexical Manifold Projection in Large Language Models: A\n  Novel Mechanism for Multi-Scale Semantic Representation", "abstract": "The integration of structured hierarchical embeddings into transformer-based\narchitectures introduces a refined approach to lexical representation, ensuring\nthat multi-scale semantic relationships are preserved without compromising\ncomputational efficiency. A projection mechanism that maps tokens onto a\nstructured manifold provides improved lexical alignment, enhancing the\nadaptability of word representations across diverse linguistic tasks. The\nstructured encoding framework ensures that hierarchical embeddings maintain\ncoherence across varying abstraction levels, allowing for stable transitions\nbetween localized syntactic features and global semantic structures.\nExperimental evaluations indicate that hierarchical embeddings consistently\noutperform conventional token representations, improving accuracy in linguistic\nbenchmarks while maintaining lower computational overhead. Comparative analysis\nacross multiple domains highlights the ability of hierarchical embeddings to\nretain contextual consistency, particularly in specialized language\napplications where structured lexical alignment is essential. Statistical\nassessments further demonstrate that hierarchical embeddings exhibit enhanced\nrobustness under perturbation conditions, ensuring that linguistic structures\nremain stable across adversarial text modifications. The integration of\nhierarchical projections with transformer attention mechanisms enables improved\ncontextual adaptation, ensuring that token representations are dynamically\nadjusted based on varying linguistic distributions. The refined hierarchical\norganization of embeddings provides greater interpretability in lexical\nmodeling, facilitating enhanced generalization capabilities across diverse text\nprocessing tasks.", "published": "2025-02-08 00:49:32", "link": "http://arxiv.org/abs/2502.05395v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Noise Preference Optimization for LLM Self-Improvement via\n  Synthetic Data", "abstract": "Although LLMs have achieved significant success, their reliance on large\nvolumes of human-annotated data has limited their potential for further\nscaling. In this situation, utilizing self-generated synthetic data has become\ncrucial for fine-tuning LLMs without extensive human annotation. However,\ncurrent methods often fail to ensure consistent improvements across iterations,\nwith performance stagnating after only minimal updates. To overcome these\nchallenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO\nemploys a dynamic sample labeling mechanism to construct preference pairs for\ntraining and introduces controlled, trainable noise into the preference\noptimization process. Our approach effectively prevents stagnation and enables\ncontinuous improvement. In experiments with Zephyr-7B, DNPO consistently\noutperforms existing methods, showing an average performance boost of 2.6%\nacross multiple benchmarks. Additionally, DNPO shows a significant improvement\nin model-generated data quality, with a 29.4% win-loss rate gap compared to the\nbaseline in GPT-4 evaluations. This highlights its effectiveness in enhancing\nmodel performance through iterative refinement.", "published": "2025-02-08 01:20:09", "link": "http://arxiv.org/abs/2502.05400v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OntoTune: Ontology-Driven Self-training for Aligning Large Language\n  Models", "abstract": "Existing domain-specific Large Language Models (LLMs) are typically developed\nby fine-tuning general-purposed LLMs with large-scale domain-specific corpora.\nHowever, training on large-scale corpora often fails to effectively organize\ndomain knowledge of LLMs, leading to fragmented understanding. Inspired by how\nhumans connect concepts and organize knowledge through mind maps, we aim to\nemulate this approach by using ontology with hierarchical conceptual knowledge\nto reorganize LLM's domain knowledge. From this perspective, we propose an\nontology-driven self-training framework called OntoTune, which aims to align\nLLMs with ontology through in-context learning, enabling the generation of\nresponses guided by the ontology. We leverage in-context learning to identify\nwhether the LLM has acquired the specific concept's ontology knowledge, and\nselect the entries not yet mastered by LLM as the training set to further align\nthe LLM with ontology. Compared to existing domain LLMs based on newly\ncollected large-scale domain-specific corpora, our OntoTune, which relies on\nthe existing, long-term developed ontology and LLM itself, significantly\nreduces data maintenance costs and offers improved generalization ability. We\nconduct our study in the medical domain to evaluate the effectiveness of\nOntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology\nsource. Experimental results demonstrate that OntoTune achieves\nstate-of-the-art performance in both in-ontology task hypernym discovery and\nout-of-ontology task medical domain QA. Moreover, compared to the latest direct\nontology injection method TaxoLLaMA, our OntoTune better preserves original\nknowledge of LLM. The code and data are available at\nhttps://github.com/zjukg/OntoTune.", "published": "2025-02-08 07:38:45", "link": "http://arxiv.org/abs/2502.05478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepThink: Aligning Language Models with Domain-Specific User Intents", "abstract": "Supervised fine-tuning with synthesized instructions has been a common\npractice for adapting LLMs to domain-specific QA tasks. However, the\nsynthesized instructions deviate from real user questions and expected answers.\nThis study proposes a novel framework called DeepThink to generate high-quality\ninstructions. DeepThink first generates a few seed questions to mimic actual\nuser questions, simulates conversations to uncover the hidden user needs, and\nrefines the answer by conversational contexts and the retrieved documents for\nmore comprehensive answers. Experiments demonstrate that DeepThink achieves an\naverage performance improvement of 7.92% compared to a GPT-4-turbo+RAG-based\nassistant on the real user test set in the advertising domain across dimensions\nsuch as relevance, completeness, clarity, accuracy, and actionability.", "published": "2025-02-08 09:04:16", "link": "http://arxiv.org/abs/2502.05497v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining\n  Strategy", "abstract": "Large language models (LLMs) have significantly advanced human language\nunderstanding and generation, with pretraining data quality and organization\nbeing crucial to their performance. Multi-stage pretraining is a promising\napproach, but existing methods often lack quantitative criteria for data\npartitioning and instead rely on intuitive heuristics. In this paper, we\npropose the novel Four-quadRAnt Multi-stage prEtraining strategy (FRAME),\nguided by the established principle of organizing the pretraining process into\nfour stages to achieve significant loss reductions four times. This principle\nis grounded in two key findings: first, training on high Perplexity (PPL) data\nfollowed by low PPL data, and second, training on low PPL difference (PD) data\nfollowed by high PD data, both causing the loss to drop significantly twice and\nperformance enhancements. By partitioning data into four quadrants and\nstrategically organizing them, FRAME achieves a remarkable 16.8% average\nimprovement over random across MMLU and CMMLU for the 3B model, effectively\nboosting LLM performance.", "published": "2025-02-08 12:46:33", "link": "http://arxiv.org/abs/2502.05551v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Structure Modulation in Large Language Models Through Stochastic\n  Concept Embedding Transitions", "abstract": "Stochastic embedding transitions introduce a probabilistic mechanism for\nadjusting token representations dynamically during inference, mitigating the\nconstraints imposed through static or deterministic embeddings. A transition\nframework was proposed in which each token embedding evolved through\nprobabilistic updates, ensuring adaptability while preserving semantic\nintegrity across linguistic contexts. Empirical evaluations demonstrated that\nmodels incorporating stochastic transitions exhibited greater lexical\ndiversity, improved generative coherence, and enhanced retention of\nlow-frequency vocabulary, contributing to more varied sentence structures and\nreduced reliance on high-probability token selections. Statistical analyses of\nembedding drift across transformer layers indicated that representations\nevolved more flexibly without losing coherence, supporting the hypothesis that\ncontrolled stochasticity facilitated context-sensitive representation learning.\nExperimental results revealed that probabilistic embeddings introduced minor\ncomputational overhead while maintaining generative efficiency, reinforcing\ntheir feasibility in large-scale applications. A comparative study with\ntraditional embedding approaches highlighted measurable gains in text\ncompletion accuracy, dialogue coherence, and structural complexity, confirming\nthe effectiveness of stochastic transitions in enhancing representation\nexpressiveness. Clustering patterns in the embedding space suggested that\nprobabilistic updates preserved meaningful semantic groupings while enabling\ncontext-driven shifts, further validating the stability of the transition\nmechanism. Performance metrics indicated that stochastic transitions balanced\nadaptability and control, ensuring that generative outputs remained\nlinguistically coherent without excessive randomness.", "published": "2025-02-08 12:53:52", "link": "http://arxiv.org/abs/2502.05553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lossless Acceleration of Large Language Models with Hierarchical\n  Drafting based on Temporal Locality in Speculative Decoding", "abstract": "Accelerating inference in Large Language Models (LLMs) is critical for\nreal-time interactions, as they have been widely incorporated into real-world\nservices. Speculative decoding, a fully algorithmic solution, has gained\nattention for improving inference speed by drafting and verifying tokens,\nthereby generating multiple tokens in a single forward pass. However, current\ndrafting strategies usually require significant fine-tuning or have\ninconsistent performance across tasks. To address these challenges, we propose\nHierarchy Drafting (HD), a novel lossless drafting approach that organizes\nvarious token sources into multiple databases in a hierarchical framework based\non temporal locality. In the drafting step, HD sequentially accesses multiple\ndatabases to obtain draft tokens from the highest to the lowest locality,\nensuring consistent acceleration across diverse tasks and minimizing drafting\nlatency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters\ndemonstrate that HD outperforms existing database drafting methods, achieving\nrobust inference speedups across model sizes, tasks, and temperatures.", "published": "2025-02-08 15:32:53", "link": "http://arxiv.org/abs/2502.05609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Sustainable NLP: Insights from Benchmarking Inference Energy in\n  Large Language Models", "abstract": "Large language models (LLMs) are increasingly recognized for their\nexceptional generative capabilities and versatility across various tasks.\nHowever, the high inference costs associated with these models have not\nreceived adequate attention, particularly when compared to the focus on\ntraining costs in existing research. In response to this gap, our study\nconducts a comprehensive benchmarking of LLM inference energy across a wide\nrange of NLP tasks, where we analyze the impact of different models, tasks,\nprompts, and system-related factors on inference energy. Specifically, our\nexperiments reveal several interesting insights, including strong correlation\nof inference energy with output token length and response time. Also, we find\nthat quantization and optimal batch sizes, along with targeted prompt phrases,\ncan significantly reduce energy usage. This study is the first to thoroughly\nbenchmark LLM inference across such a diverse range of aspects, providing\ninsights and offering several recommendations for improving energy efficiency\nin model deployment.", "published": "2025-02-08 15:34:52", "link": "http://arxiv.org/abs/2502.05610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AnyEdit: Edit Any Knowledge Encoded in Language Models", "abstract": "Large language models (LLMs) often produce incorrect or outdated information,\nnecessitating efficient and precise knowledge updates. Current model editing\nmethods, however, struggle with long-form knowledge in diverse formats, such as\npoetry, code snippets, and mathematical derivations. These limitations arise\nfrom their reliance on editing a single token's hidden state, a limitation we\nterm \"efficacy barrier\". To solve this, we propose AnyEdit, a new\nautoregressive editing paradigm. It decomposes long-form knowledge into\nsequential chunks and iteratively edits the key token in each chunk, ensuring\nconsistent and accurate outputs. Theoretically, we ground AnyEdit in the Chain\nRule of Mutual Information, showing its ability to update any knowledge within\nLLMs. Empirically, it outperforms strong baselines by 21.5% on benchmarks\nincluding UnKEBench, AKEW, and our new EditEverything dataset for long-form\ndiverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-play\nframework, enabling current editing methods to update knowledge with arbitrary\nlength and format, significantly advancing the scope and practicality of LLM\nknowledge editing.", "published": "2025-02-08 16:18:37", "link": "http://arxiv.org/abs/2502.05628v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incongruence Identification in Eyewitness Testimony", "abstract": "Incongruence detection in eyewitness narratives is critical for understanding\nthe reliability of testimonies, yet traditional approaches often fail to\naddress the nuanced inconsistencies inherent in such accounts. In this paper,\nwe introduce a novel task of incongruence detection in eyewitness testimonies.\nGiven a pair of testimonies containing of multiple pairs of question and answer\nby two subjects, we identify contextually related incongruence between the two\nsubjects. We also mark the span of incongruences in the utterances. To achieve\nthis, we developed MIND(MultI-EyewitNess Deception) - a comprehensive dataset\nconsisting of 2927 pairs of contextually related answers designed to capture\nboth explicit and implicit contradictions. INstruction - TunEd iNcongruity\nDetection framework based on 6W and multi-hop reasoning approach, aka. INTEND.\nDrawing from investigative techniques, INTEND address the task as a close-style\nproblem, contradicting on the who, what, when, where and why aspect of the\ncontent. Our findings shows that prompt tuning, especially when utilizing our\nframework, enhances the detection of incongruences by a margin of +5.63\npercent. We compare our approach with multiple fine-tuning and prompt tuning\ntechniques on MLMs and LLMs. Emperical results demonstrate convincing\nperformance improvement in F1-score over fine-tuned and regular prompt-tuning\ntechniques, highlighting the effectiveness of our approach.", "published": "2025-02-08 17:51:57", "link": "http://arxiv.org/abs/2502.05650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Shortcomings of LLMs in Step-by-Step Legal Reasoning", "abstract": "Reasoning abilities of LLMs have been a key focus in recent years. One\nchallenging reasoning domain with interesting nuances is legal reasoning, which\nrequires careful application of rules, and precedents while balancing deductive\nand analogical reasoning, and conflicts between rules. Although there have been\na few works on using LLMs for legal reasoning, their focus has been on overall\naccuracy. In this paper, we dig deeper to do a step-by-step analysis and figure\nout where they commit errors. We use the college-level Multiple Choice\nQuestion-Answering (MCQA) task from the \\textit{Civil Procedure} dataset and\npropose a new error taxonomy derived from initial manual analysis of reasoning\nchains with respect to several LLMs, including two objective measures:\nsoundness and correctness scores. We then develop an LLM-based automated\nevaluation framework to identify reasoning errors and evaluate the performance\nof LLMs. The computation of soundness and correctness on the dataset using the\nauto-evaluator framework reveals several interesting insights. Furthermore, we\nshow that incorporating the error taxonomy as feedback in popular prompting\ntechniques marginally increases LLM performance. Our work will also serve as an\nevaluation framework that can be used in detailed error analysis of reasoning\nchains for logic-intensive complex tasks.", "published": "2025-02-08 19:49:32", "link": "http://arxiv.org/abs/2502.05675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Task Representations from In-Context Learning", "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in\nin-context learning (ICL), where models adapt to new tasks through\nexample-based prompts without requiring parameter updates. However,\nunderstanding how tasks are internally encoded and generalized remains a\nchallenge. To address some of the empirical and technical gaps in the\nliterature, we introduce an automated formulation for encoding task information\nin ICL prompts as a function of attention heads within the transformer\narchitecture. This approach computes a single task vector as a weighted sum of\nattention heads, with the weights optimized causally via gradient descent. Our\nfindings show that existing methods fail to generalize effectively to\nmodalities beyond text. In response, we also design a benchmark to evaluate\nwhether a task vector can preserve task fidelity in functional regression\ntasks. The proposed method successfully extracts task-specific information from\nin-context demonstrations and excels in both text and regression tasks,\ndemonstrating its generalizability across modalities. Moreover, ablation\nstudies show that our method's effectiveness stems from aligning the\ndistribution of the last hidden state with that of an optimally performing\nin-context-learned model.", "published": "2025-02-08 00:16:44", "link": "http://arxiv.org/abs/2502.05390v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-based Molecular In-context Learning Grounded on Morgan\n  Fingerprints", "abstract": "In-context learning (ICL) effectively conditions large language models (LLMs)\nfor molecular tasks, such as property prediction and molecule captioning, by\nembedding carefully selected demonstration examples into the input prompt. This\napproach avoids the computational overhead of extensive pertaining and\nfine-tuning. However, current prompt retrieval methods for molecular tasks have\nrelied on molecule feature similarity, such as Morgan fingerprints, which do\nnot adequately capture the global molecular and atom-binding relationships. As\na result, these methods fail to represent the full complexity of molecular\nstructures during inference. Moreover, small-to-medium-sized LLMs, which offer\nsimpler deployment requirements in specialized systems, have remained largely\nunexplored in the molecular ICL literature. To address these gaps, we propose a\nself-supervised learning technique, GAMIC (Graph-Aligned Molecular In-Context\nlearning, which aligns global molecular structures, represented by graph neural\nnetworks (GNNs), with textual captions (descriptions) while leveraging local\nfeature similarity through Morgan fingerprints. In addition, we introduce a\nMaximum Marginal Relevance (MMR) based diversity heuristic during retrieval to\noptimize input prompt demonstration samples. Our experimental findings using\ndiverse benchmark datasets show GAMIC outperforms simple Morgan-based ICL\nretrieval methods across all tasks by up to 45%.", "published": "2025-02-08 02:46:33", "link": "http://arxiv.org/abs/2502.05414v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training\n  and Cross-domain Adaptation", "abstract": "Graphs are able to model interconnected entities in many online services,\nsupporting a wide range of applications on the Web. This raises an important\nquestion: How can we train a graph foundational model on multiple source\ndomains and adapt to an unseen target domain? A major obstacle is that graphs\nfrom different domains often exhibit divergent characteristics. Some studies\nleverage large language models to align multiple domains based on textual\ndescriptions associated with the graphs, limiting their applicability to\ntext-attributed graphs. For text-free graphs, a few recent works attempt to\nalign different feature distributions across domains, while generally\nneglecting structural differences. In this work, we propose a novel Structure\nAlignment framework for text-free Multi-domain Graph Pre-Training and\ncross-domain adaptation (SAMGPT). It is designed to learn multi-domain\nknowledge from graphs originating in multiple source domains, which can then be\nadapted to address applications in an unseen target domain. Specifically, we\nintroduce a set of structure tokens to harmonize structure-based aggregation\nacross source domains during the pre-training phase. Next, for cross-domain\nadaptation, we design dual prompts, namely, holistic prompts and specific\nprompts, which adapt unified multi-domain structural knowledge and\nfine-grained, domain-specific information, respectively, to a target domain.\nFinally, we conduct comprehensive experiments on seven public datasets to\nevaluate and analyze the effectiveness of SAMGPT.", "published": "2025-02-08 03:24:25", "link": "http://arxiv.org/abs/2502.05424v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Copyright Integrity and Verifiability via Multi-Bit Watermarking\n  for Intelligent Transportation Systems", "abstract": "Intelligent transportation systems (ITS) use advanced technologies such as\nartificial intelligence to significantly improve traffic flow management\nefficiency, and promote the intelligent development of the transportation\nindustry. However, if the data in ITS is attacked, such as tampering or\nforgery, it will endanger public safety and cause social losses. Therefore,\nthis paper proposes a watermarking that can verify the integrity of copyright\nin response to the needs of ITS, termed ITSmark. ITSmark focuses on functions\nsuch as extracting watermarks, verifying permission, and tracing tampered\nlocations. The scheme uses the copyright information to build the multi-bit\nspace and divides this space into multiple segments. These segments will be\nassigned to tokens. Thus, the next token is determined by its segment which\ncontains the copyright. In this way, the obtained data contains the custom\nwatermark. To ensure the authorization, key parameters are encrypted during\ncopyright embedding to obtain cipher data. Only by possessing the correct\ncipher data and private key, can the user entirely extract the watermark.\nExperiments show that ITSmark surpasses baseline performances in data quality,\nextraction accuracy, and unforgeability. It also shows unique capabilities of\npermission verification and tampered location tracing, which ensures the\nsecurity of extraction and the reliability of copyright verification.\nFurthermore, ITSmark can also customize the watermark embedding position and\nproportion according to user needs, making embedding more flexible.", "published": "2025-02-08 03:26:17", "link": "http://arxiv.org/abs/2502.05425v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Position: LLMs Can be Good Tutors in Foreign Language Education", "abstract": "While recent efforts have begun integrating large language models (LLMs) into\nforeign language education (FLE), they often rely on traditional approaches to\nlearning tasks without fully embracing educational methodologies, thus lacking\nadaptability to language learning. To address this gap, we argue that LLMs have\nthe potential to serve as effective tutors in FLE. Specifically, LLMs can play\nthree critical roles: (1) as data enhancers, improving the creation of learning\nmaterials or serving as student simulations; (2) as task predictors, serving as\nlearner assessment or optimizing learning pathway; and (3) as agents, enabling\npersonalized and inclusive education. We encourage interdisciplinary research\nto explore these roles, fostering innovation while addressing challenges and\nrisks, ultimately advancing FLE through the thoughtful integration of LLMs.", "published": "2025-02-08 06:48:49", "link": "http://arxiv.org/abs/2502.05467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mechanistic Interpretability of Emotion Inference in Large Language\n  Models", "abstract": "Large language models (LLMs) show promising capabilities in predicting human\nemotions from text. However, the mechanisms through which these models process\nemotional stimuli remain largely unexplored. Our study addresses this gap by\ninvestigating how autoregressive LLMs infer emotions, showing that emotion\nrepresentations are functionally localized to specific regions in the model.\nOur evaluation includes diverse model families and sizes and is supported by\nrobustness checks. We then show that the identified representations are\npsychologically plausible by drawing on cognitive appraisal theory, a\nwell-established psychological framework positing that emotions emerge from\nevaluations (appraisals) of environmental stimuli. By causally intervening on\nconstrued appraisal concepts, we steer the generation and show that the outputs\nalign with theoretical and intuitive expectations. This work highlights a novel\nway to causally intervene and precisely shape emotional text generation,\npotentially benefiting safety and alignment in sensitive affective domains.", "published": "2025-02-08 08:11:37", "link": "http://arxiv.org/abs/2502.05489v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Memory Construction and Retrieval for Personalized Conversational\n  Agents", "abstract": "To deliver coherent and personalized experiences in long-term conversations,\nexisting approaches typically perform retrieval augmented response generation\nby constructing memory banks from conversation history at either the\nturn-level, session-level, or through summarization techniques.In this paper,\nwe present two key findings: (1) The granularity of memory unit matters:\nturn-level, session-level, and summarization-based methods each exhibit\nlimitations in both memory retrieval accuracy and the semantic quality of the\nretrieved content. (2) Prompt compression methods, such as LLMLingua-2, can\neffectively serve as a denoising mechanism, enhancing memory retrieval accuracy\nacross different granularities. Building on these insights, we propose SeCom, a\nmethod that constructs the memory bank at segment level by introducing a\nconversation segmentation model that partitions long-term conversations into\ntopically coherent segments, while applying compression based denoising on\nmemory units to enhance memory retrieval. Experimental results show that SeCom\nexhibits a significant performance advantage over baselines on long-term\nconversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed\nconversation segmentation method demonstrates superior performance on dialogue\nsegmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.", "published": "2025-02-08 14:28:36", "link": "http://arxiv.org/abs/2502.05589v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative\n  Preference Optimization", "abstract": "A truly intelligent Large Language Model (LLM) should be capable of\ncorrecting errors in its responses through external interactions. However, even\nthe most advanced models often face challenges in improving their outputs. In\nthis paper, we explore how to cultivate LLMs with the self-refinement\ncapability through iterative preference training, and how this ability can be\nleveraged to improve model performance during inference. To this end, we\nintroduce a novel post-training and inference framework, called ARIES: Adaptive\nRefinement and Iterative Enhancement Structure. This method iteratively\nperforms preference training and self-refinement-based data collection. During\ntraining, ARIES strengthen the model's direct question-answering capability\nwhile simultaneously unlocking its self-refinement potential. During inference,\nARIES harnesses this self-refinement capability to generate a series of\nprogressively refined responses, which are then filtered using either the\nReward Model Scoring or a simple yet effective Rule-Based Selection mechanism,\nspecifically tailored to our approach, to construct a dataset for the next\nround of preference training. Experimental results demonstrate the remarkable\nperformance of ARIES. When applied to the Llama-3.1-8B model and under the\nself-refinement setting, ARIES surpasses powerful models such as GPT-4o,\nachieving 62.3% length-controlled (LC) and a 63.3% raw win rates on AlpacaEval\n2, outperforming Iterative DPO by 27.8% and 35.5% respectively, as well as a\n50.3% win rate on Arena-Hard, surpassing Iterative DPO by 26.6%. Furthermore,\nARIES consistently enhances performance on mathematical reasoning tasks like\nGSM8K and MATH.", "published": "2025-02-08 15:21:55", "link": "http://arxiv.org/abs/2502.05605v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ELMTEX: Fine-Tuning Large Language Models for Structured Clinical\n  Information Extraction. A Case Study on Clinical Reports", "abstract": "Europe's healthcare systems require enhanced interoperability and\ndigitalization, driving a demand for innovative solutions to process legacy\nclinical data. This paper presents the results of our project, which aims to\nleverage Large Language Models (LLMs) to extract structured information from\nunstructured clinical reports, focusing on patient history, diagnoses,\ntreatments, and other predefined categories. We developed a workflow with a\nuser interface and evaluated LLMs of varying sizes through prompting strategies\nand fine-tuning. Our results show that fine-tuned smaller models match or\nsurpass larger counterparts in performance, offering efficiency for\nresource-limited settings. A new dataset of 60,000 annotated English clinical\nsummaries and 24,000 German translations was validated with automated and\nmanual checks. The evaluations used ROUGE, BERTScore, and entity-level metrics.\nThe work highlights the approach's viability and outlines future improvements.", "published": "2025-02-08 16:44:56", "link": "http://arxiv.org/abs/2502.05638v1", "categories": ["cs.CL", "cs.AI", "I.2.6; I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for\n  Psychotherapy", "abstract": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI.", "published": "2025-02-08 17:53:41", "link": "http://arxiv.org/abs/2502.05651v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Vision-Language Models for Emotion Recognition", "abstract": "Large Vision-Language Models (VLMs) have achieved unprecedented success in\nseveral objective multimodal reasoning tasks. However, to further enhance their\ncapabilities of empathetic and effective communication with humans, improving\nhow VLMs process and understand emotions is crucial. Despite significant\nresearch attention on improving affective understanding, there is a lack of\ndetailed evaluations of VLMs for emotion-related tasks, which can potentially\nhelp inform downstream fine-tuning efforts. In this work, we present the first\ncomprehensive evaluation of VLMs for recognizing evoked emotions from images.\nWe create a benchmark for the task of evoked emotion recognition and study the\nperformance of VLMs for this task, from perspectives of correctness and\nrobustness. Through several experiments, we demonstrate important factors that\nemotion recognition performance depends on, and also characterize the various\nerrors made by VLMs in the process. Finally, we pinpoint potential causes for\nerrors through a human evaluation study. We use our experimental results to\ninform recommendations for the future of emotion research in the context of\nVLMs.", "published": "2025-02-08 18:25:31", "link": "http://arxiv.org/abs/2502.05660v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging", "abstract": "Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).", "published": "2025-02-08 18:43:59", "link": "http://arxiv.org/abs/2502.05664v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models Largely Exhibit Human-like Constituent Ordering\n  Preferences", "abstract": "Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering.", "published": "2025-02-08 19:13:40", "link": "http://arxiv.org/abs/2502.05670v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Word Similarity: Semantic Similarity through Classification\n  Confusion", "abstract": "Word similarity has many applications to social science and cultural\nanalytics tasks like measuring meaning change over time and making sense of\ncontested terms. Yet traditional similarity methods based on cosine similarity\nbetween word embeddings cannot capture the context-dependent, asymmetrical,\npolysemous nature of semantic similarity. We propose a new measure of\nsimilarity, Word Confusion, that reframes semantic similarity in terms of\nfeature-based classification confusion. Word Confusion is inspired by Tversky's\nsuggestion that similarity features be chosen dynamically. Here we train a\nclassifier to map contextual embeddings to word identities and use the\nclassifier confusion (the probability of choosing a confounding word c instead\nof the correct target word t) as a measure of the similarity of c and t. The\nset of potential confounding words acts as the chosen features. Our method is\ncomparable to cosine similarity in matching human similarity judgments across\nseveral datasets (MEN, WirdSim353, and SimLex), and can measure similarity\nusing predetermined features of interest. We demonstrate our model's ability to\nmake use of dynamic features by applying it to test a hypothesis about changes\nin the 18th C. meaning of the French word \"revolution\" from popular to state\naction during the French Revolution. We hope this reimagining of semantic\nsimilarity will inspire the development of new tools that better capture the\nmulti-faceted and dynamic nature of language, advancing the fields of\ncomputational social science and cultural analytics and beyond.", "published": "2025-02-08 21:55:38", "link": "http://arxiv.org/abs/2502.05704v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph-Guided Retrieval Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising technology\nfor addressing hallucination issues in the responses generated by large\nlanguage models (LLMs). Existing studies on RAG primarily focus on applying\nsemantic-based approaches to retrieve isolated relevant chunks, which ignore\ntheir intrinsic relationships. In this paper, we propose a novel Knowledge\nGraph-Guided Retrieval Augmented Generation (KG$^2$RAG) framework that utilizes\nknowledge graphs (KGs) to provide fact-level relationships between chunks,\nimproving the diversity and coherence of the retrieved results. Specifically,\nafter performing a semantic-based retrieval to provide seed chunks, KG$^2$RAG\nemploys a KG-guided chunk expansion process and a KG-based chunk organization\nprocess to deliver relevant and important knowledge in well-organized\nparagraphs. Extensive experiments conducted on the HotpotQA dataset and its\nvariants demonstrate the advantages of KG$^2$RAG compared to existing RAG-based\napproaches, in terms of both response quality and retrieval quality.", "published": "2025-02-08 02:14:31", "link": "http://arxiv.org/abs/2502.06864v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific\n  Refusal Tests", "abstract": "The development of robust safety benchmarks for large language models\nrequires open, reproducible datasets that can measure both appropriate refusal\nof harmful content and potential over-restriction of legitimate scientific\ndiscourse. We present an open-source dataset and testing framework for\nevaluating LLM safety mechanisms across mainly controlled substance queries,\nanalyzing four major models' responses to systematically varied prompts. Our\nresults reveal distinct safety profiles: Claude-3.5-sonnet demonstrated the\nmost conservative approach with 73% refusals and 27% allowances, while Mistral\nattempted to answer 100% of queries. GPT-3.5-turbo showed moderate restriction\nwith 10% refusals and 90% allowances, and Grok-2 registered 20% refusals and\n80% allowances. Testing prompt variation strategies revealed decreasing\nresponse consistency, from 85% with single prompts to 65% with five variations.\nThis publicly available benchmark enables systematic evaluation of the critical\nbalance between necessary safety restrictions and potential over-censorship of\nlegitimate scientific inquiry, while providing a foundation for measuring\nprogress in AI safety implementation. Chain-of-thought analysis reveals\npotential vulnerabilities in safety mechanisms, highlighting the complexity of\nimplementing robust safeguards without unduly restricting desirable and valid\nscientific discourse.", "published": "2025-02-08 04:27:33", "link": "http://arxiv.org/abs/2502.06867v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of\n  Knowledge Editing in Same-Subject", "abstract": "Knowledge editing has become a promising approach for efficiently and\nprecisely updating knowledge embedded in large language models (LLMs). In this\nwork, we focus on Same-Subject Editing, which involves modifying multiple\nattributes of a single entity to ensure comprehensive and consistent updates to\nentity-centric knowledge. Through preliminary observation, we identify a\nsignificant challenge: Current state-of-the-art editing methods struggle when\ntasked with editing multiple related knowledge pieces for the same subject. To\naddress the lack of relevant editing data for identical subjects in traditional\nbenchmarks, we introduce the $\\text{S}^2\\text{RKE}$(Same-Subject Related\nKnowledge Editing) benchmark. Our extensive experiments reveal that only\nmainstream locate-then-edit methods, such as ROME and MEMIT, exhibit \"related\nknowledge perturbation,\" where subsequent edits interfere with earlier ones.\nFurther analysis reveals that these methods over-rely on subject information,\nneglecting other critical factors, resulting in reduced editing effectiveness.", "published": "2025-02-08 04:47:17", "link": "http://arxiv.org/abs/2502.06868v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Trustworthy Retrieval Augmented Generation for Large Language\n  Models: A Survey", "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to\naddress the challenges of Artificial Intelligence-Generated Content (AIGC). By\nintegrating context retrieval into content generation, RAG provides reliable\nand up-to-date external knowledge, reduces hallucinations, and ensures relevant\ncontext across a wide range of tasks. However, despite RAG's success and\npotential, recent studies have shown that the RAG paradigm also introduces new\nrisks, including robustness issues, privacy concerns, adversarial attacks, and\naccountability issues. Addressing these risks is critical for future\napplications of RAG systems, as they directly impact their trustworthiness.\nAlthough various methods have been developed to improve the trustworthiness of\nRAG methods, there is a lack of a unified perspective and framework for\nresearch in this topic. Thus, in this paper, we aim to address this gap by\nproviding a comprehensive roadmap for developing trustworthy RAG systems. We\nplace our discussion around five key perspectives: reliability, privacy,\nsafety, fairness, explainability, and accountability. For each perspective, we\npresent a general framework and taxonomy, offering a structured approach to\nunderstanding the current challenges, evaluating existing solutions, and\nidentifying promising future research directions. To encourage broader adoption\nand innovation, we also highlight the downstream applications where trustworthy\nRAG systems have a significant impact.", "published": "2025-02-08 06:50:47", "link": "http://arxiv.org/abs/2502.06872v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic\n  Reasoning", "abstract": "Previous research has revealed the potential of large language models (LLMs)\nto support cognitive reframing therapy; however, their focus was primarily on\ntext-based methods, often overlooking the importance of non-verbal evidence\ncrucial in real-life therapy. To alleviate this gap, we extend the textual\ncognitive reframing to multimodality, incorporating visual clues. Specifically,\nwe present a new dataset called Multi Modal-Cognitive Support Conversation\n(M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects\nthe virtual client's facial expressions. To better mirror real psychotherapy,\nwhere facial expressions lead to interpreting implicit emotional evidence, we\npropose a multi-hop psychotherapeutic reasoning approach that explicitly\nidentifies and incorporates subtle evidence. Our comprehensive experiments with\nboth LLMs and vision-language models (VLMs) demonstrate that the VLMs'\nperformance as psychotherapists is significantly improved with the M2CoSC\ndataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables\nVLMs to provide more thoughtful and empathetic suggestions, outperforming\nstandard prompting methods.", "published": "2025-02-08 07:32:48", "link": "http://arxiv.org/abs/2502.06873v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Simulator Drives Language Models for Legal Intensive\n  Interaction", "abstract": "Large Language Models (LLMs) have significantly advanced legal intelligence,\nbut the scarcity of scenario data impedes the progress toward interactive legal\nscenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER)\nto scalably generate synthetic data by simulating interactive legal scenarios.\nLeveraging real-legal case sources, MASER ensures the consistency of legal\nattributes between participants and introduces a supervisory mechanism to align\nparticipants' characters and behaviors as well as addressing distractions. A\nMulti-stage Interactive Legal Evaluation (MILE) benchmark is further\nconstructed to evaluate LLMs' performance in dynamic legal scenarios. Extensive\nexperiments confirm the effectiveness of our framework.", "published": "2025-02-08 15:05:24", "link": "http://arxiv.org/abs/2502.06882v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Refining Positive and Toxic Samples for Dual Safety Self-Alignment of\n  LLMs with Minimal Human Interventions", "abstract": "Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction\ntuning and reinforcement learning to calibrate the output of large language\nmodels (LLMs) with human intentions, ensuring the outputs are harmless and\nhelpful. Existing methods heavily depend on the manual annotation of\nhigh-quality positive samples, while contending with issues such as noisy\nlabels and minimal distinctions between preferred and dispreferred response\ndata. However, readily available toxic samples with clear safety distinctions\nare often filtered out, removing valuable negative references that could aid\nLLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety\nself-alignment approach that minimizes human supervision by automatically\nrefining positive and toxic samples and performing fine-grained dual\ninstruction tuning. Positive samples are harmless responses, while toxic\nsamples deliberately contain extremely harmful content, serving as a new\nsupervisory signals. Specifically, we utilize LLM itself to iteratively\ngenerate and refine training instances by only exploring fewer than 50 human\nannotations. We then employ two losses, i.e., maximum likelihood estimation\n(MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance\nthe LLM's safety. The MLE loss encourages an LLM to maximize the generation of\nharmless content based on positive samples. Conversely, the fine-grained UT\nloss guides the LLM to minimize the output of harmful words based on negative\nsamples at the token-level, thereby guiding the model to decouple safety from\neffectiveness, directing it toward safer fine-tuning objectives, and increasing\nthe likelihood of generating helpful and reliable content. Experiments on 9\npopular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for\nsafety alignment, while maintaining comparable levels of helpfulness and\nusefulness.", "published": "2025-02-08 09:54:47", "link": "http://arxiv.org/abs/2502.08657v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Agentic AI Systems Applied to tasks in Financial Services: Modeling and\n  model risk management crews", "abstract": "The advent of large language models has ushered in a new era of agentic\nsystems, where artificial intelligence programs exhibit remarkable autonomous\ndecision-making capabilities across diverse domains. This paper explores\nagentic system workflows in the financial services industry. In particular, we\nbuild agentic crews that can effectively collaborate to perform complex\nmodeling and model risk management (MRM) tasks. The modeling crew consists of a\nmanager and multiple agents who perform specific tasks such as exploratory data\nanalysis, feature engineering, model selection, hyperparameter tuning, model\ntraining, model evaluation, and writing documentation. The MRM crew consists of\na manager along with specialized agents who perform tasks such as checking\ncompliance of modeling documentation, model replication, conceptual soundness,\nanalysis of outcomes, and writing documentation. We demonstrate the\neffectiveness and robustness of modeling and MRM crews by presenting a series\nof numerical examples applied to credit card fraud detection, credit card\napproval, and portfolio credit risk modeling datasets.", "published": "2025-02-08 04:03:47", "link": "http://arxiv.org/abs/2502.05439v1", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.LG", "68T01 (Primary) 68T05, 68N99, 68T05, 68T20, 68T50, 62H30, 65C20,\n  68P20 (Secondary)", "I.2.0; I.2.1; I.2.2; I.2.6; I.2.7; I.5.1; I.6.0; I.7.1"], "primary_category": "cs.AI"}
{"title": "Iterative Deepening Sampling for Large Language Models", "abstract": "The recent release of OpenAI's o1 models and other similar frameworks\nshowcasing test-time scaling laws has demonstrated their exceptional capability\nto tackle complex reasoning tasks. Inspired by this, subsequent research has\nrevealed that such test-time scaling laws hinge on the model's ability to\nsearch both within a single response (intra-response) and across multiple\nresponses (inter-response) during training. Crucially, beyond selecting a\nsingle optimal response, the model must also develop robust self-correction\ncapabilities within its own outputs. However, training models to achieve\neffective self-evaluation and self-correction remains a significant challenge,\nheavily dependent on the quality of self-reflection data. In this paper, we\naddress this challenge by focusing on enhancing the quality of self-reflection\ndata generation for complex problem-solving, which can subsequently improve the\ntraining of next-generation large language models (LLMs). Specifically, we\nexplore how manually triggering a model's self-correction mechanisms can\nimprove performance on challenging reasoning tasks. To this end, we propose a\nnovel iterative deepening sampling algorithm framework designed to enhance\nself-correction and generate higher-quality samples. Through extensive\nexperiments on Math500 and AIME benchmarks, we demonstrate that our method\nachieves a higher success rate on difficult tasks and provide detailed ablation\nstudies to analyze its effectiveness across diverse settings.", "published": "2025-02-08 04:39:51", "link": "http://arxiv.org/abs/2502.05449v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and\n  Synthesis of Data", "abstract": "Autoformalization, the process of automatically translating natural language\nmathematics into machine-verifiable formal language, has demonstrated\nadvancements with the progress of large language models (LLMs). However, a key\nobstacle to further advancements is the scarcity of paired datasets that align\nnatural language with formal language. To address this challenge, we introduce\nATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of\nData), an iterative data generation framework designed to produce large-scale,\nhigh-quality parallel theorem statements. With the proposed ATLAS running for\n10 iterations, we construct an undergraduate-level dataset comprising 300k\ntheorem statements and develop the ATLAS translator, achieving accuracies of\n80.59% (pass@8) and 92.99% (pass@128) on ProofNet, significantly outperforming\nthe base model (23.99% and 47.17%) and InternLM2-Math-Plus-7B (50.94% and\n80.32%). Furthermore, the ATLAS translator also achieves state-of-the-art\nperformance on both the high-school-level miniF2F dataset and the\ngraduate-level MathQual dataset introduced in this work. The datasets, model,\nand code will be released to the public soon.", "published": "2025-02-08 13:28:51", "link": "http://arxiv.org/abs/2502.05567v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Multimodal Models for Low-Resource Languages: A Survey", "abstract": "In this survey, we systematically analyze techniques used to adapt large\nmultimodal models (LMMs) for low-resource (LR) languages, examining approaches\nranging from visual enhancement and data creation to cross-modal transfer and\nfusion strategies. Through a comprehensive analysis of 106 studies across 75 LR\nlanguages, we identify key patterns in how researchers tackle the challenges of\nlimited data and computational resources. We find that visual information often\nserves as a crucial bridge for improving model performance in LR settings,\nthough significant challenges remain in areas such as hallucination mitigation\nand computational efficiency. We aim to provide researchers with a clear\nunderstanding of current approaches and remaining challenges in making LMMs\nmore accessible to speakers of LR (understudied) languages. We complement our\nsurvey with an open-source repository available at:\nhttps://github.com/marianlupascu/LMM4LRL-Survey.", "published": "2025-02-08 13:29:44", "link": "http://arxiv.org/abs/2502.05568v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gender Bias in Instruction-Guided Speech Synthesis Models", "abstract": "Recent advancements in controllable expressive speech synthesis, especially\nin text-to-speech (TTS) models, have allowed for the generation of speech with\nspecific styles guided by textual descriptions, known as style prompts. While\nthis development enhances the flexibility and naturalness of synthesized\nspeech, there remains a significant gap in understanding how these models\nhandle vague or abstract style prompts. This study investigates the potential\ngender bias in how models interpret occupation-related prompts, specifically\nexamining their responses to instructions like \"Act like a nurse\". We explore\nwhether these models exhibit tendencies to amplify gender stereotypes when\ninterpreting such prompts. Our experimental results reveal the model's tendency\nto exhibit gender bias for certain occupations. Moreover, models of different\nsizes show varying degrees of this bias across these occupations.", "published": "2025-02-08 17:38:24", "link": "http://arxiv.org/abs/2502.05649v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study\n  of Gemini, LLaMA and ChatGPT", "abstract": "This study investigates the performance of various large language models\n(LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that\nintegrates entity recognition and relation extraction without requiring\nannotated data. While LLMs show promise for RE, most prior work focuses on\nEnglish or assumes pre-annotated entities, leaving their effectiveness in\nChinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini,\nand LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates\nthe highest overall performance, balancing precision and recall, while Gemini\nachieves the fastest inference speed, making it suitable for real-time\napplications. LLaMA underperforms in both accuracy and latency, highlighting\nthe need for further adaptation. Our findings provide insights into the\nstrengths and limitations of LLMs for zero-shot Chinese RE, shedding light on\ntrade-offs between accuracy and efficiency. This study serves as a foundation\nfor future research aimed at improving LLM adaptability to complex linguistic\ntasks in Chinese NLP.", "published": "2025-02-08 21:12:04", "link": "http://arxiv.org/abs/2502.05694v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Group Reasoning Emission Estimation Networks", "abstract": "Accurate greenhouse gas (GHG) emission reporting is critical for governments,\nbusinesses, and investors. However, adoption remains limited particularly among\nsmall and medium enterprises due to high implementation costs, fragmented\nemission factor databases, and a lack of robust sector classification methods.\nTo address these challenges, we introduce Group Reasoning Emission Estimation\nNetworks (GREEN), an AI-driven carbon accounting framework that standardizes\nenterprise-level emission estimation, constructs a large-scale benchmark\ndataset, and leverages a novel reasoning approach with large language models\n(LLMs). Specifically, we compile textual descriptions for 20,850 companies with\nvalidated North American Industry Classification System (NAICS) labels and\nalign these with an economic model of carbon intensity factors. By reframing\nsector classification as an information retrieval task, we fine-tune\nSentence-BERT models using a contrastive learning loss. To overcome the\nlimitations of single-stage models in handling thousands of hierarchical\ncategories, we propose a Group Reasoning method that ensembles LLM classifiers\nbased on the natural NAICS ontology, decomposing the task into multiple\nsub-classification steps. We theoretically prove that this approach reduces\nclassification uncertainty and computational complexity. Experiments on 1,114\nNAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%\nTop-10 accuracy), and case studies on 20 companies report a mean absolute\npercentage error (MAPE) of 45.88%. The project is available at:\nhttps://huggingface.co/datasets/Yvnminc/ExioNAICS.", "published": "2025-02-08 09:02:43", "link": "http://arxiv.org/abs/2502.06874v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Vision: How Large Language Models Interpret Facial Expressions\n  from Valence-Arousal Values", "abstract": "Large Language Models primarily operate through text-based inputs and\noutputs, yet human emotion is communicated through both verbal and non-verbal\ncues, including facial expressions. While Vision-Language Models analyze facial\nexpressions from images, they are resource-intensive and may depend more on\nlinguistic priors than visual understanding. To address this, this study\ninvestigates whether LLMs can infer affective meaning from dimensions of facial\nexpressions-Valence and Arousal values, structured numerical representations,\nrather than using raw visual input. VA values were extracted using Facechannel\nfrom images of facial expressions and provided to LLMs in two tasks: (1)\ncategorizing facial expressions into basic (on the IIMI dataset) and complex\nemotions (on the Emotic dataset) and (2) generating semantic descriptions of\nfacial expressions (on the Emotic dataset). Results from the categorization\ntask indicate that LLMs struggle to classify VA values into discrete emotion\ncategories, particularly for emotions beyond basic polarities (e.g., happiness,\nsadness). However, in the semantic description task, LLMs produced textual\ndescriptions that align closely with human-generated interpretations,\ndemonstrating a stronger capacity for free text affective inference of facial\nexpressions.", "published": "2025-02-08 09:54:03", "link": "http://arxiv.org/abs/2502.06875v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and\n  Harmlessness of Large Language Model via Model Merging", "abstract": "Achieving balanced alignment of large language models (LLMs) in terms of\nHelpfulness, Honesty, and Harmlessness (3H optimization) constitutes a\ncornerstone of responsible AI, with existing methods like data mixture\nstrategies facing limitations including reliance on expert knowledge and\nconflicting optimization signals. While model merging offers a promising\nalternative by integrating specialized models, its potential for 3H\noptimization remains underexplored. This paper establishes the first\ncomprehensive benchmark for model merging in 3H-aligned LLMs, systematically\nevaluating 15 methods (12 training-free merging and 3 data mixture techniques)\nacross 10 datasets associated with 5 annotation dimensions, 2 LLM families, and\n2 training paradigms. Our analysis reveals three pivotal insights: (i)\npreviously overlooked collaborative/conflicting relationships among 3H\ndimensions, (ii) the consistent superiority of model merging over data mixture\napproaches in balancing alignment trade-offs, and (iii) the critical role of\nparameter-level conflict resolution through redundant component pruning and\noutlier mitigation. Building on these findings, we propose R-TSVM, a\nReweighting-enhanced Task Singular Vector Merging method that incorporates\noutlier-aware parameter weighting and sparsity-adaptive rank selection\nstrategies adapted to the heavy-tailed parameter distribution and sparsity for\nLLMs, further improving LLM alignment across multiple evaluations. We release\nour trained models for further exploration.", "published": "2025-02-08 11:56:58", "link": "http://arxiv.org/abs/2502.06876v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of Large Language Models in Automating\n  Categorization of Scientific Texts", "abstract": "The rapid advancement of Large Language Models (LLMs) has led to a multitude\nof application opportunities. One traditional task for Information Retrieval\nsystems is the summarization and classification of texts, both of which are\nimportant for supporting humans in navigating large literature bodies as they\ne.g. exist with scientific publications. Due to this rapidly growing body of\nscientific knowledge, recent research has been aiming at building research\ninformation systems that not only offer traditional keyword search\ncapabilities, but also novel features such as the automatic detection of\nresearch areas that are present at knowledge intensive organizations in\nacademia and industry. To facilitate this idea, we present the results obtained\nfrom evaluating a variety of LLMs in their ability to sort scientific\npublications into hierarchical classifications systems. Using the FORC dataset\nas ground truth data, we have found that recent LLMs (such as Meta Llama 3.1)\nare able to reach an accuracy of up to 0.82, which is up to 0.08 better than\ntraditional BERT models.", "published": "2025-02-08 20:37:21", "link": "http://arxiv.org/abs/2502.15745v1", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Expressive Voice Conversion with Discrete Pitch-Conditioned\n  Flow Matching Model", "abstract": "This paper introduces PFlow-VC, a conditional flow matching voice conversion\nmodel that leverages fine-grained discrete pitch tokens and target speaker\nprompt information for expressive voice conversion (VC). Previous VC works\nprimarily focus on speaker conversion, with further exploration needed in\nenhancing expressiveness (such as prosody and emotion) for timbre conversion.\nUnlike previous methods, we adopt a simple and efficient approach to enhance\nthe style expressiveness of voice conversion models. Specifically, we pretrain\na self-supervised pitch VQVAE model to discretize speaker-irrelevant pitch\ninformation and leverage a masked pitch-conditioned flow matching model for\nMel-spectrogram synthesis, which provides in-context pitch modeling\ncapabilities for the speaker conversion model, effectively improving the voice\nstyle transfer capacity. Additionally, we improve timbre similarity by\ncombining global timbre embeddings with time-varying timbre tokens. Experiments\non unseen LibriTTS test-clean and emotional speech dataset ESD show the\nsuperiority of the PFlow-VC model in both timbre conversion and style transfer.\nAudio samples are available on the demo page\nhttps://speechai-demo.github.io/PFlow-VC/.", "published": "2025-02-08 07:14:04", "link": "http://arxiv.org/abs/2502.05471v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Less is More for Synthetic Speech Detection in the Wild", "abstract": "Driven by advances in self-supervised learning for speech, state-of-the-art\nsynthetic speech detectors have achieved low error rates on popular benchmarks\nsuch as ASVspoof. However, prior benchmarks do not address the wide range of\nreal-world variability in speech. Are reported error rates realistic in\nreal-world conditions? To assess detector failure modes and robustness under\ncontrolled distribution shifts, we introduce ShiftySpeech, a benchmark with\nmore than 3000 hours of synthetic speech from 7 domains, 6 TTS systems, 12\nvocoders, and 3 languages. We found that all distribution shifts degraded model\nperformance, and contrary to prior findings, training on more vocoders,\nspeakers, or with data augmentation did not guarantee better generalization. In\nfact, we found that training on less diverse data resulted in better\ngeneralization, and that a detector fit using samples from a single carefully\nselected vocoder and a small number of speakers, without data augmentations,\nachieved state-of-the-art results on the challenging In-the-Wild benchmark.", "published": "2025-02-08 19:49:09", "link": "http://arxiv.org/abs/2502.05674v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning", "abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias\ndue to training and inference mismatch. Prior works propose the contrastive\nmethod to deal with caption degeneration. However, the contrastive method\nignores the temporal information when measuring similarity across acoustic and\nlinguistic modalities, leading to inferior performance. In this work, we\ndevelop the temporal-similarity score by introducing the unbiased sliced\nWasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to\naccount for temporal information across modalities. In contrast to the\nconventional sliced Wasserstein RBF kernel, we can form an unbiased estimation\nof USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to\nstochastic gradient optimization algorithms, and its approximation error\ndecreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo\nsamples. Additionally, we introduce an audio captioning framework based on the\nunbiased sliced Wasserstein kernel, incorporating stochastic decoding methods\nto mitigate caption degeneration during the generation process. We conduct\nextensive quantitative and qualitative experiments on two datasets, AudioCaps\nand Clotho, to illustrate the capability of generating high-quality audio\ncaptions. Experimental results show that our framework is able to increase\ncaption length, lexical diversity, and text-to-audio self-retrieval accuracy.", "published": "2025-02-08 03:47:06", "link": "http://arxiv.org/abs/2502.05435v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot\n  Text-To-Speech System", "abstract": "Recently, large language model (LLM) based text-to-speech (TTS) systems have\ngradually become the mainstream in the industry due to their high naturalness\nand powerful zero-shot voice cloning capabilities.Here, we introduce the\nIndexTTS system, which is mainly based on the XTTS and Tortoise model. We add\nsome novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid\nmodeling method that combines characters and pinyin, making the pronunciations\nof polyphonic characters and long-tail characters controllable. We also\nperformed a comparative analysis of the Vector Quantization (VQ) with\nFinite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech\ntokens. To further enhance the effect and stability of voice cloning, we\nintroduce a conformer-based speech conditional encoder and replace the\nspeechcode decoder with BigVGAN2. Compared with XTTS, it has achieved\nsignificant improvements in naturalness, content consistency, and zero-shot\nvoice cloning. As for the popular TTS systems in the open-source, such as\nFish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively\nsimple training process, more controllable usage, and faster inference speed.\nMoreover, its performance surpasses that of these systems. Our demos are\navailable at https://index-tts.github.io.", "published": "2025-02-08 10:23:20", "link": "http://arxiv.org/abs/2502.05512v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
