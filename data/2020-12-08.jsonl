{"title": "Improving Human-Labeled Data through Dynamic Automatic Conflict\n  Resolution", "abstract": "This paper develops and implements a scalable methodology for (a) estimating\nthe noisiness of labels produced by a typical crowdsourcing semantic annotation\ntask, and (b) reducing the resulting error of the labeling process by as much\nas 20-30% in comparison to other common labeling strategies. Importantly, this\nnew approach to the labeling process, which we name Dynamic Automatic Conflict\nResolution (DACR), does not require a ground truth dataset and is instead based\non inter-project annotation inconsistencies. This makes DACR not only more\naccurate but also available to a broad range of labeling tasks. In what follows\nwe present results from a text classification task performed at scale for a\ncommercial personal assistant, and evaluate the inherent ambiguity uncovered by\nthis annotation strategy as compared to other common labeling strategies.", "published": "2020-12-08 02:22:09", "link": "http://arxiv.org/abs/2012.04169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Label Refinement Improves Dataless Text Classification", "abstract": "Dataless text classification is capable of classifying documents into\npreviously unseen labels by assigning a score to any document paired with a\nlabel description. While promising, it crucially relies on accurate\ndescriptions of the label set for each downstream task. This reliance causes\ndataless classifiers to be highly sensitive to the choice of label descriptions\nand hinders the broader application of dataless classification in practice. In\nthis paper, we ask the following question: how can we improve dataless text\nclassification using the inputs of the downstream task dataset? Our primary\nsolution is a clustering based approach. Given a dataless classifier, our\napproach refines its set of predictions using k-means clustering. We\ndemonstrate the broad applicability of our approach by improving the\nperformance of two widely used classifier architectures, one that encodes\ntext-category pairs with two independent encoders and one with a single joint\nencoder. Experiments show that our approach consistently improves dataless\nclassification across different datasets and makes the classifier more robust\nto the choice of label descriptions.", "published": "2020-12-08 03:37:50", "link": "http://arxiv.org/abs/2012.04194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CTRLsum: Towards Generic Controllable Text Summarization", "abstract": "Current summarization systems yield generic summaries that are disconnected\nfrom users' preferences and expectations. To address this limitation, we\npresent CTRLsum, a novel framework for controllable summarization. Our approach\nenables users to control multiple aspects of generated summaries by interacting\nwith the summarization system through textual input in the form of a set of\nkeywords or descriptive prompts. Using a single unified model, CTRLsum is able\nto achieve a broad scope of summary manipulation at inference time without\nrequiring additional human annotations or pre-defining a set of control aspects\nduring training. We quantitatively demonstrate the effectiveness of our\napproach on three domains of summarization datasets and five control aspects:\n1) entity-centric and 2) length-controllable summarization, 3) contribution\nsummarization on scientific papers, 4) invention purpose summarization on\npatent filings, and 5) question-guided summarization on news articles in a\nreading comprehension setting. Moreover, when used in a standard, uncontrolled\nsummarization setting, CTRLsum achieves state-of-the-art results on the\nCNN/DailyMail dataset. Code and model checkpoints are available at\nhttps://github.com/salesforce/ctrl-sum", "published": "2020-12-08 08:54:36", "link": "http://arxiv.org/abs/2012.04281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facts2Story: Controlling Text Generation by Key Facts", "abstract": "Recent advancements in self-attention neural network architectures have\nraised the bar for open-ended text generation. Yet, while current methods are\ncapable of producing a coherent text which is several hundred words long,\nattaining control over the content that is being generated -- as well as\nevaluating it -- are still open questions. We propose a controlled generation\ntask which is based on expanding a sequence of facts, expressed in natural\nlanguage, into a longer narrative. We introduce human-based evaluation metrics\nfor this task, as well as a method for deriving a large training dataset. We\nevaluate three methods on this task, based on fine-tuning pre-trained models.\nWe show that while auto-regressive, unidirectional Language Models such as GPT2\nproduce better fluency, they struggle to adhere to the requested facts. We\npropose a plan-and-cloze model (using fine-tuned XLNet) which produces\ncompetitive fluency while adhering to the requested content.", "published": "2020-12-08 10:14:29", "link": "http://arxiv.org/abs/2012.04332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Bag of Sentences to Document: Distantly Supervised Relation\n  Extraction via Machine Reading Comprehension", "abstract": "Distant supervision (DS) is a promising approach for relation extraction but\noften suffers from the noisy label problem. Traditional DS methods usually\nrepresent an entity pair as a bag of sentences and denoise labels using\nmulti-instance learning techniques. The bag-based paradigm, however, fails to\nleverage the inter-sentence-level and the entity-level evidence for relation\nextraction, and their denoising algorithms are often specialized and\ncomplicated. In this paper, we propose a new DS paradigm--document-based\ndistant supervision, which models relation extraction as a document-based\nmachine reading comprehension (MRC) task. By re-organizing all sentences about\nan entity as a document and extracting relations via querying the document with\nrelation-specific questions, the document-based DS paradigm can simultaneously\nencode and exploit all sentence-level, inter-sentence-level, and entity-level\nevidence. Furthermore, we design a new loss function--DSLoss (distant\nsupervision loss), which can effectively train MRC models using only\n$\\langle$document, question, answer$\\rangle$ tuples, therefore noisy label\nproblem can be inherently resolved. Experiments show that our method achieves\nnew state-of-the-art DS performance.", "published": "2020-12-08 10:16:27", "link": "http://arxiv.org/abs/2012.04334v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Chinese Parsing Exploiting Lexicons", "abstract": "Chinese parsing has traditionally been solved by three pipeline systems\nincluding word-segmentation, part-of-speech tagging and dependency parsing\nmodules. In this paper, we propose an end-to-end Chinese parsing model based on\ncharacter inputs which jointly learns to output word segmentation,\npart-of-speech tags and dependency structures. In particular, our parsing model\nrelies on word-char graph attention networks, which can enrich the character\ninputs with external word knowledge. Experiments on three Chinese parsing\nbenchmark datasets show the effectiveness of our models, achieving the\nstate-of-the-art results on end-to-end Chinese parsing.", "published": "2020-12-08 12:24:36", "link": "http://arxiv.org/abs/2012.04395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extractive Opinion Summarization in Quantized Transformer Spaces", "abstract": "We present the Quantized Transformer (QT), an unsupervised system for\nextractive opinion summarization. QT is inspired by Vector-Quantized\nVariational Autoencoders, which we repurpose for popularity-driven\nsummarization. It uses a clustering interpretation of the quantized space and a\nnovel extraction algorithm to discover popular opinions among hundreds of\nreviews, a significant step towards opinion summarization of practical scope.\nIn addition, QT enables controllable summarization without further training, by\nutilizing properties of the quantized space to extract aspect-specific\nsummaries. We also make publicly available SPACE, a large-scale evaluation\nbenchmark for opinion summarizers, comprising general and aspect-specific\nsummaries for 50 hotels. Experiments demonstrate the promise of our approach,\nwhich is validated by human studies where judges showed clear preference for\nour method over competitive baselines.", "published": "2020-12-08 14:23:46", "link": "http://arxiv.org/abs/2012.04443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Interpretable Patterns in Deep Learning for Morphology", "abstract": "We examine the role of character patterns in three tasks: morphological\nanalysis, lemmatization and copy. We use a modified version of the standard\nsequence-to-sequence model, where the encoder is a pattern matching network.\nEach pattern scores all possible N character long subwords (substrings) on the\nsource side, and the highest scoring subword's score is used to initialize the\ndecoder as well as the input to the attention mechanism. This method allows\nlearning which subwords of the input are important for generating the output.\nBy training the models on the same source but different target, we can compare\nwhat subwords are important for different tasks and how they relate to each\nother. We define a similarity metric, a generalized form of the Jaccard\nsimilarity, and assign a similarity score to each pair of the three tasks that\nwork on the same source but may differ in target. We examine how these three\ntasks are related to each other in 12 languages. Our code is publicly\navailable.", "published": "2020-12-08 17:20:20", "link": "http://arxiv.org/abs/2012.04575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact-Enhanced Synthetic News Generation", "abstract": "The advanced text generation methods have witnessed great success in text\nsummarization, language translation, and synthetic news generation. However,\nthese techniques can be abused to generate disinformation and fake news. To\nbetter understand the potential threats of synthetic news, we develop a new\ngeneration method FactGen to generate high-quality news content. The existing\ntext generation methods either afford limited supplementary information or lose\nconsistency between the input and output which makes the synthetic news less\ntrustworthy. To address these issues, FactGen retrieves external facts to\nenrich the output and reconstructs the input claim from the generated content\nto improve the consistency among the input and the output. Experiment results\non real-world datasets show that the generated news contents of FactGen are\nconsistent and contain rich facts. We also discuss the possible defending\nmethod to identify these synthetic news pieces if FactGen is used to generate\nsynthetic news.", "published": "2020-12-08 22:54:35", "link": "http://arxiv.org/abs/2012.04778v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Represent Programs with Heterogeneous Graphs", "abstract": "Program source code contains complex structure information, which can be\nrepresented in structured data forms like trees or graphs. To acquire the\nstructural information in source code, most existing researches use abstract\nsyntax trees (AST). A group of works add additional edges to ASTs to convert\nsource code into graphs and use graph neural networks to learn representations\nfor program graphs. Although these works provide additional control or data\nflow information to ASTs for downstream tasks, they neglect an important aspect\nof structure information in AST itself: the different types of nodes and edges.\nIn ASTs, different nodes contain different kinds of information like variables\nor control flow, and the relation between a node and all its children can also\nbe different.\n  To address the information of node and edge types, we bring the idea of\nheterogeneous graphs to learning on source code and present a new formula of\nbuilding heterogeneous program graphs from ASTs with additional type\ninformation for nodes and edges. We use the ASDL grammar of programming\nlanguage to define the node and edge types of program graphs. Then we use\nheterogeneous graph neural networks to learn on these graphs. We evaluate our\napproach on two tasks: code comment generation and method naming. Both tasks\nrequire reasoning on the semantics of complete code snippets. Experiment\nresults show that our approach outperforms baseline models, including\nhomogeneous graph-based models, showing that leveraging the type information of\nnodes and edges in program graphs can help in learning program semantics.", "published": "2020-12-08 03:14:28", "link": "http://arxiv.org/abs/2012.04188v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Topological Method for Comparing Document Semantics", "abstract": "Comparing document semantics is one of the toughest tasks in both Natural\nLanguage Processing and Information Retrieval. To date, on one hand, the tools\nfor this task are still rare. On the other hand, most relevant methods are\ndevised from the statistic or the vector space model perspectives but nearly\nnone from a topological perspective. In this paper, we hope to make a different\nsound. A novel algorithm based on topological persistence for comparing\nsemantics similarity between two documents is proposed. Our experiments are\nconducted on a document dataset with human judges' results. A collection of\nstate-of-the-art methods are selected for comparison. The experimental results\nshow that our algorithm can produce highly human-consistent results, and also\nbeats most state-of-the-art methods though ties with NLTK.", "published": "2020-12-08 04:21:40", "link": "http://arxiv.org/abs/2012.04203v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Early Detection of Fake News by Utilizing the Credibility of News,\n  Publishers, and Users Based on Weakly Supervised Learning", "abstract": "The dissemination of fake news significantly affects personal reputation and\npublic trust. Recently, fake news detection has attracted tremendous attention,\nand previous studies mainly focused on finding clues from news content or\ndiffusion path. However, the required features of previous models are often\nunavailable or insufficient in early detection scenarios, resulting in poor\nperformance. Thus, early fake news detection remains a tough challenge.\nIntuitively, the news from trusted and authoritative sources or shared by many\nusers with a good reputation is more reliable than other news. Using the\ncredibility of publishers and users as prior weakly supervised information, we\ncan quickly locate fake news in massive news and detect them in the early\nstages of dissemination.\n  In this paper, we propose a novel Structure-aware Multi-head Attention\nNetwork (SMAN), which combines the news content, publishing, and reposting\nrelations of publishers and users, to jointly optimize the fake news detection\nand credibility prediction tasks. In this way, we can explicitly exploit the\ncredibility of publishers and users for early fake news detection. We conducted\nexperiments on three real-world datasets, and the results show that SMAN can\ndetect fake news in 4 hours with an accuracy of over 91%, which is much faster\nthan the state-of-the-art models.", "published": "2020-12-08 05:53:33", "link": "http://arxiv.org/abs/2012.04233v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Revisiting Iterative Back-Translation from the Perspective of\n  Compositional Generalization", "abstract": "Human intelligence exhibits compositional generalization (i.e., the capacity\nto understand and produce unseen combinations of seen components), but current\nneural seq2seq models lack such ability. In this paper, we revisit iterative\nback-translation, a simple yet effective semi-supervised method, to investigate\nwhether and how it can improve compositional generalization. In this work: (1)\nWe first empirically show that iterative back-translation substantially\nimproves the performance on compositional generalization benchmarks (CFQ and\nSCAN). (2) To understand why iterative back-translation is useful, we carefully\nexamine the performance gains and find that iterative back-translation can\nincreasingly correct errors in pseudo-parallel data. (3) To further encourage\nthis mechanism, we propose curriculum iterative back-translation, which better\nimproves the quality of pseudo-parallel data, thus further improving the\nperformance.", "published": "2020-12-08 08:43:13", "link": "http://arxiv.org/abs/2012.04276v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Transfer of Abstractive Summarizer to Less-resource\n  Language", "abstract": "Automatic text summarization extracts important information from texts and\npresents the information in the form of a summary. Abstractive summarization\napproaches progressed significantly by switching to deep neural networks, but\nresults are not yet satisfactory, especially for languages where large training\nsets do not exist. In several natural language processing tasks, a\ncross-lingual model transfer is successfully applied in less-resource\nlanguages. For summarization, the cross-lingual model transfer was not\nattempted due to a non-reusable decoder side of neural models that cannot\ncorrect target language generation. In our work, we use a pre-trained English\nsummarization model based on deep neural networks and sequence-to-sequence\narchitecture to summarize Slovene news articles. We address the problem of\ninadequate decoder by using an additional language model for the evaluation of\nthe generated text in target language. We test several cross-lingual\nsummarization models with different amounts of target data for fine-tuning. We\nassess the models with automatic evaluation measures and conduct a small-scale\nhuman evaluation. Automatic evaluation shows that the summaries of our best\ncross-lingual model are useful and of quality similar to the model trained only\nin the target language. Human evaluation shows that our best model generates\nsummaries with high accuracy and acceptable readability. However, similar to\nother abstractive models, our models are not perfect and may occasionally\nproduce misleading or absurd content.", "published": "2020-12-08 09:30:38", "link": "http://arxiv.org/abs/2012.04307v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CrossNER: Evaluating Cross-Domain Named Entity Recognition", "abstract": "Cross-domain named entity recognition (NER) models are able to cope with the\nscarcity issue of NER samples in target domains. However, most of the existing\nNER benchmarks lack domain-specialized entity types or do not focus on a\ncertain domain, leading to a less effective cross-domain evaluation. To address\nthese obstacles, we introduce a cross-domain NER dataset (CrossNER), a\nfully-labeled collection of NER data spanning over five diverse domains with\nspecialized entity categories for different domains. Additionally, we also\nprovide a domain-related corpus since using it to continue pre-training\nlanguage models (domain-adaptive pre-training) is effective for the domain\nadaptation. We then conduct comprehensive experiments to explore the\neffectiveness of leveraging different levels of the domain corpus and\npre-training strategies to do domain-adaptive pre-training for the cross-domain\ntask. Results show that focusing on the fractional corpus containing\ndomain-specialized entities and utilizing a more challenging pre-training\nstrategy in domain-adaptive pre-training are beneficial for the NER domain\nadaptation, and our proposed method can consistently outperform existing\ncross-domain NER baselines. Nevertheless, experiments also illustrate the\nchallenge of this cross-domain NER task. We hope that our dataset and baselines\nwill catalyze research in the NER domain adaptation area. The code and data are\navailable at https://github.com/zliucr/CrossNER.", "published": "2020-12-08 11:31:55", "link": "http://arxiv.org/abs/2012.04373v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Combining Machine Learning and Human Experts to Predict Match Outcomes\n  in Football: A Baseline Model", "abstract": "In this paper, we present a new application-focused benchmark dataset and\nresults from a set of baseline Natural Language Processing and Machine Learning\nmodels for prediction of match outcomes for games of football (soccer). By\ndoing so we give a baseline for the prediction accuracy that can be achieved\nexploiting both statistical match data and contextual articles from human\nsports journalists. Our dataset is focuses on a representative time-period over\n6 seasons of the English Premier League, and includes newspaper match previews\nfrom The Guardian. The models presented in this paper achieve an accuracy of\n63.18% showing a 6.9% boost on the traditional statistical methods.", "published": "2020-12-08 11:52:14", "link": "http://arxiv.org/abs/2012.04380v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distilling Knowledge from Reader to Retriever for Question Answering", "abstract": "The task of information retrieval is an important component of many natural\nlanguage processing systems, such as open domain question answering. While\ntraditional methods were based on hand-crafted features, continuous\nrepresentations based on neural networks recently obtained competitive results.\nA challenge of using such methods is to obtain supervised data to train the\nretriever model, corresponding to pairs of query and support documents. In this\npaper, we propose a technique to learn retriever models for downstream tasks,\ninspired by knowledge distillation, and which does not require annotated pairs\nof query and documents. Our approach leverages attention scores of a reader\nmodel, used to solve the task based on retrieved documents, to obtain synthetic\nlabels for the retriever. We evaluate our method on question answering,\nobtaining state-of-the-art results.", "published": "2020-12-08 17:36:34", "link": "http://arxiv.org/abs/2012.04584v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discourse Parsing of Contentious, Non-Convergent Online Discussions", "abstract": "Online discourse is often perceived as polarized and unproductive. While some\nconversational discourse parsing frameworks are available, they do not\nnaturally lend themselves to the analysis of contentious and polarizing\ndiscussions. Inspired by the Bakhtinian theory of Dialogism, we propose a novel\ntheoretical and computational framework, better suited for non-convergent\ndiscussions. We redefine the measure of a successful discussion, and develop a\nnovel discourse annotation schema which reflects a hierarchy of discursive\nstrategies. We consider an array of classification models -- from Logistic\nRegression to BERT. We also consider various feature types and representations,\ne.g., LIWC categories, standard embeddings, conversational sequences, and\nnon-conversational discourse markers learnt separately. Given the 31 labels in\nthe tagset, an average F-Score of 0.61 is achieved if we allow a different\nmodel for each tag, and 0.526 with a single model. The promising results\nachieved in annotating discussions according to the proposed schema paves the\nway for a number of downstream tasks and applications such as early detection\nof discussion trajectories, active moderation of open discussions, and\nteacher-assistive bots. Finally, we share the first labeled dataset of\ncontentious non-convergent online discussions.", "published": "2020-12-08 17:36:39", "link": "http://arxiv.org/abs/2012.04585v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Edited Media Understanding Frames: Reasoning About the Intent and\n  Implications of Visual Misinformation", "abstract": "Multimodal disinformation, from 'deepfakes' to simple edits that deceive, is\nan important societal problem. Yet at the same time, the vast majority of media\nedits are harmless -- such as a filtered vacation photo. The difference between\nthis example, and harmful edits that spread disinformation, is one of intent.\nRecognizing and describing this intent is a major challenge for today's AI\nsystems.\n  We present the task of Edited Media Understanding, requiring models to answer\nopen-ended questions that capture the intent and implications of an image edit.\nWe introduce a dataset for our task, EMU, with 48k question-answer pairs\nwritten in rich natural language. We evaluate a wide variety of\nvision-and-language models for our task, and introduce a new model PELICAN,\nwhich builds upon recent progress in pretrained multimodal representations. Our\nmodel obtains promising results on our dataset, with humans rating its answers\nas accurate 40.35% of the time. At the same time, there is still much work to\nbe done -- humans prefer human-annotated captions 93.56% of the time -- and we\nprovide analysis that highlights areas for further progress.", "published": "2020-12-08 20:30:43", "link": "http://arxiv.org/abs/2012.04726v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Control Flow Obfuscation for FJ using Continuation Passing", "abstract": "Control flow obfuscation deters software reverse engineering attempts by\naltering the program's control flow transfer. The alternation should not affect\nthe software's run-time behaviour. In this paper, we propose a control flow\nobfuscation approach for FJ with exception handling. The approach is based on a\nsource to source transformation using continuation passing style (CPS). We\nargue that the proposed CPS transformation causes malicious attacks using\ncontext insensitive static analysis and context sensitive analysis with fixed\ncall string to lose precision.", "published": "2020-12-08 06:41:52", "link": "http://arxiv.org/abs/2012.06340v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Large-scale Quantitative Evidence of Media Impact on Public Opinion\n  toward China", "abstract": "Do mass media influence people's opinion of other countries? Using BERT, a\ndeep neural network-based natural language processing model, we analyze a large\ncorpus of 267,907 China-related articles published by The New York Times since\n1970. We then compare our output from The New York Times to a longitudinal data\nset constructed from 101 cross-sectional surveys of the American public's views\non China. We find that the reporting of The New York Times on China in one year\nexplains 54% of the variance in American public opinion on China in the next.\nOur result confirms hypothesized links between media and public opinion and\nhelps shed light on how mass media can influence public opinion of foreign\ncountries.", "published": "2020-12-08 20:14:54", "link": "http://arxiv.org/abs/2012.07575v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Incorporating Domain Knowledge To Improve Topic Segmentation Of Long\n  MOOC Lecture Videos", "abstract": "Topical Segmentation poses a great role in reducing search space of the\ntopics taught in a lecture video specially when the video metadata lacks topic\nwise segmentation information. This segmentation information eases user efforts\nof searching, locating and browsing a topic inside a lecture video. In this\nwork we propose an algorithm, that combines state-of-the art language model and\ndomain knowledge graph for automatically detecting different coherent topics\npresent inside a long lecture video. We use the language model on\nspeech-to-text transcription to capture the implicit meaning of the whole video\nwhile the knowledge graph provides us the domain specific dependencies between\ndifferent concepts of that subjects. Also leveraging the domain knowledge we\ncan capture the way instructor binds and connects different concepts while\nteaching, which helps us in achieving better segmentation accuracy. We tested\nour approach on NPTEL lecture videos and holistic evaluation shows that it out\nperforms the other methods described in the literature.", "published": "2020-12-08 13:37:40", "link": "http://arxiv.org/abs/2012.07589v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Efficient Estimation of Influence of a Training Instance", "abstract": "Understanding the influence of a training instance on a neural network model\nleads to improving interpretability. However, it is difficult and inefficient\nto evaluate the influence, which shows how a model's prediction would be\nchanged if a training instance were not used. In this paper, we propose an\nefficient method for estimating the influence. Our method is inspired by\ndropout, which zero-masks a sub-network and prevents the sub-network from\nlearning each training instance. By switching between dropout masks, we can use\nsub-networks that learned or did not learn each training instance and estimate\nits influence. Through experiments with BERT and VGGNet on classification\ndatasets, we demonstrate that the proposed method can capture training\ninfluences, enhance the interpretability of error predictions, and cleanse the\ntraining dataset for improving generalization.", "published": "2020-12-08 04:31:38", "link": "http://arxiv.org/abs/2012.04207v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions", "abstract": "Humans are able to perceive, understand and reason about causal events.\nDeveloping models with similar physical and causal understanding capabilities\nis a long-standing goal of artificial intelligence. As a step towards this\ndirection, we introduce CRAFT, a new video question answering dataset that\nrequires causal reasoning about physical forces and object interactions. It\ncontains 58K video and question pairs that are generated from 10K videos from\n20 different virtual environments, containing various objects in motion that\ninteract with each other and the scene. Two question categories in CRAFT\ninclude previously studied descriptive and counterfactual questions.\nAdditionally, inspired by the Force Dynamics Theory in cognitive linguistics,\nwe introduce a new causal question category that involves understanding the\ncausal interactions between objects through notions like cause, enable, and\nprevent. Our results show that even though the questions in CRAFT are easy for\nhumans, the tested baseline models, including existing state-of-the-art\nmethods, do not yet deal with the challenges posed in our benchmark.", "published": "2020-12-08 09:11:32", "link": "http://arxiv.org/abs/2012.04293v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Discovering key topics from short, real-world medical inquiries via\n  natural language processing and unsupervised learning", "abstract": "Millions of unsolicited medical inquiries are received by pharmaceutical\ncompanies every year. It has been hypothesized that these inquiries represent a\ntreasure trove of information, potentially giving insight into matters\nregarding medicinal products and the associated medical treatments. However,\ndue to the large volume and specialized nature of the inquiries, it is\ndifficult to perform timely, recurrent, and comprehensive analyses. Here, we\npropose a machine learning approach based on natural language processing and\nunsupervised learning to automatically discover key topics in real-world\nmedical inquiries from customers. This approach does not require ontologies nor\nannotations. The discovered topics are meaningful and medically relevant, as\njudged by medical information specialists, thus demonstrating that unsolicited\nmedical inquiries are a source of valuable customer insights. Our work paves\nthe way for the machine-learning-driven analysis of medical inquiries in the\npharmaceutical industry, which ultimately aims at improving patient care.", "published": "2020-12-08 16:37:34", "link": "http://arxiv.org/abs/2012.04545v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Globetrotter: Connecting Languages by Connecting Images", "abstract": "Machine translation between many languages at once is highly challenging,\nsince training with ground truth requires supervision between all language\npairs, which is difficult to obtain. Our key insight is that, while languages\nmay vary drastically, the underlying visual appearance of the world remains\nconsistent. We introduce a method that uses visual observations to bridge the\ngap between languages, rather than relying on parallel corpora or topological\nproperties of the representations. We train a model that aligns segments of\ntext from different languages if and only if the images associated with them\nare similar and each image in turn is well-aligned with its textual\ndescription. We train our model from scratch on a new dataset of text in over\nfifty languages with accompanying images. Experiments show that our method\noutperforms previous work on unsupervised word and sentence translation using\nretrieval. Code, models and data are available on globetrotter.cs.columbia.edu.", "published": "2020-12-08 18:50:40", "link": "http://arxiv.org/abs/2012.04631v4", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generate Your Counterfactuals: Towards Controlled Counterfactual\n  Generation for Text", "abstract": "Machine Learning has seen tremendous growth recently, which has led to larger\nadoption of ML systems for educational assessments, credit risk, healthcare,\nemployment, criminal justice, to name a few. The trustworthiness of ML and NLP\nsystems is a crucial aspect and requires a guarantee that the decisions they\nmake are fair and robust. Aligned with this, we propose a framework GYC, to\ngenerate a set of counterfactual text samples, which are crucial for testing\nthese ML systems. Our main contributions include a) We introduce GYC, a\nframework to generate counterfactual samples such that the generation is\nplausible, diverse, goal-oriented, and effective, b) We generate counterfactual\nsamples, that can direct the generation towards a corresponding condition such\nas named-entity tag, semantic role label, or sentiment. Our experimental\nresults on various domains show that GYC generates counterfactual text samples\nexhibiting the above four properties. GYC generates counterfactuals that can\nact as test cases to evaluate a model and any text debiasing algorithm.", "published": "2020-12-08 19:34:53", "link": "http://arxiv.org/abs/2012.04698v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open Knowledge Graphs Canonicalization using Variational Autoencoders", "abstract": "Noun phrases and Relation phrases in open knowledge graphs are not\ncanonicalized, leading to an explosion of redundant and ambiguous\nsubject-relation-object triples. Existing approaches to solve this problem take\na two-step approach. First, they generate embedding representations for both\nnoun and relation phrases, then a clustering algorithm is used to group them\nusing the embeddings as features. In this work, we propose Canonicalizing Using\nVariational Autoencoders (CUVA), a joint model to learn both embeddings and\ncluster assignments in an end-to-end approach, which leads to a better vector\nrepresentation for the noun and relation phrases. Our evaluation over multiple\nbenchmarks shows that CUVA outperforms the existing state-of-the-art\napproaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate\nentity canonicalization systems.", "published": "2020-12-08 22:58:30", "link": "http://arxiv.org/abs/2012.04780v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bayesian Learning of LF-MMI Trained Time Delay Neural Networks for\n  Speech Recognition", "abstract": "Discriminative training techniques define state-of-the-art performance for\nautomatic speech recognition systems. However, they are inherently prone to\noverfitting, leading to poor generalization performance when using limited\ntraining data. In order to address this issue, this paper presents a full\nBayesian framework to account for model uncertainty in sequence discriminative\ntraining of factored TDNN acoustic models. Several Bayesian learning based TDNN\nvariant systems are proposed to model the uncertainty over weight parameters\nand choices of hidden activation functions, or the hidden layer outputs.\nEfficient variational inference approaches using a few as one single parameter\nsample ensure their computational cost in both training and evaluation time\ncomparable to that of the baseline TDNN systems. Statistically significant word\nerror rate (WER) reductions of 0.4%-1.8% absolute (5%-11% relative) were\nobtained over a state-of-the-art 900 hour speed perturbed Switchboard corpus\ntrained baseline LF-MMI factored TDNN system using multiple regularization\nmethods including F-smoothing, L2 norm penalty, natural gradient, model\naveraging and dropout, in addition to i-Vector plus learning hidden unit\ncontribution (LHUC) based speaker adaptation and RNNLM rescoring. Consistent\nperformance improvements were also obtained on a 450 hour HKUST conversational\nMandarin telephone speech recognition task. On a third cross domain adaptation\ntask requiring rapidly porting a 1000 hour LibriSpeech data trained system to a\nsmall DementiaBank elderly speech corpus, the proposed Bayesian TDNN LF-MMI\nsystems outperformed the baseline system using direct weight fine-tuning by up\nto 2.5\\% absolute WER reduction.", "published": "2020-12-08 15:32:21", "link": "http://arxiv.org/abs/2012.04494v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Adversarial Disentanglement of Speaker Representation for\n  Attribute-Driven Privacy Preservation", "abstract": "In speech technologies, speaker's voice representation is used in many\napplications such as speech recognition, voice conversion, speech synthesis\nand, obviously, user authentication. Modern vocal representations of the\nspeaker are based on neural embeddings. In addition to the targeted\ninformation, these representations usually contain sensitive information about\nthe speaker, like the age, sex, physical state, education level or ethnicity.\nIn order to allow the user to choose which information to protect, we introduce\nin this paper the concept of attribute-driven privacy preservation in speaker\nvoice representation. It allows a person to hide one or more personal aspects\nto a potential malicious interceptor and to the application provider. As a\nfirst solution to this concept, we propose to use an adversarial autoencoding\nmethod that disentangles in the voice representation a given speaker attribute\nthus allowing its concealment. We focus here on the sex attribute for an\nAutomatic Speaker Verification (ASV) task. Experiments carried out using the\nVoxCeleb datasets have shown that the proposed method enables the concealment\nof this attribute while preserving ASV ability.", "published": "2020-12-08 14:47:23", "link": "http://arxiv.org/abs/2012.04454v3", "categories": ["eess.AS", "cs.AI", "cs.CR"], "primary_category": "eess.AS"}
{"title": "A Geometric Framework for Pitch Estimation on Acoustic Musical Signals", "abstract": "This paper presents a geometric approach to pitch estimation (PE)-an\nimportant problem in Music Information Retrieval (MIR), and a precursor to a\nvariety of other problems in the field. Though there exist a number of\nhighly-accurate methods, both mono-pitch estimation and multi-pitch estimation\n(particularly with unspecified polyphonic timbre) prove computationally and\nconceptually challenging. A number of current techniques, whilst incredibly\neffective, are not targeted towards eliciting the underlying mathematical\nstructures that underpin the complex musical patterns exhibited by acoustic\nmusical signals. Tackling the approach from both a theoretical and experimental\nperspective, we present a novel framework, a basis for further work in the\narea, and results that (whilst not state of the art) demonstrate relative\nefficacy. The framework presented in this paper opens up a completely new way\nto tackle PE problems, and may have uses both in traditional analytical\napproaches, as well as in the emerging machine learning (ML) methods that\ncurrently dominate the literature.", "published": "2020-12-08 16:06:58", "link": "http://arxiv.org/abs/2012.04517v1", "categories": ["cs.SD", "cs.CG", "eess.AS", "68R01"], "primary_category": "cs.SD"}
{"title": "I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at\n  Pitch", "abstract": "Growing research demonstrates that synthetic failure modes imply poor\ngeneralization. We compare commonly used audio-to-audio losses on a synthetic\nbenchmark, measuring the pitch distance between two stationary sinusoids. The\nresults are surprising: many have poor sense of pitch direction. These\nshortcomings are exposed using simple rank assumptions. Our task is trivial for\nhumans but difficult for these audio distances, suggesting significant progress\ncan be made in self-supervised audio learning by improving current losses.", "published": "2020-12-08 17:17:28", "link": "http://arxiv.org/abs/2012.04572v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Recent Advances in Computer Audition for Diagnosing COVID-19: An\n  Overview", "abstract": "Computer audition (CA) has been demonstrated to be efficient in healthcare\ndomains for speech-affecting disorders (e.g., autism spectrum, depression, or\nParkinson's disease) and body sound-affecting abnormalities (e. g., abnormal\nbowel sounds, heart murmurs, or snore sounds). Nevertheless, CA has been\nunderestimated in the considered data-driven technologies for fighting the\nCOVID-19 pandemic caused by the SARS-CoV-2 coronavirus. In this light,\nsummarise the most recent advances in CA for COVID-19 speech and/or sound\nanalysis. While the milestones achieved are encouraging, there are yet not any\nsolid conclusions that can be made. This comes mostly, as data is still sparse,\noften not sufficiently validated and lacking in systematic comparison with\nrelated diseases that affect the respiratory system. In particular, CA-based\nmethods cannot be a standalone screening tool for SARS-CoV-2. We hope this\nbrief overview can provide a good guidance and attract more attention from a\nbroader artificial intelligence community.", "published": "2020-12-08 21:39:01", "link": "http://arxiv.org/abs/2012.04650v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68-02", "A.1"], "primary_category": "cs.SD"}
