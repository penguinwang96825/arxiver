{"title": "Unifying Token and Span Level Supervisions for Few-Shot Sequence\n  Labeling", "abstract": "Few-shot sequence labeling aims to identify novel classes based on only a few\nlabeled samples. Existing methods solve the data scarcity problem mainly by\ndesigning token-level or span-level labeling models based on metric learning.\nHowever, these methods are only trained at a single granularity (i.e., either\ntoken level or span level) and have some weaknesses of the corresponding\ngranularity. In this paper, we first unify token and span level supervisions\nand propose a Consistent Dual Adaptive Prototypical (CDAP) network for few-shot\nsequence labeling. CDAP contains the token-level and span-level networks,\njointly trained at different granularities. To align the outputs of two\nnetworks, we further propose a consistent loss to enable them to learn from\neach other. During the inference phase, we propose a consistent greedy\ninference algorithm that first adjusts the predicted probability and then\ngreedily selects non-overlapping spans with maximum probability. Extensive\nexperiments show that our model achieves new state-of-the-art results on three\nbenchmark datasets.", "published": "2023-07-16 04:50:52", "link": "http://arxiv.org/abs/2307.07946v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facilitating Multi-turn Emotional Support Conversation with Positive\n  Emotion Elicitation: A Reinforcement Learning Approach", "abstract": "Emotional support conversation (ESC) aims to provide emotional support (ES)\nto improve one's mental state. Existing works stay at fitting grounded\nresponses and responding strategies (e.g., question), which ignore the effect\non ES and lack explicit goals to guide emotional positive transition. To this\nend, we introduce a new paradigm to formalize multi-turn ESC as a process of\npositive emotion elicitation. Addressing this task requires finely adjusting\nthe elicitation intensity in ES as the conversation progresses while\nmaintaining conversational goals like coherence. In this paper, we propose\nSupporter, a mixture-of-expert-based reinforcement learning model, and well\ndesign ES and dialogue coherence rewards to guide policy's learning for\nresponding. Experiments verify the superiority of Supporter in achieving\npositive emotion elicitation during responding while maintaining conversational\ngoals including coherence.", "published": "2023-07-16 09:58:44", "link": "http://arxiv.org/abs/2307.07994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Identification of Alzheimer's Disease using Lexical Features\n  extracted from Language Samples", "abstract": "Objective: this study has a twofold goal. First, it aims to improve the\nunderstanding of the impact of Dementia of type Alzheimer's Disease (AD) on\ndifferent aspects of the lexicon. Second, it aims to demonstrate that such\naspects of the lexicon, when used as features of a machine learning classifier,\ncan help achieve state-of-the-art performance in automatically identifying\nlanguage samples produced by patients with AD. Methods: data is derived from\nthe ADDreSS challenge, which is a part of the DementiaBank corpus. The used\ndataset consists of transcripts of Cookie Theft picture descriptions, produced\nby 54 subjects in the training part and 24 subjects in the test part. The\nnumber of narrative samples is 108 in the training set and 48 in the test set.\nFirst, the impact of AD on 99 selected lexical features is studied using both\nthe training and testing parts of the dataset. Then some machine learning\nexperiments were conducted on the task of classifying transcribed speech\nsamples with text samples that were produced by people with AD from those\nproduced by normal subjects. Several experiments were conducted to compare the\ndifferent areas of lexical complexity, identify the subset of features that\nhelp achieve optimal performance, and study the impact of the size of the input\non the classification. To evaluate the generalization of the models built on\nnarrative speech, two generalization tests were conducted using written data\nfrom two British authors, Iris Murdoch and Agatha Christie, and the\ntranscription of some speeches by former President Ronald Reagan. Results:\nusing lexical features only, state-of-the-art classification, F1 and\naccuracies, of over 91% were achieved in categorizing language samples produced\nby individuals with AD from the ones produced by healthy control subjects. This\nconfirms the substantial impact of AD on lexicon processing.", "published": "2023-07-16 15:05:05", "link": "http://arxiv.org/abs/2307.08070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It's All Relative: Interpretable Models for Scoring Bias in Documents", "abstract": "We propose an interpretable model to score the bias present in web documents,\nbased only on their textual content. Our model incorporates assumptions\nreminiscent of the Bradley-Terry axioms and is trained on pairs of revisions of\nthe same Wikipedia article, where one version is more biased than the other.\nWhile prior approaches based on absolute bias classification have struggled to\nobtain a high accuracy for the task, we are able to develop a useful model for\nscoring bias by learning to perform pairwise comparisons of bias accurately. We\nshow that we can interpret the parameters of the trained model to discover the\nwords most indicative of bias. We also apply our model in three different\nsettings - studying the temporal evolution of bias in Wikipedia articles,\ncomparing news sources based on bias, and scoring bias in law amendments. In\neach case, we demonstrate that the outputs of the model can be explained and\nvalidated, even for the two domains that are outside the training-data domain.\nWe also use the model to compare the general level of bias between domains,\nwhere we see that legal texts are the least biased and news media are the most\nbiased, with Wikipedia articles in between. Given its high performance,\nsimplicity, interpretability, and wide applicability, we hope the model will be\nuseful for a large community, including Wikipedia and news editors, political\nand social scientists, and the general public.", "published": "2023-07-16 19:35:38", "link": "http://arxiv.org/abs/2307.08139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Potential and Pitfalls of using a Large Language Model such as\n  ChatGPT or GPT-4 as a Clinical Assistant", "abstract": "Recent studies have demonstrated promising performance of ChatGPT and GPT-4\non several medical domain tasks. However, none have assessed its performance\nusing a large-scale real-world electronic health record database, nor have\nevaluated its utility in providing clinical diagnostic assistance for patients\nacross a full range of disease presentation. We performed two analyses using\nChatGPT and GPT-4, one to identify patients with specific medical diagnoses\nusing a real-world large electronic health record database and the other, in\nproviding diagnostic assistance to healthcare workers in the prospective\nevaluation of hypothetical patients. Our results show that GPT-4 across disease\nclassification tasks with chain of thought and few-shot prompting can achieve\nperformance as high as 96% F1 scores. For patient assessment, GPT-4 can\naccurately diagnose three out of four times. However, there were mentions of\nfactually incorrect statements, overlooking crucial medical findings,\nrecommendations for unnecessary investigations and overtreatment. These issues\ncoupled with privacy concerns, make these models currently inadequate for real\nworld clinical use. However, limited data and time needed for prompt\nengineering in comparison to configuration of conventional machine learning\nworkflows highlight their potential for scalability across healthcare\napplications.", "published": "2023-07-16 21:19:47", "link": "http://arxiv.org/abs/2307.08152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Dataset Annotation Quality Management in the Wild", "abstract": "Data quality is crucial for training accurate, unbiased, and trustworthy\nmachine learning models as well as for their correct evaluation. Recent works,\nhowever, have shown that even popular datasets used to train and evaluate\nstate-of-the-art models contain a non-negligible amount of erroneous\nannotations, biases, or artifacts. While practices and guidelines regarding\ndataset creation projects exist, to our knowledge, large-scale analysis has yet\nto be performed on how quality management is conducted when creating natural\nlanguage datasets and whether these recommendations are followed. Therefore, we\nfirst survey and summarize recommended quality management practices for dataset\ncreation as described in the literature and provide suggestions for applying\nthem. Then, we compile a corpus of 591 scientific publications introducing text\ndatasets and annotate it for quality-related aspects, such as annotator\nmanagement, agreement, adjudication, or data validation. Using these\nannotations, we then analyze how quality management is conducted in practice. A\nmajority of the annotated publications apply good or excellent quality\nmanagement. However, we deem the effort of 30\\% of the works as only subpar.\nOur analysis also shows common errors, especially when using inter-annotator\nagreement and computing annotation error rates.", "published": "2023-07-16 21:22:40", "link": "http://arxiv.org/abs/2307.08153v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual NER for Financial Transaction Data in Low-Resource\n  Languages", "abstract": "We propose an efficient modeling framework for cross-lingual named entity\nrecognition in semi-structured text data. Our approach relies on both knowledge\ndistillation and consistency training. The modeling framework leverages\nknowledge from a large language model (XLMRoBERTa) pre-trained on the source\nlanguage, with a student-teacher relationship (knowledge distillation). The\nstudent model incorporates unsupervised consistency training (with KL\ndivergence loss) on the low-resource target language.\n  We employ two independent datasets of SMSs in English and Arabic, each\ncarrying semi-structured banking transaction information, and focus on\nexhibiting the transfer of knowledge from English to Arabic. With access to\nonly 30 labeled samples, our model can generalize the recognition of merchants,\namounts, and other fields from English to Arabic. We show that our modeling\napproach, while efficient, performs best overall when compared to\nstate-of-the-art approaches like DistilBERT pre-trained on the target language\nor a supervised model directly trained on labeled data in the target language.\n  Our experiments show that it is enough to learn to recognize entities in\nEnglish to reach reasonable performance in a low-resource language in the\npresence of a few labeled samples of semi-structured data. The proposed\nframework has implications for developing multi-lingual applications,\nespecially in geographies where digital endeavors rely on both English and one\nor more low-resource language(s), sometimes mixed with English or employed\nsingly.", "published": "2023-07-16 00:45:42", "link": "http://arxiv.org/abs/2307.08714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeoGPT: Understanding and Processing Geospatial Tasks through An\n  Autonomous GPT", "abstract": "Decision-makers in GIS need to combine a series of spatial algorithms and\noperations to solve geospatial tasks. For example, in the task of facility\nsiting, the Buffer tool is usually first used to locate areas close or away\nfrom some specific entities; then, the Intersect or Erase tool is used to\nselect candidate areas satisfied multiple requirements. Though professionals\ncan easily understand and solve these geospatial tasks by sequentially\nutilizing relevant tools, it is difficult for non-professionals to handle these\nproblems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents\nstrong performance in semantic understanding and reasoning. Especially, AutoGPT\ncan further extend the capabilities of large language models (LLMs) by\nautomatically reasoning and calling externally defined tools. Inspired by these\nstudies, we attempt to lower the threshold of non-professional users to solve\ngeospatial tasks by integrating the semantic understanding ability inherent in\nLLMs with mature tools within the GIS community. Specifically, we develop a new\nframework called GeoGPT that can conduct geospatial data collection,\nprocessing, and analysis in an autonomous manner with the instruction of only\nnatural language. In other words, GeoGPT is used to understand the demands of\nnon-professional users merely based on input natural language descriptions, and\nthen think, plan, and execute defined GIS tools to output final effective\nresults. Several cases including geospatial data crawling, spatial query,\nfacility siting, and mapping validate the effectiveness of our framework.\nThough limited cases are presented in this paper, GeoGPT can be further\nextended to various tasks by equipping with more GIS tools, and we think the\nparadigm of \"foundational plus professional\" implied in GeoGPT provides an\neffective way to develop next-generation GIS in this era of large foundation\nmodels.", "published": "2023-07-16 03:03:59", "link": "http://arxiv.org/abs/2307.07930v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Model Adaptation for ASR in low-resource Indian Languages", "abstract": "Automatic speech recognition (ASR) performance has improved drastically in\nrecent years, mainly enabled by self-supervised learning (SSL) based acoustic\nmodels such as wav2vec2 and large-scale multi-lingual training like Whisper. A\nhuge challenge still exists for low-resource languages where the availability\nof both audio and text is limited. This is further complicated by the presence\nof multiple dialects like in Indian languages. However, many Indian languages\ncan be grouped into the same families and share the same script and grammatical\nstructure. This is where a lot of adaptation and fine-tuning techniques can be\napplied to overcome the low-resource nature of the data by utilising\nwell-resourced similar languages.\n  In such scenarios, it is important to understand the extent to which each\nmodality, like acoustics and text, is important in building a reliable ASR. It\ncould be the case that an abundance of acoustic data in a language reduces the\nneed for large text-only corpora. Or, due to the availability of various\npretrained acoustic models, the vice-versa could also be true. In this proposed\nspecial session, we encourage the community to explore these ideas with the\ndata in two low-resource Indian languages of Bengali and Bhojpuri. These\napproaches are not limited to Indian languages, the solutions are potentially\napplicable to various languages spoken around the world.", "published": "2023-07-16 05:25:51", "link": "http://arxiv.org/abs/2307.07948v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "MinT: Boosting Generalization in Mathematical Reasoning via Multi-View\n  Fine-Tuning", "abstract": "Reasoning in mathematical domains remains a significant challenge for\nrelatively small language models (LMs). Many current methods focus on\nspecializing LMs in mathematical reasoning and rely heavily on knowledge\ndistillation from powerful but inefficient large LMs (LLMs). In this work, we\nexplore a new direction that avoids over-reliance on LLM teachers, introducing\na multi-view fine-tuning method that efficiently exploits existing mathematical\nproblem datasets with diverse annotation styles. Our approach uniquely\nconsiders the various annotation formats as different \"views\" and leverages\nthem in training the model. By postpending distinct instructions to input\nquestions, models can learn to generate solutions in diverse formats in a\nflexible manner. Experimental results show that our strategy enables a LLaMA-7B\nmodel to outperform prior approaches that utilize knowledge distillation, as\nwell as carefully established baselines. Additionally, the proposed method\ngrants the models promising generalization ability across various views and\ndatasets, and the capability to learn from inaccurate or incomplete noisy data.\nWe hope our multi-view training paradigm could inspire future studies in other\nmachine reasoning domains.", "published": "2023-07-16 05:41:53", "link": "http://arxiv.org/abs/2307.07951v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Neural-Symbolic Approach Towards Identifying Grammatically Correct\n  Sentences", "abstract": "Textual content around us is growing on a daily basis. Numerous articles are\nbeing written as we speak on online newspapers, blogs, or social media.\nSimilarly, recent advances in the AI field, like language models or traditional\nclassic AI approaches, are utilizing all the above to improve their learned\nrepresentation to tackle NLP challenges with human-like accuracy. It is\ncommonly accepted that it is crucial to have access to well-written text from\nvalid sources to tackle challenges like text summarization, question-answering,\nmachine translation, or even pronoun resolution. For instance, to summarize\nwell, one needs to select the most important sentences in order to concatenate\nthem to form the summary. However, what happens if we do not have access to\nwell-formed English sentences or even non-valid sentences? Despite the\nimportance of having access to well-written sentences, figuring out ways to\nvalidate them is still an open area of research. To address this problem, we\npresent a simplified way to validate English sentences through a novel\nneural-symbolic approach. Lately, neural-symbolic approaches have triggered an\nincreasing interest towards tackling various NLP challenges, as they are\ndemonstrating their effectiveness as a central component in various AI systems.\nThrough combining Classic with Modern AI, which involves the blending of\ngrammatical and syntactical rules with language models, we effectively tackle\nthe Corpus of Linguistic Acceptability (COLA), a task that shows whether or not\na sequence of words is an English grammatical sentence. Among others,\nundertaken experiments effectively show that blending symbolic and non-symbolic\nsystems helps the former provide insights about the latter's accuracy results.", "published": "2023-07-16 13:21:44", "link": "http://arxiv.org/abs/2307.08036v1", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.3; I.2.7; I.5.1"], "primary_category": "cs.CL"}
{"title": "Do Emergent Abilities Exist in Quantized Large Language Models: An\n  Empirical Study", "abstract": "Despite the superior performance, Large Language Models~(LLMs) require\nsignificant computational resources for deployment and use. To overcome this\nissue, quantization methods have been widely applied to reduce the memory\nfootprint of LLMs as well as increasing the inference rate. However, a major\nchallenge is that low-bit quantization methods often lead to performance\ndegradation. It is important to understand how quantization impacts the\ncapacity of LLMs. Different from previous studies focused on overall\nperformance, this work aims to investigate the impact of quantization on\n\\emph{emergent abilities}, which are important characteristics that distinguish\nLLMs from small language models. Specially, we examine the abilities of\nin-context learning, chain-of-thought reasoning, and instruction-following in\nquantized LLMs. Our empirical experiments show that these emergent abilities\nstill exist in 4-bit quantization models, while 2-bit models encounter severe\nperformance degradation on the test of these abilities. To improve the\nperformance of low-bit models, we conduct two special experiments: (1)\nfine-gained impact analysis that studies which components (or substructures)\nare more sensitive to quantization, and (2) performance compensation through\nmodel fine-tuning. Our work derives a series of important findings to\nunderstand the impact of quantization on emergent abilities, and sheds lights\non the possibilities of extremely low-bit quantization for LLMs.", "published": "2023-07-16 15:11:01", "link": "http://arxiv.org/abs/2307.08072v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language\n  Modelling", "abstract": "Modeling discourse -- the linguistic phenomena that go beyond individual\nsentences, is a fundamental yet challenging aspect of natural language\nprocessing (NLP). However, existing evaluation benchmarks primarily focus on\nthe evaluation of inter-sentence properties and overlook critical discourse\nphenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a\nbenchmark that can evaluate intra-sentence discourse properties across a\ndiverse set of NLP tasks, covering understanding, translation, and generation.\nDisco-Bench consists of 9 document-level testsets in the literature domain,\nwhich contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese\nand/or English. For linguistic analysis, we also design a diagnostic test suite\nthat can examine whether the target models learn discourse knowledge. We\ntotally evaluate 20 general-, in-domain and commercial models based on\nTransformer, advanced pretraining architectures and large language models\n(LLMs). Our results show (1) the challenge and necessity of our evaluation\nbenchmark; (2) fine-grained pretraining based on literary document-level\ntraining data consistently improves the modeling of discourse information. We\nwill release the datasets, pretrained models, and leaderboard, which we hope\ncan significantly facilitate research in this field:\nhttps://github.com/longyuewangdcu/Disco-Bench.", "published": "2023-07-16 15:18:25", "link": "http://arxiv.org/abs/2307.08074v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recognition of Mental Adjectives in An Efficient and Automatic Style", "abstract": "In recent years, commonsense reasoning has received more and more attention\nfrom academic community. We propose a new lexical inference task, Mental and\nPhysical Classification (MPC), to handle commonsense reasoning in a reasoning\ngraph. Mental words relate to mental activities, which fall into six\ncategories: Emotion, Need, Perceiving, Reasoning, Planning and Personality.\nPhysical words describe physical attributes of an object, like color, hardness,\nspeed and malleability. A BERT model is fine-tuned for this task and active\nlearning algorithm is adopted in the training framework to reduce the required\nannotation resources. The model using ENTROPY strategy achieves satisfactory\naccuracy and requires only about 300 labeled words. We also compare our result\nwith SentiWordNet to check the difference between MPC and subjectivity\nclassification task in sentiment analysis.", "published": "2023-07-16 01:27:08", "link": "http://arxiv.org/abs/2307.11767v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatDev: Communicative Agents for Software Development", "abstract": "Software development is a complex task that necessitates cooperation among\nmultiple members with diverse skills. Numerous studies used deep learning to\nimprove specific phases in a waterfall model, such as design, coding, and\ntesting. However, the deep learning model in each phase requires unique\ndesigns, leading to technical inconsistencies across various phases, which\nresults in a fragmented and ineffective development process. In this paper, we\nintroduce ChatDev, a chat-powered software development framework in which\nspecialized agents driven by large language models (LLMs) are guided in what to\ncommunicate (via chat chain) and how to communicate (via communicative\ndehallucination). These agents actively contribute to the design, coding, and\ntesting phases through unified language-based communication, with solutions\nderived from their multi-turn dialogues. We found their utilization of natural\nlanguage is advantageous for system design, and communicating in programming\nlanguage proves helpful in debugging. This paradigm demonstrates how linguistic\ncommunication facilitates multi-agent collaboration, establishing language as a\nunifying bridge for autonomous task-solving among LLM agents. The code and data\nare available at https://github.com/OpenBMB/ChatDev.", "published": "2023-07-16 02:11:34", "link": "http://arxiv.org/abs/2307.07924v5", "categories": ["cs.SE", "cs.CL", "cs.MA"], "primary_category": "cs.SE"}
{"title": "Deduplicating and Ranking Solution Programs for Suggesting Reference\n  Solutions", "abstract": "Referring to solution programs written by other users is helpful for learners\nin programming education. However, current online judge systems just list all\nsolution programs submitted by users for references, and the programs are\nsorted based on the submission date and time, execution time, or user rating,\nignoring to what extent the programs can be helpful to be referenced. In\naddition, users struggle to refer to a variety of solution approaches since\nthere are too many duplicated and near-duplicated programs. To motivate\nlearners to refer to various solutions to learn better solution approaches, in\nthis paper, we propose an approach to deduplicate and rank common solution\nprograms in each programming problem. Inspired by the nature that the\nmany-duplicated program adopts a more common approach and can be a general\nreference, we remove the near-duplicated solution programs and rank the unique\nprograms based on the duplicate count. The experiments on the solution programs\nsubmitted to a real-world online judge system demonstrate that the number of\nprograms is reduced by 60.20%, whereas the baseline only reduces by 29.59%\nafter the deduplication, meaning that users only need to refer to 39.80% of\nprograms on average. Furthermore, our analysis shows that top-10 ranked\nprograms cover 29.95% of programs on average, indicating that users can grasp\n29.95% of solution approaches by referring to only 10 programs. The proposed\napproach shows the potential of reducing the learners' burden of referring to\ntoo many solutions and motivating them to learn a variety of solution\napproaches.", "published": "2023-07-16 04:20:26", "link": "http://arxiv.org/abs/2307.07940v2", "categories": ["cs.SE", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "A Survey of Techniques for Optimizing Transformer Inference", "abstract": "Recent years have seen a phenomenal rise in performance and applications of\ntransformer neural networks. The family of transformer networks, including\nBidirectional Encoder Representations from Transformer (BERT), Generative\nPretrained Transformer (GPT) and Vision Transformer (ViT), have shown their\neffectiveness across Natural Language Processing (NLP) and Computer Vision (CV)\ndomains. Transformer-based networks such as ChatGPT have impacted the lives of\ncommon men. However, the quest for high predictive performance has led to an\nexponential increase in transformers' memory and compute footprint. Researchers\nhave proposed techniques to optimize transformer inference at all levels of\nabstraction. This paper presents a comprehensive survey of techniques for\noptimizing the inference phase of transformer networks. We survey techniques\nsuch as knowledge distillation, pruning, quantization, neural architecture\nsearch and lightweight network design at the algorithmic level. We further\nreview hardware-level optimization techniques and the design of novel hardware\naccelerators for transformers. We summarize the quantitative results on the\nnumber of parameters/FLOPs and accuracy of several models/techniques to\nshowcase the tradeoff exercised by them. We also outline future directions in\nthis rapidly evolving field of research. We believe that this survey will\neducate both novice and seasoned researchers and also spark a plethora of\nresearch efforts in this field.", "published": "2023-07-16 08:50:50", "link": "http://arxiv.org/abs/2307.07982v1", "categories": ["cs.LG", "cs.AR", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Assessing the Quality of Multiple-Choice Questions Using GPT-4 and\n  Rule-Based Methods", "abstract": "Multiple-choice questions with item-writing flaws can negatively impact\nstudent learning and skew analytics. These flaws are often present in\nstudent-generated questions, making it difficult to assess their quality and\nsuitability for classroom usage. Existing methods for evaluating\nmultiple-choice questions often focus on machine readability metrics, without\nconsidering their intended use within course materials and their pedagogical\nimplications. In this study, we compared the performance of a rule-based method\nwe developed to a machine-learning based method utilizing GPT-4 for the task of\nautomatically assessing multiple-choice questions based on 19 common\nitem-writing flaws. By analyzing 200 student-generated questions from four\ndifferent subject areas, we found that the rule-based method correctly detected\n91% of the flaws identified by human annotators, as compared to 79% by GPT-4.\nWe demonstrated the effectiveness of the two methods in identifying common\nitem-writing flaws present in the student-generated questions across different\nsubject areas. The rule-based method can accurately and efficiently evaluate\nmultiple-choice questions from multiple domains, outperforming GPT-4 and going\nbeyond existing metrics that do not account for the educational use of such\nquestions. Finally, we discuss the potential for using these automated methods\nto improve the quality of questions based on the identified flaws.", "published": "2023-07-16 22:12:10", "link": "http://arxiv.org/abs/2307.08161v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its\n  Departure from Current Machine Learning", "abstract": "This study presents a thorough examination of various Generative Pretrained\nTransformer (GPT) methodologies in sentiment analysis, specifically in the\ncontext of Task 4 on the SemEval 2017 dataset. Three primary strategies are\nemployed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)\nfine-tuning GPT models, and 3) an inventive approach to embedding\nclassification. The research yields detailed comparative insights among these\nstrategies and individual GPT models, revealing their unique strengths and\npotential limitations. Additionally, the study compares these GPT-based\nmethodologies with other current, high-performing models previously used with\nthe same dataset. The results illustrate the significant superiority of the GPT\napproaches in terms of predictive performance, more than 22\\% in F1-score\ncompared to the state-of-the-art. Further, the paper sheds light on common\nchallenges in sentiment analysis tasks, such as understanding context and\ndetecting sarcasm. It underscores the enhanced capabilities of the GPT models\nto effectively handle these complexities. Taken together, these findings\nhighlight the promising potential of GPT models in sentiment analysis, setting\nthe stage for future research in this field. The code can be found at\nhttps://github.com/DSAatUSU/SentimentGPT", "published": "2023-07-16 05:33:35", "link": "http://arxiv.org/abs/2307.10234v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement\n  for Large Language Models", "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new\nopportunities across numerous industrial applications and domains. However,\nerroneous generations, such as false predictions, misinformation, and\nhallucination made by LLMs, have also raised severe concerns for the\ntrustworthiness of LLMs', especially in safety-, security- and\nreliability-sensitive scenarios, potentially hindering real-world adoptions.\nWhile uncertainty estimation has shown its potential for interpreting the\nprediction risks made by general machine learning (ML) models, little is known\nabout whether and to what extent it can help explore an LLM's capabilities and\ncounteract its undesired behavior. To bridge the gap, in this paper, we\ninitiate an exploratory study on the risk assessment of LLMs from the lens of\nuncertainty. In particular, we experiment with twelve uncertainty estimation\nmethods and four LLMs on four prominent natural language processing (NLP) tasks\nto investigate to what extent uncertainty estimation techniques could help\ncharacterize the prediction risks of LLMs. Our findings validate the\neffectiveness of uncertainty estimation for revealing LLMs'\nuncertain/non-factual predictions. In addition to general NLP tasks, we\nextensively conduct experiments with four LLMs for code generation on two\ndatasets. We find that uncertainty estimation can potentially uncover buggy\nprograms generated by LLMs. Insights from our study shed light on future design\nand development for reliable LLMs, facilitating further research toward\nenhancing the trustworthiness of LLMs.", "published": "2023-07-16 08:28:04", "link": "http://arxiv.org/abs/2307.10236v4", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "NoiseBandNet: Controllable Time-Varying Neural Synthesis of Sound\n  Effects Using Filterbanks", "abstract": "Controllable neural audio synthesis of sound effects is a challenging task\ndue to the potential scarcity and spectro-temporal variance of the data.\nDifferentiable digital signal processing (DDSP) synthesisers have been\nsuccessfully employed to model and control musical and harmonic signals using\nrelatively limited data and computational resources. Here we propose\nNoiseBandNet, an architecture capable of synthesising and controlling sound\neffects by filtering white noise through a filterbank, thus going further than\nprevious systems that make assumptions about the harmonic nature of sounds. We\nevaluate our approach via a series of experiments, modelling footsteps,\nthunderstorm, pottery, knocking, and metal sound effects. Comparing\nNoiseBandNet audio reconstruction capabilities to four variants of the\nDDSP-filtered noise synthesiser, NoiseBandNet scores higher in nine out of ten\nevaluation categories, establishing a flexible DDSP method for generating\ntime-varying, inharmonic sound effects of arbitrary length with both good time\nand frequency resolution. Finally, we introduce some potential creative uses of\nNoiseBandNet, by generating variations, performing loudness transfer, and by\ntraining it on user-defined control curves.", "published": "2023-07-16 11:21:27", "link": "http://arxiv.org/abs/2307.08007v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise-aware Speech Enhancement using Diffusion Probabilistic Model", "abstract": "With recent advances of diffusion model, generative speech enhancement (SE)\nhas attracted a surge of research interest due to its great potential for\nunseen testing noises. However, existing efforts mainly focus on inherent\nproperties of clean speech, underexploiting the varying noise information in\nreal world. In this paper, we propose a noise-aware speech enhancement (NASE)\napproach that extracts noise-specific information to guide the reverse process\nin diffusion model. Specifically, we design a noise classification (NC) model\nto produce acoustic embedding as a noise conditioner to guide the reverse\ndenoising process. Meanwhile, a multi-task learning scheme is devised to\njointly optimize SE and NC tasks to enhance the noise specificity of\nconditioner. NASE is shown to be a plug-and-play module that can be generalized\nto any diffusion SE models. Experiments on VB-DEMAND dataset show that NASE\neffectively improves multiple mainstream diffusion SE models, especially on\nunseen noises.", "published": "2023-07-16 12:46:11", "link": "http://arxiv.org/abs/2307.08029v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
