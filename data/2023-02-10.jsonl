{"title": "Language-Aware Multilingual Machine Translation with Self-Supervised\n  Learning", "abstract": "Multilingual machine translation (MMT) benefits from cross-lingual transfer\nbut is a challenging multitask optimization problem. This is partly because\nthere is no clear framework to systematically learn language-specific\nparameters. Self-supervised learning (SSL) approaches that leverage large\nquantities of monolingual data (where parallel data is unavailable) have shown\npromise by improving translation performance as complementary tasks to the MMT\ntask. However, jointly optimizing SSL and MMT tasks is even more challenging.\nIn this work, we first investigate how to utilize intra-distillation to learn\nmore *language-specific* parameters and then show the importance of these\nlanguage-specific parameters. Next, we propose a novel but simple SSL task,\nconcurrent denoising, that co-trains with the MMT task by concurrently\ndenoising monolingual data on both the encoder and decoder. Finally, we apply\nintra-distillation to this co-training approach. Combining these two approaches\nsignificantly improves MMT performance, outperforming three state-of-the-art\nSSL methods by a large margin, e.g., 11.3\\% and 3.7\\% improvement on an\n8-language and a 15-language benchmark compared with MASS, respectively", "published": "2023-02-10 01:34:24", "link": "http://arxiv.org/abs/2302.05008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ControversialQA: Exploring Controversy in Question Answering", "abstract": "Controversy is widespread online. Previous studies mainly define controversy\nbased on vague assumptions of its relation to sentiment such as hate speech and\noffensive words. This paper introduces the first question-answering dataset\nthat defines content controversy by user perception, i.e., votes from plenty of\nusers. It contains nearly 10K questions, and each question has a best answer\nand a most controversial answer. Experimental results reveal that controversy\ndetection in question answering is essential and challenging, and there is no\nstrong correlation between controversy and sentiment tasks.", "published": "2023-02-10 05:39:29", "link": "http://arxiv.org/abs/2302.05061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial\n  Text Attacks", "abstract": "We propose a novel gradient-based attack against transformer-based language\nmodels that searches for an adversarial example in a continuous space of token\nprobabilities. Our algorithm mitigates the gap between adversarial loss for\ncontinuous and discrete text representations by performing multi-step\nquantization in a quantization-compensation loop. Experiments show that our\nmethod significantly outperforms other approaches on various natural language\nprocessing (NLP) tasks.", "published": "2023-02-10 08:50:51", "link": "http://arxiv.org/abs/2302.05120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plan-then-Seam: Towards Efficient Table-to-Text Generation", "abstract": "Table-to-text generation aims at automatically generating text to help people\nconveniently obtain salient information in tables. Recent works explicitly\ndecompose the generation process into content planning and surface generation\nstages, employing two autoregressive networks for them respectively. However,\nthey are computationally expensive due to the non-parallelizable nature of\nautoregressive decoding and the redundant parameters of two networks. In this\npaper, we propose the first totally non-autoregressive table-to-text model\n(Plan-then-Seam, PTS) that produces its outputs in parallel with one single\nnetwork. PTS firstly writes and calibrates one plan of the content to be\ngenerated with a novel rethinking pointer predictor, and then takes the plan as\nthe context for seaming to decode the description. These two steps share\nparameters and perform iteratively to capture token inter-dependency while\nkeeping parallel decoding. Experiments on two public benchmarks show that PTS\nachieves 3.0~5.6 times speedup for inference time, reducing 50% parameters,\nwhile maintaining as least comparable performance against strong two-stage\ntable-to-text competitors.", "published": "2023-02-10 09:43:15", "link": "http://arxiv.org/abs/2302.05138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Transformer Language Models for Contextual Commonsense\n  Inference", "abstract": "Contextualized or discourse aware commonsense inference is the task of\ngenerating coherent commonsense assertions (i.e., facts) from a given story,\nand a particular sentence from that story. Some problems with the task are:\nlack of controllability for topics of the inferred facts; lack of commonsense\nknowledge during training; and, possibly, hallucinated or false facts. In this\nwork, we utilize a transformer model for this task and develop techniques to\naddress the aforementioned problems in the task. We control the inference by\nintroducing a new technique we call \"hinting\". Hinting is a kind of language\nmodel prompting, that utilizes both hard prompts (specific words) and soft\nprompts (virtual learnable templates). This serves as a control signal to\nadvise the language model \"what to talk about\". Next, we establish a\nmethodology for performing joint inference with multiple commonsense knowledge\nbases. Joint inference of commonsense requires care, because it is imprecise\nand the level of generality is more flexible. You want to be sure that the\nresults \"still make sense\" for the context. To this end, we align the textual\nversion of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and\nGLUCOSE) with a story and a target sentence. This combination allows us to\ntrain a single model to perform joint inference with multiple knowledge graphs.\nWe show experimental results for the three knowledge graphs on joint inference.\nOur final contribution is exploring a GAN architecture that generates the\ncontextualized commonsense assertions and scores them as to their plausibility\nthrough a discriminator. The result is an integrated system for contextual\ncommonsense inference in stories, that can controllably generate plausible\ncommonsense assertions, and takes advantage of joint inference between multiple\ncommonsense knowledge bases.", "published": "2023-02-10 18:21:13", "link": "http://arxiv.org/abs/2302.05406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "POSGen: Personalized Opening Sentence Generation for Online Insurance\n  Sales", "abstract": "The insurance industry is shifting their sales mode from offline to online,\nin expectation to reach massive potential customers in the digitization era.\nDue to the complexity and the nature of insurance products, a cost-effective\nonline sales solution is to exploit chatbot AI to raise customers' attention\nand pass those with interests to human agents for further sales. For high\nresponse and conversion rates of customers, it is crucial for the chatbot to\ninitiate a conversation with personalized opening sentences, which are\ngenerated with user-specific topic selection and ordering. Such personalized\nopening sentence generation is challenging because (i) there are limited\nhistorical samples for conversation topic recommendation in online insurance\nsales and (ii) existing text generation schemes often fail to support\ncustomized topic ordering based on user preferences. We design POSGen, a\npersonalized opening sentence generation scheme dedicated for online insurance\nsales. It transfers user embeddings learned from auxiliary online user\nbehaviours to enhance conversation topic recommendation, and exploits a context\nmanagement unit to arrange the recommended topics in user-specific ordering for\nopening sentence generation. POSGen is deployed on a real-world online\ninsurance platform. It achieves 2.33x total insurance premium improvement\nthrough a two-month global test.", "published": "2023-02-10 01:40:03", "link": "http://arxiv.org/abs/2302.06470v1", "categories": ["cs.CL", "Online Insurance Recommendation, Transfer Learning, Data-to-text\n  Generation"], "primary_category": "cs.CL"}
{"title": "Event Temporal Relation Extraction with Bayesian Translational Model", "abstract": "Existing models to extract temporal relations between events lack a\nprincipled method to incorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-based method that models the\ntemporal relation representations as latent variables and infers their values\nvia Bayesian inference and translational functions. Compared to conventional\nneural approaches, instead of performing point estimation to find the best set\nparameters, the proposed model infers the parameters' posterior distribution\ndirectly, enhancing the model's capability to encode and express uncertainty\nabout the predictions. Experimental results on the three widely used datasets\nshow that Bayesian-Trans outperforms existing approaches for event temporal\nrelation extraction. We additionally present detailed analyses on uncertainty\nquantification, comparison of priors, and ablation studies, illustrating the\nbenefits of the proposed approach.", "published": "2023-02-10 00:11:19", "link": "http://arxiv.org/abs/2302.04985v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Desirable Revisions of Evidence and Reasoning in\n  Argumentative Writing", "abstract": "We develop models to classify desirable evidence and desirable reasoning\nrevisions in student argumentative writing. We explore two ways to improve\nclassifier performance - using the essay context of the revision, and using the\nfeedback students received before the revision. We perform both intrinsic and\nextrinsic evaluation for each of our models and report a qualitative analysis.\nOur results show that while a model using feedback information improves over a\nbaseline model, models utilizing context - either alone or with feedback - are\nthe most successful in identifying desirable revisions.", "published": "2023-02-10 03:59:59", "link": "http://arxiv.org/abs/2302.05039v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Selective In-Context Data Augmentation for Intent Detection using\n  Pointwise V-Information", "abstract": "This work focuses on in-context data augmentation for intent detection.\nHaving found that augmentation via in-context prompting of large pre-trained\nlanguage models (PLMs) alone does not improve performance, we introduce a novel\napproach based on PLMs and pointwise V-information (PVI), a metric that can\nmeasure the usefulness of a datapoint for training a model. Our method first\nfine-tunes a PLM on a small seed of training data and then synthesizes new\ndatapoints - utterances that correspond to given intents. It then employs\nintent-aware filtering, based on PVI, to remove datapoints that are not helpful\nto the downstream intent classifier. Our method is thus able to leverage the\nexpressive power of large language models to produce diverse training data.\nEmpirical results demonstrate that our method can produce synthetic training\ndata that achieve state-of-the-art performance on three challenging intent\ndetection datasets under few-shot settings (1.28% absolute improvement in\n5-shot and 1.18% absolute in 10-shot, on average) and perform on par with the\nstate-of-the-art in full-shot settings (within 0.01% absolute, on average).", "published": "2023-02-10 07:37:49", "link": "http://arxiv.org/abs/2302.05096v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Wisdom of Hindsight Makes Language Models Better Instruction\n  Followers", "abstract": "Reinforcement learning has seen wide success in finetuning large language\nmodels to better align with instructions via human feedback. The so-called\nalgorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates\nimpressive performance on the GPT series models. However, the underlying\nReinforcement Learning (RL) algorithm is complex and requires an additional\ntraining pipeline for reward and value networks. In this paper, we consider an\nalternative approach: converting feedback to instruction by relabeling the\noriginal one and training the model for better alignment in a supervised\nmanner. Such an algorithm doesn't require any additional parameters except for\nthe original language model and maximally reuses the pretraining pipeline. To\nachieve this, we formulate instruction alignment problem for language models as\na goal-reaching problem in decision making. We propose Hindsight Instruction\nRelabeling (HIR), a novel algorithm for aligning language models with\ninstructions. The resulting two-stage algorithm shed light to a family of\nreward-free approaches that utilize the hindsightly relabeled instructions\nbased on feedback. We evaluate the performance of HIR extensively on 12\nchallenging BigBench reasoning tasks and show that HIR outperforms the baseline\nalgorithms and is comparable to or even surpasses supervised finetuning.", "published": "2023-02-10 12:16:38", "link": "http://arxiv.org/abs/2302.05206v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Span-based Named Entity Recognition by Generating and Compressing\n  Information", "abstract": "The information bottleneck (IB) principle has been proven effective in\nvarious NLP applications. The existing work, however, only used either\ngenerative or information compression models to improve the performance of the\ntarget task. In this paper, we propose to combine the two types of IB models\ninto one system to enhance Named Entity Recognition (NER). For one type of IB\nmodel, we incorporate two unsupervised generative components, span\nreconstruction and synonym generation, into a span-based NER system. The span\nreconstruction ensures that the contextualised span representation keeps the\nspan information, while the synonym generation makes synonyms have similar\nrepresentations even in different contexts. For the other type of IB model, we\nadd a supervised IB layer that performs information compression into the system\nto preserve useful features for NER in the resulting span representations.\nExperiments on five different corpora indicate that jointly training both\ngenerative and information compression models can enhance the performance of\nthe baseline span-based NER system. Our source code is publicly available at\nhttps://github.com/nguyennth/joint-ib-models.", "published": "2023-02-10 17:40:51", "link": "http://arxiv.org/abs/2302.05392v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distillation of encoder-decoder transformers for sequence labelling", "abstract": "Driven by encouraging results on a wide range of tasks, the field of NLP is\nexperiencing an accelerated race to develop bigger language models. This race\nfor bigger models has also underscored the need to continue the pursuit of\npractical distillation approaches that can leverage the knowledge acquired by\nthese big models in a compute-efficient manner. Having this goal in mind, we\nbuild on recent work to propose a hallucination-free framework for sequence\ntagging that is especially suited for distillation. We show empirical results\nof new state-of-the-art performance across multiple sequence labelling datasets\nand validate the usefulness of this framework for distilling a large model in a\nfew-shot learning scenario.", "published": "2023-02-10 19:00:00", "link": "http://arxiv.org/abs/2302.05454v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Is Multimodal Vision Supervision Beneficial to Language?", "abstract": "Vision (image and video) - Language (VL) pre-training is the recent popular\nparadigm that achieved state-of-the-art results on multi-modal tasks like\nimage-retrieval, video-retrieval, visual question answering etc. These models\nare trained in an unsupervised way and greatly benefit from the complementary\nmodality supervision. In this paper, we explore if the language representations\ntrained using vision supervision perform better than vanilla language\nrepresentations on Natural Language Understanding and commonsense reasoning\nbenchmarks. We experiment with a diverse set of image-text models such as\nALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT),\nVIOLET. We compare the performance of language representations of stand-alone\ntext encoders of these models to the language representations of text encoders\nlearnt through vision supervision. Our experiments suggest that vanilla\nlanguage representations show superior performance on most of the tasks. These\nresults shed light on the current drawbacks of the vision-language models.", "published": "2023-02-10 02:22:44", "link": "http://arxiv.org/abs/2302.05016v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR\n  Error Correction", "abstract": "Speech-to-text errors made by automatic speech recognition (ASR) systems\nnegatively impact downstream models. Error correction models as a\npost-processing text editing method have been recently developed for refining\nthe ASR outputs. However, efficient models that meet the low latency\nrequirements of industrial grade production systems have not been well studied.\nWe propose PATCorrect-a novel non-autoregressive (NAR) approach based on\nmulti-modal fusion leveraging representations from both text and phoneme\nmodalities, to reduce word error rate (WER) and perform robustly with varying\ninput transcription quality. We demonstrate that PATCorrect consistently\noutperforms state-of-the-art NAR method on English corpus across different\nupstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to\n9.46% WERR achieved by other methods using text only modality. Besides, its\ninference latency is at tens of milliseconds, making it ideal for systems with\nlow latency requirements.", "published": "2023-02-10 04:05:24", "link": "http://arxiv.org/abs/2302.05040v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-Corpora Spoken Language Identification with Domain Diversification\n  and Generalization", "abstract": "This work addresses the cross-corpora generalization issue for the\nlow-resourced spoken language identification (LID) problem. We have conducted\nthe experiments in the context of Indian LID and identified strikingly poor\ncross-corpora generalization due to corpora-dependent non-lingual biases. Our\ncontribution to this work is twofold. First, we propose domain diversification,\nwhich diversifies the limited training data using different audio data\naugmentation methods. We then propose the concept of maximally diversity-aware\ncascaded augmentations and optimize the augmentation fold-factor for effective\ndiversification of the training data. Second, we introduce the idea of domain\ngeneralization considering the augmentation methods as pseudo-domains. Towards\nthis, we investigate both domain-invariant and domain-aware approaches. Our LID\nsystem is based on the state-of-the-art emphasized channel attention,\npropagation, and aggregation based time delay neural network (ECAPA-TDNN)\narchitecture. We have conducted extensive experiments with three widely used\ncorpora for Indian LID research. In addition, we conduct a final blind\nevaluation of our proposed methods on the Indian subset of VoxLingua107 corpus\ncollected in the wild. Our experiments demonstrate that the proposed domain\ndiversification is more promising over commonly used simple augmentation\nmethods. The study also reveals that domain generalization is a more effective\nsolution than domain diversification. We also notice that domain-aware learning\nperforms better for same-corpora LID, whereas domain-invariant learning is more\nsuitable for cross-corpora generalization. Compared to basic ECAPA-TDNN, its\nproposed domain-invariant extensions improve the cross-corpora EER up to 5.23%.\nIn contrast, the proposed domain-aware extensions also improve performance for\nsame-corpora test scenarios.", "published": "2023-02-10 08:21:42", "link": "http://arxiv.org/abs/2302.05110v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Translating Natural Language to Planning Goals with Large-Language\n  Models", "abstract": "Recent large language models (LLMs) have demonstrated remarkable performance\non a variety of natural language processing (NLP) tasks, leading to intense\nexcitement about their applicability across various domains. Unfortunately,\nrecent work has also shown that LLMs are unable to perform accurate reasoning\nnor solve planning problems, which may limit their usefulness for\nrobotics-related tasks. In this work, our central question is whether LLMs are\nable to translate goals specified in natural language to a structured planning\nlanguage. If so, LLM can act as a natural interface between the planner and\nhuman users; the translated goal can be handed to domain-independent AI\nplanners that are very effective at planning. Our empirical results on GPT 3.5\nvariants show that LLMs are much better suited towards translation rather than\nplanning. We find that LLMs are able to leverage commonsense knowledge and\nreasoning to furnish missing details from under-specified goals (as is often\nthe case in natural language). However, our experiments also reveal that LLMs\ncan fail to generate goals in tasks that involve numerical or physical (e.g.,\nspatial) reasoning, and that LLMs are sensitive to the prompts used. As such,\nthese models are promising for translation to structured planning languages,\nbut care should be taken in their use.", "published": "2023-02-10 09:17:52", "link": "http://arxiv.org/abs/2302.05128v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Realistic Conversational Question Answering with Answer Selection based\n  on Calibrated Confidence and Uncertainty Measurement", "abstract": "Conversational Question Answering (ConvQA) models aim at answering a question\nwith its relevant paragraph and previous question-answer pairs that occurred\nduring conversation multiple times. To apply such models to a real-world\nscenario, some existing work uses predicted answers, instead of unavailable\nground-truth answers, as the conversation history for inference. However, since\nthese models usually predict wrong answers, using all the predictions without\nfiltering significantly hampers the model performance. To address this problem,\nwe propose to filter out inaccurate answers in the conversation history based\non their estimated confidences and uncertainties from the ConvQA model, without\nmaking any architectural changes. Moreover, to make the confidence and\nuncertainty values more reliable, we propose to further calibrate them, thereby\nsmoothing the model predictions. We validate our models, Answer Selection-based\nrealistic Conversation Question Answering, on two standard ConvQA datasets, and\nthe results show that our models significantly outperform relevant baselines.\nCode is available at: https://github.com/starsuzi/AS-ConvQA.", "published": "2023-02-10 09:42:07", "link": "http://arxiv.org/abs/2302.05137v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Song of Ice and Fire: Analyzing Textual Autotelic Agents in\n  ScienceWorld", "abstract": "Building open-ended agents that can autonomously discover a diversity of\nbehaviours is one of the long-standing goals of artificial intelligence. This\nchallenge can be studied in the framework of autotelic RL agents, i.e. agents\nthat learn by selecting and pursuing their own goals, self-organizing a\nlearning curriculum. Recent work identified language as a key dimension of\nautotelic learning, in particular because it enables abstract goal sampling and\nguidance from social peers for hindsight relabelling. Within this perspective,\nwe study the following open scientific questions: What is the impact of\nhindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can\nthe agent learn from very rare language goal examples in its experience replay?\nHow can multiple forms of exploration be combined, and take advantage of easier\ngoals as stepping stones to reach harder ones? To address these questions, we\nuse ScienceWorld, a textual environment with rich abstract and combinatorial\nphysics. We show the importance of selectivity from the social peer's feedback;\nthat experience replay needs to over-sample examples of rare goals; and that\nfollowing self-generated goal sequences where the agent's competence is\nintermediate leads to significant improvements in final performance.", "published": "2023-02-10 13:49:50", "link": "http://arxiv.org/abs/2302.05244v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Language Decision Transformers with Exponential Tilt for Interactive\n  Text Environments", "abstract": "Text-based game environments are challenging because agents must deal with\nlong sequences of text, execute compositional actions using text and learn from\nsparse rewards. We address these challenges by proposing Language Decision\nTransformers (LDTs), a framework that is based on transformer language models\nand decision transformers (DTs). Our LDTs extend DTs with 3 components: (1)\nexponential tilt to guide the agent towards high obtainable goals, (2) novel\ngoal conditioning methods yielding better results than the traditional\nreturn-to-go (sum of all future rewards), and (3) a model of future\nobservations that improves agent performance. LDTs are the first to address\noffline RL with DTs on these challenging games. Our experiments show that LDTs\nachieve the highest scores among many different types of agents on some of the\nmost challenging Jericho games, such as Enchanter.", "published": "2023-02-10 20:50:58", "link": "http://arxiv.org/abs/2302.05507v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation\n  in Large Language Models", "abstract": "Studies have shown that large pretrained language models exhibit biases\nagainst social groups based on race, gender etc, which they inherit from the\ndatasets they are trained on. Various researchers have proposed mathematical\ntools for quantifying and identifying these biases. There have been methods\nproposed to mitigate such biases. In this paper, we present a comprehensive\nquantitative evaluation of different kinds of biases such as race, gender,\nethnicity, age etc. exhibited by popular pretrained language models such as\nBERT, GPT-2 etc. and also present a toolkit that provides plug-and-play\ninterfaces to connect mathematical tools to identify biases with large\npretrained language models such as BERT, GPT-2 etc. and also present users with\nthe opportunity to test custom models against these metrics. The toolkit also\nallows users to debias existing and custom models using the debiasing\ntechniques proposed so far. The toolkit is available at\nhttps://github.com/HrishikeshVish/Fairpy.", "published": "2023-02-10 20:54:10", "link": "http://arxiv.org/abs/2302.05508v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AV-data2vec: Self-supervised Learning of Audio-Visual Speech\n  Representations with Contextualized Target Representations", "abstract": "Self-supervision has shown great potential for audio-visual speech\nrecognition by vastly reducing the amount of labeled data required to build\ngood systems. However, existing methods are either not entirely end-to-end or\ndo not train joint representations of both modalities. In this paper, we\nintroduce AV-data2vec which addresses these challenges and builds audio-visual\nrepresentations based on predicting contextualized representations which has\nbeen successful in the uni-modal case. The model uses a shared transformer\nencoder for both audio and video and can combine both modalities to improve\nspeech recognition. Results on LRS3 show that AV-data2vec consistently\noutperforms existing methods under all settings with the same amount of data\nand model size.", "published": "2023-02-10 02:55:52", "link": "http://arxiv.org/abs/2302.06419v2", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews\n  on Social Media", "abstract": "Recent advances in generative models such as GPT may be used to fabricate\nindistinguishable fake customer reviews at a much lower cost, thus posing\nchallenges for social media platforms to detect these machine-generated fake\nreviews. We propose to leverage the high-quality elite restaurant reviews\nverified by Yelp to generate fake reviews from the OpenAI GPT review creator\nand ultimately fine-tune a GPT output detector to predict fake reviews that\nsignificantly outperform existing solutions. We further apply the model to\npredict non-elite reviews and identify the patterns across several dimensions,\nsuch as review, user and restaurant characteristics, and writing style. We show\nthat social media platforms are continuously challenged by machine-generated\nfake reviews, although they may implement detection systems to filter out\nsuspicious reviews.", "published": "2023-02-10 19:40:10", "link": "http://arxiv.org/abs/2302.07731v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Alloprof: a new French question-answer education dataset and its use in\n  an information retrieval case study", "abstract": "Teachers and students are increasingly relying on online learning resources\nto supplement the ones provided in school. This increase in the breadth and\ndepth of available resources is a great thing for students, but only provided\nthey are able to find answers to their queries. Question-answering and\ninformation retrieval systems have benefited from public datasets to train and\nevaluate their algorithms, but most of these datasets have been in English text\nwritten by and for adults. We introduce a new public French question-answering\ndataset collected from Alloprof, a Quebec-based primary and high-school help\nwebsite, containing 29 349 questions and their explanations in a variety of\nschool subjects from 10 368 students, with more than half of the explanations\ncontaining links to other questions or some of the 2 596 reference pages on the\nwebsite. We also present a case study of this dataset in an information\nretrieval task. This dataset was collected on the Alloprof public forum, with\nall questions verified for their appropriateness and the explanations verified\nboth for their appropriateness and their relevance to the question. To predict\nrelevant documents, architectures using pre-trained BERT models were fine-tuned\nand evaluated. This dataset will allow researchers to develop\nquestion-answering, information retrieval and other algorithms specifically for\nthe French speaking education context. Furthermore, the range of language\nproficiency, images, mathematical symbols and spelling mistakes will\nnecessitate algorithms based on a multimodal comprehension. The case study we\npresent as a baseline shows an approach that relies on recent techniques\nprovides an acceptable performance level, but more work is necessary before it\ncan reliably be used and trusted in a production setting.", "published": "2023-02-10 20:23:27", "link": "http://arxiv.org/abs/2302.07738v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spoken language change detection inspired by speaker change detection", "abstract": "Spoken language change detection (LCD) refers to identifying the language\ntransitions in a code-switched utterance. Similarly, identifying the speaker\ntransitions in a multispeaker utterance is known as speaker change detection\n(SCD). Since tasks-wise both are similar, the architecture/framework developed\nfor the SCD task may be suitable for the LCD task. Hence, the aim of the\npresent work is to develop LCD systems inspired by SCD. Initially, both LCD and\nSCD are performed by humans. The study suggests humans require (a) a larger\nduration around the change point and (b) language-specific prior exposure, for\nperforming LCD as compared to SCD. The larger duration requirement is\nincorporated by increasing the analysis window length of the unsupervised\ndistance-based approach. This leads to a relative performance improvement of\n29.1% and 2.4%, and a priori language knowledge provides a relative improvement\nof 31.63% and 14.27% on the synthetic and practical codeswitched datasets,\nrespectively. The performance difference between the practical and synthetic\ndatasets is mostly due to differences in the distribution of the monolingual\nsegment duration.", "published": "2023-02-10 14:25:49", "link": "http://arxiv.org/abs/2302.05265v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The LuViRA Dataset: Synchronized Vision, Radio, and Audio Sensors for\n  Indoor Localization", "abstract": "We present a synchronized multisensory dataset for accurate and robust indoor\nlocalization: the Lund University Vision, Radio, and Audio (LuViRA) Dataset.\nThe dataset includes color images, corresponding depth maps, inertial\nmeasurement unit (IMU) readings, channel response between a 5G massive\nmultiple-input and multiple-output (MIMO) testbed and user equipment, audio\nrecorded by 12 microphones, and accurate six degrees of freedom (6DOF) pose\nground truth of 0.5 mm. We synchronize these sensors to ensure that all data is\nrecorded simultaneously. A camera, speaker, and transmit antenna are placed on\ntop of a slowly moving service robot, and 89 trajectories are recorded. Each\ntrajectory includes 20 to 50 seconds of recorded sensor data and ground truth\nlabels. Data from different sensors can be used separately or jointly to\nperform localization tasks, and data from the motion capture (mocap) system is\nused to verify the results obtained by the localization algorithms. The main\naim of this dataset is to enable research on sensor fusion with the most\ncommonly used sensors for localization tasks. Moreover, the full dataset or\nsome parts of it can also be used for other research areas such as channel\nestimation, image classification, etc. Our dataset is available at:\nhttps://github.com/ilaydayaman/LuViRA_Dataset", "published": "2023-02-10 15:12:40", "link": "http://arxiv.org/abs/2302.05309v3", "categories": ["eess.SP", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "GTR-CTRL: Instrument and Genre Conditioning for Guitar-Focused Music\n  Generation with Transformers", "abstract": "Recently, symbolic music generation with deep learning techniques has\nwitnessed steady improvements. Most works on this topic focus on MIDI\nrepresentations, but less attention has been paid to symbolic music generation\nusing guitar tablatures (tabs) which can be used to encode multiple\ninstruments. Tabs include information on expressive techniques and fingerings\nfor fretted string instruments in addition to rhythm and pitch. In this work,\nwe use the DadaGP dataset for guitar tab music generation, a corpus of over 26k\nsongs in GuitarPro and token formats. We introduce methods to condition a\nTransformer-XL deep learning model to generate guitar tabs (GTR-CTRL) based on\ndesired instrumentation (inst-CTRL) and genre (genre-CTRL). Special control\ntokens are appended at the beginning of each song in the training corpus. We\nassess the performance of the model with and without conditioning. We propose\ninstrument presence metrics to assess the inst-CTRL model's response to a given\ninstrumentation prompt. We trained a BERT model for downstream genre\nclassification and used it to assess the results obtained with the genre-CTRL\nmodel. Statistical analyses evidence significant differences between the\nconditioned and unconditioned models. Overall, results indicate that the\nGTR-CTRL methods provide more flexibility and control for guitar-focused\nsymbolic music generation than an unconditioned model.", "published": "2023-02-10 17:43:03", "link": "http://arxiv.org/abs/2302.05393v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
