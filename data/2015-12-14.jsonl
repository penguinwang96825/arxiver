{"title": "Small-footprint Deep Neural Networks with Highway Connections for Speech\n  Recognition", "abstract": "For speech recognition, deep neural networks (DNNs) have significantly\nimproved the recognition accuracy in most of benchmark datasets and application\ndomains. However, compared to the conventional Gaussian mixture models,\nDNN-based acoustic models usually have much larger number of model parameters,\nmaking it challenging for their applications in resource constrained platforms,\ne.g., mobile devices. In this paper, we study the application of the recently\nproposed highway network to train small-footprint DNNs, which are {\\it thinner}\nand {\\it deeper}, and have significantly smaller number of model parameters\ncompared to conventional DNNs. We investigated this approach on the AMI meeting\nspeech transcription corpus which has around 70 hours of audio data. The\nhighway neural networks constantly outperformed their plain DNN counterparts,\nand the number of model parameters can be reduced significantly without\nsacrificing the recognition accuracy.", "published": "2015-12-14 12:29:32", "link": "http://arxiv.org/abs/1512.04280v4", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "We Are Humor Beings: Understanding and Predicting Visual Humor", "abstract": "Humor is an integral part of human lives. Despite being tremendously\nimpactful, it is perhaps surprising that we do not have a detailed\nunderstanding of humor yet. As interactions between humans and AI systems\nincrease, it is imperative that these systems are taught to understand\nsubtleties of human expressions such as humor. In this work, we are interested\nin the question - what content in a scene causes it to be funny? As a first\nstep towards understanding visual humor, we analyze the humor manifested in\nabstract scenes and design computational models for them. We collect two\ndatasets of abstract scenes that facilitate the study of humor at both the\nscene-level and the object-level. We analyze the funny scenes and explore the\ndifferent types of humor depicted in them via human studies. We model two tasks\nthat we believe demonstrate an understanding of some aspects of visual humor.\nThe tasks involve predicting the funniness of a scene and altering the\nfunniness of a scene. We show that our models perform well quantitatively, and\nqualitatively through human studies. Our datasets are publicly available.", "published": "2015-12-14 16:59:35", "link": "http://arxiv.org/abs/1512.04407v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Sentence Entailment in Compositional Distributional Semantics", "abstract": "Distributional semantic models provide vector representations for words by\ngathering co-occurrence frequencies from corpora of text. Compositional\ndistributional models extend these from words to phrases and sentences. In\ncategorical compositional distributional semantics, phrase and sentence\nrepresentations are functions of their grammatical structure and\nrepresentations of the words therein. In this setting, grammatical structures\nare formalised by morphisms of a compact closed category and meanings of words\nare formalised by objects of the same category. These can be instantiated in\nthe form of vectors or density matrices. This paper concerns the applications\nof this model to phrase and sentence level entailment. We argue that\nentropy-based distances of vectors and density matrices provide a good\ncandidate to measure word-level entailment, show the advantage of density\nmatrices over vectors for word level entailments, and prove that these\ndistances extend compositionally from words to phrases and sentences. We\nexemplify our theoretical constructions on real data and a toy entailment\ndataset and provide preliminary experimental evidence.", "published": "2015-12-14 17:36:35", "link": "http://arxiv.org/abs/1512.04419v2", "categories": ["cs.CL", "cs.AI", "math.CT", "03B65", "I.2.7"], "primary_category": "cs.CL"}
