{"title": "Event-Centric Question Answering via Contrastive Learning and Invertible\n  Event Transformation", "abstract": "Human reading comprehension often requires reasoning of event semantic\nrelations in narratives, represented by Event-centric Question-Answering (QA).\nTo address event-centric QA, we propose a novel QA model with contrastive\nlearning and invertible event transformation, call TranCLR. Our proposed model\nutilizes an invertible transformation matrix to project semantic vectors of\nevents into a common event embedding space, trained with contrastive learning,\nand thus naturally inject event semantic knowledge into mainstream QA\npipelines. The transformation matrix is fine-tuned with the annotated event\nrelation types between events that occurred in questions and those in answers,\nusing event-aware question vectors. Experimental results on the Event Semantic\nRelation Reasoning (ESTER) dataset show significant improvements in both\ngenerative and extractive settings compared to the existing strong baselines,\nachieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact\nMatch (EM) score under the multi-answer setting. Qualitative analysis reveals\nthe high quality of the generated answers by TranCLR, demonstrating the\nfeasibility of injecting event knowledge into QA model learning. Our code and\nmodels can be found at https://github.com/LuJunru/TranCLR.", "published": "2022-10-24 01:15:06", "link": "http://arxiv.org/abs/2210.12902v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun\n  Property Prediction", "abstract": "Neural language models encode rich knowledge about entities and their\nrelationships which can be extracted from their representations using probing.\nCommon properties of nouns (e.g., red strawberries, small ant) are, however,\nmore challenging to extract compared to other types of knowledge because they\nare rarely explicitly stated in texts. We hypothesize this to mainly be the\ncase for perceptual properties which are obvious to the participants in the\ncommunication. We propose to extract these properties from images and use them\nin an ensemble model, in order to complement the information that is extracted\nfrom language models. We consider perceptual properties to be more concrete\nthan abstract properties (e.g., interesting, flawless). We propose to use the\nadjectives' concreteness score as a lever to calibrate the contribution of each\nsource (text vs. images). We evaluate our ensemble model in a ranking task\nwhere the actual properties of a noun need to be ranked higher than other\nnon-relevant properties. Our results show that the proposed combination of text\nand images greatly improves noun property prediction compared to powerful\ntext-based language models.", "published": "2022-10-24 01:25:21", "link": "http://arxiv.org/abs/2210.12905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Euphemism Detection in Few-Shot and Zero-Shot Settings", "abstract": "This work builds upon the Euphemism Detection Shared Task proposed in the\nEMNLP 2022 FigLang Workshop, and extends it to few-shot and zero-shot settings.\nWe demonstrate a few-shot and zero-shot formulation using the dataset from the\nshared task, and we conduct experiments in these settings using RoBERTa and\nGPT-3. Our results show that language models are able to classify euphemistic\nterms relatively well even on new terms unseen during training, indicating that\nit is able to capture higher-level concepts related to euphemisms.", "published": "2022-10-24 02:43:43", "link": "http://arxiv.org/abs/2210.12926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Current Task-oriented Dialogue Systems Able to Satisfy Impolite\n  Users?", "abstract": "Task-oriented dialogue (TOD) systems have assisted users on many tasks,\nincluding ticket booking and service inquiries. While existing TOD systems have\nshown promising performance in serving customer needs, these systems mostly\nassume that users would interact with the dialogue agent politely. This\nassumption is unrealistic as impatient or frustrated customers may also\ninteract with TOD systems impolitely. This paper aims to address this research\ngap by investigating impolite users' effects on TOD systems. Specifically, we\nconstructed an impolite dialogue corpus and conducted extensive experiments to\nevaluate the state-of-the-art TOD systems on our impolite dialogue corpus. Our\nexperimental results show that existing TOD systems are unable to handle\nimpolite user utterances. We also present a data augmentation method to improve\nTOD performance in impolite dialogues. Nevertheless, handling impolite\ndialogues remains a very challenging research task. We hope by releasing the\nimpolite dialogue corpus and establishing the benchmark evaluations, more\nresearchers are encouraged to investigate this new challenging research task.", "published": "2022-10-24 04:11:52", "link": "http://arxiv.org/abs/2210.12942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composition, Attention, or Both?", "abstract": "In this paper, we propose a novel architecture called Composition Attention\nGrammars (CAGs) that recursively compose subtrees into a single vector\nrepresentation with a composition function, and selectively attend to previous\nstructural information with a self-attention mechanism. We investigate whether\nthese components -- the composition function and the self-attention mechanism\n-- can both induce human-like syntactic generalization. Specifically, we train\nlanguage models (LMs) with and without these two components with the model\nsizes carefully controlled, and evaluate their syntactic generalization\nperformance against six test circuits on the SyntaxGym benchmark. The results\ndemonstrated that the composition function and the self-attention mechanism\nboth play an important role to make LMs more human-like, and closer inspection\nof linguistic phenomenon implied that the composition function allowed\nsyntactic features, but not semantic features, to percolate into subtree\nrepresentations.", "published": "2022-10-24 05:30:02", "link": "http://arxiv.org/abs/2210.12958v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Type Conversational Question-Answer Generation with Closed-ended\n  and Unanswerable Questions", "abstract": "Conversational question answering (CQA) facilitates an incremental and\ninteractive understanding of a given context, but building a CQA system is\ndifficult for many domains due to the problem of data scarcity. In this paper,\nwe introduce a novel method to synthesize data for CQA with various question\ntypes, including open-ended, closed-ended, and unanswerable questions. We\ndesign a different generation flow for each question type and effectively\ncombine them in a single, shared framework. Moreover, we devise a hierarchical\nanswerability classification (hierarchical AC) module that improves quality of\nthe synthetic data while acquiring unanswerable questions. Manual inspections\nshow that synthetic data generated with our framework have characteristics very\nsimilar to those of human-generated conversations. Across four domains, CQA\nsystems trained on our synthetic data indeed show good performance close to the\nsystems trained on human-annotated data.", "published": "2022-10-24 07:01:51", "link": "http://arxiv.org/abs/2210.12979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Maknuune: A Large Open Palestinian Arabic Lexicon", "abstract": "We present Maknuune, a large open lexicon for the Palestinian Arabic dialect.\nMaknuune has over 36K entries from 17K lemmas, and 3.7K roots. All entries\ninclude diacritized Arabic orthography, phonological transcription and English\nglosses. Some entries are enriched with additional information such as broken\nplurals and templatic feminine forms, associated phrases and collocations,\nStandard Arabic glosses, and examples or notes on grammar, usage, or location\nof collected entry.", "published": "2022-10-24 07:19:03", "link": "http://arxiv.org/abs/2210.12985v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Revisiting of Linguistic Knowledge Fusion in Language\n  Understanding Tasks", "abstract": "Though linguistic knowledge emerges during large-scale language model\npretraining, recent work attempt to explicitly incorporate human-defined\nlinguistic priors into task-specific fine-tuning. Infusing language models with\nsyntactic or semantic knowledge from parsers has shown improvements on many\nlanguage understanding tasks. To further investigate the effectiveness of\nstructural linguistic priors, we conduct empirical study of replacing parsed\ngraphs or trees with trivial ones (rarely carrying linguistic knowledge e.g.,\nbalanced tree) for tasks in the GLUE benchmark. Encoding with trivial graphs\nachieves competitive or even better performance in fully-supervised and\nfew-shot settings. It reveals that the gains might not be significantly\nattributed to explicit linguistic priors but rather to more feature\ninteractions brought by fusion layers. Hence we call for attention to using\ntrivial graphs as necessary baselines to design advanced knowledge fusion\nmethods in the future.", "published": "2022-10-24 07:47:32", "link": "http://arxiv.org/abs/2210.13002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Auxiliary Tasks Training: Bridging the Gap between\n  Languages for Zero-Shot Transfer of Hate Speech Detection Models", "abstract": "Zero-shot cross-lingual transfer learning has been shown to be highly\nchallenging for tasks involving a lot of linguistic specificities or when a\ncultural gap is present between languages, such as in hate speech detection. In\nthis paper, we highlight this limitation for hate speech detection in several\ndomains and languages using strict experimental settings. Then, we propose to\ntrain on multilingual auxiliary tasks -- sentiment analysis, named entity\nrecognition, and tasks relying on syntactic information -- to improve zero-shot\ntransfer of hate speech detection models across languages. We show how hate\nspeech detection models benefit from a cross-lingual knowledge proxy brought by\nauxiliary tasks fine-tuning and highlight these tasks' positive impact on\nbridging the hate speech linguistic and cultural gap between languages.", "published": "2022-10-24 08:26:51", "link": "http://arxiv.org/abs/2210.13029v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Covid vaccine is against Covid but Oxford vaccine is made at Oxford!\"\n  Semantic Interpretation of Proper Noun Compounds", "abstract": "Proper noun compounds, e.g., \"Covid vaccine\", convey information in a\nsuccinct manner (a \"Covid vaccine\" is a \"vaccine that immunizes against the\nCovid disease\"). These are commonly used in short-form domains, such as news\nheadlines, but are largely ignored in information-seeking applications. To\naddress this limitation, we release a new manually annotated dataset, ProNCI,\nconsisting of 22.5K proper noun compounds along with their free-form semantic\ninterpretations. ProNCI is 60 times larger than prior noun compound datasets\nand also includes non-compositional examples, which have not been previously\nexplored. We experiment with various neural models for automatically generating\nthe semantic interpretations from proper noun compounds, ranging from few-shot\nprompting to supervised learning, with varying degrees of knowledge about the\nconstituent nouns. We find that adding targeted knowledge, particularly about\nthe common noun, results in performance gains of upto 2.8%. Finally, we\nintegrate our model generated interpretations with an existing Open IE system\nand observe an 7.5% increase in yield at a precision of 85%. The dataset and\ncode are available at https://github.com/dair-iitd/pronci.", "published": "2022-10-24 08:48:50", "link": "http://arxiv.org/abs/2210.13039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural generalization is hard for sequence-to-sequence models", "abstract": "Sequence-to-sequence (seq2seq) models have been successful across many NLP\ntasks, including ones that require predicting linguistic structure. However,\nrecent work on compositional generalization has shown that seq2seq models\nachieve very low accuracy in generalizing to linguistic structures that were\nnot seen in training. We present new evidence that this is a general limitation\nof seq2seq models that is present not just in semantic parsing, but also in\nsyntactic parsing and in text-to-text tasks, and that this limitation can often\nbe overcome by neurosymbolic models that have linguistic knowledge built in. We\nfurther report on some experiments that give initial answers on the reasons for\nthese limitations.", "published": "2022-10-24 09:03:03", "link": "http://arxiv.org/abs/2210.13050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Framework for Pun Generation with Humor Principles", "abstract": "We propose a unified framework to generate both homophonic and homographic\npuns to resolve the split-up in existing works. Specifically, we incorporate\nthree linguistic attributes of puns to the language models: ambiguity,\ndistinctiveness, and surprise. Our framework consists of three parts: 1) a\ncontext words/phrases selector to promote the aforementioned attributes, 2) a\ngeneration model trained on non-pun sentences to incorporate the context\nwords/phrases into the generation output, and 3) a label predictor that learns\nthe structure of puns which is used to steer the generation model at inference\ntime. Evaluation results on both pun types demonstrate the efficacy of our\nmodel over strong baselines.", "published": "2022-10-24 09:20:45", "link": "http://arxiv.org/abs/2210.13055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Full-Text Argumentation Mining on Scientific Publications", "abstract": "Scholarly Argumentation Mining (SAM) has recently gained attention due to its\npotential to help scholars with the rapid growth of published scientific\nliterature. It comprises two subtasks: argumentative discourse unit recognition\n(ADUR) and argumentative relation extraction (ARE), both of which are\nchallenging since they require e.g. the integration of domain knowledge, the\ndetection of implicit statements, and the disambiguation of argument structure.\nWhile previous work focused on dataset construction and baseline methods for\nspecific document sections, such as abstract or results, full-text scholarly\nargumentation mining has seen little progress. In this work, we introduce a\nsequential pipeline model combining ADUR and ARE for full-text SAM, and provide\na first analysis of the performance of pretrained language models (PLMs) on\nboth subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus,\noutperforming the previous best reported result by a large margin (+7% F1). We\nalso present the first results for ARE, and thus for the full AM pipeline, on\nthis benchmark dataset. Our detailed error analysis reveals that non-contiguous\nADUs as well as the interpretation of discourse connectors pose major\nchallenges and that data annotation needs to be more consistent.", "published": "2022-10-24 10:05:30", "link": "http://arxiv.org/abs/2210.13084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Legal-Tech Open Diaries: Lesson learned on how to develop and deploy\n  light-weight models in the era of humongous Language Models", "abstract": "In the era of billion-parameter-sized Language Models (LMs), start-ups have\nto follow trends and adapt their technology accordingly. Nonetheless, there are\nopen challenges since the development and deployment of large models comes with\na need for high computational resources and has economical consequences. In\nthis work, we follow the steps of the R&D group of a modern legal-tech start-up\nand present important insights on model development and deployment. We start\nfrom ground zero by pre-training multiple domain-specific multi-lingual LMs\nwhich are a better fit to contractual and regulatory text compared to the\navailable alternatives (XLM-R). We present benchmark results of such models in\na half-public half-private legal benchmark comprising 5 downstream tasks\nshowing the impact of larger model size. Lastly, we examine the impact of a\nfull-scale pipeline for model compression which includes: a) Parameter Pruning,\nb) Knowledge Distillation, and c) Quantization: The resulting models are much\nmore efficient without sacrificing performance at large.", "published": "2022-10-24 10:08:59", "link": "http://arxiv.org/abs/2210.13086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Synchronization: Restoring Translational Relationships with\n  Editing Operations", "abstract": "Machine Translation (MT) is usually viewed as a one-shot process that\ngenerates the target language equivalent of some source text from scratch. We\nconsider here a more general setting which assumes an initial target sequence,\nthat must be transformed into a valid translation of the source, thereby\nrestoring parallelism between source and target. For this bilingual\nsynchronization task, we consider several architectures (both autoregressive\nand non-autoregressive) and training regimes, and experiment with multiple\npractical settings such as simulated interactive MT, translating with\nTranslation Memory (TM) and TM cleaning. Our results suggest that one single\ngeneric edit-based system, once fine-tuned, can compare with, or even\noutperform, dedicated systems specifically trained for these tasks.", "published": "2022-10-24 12:25:44", "link": "http://arxiv.org/abs/2210.13163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Better Your Syntax, the Better Your Semantics? Probing Pretrained\n  Language Models for the English Comparative Correlative", "abstract": "Construction Grammar (CxG) is a paradigm from cognitive linguistics\nemphasising the connection between syntax and semantics. Rather than rules that\noperate on lexical items, it posits constructions as the central building\nblocks of language, i.e., linguistic units of different granularity that\ncombine syntax and semantics. As a first step towards assessing the\ncompatibility of CxG with the syntactic and semantic knowledge demonstrated by\nstate-of-the-art pretrained language models (PLMs), we present an investigation\nof their capability to classify and understand one of the most commonly studied\nconstructions, the English comparative correlative (CC). We conduct experiments\nexamining the classification accuracy of a syntactic probe on the one hand and\nthe models' behaviour in a semantic application task on the other, with BERT,\nRoBERTa, and DeBERTa as the example PLMs. Our results show that all three\ninvestigated PLMs are able to recognise the structure of the CC but fail to use\nits meaning. While human-like performance of PLMs on many NLP tasks has been\nalleged, this indicates that PLMs still suffer from substantial shortcomings in\ncentral domains of linguistic knowledge.", "published": "2022-10-24 13:01:24", "link": "http://arxiv.org/abs/2210.13181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mutual Information Alleviates Hallucinations in Abstractive\n  Summarization", "abstract": "Despite significant progress in the quality of language generated from\nabstractive summarization models, these models still exhibit the tendency to\nhallucinate, i.e., output content not supported by the source document. A\nnumber of works have tried to fix--or at least uncover the source of--the\nproblem with limited success. In this paper, we identify a simple criterion\nunder which models are significantly more likely to assign more probability to\nhallucinated content during generation: high model uncertainty. This finding\noffers a potential explanation for hallucinations: models default to favoring\ntext with high marginal probability, i.e., high-frequency occurrences in the\ntraining set, when uncertain about a continuation. It also motivates possible\nroutes for real-time intervention during decoding to prevent such\nhallucinations. We propose a decoding strategy that switches to optimizing for\npointwise mutual information of the source and target token--rather than purely\nthe probability of the target token--when the model exhibits uncertainty.\nExperiments on the XSum dataset show that our method decreases the probability\nof hallucinated tokens while maintaining the Rouge and BertS scores of\ntop-performing decoding strategies.", "published": "2022-10-24 13:30:54", "link": "http://arxiv.org/abs/2210.13210v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Use of Influence Functions for Instance-Specific Data\n  Filtering in Neural Machine Translation", "abstract": "Customer feedback can be an important signal for improving commercial machine\ntranslation systems. One solution for fixing specific translation errors is to\nremove the related erroneous training instances followed by re-training of the\nmachine translation system, which we refer to as instance-specific data\nfiltering. Influence functions (IF) have been shown to be effective in finding\nsuch relevant training examples for classification tasks such as image\nclassification, toxic speech detection and entailment task. Given a probing\ninstance, IF find influential training examples by measuring the similarity of\nthe probing instance with a set of training examples in gradient space. In this\nwork, we examine the use of influence functions for Neural Machine Translation\n(NMT). We propose two effective extensions to a state of the art influence\nfunction and demonstrate on the sub-problem of copied training examples that IF\ncan be applied more generally than handcrafted regular expressions.", "published": "2022-10-24 14:22:20", "link": "http://arxiv.org/abs/2210.13281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and\n  Effective Text Generation", "abstract": "We study the text generation task under the approach of pre-trained language\nmodels (PLMs). Typically, an auto-regressive (AR) method is adopted for\ngenerating texts in a token-by-token manner. Despite many advantages of AR\ngeneration, it usually suffers from inefficient inference. Therefore,\nnon-autoregressive (NAR) models are proposed to generate all target tokens\nsimultaneously. However, NAR models usually generate texts of lower quality due\nto the absence of token dependency in the output text. In this paper, we\npropose ELMER: an efficient and effective PLM for NAR text generation to\nexplicitly model the token dependency during NAR generation. By leveraging the\nearly exit technique, ELMER enables the token generations at different layers,\naccording to their prediction confidence (a more confident token will exit at a\nlower layer). Besides, we propose a novel pre-training objective, Layer\nPermutation Language Modeling, to pre-train ELMER by permuting the exit layer\nfor each token in sequences. Experiments on three text generation tasks show\nthat ELMER significantly outperforms NAR models and further narrows the\nperformance gap with AR PLMs (\\eg ELMER (29.92) vs BART (30.61) ROUGE-L in\nXSUM) while achieving over 10 times inference speedup.", "published": "2022-10-24 14:46:47", "link": "http://arxiv.org/abs/2210.13304v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Discard Fixed-Window Audio Segmentation in Speech-to-Text\n  Translation", "abstract": "For real-life applications, it is crucial that end-to-end spoken language\ntranslation models perform well on continuous audio, without relying on\nhuman-supplied segmentation. For online spoken language translation, where\nmodels need to start translating before the full utterance is spoken, most\nprevious work has ignored the segmentation problem. In this paper, we compare\nvarious methods for improving models' robustness towards segmentation errors\nand different segmentation strategies in both offline and online settings and\nreport results on translation quality, flicker and delay. Our findings on five\ndifferent language pairs show that a simple fixed-window audio segmentation can\nperform surprisingly well given the right conditions.", "published": "2022-10-24 16:06:33", "link": "http://arxiv.org/abs/2210.13363v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Focused Concatenation for Context-Aware Neural Machine Translation", "abstract": "A straightforward approach to context-aware neural machine translation\nconsists in feeding the standard encoder-decoder architecture with a window of\nconsecutive sentences, formed by the current sentence and a number of sentences\nfrom its context concatenated to it. In this work, we propose an improved\nconcatenation approach that encourages the model to focus on the translation of\nthe current sentence, discounting the loss generated by target context. We also\npropose an additional improvement that strengthen the notion of sentence\nboundaries and of relative sentence distance, facilitating model compliance to\nthe context-discounted objective. We evaluate our approach with both\naverage-translation quality metrics and contrastive test sets for the\ntranslation of inter-sentential discourse phenomena, proving its superiority to\nthe vanilla concatenation approach and other sophisticated context-aware\nsystems.", "published": "2022-10-24 16:41:22", "link": "http://arxiv.org/abs/2210.13388v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining Translationese: why are Neural Classifiers Better and what do\n  they Learn?", "abstract": "Recent work has shown that neural feature- and representation-learning, e.g.\nBERT, achieves superior performance over traditional manual feature engineering\nbased approaches, with e.g. SVMs, in translationese classification tasks.\nPrevious research did not show $(i)$ whether the difference is because of the\nfeatures, the classifiers or both, and $(ii)$ what the neural classifiers\nactually learn. To address $(i)$, we carefully design experiments that swap\nfeatures between BERT- and SVM-based classifiers. We show that an SVM fed with\nBERT representations performs at the level of the best BERT classifiers, while\nBERT learning and using handcrafted features performs at the level of an SVM\nusing handcrafted features. This shows that the performance differences are due\nto the features. To address $(ii)$ we use integrated gradients and find that\n$(a)$ there is indication that information captured by hand-crafted features is\nonly a subset of what BERT learns, and $(b)$ part of BERT's top performance\nresults are due to BERT learning topic differences and spurious correlations\nwith translationese.", "published": "2022-10-24 16:43:28", "link": "http://arxiv.org/abs/2210.13391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-level Sentiment Analysis in Contact Center Telephone\n  Conversations", "abstract": "Entity-level sentiment analysis predicts the sentiment about entities\nmentioned in a given text. It is very useful in a business context to\nunderstand user emotions towards certain entities, such as products or\ncompanies. In this paper, we demonstrate how we developed an entity-level\nsentiment analysis system that analyzes English telephone conversation\ntranscripts in contact centers to provide business insight. We present two\napproaches, one entirely based on the transformer-based DistilBERT model, and\nanother that uses a convolutional neural network supplemented with some\nheuristic rules.", "published": "2022-10-24 16:54:57", "link": "http://arxiv.org/abs/2210.13401v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Few-Shot and Finetuning Performance with Forgetful Causal\n  Language Models", "abstract": "Large language models (LLM) trained using the next-token-prediction\nobjective, such as GPT3 and PaLM, have revolutionized natural language\nprocessing in recent years by showing impressive zero-shot and few-shot\ncapabilities across a wide range of tasks. In this work, we propose a simple\ntechnique that significantly boosts the performance of LLMs without adding\ncomputational cost. Our key observation is that, by performing the next token\nprediction task with randomly selected past tokens masked out, we can improve\nthe quality of the learned representations for downstream language\nunderstanding tasks. We hypothesize that randomly masking past tokens prevents\nover-attending to recent tokens and encourages attention to tokens in the\ndistant past. We find that our method, Forgetful Causal Masking (FCM),\nsignificantly improves both few-shot and finetuning performance of PaLM. We\nfurther consider a simple extension, T-FCM, which introduces bidirectional\ncontext to causal language model without altering the sequence order, and\nfurther improves finetuning performance.", "published": "2022-10-24 17:46:57", "link": "http://arxiv.org/abs/2210.13432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cascading Biases: Investigating the Effect of Heuristic Annotation\n  Strategies on Data and Models", "abstract": "Cognitive psychologists have documented that humans use cognitive heuristics,\nor mental shortcuts, to make quick decisions while expending less effort. While\nperforming annotation work on crowdsourcing platforms, we hypothesize that such\nheuristic use among annotators cascades on to data quality and model\nrobustness. In this work, we study cognitive heuristic use in the context of\nannotating multiple-choice reading comprehension datasets. We propose tracking\nannotator heuristic traces, where we tangibly measure low-effort annotation\nstrategies that could indicate usage of various cognitive heuristics. We find\nevidence that annotators might be using multiple such heuristics, based on\ncorrelations with a battery of psychological tests. Importantly, heuristic use\namong annotators determines data quality along several dimensions: (1) known\nbiased models, such as partial input models, more easily solve examples\nauthored by annotators that rate highly on heuristic use, (2) models trained on\nannotators scoring highly on heuristic use don't generalize as well, and (3)\nheuristic-seeking annotators tend to create qualitatively less challenging\nexamples. Our findings suggest that tracking heuristic usage among annotators\ncan potentially help with collecting challenging datasets and diagnosing model\nbiases.", "published": "2022-10-24 17:52:09", "link": "http://arxiv.org/abs/2210.13439v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form\n  Summarization in the Legal Domain", "abstract": "Existing summarization datasets come with two main drawbacks: (1) They tend\nto focus on overly exposed domains, such as news articles or wiki-like texts,\nand (2) are primarily monolingual, with few multilingual datasets. In this\nwork, we propose a novel dataset, called EUR-Lex-Sum, based on manually curated\ndocument summaries of legal acts from the European Union law platform\n(EUR-Lex). Documents and their respective summaries exist as cross-lingual\nparagraph-aligned data in several of the 24 official European languages,\nenabling access to various cross-lingual and lower-resourced summarization\nsetups. We obtain up to 1,500 document/summary pairs per language, including a\nsubset of 375 cross-lingually aligned legal acts with texts available in all 24\nlanguages. In this work, the data acquisition process is detailed and key\ncharacteristics of the resource are compared to existing summarization\nresources. In particular, we illustrate challenging sub-problems and open\nquestions on the dataset that could help the facilitation of future research in\nthe direction of domain-specific cross-lingual summarization. Limited by the\nextreme length and language diversity of samples, we further conduct\nexperiments with suitable extractive monolingual and cross-lingual baselines\nfor future work. Code for the extraction as well as access to our data and\nbaselines is available online at: https://github.com/achouhan93/eur-lex-sum.", "published": "2022-10-24 17:58:59", "link": "http://arxiv.org/abs/2210.13448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Text Reduction", "abstract": "Producing a reduced version of a source text, as in generic or focused\nsummarization, inherently involves two distinct subtasks: deciding on targeted\ncontent and generating a coherent text conveying it. While some popular\napproaches address summarization as a single end-to-end task, prominent works\nsupport decomposed modeling for individual subtasks. Further, semi-automated\ntext reduction is also very appealing, where users may identify targeted\ncontent while models would generate a corresponding coherent summary.\n  In this paper, we focus on the second subtask, of generating coherent text\ngiven pre-selected content. Concretely, we formalize \\textit{Controlled Text\nReduction} as a standalone task, whose input is a source text with marked spans\nof targeted content (\"highlighting\"). A model then needs to generate a coherent\ntext that includes all and only the target information. We advocate the\npotential of such models, both for modular fully-automatic summarization, as\nwell as for semi-automated human-in-the-loop use cases. Facilitating proper\nresearch, we crowdsource high-quality dev and test datasets for the task.\nFurther, we automatically generate a larger \"silver\" training dataset from\navailable summarization benchmarks, leveraging a pretrained summary-source\nalignment model. Finally, employing these datasets, we present a supervised\nbaseline model, showing promising results and insightful analyses.", "published": "2022-10-24 17:59:03", "link": "http://arxiv.org/abs/2210.13449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ExPUNations: Augmenting Puns with Keywords and Explanations", "abstract": "The tasks of humor understanding and generation are challenging and\nsubjective even for humans, requiring commonsense and real-world knowledge to\nmaster. Puns, in particular, add the challenge of fusing that knowledge with\nthe ability to interpret lexical-semantic ambiguity. In this paper, we present\nthe ExPUNations (ExPUN) dataset, in which we augment an existing dataset of\npuns with detailed crowdsourced annotations of keywords denoting the most\ndistinctive words that make the text funny, pun explanations describing why the\ntext is funny, and fine-grained funniness ratings. This is the first humor\ndataset with such extensive and fine-grained annotations specifically for puns.\nBased on these annotations, we propose two tasks: explanation generation to aid\nwith pun classification and keyword-conditioned pun generation, to challenge\nthe current state-of-the-art natural language understanding and generation\nmodels' ability to understand and generate humor. We showcase that the\nannotated keywords we collect are helpful for generating better novel humorous\ntexts in human evaluation, and that our natural language explanations can be\nleveraged to improve both the accuracy and robustness of humor classifiers.", "published": "2022-10-24 18:12:02", "link": "http://arxiv.org/abs/2210.13513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Situated Pun Generation", "abstract": "Previous work on pun generation commonly begins with a given pun word (a pair\nof homophones for heterographic pun generation and a polyseme for homographic\npun generation) and seeks to generate an appropriate pun. While this may enable\nefficient pun generation, we believe that a pun is most entertaining if it fits\nappropriately within a given context, e.g., a given situation or dialogue. In\nthis work, we propose a new task, context-situated pun generation, where a\nspecific context represented by a set of keywords is provided, and the task is\nto first identify suitable pun words that are appropriate for the context, then\ngenerate puns based on the context keywords and the identified pun words. We\ncollect CUP (Context-sitUated Pun), containing 4.5k tuples of context words and\npun pairs. Based on the new data and setup, we propose a pipeline system for\ncontext-situated pun generation, including a pun word retrieval module that\nidentifies suitable pun words for a given context, and a generation module that\ngenerates puns from context keywords and pun words. Human evaluation shows that\n69% of our top retrieved pun words can be used to generate context-situated\npuns, and our generation module yields successful puns 31% of the time given a\nplausible tuple of context words and pun pair, almost tripling the yield of a\nstate-of-the-art pun generation model. With an end-to-end evaluation, our\npipeline system with the top-1 retrieved pun pair for a given context can\ngenerate successful puns 40% of the time, better than all other modeling\nvariations but 32% lower than the human success rate. This highlights the\ndifficulty of the task, and encourages more research in this direction.", "published": "2022-10-24 18:24:48", "link": "http://arxiv.org/abs/2210.13522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Pre-Training Objectives for Transformer-based Autoencoders", "abstract": "In this paper, we study trade-offs between efficiency, cost and accuracy when\npre-training Transformer encoders with different pre-training objectives. For\nthis purpose, we analyze features of common objectives and combine them to\ncreate new effective pre-training approaches. Specifically, we designed light\ntoken generators based on a straightforward statistical approach, which can\nreplace ELECTRA computationally heavy generators, thus highly reducing cost.\nOur experiments also show that (i) there are more efficient alternatives to\nBERT's MLM, and (ii) it is possible to efficiently pre-train Transformer-based\nmodels using lighter generators without a significant drop in performance.", "published": "2022-10-24 18:39:44", "link": "http://arxiv.org/abs/2210.13536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing Verbatim Short-Term Memory in Neural Language Models", "abstract": "When a language model is trained to predict natural language sequences, its\nprediction at each moment depends on a representation of prior context. What\nkind of information about the prior context can language models retrieve? We\ntested whether language models could retrieve the exact words that occurred\npreviously in a text. In our paradigm, language models (transformers and an\nLSTM) processed English text in which a list of nouns occurred twice. We\noperationalized retrieval as the reduction in surprisal from the first to the\nsecond list. We found that the transformers retrieved both the identity and\nordering of nouns from the first list. Further, the transformers' retrieval was\nmarkedly enhanced when they were trained on a larger corpus and with greater\nmodel depth. Lastly, their ability to index prior tokens was dependent on\nlearned attention patterns. In contrast, the LSTM exhibited less precise\nretrieval, which was limited to list-initial tokens and to short intervening\ntexts. The LSTM's retrieval was not sensitive to the order of nouns and it\nimproved when the list was semantically coherent. We conclude that transformers\nimplemented something akin to a working memory system that could flexibly\nretrieve individual token representations across arbitrary delays; conversely,\nthe LSTM maintained a coarser and more rapidly-decaying semantic gist of prior\ntokens, weighted toward the earliest items.", "published": "2022-10-24 19:47:56", "link": "http://arxiv.org/abs/2210.13569v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs", "abstract": "Knowledge Graph Question Answering (KGQA) involves retrieving entities as\nanswers from a Knowledge Graph (KG) using natural language queries. The\nchallenge is to learn to reason over question-relevant KG facts that traverse\nKG entities and lead to the question answers. To facilitate reasoning, the\nquestion is decoded into instructions, which are dense question representations\nused to guide the KG traversals. However, if the derived instructions do not\nexactly match the underlying KG information, they may lead to reasoning under\nirrelevant context. Our method, termed ReaRev, introduces a new way to KGQA\nreasoning with respect to both instruction decoding and execution. To improve\ninstruction decoding, we perform reasoning in an adaptive manner, where\nKG-aware information is used to iteratively update the initial instructions. To\nimprove instruction execution, we emulate breadth-first search (BFS) with graph\nneural networks (GNNs). The BFS strategy treats the instructions as a set and\nallows our method to decide on their execution order on the fly. Experimental\nresults on three KGQA benchmarks demonstrate the ReaRev's effectiveness\ncompared with previous state-of-the-art, especially when the KG is incomplete\nor when we tackle complex questions. Our code is publicly available at\nhttps://github.com/cmavro/ReaRev_KGQA.", "published": "2022-10-24 23:09:52", "link": "http://arxiv.org/abs/2210.13650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Specializing Multi-domain NMT via Penalizing Low Mutual Information", "abstract": "Multi-domain Neural Machine Translation (NMT) trains a single model with\nmultiple domains. It is appealing because of its efficacy in handling multiple\ndomains within one model. An ideal multi-domain NMT should learn distinctive\ndomain characteristics simultaneously, however, grasping the domain peculiarity\nis a non-trivial task. In this paper, we investigate domain-specific\ninformation through the lens of mutual information (MI) and propose a new\nobjective that penalizes low MI to become higher. Our method achieved the\nstate-of-the-art performance among the current competitive multi-domain NMT\nmodels. Also, we empirically show our objective promotes low MI to be higher\nresulting in domain-specialized multi-domain NMT.", "published": "2022-10-24 01:36:37", "link": "http://arxiv.org/abs/2210.12910v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TIARA: Multi-grained Retrieval for Robust Question Answering over Large\n  Knowledge Bases", "abstract": "Pre-trained language models (PLMs) have shown their effectiveness in multiple\nscenarios. However, KBQA remains challenging, especially regarding coverage and\ngeneralization settings. This is due to two main factors: i) understanding the\nsemantics of both questions and relevant knowledge from the KB; ii) generating\nexecutable logical forms with both semantic and syntactic correctness. In this\npaper, we present a new KBQA model, TIARA, which addresses those issues by\napplying multi-grained retrieval to help the PLM focus on the most relevant KB\ncontexts, viz., entities, exemplary logical forms, and schema items. Moreover,\nconstrained decoding is used to control the output space and reduce generation\nerrors. Experiments over important benchmarks demonstrate the effectiveness of\nour approach. TIARA outperforms previous SOTA, including those using PLMs or\noracle entity annotations, by at least 4.1 and 1.1 F1 points on GrailQA and\nWebQuestionsSP, respectively.", "published": "2022-10-24 02:41:10", "link": "http://arxiv.org/abs/2210.12925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Current Decoding Strategies Capable of Facing the Challenges of\n  Visual Dialogue?", "abstract": "Decoding strategies play a crucial role in natural language generation\nsystems. They are usually designed and evaluated in open-ended text-only tasks,\nand it is not clear how different strategies handle the numerous challenges\nthat goal-oriented multimodal systems face (such as grounding and\ninformativeness). To answer this question, we compare a wide variety of\ndifferent decoding strategies and hyper-parameter configurations in a Visual\nDialogue referential game. Although none of them successfully balance lexical\nrichness, accuracy in the task, and visual grounding, our in-depth analysis\nallows us to highlight the strengths and weaknesses of each decoding strategy.\nWe believe our findings and suggestions may serve as a starting point for\ndesigning more effective decoding algorithms that handle the challenges of\nVisual Dialogue tasks.", "published": "2022-10-24 07:34:39", "link": "http://arxiv.org/abs/2210.12997v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Investigating the detection of Tortured Phrases in Scientific Literature", "abstract": "With the help of online tools, unscrupulous authors can today generate a\npseudo-scientific article and attempt to publish it. Some of these tools work\nby replacing or paraphrasing existing texts to produce new content, but they\nhave a tendency to generate nonsensical expressions. A recent study introduced\nthe concept of 'tortured phrase', an unexpected odd phrase that appears instead\nof the fixed expression. E.g. counterfeit consciousness instead of artificial\nintelligence. The present study aims at investigating how tortured phrases,\nthat are not yet listed, can be detected automatically. We conducted several\nexperiments, including non-neural binary classification, neural binary\nclassification and cosine similarity comparison of the phrase tokens, yielding\nnoticeable results.", "published": "2022-10-24 08:15:22", "link": "http://arxiv.org/abs/2210.13024v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of Automated Metrics for Text Generation Systems", "abstract": "A major challenge in the field of Text Generation is evaluation because we\nlack a sound theory that can be leveraged to extract guidelines for evaluation\ncampaigns. In this work, we propose a first step towards such a theory that\nincorporates different sources of uncertainty, such as imperfect automated\nmetrics and insufficiently sized test sets. The theory has practical\napplications, such as determining the number of samples needed to reliably\ndistinguish the performance of a set of Text Generation systems in a given\nsetting. We showcase the application of the theory on the WMT 21 and\nSpot-The-Bot evaluation data and outline how it can be leveraged to improve the\nevaluation protocol regarding the reliability, robustness, and significance of\nthe evaluation outcome.", "published": "2022-10-24 08:15:28", "link": "http://arxiv.org/abs/2210.13025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Subspace Representations for Soft Set Operations and Sentence\n  Similarities", "abstract": "In the field of natural language processing (NLP), continuous vector\nrepresentations are crucial for capturing the semantic meanings of individual\nwords. Yet, when it comes to the representations of sets of words, the\nconventional vector-based approaches often struggle with expressiveness and\nlack the essential set operations such as union, intersection, and complement.\nInspired by quantum logic, we realize the representation of word sets and\ncorresponding set operations within pre-trained word embedding spaces. By\ngrounding our approach in the linear subspaces, we enable efficient computation\nof various set operations and facilitate the soft computation of membership\nfunctions within continuous spaces. Moreover, we allow for the computation of\nthe F-score directly within word vectors, thereby establishing a direct link to\nthe assessment of sentence similarity. In experiments with widely-used\npre-trained embeddings and benchmarks, we show that our subspace-based set\noperations consistently outperform vector-based ones in both sentence\nsimilarity and set retrieval tasks.", "published": "2022-10-24 08:34:10", "link": "http://arxiv.org/abs/2210.13034v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Unifying Reference Expression Generation and Comprehension", "abstract": "Reference Expression Generation (REG) and Comprehension (REC) are two highly\ncorrelated tasks. Modeling REG and REC simultaneously for utilizing the\nrelation between them is a promising way to improve both. However, the problem\nof distinct inputs, as well as building connections between them in a single\nmodel, brings challenges to the design and training of the joint model. To\naddress the problems, we propose a unified model for REG and REC, named UniRef.\nIt unifies these two tasks with the carefully-designed Image-Region-Text Fusion\nlayer (IRTF), which fuses the image, region and text via the image\ncross-attention and region cross-attention. Additionally, IRTF could generate\npseudo input regions for the REC task to enable a uniform way for sharing the\nidentical representation space across the REC and REG. We further propose\nVision-conditioned Masked Language Modeling (VMLM) and Text-Conditioned Region\nPrediction (TRP) to pre-train UniRef model on multi-granular corpora. The VMLM\nand TRP are directly related to REG and REC, respectively, but could help each\nother. We conduct extensive experiments on three benchmark datasets, RefCOCO,\nRefCOCO+ and RefCOCOg. Experimental results show that our model outperforms\nprevious state-of-the-art methods on both REG and REC.", "published": "2022-10-24 09:53:41", "link": "http://arxiv.org/abs/2210.13076v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multilingual Multimodal Learning with Machine Translated Text", "abstract": "Most vision-and-language pretraining research focuses on English tasks.\nHowever, the creation of multilingual multimodal evaluation datasets (e.g.\nMulti30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality\ntraining data that is both multilingual and multimodal. In this paper, we\ninvestigate whether machine translating English multimodal data can be an\neffective proxy for the lack of readily available multilingual data. We call\nthis framework TD-MML: Translated Data for Multilingual Multimodal Learning,\nand it can be applied to any multimodal dataset and model. We apply it to both\npretraining and fine-tuning data with a state-of-the-art model. In order to\nprevent models from learning from low-quality translated text, we propose two\nmetrics for automatically removing such translations from the resulting\ndatasets. In experiments on five tasks across 20 languages in the IGLUE\nbenchmark, we show that translated data can provide a useful signal for\nmultilingual multimodal learning, both at pretraining and fine-tuning.", "published": "2022-10-24 11:41:20", "link": "http://arxiv.org/abs/2210.13134v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Universal and Independent: Multilingual Probing Framework for Exhaustive\n  Model Interpretation and Evaluation", "abstract": "Linguistic analysis of language models is one of the ways to explain and\ndescribe their reasoning, weaknesses, and limitations. In the probing part of\nthe model interpretability research, studies concern individual languages as\nwell as individual linguistic structures. The question arises: are the detected\nregularities linguistically coherent, or on the contrary, do they dissonate at\nthe typological scale? Moreover, the majority of studies address the inherent\nset of languages and linguistic structures, leaving the actual typological\ndiversity knowledge out of scope. In this paper, we present and apply the\nGUI-assisted framework allowing us to easily probe a massive number of\nlanguages for all the morphosyntactic features present in the Universal\nDependencies data. We show that reflecting the anglo-centric trend in NLP over\nthe past years, most of the regularities revealed in the mBERT model are\ntypical for the western-European languages. Our framework can be integrated\nwith the existing probing toolboxes, model cards, and leaderboards, allowing\npractitioners to use and share their standard probing methods to interpret\nmultilingual models. Thus we propose a toolkit to systematize the multilingual\nflaws in multilingual models, providing a reproducible experimental setup for\n104 languages and 80 morphosyntactic features.\nhttps://github.com/AIRI-Institute/Probing_framework", "published": "2022-10-24 13:41:17", "link": "http://arxiv.org/abs/2210.13236v1", "categories": ["cs.CL", "cs.AI", "68-04, 68-06, 68T50", "G.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Generating Hierarchical Explanations on Text Classification Without\n  Connecting Rules", "abstract": "The opaqueness of deep NLP models has motivated the development of methods\nfor interpreting how deep models predict. Recently, work has introduced\nhierarchical attribution, which produces a hierarchical clustering of words,\nalong with an attribution score for each cluster. However, existing work on\nhierarchical attribution all follows the connecting rule, limiting the cluster\nto a continuous span in the input text. We argue that the connecting rule as an\nadditional prior may undermine the ability to reflect the model decision\nprocess faithfully. To this end, we propose to generate hierarchical\nexplanations without the connecting rule and introduce a framework for\ngenerating hierarchical clusters. Experimental results and further analysis\nshow the effectiveness of the proposed method in providing high-quality\nexplanations for reflecting model predicting process.", "published": "2022-10-24 14:11:23", "link": "http://arxiv.org/abs/2210.13270v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Different Tunes Played with Equal Skill: Exploring a Unified\n  Optimization Subspace for Delta Tuning", "abstract": "Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the\nnew paradigm for using pre-trained language models (PLMs). Up to now, various\nDETs with distinct design elements have been proposed, achieving performance on\npar with fine-tuning. However, the mechanisms behind the above success are\nstill under-explored, especially the connections among various DETs. To fathom\nthe mystery, we hypothesize that the adaptations of different DETs could all be\nreparameterized as low-dimensional optimizations in a unified optimization\nsubspace, which could be found by jointly decomposing independent solutions of\ndifferent DETs. Then we explore the connections among different DETs by\nconducting optimization within the subspace. In experiments, we find that, for\na certain DET, conducting optimization simply in the subspace could achieve\ncomparable performance to its original space, and the found solution in the\nsubspace could be transferred to another DET and achieve non-trivial\nperformance. We also visualize the performance landscape of the subspace and\nfind that there exists a substantial region where different DETs all perform\nwell. Finally, we extend our analysis and show the strong connections between\nfine-tuning and DETs.", "published": "2022-10-24 14:57:35", "link": "http://arxiv.org/abs/2210.13311v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs", "abstract": "Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial. In this\nwork, we examine the open question of social intelligence and Theory of Mind in\nmodern NLP systems from an empirical and theory-based perspective. We show that\none of today's largest language models (GPT-3; Brown et al., 2020) lacks this\nkind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et\nal., 2019), which measures models' ability to understand intents and reactions\nof participants of social interactions, and ToMi (Le et al., 2019), which\nmeasures whether models can infer mental states and realities of participants\nof situations. Our results show that models struggle substantially at these\nTheory of Mind tasks, with well-below-human accuracies of 55% and 60% on\nSocialIQa and ToMi, respectively. To conclude, we draw on theories from\npragmatics to contextualize this shortcoming of large language models, by\nexamining the limitations stemming from their data, neural architecture, and\ntraining paradigms. Challenging the prevalent narrative that only scale is\nneeded, we posit that person-centric NLP approaches might be more effective\ntowards neural Theory of Mind.\n  In our updated version, we also analyze newer instruction tuned and RLFH\nmodels for neural ToM. We find that even ChatGPT and GPT-4 do not display\nemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on\nthe ToMi questions related to mental states and realities.", "published": "2022-10-24 14:58:58", "link": "http://arxiv.org/abs/2210.13312v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Clean Text and Full-Body Transformer: Microsoft's Submission to the\n  WMT22 Shared Task on Sign Language Translation", "abstract": "This paper describes Microsoft's submission to the first shared task on sign\nlanguage translation at WMT 2022, a public competition tackling sign language\nto spoken language translation for Swiss German sign language. The task is very\nchallenging due to data scarcity and an unprecedented vocabulary size of more\nthan 20k words on the target side. Moreover, the data is taken from real\nbroadcast news, includes native signing and covers scenarios of long videos.\nMotivated by recent advances in action recognition, we incorporate full body\ninformation by extracting features from a pre-trained I3D model and applying a\nstandard transformer network. The accuracy of the system is further improved by\napplying careful data cleaning on the target text. We obtain BLEU scores of 0.6\nand 0.78 on the test and dev set respectively, which is the best score among\nthe participants of the shared task. Also in the human evaluation the\nsubmission reaches the first place. The BLEU score is further improved to 1.08\non the dev set by applying features extracted from a lip reading model.", "published": "2022-10-24 15:27:38", "link": "http://arxiv.org/abs/2210.13326v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Augmenting Task-Oriented Dialogue Systems with Relation Extraction", "abstract": "The standard task-oriented dialogue pipeline uses intent classification and\nslot-filling to interpret user utterances. While this approach can handle a\nwide range of queries, it does not extract the information needed to handle\nmore complex queries that contain relationships between slots. We propose\nintegration of relation extraction into this pipeline as an effective way to\nexpand the capabilities of dialogue systems. We evaluate our approach by using\nan internal dataset with slot and relation annotations spanning three domains.\nFinally, we show how slot-filling annotation schemes can be simplified once the\nexpressive power of relation annotations is available, reducing the number of\nslots while still capturing the user's intended meaning.", "published": "2022-10-24 15:49:14", "link": "http://arxiv.org/abs/2210.13344v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "We need to talk about random seeds", "abstract": "Modern neural network libraries all take as a hyperparameter a random seed,\ntypically used to determine the initial state of the model parameters. This\nopinion piece argues that there are some safe uses for random seeds: as part of\nthe hyperparameter search to select a good model, creating an ensemble of\nseveral models, or measuring the sensitivity of the training algorithm to the\nrandom seed hyperparameter. It argues that some uses for random seeds are\nrisky: using a fixed random seed for \"replicability\" and varying only the\nrandom seed to create score distributions for performance comparison. An\nanalysis of 85 recent publications from the ACL Anthology finds that more than\n50% contain risky uses of random seeds.", "published": "2022-10-24 16:48:45", "link": "http://arxiv.org/abs/2210.13393v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classification of Misinformation in New Articles using Natural Language\n  Processing and a Recurrent Neural Network", "abstract": "This paper seeks to address the classification of misinformation in news\narticles using a Long Short Term Memory Recurrent Neural Network. Articles were\ntaken from 2018; a year that was filled with reporters writing about President\nDonald Trump, Special Counsel Robert Mueller, the Fifa World Cup, and Russia.\nThe model presented successfully classifies these articles with an accuracy\nscore of 0.779944. We consider this to be successful because the model was\ntrained on articles that included languages other than English as well as\nincomplete, or fragmented, articles.", "published": "2022-10-24 18:37:42", "link": "http://arxiv.org/abs/2210.13534v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Self-Rationalization Improve Robustness to Spurious Correlations?", "abstract": "Rationalization is fundamental to human reasoning and learning. NLP models\ntrained to produce rationales along with predictions, called\nself-rationalization models, have been investigated for their interpretability\nand utility to end-users. However, the extent to which training with\nhuman-written rationales facilitates learning remains an under-explored\nquestion. We ask whether training models to self-rationalize can aid in their\nlearning to solve tasks for the right reasons. Specifically, we evaluate how\ntraining self-rationalization models with free-text rationales affects\nrobustness to spurious correlations in fine-tuned encoder-decoder and\ndecoder-only models of six different sizes. We evaluate robustness to spurious\ncorrelations by measuring performance on 1) manually annotated challenge\ndatasets and 2) subsets of original test sets where reliance on spurious\ncorrelations would fail to produce correct answers. We find that while\nself-rationalization can improve robustness to spurious correlations in\nlow-resource settings, it tends to hurt robustness in higher-resource settings.\nFurthermore, these effects depend on model family and size, as well as on\nrationale content. Together, our results suggest that explainability can come\nat the cost of robustness; thus, appropriate care should be taken when training\nself-rationalizing models with the goal of creating more trustworthy models.", "published": "2022-10-24 19:54:57", "link": "http://arxiv.org/abs/2210.13575v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LANS: Large-scale Arabic News Summarization Corpus", "abstract": "Text summarization has been intensively studied in many languages, and some\nlanguages have reached advanced stages. Yet, Arabic Text Summarization (ATS) is\nstill in its developing stages. Existing ATS datasets are either small or lack\ndiversity. We build, LANS, a large-scale and diverse dataset for Arabic Text\nSummarization task. LANS offers 8.4 million articles and their summaries\nextracted from newspapers websites metadata between 1999 and 2019. The\nhigh-quality and diverse summaries are written by journalists from 22 major\nArab newspapers, and include an eclectic mix of at least more than 7 topics\nfrom each source. We conduct an intrinsic evaluation on LANS by both automatic\nand human evaluations. Human evaluation of 1000 random samples reports 95.4%\naccuracy for our collected summaries, and automatic evaluation quantifies the\ndiversity and abstractness of the summaries. The dataset is publicly available\nupon request.", "published": "2022-10-24 20:54:01", "link": "http://arxiv.org/abs/2210.13600v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapters for Enhanced Modeling of Multilingual Knowledge and Text", "abstract": "Large language models appear to learn facts from the large text corpora they\nare trained on. Such facts are encoded implicitly within their many parameters,\nmaking it difficult to verify or manipulate what knowledge has been learned.\nLanguage models have recently been extended to multilingual language models\n(MLLMs), enabling knowledge to be learned across hundreds of languages.\nMeanwhile, knowledge graphs contain facts in an explicit triple format, which\nrequire careful and costly curation and are only available in a few\nhigh-resource languages, restricting their research and application. To address\nthese issues, we propose to enhance MLLMs with knowledge from multilingual\nknowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks\nacross many languages, including low-resource ones. Specifically, we introduce\na lightweight adapter set to enhance MLLMs with cross-lingual entity alignment\nand facts from MLKGs for many languages. Experiments on common benchmarks show\nthat such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable\nor improved performance for knowledge graph completion and entity alignment\nrelative to baselines, especially for low-resource languages (for which\nknowledge graphs are unavailable); and (2) improved MLLM performance on\nlanguage understanding tasks that require multilingual factual knowledge; all\nwhile maintaining performance on other general language tasks.", "published": "2022-10-24 21:33:42", "link": "http://arxiv.org/abs/2210.13617v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VLC-BERT: Visual Question Answering with Contextualized Commonsense\n  Knowledge", "abstract": "There has been a growing interest in solving Visual Question Answering (VQA)\ntasks that require the model to reason beyond the content present in the image.\nIn this work, we focus on questions that require commonsense reasoning. In\ncontrast to previous methods which inject knowledge from static knowledge\nbases, we investigate the incorporation of contextualized knowledge using\nCommonsense Transformer (COMET), an existing knowledge model trained on\nhuman-curated knowledge bases. We propose a method to generate, select, and\nencode external commonsense knowledge alongside visual and textual cues in a\nnew pre-trained Vision-Language-Commonsense transformer model, VLC-BERT.\nThrough our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets,\nwe show that VLC-BERT is capable of outperforming existing models that utilize\nstatic knowledge bases. Furthermore, through a detailed analysis, we explain\nwhich questions benefit, and which don't, from contextualized commonsense\nknowledge from COMET.", "published": "2022-10-24 22:01:17", "link": "http://arxiv.org/abs/2210.13626v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Efficiently Trained Low-Resource Mongolian Text-to-Speech System Based\n  On FullConv-TTS", "abstract": "Recurrent Neural Networks (RNNs) have become the standard modeling technique\nfor sequence data, and are used in a number of novel text-to-speech models.\nHowever, training a TTS model including RNN components has certain requirements\nfor GPU performance and takes a long time. In contrast, studies have shown that\nCNN-based sequence synthesis technology can greatly reduce training time in\ntext-to-speech models while ensuring a certain performance due to its high\nparallelism. We propose a new text-to-speech system based on deep convolutional\nneural networks that does not employ any RNN components (recurrent units). At\nthe same time, we improve the generality and robustness of our model through a\nseries of data augmentation methods such as Time Warping, Frequency Mask, and\nTime Mask. The final experimental results show that the TTS model using only\nthe CNN component can reduce the training time compared to the classic TTS\nmodels such as Tacotron while ensuring the quality of the synthesized speech.", "published": "2022-10-24 14:18:43", "link": "http://arxiv.org/abs/2211.01948v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Machine Learning Approach to Classifying Construction Cost Documents\n  into the International Construction Measurement Standard", "abstract": "We introduce the first automated models for classifying natural language\ndescriptions provided in cost documents called \"Bills of Quantities\" (BoQs)\npopular in the infrastructure construction industry, into the International\nConstruction Measurement Standard (ICMS). The models we deployed and\nsystematically evaluated for multi-class text classification are learnt from a\ndataset of more than 50 thousand descriptions of items retrieved from 24 large\ninfrastructure construction projects across the United Kingdom. We describe our\napproach to language representation and subsequent modelling to examine the\nstrength of contextual semantics and temporal dependency of language used in\nconstruction project documentation. To do that we evaluate two experimental\npipelines to inferring ICMS codes from text, on the basis of two different\nlanguage representation models and a range of state-of-the-art sequence-based\nclassification methods, including recurrent and convolutional neural network\narchitectures. The findings indicate a highly effective and accurate ICMS\nautomation model is within reach, with reported accuracy results above 90% F1\nscore on average, on 32 ICMS categories. Furthermore, due to the specific\nnature of language use in the BoQs text; short, largely descriptive and\ntechnical, we find that simpler models compare favourably to achieving higher\naccuracy results. Our analysis suggest that information is more likely embedded\nin local key features in the descriptive text, which explains why a simpler\ngeneric temporal convolutional network (TCN) exhibits comparable memory to\nrecurrent architectures with the same capacity, and subsequently outperforms\nthese at this task.", "published": "2022-10-24 11:35:53", "link": "http://arxiv.org/abs/2211.07705v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating self-supervised, weakly supervised and fully supervised\n  training approaches for multi-domain automatic speech recognition: a study on\n  Bangladeshi Bangla", "abstract": "Despite huge improvements in automatic speech recognition (ASR) employing\nneural networks, ASR systems still suffer from a lack of robustness and\ngeneralizability issues due to domain shifting. This is mainly because\nprincipal corpus design criteria are often not identified and examined\nadequately while compiling ASR datasets. In this study, we investigate the\nrobustness of the state-of-the-art transfer learning approaches such as\nself-supervised wav2vec 2.0 and weakly supervised Whisper as well as fully\nsupervised convolutional neural networks (CNNs) for multi-domain ASR. We also\ndemonstrate the significance of domain selection while building a corpus by\nassessing these models on a novel multi-domain Bangladeshi Bangla ASR\nevaluation benchmark - BanSpeech, which contains approximately 6.52 hours of\nhuman-annotated speech and 8085 utterances from 13 distinct domains. SUBAK.KO,\na mostly read speech corpus for the morphologically rich language Bangla, has\nbeen used to train the ASR systems. Experimental evaluation reveals that\nself-supervised cross-lingual pre-training is the best strategy compared to\nweak supervision and full supervision to tackle the multi-domain ASR task.\nMoreover, the ASR models trained on SUBAK.KO face difficulty recognizing speech\nfrom domains with mostly spontaneous speech. The BanSpeech will be publicly\navailable to meet the need for a challenging evaluation benchmark for Bangla\nASR.", "published": "2022-10-24 02:18:03", "link": "http://arxiv.org/abs/2210.12921v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Finding Memo: Extractive Memorization in Constrained Sequence Generation\n  Tasks", "abstract": "Memorization presents a challenge for several constrained Natural Language\nGeneration (NLG) tasks such as Neural Machine Translation (NMT), wherein the\nproclivity of neural models to memorize noisy and atypical samples reacts\nadversely with the noisy (web crawled) datasets. However, previous studies of\nmemorization in constrained NLG tasks have only focused on counterfactual\nmemorization, linking it to the problem of hallucinations. In this work, we\npropose a new, inexpensive algorithm for extractive memorization (exact\ntraining data generation under insufficient context) in constrained sequence\ngeneration tasks and use it to study extractive memorization and its effects in\nNMT. We demonstrate that extractive memorization poses a serious threat to NMT\nreliability by qualitatively and quantitatively characterizing the memorized\nsamples as well as the model behavior in their vicinity. Based on empirical\nobservations, we develop a simple algorithm which elicits non-memorized\ntranslations of memorized samples from the same model, for a large fraction of\nsuch samples. Finally, we show that the proposed algorithm could also be\nleveraged to mitigate memorization in the model through finetuning. We have\nreleased the code to reproduce our results at\nhttps://github.com/vyraun/Finding-Memo.", "published": "2022-10-24 03:01:52", "link": "http://arxiv.org/abs/2210.12929v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Label Consistency on Document-level Named Entity Recognition", "abstract": "Named entity recognition (NER) is a fundamental part of extracting\ninformation from documents in biomedical applications. A notable advantage of\nNER is its consistency in extracting biomedical entities in a document context.\nAlthough existing document NER models show consistent predictions, they still\ndo not meet our expectations. We investigated whether the adjectives and\nprepositions within an entity cause a low label consistency, which results in\ninconsistent predictions. In this paper, we present our method, ConNER, which\nenhances the label dependency of modifiers (e.g., adjectives and prepositions)\nto achieve higher label agreement. ConNER refines the draft labels of the\nmodifiers to improve the output representations of biomedical entities. The\neffectiveness of our method is demonstrated on four popular biomedical NER\ndatasets; in particular, its efficacy is proved on two datasets with 7.5-8.6%\nabsolute improvements in the F1 score. We interpret that our ConNER method is\neffective on datasets that have intrinsically low label consistency. In the\nqualitative analysis, we demonstrate how our approach makes the NER model\ngenerate consistent predictions. Our code and resources are available at\nhttps://github.com/dmis-lab/ConNER/.", "published": "2022-10-24 04:45:17", "link": "http://arxiv.org/abs/2210.12949v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Information Change in Science Communication with Semantically\n  Matched Paraphrases", "abstract": "Whether the media faithfully communicate scientific information has long been\na core issue to the science community. Automatically identifying paraphrased\nscientific findings could enable large-scale tracking and analysis of\ninformation changes in the science communication process, but this requires\nsystems to understand the similarity between scientific information across\nmultiple domains. To this end, we present the SCIENTIFIC PARAPHRASE AND\nINFORMATION CHANGE DATASET (SPICED), the first paraphrase dataset of scientific\nfindings annotated for degree of information change. SPICED contains 6,000\nscientific finding pairs extracted from news stories, social media discussions,\nand full texts of original papers. We demonstrate that SPICED poses a\nchallenging task and that models trained on SPICED improve downstream\nperformance on evidence retrieval for fact checking of real-world scientific\nclaims. Finally, we show that models trained on SPICED can reveal large-scale\ntrends in the degrees to which people and organizations faithfully communicate\nnew scientific findings. Data, code, and pre-trained models are available at\nhttp://www.copenlu.com/publication/2022_emnlp_wright/.", "published": "2022-10-24 07:44:38", "link": "http://arxiv.org/abs/2210.13001v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-supervised Rewiring of Pre-trained Speech Encoders: Towards Faster\n  Fine-tuning with Less Labels in Speech Processing", "abstract": "Pre-trained speech Transformers have facilitated great success across various\nspeech processing tasks. However, fine-tuning these encoders for downstream\ntasks require sufficiently large training data to converge or to achieve\nstate-of-the-art. In text domain this has been partly attributed to\nsub-optimality of the representation space in pre-trained Transformers. In this\nwork, we take a sober look into pre-trained speech encoders and rewire their\nrepresentation space without requiring any task-specific labels. Our method\nutilises neutrally synthesised version of audio inputs along with frame masking\nto construct positive pairs for contrastive self-supervised learning. When used\nfor augmenting the wav2vec 2 encoder, we observe consistent improvement of\nisotropy in the representation space. Our experiments on 6 speech processing\ntasks, exhibit a significant convergence speedup during task fine-tuning as\nwell as consistent task improvement, specially in low-resource settings.", "published": "2022-10-24 08:27:09", "link": "http://arxiv.org/abs/2210.13030v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A partial order view of message-passing communication models", "abstract": "There is a wide variety of message-passing communication models, ranging from\nsynchronous ''rendez-vous'' communications to fully asynchronous/out-of-order\ncommunications. For large-scale distributed systems, the communication model is\ndetermined by the transport layer of the network, and a few classes of orders\nof message delivery (FIFO, causally ordered) have been identified in the early\ndays of distributed computing. For local-scale message-passing applications,\ne.g., running on a single machine, the communication model may be determined by\nthe actual implementation of message buffers and by how FIFO queues are used.\nWhile large-scale communication models, such as causal ordering, are defined by\nlogical axioms, local-scale models are often defined by an operational\nsemantics. In this work, we connect these two approaches, and we present a\nunified hierarchy of communication models encompassing both large-scale and\nlocal-scale models, based on their concurrent behaviors. We also show that all\nthe communication models we consider can be axiomatized in the monadic second\norder logic, and may therefore benefit from several bounded verification\ntechniques based on bounded special treewidth.", "published": "2022-10-24 09:31:25", "link": "http://arxiv.org/abs/2210.13062v2", "categories": ["cs.CL", "cs.FL", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Unsupervised Term Extraction for Highly Technical Domains", "abstract": "Term extraction is an information extraction task at the root of knowledge\ndiscovery platforms. Developing term extractors that are able to generalize\nacross very diverse and potentially highly technical domains is challenging, as\nannotations for domains requiring in-depth expertise are scarce and expensive\nto obtain. In this paper, we describe the term extraction subsystem of a\ncommercial knowledge discovery platform that targets highly technical fields\nsuch as pharma, medical, and material science. To be able to generalize across\ndomains, we introduce a fully unsupervised annotator (UA). It extracts terms by\ncombining novel morphological signals from sub-word tokenization with\nterm-to-topic and intra-term similarity metrics, computed using general-domain\npre-trained sentence-encoders. The annotator is used to implement a\nweakly-supervised setup, where transformer-models are fine-tuned (or\npre-trained) over the training data generated by running the UA over large\nunlabeled corpora. Our experiments demonstrate that our setup can improve the\npredictive performance while decreasing the inference latency on both CPUs and\nGPUs. Our annotators provide a very competitive baseline for all the cases\nwhere annotations are not available.", "published": "2022-10-24 11:08:09", "link": "http://arxiv.org/abs/2210.13118v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Proficiency assessment of L2 spoken English using wav2vec 2.0", "abstract": "The increasing demand for learning English as a second language has led to a\ngrowing interest in methods for automatically assessing spoken language\nproficiency. Most approaches use hand-crafted features, but their efficacy\nrelies on their particular underlying assumptions and they risk discarding\npotentially salient information about proficiency. Other approaches rely on\ntranscriptions produced by ASR systems which may not provide a faithful\nrendition of a learner's utterance in specific scenarios (e.g., non-native\nchildren's spontaneous speech). Furthermore, transcriptions do not yield any\ninformation about relevant aspects such as intonation, rhythm or prosody. In\nthis paper, we investigate the use of wav2vec 2.0 for assessing overall and\nindividual aspects of proficiency on two small datasets, one of which is\npublicly available. We find that this approach significantly outperforms the\nBERT-based baseline system trained on ASR and manual transcriptions used for\ncomparison.", "published": "2022-10-24 12:36:49", "link": "http://arxiv.org/abs/2210.13168v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Real-time Speech Interruption Analysis: From Cloud to Client Deployment", "abstract": "Meetings are an essential form of communication for all types of\norganizations, and remote collaboration systems have been much more widely used\nsince the COVID-19 pandemic. One major issue with remote meetings is that it is\nchallenging for remote participants to interrupt and speak. We have recently\ndeveloped the first speech interruption analysis model, which detects failed\nspeech interruptions, shows very promising performance, and is being deployed\nin the cloud. To deliver this feature in a more cost-efficient and\nenvironment-friendly way, we reduced the model complexity and size to ship the\nWavLM_SI model in client devices. In this paper, we first describe how we\nsuccessfully improved the True Positive Rate (TPR) at a 1% False Positive Rate\n(FPR) from 50.9% to 68.3% for the failed speech interruption detection model by\ntraining on a larger dataset and fine-tuning. We then shrank the model size\nfrom 222.7 MB to 9.3 MB with an acceptable loss in accuracy and reduced the\ncomplexity from 31.2 GMACS (Giga Multiply-Accumulate Operations per Second) to\n4.3 GMACS. We also estimated the environmental impact of the complexity\nreduction, which can be used as a general guideline for large Transformer-based\nmodels, and thus make those models more accessible with less computation\noverhead.", "published": "2022-10-24 15:39:51", "link": "http://arxiv.org/abs/2210.13334v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition", "abstract": "Speech recognition applications cover a range of different audio and text\ndistributions, with different speaking styles, background noise, transcription\npunctuation and character casing. However, many speech recognition systems\nrequire dataset-specific tuning (audio filtering, punctuation removal and\nnormalisation of casing), therefore assuming a-priori knowledge of both the\naudio and text distributions. This tuning requirement can lead to systems\nfailing to generalise to other datasets and domains. To promote the development\nof multi-domain speech systems, we introduce the End-to-end Speech Benchmark\n(ESB) for evaluating the performance of a single automatic speech recognition\n(ASR) system across a broad set of speech datasets. Benchmarked systems must\nuse the same data pre- and post-processing algorithm across datasets - assuming\nthe audio and text data distributions are a-priori unknown. We compare a series\nof state-of-the-art (SoTA) end-to-end (E2E) systems on this benchmark,\ndemonstrating how a single speech system can be applied and evaluated on a wide\nrange of data distributions. We find E2E systems to be effective across\ndatasets: in a fair comparison, E2E systems achieve within 2.6% of SoTA systems\ntuned to a specific dataset. Our analysis reveals that transcription artefacts,\nsuch as punctuation and casing, pose difficulties for ASR systems and should be\nincluded in evaluation. We believe E2E benchmarking over a range of datasets\npromotes the research of multi-domain speech recognition systems. ESB is\navailable at https://huggingface.co/esb.", "published": "2022-10-24 15:58:48", "link": "http://arxiv.org/abs/2210.13352v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emergent World Representations: Exploring a Sequence Model Trained on a\n  Synthetic Task", "abstract": "Language models show a surprising range of capabilities, but the source of\ntheir apparent competence is unclear. Do these networks just memorize a\ncollection of surface statistics, or do they rely on internal representations\nof the process that generates the sequences they see? We investigate this\nquestion by applying a variant of the GPT model to the task of predicting legal\nmoves in a simple board game, Othello. Although the network has no a priori\nknowledge of the game or its rules, we uncover evidence of an emergent\nnonlinear internal representation of the board state. Interventional\nexperiments indicate this representation can be used to control the output of\nthe network and create \"latent saliency maps\" that can help explain predictions\nin human terms.", "published": "2022-10-24 16:29:55", "link": "http://arxiv.org/abs/2210.13382v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Development of Hybrid ASR Systems for Low Resource Medical Domain\n  Conversational Telephone Speech", "abstract": "Language barriers present a great challenge in our increasingly connected and\nglobal world. Especially within the medical domain, e.g. hospital or emergency\nroom, communication difficulties and delays may lead to malpractice and\nnon-optimal patient care. In the HYKIST project, we consider patient-physician\ncommunication, more specifically between a German-speaking physician and an\nArabic- or Vietnamese-speaking patient. Currently, a doctor can call the\nTriaphon service to get assistance from an interpreter in order to help\nfacilitate communication. The HYKIST goal is to support the usually\nnon-professional bilingual interpreter with an automatic speech translation\nsystem to improve patient care and help overcome language barriers. In this\nwork, we present our ASR system development efforts for this conversational\ntelephone speech translation task in the medical domain for two languages\npairs, data collection, various acoustic model architectures and\ndialect-induced difficulties.", "published": "2022-10-24 16:49:19", "link": "http://arxiv.org/abs/2210.13397v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speeding Up Question Answering Task of Language Models via Inverted\n  Index", "abstract": "Natural language processing applications, such as conversational agents and\ntheir question-answering capabilities, are widely used in the real world.\nDespite the wide popularity of large language models (LLMs), few real-world\nconversational agents take advantage of LLMs. Extensive resources consumed by\nLLMs disable developers from integrating them into end-user applications. In\nthis study, we leverage an inverted indexing mechanism combined with LLMs to\nimprove the efficiency of question-answering models for closed-domain\nquestions. Our experiments show that using the index improves the average\nresponse time by 97.44%. In addition, due to the reduced search scope, the\naverage BLEU score improved by 0.23 while using the inverted index.", "published": "2022-10-24 19:59:17", "link": "http://arxiv.org/abs/2210.13578v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Long-Term Citations from Short-Term Linguistic Influence", "abstract": "A standard measure of the influence of a research paper is the number of\ntimes it is cited. However, papers may be cited for many reasons, and citation\ncount offers limited information about the extent to which a paper affected the\ncontent of subsequent publications. We therefore propose a novel method to\nquantify linguistic influence in timestamped document collections. There are\ntwo main steps: first, identify lexical and semantic changes using contextual\nembeddings and word frequencies; second, aggregate information about these\nchanges into per-document influence scores by estimating a high-dimensional\nHawkes process with a low-rank parameter matrix. We show that this measure of\nlinguistic influence is predictive of $\\textit{future}$ citations: the estimate\nof linguistic influence from the two years after a paper's publication is\ncorrelated with and predictive of its citation count in the following three\nyears. This is demonstrated using an online evaluation with incremental\ntemporal training/test splits, in comparison with a strong baseline that\nincludes predictors for initial citation counts, topics, and lexical features.", "published": "2022-10-24 22:03:26", "link": "http://arxiv.org/abs/2210.13628v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Toward an Intelligent Tutoring System for Argument Mining in Legal Texts", "abstract": "We propose an adaptive environment (CABINET) to support caselaw analysis\n(identifying key argument elements) based on a novel cognitive computing\nframework that carefully matches various machine learning (ML) capabilities to\nthe proficiency of a user. CABINET supports law students in their learning as\nwell as professionals in their work. The results of our experiments focused on\nthe feasibility of the proposed framework are promising. We show that the\nsystem is capable of identifying a potential error in the analysis with very\nlow false positives rate (2.0-3.5%), as well as of predicting the key argument\nelement type (e.g., an issue or a holding) with a reasonably high F1-score\n(0.74).", "published": "2022-10-24 22:31:02", "link": "http://arxiv.org/abs/2210.13635v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Joint Training Really Help Cascaded Speech Translation?", "abstract": "Currently, in speech translation, the straightforward approach - cascading a\nrecognition system with a translation system - delivers state-of-the-art\nresults. However, fundamental challenges such as error propagation from the\nautomatic speech recognition system still remain. To mitigate these problems,\nrecently, people turn their attention to direct data and propose various joint\ntraining methods. In this work, we seek to answer the question of whether joint\ntraining really helps cascaded speech translation. We review recent papers on\nthe topic and also investigate a joint training criterion by marginalizing the\ntranscription posterior probabilities. Our findings show that a strong cascaded\nbaseline can diminish any improvements obtained using joint training, and we\nsuggest alternatives to joint training. We hope this work can serve as a\nrefresher of the current speech translation landscape, and motivate research in\nfinding more efficient and creative ways to utilize the direct data for speech\ntranslation.", "published": "2022-10-24 12:40:38", "link": "http://arxiv.org/abs/2210.13700v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Cards Against AI: Predicting Humor in a Fill-in-the-blank Party Game", "abstract": "Humor is an inherently social phenomenon, with humorous utterances shaped by\nwhat is socially and culturally accepted. Understanding humor is an important\nNLP challenge, with many applications to human-computer interactions. In this\nwork we explore humor in the context of Cards Against Humanity -- a party game\nwhere players complete fill-in-the-blank statements using cards that can be\noffensive or politically incorrect. We introduce a novel dataset of 300,000\nonline games of Cards Against Humanity, including 785K unique jokes, analyze it\nand provide insights. We trained machine learning models to predict the winning\njoke per game, achieving performance twice as good (20\\%) as random, even\nwithout any user information. On the more difficult task of judging novel\ncards, we see the models' ability to generalize is moderate. Interestingly, we\nfind that our models are primarily focused on punchline card, with the context\nhaving little impact. Analyzing feature importance, we observe that short,\ncrude, juvenile punchlines tend to win.", "published": "2022-10-24 08:05:21", "link": "http://arxiv.org/abs/2210.13016v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.GL", "68T01, 68T50", "I.2.7; I.2; K.4; J.4; J.5"], "primary_category": "cs.LG"}
{"title": "Computational Inference in Cognitive Science: Operational, Societal and\n  Ethical Considerations", "abstract": "Emerging research frontiers and computational advances have gradually\ntransformed cognitive science into a multidisciplinary and data-driven field.\nAs a result, there is a proliferation of cognitive theories investigated and\ninterpreted from different academic lens and in different levels of\nabstraction. We formulate this applied aspect of this challenge as the\ncomputational cognitive inference, and describe the major routes of\ncomputational approaches. To balance the potential optimism alongside the speed\nand scale of the data-driven era of cognitive science, we propose to inspect\nthis trend in more empirical terms by identifying the operational challenges,\nsocietal impacts and ethical guidelines in conducting research and interpreting\nresults from the computational inference in cognitive science.", "published": "2022-10-24 18:27:27", "link": "http://arxiv.org/abs/2210.13526v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "Reinforcement Learning and Bandits for Speech and Language Processing:\n  Tutorial, Review and Outlook", "abstract": "In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.", "published": "2022-10-24 21:49:12", "link": "http://arxiv.org/abs/2210.13623v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens", "abstract": "In this paper, we present TridentSE, a novel architecture for speech\nenhancement, which is capable of efficiently capturing both global information\nand local details. TridentSE maintains T-F bin level representation to capture\ndetails, and uses a small number of global tokens to process the global\ninformation. Information is propagated between the local and the global\nrepresentations through cross attention modules. To capture both inter- and\nintra-frame information, the global tokens are divided into two groups to\nprocess along the time and the frequency axis respectively. A metric\ndiscriminator is further employed to guide our model to achieve higher\nperceptual quality. Even with significantly lower computational cost, TridentSE\noutperforms a variety of previous speech enhancement methods, achieving a PESQ\nof 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test\nset. Visualization shows that the global tokens learn diverse and interpretable\nglobal patterns.", "published": "2022-10-24 07:30:42", "link": "http://arxiv.org/abs/2210.12995v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "10 hours data is all you need", "abstract": "We propose a novel procedure to generate pseudo mandarin speech data named as\nCAMP (character audio mix up), which aims at generating audio from a character\nscale. We also raise a method for building a mandarin character scale audio\ndatabase adaptive to CAMP named as META-AUDIO, which makes full use of audio\ndata and can greatly increase the data diversity of the database. Experiments\nshow that our CAMP method is simple and quite effective. For example, we train\nmodels with 10 hours of audio data in AISHELL-1 and pseudo audio data generated\nby CAMP, and achieve a competitive 11.07 character error rate (CER). Besides,\nwe also perform training with only 10 hours of audio data in AIDATATANG dataset\nand pseudo audio data generated by CAMP, which again achieves a competitive\n8.26 CER.", "published": "2022-10-24 09:34:05", "link": "http://arxiv.org/abs/2210.13067v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Weak-Supervised Dysarthria-invariant Features for Spoken Language\n  Understanding using an FHVAE and Adversarial Training", "abstract": "The scarcity of training data and the large speaker variation in dysarthric\nspeech lead to poor accuracy and poor speaker generalization of spoken language\nunderstanding systems for dysarthric speech. Through work on the speech\nfeatures, we focus on improving the model generalization ability with limited\ndysarthric data. Factorized Hierarchical Variational Auto-Encoders (FHVAE)\ntrained unsupervisedly have shown their advantage in disentangling content and\nspeaker representations. Earlier work showed that the dysarthria shows in both\nfeature vectors. Here, we add adversarial training to bridge the gap between\nthe control and dysarthric speech data domains. We extract dysarthric and\nspeaker invariant features using weak supervision. The extracted features are\nevaluated on a Spoken Language Understanding task and yield a higher accuracy\non unseen speakers with more severe dysarthria compared to features from the\nbasic FHVAE model or plain filterbanks.", "published": "2022-10-24 11:57:08", "link": "http://arxiv.org/abs/2210.13144v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Brouhaha: multi-task training for voice activity detection,\n  speech-to-noise ratio, and C50 room acoustics estimation", "abstract": "Most automatic speech processing systems register degraded performance when\napplied to noisy or reverberant speech. But how can one tell whether speech is\nnoisy or reverberant? We propose Brouhaha, a neural network jointly trained to\nextract speech/non-speech segments, speech-to-noise ratios, and C50room\nacoustics from single-channel recordings. Brouhaha is trained using a\ndata-driven approach in which noisy and reverberant audio segments are\nsynthesized. We first evaluate its performance and demonstrate that the\nproposed multi-task regime is beneficial. We then present two scenarios\nillustrating how Brouhaha can be used on naturally noisy and reverberant data:\n1) to investigate the errors made by a speaker diarization model\n(pyannote.audio); and 2) to assess the reliability of an automatic speech\nrecognition model (Whisper from OpenAI). Both our pipeline and a pretrained\nmodel are open source and shared with the speech community.", "published": "2022-10-24 13:47:36", "link": "http://arxiv.org/abs/2210.13248v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Acoustical Machine Learning Approach to Determine Abrasive Belt Wear\n  of Wide Belt Sanders", "abstract": "This paper describes a machine learning approach to determine the abrasive\nbelt wear of wide belt sanders used in industrial processes based on acoustic\ndata, regardless of the sanding process-related parameters, Feed speed, Grit\nSize, and Type of material. Our approach utilizes Decision Tree, Random Forest,\nk-nearest Neighbors, and Neural network Classifiers to detect the belt wear\nfrom Spectrograms, Mel Spectrograms, MFCC, IMFCC, and LFCC, yielding an\naccuracy of up to 86.1% on five levels of belt wear. A 96% accuracy could be\nachieved with different Decision Tree Classifiers specialized in different\nsanding parameter configurations. The classifiers could also determine with an\naccuracy of 97% if the machine is currently sanding or is idle and with an\naccuracy of 98.4% and 98.8% detect the sanding parameters Feed speed and Grit\nSize. We can show that low-dimensional mappings of high-dimensional features\ncan be used to visualize belt wear and sanding parameters meaningfully.", "published": "2022-10-24 14:12:42", "link": "http://arxiv.org/abs/2210.13273v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition", "abstract": "It has been shown that the intelligibility of noisy speech can be improved by\nspeech enhancement algorithms. However, speech enhancement has not been\nestablished as an effective frontend for robust automatic speech recognition\n(ASR) in noisy conditions compared to an ASR model trained on noisy speech\ndirectly. The divide between speech enhancement and ASR impedes the progress of\nrobust ASR systems especially as speech enhancement has made big strides in\nrecent years. In this work, we focus on eliminating this divide with an ARN\n(attentive recurrent network) based time-domain enhancement model. The proposed\nsystem fully decouples speech enhancement and an acoustic model trained only on\nclean speech. Results on the CHiME-2 corpus show that ARN enhanced speech\ntranslates to improved ASR results. The proposed system achieves $6.28\\%$\naverage word error rate, outperforming the previous best by $19.3\\%$\nrelatively.", "published": "2022-10-24 15:13:22", "link": "http://arxiv.org/abs/2210.13318v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spectral Clustering-aware Learning of Embeddings for Speaker Diarisation", "abstract": "In speaker diarisation, speaker embedding extraction models often suffer from\nthe mismatch between their training loss functions and the speaker clustering\nmethod. In this paper, we propose the method of spectral clustering-aware\nlearning of embeddings (SCALE) to address the mismatch. Specifically, besides\nan angular prototype cal (AP) loss, SCALE uses a novel affinity matrix loss\nwhich directly minimises the error between the affinity matrix estimated from\nspeaker embeddings and the reference. SCALE also includes p-percentile\nthresholding and Gaussian blur as two important hyper-parameters for spectral\nclustering in training. Experiments on the AMI dataset showed that speaker\nembeddings obtained with SCALE achieved over 50% relative speaker error rate\nreductions using oracle segmentation, and over 30% relative diarisation error\nrate reductions using automatic segmentation when compared to a strong baseline\nwith the AP-loss-based speaker embeddings.", "published": "2022-10-24 19:55:07", "link": "http://arxiv.org/abs/2210.13576v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Novel Frame Structure for Cloud-Based Audio-Visual Speech Enhancement\n  in Multimodal Hearing-aids", "abstract": "In this paper, we design a first of its kind transceiver (PHY layer)\nprototype for cloud-based audio-visual (AV) speech enhancement (SE) complying\nwith high data rate and low latency requirements of future multimodal hearing\nassistive technology. The innovative design needs to meet multiple challenging\nconstraints including up/down link communications, delay of transmission and\nsignal processing, and real-time AV SE models processing. The transceiver\nincludes device detection, frame detection, frequency offset estimation, and\nchannel estimation capabilities. We develop both uplink (hearing aid to the\ncloud) and downlink (cloud to hearing aid) frame structures based on the data\nrate and latency requirements. Due to the varying nature of uplink information\n(audio and lip-reading), the uplink channel supports multiple data rate frame\nstructure, while the downlink channel has a fixed data rate frame structure. In\naddition, we evaluate the latency of different PHY layer blocks of the\ntransceiver for developed frame structures using LabVIEW NXG. This can be used\nwith software defined radio (such as Universal Software Radio Peripheral) for\nreal-time demonstration scenarios.", "published": "2022-10-24 11:23:35", "link": "http://arxiv.org/abs/2210.13127v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "High Fidelity Neural Audio Compression", "abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec\nleveraging neural networks. It consists in a streaming encoder-decoder\narchitecture with quantized latent space trained in an end-to-end fashion. We\nsimplify and speed-up the training by using a single multiscale spectrogram\nadversary that efficiently reduces artifacts and produce high-quality samples.\nWe introduce a novel loss balancer mechanism to stabilize training: the weight\nof a loss now defines the fraction of the overall gradient it should represent,\nthus decoupling the choice of this hyper-parameter from the typical scale of\nthe loss. Finally, we study how lightweight Transformer models can be used to\nfurther compress the obtained representation by up to 40%, while staying faster\nthan real time. We provide a detailed description of the key design choices of\nthe proposed model including: training objective, architectural changes and a\nstudy of various perceptual loss functions. We present an extensive subjective\nevaluation (MUSHRA tests) together with an ablation study for a range of\nbandwidths and audio domains, including speech, noisy-reverberant speech, and\nmusic. Our approach is superior to the baselines methods across all evaluated\nsettings, considering both 24 kHz monophonic and 48 kHz stereophonic audio.\nCode and models are available at github.com/facebookresearch/encodec.", "published": "2022-10-24 17:52:02", "link": "http://arxiv.org/abs/2210.13438v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "I see what you hear: a vision-inspired method to localize words", "abstract": "This paper explores the possibility of using visual object detection\ntechniques for word localization in speech data. Object detection has been\nthoroughly studied in the contemporary literature for visual data. Noting that\nan audio can be interpreted as a 1-dimensional image, object localization\ntechniques can be fundamentally useful for word localization. Building upon\nthis idea, we propose a lightweight solution for word detection and\nlocalization. We use bounding box regression for word localization, which\nenables our model to detect the occurrence, offset, and duration of keywords in\na given audio stream. We experiment with LibriSpeech and train a model to\nlocalize 1000 words. Compared to existing work, our method reduces model size\nby 94%, and improves the F1 score by 6.5\\%.", "published": "2022-10-24 19:47:33", "link": "http://arxiv.org/abs/2210.13567v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
