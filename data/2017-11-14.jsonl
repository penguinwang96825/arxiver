{"title": "From Word Segmentation to POS Tagging for Vietnamese", "abstract": "This paper presents an empirical comparison of two strategies for Vietnamese\nPart-of-Speech (POS) tagging from unsegmented text: (i) a pipeline strategy\nwhere we consider the output of a word segmenter as the input of a POS tagger,\nand (ii) a joint strategy where we predict a combined segmentation and POS tag\nfor each syllable. We also make a comparison between state-of-the-art (SOTA)\nfeature-based and neural network-based models. On the benchmark Vietnamese\ntreebank (Nguyen et al., 2009), experimental results show that the pipeline\nstrategy produces better scores of POS tagging from unsegmented text than the\njoint strategy, and the highest accuracy is obtained by using a feature-based\nmodel.", "published": "2017-11-14 05:19:45", "link": "http://arxiv.org/abs/1711.04951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classical Structured Prediction Losses for Sequence to Sequence Learning", "abstract": "There has been much recent work on training neural attention models at the\nsequence-level using either reinforcement learning-style methods or by\noptimizing the beam. In this paper, we survey a range of classical objective\nfunctions that have been widely used to train linear models for structured\nprediction and apply them to neural sequence to sequence models. Our\nexperiments show that these losses can perform surprisingly well by slightly\noutperforming beam search optimization in a like for like setup. We also report\nnew state of the art results on both IWSLT'14 German-English translation as\nwell as Gigaword abstractive summarization. On the larger WMT'14 English-French\ntranslation task, sequence-level training achieves 41.5 BLEU which is on par\nwith the state of the art.", "published": "2017-11-14 05:47:08", "link": "http://arxiv.org/abs/1711.04956v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Fusion Networks for Machine Reading Comprehension", "abstract": "This paper presents a novel neural model - Dynamic Fusion Network (DFN), for\nmachine reading comprehension (MRC). DFNs differ from most state-of-the-art\nmodels in their use of a dynamic multi-strategy attention process, in which\npassages, questions and answer candidates are jointly fused into attention\nvectors, along with a dynamic multi-step reasoning module for generating\nanswers. With the use of reinforcement learning, for each input sample that\nconsists of a question, a passage and a list of candidate answers, an instance\nof DFN with a sample-specific network architecture can be dynamically\nconstructed by determining what attention strategy to apply and how many\nreasoning steps to take. Experiments show that DFNs achieve the best result\nreported on RACE, a challenging MRC dataset that contains real human reading\nquestions in a wide variety of types. A detailed empirical analysis also\ndemonstrates that DFNs can produce attention vectors that summarize information\nfrom questions, passages and answer candidates more effectively than other\npopular MRC models.", "published": "2017-11-14 06:17:54", "link": "http://arxiv.org/abs/1711.04964v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Pragmatic Models for Generating and Following Instructions", "abstract": "We show that explicit pragmatic inference aids in correctly generating and\nfollowing natural language instructions for complex, sequential tasks. Our\npragmatics-enabled models reason about why speakers produce certain\ninstructions, and about how listeners will react upon hearing them. Like\nprevious pragmatic models, we use learned base listener and speaker models to\nbuild a pragmatic speaker that uses the base listener to simulate the\ninterpretation of candidate descriptions, and a pragmatic listener that reasons\ncounterfactually about alternative descriptions. We extend these models to\ntasks with sequential structure. Evaluation of language generation and\ninterpretation shows that pragmatic inference improves state-of-the-art\nlistener models (at correctly interpreting human instructions) and speaker\nmodels (at producing instructions correctly interpreted by humans) in diverse\nsettings.", "published": "2017-11-14 07:55:39", "link": "http://arxiv.org/abs/1711.04987v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning an Executable Neural Semantic Parser", "abstract": "This paper describes a neural semantic parser that maps natural language\nutterances onto logical forms which can be executed against a task-specific\nenvironment, such as a knowledge base or a database, to produce a response. The\nparser generates tree-structured logical forms with a transition-based approach\nwhich combines a generic tree-generation algorithm with domain-general\noperations defined by the logical language. The generation process is modeled\nby structured recurrent neural networks, which provide a rich encoding of the\nsentential context and generation history for making predictions. To tackle\nmismatches between natural language and logical form tokens, various attention\nmechanisms are explored. Finally, we consider different training settings for\nthe neural semantic parser, including a fully supervised training where\nannotated logical forms are given, weakly-supervised training where denotations\nare provided, and distant supervision where only unlabeled sentences and a\nknowledge base are available. Experiments across a wide range of datasets\ndemonstrate the effectiveness of our parser.", "published": "2017-11-14 12:00:36", "link": "http://arxiv.org/abs/1711.05066v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuReader: a Chinese Machine Reading Comprehension Dataset from\n  Real-world Applications", "abstract": "This paper introduces DuReader, a new large-scale, open-domain Chinese ma-\nchine reading comprehension (MRC) dataset, designed to address real-world MRC.\nDuReader has three advantages over previous MRC datasets: (1) data sources:\nquestions and documents are based on Baidu Search and Baidu Zhidao; answers are\nmanually generated. (2) question types: it provides rich annotations for more\nquestion types, especially yes-no and opinion questions, that leaves more\nopportunity for the research community. (3) scale: it contains 200K questions,\n420K answers and 1M documents; it is the largest Chinese MRC dataset so far.\nExperiments show that human performance is well above current state-of-the-art\nbaseline systems, leaving plenty of room for the community to make\nimprovements. To help the community make these improvements, both DuReader and\nbaseline systems have been posted online. We also organize a shared competition\nto encourage the exploration of more models. Since the release of the task,\nthere are significant improvements over the baselines.", "published": "2017-11-14 12:13:44", "link": "http://arxiv.org/abs/1711.05073v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "False Positive and Cross-relation Signals in Distant Supervision Data", "abstract": "Distant supervision (DS) is a well-established method for relation extraction\nfrom text, based on the assumption that when a knowledge-base contains a\nrelation between a term pair, then sentences that contain that pair are likely\nto express the relation. In this paper, we use the results of a crowdsourcing\nrelation extraction task to identify two problems with DS data quality: the\nwidely varying degree of false positives across different relations, and the\nobserved causal connection between relations that are not considered by the DS\nmethod. The crowdsourcing data aggregation is performed using ambiguity-aware\nCrowdTruth metrics, that are used to capture and interpret inter-annotator\ndisagreement. We also present preliminary results of using the crowd to enhance\nDS training data for a relation classification model, without requiring the\ncrowd to annotate the entire set.", "published": "2017-11-14 16:50:40", "link": "http://arxiv.org/abs/1711.05186v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised patient representations from clinical notes with\n  interpretable classification decisions", "abstract": "We have two main contributions in this work: 1. We explore the usage of a\nstacked denoising autoencoder, and a paragraph vector model to learn\ntask-independent dense patient representations directly from clinical notes. We\nevaluate these representations by using them as features in multiple supervised\nsetups, and compare their performance with those of sparse representations. 2.\nTo understand and interpret the representations, we explore the best encoded\nfeatures within the patient representations obtained from the autoencoder\nmodel. Further, we calculate the significance of the input features of the\ntrained classifiers when we use these pretrained representations as input.", "published": "2017-11-14 17:05:51", "link": "http://arxiv.org/abs/1711.05198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Abstractive Summarization", "abstract": "Current models for document summarization disregard user preferences such as\nthe desired length, style, the entities that the user might be interested in,\nor how much of the document the user has already read. We present a neural\nsummarization model with a simple but effective mechanism to enable users to\nspecify these high level attributes in order to control the shape of the final\nsummaries to better suit their needs. With user input, our system can produce\nhigh quality summaries that follow user preferences. Without user input, we set\nthe control variables automatically. On the full text CNN-Dailymail dataset, we\noutperform state of the art abstractive systems (both in terms of F1-ROUGE1\n40.38 vs. 39.53 and human evaluation).", "published": "2017-11-14 17:30:26", "link": "http://arxiv.org/abs/1711.05217v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Semantic Relatedness using Global Relation Vectors", "abstract": "Word embedding models such as GloVe rely on co-occurrence statistics from a\nlarge corpus to learn vector representations of word meaning. These vectors\nhave proven to capture surprisingly fine-grained semantic and syntactic\ninformation. While we may similarly expect that co-occurrence statistics can be\nused to capture rich information about the relationships between different\nwords, existing approaches for modeling such relationships have mostly relied\non manipulating pre-trained word vectors. In this paper, we introduce a novel\nmethod which directly learns relation vectors from co-occurrence statistics. To\nthis end, we first introduce a variant of GloVe, in which there is an explicit\nconnection between word vectors and PMI weighted co-occurrence vectors. We then\nshow how relation vectors can be naturally embedded into the resulting vector\nspace.", "published": "2017-11-14 19:42:55", "link": "http://arxiv.org/abs/1711.05294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulating Action Dynamics with Neural Process Networks", "abstract": "Understanding procedural language requires anticipating the causal effects of\nactions, even when they are not explicitly stated. In this work, we introduce\nNeural Process Networks to understand procedural text through (neural)\nsimulation of action dynamics. Our model complements existing memory\narchitectures with dynamic entity tracking by explicitly modeling actions as\nstate transformers. The model updates the states of the entities by executing\nlearned action operators. Empirical results demonstrate that our proposed model\ncan reason about the unstated causal effects of actions, allowing it to provide\nmore accurate contextual information for understanding and generating\nprocedural text, all while offering more interpretable internal representations\nthan existing alternatives.", "published": "2017-11-14 21:07:38", "link": "http://arxiv.org/abs/1711.05313v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised and Unsupervised Transfer Learning for Question Answering", "abstract": "Although transfer learning has been shown to be successful for tasks like\nobject and speech recognition, its applicability to question answering (QA) has\nyet to be well-studied. In this paper, we conduct extensive experiments to\ninvestigate the transferability of knowledge learned from a source QA dataset\nto a target dataset using two QA models. The performance of both models on a\nTOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson\net al., 2013) is significantly improved via a simple transfer learning\ntechnique from MovieQA (Tapaswi et al., 2016). In particular, one of the models\nachieves the state-of-the-art on all target datasets; for the TOEFL listening\ncomprehension test, it outperforms the previous best model by 7%. Finally, we\nshow that transfer learning is helpful even in unsupervised scenarios when\ncorrect answers for target QA dataset examples are not available.", "published": "2017-11-14 22:57:24", "link": "http://arxiv.org/abs/1711.05345v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Learning Approach for Expert Identification in Question Answering\n  Communities", "abstract": "In this paper, we describe an effective convolutional neural network\nframework for identifying the expert in question answering community. This\napproach uses the convolutional neural network and combines user feature\nrepresentations with question feature representations to compute scores that\nthe user who gets the highest score is the expert on this question. Unlike\nprior work, this method does not measure expert based on measure answer content\nquality to identify the expert but only require question sentence and user\nembedding feature to identify the expert. Remarkably, Our model can be applied\nto different languages and different domains. The proposed framework is trained\non two datasets, The first dataset is Stack Overflow and the second one is\nZhihu. The Top-1 accuracy results of our experiments show that our framework\noutperforms the best baseline framework for expert identification.", "published": "2017-11-14 23:10:59", "link": "http://arxiv.org/abs/1711.05350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Multilingual Part-of-Speech Tagging via Adversarial Training", "abstract": "Adversarial training (AT) is a powerful regularization method for neural\nnetworks, aiming to achieve robustness to input perturbations. Yet, the\nspecific effects of the robustness obtained from AT are still unclear in the\ncontext of natural language processing. In this paper, we propose and analyze a\nneural POS tagging model that exploits AT. In our experiments on the Penn\nTreebank WSJ corpus and the Universal Dependencies (UD) dataset (27 languages),\nwe find that AT not only improves the overall tagging accuracy, but also 1)\nprevents over-fitting well in low resource languages and 2) boosts tagging\naccuracy for rare / unseen words. We also demonstrate that 3) the improved\ntagging performance by AT contributes to the downstream task of dependency\nparsing, and that 4) AT helps the model to learn cleaner word representations.\n5) The proposed AT model is generally effective in different sequence labeling\ntasks. These positive results motivate further use of AT for natural language\ntasks.", "published": "2017-11-14 01:50:30", "link": "http://arxiv.org/abs/1711.04903v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SkipFlow: Incorporating Neural Coherence Features for End-to-End\n  Automatic Text Scoring", "abstract": "Deep learning has demonstrated tremendous potential for Automatic Text\nScoring (ATS) tasks. In this paper, we describe a new neural architecture that\nenhances vanilla neural network models with auxiliary neural coherence\nfeatures. Our new method proposes a new \\textsc{SkipFlow} mechanism that models\nrelationships between snapshots of the hidden representations of a long\nshort-term memory (LSTM) network as it reads. Subsequently, the semantic\nrelationships between multiple snapshots are used as auxiliary features for\nprediction. This has two main benefits. Firstly, essays are typically long\nsequences and therefore the memorization capability of the LSTM network may be\ninsufficient. Implicit access to multiple snapshots can alleviate this problem\nby acting as a protection against vanishing gradients. The parameters of the\n\\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,\nmodeling relationships between multiple positions allows our model to learn\nfeatures that represent and approximate textual coherence. In our model, we\ncall this \\textit{neural coherence} features. Overall, we present a unified\ndeep learning architecture that generates neural coherence features as it reads\nin an end-to-end fashion. Our approach demonstrates state-of-the-art\nperformance on the benchmark ASAP dataset, outperforming not only feature\nengineering baselines but also other deep learning models.", "published": "2017-11-14 07:20:23", "link": "http://arxiv.org/abs/1711.04981v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question\n  Answering", "abstract": "A popular recent approach to answering open-domain questions is to first\nsearch for question-related passages and then apply reading comprehension\nmodels to extract answers. Existing methods usually extract answers from single\npassages independently. But some questions require a combination of evidence\nfrom across different sources to answer correctly. In this paper, we propose\ntwo models which make use of multiple passages to generate their answers. Both\nuse an answer-reranking approach which reorders the answer candidates generated\nby an existing state-of-the-art QA model. We propose two methods, namely,\nstrength-based re-ranking and coverage-based re-ranking, to make use of the\naggregated evidence from different passages to better determine the answer. Our\nmodels have achieved state-of-the-art results on three public open-domain QA\ndatasets: Quasar-T, SearchQA and the open-domain version of TriviaQA, with\nabout 8 percentage points of improvement over the former two datasets.", "published": "2017-11-14 14:39:51", "link": "http://arxiv.org/abs/1711.05116v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Extending Neural Networks with Loss Ensembles for Text Classification", "abstract": "Ensemble techniques are powerful approaches that combine several weak\nlearners to build a stronger one. As a meta learning framework, ensemble\ntechniques can easily be applied to many machine learning techniques. In this\npaper we propose a neural network extended with an ensemble loss function for\ntext classification. The weight of each weak loss function is tuned within the\ntraining phase through the gradient propagation optimization method of the\nneural network. The approach is evaluated on several text classification\ndatasets. We also evaluate its performance in various environments with several\ndegrees of label noise. Experimental results indicate an improvement of the\nresults and strong resilience against label noise in comparison with other\nmethods.", "published": "2017-11-14 16:19:34", "link": "http://arxiv.org/abs/1711.05170v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Weakly-supervised Semantic Parsing with Abstract Examples", "abstract": "Training semantic parsers from weak supervision (denotations) rather than\nstrong supervision (programs) complicates training in two ways. First, a large\nsearch space of potential programs needs to be explored at training time to\nfind a correct program. Second, spurious programs that accidentally lead to a\ncorrect denotation add noise to training. In this work we propose that in\nclosed worlds with clear semantic types, one can substantially alleviate these\nproblems by utilizing an abstract representation, where tokens in both the\nlanguage utterance and program are lifted to an abstract form. We show that\nthese abstractions can be defined with a handful of lexical rules and that they\nresult in sharing between different examples that alleviates the difficulties\nin training. To test our approach, we develop the first semantic parser for\nCNLVR, a challenging visual reasoning dataset, where the search space is large\nand overcoming spuriousness is critical, because denotations are either TRUE or\nFALSE, and thus random programs are likely to lead to a correct denotation. Our\nmethod substantially improves performance, and reaches 82.5% accuracy, a 14.7%\nabsolute accuracy improvement compared to the best reported accuracy so far.", "published": "2017-11-14 18:29:05", "link": "http://arxiv.org/abs/1711.05240v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimal Tuning of Two-Dimensional Keyboards", "abstract": "We give a new analysis of a tuning problem in music theory, pertaining\nspecifically to the approximation of harmonics on a two-dimensional keyboard.\nWe formulate the question as a linear programming problem on families of\nconstraints and provide exact solutions for many new keyboard dimensions. We\nalso show that an optimal tuning for harmonic approximation can be obtained for\nany keyboard of given width, provided sufficiently many rows of octaves.", "published": "2017-11-14 16:47:23", "link": "http://arxiv.org/abs/1711.05260v1", "categories": ["cs.SD", "eess.AS", "00A65"], "primary_category": "cs.SD"}
{"title": "Automatic Conflict Detection in Police Body-Worn Audio", "abstract": "Automatic conflict detection has grown in relevance with the advent of\nbody-worn technology, but existing metrics such as turn-taking and overlap are\npoor indicators of conflict in police-public interactions. Moreover, standard\ntechniques to compute them fall short when applied to such diversified and\nnoisy contexts. We develop a pipeline catered to this task combining adaptive\nnoise removal, non-speech filtering and new measures of conflict based on the\nrepetition and intensity of phrases in speech. We demonstrate the effectiveness\nof our approach on body-worn audio data collected by the Los Angeles Police\nDepartment.", "published": "2017-11-14 23:28:05", "link": "http://arxiv.org/abs/1711.05355v2", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
