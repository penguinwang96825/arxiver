{"title": "GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for\n  Conversational Machine Comprehension", "abstract": "Conversational machine comprehension (MC) has proven significantly more\nchallenging compared to traditional MC since it requires better utilization of\nconversation history. However, most existing approaches do not effectively\ncapture conversation history and thus have trouble handling questions involving\ncoreference or ellipsis. Moreover, when reasoning over passage text, most of\nthem simply treat it as a word sequence without exploring rich semantic\nrelationships among words. In this paper, we first propose a simple yet\neffective graph structure learning technique to dynamically construct a\nquestion and conversation history aware context graph at each conversation\nturn. Then we propose a novel Recurrent Graph Neural Network, and based on\nthat, we introduce a flow mechanism to model the temporal dependencies in a\nsequence of context graphs. The proposed GraphFlow model can effectively\ncapture conversational flow in a dialog, and shows competitive performance\ncompared to existing state-of-the-art methods on CoQA, QuAC and DoQA\nbenchmarks. In addition, visualization experiments show that our proposed model\ncan offer good interpretability for the reasoning process.", "published": "2019-07-31 19:23:38", "link": "http://arxiv.org/abs/1908.00059v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Question-Guided Video Representation for Multi-Turn Video\n  Question Answering", "abstract": "Understanding and conversing about dynamic scenes is one of the key\ncapabilities of AI agents that navigate the environment and convey useful\ninformation to humans. Video question answering is a specific scenario of such\nAI-human interaction where an agent generates a natural language response to a\nquestion regarding the video of a dynamic scene. Incorporating features from\nmultiple modalities, which often provide supplementary information, is one of\nthe challenging aspects of video question answering. Furthermore, a question\noften concerns only a small segment of the video, hence encoding the entire\nvideo sequence using a recurrent neural network is not computationally\nefficient. Our proposed question-guided video representation module efficiently\ngenerates the token-level video summary guided by each word in the question.\nThe learned representations are then fused with the question to generate the\nanswer. Through empirical evaluation on the Audio Visual Scene-aware Dialog\n(AVSD) dataset, our proposed models in single-turn and multi-turn question\nanswering achieve state-of-the-art performance on several automatic natural\nlanguage generation evaluation metrics.", "published": "2019-07-31 01:37:58", "link": "http://arxiv.org/abs/1907.13280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Normalyzing Numeronyms -- A NLP approach", "abstract": "This paper presents a method to apply Natural Language Processing for\nnormalizing numeronyms to make them understandable by humans. We approach the\nproblem through a two-step mechanism. We make use of the state of the art\nLevenshtein distance of words. We then apply Cosine Similarity for selection of\nthe normalized text and reach greater accuracy in solving the problem. Our\napproach garners accuracy figures of 71\\% and 72\\% for Bengali and English\nlanguage, respectively.", "published": "2019-07-31 07:53:57", "link": "http://arxiv.org/abs/1907.13356v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On conducting better validation studies of automatic metrics in natural\n  language generation evaluation", "abstract": "Natural language generation (NLG) has received increasing attention, which\nhas highlighted evaluation as a central methodological concern. Since human\nevaluations for these systems are costly, automatic metrics have broad appeal\nin NLG. Research in language generation often finds situations where it is\nappropriate to apply existing metrics or propose new ones. The application of\nthese metrics are entirely dependent on validation studies - studies that\ndetermine a metric's correlation to human judgment. However, there are many\ndetails and considerations in conducting strong validation studies. This\ndocument is intended for those validating existing metrics or proposing new\nones in the broad context of NLG: we 1) begin with a write-up of best practices\nin validation studies, 2) outline how to adopt these practices, 3) conduct\nanalyses in the WMT'17 metrics shared task\\footnote{Our jupyter notebook\ncontaining the analyses is available at \\url{https://github.com}}, and 4)\nhighlight promising approaches to NLG metrics 5) conclude with our opinions on\nthe future of this area.", "published": "2019-07-31 08:40:26", "link": "http://arxiv.org/abs/1907.13362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Unsupervised Summarization by Contextual Matching", "abstract": "We propose an unsupervised method for sentence summarization using only\nlanguage modeling. The approach employs two language models, one that is\ngeneric (i.e. pretrained), and the other that is specific to the target domain.\nWe show that by using a product-of-experts criteria these are enough for\nmaintaining continuous contextual matching while maintaining output fluency.\nExperiments on both abstractive and extractive sentence summarization data sets\nshow promising results of our method without being exposed to any paired data.", "published": "2019-07-31 07:11:59", "link": "http://arxiv.org/abs/1907.13337v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What BERT is not: Lessons from a new suite of psycholinguistic\n  diagnostics for language models", "abstract": "Pre-training by language modeling has become a popular and successful\napproach to NLP tasks, but we have yet to understand exactly what linguistic\ncapacities these pre-training processes confer upon models. In this paper we\nintroduce a suite of diagnostics drawn from human language experiments, which\nallow us to ask targeted questions about the information used by language\nmodels for generating predictions in context. As a case study, we apply these\ndiagnostics to the popular BERT model, finding that it can generally\ndistinguish good from bad completions involving shared category or role\nreversal, albeit with less sensitivity than humans, and it robustly retrieves\nnoun hypernyms, but it struggles with challenging inferences and role-based\nevent prediction -- and in particular, it shows clear insensitivity to the\ncontextual impacts of negation.", "published": "2019-07-31 14:37:32", "link": "http://arxiv.org/abs/1907.13528v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lifelong and Interactive Learning of Factual Knowledge in Dialogues", "abstract": "Dialogue systems are increasingly using knowledge bases (KBs) storing\nreal-world facts to help generate quality responses. However, as the KBs are\ninherently incomplete and remain fixed during conversation, it limits dialogue\nsystems' ability to answer questions and to handle questions involving entities\nor relations that are not in the KB. In this paper, we make an attempt to\npropose an engine for Continuous and Interactive Learning of Knowledge (CILK)\nfor dialogue systems to give them the ability to continuously and interactively\nlearn and infer new knowledge during conversations. With more knowledge\naccumulated over time, they will be able to learn better and answer more\nquestions. Our empirical evaluation shows that CILK is promising.", "published": "2019-07-31 03:11:33", "link": "http://arxiv.org/abs/1907.13295v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data", "abstract": "Automatic speech recognition (ASR) systems have dramatically improved over\nthe last few years. ASR systems are most often trained from 'typical' speech,\nwhich means that underrepresented groups don't experience the same level of\nimprovement. In this paper, we present and evaluate finetuning techniques to\nimprove ASR for users with non-standard speech. We focus on two types of\nnon-standard speech: speech from people with amyotrophic lateral sclerosis\n(ALS) and accented speech. We train personalized models that achieve 62% and\n35% relative WER improvement on these two groups, bringing the absolute WER for\nALS speakers, on a test set of message bank phrases, down to 10% for mild\ndysarthria and 20% for more serious dysarthria. We show that 71% of the\nimprovement comes from only 5 minutes of training data. Finetuning a particular\nsubset of layers (with many fewer parameters) often gives better results than\nfinetuning the entire model. This is the first step towards building state of\nthe art ASR models for dysarthric speech.", "published": "2019-07-31 14:07:27", "link": "http://arxiv.org/abs/1907.13511v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Quantifying Cochlear Implant Users' Ability for Speaker Identification\n  using CI Auditory Stimuli", "abstract": "Speaker recognition is a biometric modality that uses underlying speech\ninformation to determine the identity of the speaker. Speaker Identification\n(SID) under noisy conditions is one of the challenging topics in the field of\nspeech processing, specifically when it comes to individuals with cochlear\nimplants (CI). This study analyzes and quantifies the ability of CI-users to\nperform speaker identification based on direct electric auditory stimuli. CI\nusers employ a limited number of frequency bands (8 to 22) and use electrodes\nto directly stimulate the Basilar Membrane/Cochlear in order to recognize the\nspeech signal. The sparsity of electric stimulation within the CI frequency\nrange is a prime reason for loss in human speech recognition, as well as SID\nperformance. Therefore, it is assumed that CI-users might be unable to\nrecognize and distinguish a speaker given dependent information such as formant\nfrequencies, pitch etc. which are lost to un-simulated electrodes. To quantify\nthis assumption, the input speech signal is processed using a CI Advanced\nCombined Encoder (ACE) signal processing strategy to construct the CI auditory\nelectrodogram. The proposed study uses 50 speakers from each of three different\ndatabases for training the system using two different classifiers under quiet,\nand tested under both quiet and noisy conditions. The objective result shows\nthat, the CI users can effectively identify a limited number of speakers.\nHowever, their performance decreases when more speakers are added in the\nsystem, as well as when noisy conditions are introduced. This information could\ntherefore be used for improving CI-user signal processing techniques to improve\nhuman SID.", "published": "2019-07-31 18:17:15", "link": "http://arxiv.org/abs/1908.00031v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Augmenting Music Sheets with Harmonic Fingerprints", "abstract": "Conventional Music Notation (CMN) is the well-established foundation for the\nwritten communication of musical information, such as rhythm, harmony, or\ntimbre. However, CMN suffers from the complexity of its visual encoding and the\nneed for extensive training to acquire proficiency and legibility. While\nalternative notations using additional visual variables (such as color to\nimprove pitch identification) have been proposed, the music community does not\nreadily accept notation systems that vary widely from the CMN. Therefore, to\nsupport student musicians in understanding the harmonic relationship of notes,\ninstead of replacing the CMN, we present a visualization technique that\naugments a digital music sheet with a harmonic fingerprint glyph. Our design\nexploits the circle of fifths - a fundamental concept in music theory, as a\nvisual metaphor. By attaching these visual glyphs to each bar of a selected\ncomposition we provide additional information about the salient harmonic\nfeatures available in a musical piece. We conducted a user study to analyze the\nperformance of experts and non-experts in an identification and comparison task\nof recurring patterns. The evaluation shows that the harmonic fingerprint\nsupports these tasks without the need for close-reading, as when compared to a\nnot-annotated music sheet.", "published": "2019-07-31 08:17:43", "link": "http://arxiv.org/abs/1908.00003v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.0; I.7.2; H.1.1"], "primary_category": "cs.HC"}
