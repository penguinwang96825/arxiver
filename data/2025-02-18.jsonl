{"title": "Optimal covering of rectangular grid graphs with tours of constrained length", "abstract": "Given a rectangular grid graph with a special vertex at a corner called base\nstation, we study the problem of covering the vertices of the entire graph with\ntours that start and end at the base station and whose lengths do not exceed a\ngiven threshold, while minimizing a quality measure. We consider two objective\nfunctions: minimizing the number of tours and minimizing the sum of their\nlengths. We present an algorithm that computes the optimal solution for both\nobjectives in linear time with respect to the grid size.", "published": "2025-02-18 21:58:10", "link": "http://arxiv.org/abs/2502.13306v1", "categories": ["cs.DM", "cs.CG", "math.CO", "G.2.1; F.2.2"], "primary_category": "cs.DM"}
{"title": "A Machine Learning Approach That Beats Large Rubik's Cubes", "abstract": "The paper proposes a novel machine learning-based approach to the pathfinding\nproblem on extremely large graphs. This method leverages diffusion distance\nestimation via a neural network and uses beam search for pathfinding. We\ndemonstrate its efficiency by finding solutions for 4x4x4 and 5x5x5 Rubik's\ncubes with unprecedentedly short solution lengths, outperforming all available\nsolvers and introducing the first machine learning solver beyond the 3x3x3\ncase. In particular, it surpasses every single case of the combined best\nresults in the Kaggle Santa 2023 challenge, which involved over 1,000 teams.\nFor the 3x3x3 Rubik's cube, our approach achieves an optimality rate exceeding\n98%, matching the performance of task-specific solvers and significantly\noutperforming prior solutions such as DeepCubeA (60.3%) and EfficientCube\n(69.6%). Additionally, our solution is more than 26 times faster in solving\n3x3x3 Rubik's cubes while requiring up to 18.5 times less model training time\nthan the most efficient state-of-the-art competitor.", "published": "2025-02-18 20:22:38", "link": "http://arxiv.org/abs/2502.13266v1", "categories": ["cs.LG", "cs.DM", "G.2.2; I.2.8"], "primary_category": "cs.LG"}
{"title": "Las funciones booleans y el lema de Bonami", "abstract": "In this expository article, we study the relation between the boolean\nfunctions and the hypercontractivity theorems of Aline Bonami. We focus on the\nsocial choice theory, and present some of the most important results in the\narea, such as the Friedgut-Kalai-Naor (FKN) and the Kahn-Kalai-Linial (KKL)\ntheorems, and the famous Fourier Entropy/Influence conjecture.\n  --\n  En este art\\'iculo expositivo estudiamos la relaci\\'on entre las funciones\nbooleanas y los teoremas de hipercontractividad de Aline Bonami. Nos\nconcentramos en la teor\\'ia de la elecci\\'on social, y presentamos algunos de\nlos resultados m\\'as importantes en el \\'area como los teoremas de\nFriedgut-Kalai-Naor (FKN) y de Kahn-Kalai-Linial (KKL), y la famosa conjetura\nEntrop\\'i{\\i}a de Fourier/Influencia.", "published": "2025-02-18 19:10:31", "link": "http://arxiv.org/abs/2502.13231v1", "categories": ["math.CA", "cs.DM", "43.01, 68R01"], "primary_category": "math.CA"}
{"title": "The Normal Play of the Domination Game", "abstract": "In 2010, Bre\\v{s}ar, Klav\\v{z}ar and Rall introduced the optimization variant\nof the graph domination game and the game domination number. In 2024, Leo\nVersteegen obtained the celebrated proof of the Conjecture $\\frac{3}{5}$ on\nthis variant of the domination game, proposed by Kinnersley, West and Zamani in\n2013. In this paper, we investigate for the first time the normal play of the\ndomination game, which we call \\textsc{Normal Domination Game}, that is an\nimpartial game where the last to play wins. We use the Sprague-Grundy theory to\nprove that Alice (the first player) wins in the path $P_n$ if and only if $n$\nis not a multiple of $4$, and wins in the cycle $C_n$ if and only if $n=4k+3$\nfor some integer $k$. Finally, we obtain a polynomial time algorithm to decide\nthe winner for any disjoint union of paths and cycles in the \\textsc{Normal\nDomination Game} and its natural partizan variant.", "published": "2025-02-18 18:37:41", "link": "http://arxiv.org/abs/2502.13118v2", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Unique expansions in number systems via solutions of refinement equation", "abstract": "Using the subdivision schemes theory, we develop a criterion to check if any\nnatural number has at most one representation in the $n$-ary number system with\na set of non-negative integer digits $A=\\{a_1, a_2,\\ldots, a_n\\}$ that contains\nzero. This uniqueness property is shown to be equivalent to a certain\nrestriction on the roots of the trigonometric polynomial $\\sum_{k=1}^n e^{-2\\pi\ni a_k t}$. From this criterion, under a natural condition of irreducibility for\n$A$, we deduce that in case of prime $n$ the uniqueness holds if and only if\nthe digits of $A$ are distinct modulo $n$, whereas for any composite $n$ we\nshow that the latter condition is not necessary. We also establish the\nconnection of this uniqueness to the semigroup freeness problem for affine\ninteger functions of equal integer slope; this together with the two criteria\nallows to fill the gap in the work of D$.$Klarner on Erd\\\"os question about\ndensities of affine integer orbits and establish a simple algorithm to check\nthe freeness and the positivity of density when the slope is a prime number.", "published": "2025-02-18 17:09:15", "link": "http://arxiv.org/abs/2502.13066v1", "categories": ["math.NT", "cs.DM", "math.FA", "11B75, 20M05, 39A06"], "primary_category": "math.NT"}
{"title": "Edge-Colored Clustering in Hypergraphs: Beyond Minimizing Unsatisfied Edges", "abstract": "We consider a framework for clustering edge-colored hypergraphs, where the\ngoal is to cluster (equivalently, to color) objects based on the primary type\nof multiway interactions they participate in. One well-studied objective is to\ncolor nodes to minimize the number of unsatisfied hyperedges -- those\ncontaining one or more nodes whose color does not match the hyperedge color. We\nmotivate and present advances for several directions that extend beyond this\nminimization problem. We first provide new algorithms for maximizing satisfied\nedges, which is the same at optimality but is much more challenging to\napproximate, with all prior work restricted to graphs. We develop the first\napproximation algorithm for hypergraphs, and then refine it to improve the\nbest-known approximation factor for graphs. We then introduce new objective\nfunctions that incorporate notions of balance and fairness, and provide new\nhardness results, approximations, and fixed-parameter tractability results.", "published": "2025-02-18 16:20:50", "link": "http://arxiv.org/abs/2502.13000v1", "categories": ["cs.DS", "cs.DM", "cs.LG"], "primary_category": "cs.DS"}
{"title": "Approximate Tree Completion and Learning-Augmented Algorithms for Metric Minimum Spanning Trees", "abstract": "Finding a minimum spanning tree (MST) for $n$ points in an arbitrary metric\nspace is a fundamental primitive for hierarchical clustering and many other ML\ntasks, but this takes $\\Omega(n^2)$ time to even approximate. We introduce a\nframework for metric MSTs that first (1) finds a forest of disconnected\ncomponents using practical heuristics, and then (2) finds a small weight set of\nedges to connect disjoint components of the forest into a spanning tree. We\nprove that optimally solving the second step still takes $\\Omega(n^2)$ time,\nbut we provide a subquadratic 2.62-approximation algorithm. In the spirit of\nlearning-augmented algorithms, we then show that if the forest found in step\n(1) overlaps with an optimal MST, we can approximate the original MST problem\nin subquadratic time, where the approximation factor depends on a measure of\noverlap. In practice, we find nearly optimal spanning trees for a wide range of\nmetrics, while being orders of magnitude faster than exact algorithms.", "published": "2025-02-18 16:13:46", "link": "http://arxiv.org/abs/2502.12993v1", "categories": ["cs.DS", "cs.DM", "cs.LG"], "primary_category": "cs.DS"}
{"title": "Generalized De Bruijn Words, Invertible Necklaces, and the Burrows-Wheeler Transform", "abstract": "We define generalized de Bruijn words, as those words having a\nBurrows--Wheeler transform that is a concatenation of permutations of the\nalphabet. We show how to interpret generalized de Bruijn words in terms of\nHamiltonian cycles in the generalized de Bruijn graphs introduced in the early\n'80s in the context of network design. When the size of the alphabet is a\nprime, we give relations between generalized de Bruijn words, normal bases of\nfinite fields, invertible circulant matrices, and Reutenauer groups. In\nparticular, we highlight a correspondence between binary de Bruijn words of\norder $d+1$, binary necklaces of length $2^{d}$ having an odd number of $1$s,\ninvertible BWT matrices of size $2^{d}\\times 2^{d}$, and normal bases of the\nfinite field $\\mathbb{F}_{2^{2^{d}}}$.", "published": "2025-02-18 13:24:09", "link": "http://arxiv.org/abs/2502.12844v1", "categories": ["math.CO", "cs.DM", "cs.DS", "cs.FL"], "primary_category": "math.CO"}
{"title": "Generalized Hofstadter functions $G$, $H$ and beyond: numeration systems and discrepancy", "abstract": "Hofstadter's $G$ function is recursively defined via $G(0)=0$ and then\n$G(n)=n-G(G(n-1))$. Following Hofstadter, a family $(F_k)$ of similar functions\nis obtained by varying the number $k$ of nested recursive calls in this\nequation. We study here some Fibonacci-like sequences that are deeply connected\nwith these functions $F_k$. In particular, the Zeckendorf theorem can be\nadapted to provide digital expansions via sums of terms of these sequences. On\nthese digital expansions, the functions $F_k$ are acting as right shifts of the\ndigits. These Fibonacci-like sequences can be expressed in terms of zeros of\nthe polynomial $X^k{-}X^{k-1}{-}1$. Considering now the discrepancy of each\nfunction $F_k$, i.e., the maximal distance between $F_k$ and its linear\nequivalent, we retrieve the fact that this discrepancy is finite exactly when\n$k \\le 4$. Thanks to that, we solve two twenty-year-old OEIS conjectures\nstating how close the functions $F_3$ and $F_4$ are from the integer parts of\ntheir linear equivalents. Moreover we establish that $F_k$ can coincide exactly\nwith such an integer part only when $k\\le 2$, while $F_k$ is almost additive\nexactly when $k \\le 4$. Finally, a nice fractal shape a la Rauzy has been\nencountered when investigating the discrepancy of $F_3$. Almost all this\narticle has been formalized and verified in the Coq/Rocq proof assistant.", "published": "2025-02-18 07:56:53", "link": "http://arxiv.org/abs/2502.12615v4", "categories": ["cs.DM", "cs.FL", "cs.LO", "math.CO", "math.NT"], "primary_category": "cs.DM"}
{"title": "Arbitrage-free catastrophe reinsurance valuation for compound dynamic contagion claims", "abstract": "In this paper, we consider catastrophe stop-loss reinsurance valuation for a\nreinsurance company with dynamic contagion claims. To deal with conventional\nand emerging catastrophic events, we propose the use of a compound dynamic\ncontagion process for the catastrophic component of the liability. Under the\npremise that there is an absence of arbitrage opportunity in the market, we\nobtain arbitrage-free premiums for these contacts. To this end, the Esscher\ntransform is adopted to specify an equivalent martingale probability measure.\nWe show that reinsurers have various ways of levying the security loading on\nthe net premiums to quantify the catastrophic liability in light of the growing\nchallenges posed by emerging risks arising from climate change, cyberattacks,\nand pandemics. We numerically compare arbitrage-free catastrophe stop-loss\nreinsurance premiums via the Monte Carlo simulation method. Sensitivity\nanalyzes are performed by changing the Esscher parameters and the retention\nlevel.", "published": "2025-02-18 23:04:27", "link": "http://arxiv.org/abs/2502.13325v1", "categories": ["q-fin.RM", "math.PR", "math.ST", "q-fin.MF", "stat.TH"], "primary_category": "q-fin.RM"}
{"title": "A measure-valued HJB perspective on Bayesian optimal adaptive control", "abstract": "We consider a Bayesian adaptive optimal stochastic control problem where a\nhidden static signal has a non-separable influence on the drift of a noisy\nobservation. Being allowed to control the specific form of this dependence, we\naim at optimising a cost functional depending on the posterior distribution of\nthe hidden signal. Expressing the dynamics of this posterior distribution in\nthe observation filtration, we embed our problem into a genuinely\ninfinite-dimensional stochastic control problem featuring so-called\nmeasure-valued martingales. We address this problem by use of viscosity theory\nand approximation arguments. Specifically, we show equivalence to a\ncorresponding weak formulation, characterise the optimal value of the problem\nin terms of the unique continuous viscosity solution of an associated HJB\nequation, and construct a piecewise constant and arbitrarily-close-to-optimal\ncontrol to our main problem of study.", "published": "2025-02-18 15:42:05", "link": "http://arxiv.org/abs/2502.12957v1", "categories": ["math.OC", "math.PR", "q-fin.MF", "49L25, 60G35, 60H10, 62M20, 93E35, 93E10"], "primary_category": "math.OC"}
{"title": "When defaults cannot be hedged: an actuarial approach to xVA calculations via local risk-minimization", "abstract": "We consider the pricing and hedging of counterparty credit risk and funding\nwhen there is no possibility to hedge the jump to default of either the bank or\nthe counterparty. This represents the situation which is most often encountered\nin practice, due to the absence of quoted corporate bonds or CDS contracts\nwritten on the counterparty and the difficulty for the bank to buy/sell\nprotection on her own default. We apply local risk-minimization to find the\noptimal strategy and compute it via a BSDE.", "published": "2025-02-18 11:35:57", "link": "http://arxiv.org/abs/2502.12774v2", "categories": ["q-fin.MF", "math.PR", "q-fin.PR", "91G20, 91G30, 91G40"], "primary_category": "q-fin.MF"}
{"title": "Analysis of the Impact of the Union Budget Announcements on the Indian Stock Market: A Fractal Perspective", "abstract": "The stock market closely monitors macroeconomic policy announcements, such as\nannual budget events, due to their substantial influence on various economic\nparticipants. These events tend to impact the stock markets initially before\naffecting the real sector. Our study aims to analyze the effects of the budget\non the Indian stock market, specifically focusing on the announcement for the\nyear 2024. We will compare this with the years 2023, 2022, and 2020, assessing\nits impact on the NIFTY50 index using average abnormal return (AAR) and\ncumulative average abnormal return (CAAR) over a period of -15 and +15 days,\nincluding the budget day. This study utilizes an innovative approach involving\nthe fractal interpolation function, paired with fractal dimensional analysis,\nto study the fluctuations arising from budget announcements. The fractal\nperspective on the data offers an effective framework for understanding complex\nvariations.", "published": "2025-02-18 08:49:06", "link": "http://arxiv.org/abs/2502.15787v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "LLM Trading: Analysis of LLM Agent Behavior in Experimental Asset Markets", "abstract": "This paper explores how Large Language Models (LLMs) behave in a classic\nexperimental finance paradigm widely known for eliciting bubbles and crashes in\nhuman participants. We adapt an established trading design, where traders buy\nand sell a risky asset with a known fundamental value, and introduce several\nLLM-based agents, both in single-model markets (all traders are instances of\nthe same LLM) and in mixed-model \"battle royale\" settings (multiple LLMs\ncompeting in the same market). Our findings reveal that LLMs generally exhibit\na \"textbook-rational\" approach, pricing the asset near its fundamental value,\nand show only a muted tendency toward bubble formation. Further analyses\nindicate that LLM-based agents display less trading strategy variance in\ncontrast to humans. Taken together, these results highlight the risk of relying\non LLM-only data to replicate human-driven market phenomena, as key behavioral\nfeatures, such as large emergent bubbles, were not robustly reproduced. While\nLLMs clearly possess the capacity for strategic decision-making, their relative\nconsistency and rationality suggest that they do not accurately mimic human\nmarket dynamics.", "published": "2025-02-18 23:05:32", "link": "http://arxiv.org/abs/2502.15800v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Advanced Digital Simulation for Financial Market Dynamics: A Case of Commodity Futures", "abstract": "After decades of evolution, the financial system has increasingly deviated\nfrom an idealized framework based on theorems. It necessitates accurate\nprojections of complex market dynamics and human behavioral patterns. With the\ndevelopment of data science and machine intelligence, researchers are trying to\ndigitalize and automate market prediction. However, existing methodologies\nstruggle to represent the diversity of individuals and are regardless of the\ndomino effects of interactions on market dynamics, leading to the poor\nperformance facing abnormal market conditions where non-quantitative\ninformation dominates the market. To alleviate these disadvantages requires the\nintroduction of knowledge about how non-quantitative information, like news and\npolicy, affects market dynamics. This study investigates overcoming these\nchallenges through rehearsing potential market trends based on the financial\nlarge language model agents whose behaviors are aligned with their cognition\nand analyses in markets. We propose a hierarchical knowledge architecture for\nfinancial large language model agents, integrating fine-tuned language models\nand specialized generators optimized for trading scenarios. For financial\nmarket, we develop an advanced interactive behavioral simulation system that\nenables users to configure agents and automate market simulations. In this\nwork, we take commodity futures as an example to research the effectiveness of\nour methodologies. Our real-world case simulation succeeds in rehearsing\nabnormal market dynamics under geopolitical events and reaches an average\naccuracy of 3.4% across various points in time after the event on predicting\nfutures price. Experimental results demonstrate our method effectively\nleverages diverse information to simulate behaviors and their impact on market\ndynamics through systematic interaction.", "published": "2025-02-18 12:40:04", "link": "http://arxiv.org/abs/2503.20787v1", "categories": ["q-fin.TR", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages &\n  Dialects", "abstract": "As large language models (LLM) become more and more capable in languages\nother than English, it is important to collect benchmark datasets in order to\nevaluate their multilingual performance, including on tasks like machine\ntranslation (MT). In this work, we extend the WMT24 dataset to cover 55\nlanguages by collecting new human-written references and post-edits for 46 new\nlanguages and dialects in addition to post-edits of the references in 8 out of\n9 languages in the original WMT24 dataset. The dataset covers four domains:\nliterary, news, social, and speech. We benchmark a variety of MT providers and\nLLMs on the collected dataset using automatic metrics and find that LLMs are\nthe best-performing MT systems in all 55 languages. These results should be\nconfirmed using a human-based evaluation, which we leave for future work.", "published": "2025-02-18 00:39:30", "link": "http://arxiv.org/abs/2502.12404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Robust Approximation of ASR Metrics", "abstract": "Recent advances in speech foundation models are largely driven by scaling\nboth model size and data, enabling them to perform a wide range of tasks,\nincluding speech recognition. Traditionally, ASR models are evaluated using\nmetrics like Word Error Rate (WER) and Character Error Rate (CER), which depend\non ground truth labels. As a result of limited labeled data from diverse\ndomains and testing conditions, the true generalization capabilities of these\nmodels beyond standard benchmarks remain unclear. Moreover, labeling data is\nboth costly and time-consuming. To address this, we propose a novel label-free\napproach for approximating ASR performance metrics, eliminating the need for\nground truth labels. Our method utilizes multimodal embeddings in a unified\nspace for speech and transcription representations, combined with a\nhigh-quality proxy model to compute proxy metrics. These features are used to\ntrain a regression model to predict key ASR metrics like Word Error Rate (WER)\nand Character Error Rate (CER). We experiment with over 40 models across 14\ndatasets representing both standard and in-the-wild testing conditions. Our\nresults show that we approximate the metrics within a single-digit absolute\ndifference across all experimental configurations, outperforming the most\nrecent baseline by more than 50\\%.", "published": "2025-02-18 01:10:17", "link": "http://arxiv.org/abs/2502.12408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Transcription, Found in Distribution Shift: Demystifying\n  Hallucination in Speech Foundation Models", "abstract": "Speech foundation models trained at a massive scale, both in terms of model\nand data size, result in robust systems capable of performing multiple speech\ntasks, including automatic speech recognition (ASR). These models transcend\nlanguage and domain barriers, yet effectively measuring their performance\nremains a challenge. Traditional metrics like word error rate (WER) and\ncharacter error rate (CER) are commonly used to evaluate ASR performance but\noften fail to reflect transcription quality in critical contexts, particularly\nwhen detecting fabricated outputs. This phenomenon, known as hallucination, is\nespecially concerning in high-stakes domains such as healthcare, legal, and\naviation, where errors can have severe consequences. In our work, we address\nthis gap by investigating hallucination in ASR models. We examine how factors\nsuch as distribution shifts, model size, and model architecture influence the\nhallucination error rate (HER), a metric we introduce to quantify\nhallucinations. Our analysis of 20 ASR models reveals \\numinsights~key\ninsights: (1) High WERs can mask low hallucination rates, while low WERs may\nconceal dangerous hallucinations. (2) Synthetic noise, both adversarial and\ncommon perturbations like white noise, pitch shift, and time stretching,\nincrease HER. (3) Distribution shift correlates strongly with HER ($\\alpha =\n0.91$). Our findings highlight the importance of incorporating HER alongside\ntraditional metrics like WER to better assess ASR model performance,\nparticularly in high-stakes domains.", "published": "2025-02-18 01:25:39", "link": "http://arxiv.org/abs/2502.12414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wi-Chat: Large Language Model Powered Wi-Fi Sensing", "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated\nremarkable capabilities across diverse tasks. However, their potential to\nintegrate physical model knowledge for real-world signal interpretation remains\nlargely unexplored. In this work, we introduce Wi-Chat, the first LLM-powered\nWi-Fi-based human activity recognition system. We demonstrate that LLMs can\nprocess raw Wi-Fi signals and infer human activities by incorporating Wi-Fi\nsensing principles into prompts. Our approach leverages physical model insights\nto guide LLMs in interpreting Channel State Information (CSI) data without\ntraditional signal processing techniques. Through experiments on real-world\nWi-Fi datasets, we show that LLMs exhibit strong reasoning capabilities,\nachieving zero-shot activity recognition. These findings highlight a new\nparadigm for Wi-Fi sensing, expanding LLM applications beyond conventional\nlanguage tasks and enhancing the accessibility of wireless sensing for\nreal-world deployments.", "published": "2025-02-18 01:43:31", "link": "http://arxiv.org/abs/2502.12421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Should I Trust You? Detecting Deception in Negotiations using\n  Counterfactual RL", "abstract": "An increasingly prevalent socio-technical problem is people being taken in by\noffers that sound ``too good to be true'', where persuasion and trust shape\ndecision-making. This paper investigates how \\abr{ai} can help detect these\ndeceptive scenarios. We analyze how humans strategically deceive each other in\n\\textit{Diplomacy}, a board game that requires both natural language\ncommunication and strategic reasoning. This requires extracting logical forms\nof proposed agreements in player communications and computing the relative\nrewards of the proposal using agents' value functions. Combined with text-based\nfeatures, this can improve our deception detection. Our method detects human\ndeception with a high precision when compared to a Large Language Model\napproach that flags many true messages as deceptive. Future human-\\abr{ai}\ninteraction tools can build on our methods for deception detection by\ntriggering \\textit{friction} to give users a chance of interrogating suspicious\nproposals.", "published": "2025-02-18 02:11:41", "link": "http://arxiv.org/abs/2502.12436v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DSMoE: Matrix-Partitioned Experts with Dynamic Routing for\n  Computation-Efficient Dense LLMs", "abstract": "As large language models continue to scale, computational costs and resource\nconsumption have emerged as significant challenges. While existing\nsparsification methods like pruning reduce computational overhead, they risk\nlosing model knowledge through parameter removal. This paper proposes DSMoE\n(Dynamic Sparse Mixture-of-Experts), a novel approach that achieves\nsparsification by partitioning pre-trained FFN layers into computational\nblocks. We implement adaptive expert routing using sigmoid activation and\nstraight-through estimators, enabling tokens to flexibly access different\naspects of model knowledge based on input complexity. Additionally, we\nintroduce a sparsity loss term to balance performance and computational\nefficiency. Extensive experiments on LLaMA models demonstrate that under\nequivalent computational constraints, DSMoE achieves superior performance\ncompared to existing pruning and MoE approaches across language modeling and\ndownstream tasks, particularly excelling in generation tasks. Analysis reveals\nthat DSMoE learns distinctive layerwise activation patterns, providing new\ninsights for future MoE architecture design.", "published": "2025-02-18 02:37:26", "link": "http://arxiv.org/abs/2502.12455v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Evaluation of Encoder Architectures for Fast Real-Time Long\n  Conversational Understanding", "abstract": "Analyzing long text data such as customer call transcripts is a\ncost-intensive and tedious task. Machine learning methods, namely Transformers,\nare leveraged to model agent-customer interactions. Unfortunately, Transformers\nadhere to fixed-length architectures and their self-attention mechanism scales\nquadratically with input length. Such limitations make it challenging to\nleverage traditional Transformers for long sequence tasks, such as\nconversational understanding, especially in real-time use cases. In this paper\nwe explore and evaluate recently proposed efficient Transformer variants (e.g.\nPerformer, Reformer) and a CNN-based architecture for real-time and near\nreal-time long conversational understanding tasks. We show that CNN-based\nmodels are dynamic, ~2.6x faster to train, ~80% faster inference and ~72% more\nmemory efficient compared to Transformers on average. Additionally, we evaluate\nthe CNN model using the Long Range Arena benchmark to demonstrate\ncompetitiveness in general long document analysis.", "published": "2025-02-18 02:40:13", "link": "http://arxiv.org/abs/2502.12458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emulating Retrieval Augmented Generation via Prompt Engineering for\n  Enhanced Long Context Comprehension in LLMs", "abstract": "This paper addresses the challenge of comprehending very long contexts in\nLarge Language Models (LLMs) by proposing a method that emulates Retrieval\nAugmented Generation (RAG) through specialized prompt engineering and\nchain-of-thought (CoT) reasoning. While recent LLMs support over 100,000 tokens\nin a single prompt, simply enlarging context windows has not guaranteed robust\nmulti-hop reasoning when key details are scattered across massive input. Our\napproach treats the model as both the retriever and the reasoner: it first tags\nrelevant segments within a long passage, then employs a stepwise CoT workflow\nto integrate these pieces of evidence. This single-pass method thereby reduces\nreliance on an external retriever, yet maintains focus on crucial segments. We\nevaluate our approach on selected tasks from BABILong, which interleaves\nstandard bAbI QA problems with large amounts of distractor text. Compared to\nbaseline (no retrieval) and naive RAG pipelines, our approach more accurately\nhandles multi-fact questions such as object location tracking, counting, and\nindefinite knowledge. Furthermore, we analyze how prompt structure, including\nthe order of question, relevant-text tags, and overall instructions,\nsignificantly affects performance. These findings underscore that optimized\nprompt engineering, combined with guided reasoning, can enhance LLMs'\nlong-context comprehension and serve as a lightweight alternative to\ntraditional retrieval pipelines.", "published": "2025-02-18 02:49:40", "link": "http://arxiv.org/abs/2502.12462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety\n  Guardrails in Large Language Models", "abstract": "Deploying large language models (LLMs) in real-world applications requires\nrobust safety guard models to detect and block harmful user prompts. While\nlarge safety guard models achieve strong performance, their computational cost\nis substantial. To mitigate this, smaller distilled models are used, but they\noften underperform on \"hard\" examples where the larger model provides accurate\npredictions. We observe that many inputs can be reliably handled by the smaller\nmodel, while only a small fraction require the larger model's capacity.\nMotivated by this, we propose SafeRoute, a binary router that distinguishes\nhard examples from easy ones. Our method selectively applies the larger safety\nguard model to the data that the router considers hard, improving efficiency\nwhile maintaining accuracy compared to solely using the larger safety guard\nmodel. Experimental results on multiple benchmark datasets demonstrate that our\nadaptive model selection significantly enhances the trade-off between\ncomputational cost and safety performance, outperforming relevant baselines.", "published": "2025-02-18 02:51:17", "link": "http://arxiv.org/abs/2502.12464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking", "abstract": "Large Language Models (LLMs) exhibit impressive reasoning abilities, yet\ntheir reliance on structured step-by-step processing reveals a critical\nlimitation. While human cognition fluidly adapts between intuitive, heuristic\n(System 1) and analytical, deliberative (System 2) reasoning depending on the\ncontext, LLMs lack this dynamic flexibility. This rigidity can lead to brittle\nand unreliable performance when faced with tasks that deviate from their\ntrained patterns. To address this, we create a dataset of 2,000 samples with\nvalid System 1 and System 2 answers, explicitly align LLMs with these reasoning\nstyles, and evaluate their performance across reasoning benchmarks. Our results\nreveal an accuracy-efficiency trade-off: System 2-aligned models excel in\narithmetic and symbolic reasoning, while System 1-aligned models perform better\nin commonsense tasks. A mechanistic analysis of model responses shows that\nSystem 1 models employ more definitive answers, whereas System 2 models\ndemonstrate greater uncertainty. Interpolating between these extremes produces\na monotonic transition in reasoning accuracy, preserving coherence. This work\nchallenges the assumption that step-by-step reasoning is always optimal and\nhighlights the need for adapting reasoning strategies based on task demands.", "published": "2025-02-18 02:58:37", "link": "http://arxiv.org/abs/2502.12470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoCo-CoLa: Evaluating Language Adherence in Multilingual LLMs", "abstract": "Multilingual Large Language Models (LLMs) develop cross-lingual abilities\ndespite being trained on limited parallel data. However, they often struggle to\ngenerate responses in the intended language, favoring high-resource languages\nsuch as English. In this work, we introduce CoCo-CoLa (Correct Concept -\nCorrect Language), a novel metric to evaluate language adherence in\nmultilingual LLMs. Using fine-tuning experiments on a closed-book QA task\nacross seven languages, we analyze how training in one language affects others'\nperformance. Our findings reveal that multilingual models share task knowledge\nacross languages but exhibit biases in the selection of output language. We\nidentify language-specific layers, showing that final layers play a crucial\nrole in determining output language. Accordingly, we propose a partial training\nstrategy that selectively fine-tunes key layers, improving language adherence\nwhile significantly reducing computational cost. Our method achieves comparable\nor superior performance to full fine-tuning, particularly for low-resource\nlanguages, offering a more efficient multilingual adaptation.", "published": "2025-02-18 03:03:53", "link": "http://arxiv.org/abs/2502.12476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Savaal: Scalable Concept-Driven Question Generation to Enhance Human\n  Learning", "abstract": "Assessing and enhancing human learning through question-answering is vital,\nyet automating this process remains challenging. While large language models\n(LLMs) excel at summarization and query responses, their ability to generate\nmeaningful questions for learners is underexplored.\n  We propose Savaal, a scalable question-generation system with three\nobjectives: (i) scalability, enabling question generation from hundreds of\npages of text (ii) depth of understanding, producing questions beyond factual\nrecall to test conceptual reasoning, and (iii) domain-independence,\nautomatically generating questions across diverse knowledge areas. Instead of\nproviding an LLM with large documents as context, Savaal improves results with\na three-stage processing pipeline. Our evaluation with 76 human experts on 71\npapers and PhD dissertations shows that Savaal generates questions that better\ntest depth of understanding by 6.5X for dissertations and 1.5X for papers\ncompared to a direct-prompting LLM baseline. Notably, as document length\nincreases, Savaal's advantages in higher question quality and lower cost become\nmore pronounced.", "published": "2025-02-18 03:05:08", "link": "http://arxiv.org/abs/2502.12477v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MSE-Adapter: A Lightweight Plugin Endowing LLMs with the Capability to\n  Perform Multimodal Sentiment Analysis and Emotion Recognition", "abstract": "Current Multimodal Sentiment Analysis (MSA) and Emotion Recognition in\nConversations (ERC) methods based on pre-trained language models exhibit two\nprimary limitations:\n  1) Once trained for MSA and ERC tasks, these pre-trained language models lose\ntheir original generalized capabilities. 2) They demand considerable\ncomputational resources. As the size of pre-trained language models continues\nto grow, training larger multimodal sentiment analysis models using previous\napproaches could result in unnecessary computational cost. In response to this\nchallenge, we propose \\textbf{M}ultimodal \\textbf{S}entiment Analysis and\n\\textbf{E}motion Recognition \\textbf{Adapter} (MSE-Adapter), a lightweight and\nadaptable plugin. This plugin enables a large language model (LLM) to carry out\nMSA or ERC tasks with minimal computational overhead (only introduces\napproximately 2.6M to 2.8M trainable parameters upon the 6/7B models), while\npreserving the intrinsic capabilities of the LLM. In the MSE-Adapter, the\nText-Guide-Mixer (TGM) module is introduced to establish explicit connections\nbetween non-textual and textual modalities through the Hadamard product. This\nallows non-textual modalities to better align with textual modalities at the\nfeature level, promoting the generation of higher-quality pseudo tokens.\nExtensive experiments were conducted on four public English and Chinese\ndatasets using consumer-grade GPUs and open-source LLMs (Qwen-1.8B,\nChatGLM3-6B-base, and LLaMA2-7B) as the backbone. The results demonstrate the\neffectiveness of the proposed plugin. The code will be released on GitHub after\na blind review.", "published": "2025-02-18 03:06:29", "link": "http://arxiv.org/abs/2502.12478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Knowledge Microscope: Features as Better Analytical Lenses than\n  Neurons", "abstract": "Previous studies primarily utilize MLP neurons as units of analysis for\nunderstanding the mechanisms of factual knowledge in Language Models (LMs);\nhowever, neurons suffer from polysemanticity, leading to limited knowledge\nexpression and poor interpretability. In this paper, we first conduct\npreliminary experiments to validate that Sparse Autoencoders (SAE) can\neffectively decompose neurons into features, which serve as alternative\nanalytical units. With this established, our core findings reveal three key\nadvantages of features over neurons: (1) Features exhibit stronger influence on\nknowledge expression and superior interpretability. (2) Features demonstrate\nenhanced monosemanticity, showing distinct activation patterns between related\nand unrelated facts. (3) Features achieve better privacy protection than\nneurons, demonstrated through our proposed FeatureEdit method, which\nsignificantly outperforms existing neuron-based approaches in erasing\nprivacy-sensitive information from LMs.Code and dataset will be available.", "published": "2025-02-18 03:09:55", "link": "http://arxiv.org/abs/2502.12483v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning", "abstract": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications.", "published": "2025-02-18 03:15:55", "link": "http://arxiv.org/abs/2502.12486v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code\n  Generation", "abstract": "Deep learning-based code generation has completely transformed the way\ndevelopers write programs today. Existing approaches to code generation have\nfocused either on the Sequence-to-Sequence paradigm, which generates target\ncode as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputs\ncode as a sequence of actions. While these two paradigms are intuitively\ncomplementary, their combination has not been previously explored. By comparing\nthe code generated under these two paradigms, we find that integrating them\nholds significant potential. In this paper, we propose UniGenCoder for\ncode-related generation tasks, which consists of a shared encoder, a shared\ndecoder with a minimal set of additional parameters to unify two paradigms, and\na selector that dynamically chooses optimal paradigm for each instance. Also,\nduring the model training, we first perform the multi-task learning and\ndistillation strategies to facilitate knowledge transfer between two paradigms,\nand then leverage contrastive learning to train the selector. Experimental\nresults on the text-to-code and code-to-code generation tasks demonstrate the\neffectiveness of our proposed model. We release our code at\nhttps://github.com/DeepLearnXMU/UniGenCoder.", "published": "2025-02-18 03:19:48", "link": "http://arxiv.org/abs/2502.12490v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for\n  LLM-as-a-Judge", "abstract": "LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become\na widely adopted auto-evaluation method. However, its reliability is\ncompromised by the CoT reasoning's inability to capture comprehensive and\ndeeper details, often leading to incomplete outcomes. Existing methods mainly\nrely on majority voting or criteria expansion, which is insufficient to address\nthe limitation in CoT. We propose Crowd-based Comparative Evaluation, which\nintroduces additional crowd responses to compare with the candidate responses,\nthereby exposing deeper and more comprehensive details within the candidate\nresponses. This process effectively guides LLM-as-a-Judge to provide a more\ndetailed CoT judgment. Extensive experiments demonstrate that our approach\nenhances evaluation reliability, achieving an average accuracy gain of 6.7%\nacross five benchmarks. Moreover, our method produces higher-quality CoTs that\nfacilitate judge distillation and exhibit superior performance in rejection\nsampling for supervised fine-tuning (SFT), referred to as crowd rejection\nsampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs\ngenerated by ours are more comprehensive and of higher quality, and evaluation\naccuracy improves as inference scales.", "published": "2025-02-18 03:31:06", "link": "http://arxiv.org/abs/2502.12501v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient OpAmp Adaptation for Zoom Attention to Golden Contexts", "abstract": "Large language models (LLMs) have shown significant promise in\nquestion-answering (QA) tasks, particularly in retrieval-augmented generation\n(RAG) scenarios and long-context applications. However, their performance is\nhindered by noisy reference documents, which often distract from essential\ninformation. Despite fine-tuning efforts, Transformer-based architectures\nstruggle to prioritize relevant content. This is evidenced by their tendency to\nallocate disproportionate attention to irrelevant or later-positioned\ndocuments. Recent work proposes the differential attention mechanism to address\nthis issue, but this mechanism is limited by an unsuitable common-mode\nrejection ratio (CMRR) and high computational costs. Inspired by the\noperational amplifier (OpAmp), we propose the OpAmp adaptation to address these\nchallenges, which is implemented with adapters efficiently. By integrating the\nadapter into pre-trained Transformer blocks, our approach enhances focus on the\ngolden context without costly training from scratch. Empirical evaluations on\nnoisy-context benchmarks reveal that our Qwen2.5-OpAmp-72B model, trained with\nour OpAmp adaptation, surpasses the performance of state-of-the-art LLMs,\nincluding DeepSeek-V3 and GPT-4o.", "published": "2025-02-18 03:35:20", "link": "http://arxiv.org/abs/2502.12502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-Guided Multi-Level Perturbation Analysis of Large Language Models\n  in Automated Peer Review", "abstract": "We propose an aspect-guided, multi-level perturbation framework to evaluate\nthe robustness of Large Language Models (LLMs) in automated peer review. Our\nframework explores perturbations in three key components of the peer review\nprocess-papers, reviews, and rebuttals-across several quality aspects,\nincluding contribution, soundness, presentation, tone, and completeness. By\napplying targeted perturbations and examining their effects on both\nLLM-as-Reviewer and LLM-as-Meta-Reviewer, we investigate how aspect-based\nmanipulations, such as omitting methodological details from papers or altering\nreviewer conclusions, can introduce significant biases in the review process.\nWe identify several potential vulnerabilities: review conclusions that\nrecommend a strong reject may significantly influence meta-reviews, negative or\nmisleading reviews may be wrongly interpreted as thorough, and incomplete or\nhostile rebuttals can unexpectedly lead to higher acceptance rates. Statistical\ntests show that these biases persist under various Chain-of-Thought prompting\nstrategies, highlighting the lack of robust critical evaluation in current\nLLMs. Our framework offers a practical methodology for diagnosing these\nvulnerabilities, thereby contributing to the development of more reliable and\nrobust automated reviewing systems.", "published": "2025-02-18 03:50:06", "link": "http://arxiv.org/abs/2502.12510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Extract Frame-Semantic Arguments?", "abstract": "Frame-semantic parsing is a critical task in natural language understanding,\nyet the ability of large language models (LLMs) to extract frame-semantic\narguments remains underexplored. This paper presents a comprehensive evaluation\nof LLMs on frame-semantic argument identification, analyzing the impact of\ninput representation formats, model architectures, and generalization to unseen\nand out-of-domain samples. Our experiments, spanning models from 0.5B to 78B\nparameters, reveal that JSON-based representations significantly enhance\nperformance, and while larger models generally perform better, smaller models\ncan achieve competitive results through fine-tuning. We also introduce a novel\napproach to frame identification leveraging predicted frame elements, achieving\nstate-of-the-art performance on ambiguous targets. Despite strong\ngeneralization capabilities, our analysis finds that LLMs still struggle with\nout-of-domain data.", "published": "2025-02-18 04:01:50", "link": "http://arxiv.org/abs/2502.12516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How does a Language-Specific Tokenizer affect LLMs?", "abstract": "The necessity of language-specific tokenizers intuitively appears crucial for\neffective natural language processing, yet empirical analyses on their\nsignificance and underlying reasons are lacking. This study explores how\nlanguage-specific tokenizers influence the behavior of Large Language Models\npredominantly trained with English text data, through the case study of Korean.\nThe research unfolds in two main stages: (1) the development of a\nKorean-specific extended tokenizer and (2) experiments to compare models with\nthe basic tokenizer and the extended tokenizer through various Next Token\nPrediction tasks. Our in-depth analysis reveals that the extended tokenizer\ndecreases confidence in incorrect predictions during generation and reduces\ncross-entropy in complex tasks, indicating a tendency to produce less\nnonsensical outputs. Consequently, the extended tokenizer provides stability\nduring generation, potentially leading to higher performance in downstream\ntasks.", "published": "2025-02-18 05:54:56", "link": "http://arxiv.org/abs/2502.12560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self Iterative Label Refinement via Robust Unlabeled Learning", "abstract": "Recent advances in large language models (LLMs) have yielded impressive\nperformance on various tasks, yet they often depend on high-quality feedback\nthat can be costly. Self-refinement methods attempt to leverage LLMs' internal\nevaluation mechanisms with minimal human supervision; however, these approaches\nfrequently suffer from inherent biases and overconfidence, especially in\ndomains where the models lack sufficient internal knowledge, resulting in\nperformance degradation. As an initial step toward enhancing self-refinement\nfor broader applications, we introduce an iterative refinement pipeline that\nemploys the Unlabeled-Unlabeled learning framework to improve LLM-generated\npseudo-labels for classification tasks. By exploiting two unlabeled datasets\nwith differing positive class ratios, our approach iteratively denoises and\nrefines the initial pseudo-labels, thereby mitigating the adverse effects of\ninternal biases with minimal human supervision. Evaluations on diverse\ndatasets, including low-resource language corpora, patent classifications, and\nprotein structure categorizations, demonstrate that our method consistently\noutperforms both initial LLM's classification performance and the\nself-refinement approaches by cutting-edge models (e.g., GPT-4o and\nDeepSeek-R1).", "published": "2025-02-18 06:04:18", "link": "http://arxiv.org/abs/2502.12565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful\n  Synthetic Data", "abstract": "Despite the growing development of long-context large language models (LLMs),\ndata-centric approaches relying on synthetic data have been hindered by issues\nrelated to faithfulness, which limit their effectiveness in enhancing model\nperformance on tasks such as long-context reasoning and question answering\n(QA). These challenges are often exacerbated by misinformation caused by lack\nof verification, reasoning without attribution, and potential knowledge\nconflicts. We propose LongFaith, a novel pipeline for synthesizing faithful\nlong-context reasoning instruction datasets. By integrating ground truth and\ncitation-based reasoning prompts, we eliminate distractions and improve the\naccuracy of reasoning chains, thus mitigating the need for costly verification\nprocesses. We open-source two synthesized datasets, LongFaith-SFT and\nLongFaith-PO, which systematically address multiple dimensions of faithfulness,\nincluding verified reasoning, attribution, and contextual grounding. Extensive\nexperiments on multi-hop reasoning datasets and LongBench demonstrate that\nmodels fine-tuned on these datasets significantly improve performance. Our\nablation studies highlight the scalability and adaptability of the LongFaith\npipeline, showcasing its broad applicability in developing long-context LLMs.", "published": "2025-02-18 06:40:23", "link": "http://arxiv.org/abs/2502.12583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PASER: Post-Training Data Selection for Efficient Pruned Large Language\n  Model Recovery", "abstract": "Model pruning is an effective approach for compressing large language models.\nHowever, this process often leads to significant degradation of model\ncapabilities. While post-training techniques such as instruction tuning are\ncommonly employed to recover model performance, existing methods often overlook\nthe uneven deterioration of model capabilities and incur high computational\ncosts. Moreover, some instruction data irrelevant to model capability recovery\nmay introduce negative effects. To address these challenges, we propose the\n\\textbf{P}ost-training d\\textbf{A}ta \\textbf{S}election method for\n\\textbf{E}fficient pruned large language model \\textbf{R}ecovery\n(\\textbf{PASER}). PASER aims to identify instructions where model capabilities\nare most severely compromised within a certain recovery data budget. Our\napproach first applies manifold learning and spectral clustering to group\nrecovery data in the semantic space, revealing capability-specific instruction\nsets. We then adaptively allocate the data budget to different clusters based\non the degrees of model capability degradation. In each cluster, we prioritize\ndata samples where model performance has declined dramatically. To mitigate\npotential negative transfer, we also detect and filter out conflicting or\nirrelevant recovery data. Extensive experiments demonstrate that PASER\nsignificantly outperforms conventional baselines, effectively recovering the\ngeneral capabilities of pruned LLMs while utilizing merely 4\\%-20\\% of the\noriginal post-training data.", "published": "2025-02-18 07:11:08", "link": "http://arxiv.org/abs/2502.12594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bring Your Own Knowledge: A Survey of Methods for LLM Knowledge\n  Expansion", "abstract": "Adapting large language models (LLMs) to new and diverse knowledge is\nessential for their lasting effectiveness in real-world applications. This\nsurvey provides an overview of state-of-the-art methods for expanding the\nknowledge of LLMs, focusing on integrating various knowledge types, including\nfactual information, domain expertise, language proficiency, and user\npreferences. We explore techniques, such as continual learning, model editing,\nand retrieval-based explicit adaptation, while discussing challenges like\nknowledge consistency and scalability. Designed as a guide for researchers and\npractitioners, this survey sheds light on opportunities for advancing LLMs as\nadaptable and robust knowledge systems.", "published": "2025-02-18 07:15:28", "link": "http://arxiv.org/abs/2502.12598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COPU: Conformal Prediction for Uncertainty Quantification in Natural\n  Language Generation", "abstract": "Uncertainty Quantification (UQ) for Natural Language Generation (NLG) is\ncrucial for assessing the performance of Large Language Models (LLMs), as it\nreveals confidence in predictions, identifies failure modes, and gauges output\nreliability. Conformal Prediction (CP), a model-agnostic method that generates\nprediction sets with a specified error rate, has been adopted for UQ in\nclassification tasks, where the size of the prediction set indicates the\nmodel's uncertainty. However, when adapting CP to NLG, the sampling-based\nmethod for generating candidate outputs cannot guarantee the inclusion of the\nground truth, limiting its applicability across a wide range of error rates. To\naddress this, we propose \\ourmethod, a method that explicitly adds the ground\ntruth to the candidate outputs and uses logit scores to measure nonconformity.\nOur experiments with six LLMs on four NLG tasks show that \\ourmethod\noutperforms baseline methods in calibrating error rates and empirical cover\nrates, offering accurate UQ across a wide range of user-specified error rates.", "published": "2025-02-18 07:25:12", "link": "http://arxiv.org/abs/2502.12601v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Writes What: Unveiling the Impact of Author Roles on AI-generated\n  Text Detection", "abstract": "The rise of Large Language Models (LLMs) necessitates accurate AI-generated\ntext detection. However, current approaches largely overlook the influence of\nauthor characteristics. We investigate how sociolinguistic attributes-gender,\nCEFR proficiency, academic field, and language environment-impact\nstate-of-the-art AI text detectors. Using the ICNALE corpus of human-authored\ntexts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous\nevaluation employing multi-factor ANOVA and weighted least squares (WLS). Our\nresults reveal significant biases: CEFR proficiency and language environment\nconsistently affected detector accuracy, while gender and academic field showed\ndetector-dependent effects. These findings highlight the crucial need for\nsocially aware AI text detection to avoid unfairly penalizing specific\ndemographic groups. We offer novel empirical evidence, a robust statistical\nframework, and actionable insights for developing more equitable and reliable\ndetection systems in real-world, out-of-domain contexts. This work paves the\nway for future research on bias mitigation, inclusive evaluation benchmarks,\nand socially responsible LLM detectors.", "published": "2025-02-18 07:49:31", "link": "http://arxiv.org/abs/2502.12611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions", "abstract": "Chain-of-Though (CoT) represents a common strategy for reasoning in Large\nLanguage Models (LLMs) by decomposing complex tasks into intermediate inference\nsteps. However, explanations generated via CoT are susceptible to content\nbiases that negatively affect their robustness and faithfulness. To mitigate\nexisting limitations, recent work has proposed using logical formalisms coupled\nwith external symbolic solvers. However, fully symbolic approaches possess the\nbottleneck of requiring a complete translation from natural language to formal\nlanguages, a process that affects efficiency and flexibility. To achieve a\ntrade-off, this paper investigates methods to disentangle content from logical\nreasoning without a complete formalisation. In particular, we present QuaSAR\n(for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to\noperate at a higher level of abstraction via quasi-symbolic explanations. Our\nframework leverages the capability of LLMs to formalise only relevant variables\nand predicates, enabling the coexistence of symbolic elements with natural\nlanguage. We show the impact of QuaSAR for in-context learning and for\nconstructing demonstrations to improve the reasoning capabilities of smaller\nmodels. Our experiments show that quasi-symbolic abstractions can improve\nCoT-based methods by up to 8% accuracy, enhancing robustness and consistency on\nchallenging adversarial variations on both natural language (i.e. MMLU-Redux)\nand symbolic reasoning tasks (i.e., GSM-Symbolic).", "published": "2025-02-18 07:58:48", "link": "http://arxiv.org/abs/2502.12616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "R.R.: Unveiling LLM Training Privacy through Recollection and Ranking", "abstract": "Large Language Models (LLMs) pose significant privacy risks, potentially\nleaking training data due to implicit memorization. Existing privacy attacks\nprimarily focus on membership inference attacks (MIAs) or data extraction\nattacks, but reconstructing specific personally identifiable information (PII)\nin LLM's training data remains challenging. In this paper, we propose R.R.\n(Recollect and Rank), a novel two-step privacy stealing attack that enables\nattackers to reconstruct PII entities from scrubbed training data where the PII\nentities have been masked. In the first stage, we introduce a prompt paradigm\nnamed recollection, which instructs the LLM to repeat a masked text but fill in\nmasks. Then we can use PII identifiers to extract recollected PII candidates.\nIn the second stage, we design a new criterion to score each PII candidate and\nrank them. Motivated by membership inference, we leverage the reference model\nas a calibration to our criterion. Experiments across three popular PII\ndatasets demonstrate that the R.R. achieves better PII identical performance\ncompared to baselines. These results highlight the vulnerability of LLMs to PII\nleakage even when training data has been scrubbed. We release the replicate\npackage of R.R. at a link.", "published": "2025-02-18 09:05:59", "link": "http://arxiv.org/abs/2502.12658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demystifying Multilingual Chain-of-Thought in Process Reward Modeling", "abstract": "Large language models (LLMs) are designed to perform a wide range of tasks.\nTo improve their ability to solve complex problems requiring multi-step\nreasoning, recent research leverages process reward modeling to provide\nfine-grained feedback at each step of the reasoning process for reinforcement\nlearning (RL), but it predominantly focuses on English. In this paper, we\ntackle the critical challenge of extending process reward models (PRMs) to\nmultilingual settings. To achieve this, we train multilingual PRMs on a dataset\nspanning seven languages, which is translated from English. Through\ncomprehensive evaluations on two widely used reasoning benchmarks across 11\nlanguages, we demonstrate that multilingual PRMs not only improve average\naccuracy but also reduce early-stage reasoning errors. Furthermore, our results\nhighlight the sensitivity of multilingual PRMs to both the number of training\nlanguages and the volume of English data, while also uncovering the benefits\narising from more candidate responses and trainable parameters. This work opens\npromising avenues for robust multilingual applications in complex, multi-step\nreasoning tasks. In addition, we release the code to foster research along this\nline.", "published": "2025-02-18 09:11:44", "link": "http://arxiv.org/abs/2502.12663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization", "abstract": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.", "published": "2025-02-18 09:11:51", "link": "http://arxiv.org/abs/2502.12665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Best-of-N Sampling Strategies for Language Model Alignment", "abstract": "Best-of-N (BoN) sampling with a reward model has been shown to be an\neffective strategy for aligning Large Language Models (LLMs) with human\npreferences at the time of decoding. BoN sampling is susceptible to a problem\nknown as reward hacking. Since the reward model is an imperfect proxy for the\ntrue objective, an excessive focus on optimizing its value can lead to a\ncompromise of its performance on the true objective. Previous work proposes\nRegularized BoN sampling (RBoN), a BoN sampling with regularization to the\nobjective, and shows that it outperforms BoN sampling so that it mitigates\nreward hacking and empirically (Jinnai et al., 2024). However, Jinnai et al.\n(2024) introduce RBoN based on a heuristic and they lack the analysis of why\nsuch regularization strategy improves the performance of BoN sampling. The aim\nof this study is to analyze the effect of BoN sampling on regularization\nstrategies. Using the regularization strategies corresponds to robust\noptimization, which maximizes the worst case over a set of possible\nperturbations in the proxy reward. Although the theoretical guarantees are not\ndirectly applicable to RBoN, RBoN corresponds to a practical implementation.\nThis paper proposes an extension of the RBoN framework, called Stochastic RBoN\nsampling (SRBoN), which is a theoretically guaranteed approach to worst-case\nRBoN in proxy reward. We then perform an empirical evaluation using the\nAlpacaFarm and Anthropic's hh-rlhf datasets to evaluate which factors of the\nregularization strategies contribute to the improvement of the true proxy\nreward. In addition, we also propose another simple RBoN method, the Sentence\nLength Regularized BoN, which has a better performance in the experiment as\ncompared to the previous methods.", "published": "2025-02-18 09:18:02", "link": "http://arxiv.org/abs/2502.12668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Baichuan-M1: Pushing the Medical Capability of Large Language Models", "abstract": "The current generation of large language models (LLMs) is typically designed\nfor broad, general-purpose applications, while domain-specific LLMs, especially\nin vertical fields like medicine, remain relatively scarce. In particular, the\ndevelopment of highly efficient and practical LLMs for the medical domain is\nchallenging due to the complexity of medical knowledge and the limited\navailability of high-quality data. To bridge this gap, we introduce\nBaichuan-M1, a series of large language models specifically optimized for\nmedical applications. Unlike traditional approaches that simply continue\npretraining on existing models or apply post-training to a general base model,\nBaichuan-M1 is trained from scratch with a dedicated focus on enhancing medical\ncapabilities. Our model is trained on 20 trillion tokens and incorporates a\nrange of effective training methods that strike a balance between general\ncapabilities and medical expertise. As a result, Baichuan-M1 not only performs\nstrongly across general domains such as mathematics and coding but also excels\nin specialized medical fields. We have open-sourced Baichuan-M1-14B, a mini\nversion of our model, which can be accessed through the following links.", "published": "2025-02-18 09:21:12", "link": "http://arxiv.org/abs/2502.12671v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Theoretical Guarantees for Minimum Bayes Risk Decoding", "abstract": "Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing\nthe expected utility value of an underlying human distribution. While prior\nwork has shown the effectiveness of MBR decoding through empirical evaluation,\nfew studies have analytically investigated why the method is effective. As a\nresult of our analysis, we show that, given the size $n$ of the reference\nhypothesis set used in computation, MBR decoding approaches the optimal\nsolution with high probability at a rate of $O\\left(n^{-\\frac{1}{2}}\\right)$,\nunder certain assumptions, even though the language space $Y$ is significantly\nlarger $Y\\gg n$. This result helps to theoretically explain the strong\nperformance observed in several prior empirical studies on MBR decoding. In\naddition, we provide the performance gap for maximum-a-posteriori (MAP)\ndecoding and compare it to MBR decoding. The result of this paper indicates\nthat MBR decoding tends to converge to the optimal solution faster than MAP\ndecoding in several cases.", "published": "2025-02-18 09:43:15", "link": "http://arxiv.org/abs/2502.12685v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Novelty: Improve the Diversity and Novelty of Contents Generated\n  by Large Language Models via inference-time Multi-Views Brainstorming", "abstract": "Large Language Models (LLMs) demonstrate remarkable proficiency in generating\naccurate and fluent text. However, they often struggle with diversity and\nnovelty, leading to repetitive or overly deterministic responses. These\nlimitations stem from constraints in training data, including gaps in specific\nknowledge domains, outdated information, and an over-reliance on textual\nsources. Such shortcomings reduce their effectiveness in tasks requiring\ncreativity, multi-perspective reasoning, and exploratory thinking, such as LLM\nbased AI scientist agents and creative artist agents . To address this\nchallenge, we introduce inference-time multi-view brainstorming method, a novel\napproach that enriches input prompts with diverse perspectives derived from\nboth textual and visual sources, which we refere to as \"Multi-Novelty\". By\nincorporating additional contextual information as diverse starting point for\nchain of thoughts, this method enhances the variety and creativity of generated\noutputs. Importantly, our approach is model-agnostic, requiring no\narchitectural modifications and being compatible with both open-source and\nproprietary LLMs.", "published": "2025-02-18 10:04:20", "link": "http://arxiv.org/abs/2502.12700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small\n  Models for Enhanced Reasoning Distillation", "abstract": "The rapid advancement of large language models (LLMs) has significantly\nenhanced their reasoning abilities, enabling increasingly complex tasks.\nHowever, these capabilities often diminish in smaller, more computationally\nefficient models like GPT-2. Recent research shows that reasoning distillation\ncan help small models acquire reasoning capabilities, but most existing methods\nfocus primarily on improving teacher-generated reasoning paths. Our\nobservations reveal that small models can generate high-quality reasoning paths\nduring sampling, even without chain-of-thought prompting, though these paths\nare often latent due to their low probability under standard decoding\nstrategies. To address this, we propose Self-Enhanced Reasoning Training\n(SERT), which activates and leverages latent reasoning capabilities in small\nmodels through self-training on filtered, self-generated reasoning paths under\nzero-shot conditions. Experiments using OpenAI's GPT-3.5 as the teacher model\nand GPT-2 models as the student models demonstrate that SERT enhances the\nreasoning abilities of small models, improving their performance in reasoning\ndistillation.", "published": "2025-02-18 11:02:47", "link": "http://arxiv.org/abs/2502.12744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Commonsense Reasoning in Arab Culture", "abstract": "Despite progress in Arabic large language models, such as Jais and AceGPT,\ntheir evaluation on commonsense reasoning has largely relied on\nmachine-translated datasets, which lack cultural depth and may introduce\nAnglocentric biases. Commonsense reasoning is shaped by geographical and\ncultural contexts, and existing English datasets fail to capture the diversity\nof the Arab world. To address this, we introduce \\datasetname, a commonsense\nreasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13\ncountries across the Gulf, Levant, North Africa, and the Nile Valley. The\ndataset was built from scratch by engaging native speakers to write and\nvalidate culturally relevant questions for their respective countries.\n\\datasetname spans 12 daily life domains with 54 fine-grained subtopics,\nreflecting various aspects of social norms, traditions, and everyday\nexperiences. Zero-shot evaluations show that open-weight language models with\nup to 32B parameters struggle to comprehend diverse Arab cultures, with\nperformance varying across regions. These findings highlight the need for more\nculturally aware models and datasets tailored to the Arabic-speaking world.", "published": "2025-02-18 11:49:54", "link": "http://arxiv.org/abs/2502.12788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulating User Diversity in Task-Oriented Dialogue Systems using Large\n  Language Models", "abstract": "In this study, we explore the application of Large Language Models (LLMs) for\ngenerating synthetic users and simulating user conversations with a\ntask-oriented dialogue system and present detailed results and their analysis.\nWe propose a comprehensive novel approach to user simulation technique that\nuses LLMs to create diverse user profiles, set goals, engage in multi-turn\ndialogues, and evaluate the conversation success. We employ two proprietary\nLLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a\nheterogeneous base of user profiles, characterized by varied demographics,\nmultiple user goals, different conversational styles, initial knowledge levels,\ninterests, and conversational objectives. We perform a detailed analysis of the\nuser profiles generated by LLMs to assess the diversity, consistency, and\npotential biases inherent in these LLM-generated user simulations. We find that\nGPT-o1 generates more heterogeneous user distribution across most user\nattributes, while GPT-4o generates more skewed user attributes. The generated\nset of user profiles are then utilized to simulate dialogue sessions by\ninteracting with a task-oriented dialogue system.", "published": "2025-02-18 12:20:16", "link": "http://arxiv.org/abs/2502.12813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in\n  Large Language Models", "abstract": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values.", "published": "2025-02-18 12:32:11", "link": "http://arxiv.org/abs/2502.12821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional\n  Knowledge of Kazakhstan", "abstract": "Despite having a population of twenty million, Kazakhstan's culture and\nlanguage remain underrepresented in the field of natural language processing.\nAlthough large language models (LLMs) continue to advance worldwide, progress\nin Kazakh language has been limited, as seen in the scarcity of dedicated\nmodels and benchmark evaluations. To address this gap, we introduce KazMMLU,\nthe first MMLU-style dataset specifically designed for Kazakh language. KazMMLU\ncomprises 23,000 questions that cover various educational levels, including\nSTEM, humanities, and social sciences, sourced from authentic educational\nmaterials and manually validated by native speakers and educators. The dataset\nincludes 10,969 Kazakh questions and 12,031 Russian questions, reflecting\nKazakhstan's bilingual education system and rich local context. Our evaluation\nof several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,\nand DeepSeek V3) demonstrates substantial room for improvement, as even the\nbest-performing models struggle to achieve competitive performance in Kazakh\nand Russian. These findings underscore significant performance gaps compared to\nhigh-resource languages. We hope that our dataset will enable further research\nand development of Kazakh-centric LLMs. Data and code will be made available\nupon acceptance.", "published": "2025-02-18 12:48:37", "link": "http://arxiv.org/abs/2502.12829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subword models struggle with word learning, but surprisal hides it", "abstract": "We study word learning in subword and character language models with the\npsycholinguistic lexical decision task. While subword LMs struggle to discern\nwords and non-words with high accuracy, character LMs solve this task easily\nand consistently. Furthermore, when comparing word learning and syntactic\nlearning, both processes are separable in character LM where word learning\npredates syntactic learning, whereas these processes are simultaneous in\nsubword LM. This raises questions about the adequacy of subword LMs for\nmodeling language acquisition and positions character LMs as a viable\nalternative.", "published": "2025-02-18 13:09:16", "link": "http://arxiv.org/abs/2502.12835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on\n  PPG-based Heart Rate Estimation", "abstract": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent features an orchestrator that integrates user\ninteraction, data sources, and analytical tools to generate accurate health\ninsights. To evaluate its effectiveness, we implement a case study on heart\nrate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of\nPPG and Electrocardiogram (ECG) recordings in a remote health monitoring study.\nThe agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o,\nwith ECG serving as the gold standard for HR estimation. Results demonstrate\nthat our agent significantly outperforms benchmark models by achieving lower\nerror rates and more reliable HR estimations. The agent implementation is\npublicly available on GitHub.", "published": "2025-02-18 13:09:59", "link": "http://arxiv.org/abs/2502.12836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MVL-SIB: A Massively Multilingual Vision-Language Benchmark for\n  Cross-Modal Topical Matching", "abstract": "Existing multilingual vision-language (VL) benchmarks often only cover a\nhandful of languages. Consequently, evaluations of large vision-language models\n(LVLMs) predominantly target high-resource languages, underscoring the need for\nevaluation data for low-resource languages. To address this limitation, we\nintroduce MVL-SIB, a massively multilingual vision-language benchmark that\nevaluates both cross-modal and text-only topical matching across 205 languages\n-- over 100 more than the most multilingual existing VL benchmarks encompass.\nWe then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini)\non MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic\nmatching in lower-resource languages, performing no better than chance on\nlanguages like N'Koo. Our analysis further reveals that VL support in LVLMs\ndeclines disproportionately relative to textual support for lower-resource\nlanguages, as evidenced by comparison of cross-modal and text-only topical\nmatching performance. We further observe that open-weight LVLMs do not benefit\nfrom representing a topic with more than one image, suggesting that these\nmodels are not yet fully effective at handling multi-image tasks. By\ncorrelating performance on MVL-SIB with other multilingual VL benchmarks, we\nhighlight that MVL-SIB serves as a comprehensive probe of multilingual VL\nunderstanding in LVLMs.", "published": "2025-02-18 13:40:05", "link": "http://arxiv.org/abs/2502.12852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How desirable is alignment between LLMs and linguistically diverse human\n  users?", "abstract": "We discuss how desirable it is that Large Language Models (LLMs) be able to\nadapt or align their language behavior with users who may be diverse in their\nlanguage use. User diversity may come about among others due to i) age\ndifferences; ii) gender characteristics, and/or iii) multilingual experience,\nand associated differences in language processing and use. We consider\npotential consequences for usability, communication, and LLM development.", "published": "2025-02-18 14:16:03", "link": "http://arxiv.org/abs/2502.12884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Multilingual Language Models an Off-ramp for Under-resourced\n  Languages? Will we arrive at Digital Language Equality in Europe in 2030?", "abstract": "Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way.", "published": "2025-02-18 14:20:27", "link": "http://arxiv.org/abs/2502.12886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to\n  Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and\n  Gemini 2.0 Flash Thinking", "abstract": "Large Reasoning Models (LRMs) have recently extended their powerful reasoning\ncapabilities to safety checks-using chain-of-thought reasoning to decide\nwhether a request should be answered. While this new approach offers a\npromising route for balancing model utility and safety, its robustness remains\nunderexplored. To address this gap, we introduce Malicious-Educator, a\nbenchmark that disguises extremely dangerous or malicious requests beneath\nseemingly legitimate educational prompts. Our experiments reveal severe\nsecurity flaws in popular commercial-grade LRMs, including OpenAI o1/o3,\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1\nmodel initially maintains a high refusal rate of about 98%, subsequent model\nupdates significantly compromise its safety; and attackers can easily extract\ncriminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any\nadditional tricks. To further highlight these vulnerabilities, we propose\nHijacking Chain-of-Thought (H-CoT), a universal and transferable attack method\nthat leverages the model's own displayed intermediate reasoning to jailbreak\nits safety reasoning mechanism. Under H-CoT, refusal rates sharply\ndecline-dropping from 98% to below 2%-and, in some instances, even transform\ninitially cautious tones into ones that are willing to provide harmful content.\nWe hope these findings underscore the urgent need for more robust safety\nmechanisms to preserve the benefits of advanced reasoning capabilities without\ncompromising ethical standards.", "published": "2025-02-18 14:29:12", "link": "http://arxiv.org/abs/2502.12893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual European Language Models: Benchmarking Approaches and\n  Challenges", "abstract": "The breakthrough of generative large language models (LLMs) that can solve\ndifferent tasks through chat interaction has led to a significant increase in\nthe use of general benchmarks to assess the quality or performance of these\nmodels beyond individual applications. There is also a need for better methods\nto evaluate and also to compare models due to the ever increasing number of new\nmodels published. However, most of the established benchmarks revolve around\nthe English language. This paper analyses the benefits and limitations of\ncurrent evaluation datasets, focusing on multilingual European benchmarks. We\nanalyse seven multilingual benchmarks and identify four major challenges.\nFurthermore, we discuss potential solutions to enhance translation quality and\nmitigate cultural biases, including human-in-the-loop verification and\niterative translation ranking. Our analysis highlights the need for culturally\naware and rigorously validated benchmarks to assess the reasoning and\nquestion-answering capabilities of multilingual LLMs accurately.", "published": "2025-02-18 14:32:17", "link": "http://arxiv.org/abs/2502.12895v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks", "abstract": "In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.", "published": "2025-02-18 14:32:44", "link": "http://arxiv.org/abs/2502.12896v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM\n  Against Augmented Fraud and Phishing Inducements", "abstract": "We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to\ndefend against internet fraud and phishing in dynamic, real-world scenarios.\nFraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job\npostings, social media, and news, categorized into 5 major fraud types. Unlike\nprevious benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to\nassess LLMs' resistance to fraud at different stages, including credibility\nbuilding, urgency creation, and emotional manipulation. Furthermore, we\nevaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM\nprovides general decision-making assistance, and 2. Role-play, where the model\nassumes a specific persona, widely used in real-world agent-based interactions.\nOur evaluation reveals the significant challenges in defending against fraud\nand phishing inducement, especially in role-play settings and fake job\npostings. Additionally, we observe a substantial performance gap between\nChinese and English, underscoring the need for improved multilingual fraud\ndetection capabilities.", "published": "2025-02-18 14:47:02", "link": "http://arxiv.org/abs/2502.12904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Q-STRUM Debate: Query-Driven Contrastive Summarization for\n  Recommendation Comparison", "abstract": "Query-driven recommendation with unknown items poses a challenge for users to\nunderstand why certain items are appropriate for their needs. Query-driven\nContrastive Summarization (QCS) is a methodology designed to address this issue\nby leveraging language-based item descriptions to clarify contrasts between\nthem. However, existing state-of-the-art contrastive summarization methods such\nas STRUM-LLM fall short of this goal. To overcome these limitations, we\nintroduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs\ndebate-style prompting to generate focused and contrastive summarizations of\nitem aspects relevant to a query. Leveraging modern large language models\n(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides\nenhanced contrastive summaries. Experiments across three datasets demonstrate\nthat Q-STRUM Debate yields significant performance improvements over existing\nmethods on key contrastive summarization criteria, thus introducing a novel and\nperformant debate prompting methodology for QCS.", "published": "2025-02-18 15:01:30", "link": "http://arxiv.org/abs/2502.12921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On-Device LLMs for Home Assistant: Dual Role in Intent Detection and\n  Response Generation", "abstract": "This paper investigates whether Large Language Models (LLMs), fine-tuned on\nsynthetic but domain-representative data, can perform the twofold task of (i)\nslot and intent detection and (ii) natural language response generation for a\nsmart home assistant, while running solely on resource-limited, CPU-only edge\nhardware. We fine-tune LLMs to produce both JSON action calls and text\nresponses. Our experiments show that 16-bit and 8-bit quantized variants\npreserve high accuracy on slot and intent detection and maintain strong\nsemantic coherence in generated text, while the 4-bit model, while retaining\ngenerative fluency, suffers a noticeable drop in device-service classification\naccuracy. Further evaluations on noisy human (non-synthetic) prompts and\nout-of-domain intents confirm the models' generalization ability, obtaining\naround 80--86\\% accuracy. While the average inference time is 5--6 seconds per\nquery -- acceptable for one-shot commands but suboptimal for multi-turn\ndialogue -- our results affirm that an on-device LLM can effectively unify\ncommand interpretation and flexible response generation for home automation\nwithout relying on specialized hardware.", "published": "2025-02-18 15:03:17", "link": "http://arxiv.org/abs/2502.12923v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEFL: Harnessing Large Language Model Agents to Improve Educational\n  Feedback Systems", "abstract": "Providing high-quality feedback is crucial for student success but is\nconstrained by time, cost, and limited data availability. We introduce\nSynthetic Educational Feedback Loops (SEFL), a novel framework designed to\ndeliver immediate, on-demand feedback at scale without relying on extensive,\nreal-world student data. In SEFL, two large language models (LLMs) operate in\nteacher--student roles to simulate assignment completion and formative\nfeedback, generating abundant synthetic pairs of student work and corresponding\ncritiques. We then fine-tune smaller, more computationally efficient LLMs on\nthese synthetic pairs, enabling them to replicate key features of high-quality,\ngoal-oriented feedback. Unlike personalized tutoring approaches that offer\nmulti-turn, individualized instruction, SEFL specifically focuses on\nreplicating the teacher-->student feedback loop for diverse assignments.\nThrough both LLM-as-a-judge and human evaluations, we demonstrate that\nSEFL-tuned models outperform their non-tuned counterparts in feedback quality,\nclarity, and timeliness. These findings reveal SEFL's potential to transform\nfeedback processes for higher education and beyond, offering an ethical and\nscalable alternative to conventional manual feedback cycles.", "published": "2025-02-18 15:09:29", "link": "http://arxiv.org/abs/2502.12927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer\n  Fine-Grained Experts", "abstract": "Large language models have demonstrated exceptional performance across a wide\nrange of tasks. However, dense models usually suffer from sparse activation,\nwhere many activation values tend towards zero (i.e., being inactivated). We\nargue that this could restrict the efficient exploration of model\nrepresentation space. To mitigate this issue, we propose Finedeep, a\ndeep-layered fine-grained expert architecture for dense models. Our framework\npartitions the feed-forward neural network layers of traditional dense models\ninto small experts, arranges them across multiple sub-layers. A novel routing\nmechanism is proposed to determine each expert's contribution. We conduct\nextensive experiments across various model sizes, demonstrating that our\napproach significantly outperforms traditional dense architectures in terms of\nperplexity and benchmark performance while maintaining a comparable number of\nparameters and floating-point operations. Moreover, we find that Finedeep\nachieves optimal results when balancing depth and width, specifically by\nadjusting the number of expert sub-layers and the number of experts per\nsub-layer. Empirical results confirm that Finedeep effectively alleviates\nsparse activation and efficiently utilizes representation capacity in dense\nmodels.", "published": "2025-02-18 15:09:58", "link": "http://arxiv.org/abs/2502.12928v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning\n  in Low-Resource Languages", "abstract": "Quantifying reasoning capability in low-resource languages remains a\nchallenge in NLP due to data scarcity and limited access to annotators. While\nLLM-assisted dataset construction has proven useful for medium- and\nhigh-resource languages, its effectiveness in low-resource languages,\nparticularly for commonsense reasoning, is still unclear. In this paper, we\ncompare three dataset creation strategies: (1) LLM-assisted dataset generation,\n(2) machine translation, and (3) human-written data by native speakers, to\nbuild a culturally nuanced story comprehension dataset. We focus on Javanese\nand Sundanese, two major local languages in Indonesia, and evaluate the\neffectiveness of open-weight and closed-weight LLMs in assisting dataset\ncreation through extensive manual validation. To assess the utility of\nsynthetic data, we fine-tune language models on classification and generation\ntasks using this data and evaluate performance on a human-written test set. Our\nfindings indicate that LLM-assisted data creation outperforms machine\ntranslation.", "published": "2025-02-18 15:14:58", "link": "http://arxiv.org/abs/2502.12932v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing", "abstract": "Limited by the context window size of Large Language Models(LLMs), handling\nvarious tasks with input tokens exceeding the upper limit has been challenging,\nwhether it is a simple direct retrieval task or a complex multi-hop reasoning\ntask. Although various methods have been proposed to enhance the long-context\nprocessing capabilities of LLMs, they either incur substantial post-training\ncosts, or require additional tool modules(e.g.,RAG), or have not shown\nsignificant improvement in realistic tasks. Our work observes the correlation\nbetween the attention distribution and generated answers across each layer, and\nestablishes the attention allocation aligns with retrieval-augmented\ncapabilities through experiments. Drawing on the above insights, we propose a\nnovel method InfiniRetri that leverages the LLMs's own attention information to\nenable accurate retrieval across inputs of infinitely length. Our evaluations\nindicate that InfiniRetri achieves 100% accuracy in the\nNeedle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model,\nsurpassing other method or larger models and setting a new\nstate-of-the-art(SOTA). Moreover, our method achieves significant performance\nimprovements on real-world benchmarks, with a maximum 288% improvement. In\naddition, InfiniRetri can be applied to any Transformer-based LLMs without\nadditional training and substantially reduces inference latency and compute\noverhead in long texts. In summary, our comprehensive studies show\nInfiniRetri's potential for practical applications and creates a paradigm for\nretrievaling information using LLMs own capabilities under infinite-length\ntokens. Code will be released in link.", "published": "2025-02-18 15:45:36", "link": "http://arxiv.org/abs/2502.12962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs", "abstract": "Large Language Models (LLMs) often generate outputs that lack grounding in\nreal-world facts, a phenomenon known as hallucinations. Prior research has\nassociated hallucinations with model uncertainty, leveraging this relationship\nfor hallucination detection and mitigation. In this paper, we challenge the\nunderlying assumption that all hallucinations are associated with uncertainty.\nUsing knowledge detection and uncertainty measurement methods, we demonstrate\nthat models can hallucinate with high certainty even when they have the correct\nknowledge. We further show that high-certainty hallucinations are consistent\nacross models and datasets, distinctive enough to be singled out, and challenge\nexisting mitigation methods. Our findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong .", "published": "2025-02-18 15:46:31", "link": "http://arxiv.org/abs/2502.12964v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language\n  Models from Jailbreaking", "abstract": "The reasoning abilities of Large Language Models (LLMs) have demonstrated\nremarkable advancement and exceptional performance across diverse domains.\nHowever, leveraging these reasoning capabilities to enhance LLM safety against\nadversarial attacks and jailbreak queries remains largely unexplored. To bridge\nthis gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that\nintegrates safety reflections of queries and responses into LLMs' generation\nprocess, unlocking a safety-aware reasoning mechanism. This approach enables\nself-evaluation at each reasoning step to create safety pivot tokens as\nindicators of the response's safety status. Furthermore, in order to improve\nthe learning efficiency of pivot token prediction, we propose Contrastive Pivot\nOptimization(CPO), which enhances the model's ability to perceive the safety\nstatus of dialogues. Through this mechanism, LLMs dynamically adjust their\nresponse strategies during reasoning, significantly enhancing their defense\ncapabilities against jailbreak attacks. Extensive experimental results\ndemonstrate that R2D effectively mitigates various attacks and improves overall\nsafety, highlighting the substantial potential of safety-aware reasoning in\nstrengthening LLMs' robustness against jailbreaks.", "published": "2025-02-18 15:48:46", "link": "http://arxiv.org/abs/2502.12970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in\n  LLMs", "abstract": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought processes of a character. Using Lu Xun, a renowned Chinese\nwriter, as a case study, we propose four training tasks derived from his 17\nessay collections. These include a pre-training task focused on mastering\nexternal linguistic structures and knowledge, as well as three fine-tuning\ntasks: multiple-choice question answering, generative question answering, and\nstyle transfer, each aligning the LLM with Lu Xun's internal ideation and\nwriting style. To optimize learning across these tasks, we introduce a CharLoRA\nparameter updating mechanism, where a general linguistic style expert\ncollaborates with other task-specific experts to better study both the language\nstyle and the understanding of deeper thoughts. We evaluate CharacterBot on\nthree tasks for linguistic accuracy and opinion comprehension, demonstrating\nthat it significantly outperforms the baselines on our adapted metrics. We hope\nthat this work inspires future research on deep character persona simulation\nLLM.", "published": "2025-02-18 16:11:54", "link": "http://arxiv.org/abs/2502.12988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eager Updates For Overlapped Communication and Computation in DiLoCo", "abstract": "Distributed optimization methods such as DiLoCo have been shown to be\neffective in training very large models across multiple distributed workers,\nsuch as datacenters. These methods split updates into two parts: an inner\noptimization phase, where the workers independently execute multiple\noptimization steps on their own local data, and an outer optimization step,\nwhere the inner updates are synchronized. While such approaches require orders\nof magnitude less communication than standard data-parallel training, in\nsettings where the workers are datacenters, even the limited communication\nrequirements of these approaches can still cause significant slow downs due to\nthe blocking necessary at each outer optimization step. In this paper, we\ninvestigate techniques to mitigate this issue by overlapping communication with\ncomputation in a manner that allows the outer optimization step to fully\noverlap with the inner optimization phase. We show that a particular variant,\ndubbed eager updates, provides competitive performance with standard DiLoCo in\nsettings with low bandwidth between workers.", "published": "2025-02-18 16:16:14", "link": "http://arxiv.org/abs/2502.12996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Barriers: Evaluating Cross-Lingual Performance of CNN and\n  Transformer Architectures for Speech Quality Estimation", "abstract": "Objective speech quality models aim to predict human-perceived speech quality\nusing automated methods. However, cross-lingual generalization remains a major\nchallenge, as Mean Opinion Scores (MOS) vary across languages due to\nlinguistic, perceptual, and dataset-specific differences. A model trained\nprimarily on English data may struggle to generalize to languages with\ndifferent phonetic, tonal, and prosodic characteristics, leading to\ninconsistencies in objective assessments. This study investigates the\ncross-lingual performance of two speech quality models: NISQA, a CNN-based\nmodel, and a Transformer-based Audio Spectrogram Transformer (AST) model. Both\nmodels were trained exclusively on English datasets containing over 49,000\nspeech samples and subsequently evaluated on speech in German, French,\nMandarin, Swedish, and Dutch. We analyze model performance using Pearson\nCorrelation Coefficient (PCC) and Root Mean Square Error (RMSE) across five\nspeech quality dimensions: coloration, discontinuity, loudness, noise, and MOS.\nOur findings show that while AST achieves a more stable cross-lingual\nperformance, both models exhibit noticeable biases. Notably, Mandarin speech\nquality predictions correlate highly with human MOS scores, whereas Swedish and\nDutch present greater prediction challenges. Discontinuities remain difficult\nto model across all languages. These results highlight the need for more\nbalanced multilingual datasets and architecture-specific adaptations to improve\ncross-lingual generalization.", "published": "2025-02-18 16:22:43", "link": "http://arxiv.org/abs/2502.13004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented\n  Generation", "abstract": "Despite the remarkable capabilities of Large Language Models (LLMs) in\nvarious NLP tasks, they remain vulnerable to hallucinations due to their\nlimited parametric knowledge and lack of domain-specific expertise.\nRetrieval-Augmented Generation (RAG) addresses this challenge by incorporating\nexternal document retrieval to augment the knowledge base of LLMs. In this\napproach, RAG retrieves document chunks from an external corpus in response to\na query, which are then used as context for the downstream language model to\ngenerate an answer. However, these retrieved knowledge sources often include\nirrelevant or erroneous information, undermining the effectiveness of RAG in\ndownstream tasks. To overcome this limitation, we introduce a compact,\nefficient, and pluggable module designed to refine external knowledge sources\nbefore feeding them to the generator. The module reconstructs retrieved content\nby extracting the most relevant and supportive information and reorganising it\ninto a concise, query-specific format. Through a three-stage training paradigm\n- comprising supervised fine-tuning, contrastive multi-task learning, and\nreinforcement learning-based alignment - it prioritises critical knowledge and\naligns it with the generator's preferences. This method enables LLMs to produce\noutputs that are more accurate, reliable, and contextually appropriate.", "published": "2025-02-18 16:38:39", "link": "http://arxiv.org/abs/2502.13019v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Whose story is it? Personalizing story generation by inferring author\n  styles", "abstract": "Personalization has become essential for improving user experience in\ninteractive writing and educational applications, yet its potential in story\ngeneration remains largely unexplored. In this work, we propose a novel\ntwo-stage pipeline for personalized story generation. Our approach first infers\nan author's implicit story-writing characteristics from their past work and\norganizes them into an Author Writing Sheet, inspired by narrative theory. The\nsecond stage uses this sheet to simulate the author's persona through tailored\npersona descriptions and personalized story writing rules. To enable and\nvalidate our approach, we construct Mythos, a dataset of 590 stories from 64\nauthors across five distinct sources that reflect diverse story-writing\nsettings. A head-to-head comparison with a non-personalized baseline\ndemonstrates our pipeline's effectiveness in generating high-quality\npersonalized stories. Our personalized stories achieve a 75 percent win rate\n(versus 14 percent for the baseline and 11 percent ties) in capturing authors'\nwriting style based on their past works. Human evaluation highlights the high\nquality of our Author Writing Sheet and provides valuable insights into the\npersonalized story generation task. Notable takeaways are that writings from\ncertain sources, such as Reddit, are easier to personalize than others, like\nAO3, while narrative aspects, like Creativity and Language Use, are easier to\npersonalize than others, like Plot.", "published": "2025-02-18 16:45:41", "link": "http://arxiv.org/abs/2502.13028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HPSS: Heuristic Prompting Strategy Search for LLM Evaluators", "abstract": "Since the adoption of large language models (LLMs) for text evaluation has\nbecome increasingly prevalent in the field of natural language processing\n(NLP), a series of existing works attempt to optimize the prompts for LLM\nevaluators to improve their alignment with human judgment. However, their\nefforts are limited to optimizing individual factors of evaluation prompts,\nsuch as evaluation criteria or output formats, neglecting the combinatorial\nimpact of multiple factors, which leads to insufficient optimization of the\nevaluation pipeline. Nevertheless, identifying well-behaved prompting\nstrategies for adjusting multiple factors requires extensive enumeration. To\nthis end, we comprehensively integrate 8 key factors for evaluation prompts and\npropose a novel automatic prompting strategy optimization method called\nHeuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,\nHPSS conducts an iterative search to find well-behaved prompting strategies for\nLLM evaluators. A heuristic function is employed to guide the search process,\nenhancing the performance of our algorithm. Extensive experiments across four\nevaluation tasks demonstrate the effectiveness of HPSS, consistently\noutperforming both human-designed evaluation prompts and existing automatic\nprompt optimization methods.", "published": "2025-02-18 16:46:47", "link": "http://arxiv.org/abs/2502.13031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction", "abstract": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks.", "published": "2025-02-18 16:56:15", "link": "http://arxiv.org/abs/2502.13044v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile\n  Agents Against Active Environmental Injection Attacks", "abstract": "As researchers continuously optimize AI agents to perform tasks more\neffectively within operating systems, they often neglect to address the\ncritical need for enabling these agents to identify \"impostors\" within the\nsystem. Through an analysis of the agents' operating environment, we identified\na potential threat: attackers can disguise their attack methods as\nenvironmental elements, injecting active disturbances into the agents'\nexecution process, thereby disrupting their decision-making. We define this\ntype of attack as Active Environment Injection Attack (AEIA). Based on this, we\npropose AEIA-MN, an active environment injection attack scheme that exploits\ninteraction vulnerabilities in the mobile operating system to evaluate the\nrobustness of MLLM-based agents against such threats. Experimental results show\nthat even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% in the AndroidWorld benchmark.", "published": "2025-02-18 17:01:28", "link": "http://arxiv.org/abs/2502.13053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large\n  Language Models", "abstract": "The increasing application of multi-modal large language models (MLLMs)\nacross various sectors have spotlighted the essence of their output reliability\nand accuracy, particularly their ability to produce content grounded in factual\ninformation (e.g. common and domain-specific knowledge). In this work, we\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\nthe factuality ability of MLLMs to answer natural language short questions.\nSimpleVQA is characterized by six key features: it covers multiple tasks and\nmultiple scenarios, ensures high quality and challenging queries, maintains\nstatic and timeless reference answers, and is straightforward to evaluate. Our\napproach involves categorizing visual question-answering items into 9 different\ntasks around objective events or common knowledge and situating these within 9\ntopics. Rigorous quality control processes are implemented to guarantee\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\ntheir image comprehension and text generation abilities by identifying and\nanalyzing error cases.", "published": "2025-02-18 17:04:26", "link": "http://arxiv.org/abs/2502.13059v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KAPPA: A Generic Patent Analysis Framework with Keyphrase-Based\n  Portraits", "abstract": "Patent analysis highly relies on concise and interpretable document\nrepresentations, referred to as patent portraits. Keyphrases, both present and\nabsent, are ideal candidates for patent portraits due to their brevity,\nrepresentativeness, and clarity. In this paper, we introduce KAPPA, an\nintegrated framework designed to construct keyphrase-based patent portraits and\nenhance patent analysis. KAPPA operates in two phases: patent portrait\nconstruction and portrait-based analysis. To ensure effective portrait\nconstruction, we propose a semantic-calibrated keyphrase generation paradigm\nthat integrates pre-trained language models with a prompt-based hierarchical\ndecoding strategy to leverage the multi-level structural characteristics of\npatents. For portrait-based analysis, we develop a comprehensive framework that\nemploys keyphrase-based patent portraits to enable efficient and accurate\npatent analysis. Extensive experiments on benchmark datasets of keyphrase\ngeneration, the proposed model achieves significant improvements compared to\nstate-of-the-art baselines. Further experiments conducted on real-world patent\napplications demonstrate that our keyphrase-based portraits effectively capture\ndomain-specific knowledge and enrich semantic representation for patent\nanalysis tasks.", "published": "2025-02-18 17:24:00", "link": "http://arxiv.org/abs/2502.13076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The influence of motion features in temporal perception", "abstract": "This paper examines the role of manner-of-motion verbs in shaping subjective\ntemporal perception and emotional resonance. Through four complementary\nstudies, we explore how these verbs influence the conceptualization of time,\nexamining their use in literal and metaphorical (temporal) contexts. Our\nfindings reveal that faster verbs (e.g., fly, zoom) evoke dynamic and engaging\ntemporal experiences, often linked to positive emotions and greater agency. In\ncontrast, slower verbs (e.g., crawl, drag) convey passivity, monotony, and\nnegative emotions, reflecting tedious or constrained experiences of time. These\neffects are amplified in metaphorical contexts, where manner verbs encode\nemotional and experiential nuances that transcend their literal meanings. We\nalso find that participants prefer manner verbs over path verbs (e.g., go,\npass) in emotionally charged temporal contexts, as manner verbs capture the\nexperiential and emotional qualities of time more effectively. These findings\nhighlight the interplay between language, motion, and emotion in shaping\ntemporal perception, offering insights into how linguistic framing influences\nsubjective experiences of time.", "published": "2025-02-18 18:33:50", "link": "http://arxiv.org/abs/2502.13114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models", "abstract": "How should one judge whether a given large language model (LLM) can reliably\nperform economic reasoning? Most existing LLM benchmarks focus on specific\napplications and fail to present the model with a rich variety of economic\ntasks. A notable exception is Raman et al. [2024], who offer an approach for\ncomprehensively benchmarking strategic decision-making; however, this approach\nfails to address the non-strategic settings prevalent in microeconomics, such\nas supply-and-demand analysis. We address this gap by taxonomizing\nmicroeconomic reasoning into $58$ distinct elements, focusing on the logic of\nsupply and demand, each grounded in up to $10$ distinct domains, $5$\nperspectives, and $3$ types. The generation of benchmark data across this\ncombinatorial space is powered by a novel LLM-assisted data generation protocol\nthat we dub auto-STEER, which generates a set of questions by adapting\nhandwritten templates to target new domains and perspectives. Because it offers\nan automated way of generating fresh questions, auto-STEER mitigates the risk\nthat LLMs will be trained to over-fit evaluation benchmarks; we thus hope that\nit will serve as a useful tool both for evaluating and fine-tuning models for\nyears to come. We demonstrate the usefulness of our benchmark via a case study\non $27$ LLMs, ranging from small open-source models to the current state of the\nart. We examined each model's ability to solve microeconomic problems across\nour whole taxonomy and present the results across a range of prompting\nstrategies and scoring metrics.", "published": "2025-02-18 18:42:09", "link": "http://arxiv.org/abs/2502.13119v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions", "abstract": "Scaling reasoning capabilities beyond traditional domains such as math and\ncoding is hindered by the lack of diverse and high-quality questions. To\novercome this limitation, we introduce a scalable approach for generating\ndiverse and challenging reasoning questions, accompanied by reference answers.\nWe present NaturalReasoning, a comprehensive dataset comprising 2.8 million\nquestions that span multiple domains, including STEM fields (e.g., Physics,\nComputer Science), Economics, Social Sciences, and more. We demonstrate the\nutility of the questions in NaturalReasoning through knowledge distillation\nexperiments which show that NaturalReasoning can effectively elicit and\ntransfer reasoning capabilities from a strong teacher model. Furthermore, we\ndemonstrate that NaturalReasoning is also effective for unsupervised\nself-training using external reward models or self-rewarding.", "published": "2025-02-18 18:46:57", "link": "http://arxiv.org/abs/2502.13124v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading\n  Premises", "abstract": "Recent advances in large language models (LLMs) have shown that they can\nanswer questions requiring complex reasoning. However, their ability to\nidentify and respond to text containing logical fallacies or deliberately\nmisleading premises remains less studied. To address this gap, we introduce\nRuozhiBench, a bilingual dataset comprising 677 carefully curated questions\nthat contain various forms of deceptive reasoning, meticulously crafted through\nextensive human effort and expert review. In a comprehensive evaluation of 17\nLLMs from 5 Series over RuozhiBench using both open-ended and two-choice\nformats, we conduct extensive analyses on evaluation protocols and result\npatterns. Despite their high scores on conventional benchmarks, these models\nshowed limited ability to detect and reason correctly about logical fallacies,\nwith even the best-performing model, Claude-3-haiku, achieving only 62%\naccuracy compared to the human of more than 90%.", "published": "2025-02-18 18:47:11", "link": "http://arxiv.org/abs/2502.13125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facilitating Long Context Understanding via Supervised Chain-of-Thought\n  Reasoning", "abstract": "Recent advances in Large Language Models (LLMs) have enabled them to process\nincreasingly longer sequences, ranging from 2K to 2M tokens and even beyond.\nHowever, simply extending the input sequence length does not necessarily lead\nto effective long-context understanding. In this study, we integrate\nChain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate\neffective long-context understanding. To achieve this, we introduce\nLongFinanceQA, a synthetic dataset in the financial domain designed to improve\nlong-context reasoning. Unlike existing long-context synthetic data,\nLongFinanceQA includes intermediate CoT reasoning before the final conclusion,\nwhich encourages LLMs to perform explicit reasoning, improving accuracy and\ninterpretability in long-context understanding. To generate synthetic CoT\nreasoning, we propose Property-driven Agentic Inference (PAI), an agentic\nframework that simulates human-like reasoning steps, including property\nextraction, retrieval, and summarization. We evaluate PAI's reasoning\ncapabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,\noutperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune\nLLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's\nfinancial subset.", "published": "2025-02-18 18:50:06", "link": "http://arxiv.org/abs/2502.13127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Private Text Generation by Seeding Large Language Model Prompts", "abstract": "We explore how private synthetic text can be generated by suitably prompting\na large language model (LLM). This addresses a challenge for organizations like\nhospitals, which hold sensitive text data like patient medical records, and\nwish to share it in order to train machine learning models for medical tasks,\nwhile preserving patient privacy. Methods that rely on training or finetuning a\nmodel may be out of reach, either due to API limits of third-party LLMs, or due\nto ethical and legal prohibitions on sharing the private data with the LLM\nitself.\n  We propose Differentially Private Keyphrase Prompt Seeding (DP-KPS), a method\nthat generates a private synthetic text corpus from a sensitive input corpus,\nby accessing an LLM only through privatized prompts. It is based on seeding the\nprompts with private samples from a distribution over phrase embeddings, thus\ncapturing the input corpus while achieving requisite output diversity and\nmaintaining differential privacy. We evaluate DP-KPS on downstream ML text\nclassification tasks, and show that the corpora it generates preserve much of\nthe predictive power of the original ones. Our findings offer hope that\ninstitutions can reap ML insights by privately sharing data with simple prompts\nand little compute.", "published": "2025-02-18 16:50:38", "link": "http://arxiv.org/abs/2502.13193v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs", "abstract": "Linguistic evaluations of how well LMs generalize to produce or understand\nnovel text often implicitly take for granted that natural languages are\ngenerated by symbolic rules. Grammaticality is thought to be determined by\nwhether or not sentences obey such rules. Interpretation is believed to be\ncompositionally generated by syntactic rules operating on meaningful words.\nSemantic parsing is intended to map sentences into formal logic. Failures of\nLMs to obey strict rules have been taken to reveal that LMs do not produce or\nunderstand language like humans. Here we suggest that LMs' failures to obey\nsymbolic rules may be a feature rather than a bug, because natural languages\nare not based on rules. New utterances are produced and understood by a\ncombination of flexible interrelated and context-dependent schemata or\nconstructions. We encourage researchers to reimagine appropriate benchmarks and\nanalyses that acknowledge the rich flexible generalizations that comprise\nnatural languages.", "published": "2025-02-18 17:40:20", "link": "http://arxiv.org/abs/2502.13195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounding LLM Reasoning with Knowledge Graphs", "abstract": "Knowledge Graphs (KGs) are valuable tools for representing relationships\nbetween entities in a structured format. Traditionally, these knowledge bases\nare queried to extract specific information. However, question-answering (QA)\nover such KGs poses a challenge due to the intrinsic complexity of natural\nlanguage compared to the structured format and the size of these graphs.\nDespite these challenges, the structured nature of KGs can provide a solid\nfoundation for grounding the outputs of Large Language Models (LLMs), offering\norganizations increased reliability and control.\n  Recent advancements in LLMs have introduced reasoning methods at inference\ntime to improve their performance and maximize their capabilities. In this\nwork, we propose integrating these reasoning strategies with KGs to anchor\nevery step or \"thought\" of the reasoning chains in KG data. Specifically, we\nevaluate both agentic and automated search methods across several reasoning\nstrategies, including Chain-of-Thought (CoT), Tree-of-Thought (ToT), and\nGraph-of-Thought (GoT), using GRBench, a benchmark dataset for graph reasoning\nwith domain-specific graphs. Our experiments demonstrate that this approach\nconsistently outperforms baseline models, highlighting the benefits of\ngrounding LLM reasoning processes in structured KG data.", "published": "2025-02-18 19:20:46", "link": "http://arxiv.org/abs/2502.13247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Language Model Pretraining using Machine-translated Data", "abstract": "High-resource languages such as English, enables the pretraining of\nhigh-quality large language models (LLMs). The same can not be said for most\nother languages as LLMs still underperform for non-English languages, likely\ndue to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated texts from a\nsingle high-quality source language can contribute significantly to the\npretraining quality of multilingual LLMs. We translate FineWeb-Edu, a\nhigh-quality English web dataset, into nine languages, resulting in a\n1.7-trillion-token dataset, which we call TransWebEdu and pretrain a\n1.3B-parameter model, TransWebLLM, from scratch on this dataset. Across nine\nnon-English reasoning tasks, we show that TransWebLLM matches or outperforms\nstate-of-the-art multilingual models trained using closed data, such as\nLlama3.2, Qwen2.5, and Gemma, despite using an order of magnitude less data. We\ndemonstrate that adding less than 5% of TransWebEdu as domain-specific\npretraining data sets a new state-of-the-art in Arabic, Italian, Indonesian,\nSwahili, and Welsh understanding and commonsense reasoning tasks. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nOpen Source Initiative-approved licenses.", "published": "2025-02-18 19:27:53", "link": "http://arxiv.org/abs/2502.13252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation", "abstract": "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.", "published": "2025-02-18 20:29:01", "link": "http://arxiv.org/abs/2502.13270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multi-turn Task Completion in Task-Oriented Dialog Systems via\n  Prompt Chaining and Fine-Grained Feedback", "abstract": "Task-oriented dialog (TOD) systems facilitate users in accomplishing complex,\nmulti-turn tasks through natural language. While traditional approaches rely on\nextensive fine-tuning and annotated data for each domain, instruction-tuned\nlarge language models (LLMs) offer a more flexible alternative. However, LLMs\nstruggle to reliably handle multi-turn task completion, particularly with\naccurately generating API calls and adapting to new domains without explicit\ndemonstrations. To address these challenges, we propose RealTOD, a novel\nframework that enhances TOD systems through prompt chaining and fine-grained\nfeedback mechanisms. Prompt chaining enables zero-shot domain adaptation via a\ntwo-stage prompting strategy, eliminating the need for human-curated\ndemonstrations. Meanwhile, the fine-grained feedback mechanism improves task\ncompletion by verifying API calls against domain schemas and providing precise\ncorrective feedback when errors are detected. We conduct extensive experiments\non the SGD and BiTOD benchmarks using four LLMs. RealTOD improves API accuracy,\nsurpassing AutoTOD by 37.74% on SGD and SimpleTOD by 11.26% on BiTOD. Human\nevaluations further confirm that LLMs integrated with RealTOD achieve superior\ntask completion, fluency, and informativeness compared to existing methods.", "published": "2025-02-18 21:36:19", "link": "http://arxiv.org/abs/2502.13298v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented\n  Dialog Systems for Task Completion without Turn-level Dialog Annotations", "abstract": "Traditional task-oriented dialog (ToD) systems rely heavily on\nlabor-intensive turn-level annotations, such as dialogue states and policy\nlabels, for training. This work explores whether large language models (LLMs)\ncan be fine-tuned solely on natural language dialogs to perform ToD tasks,\nwithout requiring such annotations. We evaluate their ability to generalize to\nunseen domains and compare their performance with models trained on fully\nannotated data. Through extensive experiments with three open-source LLMs of\nvarying sizes and two diverse ToD datasets, we find that models fine-tuned\nwithout turn-level annotations generate coherent and contextually appropriate\nresponses. However, their task completion performance - measured by accurate\nexecution of API calls - remains suboptimal, with the best models achieving\nonly around 53% success in unseen domains. To improve task completion, we\npropose ZeroToD, a framework that incorporates a schema augmentation mechanism\nto enhance API call accuracy and overall task completion rates, particularly in\nout-of-domain settings. We also compare ZeroToD with fine-tuning-free\nalternatives, such as prompting off-the-shelf LLMs, and find that our framework\nenables smaller, fine-tuned models that outperform large-scale proprietary LLMs\nin task completion. Additionally, a human study evaluating informativeness,\nfluency, and task completion confirms our empirical findings. These findings\nsuggest the feasibility of developing cost-effective, scalable, and zero-shot\ngeneralizable ToD systems for real-world applications.", "published": "2025-02-18 22:10:51", "link": "http://arxiv.org/abs/2502.13310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare", "abstract": "We know from prior work that LLMs encode social biases, and that this\nmanifests in clinical tasks. In this work we adopt tools from mechanistic\ninterpretability to unveil sociodemographic representations and biases within\nLLMs in the context of healthcare. Specifically, we ask: Can we identify\nactivations within LLMs that encode sociodemographic information (e.g., gender,\nrace)? We find that gender information is highly localized in middle MLP layers\nand can be reliably manipulated at inference time via patching. Such\ninterventions can surgically alter generated clinical vignettes for specific\nconditions, and also influence downstream clinical predictions which correlate\nwith gender, e.g., patient risk of depression. We find that representation of\npatient race is somewhat more distributed, but can also be intervened upon, to\na degree. To our knowledge, this is the first application of mechanistic\ninterpretability methods to LLMs for healthcare.", "published": "2025-02-18 22:40:40", "link": "http://arxiv.org/abs/2502.13319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing Human Cognitive Styles with Language: Towards an Experimental\n  Evaluation Paradigm", "abstract": "While NLP models often seek to capture cognitive states via language, the\nvalidity of predicted states is determined by comparing them to annotations\ncreated without access the cognitive states of the authors. In behavioral\nsciences, cognitive states are instead measured via experiments. Here, we\nintroduce an experiment-based framework for evaluating language-based cognitive\nstyle models against human behavior. We explore the phenomenon of decision\nmaking, and its relationship to the linguistic style of an individual talking\nabout a recent decision they made. The participants then follow a classical\ndecision-making experiment that captures their cognitive style, determined by\nhow preferences change during a decision exercise. We find that language\nfeatures, intended to capture cognitive style, can predict participants'\ndecision style with moderate-to-high accuracy (AUC ~ 0.8), demonstrating that\ncognitive style can be partly captured and revealed by discourse patterns.", "published": "2025-02-18 23:08:15", "link": "http://arxiv.org/abs/2502.13326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond De-Identification: A Structured Approach for Defining and\n  Detecting Indirect Identifiers in Medical Texts", "abstract": "Sharing sensitive texts for scientific purposes requires appropriate\ntechniques to protect the privacy of patients and healthcare personnel.\nAnonymizing textual data is particularly challenging due to the presence of\ndiverse unstructured direct and indirect identifiers. To mitigate the risk of\nre-identification, this work introduces a schema of nine categories of indirect\nidentifiers designed to account for different potential adversaries, including\nacquaintances, family members and medical staff. Using this schema, we annotate\n100 MIMIC-III discharge summaries and propose baseline models for identifying\nindirect identifiers. We will release the annotation guidelines, annotation\nspans (6,199 annotations in total) and the corresponding MIMIC-III document IDs\nto support further research in this area.", "published": "2025-02-18 23:52:29", "link": "http://arxiv.org/abs/2502.13342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Euskarazko lehen C1 ebaluatzaile automatikoa", "abstract": "Throughout this project, we have attempted to develop an automatic evaluator\nthat determines whether Basque language compositions meet the C1 level. To\nachieve our goal, we obtained 10,000 transcribed compositions through an\nagreement between HABE and HiTZ to train our system. We have developed\ndifferent techniques to avoid data scarcity and system overfitting: EDA, SCL\nand regulation; We have also conducted tests with different Language Models to\nanalyze their behavior. Finally, we have also performed analyses of different\nsystem behaviors to measure model calibration and the impact of artifacts.\n  --\n  Proiektu honetan zehar euskarazko idazlanek C1 maila duten edo ez zehazten\nduen ebaluatzaile automatiko bat garatzen saiatu gara. Gure helburua betetzeko\nHABE eta HiTZ arteko hitzarmenaren bitartez 10.000 transkribatutako idazlan\neskuratu ditugu gure sistema entrenatzeko. Datu eskasia eta sistemaren\ngaindoitzea ekiditeko teknika ezberdinak landu ditugu: EDA, SCL eta\nerregulazioa; Hizkuntza Eredu ezberdinekin ere probak egin ditugu duten\nportaera aztertzeko. Azkenik, sistema ezberdinen portaeren analisiak ere egin\nditugu, ereduen kalibrazioa eta artefaktuen eragina neurtzeko.", "published": "2025-02-18 14:56:32", "link": "http://arxiv.org/abs/2503.01851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large\n  Language Models", "abstract": "Unsafe prompts pose significant safety risks to large language models (LLMs).\nExisting methods for detecting unsafe prompts rely on data-driven fine-tuning\nto train guardrail models, necessitating significant data and computational\nresources. In contrast, recent few-shot gradient-based methods emerge,\nrequiring only few safe and unsafe reference prompts. A gradient-based approach\nidentifies unsafe prompts by analyzing consistent patterns of the gradients of\nsafety-critical parameters in LLMs. Although effective, its restriction to\ndirectional similarity (cosine similarity) introduces ``directional bias'',\nlimiting its capability to identify unsafe prompts. To overcome this\nlimitation, we introduce GradCoo, a novel gradient co-occurrence analysis\nmethod that expands the scope of safety-critical parameter identification to\ninclude unsigned gradient similarity, thereby reducing the impact of\n``directional bias'' and enhancing the accuracy of unsafe prompt detection.\nComprehensive experiments on the widely-used benchmark datasets ToxicChat and\nXStest demonstrate that our proposed method can achieve state-of-the-art (SOTA)\nperformance compared to existing methods. Moreover, we confirm the\ngeneralizability of GradCoo in detecting unsafe prompts across a range of LLM\nbase models with various sizes and origins.", "published": "2025-02-18 01:14:46", "link": "http://arxiv.org/abs/2502.12411v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large\n  Language Models", "abstract": "Recent advances in large language models have led to numerous\ntask-specialized fine-tuned variants, creating a need for efficient model\nmerging techniques that preserve specialized capabilities while avoiding costly\nretraining. While existing task vector-based merging methods show promise, they\ntypically apply uniform coefficients across all parameters, overlooking varying\nparameter importance both within and across tasks. We present Sens-Merging, a\nsensitivity-guided coefficient adjustment method that enhances existing model\nmerging techniques by operating at both task-specific and cross-task levels.\nOur method analyzes parameter sensitivity within individual tasks and evaluates\ncross-task transferability to determine optimal merging coefficients. Extensive\nexperiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that\nSens-Merging significantly improves performance across general knowledge,\nmathematical reasoning, and code generation tasks. Notably, when combined with\nexisting merging techniques, our method enables merged models to outperform\nspecialized fine-tuned models, particularly in code generation tasks. Our\nfindings reveal important trade-offs between task-specific and cross-task\nscalings, providing insights for future model merging strategies.", "published": "2025-02-18 01:41:13", "link": "http://arxiv.org/abs/2502.12420v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Large Language Models for Automated Planning", "abstract": "The planning ability of Large Language Models (LLMs) has garnered increasing\nattention in recent years due to their remarkable capacity for multi-step\nreasoning and their ability to generalize across a wide range of domains. While\nsome researchers emphasize the potential of LLMs to perform complex planning\ntasks, others highlight significant limitations in their performance,\nparticularly when these models are tasked with handling the intricacies of\nlong-horizon reasoning. In this survey, we critically investigate existing\nresearch on the use of LLMs in automated planning, examining both their\nsuccesses and shortcomings in detail. We illustrate that although LLMs are not\nwell-suited to serve as standalone planners because of these limitations, they\nnonetheless present an enormous opportunity to enhance planning applications\nwhen combined with other approaches. Thus, we advocate for a balanced\nmethodology that leverages the inherent flexibility and generalized knowledge\nof LLMs alongside the rigor and cost-effectiveness of traditional planning\nmethods.", "published": "2025-02-18 02:11:03", "link": "http://arxiv.org/abs/2502.12435v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented\n  Generation", "abstract": "Retrieval-Augmented Generation (RAG) systems often struggle with imperfect\nretrieval, as traditional retrievers focus on lexical or semantic similarity\nrather than logical relevance. To address this, we propose HopRAG, a novel RAG\nframework that augments retrieval with logical reasoning through\ngraph-structured knowledge exploration. During indexing, HopRAG constructs a\npassage graph, with text chunks as vertices and logical connections established\nvia LLM-generated pseudo-queries as edges. During retrieval, it employs a\nretrieve-reason-prune mechanism: starting with lexically or semantically\nsimilar passages, the system explores multi-hop neighbors guided by\npseudo-queries and LLM reasoning to identify truly relevant ones. Extensive\nexperiments demonstrate HopRAG's superiority, achieving 76.78\\% higher answer\naccuracy and 65.07\\% improved retrieval F1 score compared to conventional\nmethods. The repository is available at https://github.com/LIU-Hao-2002/HopRAG.", "published": "2025-02-18 02:24:42", "link": "http://arxiv.org/abs/2502.12442v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Safe at the Margins: A General Approach to Safety Alignment in\n  Low-Resource English Languages -- A Singlish Case Study", "abstract": "Ensuring the safety of Large Language Models (LLMs) in diverse linguistic\nsettings remains challenging, particularly for low-resource languages. Existing\nsafety alignment methods are English-centric, limiting their effectiveness. We\nsystematically compare Supervised Fine-Tuning (SFT), Direct Preference\nOptimization (DPO), and Kahneman-Tversky Optimization (KTO) for aligning\nSEA-Lion-v2.1-Instruct, a Llama 3-8B variant, to reduce toxicity in Singlish.\nOur results show that SFT+KTO achieves superior safety alignment with higher\nsample efficiency than DPO. Additionally, we introduce KTO-S, which enhances\nstability via improved KL divergence regularization. Our approach reduces\nSinglish toxicity by 99\\%, generalizes to TOXIGEN, and maintains strong\nperformance on standard LLM benchmarks, providing a scalable framework for\nsafer AI deployment in multilingual contexts.", "published": "2025-02-18 03:11:06", "link": "http://arxiv.org/abs/2502.12485v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LegalCore: A Dataset for Event Coreference Resolution in Legal Documents", "abstract": "Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code.", "published": "2025-02-18 03:47:53", "link": "http://arxiv.org/abs/2502.12509v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching\n  Generated Rewards", "abstract": "As humans increasingly share environments with diverse agents powered by RL,\nLLMs, and beyond, the ability to explain their policies in natural language\nwill be vital for reliable coexistence. In this paper, we build a\nmodel-agnostic explanation generator based on an LLM. The technical novelty is\nthat the rewards for training this LLM are generated by a generative flow\nmatching model. This model has a specially designed structure with a hidden\nlayer merged with an LLM to harness the linguistic cues of explanations into\ngenerating appropriate rewards. Experiments on both RL and LLM tasks\ndemonstrate that our method can generate dense and effective rewards while\nsaving on expensive human feedback; it thus enables effective explanations and\neven improves the accuracy of the decisions in original tasks.", "published": "2025-02-18 04:34:45", "link": "http://arxiv.org/abs/2502.12530v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design", "abstract": "Usability testing is a fundamental yet challenging (e.g., inflexible to\niterate the study design flaws and hard to recruit study participants) research\nmethod for user experience (UX) researchers to evaluate a web design. Recent\nadvances in Large Language Model-simulated Agent (LLM-Agent) research inspired\nus to design UXAgent to support UX researchers in evaluating and reiterating\ntheir usability testing study design before they conduct the real human subject\nstudy. Our system features an LLM-Agent module and a universal browser\nconnector module so that UX researchers can automatically generate thousands of\nsimulated users to test the target website. The results are shown in\nqualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of\nactions), and video recording formats for UX researchers to analyze. Through a\nheuristic user evaluation with five UX researchers, participants praised the\ninnovation of our system but also expressed concerns about the future of LLM\nAgent-assisted UX study.", "published": "2025-02-18 05:55:18", "link": "http://arxiv.org/abs/2502.12561v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "A Cognitive Writing Perspective for Constrained Long-Form Text\n  Generation", "abstract": "Like humans, Large Language Models (LLMs) struggle to generate high-quality\nlong-form text that adheres to strict requirements in a single pass. This\nchallenge is unsurprising, as successful human writing, according to the\nCognitive Writing Theory, is a complex cognitive process involving iterative\nplanning, translating, reviewing, and monitoring. Motivated by these cognitive\nprinciples, we aim to equip LLMs with human-like cognitive writing capabilities\nthrough CogWriter, a novel training-free framework that transforms LLM\nconstrained long-form text generation into a systematic cognitive writing\nparadigm. Our framework consists of two key modules: (1) a Planning Agent that\nperforms hierarchical planning to decompose the task, and (2) multiple\nGeneration Agents that execute these plans in parallel. The system maintains\nquality via continuous monitoring and reviewing mechanisms, which evaluate\noutputs against specified requirements and trigger necessary revisions.\nCogWriter demonstrates exceptional performance on LongGenBench, a benchmark for\ncomplex constrained long-form text generation. Even when using Qwen-2.5-14B as\nits backbone, CogWriter surpasses GPT-4o by 22% in complex instruction\ncompletion accuracy while reliably generating texts exceeding 10,000 words. We\nhope this cognitive science-inspired approach provides a paradigm for LLM\nwriting advancements:\n\\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.", "published": "2025-02-18 06:12:14", "link": "http://arxiv.org/abs/2502.12568v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable\n  Recommendation", "abstract": "Explainable recommendation has demonstrated significant advantages in\ninforming users about the logic behind recommendations, thereby increasing\nsystem transparency, effectiveness, and trustworthiness. To provide\npersonalized and interpretable explanations, existing works often combine the\ngeneration capabilities of large language models (LLMs) with collaborative\nfiltering (CF) information. CF information extracted from the user-item\ninteraction graph captures the user behaviors and preferences, which is crucial\nfor providing informative explanations. However, due to the complexity of graph\nstructure, effectively extracting the CF information from graphs still remains\na challenge. Moreover, existing methods often struggle with the integration of\nextracted CF information with LLMs due to its implicit representation and the\nmodality gap between graph structures and natural language explanations. To\naddress these challenges, we propose G-Refer, a framework using graph\nretrieval-augmented large language models (LLMs) for explainable\nrecommendation. Specifically, we first employ a hybrid graph retrieval\nmechanism to retrieve explicit CF signals from both structural and semantic\nperspectives. The retrieved CF information is explicitly formulated as\nhuman-understandable text by the proposed graph translation and accounts for\nthe explanations generated by LLMs. To bridge the modality gap, we introduce\nknowledge pruning and retrieval-augmented fine-tuning to enhance the ability of\nLLMs to process and utilize the retrieved CF information to generate\nexplanations. Extensive experiments show that G-Refer achieves superior\nperformance compared with existing methods in both explainability and\nstability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.", "published": "2025-02-18 06:42:38", "link": "http://arxiv.org/abs/2502.12586v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "RSMLP: A light Sampled MLP Structure for Incomplete Utterance Rewrite", "abstract": "The Incomplete Utterance Rewriting (IUR) task has garnered significant\nattention in recent years. Its goal is to reconstruct conversational utterances\nto better align with the current context, thereby enhancing comprehension. In\nthis paper, we introduce a novel and versatile lightweight method,\nRewritten-Sampled MLP (RSMLP). By employing an MLP based architecture with a\ncarefully designed down-sampling strategy, RSMLP effectively extracts latent\nsemantic information between utterances and makes appropriate edits to restore\nincomplete utterances. Due to its simple yet efficient structure, our method\nachieves competitive performance on public IUR datasets and in real-world\napplications.", "published": "2025-02-18 06:45:21", "link": "http://arxiv.org/abs/2502.12587v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CutPaste&Find: Efficient Multimodal Hallucination Detector with\n  Visual-aid Knowledge Base", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nreasoning capabilities, but they remain susceptible to hallucination,\nparticularly object hallucination where non-existent objects or incorrect\nattributes are fabricated in generated descriptions. Existing detection methods\nachieve strong performance but rely heavily on expensive API calls and\niterative LVLM-based validation, making them impractical for large-scale or\noffline use. To address these limitations, we propose CutPaste\\&Find, a\nlightweight and training-free framework for detecting hallucinations in\nLVLM-generated outputs. Our approach leverages off-the-shelf visual and\nlinguistic modules to perform multi-step verification efficiently without\nrequiring LVLM inference. At the core of our framework is a Visual-aid\nKnowledge Base that encodes rich entity-attribute relationships and associated\nimage representations. We introduce a scaling factor to refine similarity\nscores, mitigating the issue of suboptimal alignment values even for\nground-truth image-text pairs. Comprehensive evaluations on benchmark datasets,\nincluding POPE and R-Bench, demonstrate that CutPaste\\&Find achieves\ncompetitive hallucination detection performance while being significantly more\nefficient and cost-effective than previous methods.", "published": "2025-02-18 07:06:36", "link": "http://arxiv.org/abs/2502.12591v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Label Drop for Multi-Aspect Relation Modeling in Universal Information\n  Extraction", "abstract": "Universal Information Extraction (UIE) has garnered significant attention due\nto its ability to address model explosion problems effectively. Extractive UIE\ncan achieve strong performance using a relatively small model, making it widely\nadopted. Extractive UIEs generally rely on task instructions for different\ntasks, including single-target instructions and multiple-target instructions.\nSingle-target instruction UIE enables the extraction of only one type of\nrelation at a time, limiting its ability to model correlations between\nrelations and thus restricting its capability to extract complex relations.\nWhile multiple-target instruction UIE allows for the extraction of multiple\nrelations simultaneously, the inclusion of irrelevant relations introduces\ndecision complexity and impacts extraction accuracy. Therefore, for\nmulti-relation extraction, we propose LDNet, which incorporates multi-aspect\nrelation modeling and a label drop mechanism. By assigning different relations\nto different levels for understanding and decision-making, we reduce decision\nconfusion. Additionally, the label drop mechanism effectively mitigates the\nimpact of irrelevant relations. Experiments show that LDNet outperforms or\nachieves competitive performance with state-of-the-art systems on 9 tasks, 33\ndatasets, in both single-modal and multi-modal, few-shot and zero-shot\nsettings.\\footnote{https://github.com/Lu-Yang666/LDNet}", "published": "2025-02-18 07:53:26", "link": "http://arxiv.org/abs/2502.12614v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One Size doesn't Fit All: A Personalized Conversational Tutoring Agent\n  for Mathematics Instruction", "abstract": "Large language models (LLMs) have been increasingly employed in various\nintelligent educational systems, simulating human tutors to facilitate\neffective human-machine interaction. However, previous studies often overlook\nthe significance of recognizing and adapting to individual learner\ncharacteristics. Such adaptation is crucial for enhancing student engagement\nand learning efficiency, particularly in mathematics instruction, where diverse\nlearning styles require personalized strategies to promote comprehension and\nenthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized\n\\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics\ninstruction. PACE simulates students' learning styles based on the Felder and\nSilverman learning style model, aligning with each student's persona. In this\nway, our PACE can effectively assess the personality of students, allowing to\ndevelop individualized teaching strategies that resonate with their unique\nlearning styles. To further enhance students' comprehension, PACE employs the\nSocratic teaching method to provide instant feedback and encourage deep\nthinking. By constructing personalized teaching data and training models, PACE\ndemonstrates the ability to identify and adapt to the unique needs of each\nstudent, significantly improving the overall learning experience and outcomes.\nMoreover, we establish multi-aspect evaluation criteria and conduct extensive\nanalysis to assess the performance of personalized teaching. Experimental\nresults demonstrate the superiority of our model in personalizing the\neducational experience and motivating students compared to existing methods.", "published": "2025-02-18 08:24:52", "link": "http://arxiv.org/abs/2502.12633v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speech-FT: A Fine-tuning Strategy for Enhancing Speech Representation\n  Models Without Compromising Generalization Ability", "abstract": "Speech representation models are highly effective at extracting general\nfeatures for various tasks. While fine-tuning can enhance these representations\nfor specific applications, it often compromises their generalization ability.\nTo address this challenge, we propose Speech-FT, a fine-tuning strategy for\nspeech representation models that leverages model merging to preserve\ngeneralization ability while still benefiting from fine-tuning. Speech-FT is\neffective across different fine-tuning scenarios and is compatible with various\ntypes of speech representation models, providing a versatile solution.\nSpeech-FT offers an efficient and practical approach to further improving\ngeneral speech representations after pre-training.", "published": "2025-02-18 09:23:42", "link": "http://arxiv.org/abs/2502.12672v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Playing with Voices: Tabletop Role-Playing Game Recordings as a\n  Diarization Challenge", "abstract": "This paper provides a proof of concept that audio of tabletop role-playing\ngames (TTRPG) could serve as a challenge for diarization systems. TTRPGs are\ncarried out mostly by conversation. Participants often alter their voices to\nindicate that they are talking as a fictional character. Audio processing\nsystems are susceptible to voice conversion with or without technological\nassistance. TTRPG present a conversational phenomenon in which voice conversion\nis an inherent characteristic for an immersive gaming experience. This could\nmake it more challenging for diarizers to pick the real speaker and determine\nthat impersonating is just that. We present the creation of a small TTRPG audio\ndataset and compare it against the AMI and the ICSI corpus. The performance of\ntwo diarizers, pyannote.audio and wespeaker, were evaluated. We observed that\nTTRPGs' properties result in a higher confusion rate for both diarizers.\nAdditionally, wespeaker strongly underestimates the number of speakers in the\nTTRPG audio files. We propose TTRPG audio as a promising challenge for\ndiarization systems.", "published": "2025-02-18 10:26:49", "link": "http://arxiv.org/abs/2502.12714v1", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text\n  Detection with Adversarial Training", "abstract": "Machine-generated Text (MGT) detection is crucial for regulating and\nattributing online texts. While the existing MGT detectors achieve strong\nperformance, they remain vulnerable to simple perturbations and adversarial\nattacks. To build an effective defense against malicious perturbations, we view\nMGT detection from a threat modeling perspective, that is, analyzing the\nmodel's vulnerability from an adversary's point of view and exploring effective\nmitigations. To this end, we introduce an adversarial framework for training a\nrobust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The\nGREATER consists of two key components: an adversary GREATER-A and a detector\nGREATER-D. The GREATER-D learns to defend against the adversarial attack from\nGREATER-A and generalizes the defense to other attacks. GREATER-A identifies\nand perturbs the critical tokens in embedding space, along with greedy search\nand pruning to generate stealthy and disruptive adversarial examples. Besides,\nwe update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D\nto generalize its defense to different attacks and varying attack intensities.\nOur experimental results across 9 text perturbation strategies and 5\nadversarial attacks show that our GREATER-D reduces the Attack Success Rate\n(ASR) by 10.61% compared with SOTA defense methods while our GREATER-A is\ndemonstrated to be more effective and efficient than SOTA attack approaches.", "published": "2025-02-18 10:48:53", "link": "http://arxiv.org/abs/2502.12734v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided\n  Logical Form Generation", "abstract": "Knowledge base question answering (KBQA) aims to answer user questions in\nnatural language using rich human knowledge stored in large KBs. As current\nKBQA methods struggle with unseen knowledge base elements at test time,we\nintroduce SG-KBQA: a novel model that injects schema contexts into entity\nretrieval and logical form generation to tackle this issue. It uses the richer\nsemantics and awareness of the knowledge base structure provided by schema\ncontexts to enhance generalizability. We show that SG-KBQA achieves strong\ngeneralizability, outperforming state-of-the-art models on two commonly used\nbenchmark datasets across a variety of test settings. Our source code is\navailable at https://github.com/gaosx2000/SG_KBQA.", "published": "2025-02-18 10:53:41", "link": "http://arxiv.org/abs/2502.12737v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"I know myself better, but not really greatly\": Using LLMs to Detect and\n  Explain LLM-Generated Texts", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\ngenerating human-like texts, but the potential misuse of such LLM-generated\ntexts raises the need to distinguish between human-generated and LLM-generated\ncontent. This paper explores the detection and explanation capabilities of\nLLM-based detectors of LLM-generated texts, in the context of a binary\nclassification task (human-generated texts vs LLM-generated texts) and a\nternary classification task (human-generated texts, LLM-generated texts, and\nundecided). By evaluating on six close/open-source LLMs with different sizes,\nour findings reveal that while self-detection consistently outperforms\ncross-detection, i.e., LLMs can detect texts generated by themselves more\naccurately than those generated by other LLMs, the performance of\nself-detection is still far from ideal, indicating that further improvements\nare needed. We also show that extending the binary to the ternary\nclassification task with a new class \"Undecided\" can enhance both detection\naccuracy and explanation quality, with improvements being statistically\nsignificant and consistent across all LLMs. We finally conducted comprehensive\nqualitative and quantitative analyses on the explanation errors, which are\ncategorized into three types: reliance on inaccurate features (the most\nfrequent error), hallucinations, and incorrect reasoning. These findings with\nour human-annotated dataset emphasize the need for further research into\nimproving both self-detection and self-explanation, particularly to address\noverfitting issues that may hinder generalization.", "published": "2025-02-18 11:00:28", "link": "http://arxiv.org/abs/2502.12743v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs", "abstract": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks are often rigid, struggling to adapt to KG or task changes. They\nalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.\nTo address this, We introduce R2-KG, a plug-and-play, dual-agent framework that\nseparates reasoning into two roles: an Operator (a low-capacity LLM) that\ngathers evidence and a Supervisor (a high-capacity LLM) that makes final\njudgments. This design is cost-efficient for LLM inference while still\nmaintaining strong reasoning accuracy. Additionally, R2-KG employs an\nAbstention mechanism, generating answers only when sufficient evidence is\ncollected from KG, which significantly enhances reliability. Experiments across\nmultiple KG-based reasoning tasks show that R2-KG consistently outperforms\nbaselines in both accuracy and reliability, regardless of the inherent\ncapability of LLMs used as the Operator. Further experiments reveal that the\nsingle-agent version of R2-KG, equipped with a strict self-consistency\nstrategy, achieves significantly higher-than-baseline reliability while\nreducing inference cost. However, it also leads to a higher abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning. It reduces reliance on high-capacity LLMs\nwhile ensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.", "published": "2025-02-18 11:31:52", "link": "http://arxiv.org/abs/2502.12767v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild", "abstract": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.", "published": "2025-02-18 11:32:43", "link": "http://arxiv.org/abs/2502.12769v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mind the Gap: Aligning the Brain with Language Models Requires a\n  Nonlinear and Multimodal Approach", "abstract": "Self-supervised language and audio models effectively predict brain responses\nto speech. However, traditional prediction models rely on linear mappings from\nunimodal features, despite the complex integration of auditory signals with\nlinguistic and semantic information across widespread brain networks during\nspeech comprehension. Here, we introduce a nonlinear, multimodal prediction\nmodel that combines audio and linguistic features from pre-trained models\n(e.g., LLAMA, Whisper). Our approach achieves a 17.2% and 17.9% improvement in\nprediction performance (unnormalized and normalized correlation) over\ntraditional unimodal linear models, as well as a 7.7% and 14.4% improvement,\nrespectively, over prior state-of-the-art models. These improvements represent\na major step towards future robust in-silico testing and improved decoding\nperformance. They also reveal how auditory and semantic information are fused\nin motor, somatosensory, and higher-level semantic regions, aligning with\nexisting neurolinguistic theories. Overall, our work highlights the often\nneglected potential of nonlinear and multimodal approaches to brain modeling,\npaving the way for future studies to embrace these strategies in naturalistic\nneurolinguistics research.", "published": "2025-02-18 11:33:28", "link": "http://arxiv.org/abs/2502.12771v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models", "abstract": "When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.", "published": "2025-02-18 12:46:18", "link": "http://arxiv.org/abs/2502.12825v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Equitable AI: Detecting Bias in Using Large Language Models for\n  Marketing", "abstract": "The recent advances in large language models (LLMs) have revolutionized\nindustries such as finance, marketing, and customer service by enabling\nsophisticated natural language processing tasks. However, the broad adoption of\nLLMs brings significant challenges, particularly in the form of social biases\nthat can be embedded within their outputs. Biases related to gender, age, and\nother sensitive attributes can lead to unfair treatment, raising ethical\nconcerns and risking both company reputation and customer trust. This study\nexamined bias in finance-related marketing slogans generated by LLMs (i.e.,\nChatGPT) by prompting tailored ads targeting five demographic categories:\ngender, marital status, age, income level, and education level. A total of\n1,700 slogans were generated for 17 unique demographic groups, and key terms\nwere categorized into four thematic groups: empowerment, financial, benefits\nand features, and personalization. Bias was systematically assessed using\nrelative bias calculations and statistically tested with the Kolmogorov-Smirnov\n(KS) test against general slogans generated for any individual. Results\nrevealed that marketing slogans are not neutral; rather, they emphasize\ndifferent themes based on demographic factors. Women, younger individuals,\nlow-income earners, and those with lower education levels receive more distinct\nmessaging compared to older, higher-income, and highly educated individuals.\nThis underscores the need to consider demographic-based biases in AI-generated\nmarketing strategies and their broader societal implications. The findings of\nthis study provide a roadmap for developing more equitable AI systems,\nhighlighting the need for ongoing bias detection and mitigation efforts in\nLLMs.", "published": "2025-02-18 13:11:16", "link": "http://arxiv.org/abs/2502.12838v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "MeMo: Towards Language Models with Associative Memory Mechanisms", "abstract": "Memorization is a fundamental ability of Transformer-based Large Language\nModels, achieved through learning. In this paper, we propose a paradigm shift\nby designing an architecture to memorize text directly, bearing in mind the\nprinciple that memorization precedes learning. We introduce MeMo, a novel\narchitecture for language modeling that explicitly memorizes sequences of\ntokens in layered associative memories. By design, MeMo offers transparency and\nthe possibility of model editing, including forgetting texts. We experimented\nwith the MeMo architecture, showing the memorization power of the one-layer and\nthe multi-layer configurations.", "published": "2025-02-18 13:39:22", "link": "http://arxiv.org/abs/2502.12851v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; I.2.4"], "primary_category": "cs.CL"}
{"title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement\n  Learning", "abstract": "Recent studies have demonstrated the effectiveness of LLM test-time scaling.\nHowever, existing approaches to incentivize LLMs' deep thinking abilities\ngenerally require large-scale data or significant training efforts. Meanwhile,\nit remains unclear how to improve the thinking abilities of less powerful base\nmodels. In this work, we introduce S$^2$R, an efficient framework that enhances\nLLM reasoning by teaching models to self-verify and self-correct during\ninference. Specifically, we first initialize LLMs with iterative\nself-verification and self-correction behaviors through supervised fine-tuning\non carefully curated data. The self-verification and self-correction skills are\nthen further strengthened by both outcome-level and process-level reinforcement\nlearning, with minimized resource requirements, enabling the model to\nadaptively refine its reasoning process during inference. Our results\ndemonstrate that, with only 3.1k self-verifying and self-correcting behavior\ninitialization samples, Qwen2.5-math-7B achieves an accuracy improvement from\n51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of\nlong-CoT distilled data. Extensive experiments and analysis based on three base\nmodels across both in-domain and out-of-domain benchmarks validate the\neffectiveness of S$^2$R. Our code and data are available at\nhttps://github.com/NineAbyss/S2R.", "published": "2025-02-18 13:40:22", "link": "http://arxiv.org/abs/2502.12853v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PAFT: Prompt-Agnostic Fine-Tuning", "abstract": "While Large Language Models (LLMs) adapt well to downstream tasks after\nfine-tuning, this adaptability often compromises prompt robustness, as even\nminor prompt variations can significantly degrade performance. To address this,\nwe propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach\nthat dynamically adjusts prompts during fine-tuning. This encourages the model\nto learn underlying task principles rather than overfitting to specific prompt\nformulations. PAFT operates in two stages: First, a diverse set of meaningful,\nsynthetic candidate prompts is constructed. Second, during fine-tuning, prompts\nare randomly sampled from this set to create dynamic training inputs. Extensive\nexperiments across diverse datasets and LLMs demonstrate that models trained\nwith PAFT exhibit strong robustness and generalization across a wide range of\nprompts, including unseen ones. This enhanced robustness improves both model\nperformance and inference speed while maintaining training efficiency. Ablation\nstudies further confirm the effectiveness of PAFT.", "published": "2025-02-18 13:46:47", "link": "http://arxiv.org/abs/2502.12859v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation", "abstract": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose an enhanced schema linking metric by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Agent (KaSLA), a plug-in schema linking agent\ndesigned to prevent the missing of relevant schema elements while minimizing\nthe inclusion of redundant ones. KaSLA employs a hierarchical linking strategy\nthat first identifies the optimal table linking and subsequently links columns\nwithin the selected table to reduce linking candidate space. In each linking\nprocess, it utilize a knapsack optimization approach to link potentially\nrelevant elements while accounting for a limited tolerance of potential\nredundant ones.With this optimization, KaSLA-1.6B achieves superior schema\nlinking results compared to large-scale LLMs, including deepseek-v3 with\nstate-of-the-art (SOTA) schema linking method. Extensive experiments on Spider\nand BIRD benchmarks verify that KaSLA can significantly improve the SQL\ngeneration performance of SOTA text-to-SQL models by substituting their schema\nlinking processes.", "published": "2025-02-18 14:53:45", "link": "http://arxiv.org/abs/2502.12911v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded\n  in Naturally Occurring Data", "abstract": "Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license.", "published": "2025-02-18 15:04:13", "link": "http://arxiv.org/abs/2502.12924v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular\n  Micro-video Generation", "abstract": "Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold\nsignificant commercial value. The rise of high-quality AI-generated content has\nspurred interest in AI-driven micro-video creation. However, despite the\nadvanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek\nin text generation and reasoning, their potential to assist the creation of\npopular micro-videos remains largely unexplored.\n  In this paper, we conduct an empirical study on LLM-assisted popular\nmicro-video generation (LLMPopcorn). Specifically, we investigate the following\nresearch questions: (i) How can LLMs be effectively utilized to assist popular\nmicro-video generation? (ii) To what extent can prompt-based enhancements\noptimize the LLM-generated content for higher popularity? (iii) How well do\nvarious LLMs and video generators perform in the popular micro-video generation\ntask? By exploring these questions, we show that advanced LLMs like DeepSeek-V3\nenable micro-video generation to achieve popularity comparable to human-created\ncontent. Prompt enhancements further boost popularity, and benchmarking\nhighlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and\nHunyuanVideo lead in video generation. This pioneering work advances\nAI-assisted micro-video creation, uncovering new research opportunities. We\nwill release the code and datasets to support future studies.", "published": "2025-02-18 15:29:05", "link": "http://arxiv.org/abs/2502.12945v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "AlignFreeze: Navigating the Impact of Realignment on the Layers of\n  Multilingual Models Across Diverse Languages", "abstract": "Realignment techniques are often employed to enhance cross-lingual transfer\nin multilingual language models, still, they can sometimes degrade performance\nin languages that differ significantly from the fine-tuned source language.\nThis paper introduces AlignFreeze, a method that freezes either the layers'\nlower half or upper half during realignment. Through controlled experiments on\n4 tasks, 3 models, and in 35 languages, we find that realignment affects all\nthe layers but can be the most detrimental to the lower ones. Freezing the\nlower layers can prevent performance degradation. Particularly, AlignFreeze\nimproves Part-of-Speech (PoS) tagging performances in languages where full\nrealignment fails: with XLM-R, it provides improvements of more than one\nstandard deviation in accuracy in seven more languages than full realignment.", "published": "2025-02-18 15:43:27", "link": "http://arxiv.org/abs/2502.12959v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger", "abstract": "Large language models (LLMs) have shown remarkable emergent capabilities,\ntransforming the execution of functional tasks by leveraging external tools for\ncomplex problems that require specialized processing or real-time data. While\nexisting research expands LLMs access to diverse tools (e.g., program\ninterpreters, search engines, weather/map apps), the necessity of using these\ntools is often overlooked, leading to indiscriminate tool invocation. This\nnaive approach raises two key issues:(1) increased delays due to unnecessary\ntool calls, and (2) potential errors resulting from faulty interactions with\nexternal tools. In this paper, we introduce meta-cognition as a proxy for LLMs\nself-assessment of their capabilities, representing the model's awareness of\nits own limitations. Based on this, we propose MeCo, an adaptive\ndecision-making strategy for external tool use. MeCo quantifies metacognitive\nscores by capturing high-level cognitive signals in the representation space,\nguiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs\nminimal cost. Our experiments show that MeCo accurately detects LLMs' internal\ncognitive signals and significantly improves tool-use decision-making across\nmultiple base models and benchmarks.", "published": "2025-02-18 15:45:01", "link": "http://arxiv.org/abs/2502.12961v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for\n  Improved Explainability", "abstract": "Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural models. Meanwhile, B-cos networks have been introduced to\nimprove model explainability through architectural and computational\nadaptations, but their application has so far been limited to computer vision\nmodels and their associated training pipelines. In this work, we introduce\nB-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly\ntransforms pre-trained language models into B-cos LMs by combining B-cos\nconversion and task fine-tuning, improving efficiency compared to previous\nB-cos methods. Our automatic and human evaluation results demonstrate that\nB-cos LMs produce more faithful and human interpretable explanations than post\nhoc methods, while maintaining task performance comparable to conventional\nfine-tuning. Our in-depth analysis explores how B-cos LMs differ from\nconventionally fine-tuned models in their learning processes and explanation\npatterns. Finally, we provide practical guidelines for effectively building\nB-cos LMs based on our findings. Our code is available at\nhttps://anonymous.4open.science/r/bcos_lm.", "published": "2025-02-18 16:13:08", "link": "http://arxiv.org/abs/2502.12992v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with\n  a Multi-Agent Conversations", "abstract": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints.", "published": "2025-02-18 16:21:22", "link": "http://arxiv.org/abs/2502.13001v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging\n  the Gap Between LLMs and Evolving Medical Knowledge", "abstract": "Large Language Models (LLMs) have significantly advanced medical\nquestion-answering by leveraging extensive clinical data and medical\nliterature. However, the rapid evolution of medical knowledge and the\nlabor-intensive process of manually updating domain-specific resources pose\nchallenges to the reliability of these systems. To address this, we introduce\nAdaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates\nthe construction and continuous updating of medical knowledge graphs,\nintegrates reasoning, and retrieves current external evidence, such as PubMed\nand WikiSearch. By dynamically linking new findings and complex medical\nconcepts, AMG-RAG not only improves accuracy but also enhances interpretability\nin medical queries.\n  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness\nof AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of\n66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to\n100 times larger. Notably, these improvements are achieved without increasing\ncomputational overhead, highlighting the critical role of automated knowledge\ngraph generation and external evidence retrieval in delivering up-to-date,\ntrustworthy medical insights.", "published": "2025-02-18 16:29:45", "link": "http://arxiv.org/abs/2502.13010v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large\n  Language Model-Based Role-Playing Agents", "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\nis challenging due to diverse task requirements and agent designs. This paper\nproposes an evidence-based, actionable, and generalizable evaluation design\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\nseven task attributes, and seven evaluation metrics from existing literature.\nBased on these findings, we present an RPA evaluation design guideline to help\nresearchers develop more systematic and consistent evaluation methods.", "published": "2025-02-18 16:33:33", "link": "http://arxiv.org/abs/2502.13012v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity", "abstract": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.", "published": "2025-02-18 17:08:45", "link": "http://arxiv.org/abs/2502.13063v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation", "abstract": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.", "published": "2025-02-18 17:59:48", "link": "http://arxiv.org/abs/2502.13092v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language\n  in a Coreference Context", "abstract": "Gender-inclusive language is often used with the aim of ensuring that all\nindividuals, regardless of gender, can be associated with certain concepts.\nWhile psycholinguistic studies have examined its effects in relation to human\ncognition, it remains unclear how Large Language Models (LLMs) process\ngender-inclusive language. Given that commercial LLMs are gaining an\nincreasingly strong foothold in everyday applications, it is crucial to examine\nwhether LLMs in fact interpret gender-inclusive language neutrally, because the\nlanguage they generate has the potential to influence the language of their\nusers. This study examines whether LLM-generated coreferent terms align with a\ngiven gender expression or reflect model biases. Adapting psycholinguistic\nmethods from French to English and German, we find that in English, LLMs\ngenerally maintain the antecedent's gender but exhibit underlying masculine\nbias. In German, this bias is much stronger, overriding all tested\ngender-neutralization strategies.", "published": "2025-02-18 18:42:11", "link": "http://arxiv.org/abs/2502.13120v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Diverse Human Preference Learning through Principal Component\n  Analysis", "abstract": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment.", "published": "2025-02-18 18:55:26", "link": "http://arxiv.org/abs/2502.13131v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "When People are Floods: Analyzing Dehumanizing Metaphors in Immigration\n  Discourse with Large Language Models", "abstract": "Metaphor, discussing one concept in terms of another, is abundant in politics\nand can shape how people understand important issues. We develop a\ncomputational approach to measure metaphorical language, focusing on\nimmigration discourse on social media. Grounded in qualitative social science\nresearch, we identify seven concepts evoked in immigration discourse (e.g.\n\"water\" or \"vermin\"). We propose and evaluate a novel technique that leverages\nboth word-level and document-level signals to measure metaphor with respect to\nthese concepts. We then study the relationship between metaphor, political\nideology, and user engagement in 400K US tweets about immigration. While\nconservatives tend to use dehumanizing metaphors more than liberals, this\neffect varies widely across concepts. Moreover, creature-related metaphor is\nassociated with more retweets, especially for liberal authors. Our work\nhighlights the potential for computational methods to complement qualitative\napproaches in understanding subtle and implicit language in political\ndiscourse.", "published": "2025-02-18 19:19:01", "link": "http://arxiv.org/abs/2502.13246v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Neural Attention Search", "abstract": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.", "published": "2025-02-18 19:22:44", "link": "http://arxiv.org/abs/2502.13251v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Performance Evaluation of Sentiment Analysis on Text and Emoji Data\n  Using End-to-End, Transfer Learning, Distributed and Explainable AI Models", "abstract": "Emojis are being frequently used in todays digital world to express from\nsimple to complex thoughts more than ever before. Hence, they are also being\nused in sentiment analysis and targeted marketing campaigns. In this work, we\nperformed sentiment analysis of Tweets as well as on emoji dataset from the\nKaggle. Since tweets are sentences we have used Universal Sentence Encoder\n(USE) and Sentence Bidirectional Encoder Representations from Transformers\n(SBERT) end-to-end sentence embedding models to generate the embeddings which\nare used to train the Standard fully connected Neural Networks (NN), and LSTM\nNN models. We observe the text classification accuracy was almost the same for\nboth the models around 98 percent. On the contrary, when the validation set was\nbuilt using emojis that were not present in the training set then the accuracy\nof both the models reduced drastically to 70 percent. In addition, the models\nwere also trained using the distributed training approach instead of a\ntraditional singlethreaded model for better scalability. Using the distributed\ntraining approach, we were able to reduce the run-time by roughly 15% without\ncompromising on accuracy. Finally, as part of explainable AI the Shap algorithm\nwas used to explain the model behaviour and check for model biases for the\ngiven feature set.", "published": "2025-02-18 20:58:37", "link": "http://arxiv.org/abs/2502.13278v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.11"], "primary_category": "cs.CL"}
{"title": "Understanding and Tackling Label Errors in Individual-Level Nature\n  Language Understanding", "abstract": "Natural language understanding (NLU) is a task that enables machines to\nunderstand human language. Some tasks, such as stance detection and sentiment\nanalysis, are closely related to individual subjective perspectives, thus\ntermed individual-level NLU. Previously, these tasks are often simplified to\ntext-level NLU tasks, ignoring individual factors. This not only makes\ninference difficult and unexplainable but often results in a large number of\nlabel errors when creating datasets. To address the above limitations, we\npropose a new NLU annotation guideline based on individual-level factors.\nSpecifically, we incorporate other posts by the same individual and then\nannotate individual subjective perspectives after considering all individual\nposts. We use this guideline to expand and re-annotate the stance detection and\ntopic-based sentiment analysis datasets. We find that error rates in the\nsamples were as high as 31.7\\% and 23.3\\%. We further use large language models\nto conduct experiments on the re-annotation datasets and find that the large\nlanguage models perform well on both datasets after adding individual factors.\nBoth GPT-4o and Llama3-70B can achieve an accuracy greater than 87\\% on the\nre-annotation datasets. We also verify the effectiveness of individual factors\nthrough ablation studies. We call on future researchers to add individual\nfactors when creating such datasets. Our re-annotation dataset can be found at\nhttps://github.com/24yearsoldstudent/Individual-NLU", "published": "2025-02-18 21:35:46", "link": "http://arxiv.org/abs/2502.13297v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The\n  Curious Case of LLMs as Your Coding Tutors", "abstract": "Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized guidance in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students toward completing predefined\ncoding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER),\nwhich combines knowledge tracing to estimate a student's knowledge state and\nturn-by-turn verification to ensure effective guidance toward task completion.\nWe introduce DICT, an automatic evaluation protocol that assesses tutor agents\nholistically using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our results and findings can be extended\nbeyond coding, providing valuable insights into advancing tutoring agents for a\nvariety of tasks.", "published": "2025-02-18 22:13:00", "link": "http://arxiv.org/abs/2502.13311v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models are Few-Shot Graders", "abstract": "Providing evaluations to student work is a critical component of effective\nstudent learning, and automating its process can significantly reduce the\nworkload on human graders. Automatic Short Answer Grading (ASAG) systems,\nenabled by advancements in Large Language Models (LLMs), offer a promising\nsolution for assessing and providing instant feedback for open-ended student\nresponses. In this paper, we present an ASAG pipeline leveraging\nstate-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better\nperformances than existing custom-built models on the same datasets. We also\ncompare the grading performance of three OpenAI models: GPT-4, GPT-4o, and\no1-preview. Our results demonstrate that GPT-4o achieves the best balance\nbetween accuracy and cost-effectiveness. On the other hand, o1-preview, despite\nhigher accuracy, exhibits a larger variance in error that makes it less\npractical for classroom use. We investigate the effects of incorporating\ninstructor-graded examples into prompts using no examples, random selection,\nand Retrieval-Augmented Generation (RAG)-based selection strategies. Our\nfindings indicate that providing graded examples enhances grading accuracy,\nwith RAG-based selection outperforming random selection. Additionally,\nintegrating grading rubrics improves accuracy by offering a structured standard\nfor evaluation.", "published": "2025-02-18 23:38:21", "link": "http://arxiv.org/abs/2502.13337v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can AI mimic the human ability to define neologisms?", "abstract": "One ongoing debate in linguistics is whether Artificial Intelligence (AI) can\neffectively mimic human performance in language-related tasks. While much\nresearch has focused on various linguistic abilities of AI, little attention\nhas been given to how it defines neologisms formed through different word\nformation processes. This study addresses this gap by examining the degree of\nagreement between human and AI-generated responses in defining three types of\nGreek neologisms: blends, compounds, and derivatives. The study employed an\nonline experiment in which human participants selected the most appropriate\ndefinitions for neologisms, while ChatGPT received identical prompts. The\nresults revealed fair agreement between human and AI responses for blends and\nderivatives but no agreement for compounds. However, when considering the\nmajority response among humans, agreement with AI was high for blends and\nderivatives. These findings highlight the complexity of human language and the\nchallenges AI still faces in capturing its nuances. In particular, they suggest\na need for integrating more advanced semantic networks and contextual learning\nmechanisms into AI models to improve their interpretation of complex word\nformations, especially compounds.", "published": "2025-02-18 09:46:38", "link": "http://arxiv.org/abs/2502.14900v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models", "abstract": "Investigating value alignment in Large Language Models (LLMs) based on\ncultural context has become a critical area of research. However, similar\nbiases have not been extensively explored in large vision-language models\n(VLMs). As the scale of multimodal models continues to grow, it becomes\nincreasingly important to assess whether images can serve as reliable proxies\nfor culture and how these values are embedded through the integration of both\nvisual and textual data. In this paper, we conduct a thorough evaluation of\nmultimodal model at different scales, focusing on their alignment with cultural\nvalues. Our findings reveal that, much like LLMs, VLMs exhibit sensitivity to\ncultural values, but their performance in aligning with these values is highly\ncontext-dependent. While VLMs show potential in improving value understanding\nthrough the use of images, this alignment varies significantly across contexts\nhighlighting the complexities and underexplored challenges in the alignment of\nmultimodal models.", "published": "2025-02-18 19:03:02", "link": "http://arxiv.org/abs/2502.14906v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Attribute Steering of Language Models via Targeted Intervention", "abstract": "Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfinetuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline).", "published": "2025-02-18 02:27:23", "link": "http://arxiv.org/abs/2502.12446v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stress Testing Generalization: How Minor Modifications Undermine Large\n  Language Model Performance", "abstract": "This paper investigates the fragility of Large Language Models (LLMs) in\ngeneralizing to novel inputs, specifically focusing on minor perturbations in\nwell-established benchmarks (e.g., slight changes in question format or\ndistractor length). Despite high benchmark scores, LLMs exhibit significant\naccuracy drops and unexpected biases (e.g., preference for longer distractors)\nwhen faced with these minor but content-preserving modifications. For example,\nQwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when\noption lengths are changed without altering the question. Even GPT-4\nexperiences a 25-point accuracy loss when question types are changed, with a\n6-point drop across all three modification categories. These analyses suggest\nthat LLMs rely heavily on superficial cues rather than forming robust, abstract\nrepresentations that generalize across formats, lexical variations, and\nirrelevant content shifts. This work aligns with the ACL 2025 theme track on\nthe Generalization of NLP models, proposing a \"Generalization Stress Test\" to\nassess performance shifts under controlled perturbations. The study calls for\nreevaluating benchmarks and developing more reliable evaluation methodologies\nto capture LLM generalization abilities better.", "published": "2025-02-18 02:42:53", "link": "http://arxiv.org/abs/2502.12459v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings", "abstract": "Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps://github.com/ZeroNLP/SEA.", "published": "2025-02-18 05:57:35", "link": "http://arxiv.org/abs/2502.12562v1", "categories": ["cs.CL", "cs.CR", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Evaluating Language Models on Grooming Risk Estimation Using Fuzzy\n  Theory", "abstract": "Encoding implicit language presents a challenge for language models,\nespecially in high-risk domains where maintaining high precision is important.\nAutomated detection of online child grooming is one such critical domain, where\npredators manipulate victims using a combination of explicit and implicit\nlanguage to convey harmful intentions. While recent studies have shown the\npotential of Transformer language models like SBERT for preemptive grooming\ndetection, they primarily depend on surface-level features and approximate real\nvictim grooming processes using vigilante and law enforcement conversations.\nThe question of whether these features and approximations are reasonable has\nnot been addressed thus far. In this paper, we address this gap and study\nwhether SBERT can effectively discern varying degrees of grooming risk inherent\nin conversations, and evaluate its results across different participant groups.\nOur analysis reveals that while fine-tuning aids language models in learning to\nassign grooming scores, they show high variance in predictions, especially for\ncontexts containing higher degrees of grooming risk. These errors appear in\ncases that 1) utilize indirect speech pathways to manipulate victims and 2)\nlack sexually explicit content. This finding underscores the necessity for\nrobust modeling of indirect speech acts by language models, particularly those\nemployed by predators.", "published": "2025-02-18 05:59:54", "link": "http://arxiv.org/abs/2502.12563v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Fuzzy Evaluation of Sentence Encoders on Grooming Risk Classification", "abstract": "With the advent of social media, children are becoming increasingly\nvulnerable to the risk of grooming in online settings. Detecting grooming\ninstances in an online conversation poses a significant challenge as the\ninteractions are not necessarily sexually explicit, since the predators take\ntime to build trust and a relationship with their victim. Moreover, predators\nevade detection using indirect and coded language. While previous studies have\nfine-tuned Transformers to automatically identify grooming in chat\nconversations, they overlook the impact of coded and indirect language on model\npredictions, and how these align with human perceptions of grooming. In this\npaper, we address this gap and evaluate bi-encoders on the task of classifying\ndifferent degrees of grooming risk in chat contexts, for three different\nparticipant groups, i.e. law enforcement officers, real victims, and decoys.\nUsing a fuzzy-theoretic framework, we map human assessments of grooming\nbehaviors to estimate the actual degree of grooming risk. Our analysis reveals\nthat fine-tuned models fail to tag instances where the predator uses indirect\nspeech pathways and coded language to evade detection. Further, we find that\nsuch instances are characterized by a higher presence of out-of-vocabulary\n(OOV) words in samples, causing the model to misclassify. Our findings\nhighlight the need for more robust models to identify coded language from noisy\nchat inputs in grooming contexts.", "published": "2025-02-18 06:26:46", "link": "http://arxiv.org/abs/2502.12576v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Step Alignment as Markov Games: An Optimistic Online Gradient\n  Descent Approach with Convergence Guarantees", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been highly successful\nin aligning large language models with human preferences. While prevalent\nmethods like DPO have demonstrated strong performance, they frame interactions\nwith the language model as a bandit problem, which limits their applicability\nin real-world scenarios where multi-turn conversations are common.\nAdditionally, DPO relies on the Bradley-Terry model assumption, which does not\nadequately capture the non-transitive nature of human preferences. In this\npaper, we address these challenges by modeling the alignment problem as a\ntwo-player constant-sum Markov game, where each player seeks to maximize their\nwinning rate against the other across all steps of the conversation. Our\napproach Multi-step Preference Optimization (MPO) is built upon the natural\nactor-critic framework~\\citep{peters2008natural}. We further develop OMPO based\non the optimistic online gradient descent\nalgorithm~\\citep{rakhlin2013online,joulani17a}. Theoretically, we provide a\nrigorous analysis for both algorithms on convergence and show that OMPO\nrequires $\\mathcal{O}(\\epsilon^{-1})$ policy updates to converge to an\n$\\epsilon$-approximate Nash equilibrium. We also validate the effectiveness of\nour method on multi-turn conversations dataset and math reasoning dataset.", "published": "2025-02-18 09:33:48", "link": "http://arxiv.org/abs/2502.12678v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Translate Smart, not Hard: Cascaded Translation Systems with\n  Quality-Aware Deferral", "abstract": "Larger models often outperform smaller ones but come with high computational\ncosts. Cascading offers a potential solution. By default, it uses smaller\nmodels and defers only some instances to larger, more powerful models. However,\ndesigning effective deferral rules remains a challenge. In this paper, we\npropose a simple yet effective approach for machine translation, using existing\nquality estimation (QE) metrics as deferral rules. We show that QE-based\ndeferral allows a cascaded system to match the performance of a larger model\nwhile invoking it for a small fraction (30% to 50%) of the examples,\nsignificantly reducing computational costs. We validate this approach through\nboth automatic and human evaluation.", "published": "2025-02-18 10:05:40", "link": "http://arxiv.org/abs/2502.12701v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MediaMind: Revolutionizing Media Monitoring using Agentification", "abstract": "In an era of rapid technological advancements, agentification of software\ntools has emerged as a critical innovation, enabling systems to function\nautonomously and adaptively. This paper introduces MediaMind as a case study to\ndemonstrate the agentification process, highlighting how existing software can\nbe transformed into intelligent agents capable of independent decision-making\nand dynamic interaction. Developed by aiXplain, MediaMind leverages agent-based\narchitecture to autonomously monitor, analyze, and provide insights from\nmultilingual media content in real time. The focus of this paper is on the\ntechnical methodologies and design principles behind agentifying MediaMind,\nshowcasing how agentification enhances adaptability, efficiency, and\nresponsiveness. Through detailed case studies and practical examples, we\nillustrate how the agentification of MediaMind empowers organizations to\nstreamline workflows, optimize decision-making, and respond to evolving trends.\nThis work underscores the broader potential of agentification to revolutionize\nsoftware tools across various domains.", "published": "2025-02-18 11:05:38", "link": "http://arxiv.org/abs/2502.12745v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Machine Translation Corpus Generation: Integrating\n  Human-in-the-Loop Post-Editing with Large Language Models", "abstract": "This paper introduces an advanced methodology for machine translation (MT)\ncorpus generation, integrating semi-automated, human-in-the-loop post-editing\nwith large language models (LLMs) to enhance efficiency and translation\nquality. Building upon previous work that utilized real-time training of a\ncustom MT quality estimation metric, this system incorporates novel LLM\nfeatures such as Enhanced Translation Synthesis and Assisted Annotation\nAnalysis, which improve initial translation hypotheses and quality assessments,\nrespectively. Additionally, the system employs LLM-Driven Pseudo Labeling and a\nTranslation Recommendation System to reduce human annotator workload in\nspecific contexts. These improvements not only retain the original benefits of\ncost reduction and enhanced post-edit quality but also open new avenues for\nleveraging cutting-edge LLM advancements. The project's source code is\navailable for community use, promoting collaborative developments in the field.\nThe demo video can be accessed here.", "published": "2025-02-18 11:16:38", "link": "http://arxiv.org/abs/2502.12755v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Towards Text-Image Interleaved Retrieval", "abstract": "Current multimodal information retrieval studies mainly focus on single-image\ninputs, which limits real-world applications involving multiple images and\ntext-image interleaved content. In this work, we introduce the text-image\ninterleaved retrieval (TIIR) task, where the query and document are interleaved\ntext-image sequences, and the model is required to understand the semantics\nfrom the interleaved context for effective retrieval. We construct a TIIR\nbenchmark based on naturally interleaved wikiHow tutorials, where a specific\npipeline is designed to generate interleaved queries. To explore the task, we\nadapt several off-the-shelf retrievers and build a dense baseline by\ninterleaved multimodal large language model (MLLM). We then propose a novel\nMatryoshka Multimodal Embedder (MME), which compresses the number of visual\ntokens at different granularity, to address the challenge of excessive visual\ntokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption\nof existing models does not consistently yield effective results. Our MME\nachieves significant improvements over the baseline by substantially fewer\nvisual tokens. We provide extensive analysis and will release the dataset and\ncode to facilitate future research.", "published": "2025-02-18 12:00:47", "link": "http://arxiv.org/abs/2502.12799v1", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Integrating Arithmetic Learning Improves Mathematical Reasoning in\n  Smaller Models", "abstract": "While large models pre-trained on high-quality data exhibit excellent\nperformance across various reasoning tasks, including mathematical reasoning\n(e.g. GSM8k, MultiArith), specializing smaller models to excel at mathematical\nreasoning remains a challenging problem. Common approaches to address this\nchallenge include knowledge distillation, where smaller student models learn\nfrom large pre-trained teacher models, and data augmentation, such as\nrephrasing questions. Despite these efforts, smaller models struggle with\narithmetic computations, leading to errors in mathematical reasoning. In this\nwork, we focus on leveraging a programmatically generated arithmetic dataset to\nenhance the reasoning capabilities of smaller models. We investigate two key\napproaches to incorporate this dataset -- (1) intermediate fine-tuning, where a\nmodel is fine-tuned on the arithmetic dataset before being trained on a\nreasoning dataset, and (2) integrating the arithmetic dataset into the\ninstruction-tuning mixture, allowing the model to learn arithmetic skills\nalongside general instruction-following abilities. Our experiments on multiple\nreasoning benchmarks demonstrate that incorporating an arithmetic dataset,\nwhether through targeted fine-tuning or within the instruction-tuning mixture,\nenhances the models' arithmetic capabilities, which in turn improves their\nmathematical reasoning performance.", "published": "2025-02-18 13:43:06", "link": "http://arxiv.org/abs/2502.12855v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rejected Dialects: Biases Against African American Language in Reward\n  Models", "abstract": "Preference alignment via reward models helps build safe, helpful, and\nreliable large language models (LLMs). However, subjectivity in preference\njudgments and the lack of representative sampling in preference data collection\ncan introduce new biases, hindering reward models' fairness and equity. In this\nwork, we introduce a framework for evaluating dialect biases in reward models\nand conduct a case study on biases against African American Language (AAL)\nthrough several experiments comparing reward model preferences and behavior on\npaired White Mainstream English (WME) and both machine-translated and\nhuman-written AAL corpora. We show that reward models are less aligned with\nhuman preferences when processing AAL texts vs. WME ones (-4\\% accuracy on\naverage), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and\nsteer conversations toward WME, even when prompted with AAL texts. Our findings\nprovide a targeted analysis of anti-AAL biases at a relatively understudied\nstage in LLM development, highlighting representational harms and ethical\nquestions about the desired behavior of LLMs concerning AAL.", "published": "2025-02-18 13:45:42", "link": "http://arxiv.org/abs/2502.12858v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4.2"], "primary_category": "cs.CL"}
{"title": "Soundwave: Less is More for Speech-Text Alignment in LLMs", "abstract": "Existing end-to-end speech large language models (LLMs) usually rely on\nlarge-scale annotated data for training, while data-efficient training has not\nbeen discussed in depth. We focus on two fundamental problems between speech\nand text: the representation space gap and sequence length inconsistency. We\npropose Soundwave, which utilizes an efficient training strategy and a novel\narchitecture to address these issues. Results show that Soundwave outperforms\nthe advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,\nusing only one-fiftieth of the training data. Further analysis shows that\nSoundwave still retains its intelligence during conversation. The project is\navailable at https://github.com/FreedomIntelligence/Soundwave.", "published": "2025-02-18 14:36:39", "link": "http://arxiv.org/abs/2502.12900v1", "categories": ["cs.CL", "cs.AI", "cs.SD"], "primary_category": "cs.CL"}
{"title": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning", "abstract": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nBF16-based fine-tuning while significantly reducing 1.85x memory usage.\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.", "published": "2025-02-18 14:54:55", "link": "http://arxiv.org/abs/2502.12913v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking\n  Through Options", "abstract": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs\nto systematically explore a diverse range of possibilities in their reasoning,\nas demonstrated by an FoO-based agentic system for autonomously solving Machine\nLearning tasks (AutoML). Our framework outperforms state-of-the-art baselines,\nachieving improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost\nunder $1 per task, our framework is well-suited for cost-sensitive\napplications. Beyond classification and regression, we illustrate the broader\napplicability of our FoO-based agentic system to tasks such as reinforcement\nlearning and image generation. Our framework presents significant advancements\ncompared to current state-of-the-art agentic systems for AutoML, due to the\nbenefits of FoO in enforcing diversity in LLM solutions through compressed,\nexplainable representations that also support long-term memory when combined\nwith case-based reasoning.", "published": "2025-02-18 15:11:46", "link": "http://arxiv.org/abs/2502.12929v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Every Expert Matters: Towards Effective Knowledge Distillation for\n  Mixture-of-Experts Language Models", "abstract": "With the emergence of Mixture-of-Experts (MoE), the efficient scaling of\nmodel size has accelerated the development of large language models in recent\nyears. However, their high memory requirements prevent their use in\nresource-constrained environments. While knowledge distillation (KD) has been a\nproven method for model compression, its application to MoE teacher models\nremains underexplored. Through our investigation, we discover that\nnon-activated experts in MoE models possess valuable knowledge that benefits\nstudent models. We further demonstrate that existing KD methods are not optimal\nfor compressing MoE models, as they fail to leverage this knowledge\neffectively. To address this, we propose two intuitive MoE-specific KD methods\nfor the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR),\nboth designed to effectively extract knowledge from all experts. Specifically,\nKA augments knowledge by sampling experts multiple times, while SAR uses all\nexperts and adjusts the expert weights through router training to provide\noptimal knowledge. Extensive experiments show that our methods outperform\nconventional KD methods, demonstrating their effectiveness for MoE teacher\nmodels.", "published": "2025-02-18 15:30:34", "link": "http://arxiv.org/abs/2502.12947v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Task-Informed Anti-Curriculum by Masking Improves Downstream Performance\n  on Text", "abstract": "Masked language modeling has become a widely adopted unsupervised technique\nto pre-train language models. However, the process of selecting tokens for\nmasking is random, and the percentage of masked tokens is typically fixed for\nthe entire training process. In this paper, we propose to adjust the masking\nratio and to decide which tokens to mask based on a novel task-informed\nanti-curriculum learning scheme. First, we harness task-specific knowledge\nabout useful and harmful tokens in order to determine which tokens to mask.\nSecond, we propose a cyclic decaying masking ratio, which corresponds to an\nanti-curriculum schedule (from hard to easy). We exemplify our novel\ntask-informed anti-curriculum by masking (TIACBM) approach across three diverse\ndownstream tasks: sentiment analysis, text classification by topic, and\nauthorship attribution. Our findings suggest that TIACBM enhances the ability\nof the model to focus on key task-relevant features, contributing to\nstatistically significant performance gains across tasks. We release our code\nat https://github.com/JarcaAndrei/TIACBM.", "published": "2025-02-18 15:36:16", "link": "http://arxiv.org/abs/2502.12953v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Text Classification Under Class Distribution Shift", "abstract": "The basic underlying assumption of machine learning (ML) models is that the\ntraining and test data are sampled from the same distribution. However, in\ndaily practice, this assumption is often broken, i.e.~the distribution of the\ntest data changes over time, which hinders the application of conventional ML\nmodels. One domain where the distribution shift naturally occurs is text\nclassification, since people always find new topics to discuss. To this end, we\nsurvey research articles studying open-set text classification and related\ntasks. We divide the methods in this area based on the constraints that define\nthe kind of distribution shift and the corresponding problem formulation,\ni.e.~learning with the Universum, zero-shot learning, and open-set learning. We\nnext discuss the predominant mitigation approaches for each problem setup.\nFinally, we identify several future work directions, aiming to push the\nboundaries beyond the state of the art. Interestingly, we find that continual\nlearning can solve many of the issues caused by the shifting class\ndistribution. We maintain a list of relevant papers at\nhttps://github.com/Eduard6421/Open-Set-Survey.", "published": "2025-02-18 15:46:54", "link": "http://arxiv.org/abs/2502.12965v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs", "abstract": "Sailor2 is a family of cutting-edge multilingual language models for\nSouth-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit\ndiverse applications. Building on Qwen2.5, Sailor2 undergoes continuous\npre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to\nsupport 13 SEA languages while retaining proficiency in Chinese and English.\nSailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA\nlanguages. We also deliver a comprehensive cookbook on how to develop the\nmultilingual model in an efficient manner, including five key aspects: data\ncuration, pre-training, post-training, model customization and evaluation. We\nhope that Sailor2 model (Apache 2.0 license) will drive language development in\nthe SEA region, and Sailor2 cookbook will inspire researchers to build more\ninclusive LLMs for other under-served languages.", "published": "2025-02-18 16:04:57", "link": "http://arxiv.org/abs/2502.12982v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks", "abstract": "We present an agentic, autonomous graph expansion framework that iteratively\nstructures and refines knowledge in situ. Unlike conventional knowledge graph\nconstruction methods relying on static extraction or single-pass learning, our\napproach couples a reasoning-native large language model with a continually\nupdated graph representation. At each step, the system actively generates new\nconcepts and relationships, merges them into a global graph, and formulates\nsubsequent prompts based on its evolving structure. Through this\nfeedback-driven loop, the model organizes information into a scale-free network\ncharacterized by hub formation, stable modularity, and bridging nodes that link\ndisparate knowledge clusters. Over hundreds of iterations, new nodes and edges\ncontinue to appear without saturating, while centrality measures and shortest\npath distributions evolve to yield increasingly distributed connectivity. Our\nanalysis reveals emergent patterns, such as the rise of highly connected 'hub'\nconcepts and the shifting influence of 'bridge' nodes, indicating that agentic,\nself-reinforcing graph construction can yield open-ended, coherent knowledge\nstructures. Applied to materials design problems, we present compositional\nreasoning experiments by extracting node-specific and synergy-level principles\nto foster genuinely novel knowledge synthesis, yielding cross-domain ideas that\ntranscend rote summarization and strengthen the framework's potential for\nopen-ended scientific discovery. We discuss other applications in scientific\ndiscovery and outline future directions for enhancing scalability and\ninterpretability.", "published": "2025-02-18 16:44:42", "link": "http://arxiv.org/abs/2502.13025v1", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Natural Language Generation from Visual Sequences: Challenges and Future\n  Directions", "abstract": "The ability to use natural language to talk about visual content is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. Various studies have focused on generating text for single images. In\ncontrast, comparatively little attention has been paid to exhaustively\nanalyzing and advancing work on multiple-image vision-to-text settings. In this\nposition paper, we claim that any task dealing with temporally ordered\nsequences of multiple images or frames is an instance of a broader, more\ngeneral problem involving the understanding of intricate relationships between\nthe visual content and the corresponding text. We comprehensively analyze five\ntasks that are instances of this problem and argue that they pose a common set\nof challenges and share similarities in terms of modeling and evaluation\napproaches. Based on the insights from these various aspects and stages of\nmulti-image-to-text generation, we highlight several open questions and suggest\nfuture research directions. We believe that these directions can advance the\nunderstanding of complex phenomena in this domain and the development of better\nmodels.", "published": "2025-02-18 16:48:18", "link": "http://arxiv.org/abs/2502.13034v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved Fine-Tuning of Large Multimodal Models for Hateful Meme\n  Detection", "abstract": "Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While large multimodal models\nhave shown strong generalization across various tasks, they exhibit poor\ngeneralization to hateful meme detection due to the dynamic nature of memes\ntied to emerging social trends and breaking news. Recent work further\nhighlights the limitations of conventional supervised fine-tuning for large\nmultimodal models in this context. To address these challenges, we propose\nLarge Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a\nnovel two-stage fine-tuning framework designed to improve both in-domain\naccuracy and cross-domain generalization. Experimental results on six widely\nused meme classification datasets demonstrate that LMM-RGCL achieves\nstate-of-the-art performance, outperforming agent-based systems such as\nVPD-PALI-X-55B. Furthermore, our method effectively generalizes to\nout-of-domain memes under low-resource settings, surpassing models like GPT-4o.", "published": "2025-02-18 17:07:29", "link": "http://arxiv.org/abs/2502.13061v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding and Rectifying Safety Perception Distortion in VLMs", "abstract": "Recent studies reveal that vision-language models (VLMs) become more\nsusceptible to harmful requests and jailbreak attacks after integrating the\nvision modality, exhibiting greater vulnerability than their text-only LLM\nbackbones. To uncover the root cause of this phenomenon, we conduct an in-depth\nanalysis and identify a key issue: multimodal inputs introduce an\nmodality-induced activation shift toward a \"safer\" direction compared to their\ntext-only counterparts, leading VLMs to systematically overestimate the safety\nof harmful inputs. We refer to this issue as safety perception distortion. To\nmitigate such distortion, we propose Activation Shift Disentanglement and\nCalibration (ShiftDC), a training-free method that decomposes and calibrates\nthe modality-induced activation shift to reduce the impact of modality on\nsafety. By isolating and removing the safety-relevant component, ShiftDC\nrestores the inherent safety alignment of the LLM backbone while preserving the\nvision-language capabilities of VLMs. Empirical results demonstrate that\nShiftDC significantly enhances alignment performance on safety benchmarks\nwithout impairing model utility.", "published": "2025-02-18 18:06:48", "link": "http://arxiv.org/abs/2502.13095v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving Clinical Question Answering with Multi-Task Learning: A Joint\n  Approach for Answer Extraction and Medical Categorization", "abstract": "Clinical Question Answering (CQA) plays a crucial role in medical\ndecision-making, enabling physicians to extract relevant information from\nElectronic Medical Records (EMRs). While transformer-based models such as BERT,\nBioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in\nCQA, existing models lack the ability to categorize extracted answers, which is\ncritical for structured retrieval, content filtering, and medical decision\nsupport.\n  To address this limitation, we introduce a Multi-Task Learning (MTL)\nframework that jointly trains CQA models for both answer extraction and medical\ncategorization. In addition to predicting answer spans, our model classifies\nresponses into five standardized medical categories: Diagnosis, Medication,\nSymptoms, Procedure, and Lab Reports. This categorization enables more\nstructured and interpretable outputs, making clinical QA models more useful in\nreal-world healthcare settings.\n  We evaluate our approach on emrQA, a large-scale dataset for medical question\nanswering. Results show that MTL improves F1-score by 2.2% compared to standard\nfine-tuning, while achieving 90.7% accuracy in answer categorization. These\nfindings suggest that MTL not only enhances CQA performance but also introduces\nan effective mechanism for categorization and structured medical information\nretrieval.", "published": "2025-02-18 18:20:37", "link": "http://arxiv.org/abs/2502.13108v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health\n  Conditions for Realistic Coaching Agent Interactions", "abstract": "We present an end-to-end framework for generating synthetic users for\nevaluating interactive agents designed to encourage positive behavior changes,\nsuch as in health and lifestyle coaching. The synthetic users are grounded in\nhealth and lifestyle conditions, specifically sleep and diabetes management in\nthis study, to ensure realistic interactions with the health coaching agent.\nSynthetic users are created in two stages: first, structured data are generated\ngrounded in real-world health and lifestyle factors in addition to basic\ndemographics and behavioral attributes; second, full profiles of the synthetic\nusers are developed conditioned on the structured data. Interactions between\nsynthetic users and the coaching agent are simulated using generative\nagent-based models such as Concordia, or directly by prompting a language\nmodel. Using two independently-developed agents for sleep and diabetes coaching\nas case studies, the validity of this framework is demonstrated by analyzing\nthe coaching agent's understanding of the synthetic users' needs and\nchallenges. Finally, through multiple blinded evaluations of user-coach\ninteractions by human experts, we demonstrate that our synthetic users with\nhealth and behavioral attributes more accurately portray real human users with\nthe same attributes, compared to generic synthetic users not grounded in such\nattributes. The proposed framework lays the foundation for efficient\ndevelopment of conversational agents through extensive, realistic, and grounded\nsimulated interactions.", "published": "2025-02-18 18:56:44", "link": "http://arxiv.org/abs/2502.13135v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor\n  Attacks and Adversarial Attacks in Large Language Models", "abstract": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.", "published": "2025-02-18 18:59:00", "link": "http://arxiv.org/abs/2502.13141v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MoBA: Mixture of Block Attention for Long-Context LLMs", "abstract": "Scaling the effective context length is essential for advancing large\nlanguage models (LLMs) toward artificial general intelligence (AGI). However,\nthe quadratic increase in computational complexity inherent in traditional\nattention mechanisms presents a prohibitive overhead. Existing approaches\neither impose strongly biased structures, such as sink or window attention\nwhich are task-specific, or radically modify the attention mechanism into\nlinear approximations, whose performance in complex reasoning tasks remains\ninadequately explored.\n  In this work, we propose a solution that adheres to the ``less structure''\nprinciple, allowing the model to determine where to attend autonomously, rather\nthan introducing predefined biases. We introduce Mixture of Block Attention\n(MoBA), an innovative approach that applies the principles of Mixture of\nExperts (MoE) to the attention mechanism. This novel architecture demonstrates\nsuperior performance on long-context tasks while offering a key advantage: the\nability to seamlessly transition between full and sparse attention, enhancing\nefficiency without the risk of compromising performance. MoBA has already been\ndeployed to support Kimi's long-context requests and demonstrates significant\nadvancements in efficient attention computation for LLMs. Our code is available\nat https://github.com/MoonshotAI/MoBA.", "published": "2025-02-18 14:06:05", "link": "http://arxiv.org/abs/2502.13189v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing\n  Value and Originality in Neural Text Generation", "abstract": "Despite the increasing use of large language models for creative tasks, their\noutputs often lack diversity. Common solutions, such as sampling at higher\ntemperatures, can compromise the quality of the results. Drawing on information\ntheory, we propose a context-based score to quantitatively evaluate value and\noriginality. This score incentivizes accuracy and adherence to the request\nwhile fostering divergence from the learned distribution. We propose using our\nscore as a reward in a reinforcement learning framework to fine-tune large\nlanguage models for maximum performance. We validate our strategy through\nexperiments in poetry generation and math problem solving, demonstrating that\nit enhances the value and originality of the generated solutions.", "published": "2025-02-18 19:00:01", "link": "http://arxiv.org/abs/2502.13207v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HumT DumT: Measuring and controlling human-like language in LLMs", "abstract": "Should LLMs generate language that makes them seem human? Human-like language\nmight improve user experience, but might also lead to overreliance and\nstereotyping. Assessing these potential impacts requires a systematic way to\nmeasure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics\nfor human-like tone and other dimensions of social perceptions in text data\nbased on relative probabilities from an LLM. By measuring HumT across\npreference and usage datasets, we find that users prefer less human-like\noutputs from LLMs. HumT also offers insights into the impacts of\nanthropomorphism: human-like LLM outputs are highly correlated with warmth,\nsocial closeness, femininity, and low status, which are closely linked to the\naforementioned harms. We introduce DumT, a method using HumT to systematically\ncontrol and reduce the degree of human-like tone while preserving model\nperformance. DumT offers a practical approach for mitigating risks associated\nwith anthropomorphic language generation.", "published": "2025-02-18 20:04:09", "link": "http://arxiv.org/abs/2502.13259v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought\n  Reasoning in Large Language Models", "abstract": "Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into\nintermediate reasoning steps, has significantly enhanced the performance of\nlarge language models (LLMs) on challenging tasks. However, the detailed\nreasoning process in CoT often incurs long generation times and high\ncomputational costs, partly due to the inclusion of unnecessary steps. To\naddress this, we propose a method to identify critical reasoning steps using\nperplexity as a measure of their importance: a step is deemed critical if its\nremoval causes a significant increase in perplexity. Our method enables models\nto focus solely on generating these critical steps. This can be achieved\nthrough two approaches: refining demonstration examples in few-shot CoT or\nfine-tuning the model using selected examples that include only critical steps.\nComprehensive experiments validate the effectiveness of our method, which\nachieves a better balance between the reasoning accuracy and efficiency of CoT.", "published": "2025-02-18 20:04:51", "link": "http://arxiv.org/abs/2502.13260v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI\n  Assistance", "abstract": "Trust biases how users rely on AI recommendations in AI-assisted\ndecision-making tasks, with low and high levels of trust resulting in increased\nunder- and over-reliance, respectively. We propose that AI assistants should\nadapt their behavior through trust-adaptive interventions to mitigate such\ninappropriate reliance. For instance, when user trust is low, providing an\nexplanation can elicit more careful consideration of the assistant's advice by\nthe user. In two decision-making scenarios -- laypeople answering science\nquestions and doctors making medical diagnoses -- we find that providing\nsupporting and counter-explanations during moments of low and high trust,\nrespectively, yields up to 38% reduction in inappropriate reliance and 20%\nimprovement in decision accuracy. We are similarly able to reduce over-reliance\nby adaptively inserting forced pauses to promote deliberation. Our results\nhighlight how AI adaptation to user trust facilitates appropriate reliance,\npresenting exciting avenues for improving human-AI collaboration.", "published": "2025-02-18 22:42:39", "link": "http://arxiv.org/abs/2502.13321v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Language Models Can Predict Their Own Behavior", "abstract": "Autoregressive Language Models output text by sequentially predicting the\nnext token to generate, with modern methods like Chain-of-Thought (CoT)\nprompting achieving state-of-the-art reasoning capabilities by scaling the\nnumber of generated tokens. However, are there times when we can infer how the\nmodel will behave (e.g. abstain from answering a question) early in the\ncomputation, making generation unnecessary? We show that internal\nrepresentation of input tokens alone can often precisely predict, not just the\nnext token, but eventual behavior over the entire output sequence. We leverage\nthis capacity and learn probes on internal states to create early warning (and\nexit) systems. Specifically, if the probes can confidently estimate the way the\nLM is going to behave, then the system will avoid generating tokens altogether\nand return the estimated behavior instead. On 27 text classification datasets\nspanning five different tasks, we apply this method to estimate the eventual\nanswer of an LM under CoT prompting, reducing inference costs by 65% (average)\nwhile suffering an accuracy loss of no more than 1.4% (worst case). We\ndemonstrate the potential of this method to pre-emptively identify when a model\nwill abstain from answering a question, fail to follow output format\nspecifications, or give a low-confidence response. We explore the limits of\nthis capability, showing that probes generalize to unseen datasets, but perform\nworse when LM outputs are longer and struggle to predict properties that\nrequire access to knowledge that the models themselves lack. Encouragingly,\nperformance scales with model size, suggesting applicability to the largest of\nmodels", "published": "2025-02-18 23:13:16", "link": "http://arxiv.org/abs/2502.13329v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug\n  Interaction Prediction", "abstract": "Drug discovery is a complex and time-intensive process that requires\nidentifying and validating new therapeutic candidates. Computational approaches\nusing large-scale biomedical knowledge graphs (KGs) offer a promising solution\nto accelerate this process. However, extracting meaningful insights from\nlarge-scale KGs remains challenging due to the complexity of graph traversal.\nExisting subgraph-based methods are tailored to graph neural networks (GNNs),\nmaking them incompatible with other models, such as large language models\n(LLMs). We introduce K-Paths, a retrieval framework that extracts structured,\ndiverse, and biologically meaningful paths from KGs. Integrating these paths\nenables LLMs and GNNs to effectively predict unobserved drug-drug and\ndrug-disease interactions. Unlike traditional path-ranking approaches, K-Paths\nretrieves and transforms paths into a structured format that LLMs can directly\nprocess, facilitating explainable reasoning. K-Paths employs a diversity-aware\nadaptation of Yen's algorithm to retrieve the K shortest loopless paths between\nentities in an interaction query, prioritizing biologically relevant and\ndiverse relationships. Our experiments on benchmark datasets show that K-Paths\nimproves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on\ndrug repurposing and 13.42 points on interaction severity prediction. We also\nshow that Llama 70B achieves F1-score gains of 6.18 and 8.46 points,\nrespectively. K-Paths also improves the supervised training efficiency of\nEmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining\nstrong predictive performance. Beyond its scalability and efficiency, K-Paths\nuniquely bridges the gap between KGs and LLMs, providing explainable rationales\nfor predicted interactions. These capabilities show that K-Paths is a valuable\ntool for efficient data-driven drug discovery.", "published": "2025-02-18 23:55:24", "link": "http://arxiv.org/abs/2502.13344v1", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Retrieval-augmented systems can be dangerous medical communicators", "abstract": "Patients have long sought health information online, and increasingly, they\nare turning to generative AI to answer their health-related queries. Given the\nhigh stakes of the medical domain, techniques like retrieval-augmented\ngeneration and citation grounding have been widely promoted as methods to\nreduce hallucinations and improve the accuracy of AI-generated responses and\nhave been widely adopted into search engines. This paper argues that even when\nthese methods produce literally accurate content drawn from source documents\nsans hallucinations, they can still be highly misleading. Patients may derive\nsignificantly different interpretations from AI-generated outputs than they\nwould from reading the original source material, let alone consulting a\nknowledgeable clinician. Through a large-scale query analysis on topics\nincluding disputed diagnoses and procedure safety, we support our argument with\nquantitative and qualitative evidence of the suboptimal answers resulting from\ncurrent systems. In particular, we highlight how these models tend to\ndecontextualize facts, omit critical relevant sources, and reinforce patient\nmisconceptions or biases. We propose a series of recommendations -- such as the\nincorporation of communication pragmatics and enhanced comprehension of source\ndocuments -- that could help mitigate these issues and extend beyond the\nmedical domain.", "published": "2025-02-18 01:57:02", "link": "http://arxiv.org/abs/2502.14898v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Reading the unreadable: Creating a dataset of 19th century English\n  newspapers using image-to-text language models", "abstract": "Oscar Wilde said, \"The difference between literature and journalism is that\njournalism is unreadable, and literature is not read.\" Unfortunately, The\ndigitally archived journalism of Oscar Wilde's 19th century often has no or\npoor quality Optical Character Recognition (OCR), reducing the accessibility of\nthese archives and making them unreadable both figuratively and literally. This\npaper helps address the issue by performing OCR on \"The Nineteenth Century\nSerials Edition\" (NCSE), an 84k-page collection of 19th-century English\nnewspapers and periodicals, using Pixtral 12B, a pre-trained image-to-text\nlanguage model. The OCR capability of Pixtral was compared to 4 other OCR\napproaches, achieving a median character error rate of 1%, 5x lower than the\nnext best model. The resulting NCSE v2.0 dataset features improved article\nidentification, high-quality OCR, and text classified into four types and\nseventeen topics. The dataset contains 1.4 million entries, and 321 million\nwords. Example use cases demonstrate analysis of topic similarity, readability,\nand event tracking. NCSE v2.0 is freely available to encourage historical and\nsociological research. As a result, 21st-century readers can now share Oscar\nWilde's disappointment with 19th-century journalistic standards, reading the\nunreadable from the comfort of their own computers.", "published": "2025-02-18 11:10:48", "link": "http://arxiv.org/abs/2502.14901v1", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PathRAG: Pruning Graph-based Retrieval Augmented Generation with\n  Relational Paths", "abstract": "Retrieval-augmented generation (RAG) improves the response quality of large\nlanguage models (LLMs) by retrieving knowledge from external databases. Typical\nRAG approaches split the text database into chunks, organizing them in a flat\nstructure for efficient searches. To better capture the inherent dependencies\nand structured relationships across the text database, researchers propose to\norganize textual information into an indexing graph, known asgraph-based RAG.\nHowever, we argue that the limitation of current graph-based RAG methods lies\nin the redundancy of the retrieved information, rather than its insufficiency.\nMoreover, previous methods use a flat structure to organize retrieved\ninformation within the prompts, leading to suboptimal performance. To overcome\nthese limitations, we propose PathRAG, which retrieves key relational paths\nfrom the indexing graph, and converts these paths into textual form for\nprompting LLMs. Specifically, PathRAG effectively reduces redundant information\nwith flow-based pruning, while guiding LLMs to generate more logical and\ncoherent responses with path-based prompting. Experimental results show that\nPathRAG consistently outperforms state-of-the-art baselines across six datasets\nand five evaluation dimensions. The code is available at the following link:\nhttps://github.com/BUPT-GAMMA/PathRAG", "published": "2025-02-18 11:18:55", "link": "http://arxiv.org/abs/2502.14902v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema\n  Adherence", "abstract": "In this paper, we address the challenge of enforcing strict schema adherence\nin large language model (LLM) generation by leveraging LLM reasoning\ncapabilities. Building on the DeepSeek R1 reinforcement learning framework, our\napproach trains structured reasoning skills of a 1.5B parameter model through a\nnovel pipeline that combines synthetic reasoning dataset construction with\ncustom reward functions under Group Relative Policy Optimization (GRPO).\nSpecifically, we first perform R1 reinforcement learning on a 20K sample\nunstructured-to-structured dataset, mirroring the original DeepSeek R1 methods,\nto establish core reasoning abilities. Subsequently, we performed supervised\nfine-tuning on a separate 10K reasoning sample dataset, focusing on refining\nschema adherence for downstream tasks. Despite the relatively modest training\nscope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO\ntraining and 3 hours on 1xA100 for SFT, our model demonstrates robust\nperformance in enforcing schema consistency. We compare our ThinkJSON approach\nagainst the original DeepSeek R1 (671B), distilled versions of DeepSeek R1\n(Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its\neffectiveness in real-world applications. Our results underscore the practical\nutility of a resource-efficient framework for schema-constrained text\ngeneration.", "published": "2025-02-18 16:44:55", "link": "http://arxiv.org/abs/2502.14905v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Transformers as Iterative Solution Improvers for\n  Constraint Satisfaction", "abstract": "We present a Transformer-based framework for Constraint Satisfaction Problems\n(CSPs). CSPs find use in many applications and thus accelerating their solution\nwith machine learning is of wide interest. Most existing approaches rely on\nsupervised learning from feasible solutions or reinforcement learning,\nparadigms that require either feasible solutions to these NP-Complete CSPs or\nlarge training budgets and a complex expert-designed reward signal. To address\nthese challenges, we propose ConsFormer, a self-supervised framework that\nleverages a Transformer as a solution refiner. ConsFormer constructs a solution\nto a CSP iteratively in a process that mimics local search. Instead of using\nfeasible solutions as labeled data, we devise differentiable approximations to\nthe discrete constraints of a CSP to guide model training. Our model is trained\nto improve random assignments for a single step but is deployed iteratively at\ntest time, circumventing the bottlenecks of supervised and reinforcement\nlearning. Our method can tackle out-of-distribution CSPs simply through\nadditional iterations.", "published": "2025-02-18 16:51:01", "link": "http://arxiv.org/abs/2502.15794v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.LG"}
{"title": "Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual\n  Data in AutoFormalization", "abstract": "Autoformalization, the process of transforming informal mathematical language\ninto formal specifications and proofs remains a difficult task for\nstate-of-the-art (large) language models. Existing works point to competing\nexplanations for the performance gap. To this end, we introduce a novel\nmethodology that leverages back-translation with hand-curated prompts to\nenhance the mathematical capabilities of language models, particularly\naddressing the challenge posed by the scarcity of labeled data. Specifically,\nwe evaluate three primary variations of this strategy: (1) on-the-fly (online)\nbacktranslation, (2) distilled (offline) backtranslation with few-shot\namplification, and (3) line-by-line proof analysis integrated with proof state\ninformation. Each variant is designed to optimize data quality over quantity,\nfocusing on the high fidelity of generated proofs rather than sheer data scale.\nOur findings provide evidence that employing our proposed approaches to\ngenerate synthetic data, which prioritizes quality over volume, improves the\nAutoformalization performance of LLMs as measured by standard benchmarks such\nas ProofNet. Crucially, our approach outperforms pretrained models using a\nminimal number of tokens. We also show, through strategic prompting and\nbacktranslation, that our approaches surpass the performance of fine-tuning\nwith extensive multilingual datasets such as MMA on ProofNet with only 1/150th\nof the tokens. Taken together, our methods show a promising new approach to\nsignificantly reduce the resources required to formalize proofs, thereby\naccelerating AI for math.", "published": "2025-02-18 19:16:54", "link": "http://arxiv.org/abs/2502.15795v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.AI"}
{"title": "Pruning as a Defense: Reducing Memorization in Large Language Models", "abstract": "Large language models have been shown to memorize significant portions of\ntheir training data, which they can reproduce when appropriately prompted. This\nwork investigates the impact of simple pruning techniques on this behavior. Our\nfindings reveal that pruning effectively reduces the extent of memorization in\nLLMs, demonstrating its potential as a foundational approach for mitigating\nmembership inference attacks.", "published": "2025-02-18 19:32:10", "link": "http://arxiv.org/abs/2502.15796v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Brain-to-Text Decoding: A Non-invasive Approach via Typing", "abstract": "Modern neuroprostheses can now restore communication in patients who have\nlost the ability to speak or move. However, these invasive devices entail risks\ninherent to neurosurgery. Here, we introduce a non-invasive method to decode\nthe production of sentences from brain activity and demonstrate its efficacy in\na cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new\ndeep learning architecture trained to decode sentences from either electro-\n(EEG) or magneto-encephalography (MEG), while participants typed briefly\nmemorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on\naverage, a character-error-rate (CER) of 32% and substantially outperforms EEG\n(CER: 67%). For the best participants, the model achieves a CER of 19%, and can\nperfectly decode a variety of sentences outside of the training set. While\nerror analyses suggest that decoding depends on motor processes, the analysis\nof typographical errors suggests that it also involves higher-level cognitive\nfactors. Overall, these results narrow the gap between invasive and\nnon-invasive methods and thus open the path for developing safe brain-computer\ninterfaces for non-communicating patients.", "published": "2025-02-18 08:36:46", "link": "http://arxiv.org/abs/2502.17480v1", "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "eess.SP"}
{"title": "Dr Web: a modern, query-based web data retrieval engine", "abstract": "This article introduces the Data Retrieval Web Engine (also referred to as\ndoctor web), a flexible and modular tool for extracting structured data from\nweb pages using a simple query language. We discuss the engineering challenges\naddressed during its development, such as dynamic content handling and messy\ndata extraction. Furthermore, we cover the steps for making the DR Web Engine\npublic, highlighting its open source potential.", "published": "2025-02-18 16:37:20", "link": "http://arxiv.org/abs/2504.05311v1", "categories": ["cs.DB", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "EquiBench: Benchmarking Code Reasoning Capabilities of Large Language\n  Models via Equivalence Checking", "abstract": "Equivalence checking, i.e., determining whether two programs produce\nidentical outputs for all possible inputs, underpins a broad range of\napplications, including software refactoring, testing, and optimization. We\npresent the task of equivalence checking as a new way to evaluate the code\nreasoning abilities of large language models (LLMs). We introduce EquiBench, a\ndataset of 2400 program pairs spanning four programming languages and six\nequivalence categories. These pairs are systematically generated through\nprogram analysis, compiler scheduling, and superoptimization, covering\nnontrivial structural transformations that demand deep semantic reasoning\nbeyond simple syntactic variations. Our evaluation of 17 state-of-the-art LLMs\nshows that OpenAI o3-mini achieves the highest overall accuracy of 78.0%. In\nthe most challenging categories, the best accuracies are 62.3% and 68.8%, only\nmodestly above the 50% random baseline for binary classification, indicating\nsignificant room for improvement in current models' code reasoning\ncapabilities.", "published": "2025-02-18 02:54:25", "link": "http://arxiv.org/abs/2502.12466v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning", "abstract": "Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a\npre-alignment Transformer to enhance modality fusion prior to input into text\nLLMs, tailoring DeepResonance for multi-way instruction tuning. Our model\nachieves state-of-the-art performances across six music understanding tasks,\nhighlighting the benefits of the auxiliary modalities and the structural\nsuperiority of DeepResonance. We plan to open-source the models and the newly\nconstructed datasets.", "published": "2025-02-18 08:09:42", "link": "http://arxiv.org/abs/2502.12623v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question\n  Answering?", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general\ndomains but often struggle with tasks requiring specialized knowledge.\nConventional Retrieval-Augmented Generation (RAG) techniques typically retrieve\nexternal information from static knowledge bases, which can be outdated or\nincomplete, missing fine-grained clinical details essential for accurate\nmedical question answering. In this work, we propose SearchRAG, a novel\nframework that overcomes these limitations by leveraging real-time search\nengines. Our method employs synthetic query generation to convert complex\nmedical questions into search-engine-friendly queries and utilizes\nuncertainty-based knowledge selection to filter and incorporate the most\nrelevant and informative medical knowledge into the LLM's input. Experimental\nresults demonstrate that our method significantly improves response accuracy in\nmedical question answering tasks, particularly for complex questions requiring\ndetailed and up-to-date knowledge.", "published": "2025-02-18 19:12:15", "link": "http://arxiv.org/abs/2502.13233v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Benchmarking Automatic Speech Recognition coupled LLM Modules for\n  Medical Diagnostics", "abstract": "Natural Language Processing (NLP) and Voice Recognition agents are rapidly\nevolving healthcare by enabling efficient, accessible, and professional patient\nsupport while automating grunt work. This report serves as my self project\nwherein models finetuned on medical call recordings are analysed through a\ntwo-stage system: Automatic Speech Recognition (ASR) for speech transcription\nand a Large Language Model (LLM) for context-aware, professional responses.\nASR, finetuned on phone call recordings provides generalised transcription of\ndiverse patient speech over call, while the LLM matches transcribed text to\nmedical diagnosis. A novel audio preprocessing strategy, is deployed to provide\ninvariance to incoming recording/call data, laden with sufficient augmentation\nwith noise/clipping to make the pipeline robust to the type of microphone and\nambient conditions the patient might have while calling/recording.", "published": "2025-02-18 14:05:13", "link": "http://arxiv.org/abs/2502.13982v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Gesture-Aware Zero-Shot Speech Recognition for Patients with Language\n  Disorders", "abstract": "Individuals with language disorders often face significant communication\nchallenges due to their limited language processing and comprehension\nabilities, which also affect their interactions with voice-assisted systems\nthat mostly rely on Automatic Speech Recognition (ASR). Despite advancements in\nASR that address disfluencies, there has been little attention on integrating\nnon-verbal communication methods, such as gestures, which individuals with\nlanguage disorders substantially rely on to supplement their communication.\nRecognizing the need to interpret the latent meanings of visual information not\ncaptured by speech alone, we propose a gesture-aware ASR system utilizing a\nmultimodal large language model with zero-shot learning for individuals with\nspeech impairments. Our experiment results and analyses show that including\ngesture information significantly enhances semantic understanding. This study\ncan help develop effective communication technologies, specifically designed to\nmeet the unique needs of individuals with language impairments.", "published": "2025-02-18 14:15:55", "link": "http://arxiv.org/abs/2502.13983v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Note-Level Singing Melody Transcription for Time-Aligned Musical Score\n  Generation", "abstract": "Automatic music transcription converts audio recordings into symbolic\nrepresentations, facilitating music analysis, retrieval, and generation. A\nmusical note is characterized by pitch, onset, and offset in an audio domain,\nwhereas it is defined in terms of pitch and note value in a musical score\ndomain. A time-aligned score, derived from timing information along with pitch\nand note value, allows matching a part of the score with the corresponding part\nof the music audio, enabling various applications. In this paper, we consider\nan extended version of the traditional note-level transcription task that\nrecognizes onset, offset, and pitch, through including extraction of additional\nnote value to generate a time-aligned score from an audio input. To address\nthis new challenge, we propose an end-to-end framework that integrates\nrecognition of the note value, pitch, and temporal information. This approach\navoids error accumulation inherent in multi-stage methods and enhances accuracy\nthrough mutual reinforcement. Our framework employs tokenized representations\nspecifically targeted for this task, through incorporating note value\ninformation. Furthermore, we introduce a pseudo-labeling technique to address a\nscarcity problem of annotated note value data. This technique produces\napproximate note value labels from existing datasets for the traditional\nnote-level transcription. Experimental results demonstrate the superior\nperformance of the proposed model in note-level transcription tasks when\ncompared to existing state-of-the-art approaches. We also introduce new\nevaluation metrics that assess both temporal and note value aspects to\ndemonstrate the robustness of the model. Moreover, qualitative assessments via\nvisualized musical scores confirmed the effectiveness of our model in capturing\nthe note values.", "published": "2025-02-18 02:12:56", "link": "http://arxiv.org/abs/2502.12438v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "A Comprehensive Survey on Generative AI for Video-to-Music Generation", "abstract": "The burgeoning growth of video-to-music generation can be attributed to the\nascendancy of multimodal generative models. However, there is a lack of\nliterature that comprehensively combs through the work in this field. To fill\nthis gap, this paper presents a comprehensive review of video-to-music\ngeneration using deep generative AI techniques, focusing on three key\ncomponents: visual feature extraction, music generation frameworks, and\nconditioning mechanisms. We categorize existing approaches based on their\ndesigns for each component, clarifying the roles of different strategies.\nPreceding this, we provide a fine-grained classification of video and music\nmodalities, illustrating how different categories influence the design of\ncomponents within the generation pipelines. Furthermore, we summarize available\nmultimodal datasets and evaluation metrics while highlighting ongoing\nchallenges in the field.", "published": "2025-02-18 03:18:54", "link": "http://arxiv.org/abs/2502.12489v1", "categories": ["eess.AS", "cs.AI", "cs.MM"], "primary_category": "eess.AS"}
