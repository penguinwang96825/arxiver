{"title": "MovieQA: Understanding Stories in Movies through Question-Answering", "abstract": "We introduce the MovieQA dataset which aims to evaluate automatic story\ncomprehension from both video and text. The dataset consists of 14,944\nquestions about 408 movies with high semantic diversity. The questions range\nfrom simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events\noccurred. Each question comes with a set of five possible answers; a correct\none and four deceiving answers provided by human annotators. Our dataset is\nunique in that it contains multiple sources of information -- video clips,\nplots, subtitles, scripts, and DVS. We analyze our data through various\nstatistics and methods. We further extend existing QA techniques to show that\nquestion-answering with such open-ended semantics is hard. We make this data\nset public along with an evaluation benchmark to encourage inspiring work in\nthis challenging domain.", "published": "2015-12-09 15:34:31", "link": "http://arxiv.org/abs/1512.02902v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
