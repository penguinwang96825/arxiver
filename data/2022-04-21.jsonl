{"title": "A Masked Image Reconstruction Network for Document-level Relation\n  Extraction", "abstract": "Document-level relation extraction aims to extract relations among entities\nwithin a document. Compared with its sentence-level counterpart, Document-level\nrelation extraction requires inference over multiple sentences to extract\ncomplex relational triples. Previous research normally complete reasoning\nthrough information propagation on the mention-level or entity-level\ndocument-graphs, regardless of the correlations between the relationships. In\nthis paper, we propose a novel Document-level Relation Extraction model based\non a Masked Image Reconstruction network (DRE-MIR), which models inference as a\nmasked image reconstruction problem to capture the correlations between\nrelationships. Specifically, we first leverage an encoder module to get the\nfeatures of entities and construct the entity-pair matrix based on the\nfeatures. After that, we look on the entity-pair matrix as an image and then\nrandomly mask it and restore it through an inference module to capture the\ncorrelations between the relationships. We evaluate our model on three public\ndocument-level relation extraction datasets, i.e. DocRED, CDR, and GDA.\nExperimental results demonstrate that our model achieves state-of-the-art\nperformance on these three datasets and has excellent robustness against the\nnoises during the inference process.", "published": "2022-04-21 02:41:21", "link": "http://arxiv.org/abs/2204.09851v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Neural Topic Modelling Better than Clustering? An Empirical Study on\n  Clustering with Contextual Embeddings for Topics", "abstract": "Recent work incorporates pre-trained word embeddings such as BERT embeddings\ninto Neural Topic Models (NTMs), generating highly coherent topics. However,\nwith high-quality contextualized document representations, do we really need\nsophisticated neural models to obtain coherent and interpretable topics? In\nthis paper, we conduct thorough experiments showing that directly clustering\nhigh-quality sentence embeddings with an appropriate word selecting method can\ngenerate more coherent and diverse topics than NTMs, achieving also higher\nefficiency and simplicity.", "published": "2022-04-21 04:26:51", "link": "http://arxiv.org/abs/2204.09874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Attention-Based Model for Predicting Contextual Informativeness and\n  Curriculum Learning Applications", "abstract": "Both humans and machines learn the meaning of unknown words through\ncontextual information in a sentence, but not all contexts are equally helpful\nfor learning. We introduce an effective method for capturing the level of\ncontextual informativeness with respect to a given target word. Our study makes\nthree main contributions. First, we develop models for estimating contextual\ninformativeness, focusing on the instructional aspect of sentences. Our\nattention-based approach using pre-trained embeddings demonstrates\nstate-of-the-art performance on our single-context dataset and an existing\nmulti-sentence context dataset. Second, we show how our model identifies key\ncontextual elements in a sentence that are likely to contribute most to a\nreader's understanding of the target word. Third, we examine how our contextual\ninformativeness model, originally developed for vocabulary learning\napplications for students, can be used for developing better training curricula\nfor word embedding models in batch learning and few-shot machine learning\nsettings. We believe our results open new possibilities for applications that\nsupport language learning for both human and machine learners.", "published": "2022-04-21 05:17:49", "link": "http://arxiv.org/abs/2204.09885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spurious Correlations in Reference-Free Evaluation of Text Generation", "abstract": "Model-based, reference-free evaluation metrics have been proposed as a fast\nand cost-effective approach to evaluate Natural Language Generation (NLG)\nsystems. Despite promising recent results, we find evidence that reference-free\nevaluation metrics of summarization and dialog generation may be relying on\nspurious correlations with measures such as word overlap, perplexity, and\nlength. We further observe that for text summarization, these metrics have high\nerror rates when ranking current state-of-the-art abstractive summarization\nsystems. We demonstrate that these errors can be mitigated by explicitly\ndesigning evaluation metrics to avoid spurious features in reference-free\nevaluation.", "published": "2022-04-21 05:32:38", "link": "http://arxiv.org/abs/2204.09890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation\n  and ensemble to recognize complex Named Entities in Bangla", "abstract": "Many areas, such as the biological and healthcare domain, artistic works, and\norganization names, have nested, overlapping, discontinuous entity mentions\nthat may even be syntactically or semantically ambiguous in practice.\nTraditional sequence tagging algorithms are unable to recognize these complex\nmentions because they may violate the assumptions upon which sequence tagging\nschemes are founded. In this paper, we describe our contribution to SemEval\n2022 Task 11 on identifying such complex Named Entities. We have leveraged the\nensemble of multiple ELECTRA-based models that were exclusively pretrained on\nthe Bangla language with the performance of ELECTRA-based models pretrained on\nEnglish to achieve competitive performance on the Track-11. Besides providing a\nsystem description, we will also present the outcomes of our experiments on\narchitectural decisions, dataset augmentations, and post-competition findings.", "published": "2022-04-21 08:40:17", "link": "http://arxiv.org/abs/2204.09964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence\n  Embedding", "abstract": "This paper presents the shared task on Multilingual Idiomaticity Detection\nand Sentence Embedding, which consists of two subtasks: (a) a binary\nclassification task aimed at identifying whether a sentence contains an\nidiomatic expression, and (b) a task based on semantic text similarity which\nrequires the model to adequately represent potentially idiomatic expressions in\ncontext. Each subtask includes different settings regarding the amount of\ntraining data. Besides the task description, this paper introduces the datasets\nin English, Portuguese, and Galician and their annotation procedure, the\nevaluation metrics, and a summary of the participant systems and their results.\nThe task had close to 100 registered participants organised into twenty five\nteams making over 650 and 150 submissions in the practice and evaluation phases\nrespectively.", "published": "2022-04-21 12:20:52", "link": "http://arxiv.org/abs/2204.10050v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OTExtSum: Extractive Text Summarisation with Optimal Transport", "abstract": "Extractive text summarisation aims to select salient sentences from a\ndocument to form a short yet informative summary. While learning-based methods\nhave achieved promising results, they have several limitations, such as\ndependence on expensive training and lack of interpretability. Therefore, in\nthis paper, we propose a novel non-learning-based method by for the first time\nformulating text summarisation as an Optimal Transport (OT) problem, namely\nOptimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction\nis conceptualised as obtaining an optimal summary that minimises the\ntransportation cost to a given document regarding their semantic distributions.\nSuch a cost is defined by the Wasserstein distance and used to measure the\nsummary's semantic coverage of the original document. Comprehensive experiments\non four challenging and widely used datasets - MultiNews, PubMed, BillSum, and\nCNN/DM demonstrate that our proposed method outperforms the state-of-the-art\nnon-learning-based methods and several recent learning-based methods in terms\nof the ROUGE metric.", "published": "2022-04-21 13:25:34", "link": "http://arxiv.org/abs/2204.10086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Answer Verification Methods for Question Answering-Based\n  Summarization Evaluation Metrics", "abstract": "Question answering-based summarization evaluation metrics must automatically\ndetermine whether the QA model's prediction is correct or not, a task known as\nanswer verification. In this work, we benchmark the lexical answer verification\nmethods which have been used by current QA-based metrics as well as two more\nsophisticated text comparison methods, BERTScore and LERC. We find that LERC\nout-performs the other methods in some settings while remaining statistically\nindistinguishable from lexical overlap in others. However, our experiments\nreveal that improved verification performance does not necessarily translate to\noverall QA-based metric quality: In some scenarios, using a worse verification\nmethod -- or using none at all -- has comparable performance to using the best\nverification method, a result that we attribute to properties of the datasets.", "published": "2022-04-21 15:43:45", "link": "http://arxiv.org/abs/2204.10206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re-Examining System-Level Correlations of Automatic Summarization\n  Evaluation Metrics", "abstract": "How reliably an automatic summarization evaluation metric replicates human\njudgments of summary quality is quantified by system-level correlations. We\nidentify two ways in which the definition of the system-level correlation is\ninconsistent with how metrics are used to evaluate systems in practice and\npropose changes to rectify this disconnect. First, we calculate the system\nscore for an automatic metric using the full test set instead of the subset of\nsummaries judged by humans, which is currently standard practice. We\ndemonstrate how this small change leads to more precise estimates of\nsystem-level correlations. Second, we propose to calculate correlations only on\npairs of systems that are separated by small differences in automatic scores\nwhich are commonly observed in practice. This allows us to demonstrate that our\nbest estimate of the correlation of ROUGE to human judgments is near 0 in\nrealistic scenarios. The results from the analyses point to the need to collect\nmore high-quality human judgments and to improve automatic metrics when\ndifferences in system scores are small.", "published": "2022-04-21 15:52:14", "link": "http://arxiv.org/abs/2204.10216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in\n  the Entity Space", "abstract": "Translation distance based knowledge graph embedding (KGE) methods, such as\nTransE and RotatE, model the relation in knowledge graphs as translation or\nrotation in the vector space. Both translation and rotation are injective; that\nis, the translation or rotation of different vectors results in different\nresults. In knowledge graphs, different entities may have a relation with the\nsame entity; for example, many actors starred in one movie. Such a\nnon-injective relation pattern cannot be well modeled by the translation or\nrotation operations in existing translation distance based KGE methods. To\ntackle the challenge, we propose a translation distance-based KGE method called\nSpaceE to model relations as linear transformations. The proposed SpaceE embeds\nboth entities and relations in knowledge graphs as matrices and SpaceE\nnaturally models non-injective relations with singular linear transformations.\nWe theoretically demonstrate that SpaceE is a fully expressive model with the\nability to infer multiple desired relation patterns, including symmetry,\nskew-symmetry, inversion, Abelian composition, and non-Abelian composition.\nExperimental results on link prediction datasets illustrate that SpaceE\nsubstantially outperforms many previous translation distance based knowledge\ngraph embedding methods, especially on datasets with many non-injective\nrelations. The code is available based on the PaddlePaddle deep learning\nplatform https://www.paddlepaddle.org.cn.", "published": "2022-04-21 16:26:20", "link": "http://arxiv.org/abs/2204.10245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings", "abstract": "We propose DiffCSE, an unsupervised contrastive learning framework for\nlearning sentence embeddings. DiffCSE learns sentence embeddings that are\nsensitive to the difference between the original sentence and an edited\nsentence, where the edited sentence is obtained by stochastically masking out\nthe original sentence and then sampling from a masked language model. We show\nthat DiffSCE is an instance of equivariant contrastive learning (Dangovski et\nal., 2021), which generalizes contrastive learning and learns representations\nthat are insensitive to certain types of augmentations and sensitive to other\n\"harmful\" types of augmentations. Our experiments show that DiffCSE achieves\nstate-of-the-art results among unsupervised sentence representation learning\nmethods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic\ntextual similarity tasks.", "published": "2022-04-21 17:32:01", "link": "http://arxiv.org/abs/2204.10298v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decorate the Examples: A Simple Method of Prompt Design for Biomedical\n  Relation Extraction", "abstract": "Relation extraction is a core problem for natural language processing in the\nbiomedical domain. Recent research on relation extraction showed that\nprompt-based learning improves the performance on both fine-tuning on full\ntraining set and few-shot training. However, less effort has been made on\ndomain-specific tasks where good prompt design can be even harder. In this\npaper, we investigate prompting for biomedical relation extraction, with\nexperiments on the ChemProt dataset. We present a simple yet effective method\nto systematically generate comprehensive prompts that reformulate the relation\nextraction task as a cloze-test task under a simple prompt formulation. In\nparticular, we experiment with different ranking scores for prompt selection.\nWith BioMed-RoBERTa-base, our results show that prompting-based fine-tuning\nobtains gains by 14.21 F1 over its regular fine-tuning baseline, and 1.14 F1\nover SciFive-Large, the current state-of-the-art on ChemProt. Besides, we find\nprompt-based learning requires fewer training examples to make reasonable\npredictions. The results demonstrate the potential of our methods in such a\ndomain-specific relation extraction task.", "published": "2022-04-21 18:39:22", "link": "http://arxiv.org/abs/2204.10360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "German Parliamentary Corpus (GerParCor)", "abstract": "Parliamentary debates represent a large and partly unexploited treasure trove\nof publicly accessible texts. In the German-speaking area, there is a certain\ndeficit of uniformly accessible and annotated corpora covering all\nGerman-speaking parliaments at the national and federal level. To address this\ngap, we introduce the German Parliament Corpus (GerParCor). GerParCor is a\ngenre-specific corpus of (predominantly historical) German-language\nparliamentary protocols from three centuries and four countries, including\nstate and federal level data. In addition, GerParCor contains conversions of\nscanned protocols and, in particular, of protocols in Fraktur converted via an\nOCR process based on Tesseract. All protocols were preprocessed by means of the\nNLP pipeline of spaCy3 and automatically annotated with metadata regarding\ntheir session date. GerParCor is made available in the XMI format of the UIMA\nproject. In this way, GerParCor can be used as a large corpus of historical\ntexts in the field of political communication for various tasks in NLP.", "published": "2022-04-21 22:06:55", "link": "http://arxiv.org/abs/2204.10422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Generalizability of Depression Detection by Leveraging\n  Clinical Questionnaires", "abstract": "Automated methods have been widely used to identify and analyze mental health\nconditions (e.g., depression) from various sources of information, including\nsocial media. Yet, deployment of such models in real-world healthcare\napplications faces challenges including poor out-of-domain generalization and\nlack of trust in black box models. In this work, we propose approaches for\ndepression detection that are constrained to different degrees by the presence\nof symptoms described in PHQ9, a questionnaire used by clinicians in the\ndepression screening process. In dataset-transfer experiments on three social\nmedia datasets, we find that grounding the model in PHQ9's symptoms\nsubstantially improves its ability to generalize to out-of-distribution data\ncompared to a standard BERT-based approach. Furthermore, this approach can\nstill perform competitively on in-domain data. These results and our\nqualitative analyses suggest that grounding model predictions in\nclinically-relevant symptoms can improve generalizability while producing a\nmodel that is easier to inspect.", "published": "2022-04-21 22:57:11", "link": "http://arxiv.org/abs/2204.10432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making the Most of Text Semantics to Improve Biomedical Vision--Language\n  Processing", "abstract": "Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.", "published": "2022-04-21 00:04:35", "link": "http://arxiv.org/abs/2204.09817v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Model-Agnostic Data Manipulation Method for Persona-based Dialogue\n  Generation", "abstract": "Towards building intelligent dialogue agents, there has been a growing\ninterest in introducing explicit personas in generation models. However, with\nlimited persona-based dialogue data at hand, it may be difficult to train a\ndialogue generation model well. We point out that the data challenges of this\ngeneration task lie in two aspects: first, it is expensive to scale up current\npersona-based dialogue datasets; second, each data sample in this task is more\ncomplex to learn with than conventional dialogue data. To alleviate the above\ndata issues, we propose a data manipulation method, which is model-agnostic to\nbe packed with any persona-based dialogue generation model to improve its\nperformance. The original training samples will first be distilled and thus\nexpected to be fitted more easily. Next, we show various effective ways that\ncan diversify such easier distilled data. A given base model will then be\ntrained via the constructed data curricula, i.e. first on augmented distilled\nsamples and then on original ones. Experiments illustrate the superiority of\nour method with two strong base dialogue models (Transformer encoder-decoder\nand GPT2).", "published": "2022-04-21 03:49:54", "link": "http://arxiv.org/abs/2204.09867v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-task recommendation system for scientific papers with high-way\n  networks", "abstract": "Finding and selecting the most relevant scientific papers from a large number\nof papers written in a research community is one of the key challenges for\nresearchers these days. As we know, much information around research interest\nfor scholars and academicians belongs to papers they read. Analysis and\nextracting contextual features from these papers could help us to suggest the\nmost related paper to them. In this paper, we present a multi-task\nrecommendation system (RS) that predicts a paper recommendation and generates\nits meta-data such as keywords. The system is implemented as a three-stage deep\nneural network encoder that tries to maps longer sequences of text to an\nembedding vector and learns simultaneously to predict the recommendation rate\nfor a particular user and the paper's keywords. The motivation behind this\napproach is that the paper's topics expressed as keywords are a useful\npredictor of preferences of researchers. To achieve this goal, we use a system\ncombination of RNNs, Highway and Convolutional Neural Networks to train\nend-to-end a context-aware collaborative matrix. Our application uses Highway\nnetworks to train the system very deep, combine the benefits of RNN and CNN to\nfind the most important factor and make latent representation. Highway Networks\nallow us to enhance the traditional RNN and CNN pipeline by learning more\nsophisticated semantic structural representations. Using this method we can\nalso overcome the cold start problem and learn latent features over large\nsequences of text.", "published": "2022-04-21 07:40:47", "link": "http://arxiv.org/abs/2204.09930v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Recovering Patient Journeys: A Corpus of Biomedical Entities and\n  Relations on Twitter (BEAR)", "abstract": "Text mining and information extraction for the medical domain has focused on\nscientific text generated by researchers. However, their direct access to\nindividual patient experiences or patient-doctor interactions can be limited.\nInformation provided on social media, e.g., by patients and their relatives,\ncomplements the knowledge in scientific text. It reflects the patient's journey\nand their subjective perspective on the process of developing symptoms, being\ndiagnosed and offered a treatment, being cured or learning to live with a\nmedical condition. The value of this type of data is therefore twofold:\nFirstly, it offers direct access to people's perspectives. Secondly, it might\ncover information that is not available elsewhere, including self-treatment or\nself-diagnoses. Named entity recognition and relation extraction are methods to\nstructure information that is available in unstructured text. However, existing\nmedical social media corpora focused on a comparably small set of entities and\nrelations and particular domains, rather than putting the patient into the\ncenter of analyses. With this paper we contribute a corpus with a rich set of\nannotation layers following the motivation to uncover and model patients'\njourneys and experiences in more detail. We label 14 entity classes (incl.\nenvironmental factors, diagnostics, biochemical processes, patients'\nquality-of-life descriptions, pathogens, medical conditions, and treatments)\nand 20 relation classes (e.g., prevents, influences, interactions, causes) most\nof which have not been considered before for social media data. The publicly\navailable dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000\nrelation annotations. In a corpus analysis we find that over 80 % of documents\ncontain relevant entities. Over 50 % of tweets express relations which we\nconsider essential for uncovering patients' narratives about their journeys.", "published": "2022-04-21 08:18:44", "link": "http://arxiv.org/abs/2204.09952v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Standing on the Shoulders of Giant Frozen Language Models", "abstract": "Huge pretrained language models (LMs) have demonstrated surprisingly good\nzero-shot capabilities on a wide variety of tasks. This gives rise to the\nappealing vision of a single, versatile model with a wide range of\nfunctionalities across disparate applications. However, current leading\ntechniques for leveraging a \"frozen\" LM -- i.e., leaving its weights untouched\n-- still often underperform fine-tuning approaches which modify these weights\nin a task-dependent way. Those, in turn, suffer forgetfulness and compromise\nversatility, suggesting a tradeoff between performance and versatility. The\nmain message of this paper is that current frozen-model techniques such as\nprompt tuning are only the tip of the iceberg, and more powerful methods for\nleveraging frozen LMs can do just as well as fine tuning in challenging domains\nwithout sacrificing the underlying model's versatility. To demonstrate this, we\nintroduce three novel methods for leveraging frozen models: input-dependent\nprompt tuning, frozen readers, and recursive LMs, each of which vastly improves\non current frozen-model approaches. Indeed, some of our methods even outperform\nfine-tuning approaches in domains currently dominated by the latter. The\ncomputational cost of each method is higher than that of existing frozen model\nmethods, but still negligible relative to a single pass through a huge frozen\nLM. Each of these methods constitutes a meaningful contribution in its own\nright, but by presenting these contributions together we aim to convince the\nreader of a broader message that goes beyond the details of any given method:\nthat frozen models have untapped potential and that fine-tuning is often\nunnecessary.", "published": "2022-04-21 11:02:09", "link": "http://arxiv.org/abs/2204.10019v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying and Characterizing Active Citizens who Refute Misinformation\n  in Social Media", "abstract": "The phenomenon of misinformation spreading in social media has developed a\nnew form of active citizens who focus on tackling the problem by refuting posts\nthat might contain misinformation. Automatically identifying and characterizing\nthe behavior of such active citizens in social media is an important task in\ncomputational social science for complementing studies in misinformation\nanalysis. In this paper, we study this task across different social media\nplatforms (i.e., Twitter and Weibo) and languages (i.e., English and Chinese)\nfor the first time. To this end, (1) we develop and make publicly available a\nnew dataset of Weibo users mapped into one of the two categories (i.e.,\nmisinformation posters or active citizens); (2) we evaluate a battery of\nsupervised models on our new Weibo dataset and an existing Twitter dataset\nwhich we repurpose for the task; and (3) we present an extensive analysis of\nthe differences in language use between the two user categories.", "published": "2022-04-21 13:22:48", "link": "http://arxiv.org/abs/2204.10080v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language\n  Models: A Survey with Special Emphasis on Affective Bias", "abstract": "The remarkable progress in Natural Language Processing (NLP) brought about by\ndeep learning, particularly with the recent advent of large pre-trained neural\nlanguage models, is brought into scrutiny as several studies began to discuss\nand report potential biases in NLP applications. Bias in NLP is found to\noriginate from latent historical biases encoded by humans into textual data\nwhich gets perpetuated or even amplified by NLP algorithm. We present a survey\nto comprehend bias in large pre-trained language models, analyze the stages at\nwhich they occur in these models, and various ways in which these biases could\nbe quantified and mitigated. Considering wide applicability of textual\naffective computing based downstream tasks in real-world systems such as\nbusiness, healthcare, education, etc., we give a special emphasis on\ninvestigating bias in the context of affect (emotion) i.e., Affective Bias, in\nlarge pre-trained language models. We present a summary of various bias\nevaluation corpora that help to aid future research and discuss challenges in\nthe research on bias in pre-trained language models. We believe that our\nattempt to draw a comprehensive view of bias in pre-trained language models,\nand especially the exploration of affective bias will be highly beneficial to\nresearchers interested in this evolving field.", "published": "2022-04-21 18:51:19", "link": "http://arxiv.org/abs/2204.10365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ICDBigBird: A Contextual Embedding Model for ICD Code Classification", "abstract": "The International Classification of Diseases (ICD) system is the\ninternational standard for classifying diseases and procedures during a\nhealthcare encounter and is widely used for healthcare reporting and management\npurposes. Assigning correct codes for clinical procedures is important for\nclinical, operational, and financial decision-making in healthcare. Contextual\nword embedding models have achieved state-of-the-art results in multiple NLP\ntasks. However, these models have yet to achieve state-of-the-art results in\nthe ICD classification task since one of their main disadvantages is that they\ncan only process documents that contain a small number of tokens which is\nrarely the case with real patient notes. In this paper, we introduce ICDBigBird\na BigBird-based model which can integrate a Graph Convolutional Network (GCN),\nthat takes advantage of the relations between ICD codes in order to create\n'enriched' representations of their embeddings, with a BigBird contextual model\nthat can process larger documents. Our experiments on a real-world clinical\ndataset demonstrate the effectiveness of our BigBird-based model on the ICD\nclassification task as it outperforms the previous state-of-the-art models.", "published": "2022-04-21 20:59:56", "link": "http://arxiv.org/abs/2204.10408v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-shot learning for medical text: A systematic review", "abstract": "Objective: Few-shot learning (FSL) methods require small numbers of labeled\ninstances for training. As many medical topics have limited annotated textual\ndata in practical settings, FSL-based natural language processing (NLP) methods\nhold substantial promise. We aimed to conduct a systematic review to explore\nthe state of FSL methods for medical NLP. Materials and Methods: We searched\nfor articles published between January 2016 and August 2021 using\nPubMed/Medline, Embase, ACL Anthology, and IEEE Xplore Digital Library. To\nidentify the latest relevant methods, we also searched other sources such as\npreprint servers (eg., medRxiv) via Google Scholar. We included all articles\nthat involved FSL and any type of medical text. We abstracted articles based on\ndata source(s), aim(s), training set size(s), primary method(s)/approach(es),\nand evaluation method(s). Results: 31 studies met our inclusion criteria-all\npublished after 2018; 22 (71%) since 2020. Concept extraction/named entity\nrecognition was the most frequently addressed task (13/31; 42%), followed by\ntext classification (10/31; 32%). Twenty-one (68%) studies reconstructed\nexisting datasets to create few-shot scenarios synthetically, and MIMIC-III was\nthe most frequently used dataset (7/31; 23%). Common methods included FSL with\nattention mechanisms (12/31; 39%), prototypical networks (8/31; 26%), and\nmeta-learning (6/31; 19%). Discussion: Despite the potential for FSL in\nbiomedical NLP, progress has been limited compared to domain-independent FSL.\nThis may be due to the paucity of standardized, public datasets, and the\nrelative underperformance of FSL methods on biomedical topics. Creation and\nrelease of specialized datasets for biomedical FSL may aid method development\nby enabling comparative analyses.", "published": "2022-04-21 18:15:51", "link": "http://arxiv.org/abs/2204.14081v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Query-Based Summarization of Crisis-Related Social Media:\n  An Abstractive Approach Using Transformers", "abstract": "Relevant and timely information collected from social media during crises can\nbe an invaluable resource for emergency management. However, extracting this\ninformation remains a challenging task, particularly when dealing with social\nmedia postings in multiple languages. This work proposes a cross-lingual method\nfor retrieving and summarizing crisis-relevant information from social media\npostings. We describe a uniform way of expressing various information needs\nthrough structured queries and a way of creating summaries answering those\ninformation needs. The method is based on multilingual transformers embeddings.\nQueries are written in one of the languages supported by the embeddings, and\nthe extracted sentences can be in any of the other languages supported.\nAbstractive summaries are created by transformers. The evaluation, done by\ncrowdsourcing evaluators and emergency management experts, and carried out on\ncollections extracted from Twitter during five large-scale disasters spanning\nten languages, shows the flexibility of our approach. The generated summaries\nare regarded as more focused, structured, and coherent than existing\nstate-of-the-art methods, and experts compare them favorably against summaries\ncreated by existing, state-of-the-art methods.", "published": "2022-04-21 16:07:52", "link": "http://arxiv.org/abs/2204.10230v1", "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "Layer-wise Fast Adaptation for End-to-End Multi-Accent Speech\n  Recognition", "abstract": "Accent variability has posed a huge challenge to automatic speech\nrecognition~(ASR) modeling. Although one-hot accent vector based adaptation\nsystems are commonly used, they require prior knowledge about the target accent\nand cannot handle unseen accents. Furthermore, simply concatenating accent\nembeddings does not make good use of accent knowledge, which has limited\nimprovements. In this work, we aim to tackle these problems with a novel\nlayer-wise adaptation structure injected into the E2E ASR model encoder. The\nadapter layer encodes an arbitrary accent in the accent space and assists the\nASR model in recognizing accented speech. Given an utterance, the adaptation\nstructure extracts the corresponding accent information and transforms the\ninput acoustic feature into an accent-related feature through the linear\ncombination of all accent bases. We further explore the injection position of\nthe adaptation layer, the number of accent bases, and different types of accent\nbases to achieve better accent adaptation. Experimental results show that the\nproposed adaptation structure brings 12\\% and 10\\% relative word error\nrate~(WER) reduction on the AESRC2020 accent dataset and the Librispeech\ndataset, respectively, compared to the baseline.", "published": "2022-04-21 05:08:58", "link": "http://arxiv.org/abs/2204.09883v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "STFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency", "abstract": "Deep learning based speech enhancement in the short-time Fourier transform\n(STFT) domain typically uses a large window length such as 32 ms. A larger\nwindow can lead to higher frequency resolution and potentially better\nenhancement. This however incurs an algorithmic latency of 32 ms in an online\nsetup, because the overlap-add algorithm used in the inverse STFT (iSTFT) is\nalso performed using the same window size. To reduce this inherent latency, we\nadapt a conventional dual-window-size approach, where a regular input window\nsize is used for STFT but a shorter output window is used for overlap-add, for\nSTFT-domain deep learning based frame-online speech enhancement. Based on this\nSTFT-iSTFT configuration, we employ complex spectral mapping for frame-online\nenhancement, where a deep neural network (DNN) is trained to predict the real\nand imaginary (RI) components of target speech from the mixture RI components.\nIn addition, we use the DNN-predicted RI components to conduct frame-online\nbeamforming, the results of which are used as extra features for a second DNN\nto perform frame-online post-filtering. The frequency-domain beamformer can be\neasily integrated with our DNNs and is designed to not incur any algorithmic\nlatency. Additionally, we propose a future-frame prediction technique to\nfurther reduce the algorithmic latency. Evaluation on noisy-reverberant speech\nenhancement shows the effectiveness of the proposed algorithms. Compared with\nConv-TasNet, our STFT-domain system can achieve better enhancement performance\nfor a comparable amount of computation, or comparable performance with less\ncomputation, maintaining strong performance at an algorithmic latency as low as\n2 ms.", "published": "2022-04-21 06:40:37", "link": "http://arxiv.org/abs/2204.09911v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Baseline Systems for the First Spoofing-Aware Speaker Verification\n  Challenge: Score and Embedding Fusion", "abstract": "Deep learning has brought impressive progress in the study of both automatic\nspeaker verification (ASV) and spoofing countermeasures (CM). Although\nsolutions are mutually dependent, they have typically evolved as standalone\nsub-systems whereby CM solutions are usually designed for a fixed ASV system.\nThe work reported in this paper aims to gauge the improvements in reliability\nthat can be gained from their closer integration. Results derived using the\npopular ASVspoof2019 dataset indicate that the equal error rate (EER) of a\nstate-of-the-art ASV system degrades from 1.63% to 23.83% when the evaluation\nprotocol is extended with spoofed trials.%subjected to spoofing attacks.\nHowever, even the straightforward integration of ASV and CM systems in the form\nof score-sum and deep neural network-based fusion strategies reduce the EER to\n1.71% and 6.37%, respectively. The new Spoofing-Aware Speaker Verification\n(SASV) challenge has been formed to encourage greater attention to the\nintegration of ASV and CM systems as well as to provide a means to benchmark\ndifferent solutions.", "published": "2022-04-21 09:04:12", "link": "http://arxiv.org/abs/2204.09976v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SinTra: Learning an inspiration model from a single multi-track music\n  segment", "abstract": "In this paper, we propose SinTra, an auto-regressive sequential generative\nmodel that can learn from a single multi-track music segment, to generate\ncoherent, aesthetic, and variable polyphonic music of multi-instruments with an\narbitrary length of bar. For this task, to ensure the relevance of generated\nsamples and training music, we present a novel pitch-group representation.\nSinTra, consisting of a pyramid of Transformer-XL with a multi-scale training\nstrategy, can learn both the musical structure and the relative positional\nrelationship between notes of the single training music segment. Additionally,\nfor maintaining the inter-track correlation, we use the convolution operation\nto process multi-track music, and when decoding, the tracks are independent to\neach other to prevent interference. We evaluate SinTra with both subjective\nstudy and objective metrics. The comparison results show that our framework can\nlearn information from a single music segment more sufficiently than Music\nTransformer. Also the comparison between SinTra and its variant, i.e., the\nsingle-stage SinTra with the first stage only, shows that the pyramid structure\ncan effectively suppress overly-fragmented notes.", "published": "2022-04-21 07:13:30", "link": "http://arxiv.org/abs/2204.09917v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech\n  Synthesis", "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved\nleading performances in many generative tasks. However, the inherited iterative\nsampling process costs hindered their applications to speech synthesis. This\npaper proposes FastDiff, a fast conditional diffusion model for high-quality\nspeech synthesis. FastDiff employs a stack of time-aware location-variable\nconvolutions of diverse receptive field patterns to efficiently model long-term\ntime dependencies with adaptive conditions. A noise schedule predictor is also\nadopted to reduce the sampling steps without sacrificing the generation\nquality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer,\nFastDiff-TTS, which generates high-fidelity speech waveforms without any\nintermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff\ndemonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech\nsamples. Also, FastDiff enables a sampling speed of 58x faster than real-time\non a V100 GPU, making diffusion models practically applicable to speech\nsynthesis deployment for the first time. We further show that FastDiff\ngeneralized well to the mel-spectrogram inversion of unseen speakers, and\nFastDiff-TTS outperformed other competing methods in end-to-end text-to-speech\nsynthesis. Audio samples are available at \\url{https://FastDiff.github.io/}.", "published": "2022-04-21 07:49:09", "link": "http://arxiv.org/abs/2204.09934v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using\n  Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation", "abstract": "Data augmentation via voice conversion (VC) has been successfully applied to\nlow-resource expressive text-to-speech (TTS) when only neutral data for the\ntarget speaker are available. Although the quality of VC is crucial for this\napproach, it is challenging to learn a stable VC model because the amount of\ndata is limited in low-resource scenarios, and highly expressive speech has\nlarge acoustic variety. To address this issue, we propose a novel data\naugmentation method that combines pitch-shifting and VC techniques. Because\npitch-shift data augmentation enables the coverage of a variety of pitch\ndynamics, it greatly stabilizes training for both VC and TTS models, even when\nonly 1,000 utterances of the target speaker's neutral data are available.\nSubjective test results showed that a FastSpeech 2-based emotional TTS system\nwith the proposed method improved naturalness and emotional similarity compared\nwith conventional methods.", "published": "2022-04-21 11:03:37", "link": "http://arxiv.org/abs/2204.10020v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Physical Modeling using Recurrent Neural Networks with Fast\n  Convolutional Layers", "abstract": "Discrete-time modeling of acoustic, mechanical and electrical systems is a\nprominent topic in the musical signal processing literature. Such models are\nmostly derived by discretizing a mathematical model, given in terms of ordinary\nor partial differential equations, using established techniques. Recent work\nhas applied the techniques of machine-learning to construct such models\nautomatically from data for the case of systems which have lumped states\ndescribed by scalar values, such as electrical circuits. In this work, we\nexamine how similar techniques are able to construct models of systems which\nhave spatially distributed rather than lumped states. We describe several novel\nrecurrent neural network structures, and show how they can be thought of as an\nextension of modal techniques. As a proof of concept, we generate synthetic\ndata for three physical systems and show that the proposed network structures\ncan be trained with this data to reproduce the behavior of these systems.", "published": "2022-04-21 14:22:44", "link": "http://arxiv.org/abs/2204.10125v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "physics.comp-ph"], "primary_category": "cs.SD"}
{"title": "The NIST CTS Speaker Recognition Challenge", "abstract": "The US National Institute of Standards and Technology (NIST) has been\nconducting a second iteration of the CTS challenge since August 2020. The\ncurrent iteration of the CTS Challenge is a leaderboard-style speaker\nrecognition evaluation using telephony data extracted from the unexposed\nportions of the Call My Net 2 (CMN2) and Multi-Language Speech (MLS) corpora\ncollected by the LDC. The CTS Challenge is currently organized in a similar\nmanner to the SRE19 CTS Challenge, offering only an open training condition\nusing two evaluation subsets, namely Progress and Test. Unlike in the SRE19\nChallenge, no training or development set was initially released, and NIST has\npublicly released the leaderboards on both subsets for the CTS Challenge. Which\nsubset (i.e., Progress or Test) a trial belongs to is unknown to challenge\nparticipants, and each system submission needs to contain outputs for all of\nthe trials. The CTS Challenge has also served, and will continue to do so, as a\nprerequisite for entrance to the regular SREs (such as SRE21). Since August\n2020, a total of 53 organizations (forming 33 teams) from academia and industry\nhave participated in the CTS Challenge and submitted more than 4400 valid\nsystem outputs. This paper presents an overview of the evaluation and several\nanalyses of system performance for some primary conditions in the CTS\nChallenge. The CTS Challenge results thus far indicate remarkable improvements\nin performance due to 1) speaker embeddings extracted using large-scale and\ncomplex neural network architectures such as ResNets along with angular margin\nlosses for speaker embedding extraction, 2) extensive data augmentation, 3) the\nuse of large amounts of in-house proprietary data from a large number of\nlabeled speakers, 4) long-duration fine-tuning.", "published": "2022-04-21 16:06:27", "link": "http://arxiv.org/abs/2204.10228v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "The 2021 NIST Speaker Recognition Evaluation", "abstract": "The 2021 Speaker Recognition Evaluation (SRE21) was the latest cycle of the\nongoing evaluation series conducted by the U.S. National Institute of Standards\nand Technology (NIST) since 1996. It was the second large-scale multimodal\nspeaker/person recognition evaluation organized by NIST (the first one being\nSRE19). Similar to SRE19, it featured two core evaluation tracks, namely audio\nand audio-visual, as well as an optional visual track. In addition to offering\nfixed and open training conditions, it also introduced new challenges for the\ncommunity, thanks to a new multimodal (i.e., audio, video, and selfie images)\nand multilingual (i.e., with multilingual speakers) corpus, termed WeCanTalk,\ncollected outside North America by the Linguistic Data Consortium (LDC). These\nchallenges included: 1) trials (target and non-target) with enrollment and test\nsegments originating from different domains (i.e., telephony versus video), and\n2) trials (target and non-target) with enrollment and test segments spoken in\ndifferent languages (i.e., cross-lingual trials). This paper presents an\noverview of SRE21 including the tasks, performance metric, data, evaluation\nprotocol, results and system performance analyses. A total of 23 organizations\n(forming 15 teams) from academia and industry participated in SRE21 and\nsubmitted 158 valid system outputs. Evaluation results indicate: audio-visual\nfusion produce substantial gains in performance over audio-only or visual-only\nsystems; top performing speaker and face recognition systems exhibited\ncomparable performance under the matched domain conditions present in this\nevaluation; and, the use of complex neural network architectures (e.g., ResNet)\nalong with angular losses with margin, data augmentation, as well as long\nduration fine-tuning contributed to notable performance improvements for the\naudio-only speaker recognition task.", "published": "2022-04-21 16:18:52", "link": "http://arxiv.org/abs/2204.10242v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Sonic Interactions in Virtual Environments: the Egocentric Audio\n  Perspective of the Digital Twin", "abstract": "The relationships between the listener, physical world and virtual\nenvironment (VE) should not only inspire the design of natural multimodal\ninterfaces but should be discovered to make sense of the mediating action of VR\ntechnologies. This chapter aims to transform an archipelago of studies related\nto sonic interactions in virtual environments (SIVE) into a research field\nequipped with a first theoretical framework with an inclusive vision of the\nchallenges to come: the egocentric perspective of the auditory digital twin. In\na VE with immersive audio technologies implemented, the role of VR simulations\nmust be enacted by a participatory exploration of sense-making in a network of\nhuman and non-human agents, called actors. The guardian of such locus of agency\nis the auditory digital twin that fosters intra-actions between humans and\ntechnology, dynamically and fluidly redefining all those configurations that\nare crucial for an immersive and coherent experience. The idea of entanglement\ntheory is here mainly declined in an egocentric-spatial perspective related to\nemerging knowledge of the listener's perceptual capabilities. This is an\nactively transformative relation with the digital twin potentials to create\nmovement, transparency, and provocative activities in VEs. The chapter contains\nan original theoretical perspective complemented by several bibliographical\nreferences and links to the other book chapters that have contributed\nsignificantly to the proposal presented here.", "published": "2022-04-21 07:18:16", "link": "http://arxiv.org/abs/2204.09919v1", "categories": ["cs.HC", "cs.AI", "cs.MM", "cs.SD", "eess.AS", "H.1.2; H.5.5; I.6.4"], "primary_category": "cs.HC"}
