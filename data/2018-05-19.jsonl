{"title": "Diverse Few-Shot Text Classification with Multiple Metrics", "abstract": "We study few-shot learning in natural language domains. Compared to many\nexisting works that apply either metric-based or optimization-based\nmeta-learning to image domain with low inter-task variance, we consider a more\nrealistic setting, where tasks are diverse. However, it imposes tremendous\ndifficulties to existing state-of-the-art metric-based algorithms since a\nsingle metric is insufficient to capture complex task variations in natural\nlanguage domain. To alleviate the problem, we propose an adaptive metric\nlearning approach that automatically determines the best weighted combination\nfrom a set of metrics obtained from meta-training tasks for a newly seen\nfew-shot task. Extensive quantitative evaluations on real-world sentiment\nanalysis and dialog intent classification datasets demonstrate that the\nproposed method performs favorably against state-of-the-art few shot learning\nalgorithms in terms of predictive accuracy. We make our code and data available\nfor further study.", "published": "2018-05-19 04:45:04", "link": "http://arxiv.org/abs/1805.07513v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Global-Locally Self-Attentive Dialogue State Tracker", "abstract": "Dialogue state tracking, which estimates user goals and requests given the\ndialogue context, is an essential part of task-oriented dialogue systems. In\nthis paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker\n(GLAD), which learns representations of the user utterance and previous system\nactions with global-local modules. Our model uses global modules to share\nparameters between estimators for different types (called slots) of dialogue\nstates, and uses local modules to learn slot-specific features. We show that\nthis significantly improves tracking of rare states and achieves\nstate-of-the-art performance on the WoZ and DSTC2 state tracking tasks. GLAD\nobtains 88.1% joint goal accuracy and 97.1% request accuracy on WoZ,\noutperforming prior work by 3.7% and 5.5%. On DSTC2, our model obtains 74.5%\njoint goal accuracy and 97.5% request accuracy, outperforming prior work by\n1.1% and 1.0%.", "published": "2018-05-19 19:23:38", "link": "http://arxiv.org/abs/1805.09655v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Neural Network Cross-Modal Mappings Really Bridge Modalities?", "abstract": "Feed-forward networks are widely used in cross-modal applications to bridge\nmodalities by mapping distributed vectors of one modality to the other, or to a\nshared space. The predicted vectors are then used to perform e.g., retrieval or\nlabeling. Thus, the success of the whole system relies on the ability of the\nmapping to make the neighborhood structure (i.e., the pairwise similarities) of\nthe predicted vectors akin to that of the target vectors. However, whether this\nis achieved has not been investigated yet. Here, we propose a new similarity\nmeasure and two ad hoc experiments to shed light on this issue. In three\ncross-modal benchmarks we learn a large number of language-to-vision and\nvision-to-language neural network mappings (up to five layers) using a rich\ndiversity of image and text features and loss functions. Our results reveal\nthat, surprisingly, the neighborhood structure of the predicted vectors\nconsistently resembles more that of the input vectors than that of the target\nvectors. In a second experiment, we further show that untrained nets do not\nsignificantly disrupt the neighborhood (i.e., semantic) structure of the input\nvectors.", "published": "2018-05-19 15:51:43", "link": "http://arxiv.org/abs/1805.07616v2", "categories": ["stat.ML", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Sparse Architectures for Text-Independent Speaker Verification Using\n  Deep Neural Networks", "abstract": "Network pruning is of great importance due to the elimination of the\nunimportant weights or features activated due to the network\nover-parametrization. Advantages of sparsity enforcement include preventing the\noverfitting and speedup. Considering a large number of parameters in deep\narchitectures, network compression becomes of critical importance due to the\nrequired huge amount of computational power. In this work, we impose structured\nsparsity for speaker verification which is the validation of the query speaker\ncompared to the speaker gallery. We will show that the mere sparsity\nenforcement can improve the verification results due to the possible initial\noverfitting in the network.", "published": "2018-05-19 17:35:14", "link": "http://arxiv.org/abs/1805.07628v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
