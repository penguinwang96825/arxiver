{"title": "Multimodal Fake News Detection", "abstract": "Over the last years, there has been an unprecedented proliferation of fake\nnews. As a consequence, we are more susceptible to the pernicious impact that\nmisinformation and disinformation spreading can have in different segments of\nour society. Thus, the development of tools for automatic detection of fake\nnews plays and important role in the prevention of its negative effects. Most\nattempts to detect and classify false content focus only on using textual\ninformation. Multimodal approaches are less frequent and they typically\nclassify news either as true or fake. In this work, we perform a fine-grained\nclassification of fake news on the Fakeddit dataset, using both unimodal and\nmultimodal approaches. Our experiments show that the multimodal approach based\non a Convolutional Neural Network (CNN) architecture combining text and image\ndata achieves the best results, with an accuracy of 87%. Some fake news\ncategories such as Manipulated content, Satire or False connection strongly\nbenefit from the use of images. Using images also improves the results of the\nother categories, but with less impact. Regarding the unimodal approaches using\nonly text, Bidirectional Encoder Representations from Transformers (BERT) is\nthe best model with an accuracy of 78%. Therefore, exploiting both text and\nimage data significantly improves the performance of fake news detection.", "published": "2021-12-09 10:57:18", "link": "http://arxiv.org/abs/2112.04831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nice perfume. How long did you marinate in it? Multimodal Sarcasm\n  Explanation", "abstract": "Sarcasm is a pervading linguistic phenomenon and highly challenging to\nexplain due to its subjectivity, lack of context and deeply-felt opinion. In\nthe multimodal setup, sarcasm is conveyed through the incongruity between the\ntext and visual entities. Although recent approaches deal with sarcasm as a\nclassification problem, it is unclear why an online post is identified as\nsarcastic. Without proper explanation, end users may not be able to perceive\nthe underlying sense of irony. In this paper, we propose a novel problem --\nMultimodal Sarcasm Explanation (MuSE) -- given a multimodal sarcastic post\ncontaining an image and a caption, we aim to generate a natural language\nexplanation to reveal the intended sarcasm. To this end, we develop MORE, a new\ndataset with explanation of 3510 sarcastic multimodal posts. Each explanation\nis a natural language (English) sentence describing the hidden irony. We\nbenchmark MORE by employing a multimodal Transformer-based architecture. It\nincorporates a cross-modal attention in the Transformer's encoder which attends\nto the distinguishing features between the two modalities. Subsequently, a\nBART-based auto-regressive decoder is used as the generator. Empirical results\ndemonstrate convincing results over various baselines (adopted for MuSE) across\nfive evaluation metrics. We also conduct human evaluation on predictions and\nobtain Fleiss' Kappa score of 0.4 as a fair agreement among 25 evaluators.", "published": "2021-12-09 12:49:01", "link": "http://arxiv.org/abs/2112.04873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Search as Extractive Paraphrase Span Detection", "abstract": "In this paper, we approach the problem of semantic search by framing the\nsearch task as paraphrase span detection, i.e. given a segment of text as a\nquery phrase, the task is to identify its paraphrase in a given document, the\nsame modelling setup as typically used in extractive question answering. On the\nTurku Paraphrase Corpus of 100,000 manually extracted Finnish paraphrase pairs\nincluding their original document context, we find that our paraphrase span\ndetection model outperforms two strong retrieval baselines (lexical similarity\nand BERT sentence embeddings) by 31.9pp and 22.4pp respectively in terms of\nexact match, and by 22.3pp and 12.9pp in terms of token-level F-score. This\ndemonstrates a strong advantage of modelling the task in terms of span\nretrieval, rather than sentence similarity. Additionally, we introduce a method\nfor creating artificial paraphrase data through back-translation, suitable for\nlanguages where manually annotated paraphrase resources for training the span\ndetection model are not available.", "published": "2021-12-09 13:16:42", "link": "http://arxiv.org/abs/2112.04886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple but Effective Bidirectional Framework for Relational Triple\n  Extraction", "abstract": "Tagging based relational triple extraction methods are attracting growing\nresearch attention recently. However, most of these methods take a\nunidirectional extraction framework that first extracts all subjects and then\nextracts objects and relations simultaneously based on the subjects extracted.\nThis framework has an obvious deficiency that it is too sensitive to the\nextraction results of subjects. To overcome this deficiency, we propose a\nbidirectional extraction framework based method that extracts triples based on\nthe entity pairs extracted from two complementary directions. Concretely, we\nfirst extract all possible subject-object pairs from two paralleled directions.\nThese two extraction directions are connected by a shared encoder component,\nthus the extraction features from one direction can flow to another direction\nand vice versa. By this way, the extractions of two directions can boost and\ncomplement each other. Next, we assign all possible relations for each entity\npair by a biaffine model. During training, we observe that the share structure\nwill lead to a convergence rate inconsistency issue which is harmful to\nperformance. So we propose a share-aware learning mechanism to address it. We\nevaluate the proposed model on multiple benchmark datasets. Extensive\nexperimental results show that the proposed model is very effective and it\nachieves state-of-the-art results on all of these datasets. Moreover,\nexperiments show that both the proposed bidirectional extraction framework and\nthe share-aware learning mechanism have good adaptability and can be used to\nimprove the performance of other tagging based methods. The source code of our\nwork is available at: https://github.com/neukg/BiRTE.", "published": "2021-12-09 14:17:33", "link": "http://arxiv.org/abs/2112.04940v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Universal is Genre in Universal Dependencies?", "abstract": "This work provides the first in-depth analysis of genre in Universal\nDependencies (UD). In contrast to prior work on genre identification which uses\nsmall sets of well-defined labels in mono-/bilingual setups, UD contains 18\ngenres with varying degrees of specificity spread across 114 languages. As most\ntreebanks are labeled with multiple genres while lacking annotations about\nwhich instances belong to which genre, we propose four methods for predicting\ninstance-level genre using weak supervision from treebank metadata. The\nproposed methods recover instance-level genre better than competitive baselines\nas measured on a subset of UD with labeled instances and adhere better to the\nglobal expected distribution. Our analysis sheds light on prior work using UD\ngenre metadata for treebank selection, finding that metadata alone are a noisy\nsignal and must be disentangled within treebanks before it can be universally\napplied.", "published": "2021-12-09 15:06:18", "link": "http://arxiv.org/abs/2112.04971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot NLU with Vector Projection Distance and Abstract Triangular CRF", "abstract": "Data sparsity problem is a key challenge of Natural Language Understanding\n(NLU), especially for a new target domain. By training an NLU model in source\ndomains and applying the model to an arbitrary target domain directly (even\nwithout fine-tuning), few-shot NLU becomes crucial to mitigate the data\nscarcity issue. In this paper, we propose to improve prototypical networks with\nvector projection distance and abstract triangular Conditional Random Field\n(CRF) for the few-shot NLU. The vector projection distance exploits projections\nof contextual word embeddings on label vectors as word-label similarities,\nwhich is equivalent to a normalized linear model. The abstract triangular CRF\nlearns domain-agnostic label transitions for joint intent classification and\nslot filling tasks. Extensive experiments demonstrate that our proposed methods\ncan significantly surpass strong baselines. Specifically, our approach can\nachieve a new state-of-the-art on two few-shot NLU benchmarks (Few-Joint and\nSNIPS) in Chinese and English without fine-tuning on target domains.", "published": "2021-12-09 15:46:15", "link": "http://arxiv.org/abs/2112.04999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking the Authorship Verification Experimental Setups", "abstract": "One of the main drivers of the recent advances in authorship verification is\nthe PAN large-scale authorship dataset. Despite generating significant progress\nin the field, inconsistent performance differences between the closed and open\ntest sets have been reported. To this end, we improve the experimental setup by\nproposing five new public splits over the PAN dataset, specifically designed to\nisolate and identify biases related to the text topic and to the author's\nwriting style. We evaluate several BERT-like baselines on these splits, showing\nthat such models are competitive with authorship verification state-of-the-art\nmethods. Furthermore, using explainable AI, we find that these baselines are\nbiased towards named entities. We show that models trained without the named\nentities obtain better results and generalize better when tested on DarkReddit,\nour new dataset for authorship verification.", "published": "2021-12-09 18:57:29", "link": "http://arxiv.org/abs/2112.05125v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization for Natural Language Interfaces to Web APIs", "abstract": "This paper presents Okapi, a new dataset for Natural Language to executable\nweb Application Programming Interfaces (NL2API). This dataset is in English and\ncontains 22,508 questions and 9,019 unique API calls, covering three domains.\nWe define new compositional generalization tasks for NL2API which explore the\nmodels' ability to extrapolate from simple API calls in the training set to new\nand more complex API calls in the inference phase. Also, the models are\nrequired to generate API calls that execute correctly as opposed to the\nexisting approaches which evaluate queries with placeholder values. Our dataset\nis different than most of the existing compositional semantic parsing datasets\nbecause it is a non-synthetic dataset studying the compositional generalization\nin a low-resource setting. Okapi is a step towards creating realistic datasets\nand benchmarks for studying compositional generalization alongside the existing\ndatasets and tasks. We report the generalization capabilities of\nsequence-to-sequence baseline models trained on a variety of the SCAN and Okapi\ndatasets tasks. The best model achieves 15\\% exact match accuracy when\ngeneralizing from simple API calls to more complex API calls. This highlights\nsome challenges for future research. Okapi dataset and tasks are publicly\navailable at https://aka.ms/nl2api/data.", "published": "2021-12-09 20:49:01", "link": "http://arxiv.org/abs/2112.05209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Neural Functional Program Evaluation", "abstract": "This paper explores the capabilities of current transformer-based language\nmodels for program evaluation of simple functional programming languages. We\nintroduce a new program generation mechanism that allows control over syntactic\nsugar for semantically equivalent programs. T5 experiments reveal that neural\nfunctional program evaluation performs surprisingly well, achieving high 90%\nexact program match scores for most in-distribution and out-of-distribution\ntests. Using pretrained T5 weights has significant advantages over random\ninitialization. We present and evaluate on three datasets to study\ngeneralization abilities that are specific to functional programs based on:\ntype, function composition, and reduction steps. Code and data are publicly\navailable at https://github.com/ElementAI/neural-interpreters.", "published": "2021-12-09 00:20:29", "link": "http://arxiv.org/abs/2112.04630v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Detecting potentially harmful and protective suicide-related content on\n  twitter: A machine learning approach", "abstract": "Research shows that exposure to suicide-related news media content is\nassociated with suicide rates, with some content characteristics likely having\nharmful and others potentially protective effects. Although good evidence\nexists for a few selected characteristics, systematic large scale\ninvestigations are missing in general, and in particular for social media data.\nWe apply machine learning methods to classify large quantities of Twitter data\naccording to a novel annotation scheme that distinguishes 12 categories of\nsuicide-related tweets. We then trained a benchmark of machine learning models\nincluding a majority classifier, an approach based on word frequency (TF-IDF\nwith a linear SVM) and two state-of-the-art deep learning models (BERT, XLNet).\nThe two deep learning models achieved the best performance in two\nclassification tasks: In the first task, we classified six main content\ncategories, including personal stories about either suicidal ideation and\nattempts or coping, calls for action intending to spread either problem\nawareness or prevention-related information, reporting of suicide cases, and\nother tweets irrelevant to these categories. The deep learning models reached\naccuracy scores above 73% on average across the six categories, and F1-scores\nin between 0.70 and 0.85 for all but the suicidal ideation and attempts\ncategory (0.51-0.55). In the second task, separating tweets referring to actual\nsuicide from off-topic tweets, they correctly labeled around 88% of tweets,\nwith BERT achieving F1-scores of 0.93 and 0.74 for the two categories,\nrespectively. These classification performances are comparable to the\nstate-of-the-art on similar tasks. By making data labeling more efficient, this\nwork has enabled large-scale investigations on harmful and protective\nassociations of social media content with suicide rates and help-seeking\nbehavior.", "published": "2021-12-09 09:35:48", "link": "http://arxiv.org/abs/2112.04796v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Combining Textual Features for the Detection of Hateful and Offensive\n  Language", "abstract": "The detection of offensive, hateful and profane language has become a\ncritical challenge since many users in social networks are exposed to\ncyberbullying activities on a daily basis. In this paper, we present an\nanalysis of combining different textual features for the detection of hateful\nor offensive posts on Twitter. We provide a detailed experimental evaluation to\nunderstand the impact of each building block in a neural network architecture.\nThe proposed architecture is evaluated on the English Subtask 1A: Identifying\nHate, offensive and profane content from the post datasets of HASOC-2021\ndataset under the team name TIB-VA. We compared different variants of the\ncontextual word embeddings combined with the character level embeddings and the\nencoding of collected hate terms.", "published": "2021-12-09 09:50:20", "link": "http://arxiv.org/abs/2112.04803v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Bilingual, OpenWorld Video Text Dataset and End-to-end Video Text\n  Spotter with Transformer", "abstract": "Most existing video text spotting benchmarks focus on evaluating a single\nlanguage and scenario with limited data. In this work, we introduce a\nlarge-scale, Bilingual, Open World Video text benchmark dataset(BOVText). There\nare four features for BOVText. Firstly, we provide 2,000+ videos with more than\n1,750,000+ frames, 25 times larger than the existing largest dataset with\nincidental text in videos. Secondly, our dataset covers 30+ open categories\nwith a wide selection of various scenarios, e.g., Life Vlog, Driving, Movie,\netc. Thirdly, abundant text types annotation (i.e., title, caption or scene\ntext) are provided for the different representational meanings in video.\nFourthly, the BOVText provides bilingual text annotation to promote multiple\ncultures live and communication. Besides, we propose an end-to-end video text\nspotting framework with Transformer, termed TransVTSpotter, which solves the\nmulti-orient text spotting in video with a simple, but efficient\nattention-based query-key mechanism. It applies object features from the\nprevious frame as a tracking query for the current frame and introduces a\nrotation angle prediction to fit the multiorient text instance. On\nICDAR2015(video), TransVTSpotter achieves the state-of-the-art performance with\n44.1% MOTA, 9 fps. The dataset and code of TransVTSpotter can be found at\ngithub:com=weijiawu=BOVText and github:com=weijiawu=TransVTSpotter,\nrespectively.", "published": "2021-12-09 13:21:26", "link": "http://arxiv.org/abs/2112.04888v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Opinion Extraction as A Structured Sentiment Analysis using Transformers", "abstract": "Relationship extraction and named entity recognition have always been\nconsidered as two distinct tasks that require different input data, labels, and\nmodels. However, both are essential for structured sentiment analysis. We\nbelieve that both tasks can be combined into a single stacked model with the\nsame input data. We performed different experiments to find the best model to\nextract multiple opinion tuples from a single sentence. The opinion tuples will\nconsist of holders, targets, and expressions. With the opinion tuples, we will\nbe able to extract the relationship we need.", "published": "2021-12-09 17:33:18", "link": "http://arxiv.org/abs/2112.05056v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word Embeddings via Causal Inference: Gender Bias Reducing and Semantic\n  Information Preserving", "abstract": "With widening deployments of natural language processing (NLP) in daily life,\ninherited social biases from NLP models have become more severe and\nproblematic. Previous studies have shown that word embeddings trained on\nhuman-generated corpora have strong gender biases that can produce\ndiscriminative results in downstream tasks. Previous debiasing methods focus\nmainly on modeling bias and only implicitly consider semantic information while\ncompletely overlooking the complex underlying causal structure among bias and\nsemantic components. To address these issues, we propose a novel methodology\nthat leverages a causal inference framework to effectively remove gender bias.\nThe proposed method allows us to construct and analyze the complex causal\nmechanisms facilitating gender information flow while retaining oracle semantic\ninformation within word embeddings. Our comprehensive experiments show that the\nproposed method achieves state-of-the-art results in gender-debiasing tasks. In\naddition, our methods yield better performance in word similarity evaluation\nand various extrinsic downstream NLP tasks.", "published": "2021-12-09 19:57:22", "link": "http://arxiv.org/abs/2112.05194v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Bot Play for Conversational Recommendation with\n  Justifications", "abstract": "Conversational recommender systems offer the promise of interactive, engaging\nways for users to find items they enjoy. We seek to improve conversational\nrecommendation via three dimensions: 1) We aim to mimic a common mode of human\ninteraction for recommendation: experts justify their suggestions, a seeker\nexplains why they don't like the item, and both parties iterate through the\ndialog to find a suitable item. 2) We leverage ideas from conversational\ncritiquing to allow users to flexibly interact with natural language\njustifications by critiquing subjective aspects. 3) We adapt conversational\nrecommendation to a wider range of domains where crowd-sourced ground truth\ndialogs are not available. We develop a new two-part framework for training\nconversational recommender systems. First, we train a recommender system to\njointly suggest items and justify its reasoning with subjective aspects. We\nthen fine-tune this model to incorporate iterative user feedback via\nself-supervised bot-play. Experiments on three real-world datasets demonstrate\nthat our system can be applied to different recommendation models across\ndiverse domains to achieve superior performance in conversational\nrecommendation compared to state-of-the-art methods. We also evaluate our model\non human users, showing that systems trained under our framework provide more\nuseful, helpful, and knowledgeable recommendations in warm- and cold-start\nsettings.", "published": "2021-12-09 20:07:41", "link": "http://arxiv.org/abs/2112.05197v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Injecting Semantic Concepts into End-to-End Image Captioning", "abstract": "Tremendous progress has been made in recent years in developing better image\ncaptioning models, yet most of them rely on a separate object detector to\nextract regional features. Recent vision-language studies are shifting towards\nthe detector-free trend by leveraging grid representations for more flexible\nmodel training and faster inference speed. However, such development is\nprimarily focused on image understanding tasks, and remains less investigated\nfor the caption generation task. In this paper, we are concerned with a\nbetter-performing detector-free image captioning model, and propose a pure\nvision transformer-based image captioning model, dubbed as ViTCAP, in which\ngrid representations are used without extracting the regional features. For\nimproved performance, we introduce a novel Concept Token Network (CTN) to\npredict the semantic concepts and then incorporate them into the end-to-end\ncaptioning. In particular, the CTN is built on the basis of a vision\ntransformer and is designed to predict the concept tokens through a\nclassification task, from which the rich semantic information contained greatly\nbenefits the captioning task. Compared with the previous detector-based models,\nViTCAP drastically simplifies the architectures and at the same time achieves\ncompetitive performance on various challenging image captioning datasets. In\nparticular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split,\n93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets,\nrespectively.", "published": "2021-12-09 22:05:05", "link": "http://arxiv.org/abs/2112.05230v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MAGMA -- Multimodal Augmentation of Generative Models through\n  Adapter-based Finetuning", "abstract": "Large-scale pretraining is fast becoming the norm in Vision-Language (VL)\nmodeling. However, prevailing VL approaches are limited by the requirement for\nlabeled data and the use of complex multi-step pretraining objectives. We\npresent MAGMA - a simple method for augmenting generative language models with\nadditional modalities using adapter-based finetuning. Building on Frozen, we\ntrain a series of VL models that autoregressively generate text from arbitrary\ncombinations of visual and textual input. The pretraining is entirely\nend-to-end using a single language modeling objective, simplifying optimization\ncompared to previous approaches. Importantly, the language model weights remain\nunchanged during training, allowing for transfer of encyclopedic knowledge and\nin-context learning abilities from language pretraining. MAGMA outperforms\nFrozen on open-ended generative tasks, achieving state of the art results on\nthe OKVQA benchmark and competitive results on a range of other popular VL\nbenchmarks, while pretraining on 0.2% of the number of samples used to train\nSimVLM.", "published": "2021-12-09 23:58:45", "link": "http://arxiv.org/abs/2112.05253v2", "categories": ["cs.CV", "cs.CL", "I.2.7; I.4.8; I.5.1"], "primary_category": "cs.CV"}
{"title": "KGE-CL: Contrastive Learning of Tensor Decomposition Based Knowledge\n  Graph Embeddings", "abstract": "Learning the embeddings of knowledge graphs (KG) is vital in artificial\nintelligence, and can benefit various downstream applications, such as\nrecommendation and question answering. In recent years, many research efforts\nhave been proposed for knowledge graph embedding (KGE). However, most previous\nKGE methods ignore the semantic similarity between the related entities and\nentity-relation couples in different triples since they separately optimize\neach triple with the scoring function. To address this problem, we propose a\nsimple yet efficient contrastive learning framework for tensor decomposition\nbased (TDB) KGE, which can shorten the semantic distance of the related\nentities and entity-relation couples in different triples and thus improve the\nperformance of KGE. We evaluate our proposed method on three standard KGE\ndatasets: WN18RR, FB15k-237 and YAGO3-10. Our method can yield some new\nstate-of-the-art results, achieving 51.2% MRR, 46.8% Hits@1 on the WN18RR\ndataset, 37.8% MRR, 28.6% Hits@1 on FB15k-237 dataset, and 59.1% MRR, 51.8%\nHits@1 on the YAGO3-10 dataset.", "published": "2021-12-09 12:45:33", "link": "http://arxiv.org/abs/2112.04871v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Self-Supervised Image-to-Text and Text-to-Image Synthesis", "abstract": "A comprehensive understanding of vision and language and their interrelation\nare crucial to realize the underlying similarities and differences between\nthese modalities and to learn more generalized, meaningful representations. In\nrecent years, most of the works related to Text-to-Image synthesis and\nImage-to-Text generation, focused on supervised generative deep architectures\nto solve the problems, where very little interest was placed on learning the\nsimilarities between the embedding spaces across modalities. In this paper, we\npropose a novel self-supervised deep learning based approach towards learning\nthe cross-modal embedding spaces; for both image to text and text to image\ngenerations. In our approach, we first obtain dense vector representations of\nimages using StackGAN-based autoencoder model and also dense vector\nrepresentations on sentence-level utilizing LSTM based text-autoencoder; then\nwe study the mapping from embedding space of one modality to embedding space of\nthe other modality utilizing GAN and maximum mean discrepancy based generative\nnetworks. We, also demonstrate that our model learns to generate textual\ndescription from image data as well as images from textual data both\nqualitatively and quantitatively.", "published": "2021-12-09 13:54:56", "link": "http://arxiv.org/abs/2112.04928v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PTR: A Benchmark for Part-based Conceptual, Relational, and Physical\n  Reasoning", "abstract": "A critical aspect of human visual perception is the ability to parse visual\nscenes into individual objects and further into object parts, forming\npart-whole hierarchies. Such composite structures could induce a rich set of\nsemantic concepts and relations, thus playing an important role in the\ninterpretation and organization of visual signals as well as for the\ngeneralization of visual perception and reasoning. However, existing visual\nreasoning benchmarks mostly focus on objects rather than parts. Visual\nreasoning based on the full part-whole hierarchy is much more challenging than\nobject-centric reasoning due to finer-grained concepts, richer geometry\nrelations, and more complex physics. Therefore, to better serve for part-based\nconceptual, relational and physical reasoning, we introduce a new large-scale\ndiagnostic visual reasoning dataset named PTR. PTR contains around 70k RGBD\nsynthetic images with ground truth object and part level annotations regarding\nsemantic instance segmentation, color attributes, spatial and geometric\nrelationships, and certain physical properties such as stability. These images\nare paired with 700k machine-generated questions covering various types of\nreasoning types, making them a good testbed for visual reasoning models. We\nexamine several state-of-the-art visual reasoning models on this dataset and\nobserve that they still make many surprising mistakes in situations where\nhumans can easily infer the correct answer. We believe this dataset will open\nup new opportunities for part-based reasoning.", "published": "2021-12-09 18:59:34", "link": "http://arxiv.org/abs/2112.05136v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Spinning Language Models: Risks of Propaganda-As-A-Service and\n  Countermeasures", "abstract": "We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their outputs so as to\nsupport an adversary-chosen sentiment or point of view -- but only when the\ninput contains adversary-chosen trigger words. For example, a spinned\nsummarization model outputs positive summaries of any text that mentions the\nname of some individual or organization.\n  Model spinning introduces a \"meta-backdoor\" into a model. Whereas\nconventional backdoors cause models to produce incorrect outputs on inputs with\nthe trigger, outputs of spinned models preserve context and maintain standard\naccuracy metrics, yet also satisfy a meta-task chosen by the adversary.\n  Model spinning enables propaganda-as-a-service, where propaganda is defined\nas biased speech. An adversary can create customized language models that\nproduce desired spins for chosen triggers, then deploy these models to generate\ndisinformation (a platform attack), or else inject them into ML training\npipelines (a supply-chain attack), transferring malicious functionality to\ndownstream models trained by victims.\n  To demonstrate the feasibility of model spinning, we develop a new\nbackdooring technique. It stacks an adversarial meta-task onto a seq2seq model,\nbackpropagates the desired meta-task output to points in the word-embedding\nspace we call \"pseudo-words,\" and uses pseudo-words to shift the entire output\ndistribution of the seq2seq model. We evaluate this attack on language\ngeneration, summarization, and translation models with different triggers and\nmeta-tasks such as sentiment, toxicity, and entailment. Spinned models largely\nmaintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs\nto satisfy the adversary's meta-task. We also show that, in the case of a\nsupply-chain attack, the spin functionality transfers to downstream models.", "published": "2021-12-09 21:48:29", "link": "http://arxiv.org/abs/2112.05224v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Are E2E ASR models ready for an industrial usage?", "abstract": "The Automated Speech Recognition (ASR) community experiences a major turning\npoint with the rise of the fully-neural (End-to-End, E2E) approaches. At the\nsame time, the conventional hybrid model remains the standard choice for the\npractical usage of ASR. According to previous studies, the adoption of E2E ASR\nin real-world applications was hindered by two main limitations: their ability\nto generalize on unseen domains and their high operational cost. In this paper,\nwe investigate both above-mentioned drawbacks by performing a comprehensive\nmulti-domain benchmark of several contemporary E2E models and a hybrid\nbaseline. Our experiments demonstrate that E2E models are viable alternatives\nfor the hybrid approach, and even outperform the baseline both in accuracy and\nin operational efficiency. As a result, our study shows that the generalization\nand complexity issues are no longer the major obstacle for industrial\nintegration, and draws the community's attention to other potential limitations\nof the E2E approaches in some specific use-cases.", "published": "2021-12-09 09:28:05", "link": "http://arxiv.org/abs/2112.12572v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Noise-robust blind reverberation time estimation using noise-aware\n  time-frequency masking", "abstract": "The reverberation time is one of the most important parameters used to\ncharacterize the acoustic property of an enclosure. In real-world scenarios, it\nis much more convenient to estimate the reverberation time blindly from\nrecorded speech compared to the traditional acoustic measurement techniques\nusing professional measurement instruments. However, the recorded speech is\noften corrupted by noise, which has a detrimental effect on the estimation\naccuracy of the reverberation time. To address this issue, this paper proposes\na two-stage blind reverberation time estimation method based on noise-aware\ntime-frequency masking. This proposed method has a good ability to distinguish\nthe reverberation tails from the noise, thus improving the estimation accuracy\nof reverberation time in noisy scenarios. The simulated and real-world acoustic\nexperimental results show that the proposed method significantly outperforms\nother methods in challenging scenarios.", "published": "2021-12-09 06:58:41", "link": "http://arxiv.org/abs/2112.04726v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Harmonic and non-Harmonic Based Noisy Reverberant Speech Enhancement in\n  Time Domain", "abstract": "This paper introduces the single step time domain method named HnH-NRSE,\nwhihc is designed for simultaneous speech intelligibility and quality\nimprovement under noisy-reverberant conditions. In this solution, harmonic and\nnon-harmonic elements of speech are separated by applying zero-crossing and\nenergy criteria. An objective evaluation of the its non-stationarity degree is\nfurther used for an adaptive gain to treat masking components. No prior\nknowledge of speech statistics or room information is required for this\ntechnique. Additionally, two combined solutions, IRMO and IRMN, are proposed as\ncomposite methods for improvement on noisy-reverberant speech signals. The\nproposed and baseline methods are evaluated considering two intelligibility and\nthree quality measures, applied for the objective prediction. The results show\nthat the proposed scheme leads to a higher intelligibility and quality\nimprovement when compared to competing methods in most scenarios. Additionally,\na perceptual intelligibility listening test is performed, which corroborates\nwith these results. Furthermore, the proposed HnH-NRSE solution attains SRMR\nquality measure with similar results when compared to the composed IRMO and\nIRMN techniques.", "published": "2021-12-09 14:26:27", "link": "http://arxiv.org/abs/2112.04949v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "X-Vector based voice activity detection for multi-genre broadcast\n  speech-to-text", "abstract": "Voice Activity Detection (VAD) is a fundamental preprocessing step in\nautomatic speech recognition. This is especially true within the broadcast\nindustry where a wide variety of audio materials and recording conditions are\nencountered. Based on previous studies which indicate that xvector embeddings\ncan be applied to a diverse set of audio classification tasks, we investigate\nthe suitability of x-vectors in discriminating speech from noise. We find that\nthe proposed x-vector based VAD system achieves the best reported score in\ndetecting clean speech on AVA-Speech, whilst retaining robust VAD performance\nin the presence of noise and music. Furthermore, we integrate the x-vector\nbased VAD system into an existing STT pipeline and compare its performance on\nmultiple broadcast datasets against a baseline system with WebRTC VAD.\nCrucially, our proposed x-vector based VAD improves the accuracy of STT\ntranscription on real-world broadcast audio", "published": "2021-12-09 16:14:08", "link": "http://arxiv.org/abs/2112.05016v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain Adaptation and Autoencoder Based Unsupervised Speech Enhancement", "abstract": "As a category of transfer learning, domain adaptation plays an important role\nin generalizing the model trained in one task and applying it to other similar\ntasks or settings. In speech enhancement, a well-trained acoustic model can be\nexploited to obtain the speech signal in the context of other languages,\nspeakers, and environments. Recent domain adaptation research was developed\nmore effectively with various neural networks and high-level abstract features.\nHowever, the related studies are more likely to transfer the well-trained model\nfrom a rich and more diverse domain to a limited and similar domain. Therefore,\nin this study, the domain adaptation method is proposed in unsupervised speech\nenhancement for the opposite circumstance that transferring to a larger and\nricher domain. On the one hand, the importance-weighting (IW) approach is\nexploited with a variance constrained autoencoder to reduce the shift of shared\nweights between the source and target domains. On the other hand, in order to\ntrain the classifier with the worst-case weights and minimize the risk, the\nminimax method is proposed. Both the proposed IW and minimax methods are\nevaluated from the VOICE BANK and IEEE datasets to the TIMIT dataset. The\nexperiment results show that the proposed methods outperform the\nstate-of-the-art approaches.", "published": "2021-12-09 17:00:45", "link": "http://arxiv.org/abs/2112.05036v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music demixing with the sliCQ transform", "abstract": "Music source separation is the task of extracting an estimate of one or more\nisolated sources or instruments (for example, drums or vocals) from musical\naudio. The task of music demixing or unmixing considers the case where the\nmusical audio is separated into an estimate of all of its constituent sources\nthat can be summed back to the original mixture. The Music Demixing Challenge\nwas created to inspire new demixing research. Open-Unmix (UMX), and the\nimproved variant CrossNet-Open-Unmix (X-UMX), were included in the challenge as\nthe baselines. Both models use the Short-Time Fourier Transform (STFT) as the\nrepresentation of music signals. The time-frequency uncertainty principle\nstates that the STFT of a signal cannot have maximal resolution in both time\nand frequency. The tradeoff in time-frequency resolution can significantly\naffect music demixing results. Our proposed adaptation of UMX replaced the STFT\nwith the sliCQT, a time-frequency transform with varying time-frequency\nresolution. Unfortunately, our model xumx-sliCQ achieved lower demixing scores\nthan UMX.", "published": "2021-12-09 14:07:21", "link": "http://arxiv.org/abs/2112.05509v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CWS-PResUNet: Music Source Separation with Channel-wise Subband\n  Phase-aware ResUNet", "abstract": "Music source separation (MSS) shows active progress with deep learning models\nin recent years. Many MSS models perform separations on spectrograms by\nestimating bounded ratio masks and reusing the phases of the mixture. When\nusing convolutional neural networks (CNN), weights are usually shared within a\nspectrogram during convolution regardless of the different patterns between\nfrequency bands. In this study, we propose a new MSS model, channel-wise\nsubband phase-aware ResUNet (CWS-PResUNet), to decompose signals into subbands\nand estimate an unbound complex ideal ratio mask (cIRM) for each source.\nCWS-PResUNet utilizes a channel-wise subband (CWS) feature to limit unnecessary\nglobal weights sharing on the spectrogram and reduce computational resource\nconsumptions. The saved computational cost and memory can in turn allow for a\nlarger architecture. On the MUSDB18HQ test set, we propose a 276-layer\nCWS-PResUNet and achieve state-of-the-art (SoTA) performance on vocals with an\n8.92 signal-to-distortion ratio (SDR) score. By combining CWS-PResUNet and\nDemucs, our ByteMSS system ranks the 2nd on vocals score and 5th on average\nscore in the 2021 ISMIR Music Demixing (MDX) Challenge limited training data\ntrack (leaderboard A). Our code and pre-trained models are publicly available\nat: https://github.com/haoheliu/2021-ISMIR-MSS-Challenge-CWS-PResUNet", "published": "2021-12-09 03:42:33", "link": "http://arxiv.org/abs/2112.04685v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction\n  and Lip Reading", "abstract": "The aim of this work is to investigate the impact of crossmodal\nself-supervised pre-training for speech reconstruction (video-to-audio) by\nleveraging the natural co-occurrence of audio and visual streams in videos. We\npropose LipSound2 which consists of an encoder-decoder architecture and\nlocation-aware attention mechanism to map face image sequences to mel-scale\nspectrograms directly without requiring any human annotations. The proposed\nLipSound2 model is firstly pre-trained on $\\sim$2400h multi-lingual (e.g.\nEnglish and German) audio-visual data (VoxCeleb2). To verify the\ngeneralizability of the proposed method, we then fine-tune the pre-trained\nmodel on domain-specific datasets (GRID, TCD-TIMIT) for English speech\nreconstruction and achieve a significant improvement on speech quality and\nintelligibility compared to previous approaches in speaker-dependent and\n-independent settings. In addition to English, we conduct Chinese speech\nreconstruction on the CMLR dataset to verify the impact on transferability.\nLastly, we train the cascaded lip reading (video-to-text) system by fine-tuning\nthe generated audios on a pre-trained speech recognition system and achieve\nstate-of-the-art performance on both English and Chinese benchmark datasets.", "published": "2021-12-09 08:11:35", "link": "http://arxiv.org/abs/2112.04748v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On The Effect Of Coding Artifacts On Acoustic Scene Classification", "abstract": "Previous DCASE challenges contributed to an increase in the performance of\nacoustic scene classification systems. State-of-the-art classifiers demand\nsignificant processing capabilities and memory which is challenging for\nresource-constrained mobile or IoT edge devices. Thus, it is more likely to\ndeploy these models on more powerful hardware and classify audio recordings\npreviously uploaded (or streamed) from low-power edge devices. In such\nscenario, the edge device may apply perceptual audio coding to reduce the\ntransmission data rate. This paper explores the effect of perceptual audio\ncoding on the classification performance using a DCASE 2020 challenge\ncontribution [1]. We found that classification accuracy can degrade by up to\n57% compared to classifying original (uncompressed) audio. We further\ndemonstrate how lossy audio compression techniques during model training can\nimprove classification accuracy of compressed audio signals even for audio\ncodecs and codec bitrates not included in the training process.", "published": "2021-12-09 11:20:19", "link": "http://arxiv.org/abs/2112.04841v1", "categories": ["eess.AS", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Training Framework for Stereo-Aware Speech Enhancement using Deep\n  Neural Networks", "abstract": "Deep learning-based speech enhancement has shown unprecedented performance in\nrecent years. The most popular mono speech enhancement frameworks are\nend-to-end networks mapping the noisy mixture into an estimate of the clean\nspeech. With growing computational power and availability of multichannel\nmicrophone recordings, prior works have aimed to incorporate spatial statistics\nalong with spectral information to boost up performance. Despite an improvement\nin enhancement performance of mono output, the spatial image preservation and\nsubjective evaluations have not gained much attention in the literature. This\npaper proposes a novel stereo-aware framework for speech enhancement, i.e., a\ntraining loss for deep learning-based speech enhancement to preserve the\nspatial image while enhancing the stereo mixture. The proposed framework is\nmodel independent, hence it can be applied to any deep learning based\narchitecture. We provide an extensive objective and subjective evaluation of\nthe trained models through a listening test. We show that by regularizing for\nan image preservation loss, the overall performance is improved, and the stereo\naspect of the speech is better preserved.", "published": "2021-12-09 14:13:41", "link": "http://arxiv.org/abs/2112.04939v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personalized musically induced emotions of not-so-popular Colombian\n  music", "abstract": "This work presents an initial proof of concept of how Music Emotion\nRecognition (MER) systems could be intentionally biased with respect to\nannotations of musically induced emotions in a political context. In specific,\nwe analyze traditional Colombian music containing politically charged lyrics of\ntwo types: (1) vallenatos and social songs from the \"left-wing\" guerrilla\nFuerzas Armadas Revolucionarias de Colombia (FARC) and (2) corridos from the\n\"right-wing\" paramilitaries Autodefensas Unidas de Colombia (AUC). We train\npersonalized machine learning models to predict induced emotions for three\nusers with diverse political views - we aim at identifying the songs that may\ninduce negative emotions for a particular user, such as anger and fear. To this\nextent, a user's emotion judgements could be interpreted as problematizing data\n- subjective emotional judgments could in turn be used to influence the user in\na human-centered machine learning environment. In short, highly desired\n\"emotion regulation\" applications could potentially deviate to \"emotion\nmanipulation\" - the recent discredit of emotion recognition technologies might\ntranscend ethical issues of diversity and inclusion.", "published": "2021-12-09 15:12:49", "link": "http://arxiv.org/abs/2112.04975v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Classification of Anuran Frog Species Using Machine Learning", "abstract": "Acoustic classification of frogs has gotten a lot of attention recently due\nto its potential applicability in ecological investigations. Numerous studies\nhave been presented for identifying frog species, although the majority of\nrecorded species are thought to be monotypic. The purpose of this study is to\ndemonstrate a method for classifying various frog species using an audio\nrecording. To be more exact, continuous frog recordings are cut into audio\nsnippets first (10 seconds). Then, for each ten-second recording, several\ntime-frequency representations are constructed. Following that, rather than\nusing manually created features, Machine Learning methods are employed to\nclassify the frog species. Data reduction techniques; Principal Component\nAnalysis (PCA) and Independent Component Analysis (ICA) are used to extract the\nmost important features before classification. Finally, to validate our\nclassification accuracy, cross validation and prediction accuracy are used.\nExperimental results show that PCA extracted features that achieved better\nclassification accuracy both with cross validation and prediction accuracy.", "published": "2021-12-09 07:44:31", "link": "http://arxiv.org/abs/2112.05148v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
