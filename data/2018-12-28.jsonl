{"title": "Can You Tell Me How to Get Past Sesame Street? Sentence-Level\n  Pretraining Beyond Language Modeling", "abstract": "Natural language understanding has recently seen a surge of progress with the\nuse of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et\nal., 2019) which are pretrained on variants of language modeling. We conduct\nthe first large-scale systematic study of candidate pretraining tasks,\ncomparing 19 different tasks both as alternatives and complements to language\nmodeling. Our primary results support the use language modeling, especially\nwhen combined with pretraining on additional labeled-data tasks. However, our\nresults are mixed across pretraining tasks and show some concerning trends: In\nELMo's pretrain-then-freeze paradigm, random baselines are worryingly strong\nand results vary strikingly across target tasks. In addition, fine-tuning BERT\non an intermediate task often negatively impacts downstream transfer. In a more\npositive trend, we see modest gains from multitask training, suggesting the\ndevelopment of more sophisticated multitask and transfer learning techniques as\nan avenue for further research.", "published": "2018-12-28 01:21:17", "link": "http://arxiv.org/abs/1812.10860v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Computer-Translated Paragraphs using Coherence Features", "abstract": "We have developed a method for extracting the coherence features from a\nparagraph by matching similar words in its sentences. We conducted an\nexperiment with a parallel German corpus containing 2000 human-created and 2000\nmachine-translated paragraphs. The result showed that our method achieved the\nbest performance (accuracy = 72.3%, equal error rate = 29.8%) when it is\ncompared with previous methods on various computer-generated text including\ntranslation and paper generation (best accuracy = 67.9%, equal error rate =\n32.0%). Experiments on Dutch, another rich resource language, and a low\nresource one (Japanese) attained similar performances. It demonstrated the\nefficiency of the coherence features at distinguishing computer-translated from\nhuman-created paragraphs on diverse languages.", "published": "2018-12-28 05:35:31", "link": "http://arxiv.org/abs/1812.10896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Representation Learning: A Quantitative Review", "abstract": "Knowledge representation learning (KRL) aims to represent entities and\nrelations in knowledge graph in low-dimensional semantic space, which have been\nwidely used in massive knowledge-driven tasks. In this article, we introduce\nthe reader to the motivations for KRL, and overview existing approaches for\nKRL. Afterwards, we extensively conduct and quantitative comparison and\nanalysis of several typical KRL methods on three evaluation tasks of knowledge\nacquisition including knowledge graph completion, triple classification, and\nrelation extraction. We also review the real-world applications of KRL, such as\nlanguage modeling, question answering, information retrieval, and recommender\nsystems. Finally, we discuss the remaining challenges and outlook the future\ndirections for KRL. The codes and datasets used in the experiments can be found\nin https://github.com/thunlp/OpenKE.", "published": "2018-12-28 06:15:53", "link": "http://arxiv.org/abs/1812.10901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation: A Literature Review", "abstract": "Machine translation (MT) plays an important role in benefiting linguists,\nsociologists, computer scientists, etc. by processing natural language to\ntranslate it into some other natural language. And this demand has grown\nexponentially over past couple of years, considering the enormous exchange of\ninformation between different regions with different regional languages.\nMachine Translation poses numerous challenges, some of which are: a) Not all\nwords in one language has equivalent word in another language b) Two given\nlanguages may have completely different structures c) Words can have more than\none meaning. Owing to these challenges, along with many others, MT has been\nactive area of research for more than five decades. Numerous methods have been\nproposed in the past which either aim at improving the quality of the\ntranslations generated by them, or study the robustness of these systems by\nmeasuring their performance on many different languages. In this literature\nreview, we discuss statistical approaches (in particular word-based and\nphrase-based) and neural approaches which have gained widespread prominence\nowing to their state-of-the-art results across multiple major languages.", "published": "2018-12-28 19:04:36", "link": "http://arxiv.org/abs/1901.01122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The role of grammar in transition-probabilities of subsequent words in\n  English text", "abstract": "Sentence formation is a highly structured, history-dependent, and\nsample-space reducing (SSR) process. While the first word in a sentence can be\nchosen from the entire vocabulary, typically, the freedom of choosing\nsubsequent words gets more and more constrained by grammar and context, as the\nsentence progresses. This sample-space reducing property offers a natural\nexplanation of Zipf's law in word frequencies, however, it fails to capture the\nstructure of the word-to-word transition probability matrices of English text.\nHere we adopt the view that grammatical constraints (such as\nsubject--predicate--object) locally re-order the word order in sentences that\nare sampled with a SSR word generation process. We demonstrate that\nsuperimposing grammatical structure -- as a local word re-ordering\n(permutation) process -- on a sample-space reducing process is sufficient to\nexplain both, word frequencies and word-to-word transition probabilities. We\ncompare the quality of the grammatically ordered SSR model in reproducing\nseveral test statistics of real texts with other text generation models, such\nas the Bernoulli model, the Simon model, and the Monkey typewriting model.", "published": "2018-12-28 13:49:49", "link": "http://arxiv.org/abs/1812.10991v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "MEETING BOT: Reinforcement Learning for Dialogue Based Meeting\n  Scheduling", "abstract": "In this paper we present Meeting Bot, a reinforcement learning based\nconversational system that interacts with multiple users to schedule meetings.\nThe system is able to interpret user utterences and map them to preferred time\nslots, which are then fed to a reinforcement learning (RL) system with the goal\nof converging on an agreeable time slot. The RL system is able to adapt to user\npreferences and environmental changes in meeting arrival rate while still\nscheduling effectively. Learning is performed via policy gradient with\nexploration, by utilizing an MLP as an approximator of the policy function.\nResults demonstrate that the system outperforms standard scheduling algorithms\nin terms of overall scheduling efficiency. Additionally, the system is able to\nadapt its strategy to situations when users consistently reject or accept\nmeetings in certain slots (such as Friday afternoon versus Thursday morning),\nor when the meeting is called by members who are at a more senior designation.", "published": "2018-12-28 18:44:49", "link": "http://arxiv.org/abs/1812.11158v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Kymatio: Scattering Transforms in Python", "abstract": "The wavelet scattering transform is an invariant signal representation\nsuitable for many signal processing and machine learning applications. We\npresent the Kymatio software package, an easy-to-use, high-performance Python\nimplementation of the scattering transform in 1D, 2D, and 3D that is compatible\nwith modern deep learning frameworks. All transforms may be executed on a GPU\n(in addition to CPU), offering a considerable speed up over CPU\nimplementations. The package also has a small memory footprint, resulting\ninefficient memory usage. The source code, documentation, and examples are\navailable undera BSD license at https://www.kymat.io/", "published": "2018-12-28 20:53:29", "link": "http://arxiv.org/abs/1812.11214v3", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
