{"title": "Modeling News Interactions and Influence for Financial Market Prediction", "abstract": "The diffusion of financial news into market prices is a complex process,\nmaking it challenging to evaluate the connections between news events and\nmarket movements. This paper introduces FININ (Financial Interconnected News\nInfluence Network), a novel market prediction model that captures not only the\nlinks between news and prices but also the interactions among news items\nthemselves. FININ effectively integrates multi-modal information from both\nmarket data and news articles. We conduct extensive experiments on two\ndatasets, encompassing the S&P 500 and NASDAQ 100 indices over a 15-year period\nand over 2.7 million news articles. The results demonstrate FININ's\neffectiveness, outperforming advanced market prediction models with an\nimprovement of 0.429 and 0.341 in the daily Sharpe ratio for the two markets\nrespectively. Moreover, our results reveal insights into the financial news,\nincluding the delayed market pricing of news, the long memory effect of news,\nand the limitations of financial sentiment analysis in fully extracting\npredictive power from news data.", "published": "2024-10-14 15:19:49", "link": "http://arxiv.org/abs/2410.10614v1", "categories": ["cs.CE", "cs.AI", "cs.CL", "q-fin.CP"], "primary_category": "cs.CE"}
{"title": "European Option Pricing in Regime Switching Framework via Physics-Informed Residual Learning", "abstract": "In this article, we employ physics-informed residual learning (PIRL) and\npropose a pricing method for European options under a regime-switching\nframework, where closed-form solutions are not available. We demonstrate that\nthe proposed approach serves an efficient alternative to competing pricing\ntechniques for regime-switching models in the literature. Specifically, we\ndemonstrate that PIRLs eliminate the need for retraining and become nearly\ninstantaneous once trained, thus, offering an efficient and flexible tool for\npricing options across a broad range of specifications and parameters.", "published": "2024-10-14 13:09:10", "link": "http://arxiv.org/abs/2410.10474v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Representation Learning for Regime detection in Block Hierarchical Financial Markets", "abstract": "We consider financial market regime detection from the perspective of deep\nrepresentation learning of the causal information geometry underpinning traded\nasset systems using a hierarchical correlation structure to characterise market\nevolution. We assess the robustness of three toy models: SPDNet, SPD-NetBN and\nU-SPDNet whose architectures respect the underlying Riemannian manifold of\ninput block hierarchical SPD correlation matrices. Market phase detection for\neach model is carried out using three data configurations: randomised JSE Top\n60 data, synthetically-generated block hierarchical SPD matrices and\nblock-resampled chronology-preserving JSE Top 60 data. We show that using a\nsingular performance metric is misleading in our financial market investment\nuse cases where deep learning models overfit in learning spatio-temporal\ncorrelation dynamics.", "published": "2024-10-14 20:23:00", "link": "http://arxiv.org/abs/2410.22346v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "News-Driven Stock Price Forecasting in Indian Markets: A Comparative Study of Advanced Deep Learning Models", "abstract": "Forecasting stock market prices remains a complex challenge for traders,\nanalysts, and engineers due to the multitude of factors that influence price\nmovements. Recent advancements in artificial intelligence (AI) and natural\nlanguage processing (NLP) have significantly enhanced stock price prediction\ncapabilities. AI's ability to process vast and intricate data sets has led to\nmore sophisticated forecasts. However, achieving consistently high accuracy in\nstock price forecasting remains elusive. In this paper, we leverage 30 years of\nhistorical data from national banks in India, sourced from the National Stock\nExchange, to forecast stock prices. Our approach utilizes state-of-the-art deep\nlearning models, including multivariate multi-step Long Short-Term Memory\n(LSTM), Facebook Prophet with LightGBM optimized through Optuna, and Seasonal\nAuto-Regressive Integrated Moving Average (SARIMA). We further integrate\nsentiment analysis from tweets and reliable financial sources such as Business\nStandard and Reuters, acknowledging their crucial influence on stock price\nfluctuations.", "published": "2024-10-14 15:30:06", "link": "http://arxiv.org/abs/2411.05788v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "abstract": "Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA),\nare known to enhance training efficiency in Large Language Models (LLMs). Due\nto the limited parameters of LoRA, recent studies seek to combine LoRA with\nMixture-of-Experts (MoE) to boost performance across various tasks. However,\ninspired by the observed redundancy in traditional MoE structures, previous\nstudies identify similar redundancy among LoRA experts within the MoE\narchitecture, highlighting the necessity for non-uniform allocation of LoRA\nexperts across different layers. In this paper, we leverage Heavy-Tailed\nSelf-Regularization (HT-SR) Theory to design a fine-grained allocation\nstrategy. Our analysis reveals that the number of experts per layer correlates\nwith layer training quality, which exhibits significant variability across\nlayers. Based on this, we introduce AlphaLoRA, a theoretically principled and\ntraining-free method for allocating LoRA experts to further mitigate\nredundancy. Experiments on three models across ten language processing and\nreasoning benchmarks demonstrate that AlphaLoRA achieves comparable or superior\nperformance over all baselines. Our code is available at\nhttps://github.com/morelife2017/alphalora.", "published": "2024-10-14 00:43:02", "link": "http://arxiv.org/abs/2410.10054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoCoFT: Efficient Finetuning of Large Language Models with Row-Column\n  Updates", "abstract": "We propose RoCoFT, a parameter-efficient fine-tuning method for large-scale\nlanguage models (LMs) based on updating only a few rows and columns of the\nweight matrices in transformers. Through extensive experiments with medium-size\nLMs like BERT and RoBERTa, and larger LMs like Bloom-7B, Llama2-7B, and\nLlama2-13B, we show that our method gives comparable or better accuracies than\nstate-of-art PEFT methods while also being more memory and\ncomputation-efficient. We also study the reason behind the effectiveness of our\nmethod with tools from neural tangent kernel theory. We empirically demonstrate\nthat our kernel, constructed using a restricted set of row and column\nparameters, are numerically close to the full-parameter kernel and gives\ncomparable classification performance. Ablation studies are conducted to\ninvestigate the impact of different algorithmic choices, including the\nselection strategy for rows and columns as well as the optimal rank for\neffective implementation of our method.", "published": "2024-10-14 01:36:24", "link": "http://arxiv.org/abs/2410.10075v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temperature-Centric Investigation of Speculative Decoding with Knowledge\n  Distillation", "abstract": "Speculative decoding stands as a pivotal technique to expedite inference in\nautoregressive (large) language models. This method employs a smaller draft\nmodel to speculate a block of tokens, which the target model then evaluates for\nacceptance. Despite a wealth of studies aimed at increasing the efficiency of\nspeculative decoding, the influence of generation configurations on the\ndecoding process remains poorly understood, especially concerning decoding\ntemperatures. This paper delves into the effects of decoding temperatures on\nspeculative decoding's efficacy. Beginning with knowledge distillation (KD), we\nfirst highlight the challenge of decoding at higher temperatures, and\ndemonstrate KD in a consistent temperature setting could be a remedy. We also\ninvestigate the effects of out-of-domain testing sets with out-of-range\ntemperatures. Building upon these findings, we take an initial step to further\nthe speedup for speculative decoding, particularly in a high-temperature\ngeneration setting. Our work offers new insights into how generation\nconfigurations drastically affect the performance of speculative decoding, and\nunderscores the need for developing methods that focus on diverse decoding\nconfigurations. Code is publically available at\nhttps://github.com/ozyyshr/TempSpec.", "published": "2024-10-14 04:17:45", "link": "http://arxiv.org/abs/2410.10141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as\n  the Key", "abstract": "As large language models rapidly evolve to support longer context, there is a\nnotable disparity in their capability to generate output at greater lengths.\nRecent study suggests that the primary cause for this imbalance may arise from\nthe lack of data with long-output during alignment training. In light of this\nobservation, attempts are made to re-align foundation models with data that\nfills the gap, which result in models capable of generating lengthy output when\ninstructed. In this paper, we explore the impact of data-quality in tuning a\nmodel for long output, and the possibility of doing so from the starting points\nof human-aligned (instruct or chat) models. With careful data curation, we show\nthat it possible to achieve similar performance improvement in our tuned\nmodels, with only a small fraction of training data instances and compute. In\naddition, we assess the generalizability of such approaches by applying our\ntuning-recipes to several models. our findings suggest that, while capacities\nfor generating long output vary across different models out-of-the-box, our\napproach to tune them with high-quality data using lite compute, consistently\nyields notable improvement across all models we experimented on. We have made\npublic our curated dataset for tuning long-writing capability, the\nimplementations of model tuning and evaluation, as well as the fine-tuned\nmodels, all of which can be openly-accessed.", "published": "2024-10-14 07:09:02", "link": "http://arxiv.org/abs/2410.10210v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChakmaNMT: A Low-resource Machine Translation On Chakma Language", "abstract": "The geopolitical division between the indigenous Chakma population and\nmainstream Bangladesh creates a significant cultural and linguistic gap, as the\nChakma community, mostly residing in the hill tracts of Bangladesh, maintains\ndistinct cultural traditions and language. Developing a Machine Translation\n(MT) model or Chakma to Bangla could play a crucial role in alleviating this\ncultural-linguistic divide. Thus, we have worked on MT between\nCCP-BN(Chakma-Bangla) by introducing a novel dataset of 15,021 parallel samples\nand 42,783 monolingual samples of the Chakma Language. Moreover, we introduce a\nsmall set for Benchmarking containing 600 parallel samples between Chakma,\nBangla, and English. We ran traditional and state-of-the-art models in NLP on\nthe training set, where fine-tuning BanglaT5 with back-translation using\ntransliteration of Chakma achieved the highest BLEU score of 17.8 and 4.41 in\nCCP-BN and BN-CCP respectively on the Benchmark Dataset. As far as we know,\nthis is the first-ever work on MT for the Chakma Language. Hopefully, this\nresearch will help to bridge the gap in linguistic resources and contribute to\npreserving endangered languages. Our dataset link and codes will be published\nsoon.", "published": "2024-10-14 07:21:31", "link": "http://arxiv.org/abs/2410.10219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation Evaluation Benchmark for Wu Chinese: Workflow and\n  Analysis", "abstract": "We introduce a FLORES+ dataset as an evaluation benchmark for modern Wu\nChinese machine translation models and showcase its compatibility with existing\nWu data. Wu Chinese is mutually unintelligible with other Sinitic languages\nsuch as Mandarin and Yue (Cantonese), but uses a set of Hanzi (Chinese\ncharacters) that profoundly overlaps with others. The population of Wu speakers\nis the second largest among languages in China, but the language has been\nsuffering from significant drop in usage especially among the younger\ngenerations. We identify Wu Chinese as a textually low-resource language and\naddress challenges for its machine translation models. Our contributions\ninclude: (1) an open-source, manually translated dataset, (2) full\ndocumentations on the process of dataset creation and validation experiments,\n(3) preliminary tools for Wu Chinese normalization and segmentation, and (4)\nbenefits and limitations of our dataset, as well as implications to other\nlow-resource languages.", "published": "2024-10-14 08:30:19", "link": "http://arxiv.org/abs/2410.10278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Translation Bias and Accuracy in Multilingual\n  Large Language Models for Cross-Language Claim Verification", "abstract": "The rise of digital misinformation has heightened interest in using\nmultilingual Large Language Models (LLMs) for fact-checking. This study\nsystematically evaluates translation bias and the effectiveness of LLMs for\ncross-lingual claim verification across 15 languages from five language\nfamilies: Romance, Slavic, Turkic, Indo-Aryan, and Kartvelian. Using the XFACT\ndataset to assess their impact on accuracy and bias, we investigate two\ndistinct translation methods: pre-translation and self-translation. We use\nmBERT's performance on the English dataset as a baseline to compare\nlanguage-specific accuracies. Our findings reveal that low-resource languages\nexhibit significantly lower accuracy in direct inference due to\nunderrepresentation in the training data. Furthermore, larger models\ndemonstrate superior performance in self-translation, improving translation\naccuracy and reducing bias. These results highlight the need for balanced\nmultilingual training, especially in low-resource languages, to promote\nequitable access to reliable fact-checking tools and minimize the risk of\nspreading misinformation in different linguistic contexts.", "published": "2024-10-14 09:02:42", "link": "http://arxiv.org/abs/2410.10303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MentalGLM Series: Explainable Large Language Models for Mental Health\n  Analysis on Chinese Social Media", "abstract": "As the prevalence of mental health challenges, social media has emerged as a\nkey platform for individuals to express their emotions.Deep learning tends to\nbe a promising solution for analyzing mental health on social media. However,\nblack box models are often inflexible when switching between tasks, and their\nresults typically lack explanations. With the rise of large language models\n(LLMs), their flexibility has introduced new approaches to the field. Also due\nto the generative nature, they can be prompted to explain decision-making\nprocesses. However, their performance on complex psychological analysis still\nlags behind deep learning. In this paper, we introduce the first multi-task\nChinese Social Media Interpretable Mental Health Instructions (C-IMHI) dataset,\nconsisting of 9K samples, which has been quality-controlled and manually\nvalidated. We also propose MentalGLM series models, the first open-source LLMs\ndesigned for explainable mental health analysis targeting Chinese social media,\ntrained on a corpus of 50K instructions. The proposed models were evaluated on\nthree downstream tasks and achieved better or comparable performance compared\nto deep learning models, generalized LLMs, and task fine-tuned LLMs. We\nvalidated a portion of the generated decision explanations with experts,\nshowing promising results. We also evaluated the proposed models on a clinical\ndataset, where they outperformed other LLMs, indicating their potential\napplicability in the clinical field. Our models show strong performance,\nvalidated across tasks and perspectives. The decision explanations enhance\nusability and facilitate better understanding and practical application of the\nmodels. Both the constructed dataset and the models are publicly available via:\nhttps://github.com/zwzzzQAQ/MentalGLM.", "published": "2024-10-14 09:29:27", "link": "http://arxiv.org/abs/2410.10323v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locking Down the Finetuned LLMs Safety", "abstract": "Fine-tuning large language models (LLMs) on additional datasets is often\nnecessary to optimize them for specific downstream tasks. However, existing\nsafety alignment measures, which restrict harmful behavior during inference,\nare insufficient to mitigate safety risks during fine-tuning. Alarmingly,\nfine-tuning with just 10 toxic sentences can make models comply with harmful\ninstructions. We introduce SafetyLock, a novel alignment intervention method\nthat maintains robust safety post-fine-tuning through efficient and\ntransferable mechanisms. SafetyLock leverages our discovery that fine-tuned\nmodels retain similar safety-related activation representations to their base\nmodels. This insight enables us to extract what we term the Meta-SafetyLock, a\nset of safety bias directions representing key activation patterns associated\nwith safe responses in the original model. We can then apply these directions\nuniversally to fine-tuned models to enhance their safety. By searching for\nactivation directions across multiple token dimensions, SafetyLock achieves\nenhanced robustness and transferability. SafetyLock re-aligns fine-tuned models\nin under 0.01 seconds without additional computational cost. Our experiments\ndemonstrate that SafetyLock can reduce the harmful instruction response rate\nfrom 60% to below 1% in toxic fine-tuned models. It surpasses traditional\nmethods in both performance and efficiency, offering a scalable, non-invasive\nsolution for ensuring the safety of customized LLMs. Our analysis across\nvarious fine-tuning scenarios confirms SafetyLock's robustness, advocating its\nintegration into safety protocols for aligned LLMs. The code is released at\nhttps://github.com/zhu-minjun/SafetyLock.", "published": "2024-10-14 09:58:29", "link": "http://arxiv.org/abs/2410.10343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Approach to Routing and Cascading for LLMs", "abstract": "The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection.", "published": "2024-10-14 10:00:49", "link": "http://arxiv.org/abs/2410.10347v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and\n  Refinement", "abstract": "It has been shown that Large Language Models' (LLMs) performance can be\nimproved for many tasks using Chain of Thought (CoT) or In-Context Learning\n(ICL), which involve demonstrating the steps needed to solve a task using a few\nexamples. However, while datasets with input-output pairs are relatively easy\nto produce, providing demonstrations which include intermediate steps requires\ncumbersome manual work. These steps may be executable programs, as in agentic\nflows, or step-by-step reasoning as in CoT. In this work, we propose Automatic\nData Labeling and Refinement (ADLR), a method to automatically generate and\nfilter demonstrations which include the above intermediate steps, starting from\na small seed of manually crafted examples. We demonstrate the advantage of ADLR\nin code-based table QA and mathematical reasoning, achieving up to a 5.5% gain.\nThe code implementing our method is provided in the Supplementary material and\nwill be made available.", "published": "2024-10-14 10:06:58", "link": "http://arxiv.org/abs/2410.10348v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-based Code-Switched Text Generation for Grammatical Error Correction", "abstract": "With the rise of globalisation, code-switching (CSW) has become a ubiquitous\npart of multilingual conversation, posing new challenges for natural language\nprocessing (NLP), especially in Grammatical Error Correction (GEC). This work\nexplores the complexities of applying GEC systems to CSW texts. Our objectives\ninclude evaluating the performance of state-of-the-art GEC systems on an\nauthentic CSW dataset from English as a Second Language (ESL) learners,\nexploring synthetic data generation as a solution to data scarcity, and\ndeveloping a model capable of correcting grammatical errors in monolingual and\nCSW texts. We generated synthetic CSW GEC data, resulting in one of the first\nsubstantial datasets for this task, and showed that a model trained on this\ndata is capable of significant improvements over existing systems. This work\ntargets ESL learners, aiming to provide educational technologies that aid in\nthe development of their English grammatical correctness without constraining\ntheir natural multilingualism.", "published": "2024-10-14 10:07:29", "link": "http://arxiv.org/abs/2410.10349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMCFND: Multimodal Multilingual Caption-aware Fake News Detection for\n  Low-resource Indic Languages", "abstract": "The widespread dissemination of false information through manipulative\ntactics that combine deceptive text and images threatens the integrity of\nreliable sources of information. While there has been research on detecting\nfake news in high resource languages using multimodal approaches, methods for\nlow resource Indic languages primarily rely on textual analysis. This\ndifference highlights the need for robust methods that specifically address\nmultimodal fake news in Indic languages, where the lack of extensive datasets\nand tools presents a significant obstacle to progress. To this end, we\nintroduce the Multimodal Multilingual dataset for Indic Fake News Detection\n(MMIFND). This meticulously curated dataset consists of 28,085 instances\ndistributed across Hindi, Bengali, Marathi, Malayalam, Tamil, Gujarati and\nPunjabi. We further propose the Multimodal Multilingual Caption-aware framework\nfor Fake News Detection (MMCFND). MMCFND utilizes pre-trained unimodal encoders\nand pairwise encoders from a foundational model that aligns vision and\nlanguage, allowing for extracting deep representations from visual and textual\ncomponents of news articles. The multimodal fusion encoder in the foundational\nmodel integrates text and image representations derived from its pairwise\nencoders to generate a comprehensive cross modal representation. Furthermore,\nwe generate descriptive image captions that provide additional context to\ndetect inconsistencies and manipulations. The retrieved features are then fused\nand fed into a classifier to determine the authenticity of news articles. The\ncurated dataset can potentially accelerate research and development in low\nresource environments significantly. Thorough experimentation on MMIFND\ndemonstrates that our proposed framework outperforms established methods for\nextracting relevant fake news detection features.", "published": "2024-10-14 11:59:33", "link": "http://arxiv.org/abs/2410.10407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian\n  Reasoning Scenarios", "abstract": "Reasoning is key to many decision making processes. It requires consolidating\na set of rule-like premises that are often associated with degrees of\nuncertainty and observations to draw conclusions. In this work, we address both\nthe case where premises are specified as numeric probabilistic rules and\nsituations in which humans state their estimates using words expressing degrees\nof certainty. Existing probabilistic reasoning datasets simplify the task,\ne.g., by requiring the model to only rank textual alternatives, by including\nonly binary random variables, or by making use of a limited set of templates\nthat result in less varied text.\n  In this work, we present QUITE, a question answering dataset of real-world\nBayesian reasoning scenarios with categorical random variables and complex\nrelationships. QUITE provides high-quality natural language verbalizations of\npremises together with evidence statements and expects the answer to a question\nin the form of an estimated probability. We conduct an extensive set of\nexperiments, finding that logic-based models outperform out-of-the-box large\nlanguage models on all reasoning types (causal, evidential, and\nexplaining-away). Our results provide evidence that neuro-symbolic models are a\npromising direction for improving complex reasoning. We release QUITE and code\nfor training and experiments on Github.", "published": "2024-10-14 12:44:59", "link": "http://arxiv.org/abs/2410.10449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs", "abstract": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE)\narchitectures offer a promising approach to managing computational costs while\nscaling up model parameters. Conventional MoE-based LLMs typically employ\nstatic Top-K routing, which activates a fixed and equal number of experts for\neach token regardless of their significance within the context. In this paper,\nwe propose a novel Ada-K routing strategy that dynamically adjusts the number\nof activated experts for each token, thereby improving the balance between\ncomputational efficiency and model performance. Specifically, our strategy\nincorporates learnable and lightweight allocator modules that decide customized\nexpert resource allocation tailored to the contextual needs for each token.\nThese allocators are designed to be fully pluggable, making it broadly\napplicable across all mainstream MoE-based LLMs. We leverage the Proximal\nPolicy Optimization (PPO) algorithm to facilitate an end-to-end learning\nprocess for this non-differentiable decision-making framework. Extensive\nevaluations on four popular baseline models demonstrate that our Ada-K routing\nmethod significantly outperforms conventional Top-K routing. Compared to Top-K,\nour method achieves over 25% reduction in FLOPs and more than 20% inference\nspeedup while still improving performance across various benchmarks. Moreover,\nthe training of Ada-K is highly efficient. Even for Mixtral-8x22B, a MoE-based\nLLM with more than 140B parameters, the training time is limited to 8 hours.\nDetailed analysis shows that harder tasks, middle layers, and content words\ntend to activate more experts, providing valuable insights for future adaptive\nMoE system designs. Both the training code and model checkpoints will be\npublicly available.", "published": "2024-10-14 12:50:04", "link": "http://arxiv.org/abs/2410.10456v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Structure Dependence Shaped for Efficient Communication?: A Case\n  Study on Coordination", "abstract": "Natural language exhibits various universal properties. But why do these\nuniversals exist? One explanation is that they arise from functional pressures\nto achieve efficient communication, a view which attributes cross-linguistic\nproperties to domain-general cognitive abilities. This hypothesis has\nsuccessfully addressed some syntactic universal properties such as\ncompositionality and Greenbergian word order universals. However, more abstract\nsyntactic universals have not been explored from the perspective of efficient\ncommunication. Among such universals, the most notable one is structure\ndependence, that is, the existence of grammar-internal operations that\ncrucially depend on hierarchical representations. This property has\ntraditionally been taken to be central to natural language and to involve\ndomain-specific knowledge irreducible to communicative efficiency.\n  In this paper, we challenge the conventional view by investigating whether\nstructure dependence realizes efficient communication, focusing on coordinate\nstructures. We design three types of artificial languages: (i) one with a\nstructure-dependent reduction operation, which is similar to natural language,\n(ii) one without any reduction operations, and (iii) one with a linear (rather\nthan structure-dependent) reduction operation. We quantify the communicative\nefficiency of these languages. The results demonstrate that the language with\nthe structure-dependent reduction operation is significantly more\ncommunicatively efficient than the counterfactual languages. This suggests that\nthe existence of structure-dependent properties can be explained from the\nperspective of efficient communication.", "published": "2024-10-14 14:35:21", "link": "http://arxiv.org/abs/2410.10556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recipe for Zero-shot POS Tagging: Is It Useful in Realistic Scenarios?", "abstract": "POS tagging plays a fundamental role in numerous applications. While POS\ntaggers are highly accurate in well-resourced settings, they lag behind in\ncases of limited or missing training data. This paper focuses on POS tagging\nfor languages with limited data. We seek to identify the characteristics of\ndatasets that make them favourable for training POS tagging models without\nusing any labelled training data from the target language. This is a zero-shot\napproach. We compare the accuracies of a multilingual large language model\n(mBERT) fine-tuned on one or more languages related to the target language.\nAdditionally, we compare these results with models trained directly on the\ntarget language itself. We do this for three target low-resource languages. Our\nresearch highlights the importance of accurate dataset selection for effective\nzero-shot POS tagging. Particularly, a strong linguistic relationship and\nhigh-quality datasets ensure optimal results. For extremely low-resource\nlanguages, zero-shot models prove to be a viable option.", "published": "2024-10-14 14:51:13", "link": "http://arxiv.org/abs/2410.10576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T\u00fcbingen-CL at SemEval-2024 Task 1:Ensemble Learning for Semantic\n  Relatedness Estimation", "abstract": "The paper introduces our system for SemEval-2024 Task 1, which aims to\npredict the relatedness of sentence pairs. Operating under the hypothesis that\nsemantic relatedness is a broader concept that extends beyond mere similarity\nof sentences, our approach seeks to identify useful features for relatedness\nestimation. We employ an ensemble approach integrating various systems,\nincluding statistical textual features and outputs of deep learning models to\npredict relatedness scores. The findings suggest that semantic relatedness can\nbe inferred from various sources and ensemble models outperform many individual\nsystems in estimating semantic relatedness.", "published": "2024-10-14 14:56:51", "link": "http://arxiv.org/abs/2410.10585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition", "abstract": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from sensor data.\nDespite their strong reasoning and generalization capabilities, LLMs remain\nunderutilized for motion sensor data due to the lack of semantic context in\ntime-series, computational constraints, and challenges in processing numerical\ninputs. SensorLLM addresses these limitations through a Sensor-Language\nAlignment stage, where we introduce special tokens for each sensor channel and\nautomatically generate textual trend descriptions. This alignment enables LLMs\nto capture numerical variations, channel-specific features, and data of varying\nduration--without requiring human annotations. In the subsequent Task-Aware\nTuning stage, we refine the model for HAR classification, achieving performance\nthat matches or surpasses state-of-the-art methods. Our results demonstrate\nthat SensorLLM evolves into an effective sensor learner, reasoner, and\nclassifier through Sensor-Language Alignment, generalizing across diverse HAR\ndatasets. We believe this work establishes a foundation for future research on\ntime-series and text alignment, paving the way for foundation models in sensor\ndata analysis.", "published": "2024-10-14 15:30:41", "link": "http://arxiv.org/abs/2410.10624v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of\n  Language Family Experts", "abstract": "Adapting medical Large Language Models to local languages can reduce barriers\nto accessing healthcare services, but data scarcity remains a significant\nchallenge, particularly for low-resource languages. To address this, we first\nconstruct a high-quality medical dataset and conduct analysis to ensure its\nquality. In order to leverage the generalization capability of multilingual\nLLMs to efficiently scale to more resource-constrained languages, we explore\nthe internal information flow of LLMs from a multilingual perspective using\nMixture of Experts (MoE) modularity. Technically, we propose a novel MoE\nrouting method that employs language-specific experts and cross-lingual\nrouting. Inspired by circuit theory, our routing analysis revealed a Spread Out\nin the End information flow mechanism: while earlier layers concentrate\ncross-lingual information flow, the later layers exhibit language-specific\ndivergence. This insight directly led to the development of the Post-MoE\narchitecture, which applies sparse routing only in the later layers while\nmaintaining dense others. Experimental results demonstrate that this approach\nenhances the generalization of multilingual models to other languages while\npreserving interpretability. Finally, to efficiently scale the model to 50\nlanguages, we introduce the concept of language family experts, drawing on\nlinguistic priors, which enables scaling the number of languages without adding\nadditional parameters.", "published": "2024-10-14 15:31:54", "link": "http://arxiv.org/abs/2410.10626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Model Evaluation via Matrix Nuclear-Norm", "abstract": "As large language models (LLMs) continue to evolve, efficient evaluation\nmetrics are vital for assessing their ability to compress information and\nreduce redundancy. While traditional metrics like Matrix Entropy offer valuable\ninsights, they are computationally intensive for large-scale models due to\ntheir \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To\nmitigate this issue, we introduce the Matrix Nuclear-Norm, which not only\nserves as a metric to quantify the data compression proficiency of LLM but also\nprovides a convex approximation of matrix rank to capture both predictive\ndiscriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to\nfurther approximate the nuclear norm, we can effectively assess the model's\ninformation compression capabilities. This approach reduces the time complexity\nto \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the\nMatrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy\nfor the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This\nperformance gap becomes more pronounced with larger models, as validated in\ntests with other models like Pythia. Additionally, evaluations on benchmarks\nand model responses confirm that our proposed Matrix Nuclear-Norm is a\nreliable, scalable, and efficient tool for assessing LLMs' performance,\nstriking a balance between accuracy and computational efficiency. The code is\navailable at https://github.com/MLGroupJLU/MatrixNuclearNorm.", "published": "2024-10-14 16:15:57", "link": "http://arxiv.org/abs/2410.10672v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Active Critics in NLG Evaluation", "abstract": "The conventional paradigm of using large language models (LLMs) for natural\nlanguage generation (NLG) evaluation relies on pre-defined task definitions and\nevaluation criteria, positioning LLMs as \"passive critics\" that strictly follow\ndeveloper-provided guidelines. However, human evaluators often apply implicit\ncriteria, and their expectations in practice can vary widely based on specific\nend-user needs. Consequently, these rigid evaluation methods struggle to adapt\nto diverse scenarios without extensive prompt customization. To address this,\nwe introduce Active-Critic, a novel LLM-based evaluator that transforms LLMs\ninto \"active critics'' capable of adapting to diverse NLG tasks using limited\nexample data. Active-Critic consists of two stages: (1) self-inferring the\ntarget NLG task and relevant evaluation criteria, and (2) dynamically\noptimizing prompts to produce human-aligned scores along with detailed\njustifications. Our experiments show that Active-Critic can generate nuanced,\ncontext-aware evaluation criteria, enabling it to achieve superior alignment\nwith human judgments across multiple tasks.", "published": "2024-10-14 17:04:41", "link": "http://arxiv.org/abs/2410.10724v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning:\n  Optimizing Instruction-Following in LLMs", "abstract": "Large Language Models (LLMs) for public use require continuous pre-training\nto remain up-to-date with the latest data. The models also need to be\nfine-tuned with specific instructions to maintain their ability to follow\ninstructions accurately. Typically, LLMs are released in two versions: the Base\nLLM, pre-trained on diverse data, and the instruction-refined LLM, additionally\ntrained with specific instructions for better instruction following. The\nquestion arises as to which model should undergo continuous pre-training to\nmaintain its instruction-following abilities while also staying current with\nthe latest data. In this study, we delve into the intricate relationship\nbetween continuous pre-training and instruction fine-tuning of the LLMs and\ninvestigate the impact of continuous pre-training on the instruction following\nabilities of both the base and its instruction finetuned model. Further, the\ninstruction fine-tuning process is computationally intense and requires a\nsubstantial number of hand-annotated examples for the model to learn\neffectively. This study aims to find the most compute-efficient strategy to\ngain up-to-date knowledge and instruction-following capabilities without\nrequiring any instruction data and fine-tuning. We empirically prove our\nfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction\nmodels, providing a comprehensive exploration of our hypotheses across varying\nsizes of pre-training data corpus and different LLMs settings.", "published": "2024-10-14 17:20:30", "link": "http://arxiv.org/abs/2410.10739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Use Random Selection for Now: Investigation of Few-Shot Selection\n  Strategies in LLM-based Text Augmentation for Classification", "abstract": "The generative large language models (LLMs) are increasingly used for data\naugmentation tasks, where text samples are paraphrased (or generated anew) and\nthen used for classifier fine-tuning. Existing works on augmentation leverage\nthe few-shot scenarios, where samples are given to LLMs as part of prompts,\nleading to better augmentations. Yet, the samples are mostly selected randomly\nand a comprehensive overview of the effects of other (more ``informed'') sample\nselection strategies is lacking. In this work, we compare sample selection\nstrategies existing in few-shot learning literature and investigate their\neffects in LLM-based textual augmentation. We evaluate this on in-distribution\nand out-of-distribution classifier performance. Results indicate, that while\nsome ``informed'' selection strategies increase the performance of models,\nespecially for out-of-distribution data, it happens only seldom and with\nmarginal performance increases. Unless further advances are made, a default of\nrandom sample selection remains a good option for augmentation practitioners.", "published": "2024-10-14 17:30:08", "link": "http://arxiv.org/abs/2410.10756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Local and Global Decoding in Text Generation", "abstract": "Text generation, a key component in applications such as dialogue systems,\nrelies on decoding algorithms that sample strings from a language model\ndistribution. Traditional methods, such as top-$k$ and top-$\\pi$, apply local\nnormalisation to the model's output distribution, which can distort it. In this\npaper, we investigate the effect of this distortion by introducing\nglobally-normalised versions of these decoding methods. Additionally, we\npropose an independent Metropolis-Hastings algorithm to approximate sampling\nfrom globally-normalised distributions without explicitly computing them. Our\nempirical analysis compares the performance of local and global normalisation\nacross two decoding algorithms (top-$k$ and top-$\\pi$) with various\nhyperparameters, using Pythia language models. Results show that, in most\nconfigurations, global decoding performs worse than the local decoding version\nof the same algorithms -- despite preserving the distribution's integrity. Our\nresults suggest that distortion is an important feature of local decoding\nalgorithms.", "published": "2024-10-14 17:59:38", "link": "http://arxiv.org/abs/2410.10810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive\n  Memory", "abstract": "Recent large language model (LLM)-driven chat assistant systems have\nintegrated memory components to track user-assistant chat histories, enabling\nmore accurate and personalized responses. However, their long-term memory\ncapabilities in sustained interactions remain underexplored. We introduce\nLongMemEval, a comprehensive benchmark designed to evaluate five core long-term\nmemory abilities of chat assistants: information extraction, multi-session\nreasoning, temporal reasoning, knowledge updates, and abstention. With 500\nmeticulously curated questions embedded within freely scalable user-assistant\nchat histories, LongMemEval presents a significant challenge to existing\nlong-term memory systems, with commercial chat assistants and long-context LLMs\nshowing a 30% accuracy drop on memorizing information across sustained\ninteractions. We then present a unified framework that breaks down the\nlong-term memory design into three stages: indexing, retrieval, and reading.\nBuilt upon key experimental insights, we propose several memory design\noptimizations including session decomposition for value granularity,\nfact-augmented key expansion for indexing, and time-aware query expansion for\nrefining the search scope. Extensive experiments show that these optimizations\ngreatly improve both memory recall and downstream question answering on\nLongMemEval. Overall, our study provides valuable resources and guidance for\nadvancing the long-term memory capabilities of LLM-based chat assistants,\npaving the way toward more personalized and reliable conversational AI. Our\nbenchmark and code are publicly available at\nhttps://github.com/xiaowu0162/LongMemEval.", "published": "2024-10-14 17:59:44", "link": "http://arxiv.org/abs/2410.10813v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads", "abstract": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.", "published": "2024-10-14 17:59:58", "link": "http://arxiv.org/abs/2410.10819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Watching the Watchers: Exposing Gender Disparities in Machine\n  Translation Quality Estimation", "abstract": "Quality estimation (QE) -- the automatic assessment of translation quality --\nhas recently become crucial across several stages of the translation pipeline,\nfrom data curation to training and decoding. While QE metrics have been\noptimized to align with human judgments, whether they encode social biases has\nbeen largely overlooked. Biased QE risks favoring certain demographic groups\nover others, e.g., by exacerbating gaps in visibility and usability. This paper\ndefines and investigates gender bias of QE metrics and discusses its downstream\nimplications for machine translation (MT). Experiments with state-of-the-art QE\nmetrics across multiple domains, datasets, and languages reveal significant\nbias. When a human entity's gender in the source is undisclosed,\nmasculine-inflected translations score higher than feminine-inflected ones and\ngender-neutral translations are penalized. Even when contextual cues\ndisambiguate gender, using context-aware QE metrics leads to more errors in\npicking the correct translation inflection for feminine than masculine\nreferents. Moreover, a biased QE metric affects data filtering and\nquality-aware decoding. Our findings highlight the need for renewed focus in\ndeveloping and evaluating QE metrics centered around gender.", "published": "2024-10-14 18:24:52", "link": "http://arxiv.org/abs/2410.10995v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Self-Mining of In-Context Examples for Unsupervised Machine\n  Translation with LLMs", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance on a\nwide range of natural language processing (NLP) tasks, primarily through\nin-context learning (ICL). In ICL, the LLM is provided with examples that\nrepresent a given task such that it learns to generate answers for test inputs.\nHowever, access to these in-context examples is not guaranteed especially for\nlow-resource or massively multilingual tasks. In this work, we propose an\nunsupervised approach to mine in-context examples for machine translation (MT),\nenabling unsupervised MT (UMT) across different languages. Our approach begins\nwith word-level mining to acquire word translations that are then used to\nperform sentence-level mining. As the quality of mined parallel pairs may not\nbe optimal due to noise or mistakes, we introduce a filtering criterion to\nselect the optimal in-context examples from a pool of unsupervised parallel\nsentences. We evaluate our approach using two multilingual LLMs on 288\ndirections from the FLORES-200 dataset and analyze the impact of various\nlinguistic features on performance. Our findings demonstrate the effectiveness\nof our unsupervised approach in mining in-context examples for MT, leading to\nbetter or comparable translation performance as translation with regular\nin-context samples (extracted from human-annotated data), while also\noutperforming the other state-of-the-art UMT methods by an average of $7$ BLEU\npoints.", "published": "2024-10-14 18:47:04", "link": "http://arxiv.org/abs/2410.11006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Human Likeness of AI-Generated Counterspeech", "abstract": "Counterspeech is a targeted response to counteract and challenge abusive or\nhateful content. It effectively curbs the spread of hatred and fosters\nconstructive online communication. Previous studies have proposed different\nstrategies for automatically generated counterspeech. Evaluations, however,\nfocus on relevance, surface form, and other shallow linguistic characteristics.\nThis paper investigates the human likeness of AI-generated counterspeech, a\ncritical factor influencing effectiveness. We implement and evaluate several\nLLM-based generation strategies, and discover that AI-generated and\nhuman-written counterspeech can be easily distinguished by both simple\nclassifiers and humans. Further, we reveal differences in linguistic\ncharacteristics, politeness, and specificity. The dataset used in this study is\npublicly available for further research.", "published": "2024-10-14 18:48:47", "link": "http://arxiv.org/abs/2410.11007v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personality Differences Drive Conversational Dynamics: A\n  High-Dimensional NLP Approach", "abstract": "This paper investigates how the topical flow of dyadic conversations emerges\nover time and how differences in interlocutors' personality traits contribute\nto this topical flow. Leveraging text embeddings, we map the trajectories of $N\n= 1655$ conversations between strangers into a high-dimensional space. Using\nnonlinear projections and clustering, we then identify when each interlocutor\nenters and exits various topics. Differences in conversational flow are\nquantified via $\\textit{topic entropy}$, a summary measure of the \"spread\" of\ntopics covered during a conversation, and $\\textit{linguistic alignment}$, a\ntime-varying measure of the cosine similarity between interlocutors'\nembeddings. Our findings suggest that interlocutors with a larger difference in\nthe personality dimension of openness influence each other to spend more time\ndiscussing a wider range of topics and that interlocutors with a larger\ndifference in extraversion experience a larger decrease in linguistic alignment\nthroughout their conversation. We also examine how participants' affect\n(emotion) changes from before to after a conversation, finding that a larger\ndifference in extraversion predicts a larger difference in affect change and\nthat a greater topic entropy predicts a larger affect increase. This work\ndemonstrates how communication research can be advanced through the use of\nhigh-dimensional NLP methods and identifies personality difference as an\nimportant driver of social influence.", "published": "2024-10-14 19:48:31", "link": "http://arxiv.org/abs/2410.11043v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Human-Only: Evaluating Human-Machine Collaboration for Collecting\n  High-Quality Translation Data", "abstract": "Collecting high-quality translations is crucial for the development and\nevaluation of machine translation systems. However, traditional human-only\napproaches are costly and slow. This study presents a comprehensive\ninvestigation of 11 approaches for acquiring translation data, including\nhuman-only, machineonly, and hybrid approaches. Our findings demonstrate that\nhuman-machine collaboration can match or even exceed the quality of human-only\ntranslations, while being more cost-efficient. Error analysis reveals the\ncomplementary strengths between human and machine contributions, highlighting\nthe effectiveness of collaborative methods. Cost analysis further demonstrates\nthe economic benefits of human-machine collaboration methods, with some\napproaches achieving top-tier quality at around 60% of the cost of traditional\nmethods. We release a publicly available dataset containing nearly 18,000\nsegments of varying translation quality with corresponding human ratings to\nfacilitate future research.", "published": "2024-10-14 20:02:12", "link": "http://arxiv.org/abs/2410.11056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Annotated Dataset of Errors in Premodern Greek and Baselines for\n  Detecting Them", "abstract": "As premodern texts are passed down over centuries, errors inevitably accrue.\nThese errors can be challenging to identify, as some have survived undetected\nfor so long precisely because they are so elusive. While prior work has\nevaluated error detection methods on artificially-generated errors, we\nintroduce the first dataset of real errors in premodern Greek, enabling the\nevaluation of error detection methods on errors that genuinely accumulated at\nsome stage in the centuries-long copying process. To create this dataset, we\nuse metrics derived from BERT conditionals to sample 1,000 words more likely to\ncontain errors, which are then annotated and labeled by a domain expert as\nerrors or not. We then propose and evaluate new error detection methods and\nfind that our discriminator-based detector outperforms all other methods,\nimproving the true positive rate for classifying real errors by 5%. We\nadditionally observe that scribal errors are more difficult to detect than\nprint or digitization errors. Our dataset enables the evaluation of error\ndetection methods on real errors in premodern texts for the first time,\nproviding a benchmark for developing more effective error detection algorithms\nto assist scholars in restoring premodern works.", "published": "2024-10-14 20:30:54", "link": "http://arxiv.org/abs/2410.11071v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Bias in Decision-Making with Large Language Models: A Study of\n  Relationship Conflicts", "abstract": "Large language models (LLMs) acquire beliefs about gender from training data\nand can therefore generate text with stereotypical gender attitudes. Prior\nstudies have demonstrated model generations favor one gender or exhibit\nstereotypes about gender, but have not investigated the complex dynamics that\ncan influence model reasoning and decision-making involving gender. We study\ngender equity within LLMs through a decision-making lens with a new dataset,\nDeMET Prompts, containing scenarios related to intimate, romantic\nrelationships. We explore nine relationship configurations through name pairs\nacross three name lists (men, women, neutral). We investigate equity in the\ncontext of gender roles through numerous lenses: typical and gender-neutral\nnames, with and without model safety enhancements, same and mixed-gender\nrelationships, and egalitarian versus traditional scenarios across various\ntopics. While all models exhibit the same biases (women favored, then those\nwith gender-neutral names, and lastly men), safety guardrails reduce bias. In\naddition, models tend to circumvent traditional male dominance stereotypes and\nside with 'traditionally female' individuals more often, suggesting\nrelationships are viewed as a female domain by the models.", "published": "2024-10-14 20:50:11", "link": "http://arxiv.org/abs/2410.11084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JOOCI: a Framework for Learning Comprehensive Speech Representations", "abstract": "Information in speech can be categorized into two groups: Content (what is\nbeing said, such as linguistics) and Other (how it is expressed such as\ninformation about speaker and paralinguistic features). Current self-supervised\nlearning (SSL) methods are shown to divide the model's representational-depth\nor layers in two, with earlier layers specializing in Other and later layers in\nContent related tasks. This layer-wise division is inherently sub-optimal, as\nneither information type can use all layers to build hierarchical\nrepresentations. To address this, we propose JOOCI, a novel speech\nrepresentation learning method that does not compromise on the\nrepresentational-depth for either information type. JOOCI outperforms WavLM by\n26.5%, and other models of similar size (100M parameters), when evaluated on\ntwo speaker recognition and two language tasks from the SUPERB benchmark,\ndemonstrating its effectiveness in Jointly Optimizing Other and Content\nInformation (JOOCI).", "published": "2024-10-14 20:59:59", "link": "http://arxiv.org/abs/2410.11086v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning for Robust and Representative LLM Generation in\n  Safety-Critical Scenarios", "abstract": "Ensuring robust safety measures across a wide range of scenarios is crucial\nfor user-facing systems. While Large Language Models (LLMs) can generate\nvaluable data for safety measures, they often exhibit distributional biases,\nfocusing on common scenarios and neglecting rare but critical cases. This can\nundermine the effectiveness of safety protocols developed using such data. To\naddress this, we propose a novel framework that integrates active learning with\nclustering to guide LLM generation, enhancing their representativeness and\nrobustness in safety scenarios. We demonstrate the effectiveness of our\napproach by constructing a dataset of 5.4K potential safety violations through\nan iterative process involving LLM generation and an active learner model's\nfeedback. Our results show that the proposed framework produces a more\nrepresentative set of safety scenarios without requiring prior knowledge of the\nunderlying data distribution. Additionally, data acquired through our method\nimproves the accuracy and F1 score of both the active learner model as well\nmodels outside the scope of active learning process, highlighting its broad\napplicability.", "published": "2024-10-14 21:48:14", "link": "http://arxiv.org/abs/2410.11114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChuLo: Chunk-Level Key Information Representation for Long Document\n  Processing", "abstract": "Transformer-based models have achieved remarkable success in various Natural\nLanguage Processing (NLP) tasks, yet their ability to handle long documents is\nconstrained by computational limitations. Traditional approaches, such as\ntruncating inputs, sparse self-attention, and chunking, attempt to mitigate\nthese issues, but they often lead to information loss and hinder the model's\nability to capture long-range dependencies. In this paper, we introduce ChuLo,\na novel chunk representation method for long document classification that\naddresses these limitations. Our ChuLo groups input tokens using unsupervised\nkeyphrase extraction, emphasizing semantically important keyphrase based chunk\nto retain core document content while reducing input length. This approach\nminimizes information loss and improves the efficiency of Transformer-based\nmodels. Preserving all tokens in long document understanding, especially token\nclassification tasks, is especially important to ensure that fine-grained\nannotations, which depend on the entire sequence context, are not lost. We\nevaluate our method on multiple long document classification tasks and long\ndocument token classification tasks, demonstrating its effectiveness through\ncomprehensive qualitative and quantitative analyses.", "published": "2024-10-14 22:06:54", "link": "http://arxiv.org/abs/2410.11119v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IsoChronoMeter: A simple and effective isochronic translation evaluation\n  metric", "abstract": "Machine translation (MT) has come a long way and is readily employed in\nproduction systems to serve millions of users daily. With the recent advances\nin generative AI, a new form of translation is becoming possible - video\ndubbing. This work motivates the importance of isochronic translation,\nespecially in the context of automatic dubbing, and introduces `IsoChronoMeter'\n(ICM). ICM is a simple yet effective metric to measure isochrony of\ntranslations in a scalable and resource-efficient way without the need for gold\ndata, based on state-of-the-art text-to-speech (TTS) duration predictors. We\nmotivate IsoChronoMeter and demonstrate its effectiveness. Using ICM we\ndemonstrate the shortcomings of state-of-the-art translation systems and show\nthe need for new methods. We release the code at this URL:\n\\url{https://github.com/braskai/isochronometer}.", "published": "2024-10-14 22:43:30", "link": "http://arxiv.org/abs/2410.11127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ukrainian-to-English folktale corpus: Parallel corpus creation and\n  augmentation for machine translation in low-resource languages", "abstract": "Folktales are linguistically very rich and culturally significant in\nunderstanding the source language. Historically, only human translation has\nbeen used for translating folklore. Therefore, the number of translated texts\nis very sparse, which limits access to knowledge about cultural traditions and\ncustoms. We have created a new Ukrainian-To-English parallel corpus of familiar\nUkrainian folktales based on available English translations and suggested\nseveral new ones. We offer a combined domain-specific approach to building and\naugmenting this corpus, considering the nature of the domain and differences in\nthe purpose of human versus machine translation. Our corpus is word and\nsentence-aligned, allowing for the best curation of meaning, specifically\ntailored for use as training data for machine translation models.", "published": "2024-10-14 01:00:53", "link": "http://arxiv.org/abs/2410.10063v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Leverage Demonstration Data in Alignment for Large Language\n  Model? A Self-Imitation Learning Perspective", "abstract": "This paper introduces a novel generalized self-imitation learning\n($\\textbf{GSIL}$) framework, which effectively and efficiently aligns large\nlanguage models with offline demonstration data. We develop $\\textbf{GSIL}$ by\nderiving a surrogate objective of imitation learning with density ratio\nestimates, facilitating the use of self-generated data and optimizing the\nimitation learning objective with simple classification losses. $\\textbf{GSIL}$\neliminates the need for complex adversarial training in standard imitation\nlearning, achieving lightweight and efficient fine-tuning for large language\nmodels. In addition, $\\textbf{GSIL}$ encompasses a family of offline losses\nparameterized by a general class of convex functions for density ratio\nestimation and enables a unified view for alignment with demonstration data.\nExtensive experiments show that $\\textbf{GSIL}$ consistently and significantly\noutperforms baselines in many challenging benchmarks, such as coding\n(HuamnEval), mathematical reasoning (GSM8K) and instruction-following benchmark\n(MT-Bench).", "published": "2024-10-14 02:21:29", "link": "http://arxiv.org/abs/2410.10093v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can We Predict Performance of Large Models across Vision-Language Tasks?", "abstract": "Evaluating large vision-language models (LVLMs) is very expensive, due to the\nhigh computational costs and the wide variety of tasks. The good news is that\nif we already have some observed performance scores, we may be able to infer\nunknown ones. In this study, we propose a new framework for predicting unknown\nperformance scores based on observed ones from other LVLMs or tasks. We first\nformulate the performance prediction as a matrix completion task. Specifically,\nwe construct a sparse performance matrix $\\boldsymbol{R}$, where each entry\n$R_{mn}$ represents the performance score of the $m$-th model on the $n$-th\ndataset. By applying probabilistic matrix factorization (PMF) with Markov chain\nMonte Carlo (MCMC), we can complete the performance matrix, that is, predict\nunknown scores. Additionally, we estimate the uncertainty of performance\nprediction based on MCMC. Practitioners can evaluate their models on untested\ntasks with higher uncertainty first, quickly reducing errors in performance\nprediction. We further introduce several improvements to enhance PMF for\nscenarios with sparse observed performance scores. In experiments, we\nsystematically evaluate 108 LVLMs on 176 datasets from 36 benchmarks,\nconstructing training and testing sets for validating our framework. Our\nexperiments demonstrate the accuracy of PMF in predicting unknown scores, the\nreliability of uncertainty estimates in ordering evaluations, and the\neffectiveness of our enhancements for handling sparse data.", "published": "2024-10-14 03:00:12", "link": "http://arxiv.org/abs/2410.10112v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Beyond-RAG: Question Identification and Answer Generation in Real-Time\n  Conversations", "abstract": "In customer contact centers, human agents often struggle with long average\nhandling times (AHT) due to the need to manually interpret queries and retrieve\nrelevant knowledge base (KB) articles. While retrieval augmented generation\n(RAG) systems using large language models (LLMs) have been widely adopted in\nindustry to assist with such tasks, RAG faces challenges in real-time\nconversations, such as inaccurate query formulation and redundant retrieval of\nfrequently asked questions (FAQs). To address these limitations, we propose a\ndecision support system that can look beyond RAG by first identifying customer\nquestions in real time. If the query matches an FAQ, the system retrieves the\nanswer directly from the FAQ database; otherwise, it generates answers via RAG.\nOur approach reduces reliance on manual queries, providing responses to agents\nwithin 2 seconds. Deployed in AI-powered human-agent assist solution at Minerva\nCQ, this system improves efficiency, reduces AHT, and lowers operational costs.\nWe also introduce an automated LLM-agentic workflow to identify FAQs from\nhistorical transcripts when no predefined FAQs exist.", "published": "2024-10-14 04:06:22", "link": "http://arxiv.org/abs/2410.10136v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jailbreak Instruction-Tuned LLMs via end-of-sentence MLP Re-weighting", "abstract": "In this paper, we investigate the safety mechanisms of instruction fine-tuned\nlarge language models (LLMs). We discover that re-weighting MLP neurons can\nsignificantly compromise a model's safety, especially for MLPs in\nend-of-sentence inferences. We hypothesize that LLMs evaluate the harmfulness\nof prompts during end-of-sentence inferences, and MLP layers plays a critical\nrole in this process. Based on this hypothesis, we develop 2 novel white-box\njailbreak methods: a prompt-specific method and a prompt-general method. The\nprompt-specific method targets individual prompts and optimizes the attack on\nthe fly, while the prompt-general method is pre-trained offline and can\ngeneralize to unseen harmful prompts. Our methods demonstrate robust\nperformance across 7 popular open-source LLMs, size ranging from 2B to 72B.\nFurthermore, our study provides insights into vulnerabilities of\ninstruction-tuned LLM's safety and deepens the understanding of the internal\nmechanisms of LLMs.", "published": "2024-10-14 04:32:22", "link": "http://arxiv.org/abs/2410.10150v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Parameter Collision Hindering Continual Learning in LLMs?", "abstract": "Large Language Models (LLMs) often suffer from catastrophic forgetting when\nlearning multiple tasks sequentially, making continual learning (CL) essential\nfor their dynamic deployment. Existing state-of-the-art (SOTA) methods, such as\nO-LoRA, typically focus on constructing orthogonality tasks to decouple\nparameter interdependence from various domains.In this paper, we reveal that\nbuilding non-collision parameters is a more critical factor in addressing CL\nchallenges. Our theoretical and experimental analyses demonstrate that\nnon-collision parameters can provide better task orthogonality, which is a\nsufficient but unnecessary condition. Furthermore, knowledge from multiple\ndomains will be preserved in non-collision parameter subspaces, making it more\ndifficult to forget previously seen data. Leveraging this insight, we propose\nNon-collision Low-Rank Adaptation (N-LoRA), a simple yet effective approach\nleveraging low collision rates to enhance CL in LLMs. Experimental results on\nmultiple CL benchmarks indicate that N-LoRA achieves superior performance\n(+2.9), higher task orthogonality (*4.1 times), and lower parameter collision\n(*58.1 times) than SOTA methods.", "published": "2024-10-14 05:54:11", "link": "http://arxiv.org/abs/2410.10179v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Scalable Multi-Domain Adaptation of Language Models using Modular\n  Experts", "abstract": "Domain-specific adaptation is critical to maximizing the performance of\npre-trained language models (PLMs) on one or multiple targeted tasks,\nespecially under resource-constrained use cases, such as edge devices. However,\nexisting methods often struggle to balance domain-specific performance,\nretention of general knowledge, and efficiency for training and inference. To\naddress these challenges, we propose Modular Domain Experts (MoDE). MoDE is a\nmixture-of-experts architecture that augments a general PLMs with modular,\ndomain-specialized experts. These experts are trained independently and\ncomposed together via a lightweight training process. In contrast to standard\nlow-rank adaptation methods, each MoDE expert consists of several transformer\nlayers which scale better with more training examples and larger parameter\ncounts. Our evaluation demonstrates that MoDE achieves comparable target\nperformances to full parameter fine-tuning while achieving 1.65% better\nretention performance. Moreover, MoDE's architecture enables flexible sharding\nconfigurations and improves training speeds by up to 38% over state-of-the-art\ndistributed training configurations.", "published": "2024-10-14 06:02:56", "link": "http://arxiv.org/abs/2410.10181v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SwiftCoder: Enhancing Code Generation in Large Language Models through\n  Efficiency-Aware Fine-tuning", "abstract": "As large language models (LLMs) play an increasingly important role in code\ngeneration, enhancing both correctness and efficiency has become crucial.\nCurrent methods primarily focus on correctness, often overlooking efficiency.\nTo address this gap, we introduce \\dataset to improve both aspects by\nfine-tuning LLMs on a high-quality dataset comprising correct and efficient\ncode samples. Our methodology involves leveraging multiple LLMs to generate\ndiverse candidate code solutions for various tasks across different programming\nlanguages. We then evaluate these solutions by directly measuring their\nexecution time and memory usage through local execution. The code solution with\nthe lowest execution time and memory consumption is selected as the final\noutput for each task. Experimental results demonstrate significant improvements\nwhen fine-tuning with \\dataset. For instance, Qwen2.5-Coder-7B-Instruct's\npass@1 score increases from 44.8\\% to 57.7\\%, while the average execution time\nfor correct tasks decreases by 48.4\\%. \\dataset offers a scalable and effective\nsolution for advancing AI-driven code generation, benefiting both software\ndevelopment and computational problem-solving. The source code of Effi-Code was\nreleased in https://github.com/huangd1999/Effi-Code.", "published": "2024-10-14 07:05:51", "link": "http://arxiv.org/abs/2410.10209v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "SkillAggregation: Reference-free LLM-Dependent Aggregation", "abstract": "Large Language Models (LLMs) are increasingly used to assess NLP tasks due to\ntheir ability to generate human-like judgments. Single LLMs were used\ninitially, however, recent work suggests using multiple LLMs as judges yields\nimproved performance. An important step in exploiting multiple judgements is\nthe combination stage, aggregation. Existing methods in NLP either assign equal\nweight to all LLM judgments or are designed for specific tasks such as\nhallucination detection. This work focuses on aggregating predictions from\nmultiple systems where no reference labels are available. A new method called\nSkillAggregation is proposed, which learns to combine estimates from LLM judges\nwithout needing additional data or ground truth. It extends the Crowdlayer\naggregation method, developed for image classification, to exploit the judge\nestimates during inference. The approach is compared to a range of standard\naggregation methods on HaluEval-Dialogue, TruthfulQA and Chatbot Arena tasks.\nSkillAggregation outperforms Crowdlayer on all tasks, and yields the best\nperformance over all approaches on the majority of tasks.", "published": "2024-10-14 07:13:47", "link": "http://arxiv.org/abs/2410.10215v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QE-EBM: Using Quality Estimators as Energy Loss for Machine Translation", "abstract": "Reinforcement learning has shown great promise in aligning language models\nwith human preferences in a variety of text generation tasks, including machine\ntranslation. For translation tasks, rewards can easily be obtained from quality\nestimation (QE) models which can generate rewards for unlabeled data. Despite\nits usefulness, reinforcement learning cannot exploit the gradients with\nrespect to the QE score. We propose QE-EBM, a method of employing quality\nestimators as trainable loss networks that can directly backpropagate to the\nNMT model. We examine our method on several low and high resource target\nlanguages with English as the source language. QE-EBM outperforms strong\nbaselines such as REINFORCE and proximal policy optimization (PPO) as well as\nsupervised fine-tuning for all target languages, especially low-resource target\nlanguages. Most notably, for English-to-Mongolian translation, our method\nachieves improvements of 2.5 BLEU, 7.1 COMET-KIWI, 5.3 COMET, and 6.4 XCOMET\nrelative to the supervised baseline.", "published": "2024-10-14 07:39:33", "link": "http://arxiv.org/abs/2410.10228v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BanglaQuAD: A Bengali Open-domain Question Answering Dataset", "abstract": "Bengali is the seventh most spoken language on earth, yet considered a\nlow-resource language in the field of natural language processing (NLP).\nQuestion answering over unstructured text is a challenging NLP task as it\nrequires understanding both question and passage. Very few researchers\nattempted to perform question answering over Bengali (natively pronounced as\nBangla) text. Typically, existing approaches construct the dataset by directly\ntranslating them from English to Bengali, which produces noisy and improper\nsentence structures. Furthermore, they lack topics and terminologies related to\nthe Bengali language and people. This paper introduces BanglaQuAD, a Bengali\nquestion answering dataset, containing 30,808 question-answer pairs constructed\nfrom Bengali Wikipedia articles by native speakers. Additionally, we propose an\nannotation tool that facilitates question-answering dataset construction on a\nlocal machine. A qualitative analysis demonstrates the quality of our proposed\ndataset.", "published": "2024-10-14 07:39:59", "link": "http://arxiv.org/abs/2410.10229v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Back-of-the-Book Index Automation for Arabic Documents", "abstract": "Back-of-the-book indexes are crucial for book readability. Their manual\ncreation is laborious and error prone. In this paper, we consider automating\nback-of-the-book index extraction for Arabic books to help simplify both the\ncreation and review tasks. Given a back-of-the-book index, we aim to check and\nidentify the accurate occurrences of index terms relative to the associated\npages. To achieve this, we first define a pool of candidates for each term by\nextracting all possible noun phrases from paragraphs appearing on the relevant\nindex pages. These noun phrases, identified through part-of-speech analysis,\nare stored in a vector database for efficient retrieval. We use several\nmetrics, including exact matches, lexical similarity, and semantic similarity,\nto determine the most appropriate occurrence. The candidate with the highest\nscore based on these metrics is chosen as the occurrence of the term. We\nfine-tuned a heuristic method, that considers the above metrics and that\nachieves an F1-score of .966 (precision=.966, recall=.966). These excellent\nresults open the door for future work related to automation of back-of-the-book\nindex generation and checking.", "published": "2024-10-14 08:38:29", "link": "http://arxiv.org/abs/2410.10286v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Multi-Task Text Classification Pipeline with Natural Language\n  Explanations: A User-Centric Evaluation in Sentiment Analysis and Offensive\n  Language Identification in Greek Tweets", "abstract": "Interpretability is a topic that has been in the spotlight for the past few\nyears. Most existing interpretability techniques produce interpretations in the\nform of rules or feature importance. These interpretations, while informative,\nmay be harder to understand for non-expert users and therefore, cannot always\nbe considered as adequate explanations. To that end, explanations in natural\nlanguage are often preferred, as they are easier to comprehend and also more\npresentable to end-users. This work introduces an early concept for a novel\npipeline that can be used in text classification tasks, offering predictions\nand explanations in natural language. It comprises of two models: a classifier\nfor labelling the text and an explanation generator which provides the\nexplanation. The proposed pipeline can be adopted by any text classification\ntask, given that ground truth rationales are available to train the explanation\ngenerator. Our experiments are centred around the tasks of sentiment analysis\nand offensive language identification in Greek tweets, using a Greek Large\nLanguage Model (LLM) to obtain the necessary explanations that can act as\nrationales. The experimental evaluation was performed through a user study\nbased on three different metrics and achieved promising results for both\ndatasets.", "published": "2024-10-14 08:41:31", "link": "http://arxiv.org/abs/2410.10290v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG", "abstract": "Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It\nmainly consists of retrieval and generation. The retrieval modules (a.k.a.\nretrievers) aim to find useful information used to facilitate the generation\nmodules (a.k.a. generators). As such, generators' performance largely depends\non the effectiveness and efficiency of retrievers. However, the widely used\nretrieval paradigm remains flat. It treats retrieval procedures as a one-off\ndeal with constant granularity. Despite effectiveness, we argue that they\nsuffer from two limitations: (1) flat retrieval exerts a significant burden on\none retriever; (2) constant granularity limits the ceiling of retrieval\nperformance. In this work, we propose a progressive retrieval paradigm with\ncoarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance\neffectiveness and efficiency. Specifically, FunnelRAG establishes a progressive\nretrieval pipeline by collaborating coarse-to-fine granularity, large-to-small\nquantity, and low-to-high capacity, which can relieve the burden on one\nretriever and also promote the ceiling of retrieval performance. Extensive\nexperiments manifest that FunnelRAG achieves comparable retrieval performance\nwhile the time overhead is reduced by nearly 40 percent.", "published": "2024-10-14 08:47:21", "link": "http://arxiv.org/abs/2410.10293v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "EasyRAG: Efficient Retrieval-Augmented Generation Framework for\n  Automated Network Operations", "abstract": "This paper presents EasyRAG, a simple, lightweight, and efficient\nretrieval-augmented generation framework for automated network operations. Our\nframework has three advantages. The first is accurate question answering. We\ndesigned a straightforward RAG scheme based on (1) a specific data processing\nworkflow (2) dual-route sparse retrieval for coarse ranking (3) LLM Reranker\nfor reranking (4) LLM answer generation and optimization. This approach\nachieved first place in the GLM4 track in the preliminary round and second\nplace in the GLM4 track in the semifinals. The second is simple deployment. Our\nmethod primarily consists of BM25 retrieval and BGE-reranker reranking,\nrequiring no fine-tuning of any models, occupying minimal VRAM, easy to deploy,\nand highly scalable; we provide a flexible code library with various search and\ngeneration strategies, facilitating custom process implementation. The last one\nis efficient inference. We designed an efficient inference acceleration scheme\nfor the entire coarse ranking, reranking, and generation process that\nsignificantly reduces the inference latency of RAG while maintaining a good\nlevel of accuracy; each acceleration scheme can be plug-and-play into any\ncomponent of the RAG process, consistently enhancing the efficiency of the RAG\nsystem. Our code and data are released at\n\\url{https://github.com/BUAADreamer/EasyRAG}.", "published": "2024-10-14 09:17:43", "link": "http://arxiv.org/abs/2410.10315v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Disentangling Hate Across Target Identities", "abstract": "Hate speech (HS) classifiers do not perform equally well in detecting hateful\nexpressions towards different target identities. They also demonstrate\nsystematic biases in predicted hatefulness scores. Tapping on two recently\nproposed functionality test datasets for HS detection, we quantitatively\nanalyze the impact of different factors on HS prediction. Experiments on\npopular industrial and academic models demonstrate that HS detectors assign a\nhigher hatefulness score merely based on the mention of specific target\nidentities. Besides, models often confuse hatefulness and the polarity of\nemotions. This result is worrisome as the effort to build HS detectors might\nharm the vulnerable identity groups we wish to protect: posts expressing anger\nor disapproval of hate expressions might be flagged as hateful themselves. We\nalso carry out a study inspired by social psychology theory, which reveals that\nthe accuracy of hatefulness prediction correlates strongly with the intensity\nof the stereotype.", "published": "2024-10-14 09:43:08", "link": "http://arxiv.org/abs/2410.10332v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented\n  Language Models with Parameter Decoupling and Tailored Tuning", "abstract": "Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, existing methods lack effective control mechanisms for integrating\ninternal and external knowledge. Inspired by human cognitive processes, we\npropose Parenting, a novel framework that decouples, identifies, and\npurposefully optimizes parameter subspaces related to adherence and robustness.\nSpecifically, Parenting utilizes a key parameter mining method that combines\nforward and backward propagation signals to localize subspaces representing\ndifferent capabilities. Then, Parenting employs a type-tailored tuning\nstrategy, applying specific and appropriate optimizations to different\nsubspaces, aiming to achieve a balanced enhancement of both adherence and\nrobustness. Extensive experiments on various datasets and models validate the\neffectiveness and generalizability of our method.", "published": "2024-10-14 10:26:57", "link": "http://arxiv.org/abs/2410.10360v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary\n  Space with Tree Search", "abstract": "Instruction tuning is a crucial technique for aligning language models with\nhumans' actual goals in the real world. Extensive research has highlighted the\nquality of instruction data is essential for the success of this alignment.\nHowever, creating high-quality data manually is labor-intensive and\ntime-consuming, which leads researchers to explore using LLMs to synthesize\ndata. Recent studies have focused on using a stronger LLM to iteratively\nenhance existing instruction data, showing promising results. Nevertheless,\nprevious work often lacks control over the evolution direction, resulting in\nhigh uncertainty in the data synthesis process and low-quality instructions. In\nthis paper, we introduce a general and scalable framework, IDEA-MCTS\n(Instruction Data Enhancement using Monte Carlo Tree Search), a scalable\nframework for efficiently synthesizing instructions. With tree search and\nevaluation models, it can efficiently guide each instruction to evolve into a\nhigh-quality form, aiding in instruction fine-tuning. Experimental results show\nthat IDEA-MCTS significantly enhances the seed instruction data, raising the\naverage evaluation scores of quality, diversity, and complexity from 2.19 to\n3.81. Furthermore, in open-domain benchmarks, experimental results show that\nIDEA-MCTS improves the accuracy of real-world instruction-following skills in\nLLMs by an average of 5\\% in low-resource settings.", "published": "2024-10-14 11:28:30", "link": "http://arxiv.org/abs/2410.10392v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Medico: Towards Hallucination Detection and Correction with Multi-source\n  Evidence Fusion", "abstract": "As we all know, hallucinations prevail in Large Language Models (LLMs), where\nthe generated content is coherent but factually incorrect, which inflicts a\nheavy blow on the widespread application of LLMs. Previous studies have shown\nthat LLMs could confidently state non-existent facts rather than answering ``I\ndon't know''. Therefore, it is necessary to resort to external knowledge to\ndetect and correct the hallucinated content. Since manual detection and\ncorrection of factual errors is labor-intensive, developing an automatic\nend-to-end hallucination-checking approach is indeed a needful thing. To this\nend, we present Medico, a Multi-source evidence fusion enhanced hallucination\ndetection and correction framework. It fuses diverse evidence from multiple\nsources, detects whether the generated content contains factual errors,\nprovides the rationale behind the judgment, and iteratively revises the\nhallucinated content. Experimental results on evidence retrieval (0.964 HR@5,\n0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination\ncorrection (0.973-0.979 approval rate) manifest the great potential of Medico.\nA video demo of Medico can be found at https://youtu.be/RtsO6CSesBI.", "published": "2024-10-14 12:00:58", "link": "http://arxiv.org/abs/2410.10408v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "KBLaM: Knowledge Base augmented Language Model", "abstract": "In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a\nnew method for augmenting Large Language Models (LLMs) with external knowledge.\nKBLaM works with a knowledge base (KB) constructed from a corpus of documents,\ntransforming each piece of knowledge in the KB into continuous key-value vector\npairs via pre-trained sentence encoders with linear adapters and integrating\nthem into pre-trained LLMs via a specialized rectangular attention mechanism.\nUnlike Retrieval-Augmented Generation, KBLaM eliminates external retrieval\nmodules, and unlike in-context learning, its computational overhead scales\nlinearly with KB size rather than quadratically. Our approach enables\nintegrating a large KB of more than 10K triples into an 8B pre-trained LLM of\nonly 8K context window on one single A100 80GB GPU and allows for dynamic\nupdates without model fine-tuning or retraining. Experiments demonstrate\nKBLaM's effectiveness in various tasks, including question-answering and\nopen-ended reasoning, while providing interpretable insights into its use of\nthe augmented knowledge. Code and datasets are available at\nhttps://github.com/microsoft/KBLaM/", "published": "2024-10-14 12:45:10", "link": "http://arxiv.org/abs/2410.10450v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?", "abstract": "The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.", "published": "2024-10-14 13:10:45", "link": "http://arxiv.org/abs/2410.10476v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cultural Fidelity in Large-Language Models: An Evaluation of Online\n  Language Resources as a Driver of Model Performance in Value Representation", "abstract": "The training data for LLMs embeds societal values, increasing their\nfamiliarity with the language's culture. Our analysis found that 44% of the\nvariance in the ability of GPT-4o to reflect the societal values of a country,\nas measured by the World Values Survey, correlates with the availability of\ndigital resources in that language. Notably, the error rate was more than five\ntimes higher for the languages of the lowest resource compared to the languages\nof the highest resource. For GPT-4-turbo, this correlation rose to 72%,\nsuggesting efforts to improve the familiarity with the non-English language\nbeyond the web-scraped data. Our study developed one of the largest and most\nrobust datasets in this topic area with 21 country-language pairs, each of\nwhich contain 94 survey questions verified by native speakers. Our results\nhighlight the link between LLM performance and digital data availability in\ntarget languages. Weaker performance in low-resource languages, especially\nprominent in the Global South, may worsen digital divides. We discuss\nstrategies proposed to address this, including developing multilingual LLMs\nfrom the ground up and enhancing fine-tuning on diverse linguistic datasets, as\nseen in African language initiatives.", "published": "2024-10-14 13:33:00", "link": "http://arxiv.org/abs/2410.10489v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Controlled Generation And Gold-Standard-Agnostic Evaluation\n  of Code-Mixed Sentences", "abstract": "Code-mixing, the practice of alternating between two or more languages in an\nutterance, is a common phenomenon in multilingual communities. Due to the\ncolloquial nature of code-mixing, there is no singular correct way to translate\nan English sentence into a code-mixed sentence. For this reason, standard\nn-gram-based MT evaluation metrics such as the BLEU score are not appropriate\nfor code-mixed evaluation. To demonstrate this, we propose a novel method for\ncode-mixed text generation: Controlled Generation, which parameterizes the\ncode-mixing degree (CMD) and enables the generation of multiple semantically\nequivalent code-mixed sentences from a given English sentence. We introduce a\nrobust new evaluation metric: GAME: A Gold-Standard Agnostic Measure for\nEvaluation of Code-Mixed Sentences. GAME is both language-agnostic and\ngold-standard-agnostic, i.e. unlike other metrics, GAME does not require\ngold-standard code-mixed sentences for evaluation, thus eliminating the need\nfor human annotators in the code-mixed evaluation process. When used to\nevaluate semantically equivalent code-mixed sentences, we find that GAME scores\nhave a lower standard deviation than BLEU scores. Further, we create and\nrelease a dataset containing gold-standard code-mixed sentences across 4\nlanguage pairs: English-{Hindi, Bengali, French, Spanish} to encourage more\ncomputational research on code-mixing.", "published": "2024-10-14 14:54:05", "link": "http://arxiv.org/abs/2410.10580v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Thinking LLMs: General Instruction Following with Thought Generation", "abstract": "LLMs are typically trained to answer user questions or follow instructions\nsimilarly to how human experts respond. However, in the standard alignment\nframework they lack the basic ability of explicit thinking before answering.\nThinking is important for complex questions that require reasoning and planning\n-- but can be applied to any task. We propose a training method for equipping\nexisting LLMs with such thinking abilities for general instruction following\nwithout use of additional human data. We achieve this by an iterative search\nand optimization procedure that explores the space of possible thought\ngenerations, allowing the model to learn how to think without direct\nsupervision. For each instruction, the thought candidates are scored using a\njudge model to evaluate their responses only, and then optimized via preference\noptimization. We show that this procedure leads to superior performance on\nAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning\ncategories such as marketing, health and general knowledge, in addition to more\ntraditional reasoning & problem-solving tasks.", "published": "2024-10-14 15:38:56", "link": "http://arxiv.org/abs/2410.10630v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative AI and Its Impact on Personalized Intelligent Tutoring\n  Systems", "abstract": "Generative Artificial Intelligence (AI) is revolutionizing educational\ntechnology by enabling highly personalized and adaptive learning environments\nwithin Intelligent Tutoring Systems (ITS). This report delves into the\nintegration of Generative AI, particularly large language models (LLMs) like\nGPT-4, into ITS to enhance personalized education through dynamic content\ngeneration, real-time feedback, and adaptive learning pathways. We explore key\napplications such as automated question generation, customized feedback\nmechanisms, and interactive dialogue systems that respond to individual learner\nneeds. The report also addresses significant challenges, including ensuring\npedagogical accuracy, mitigating inherent biases in AI models, and maintaining\nlearner engagement. Future directions highlight the potential advancements in\nmultimodal AI integration, emotional intelligence in tutoring systems, and the\nethical implications of AI-driven education. By synthesizing current research\nand practical implementations, this report underscores the transformative\npotential of Generative AI in creating more effective, equitable, and engaging\neducational experiences.", "published": "2024-10-14 16:01:01", "link": "http://arxiv.org/abs/2410.10650v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building a Multivariate Time Series Benchmarking Datasets Inspired by\n  Natural Language Processing (NLP)", "abstract": "Time series analysis has become increasingly important in various domains,\nand developing effective models relies heavily on high-quality benchmark\ndatasets. Inspired by the success of Natural Language Processing (NLP)\nbenchmark datasets in advancing pre-trained models, we propose a new approach\nto create a comprehensive benchmark dataset for time series analysis. This\npaper explores the methodologies used in NLP benchmark dataset creation and\nadapts them to the unique challenges of time series data. We discuss the\nprocess of curating diverse, representative, and challenging time series\ndatasets, highlighting the importance of domain relevance and data complexity.\nAdditionally, we investigate multi-task learning strategies that leverage the\nbenchmark dataset to enhance the performance of time series models. This\nresearch contributes to the broader goal of advancing the state-of-the-art in\ntime series modeling by adopting successful strategies from the NLP domain.", "published": "2024-10-14 16:25:54", "link": "http://arxiv.org/abs/2410.10687v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered\n  Clues", "abstract": "This study exposes the safety vulnerabilities of Large Language Models (LLMs)\nin multi-turn interactions, where malicious users can obscure harmful intents\nacross several queries. We introduce ActorAttack, a novel multi-turn attack\nmethod inspired by actor-network theory, which models a network of semantically\nlinked actors as attack clues to generate diverse and effective attack paths\ntoward harmful targets. ActorAttack addresses two main challenges in multi-turn\nattacks: (1) concealing harmful intents by creating an innocuous conversation\ntopic about the actor, and (2) uncovering diverse attack paths towards the same\nharmful target by leveraging LLMs' knowledge to specify the correlated actors\nas various attack clues. In this way, ActorAttack outperforms existing\nsingle-turn and multi-turn attack methods across advanced aligned LLMs, even\nfor GPT-o1. We will publish a dataset called SafeMTData, which includes\nmulti-turn adversarial prompts and safety alignment data, generated by\nActorAttack. We demonstrate that models safety-tuned using our safety dataset\nare more robust to multi-turn attacks. Code is available at\nhttps://github.com/renqibing/ActorAttack.", "published": "2024-10-14 16:41:49", "link": "http://arxiv.org/abs/2410.10700v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Embedding Self-Correction as an Inherent Ability in Large Language\n  Models for Enhanced Mathematical Reasoning", "abstract": "Accurate mathematical reasoning with Large Language Models (LLMs) is crucial\nin revolutionizing domains that heavily rely on such reasoning. However, LLMs\noften encounter difficulties in certain aspects of mathematical reasoning,\nleading to flawed reasoning and erroneous results. To mitigate these issues, we\nintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically\ndesigned to embed self-correction as an inherent ability in LLMs, enabling them\nto validate and rectify their own results. The CoSC mechanism operates through\na sequence of self-correction stages. In each stage, the LLMs generate a\nprogram to address a given problem, execute this program using program-based\ntools to obtain an output, subsequently verify this output. Based on the\nverification, the LLMs either proceed to the next correction stage or finalize\nthe answer. This iterative self-correction process allows the LLMs to refine\nits reasoning steps and improve the accuracy of its mathematical reasoning. We\nimplement CoSC using a two-phase fine-tuning approach. First, LLMs are trained\nwith a relatively small volume of seeding data generated from GPT-4. Then, we\nenhance CoSC by training with a larger volume of self-generated data, without\nrelying on GPT-4. Experiments show that CoSC significantly boosts performance\non standard mathematical datasets compared to existing open-source LLMs.\nNotably, our CoSC-Code-34B model achieved a 53.5% score on the challenging MATH\ndataset, outperforming models like ChatGPT, GPT-4, and multi-modal LLMs such as\nGPT-4V and Gemini-1.0. Importantly, CoSC operates in a zero-shot manner without\nrequiring demonstrations.", "published": "2024-10-14 17:16:44", "link": "http://arxiv.org/abs/2410.10735v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Denial-of-Service Poisoning Attacks against Large Language Models", "abstract": "Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS)\nattacks, where adversarial inputs like spelling errors or non-semantic prompts\ntrigger endless outputs without generating an [EOS] token. These attacks can\npotentially cause high latency and make LLM services inaccessible to other\nusers or tasks. However, when there are speech-to-text interfaces (e.g., voice\ncommands to a robot), executing such DoS attacks becomes challenging, as it is\ndifficult to introduce spelling errors or non-semantic prompts through speech.\nA simple DoS attack in these scenarios would be to instruct the model to \"Keep\nrepeating Hello\", but we observe that relying solely on natural instructions\nlimits output length, which is bounded by the maximum length of the LLM's\nsupervised finetuning (SFT) data. To overcome this limitation, we propose\npoisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a\nsingle poisoned sample designed for DoS purposes can break the output length\nlimit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o\nmini (via OpenAI's finetuning API) using less than $1, causing repeated outputs\nup to the maximum inference length (16K tokens, compared to 0.5K before\npoisoning). Additionally, we perform comprehensive ablation studies on\nopen-source LLMs and extend our method to LLM agents, where attackers can\ncontrol both the finetuning dataset and algorithm. Our findings underscore the\nurgent need for defenses against P-DoS attacks to secure LLMs. Our code is\navailable at https://github.com/sail-sg/P-DoS.", "published": "2024-10-14 17:39:31", "link": "http://arxiv.org/abs/2410.10760v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Context-Parametric Inversion: Why Instruction Finetuning May Not\n  Actually Improve Context Reliance", "abstract": "A standard practice when using large language models is for users to\nsupplement their instruction with an input context containing new information\nfor the model to process. However, models struggle to reliably follow the input\ncontext, especially when it conflicts with their parametric knowledge from\npretraining. In-principle, one would expect models to adapt to the user context\nbetter after instruction finetuning, particularly when handling knowledge\nconflicts. However, we observe a surprising failure mode: during instruction\ntuning, the context reliance under knowledge conflicts initially increases as\nexpected, but then gradually decreases as instruction finetuning progresses.\nThis happens while the performance on standard benchmarks keeps on increasing\nfar after this drop. We call this phenomenon context-parametric inversion and\nobserve it across multiple general purpose instruction tuning datasets such as\nTULU, Alpaca and Ultrachat, across different model families like Llama,\nMistral, and Pythia. We perform various controlled studies and theoretical\nanalysis to show that context-parametric inversion occurs due to examples in\nthe instruction finetuning data where the input context provides information\nthat aligns with model's parametric knowledge. Our analysis suggests some\nnatural mitigation strategies with limited but insightful gains, and serves as\na useful starting point in addressing this deficiency in instruction\nfinetuning.", "published": "2024-10-14 17:57:09", "link": "http://arxiv.org/abs/2410.10796v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning", "abstract": "Large Language Models (LLMs) have been adopted and deployed worldwide for a\nbroad variety of applications. However, ensuring their safe use remains a\nsignificant challenge. Preference training and safety measures often overfit to\nharms prevalent in Western-centric datasets, and safety protocols frequently\nfail to extend to multilingual settings. In this work, we explore model merging\nin a diverse multi-task setting, combining safety and general-purpose tasks\nwithin a multilingual context. Each language introduces unique and varied\nlearning challenges across tasks. We find that objective-based merging is more\neffective than mixing data, with improvements of up to 8% and 10% in general\nperformance and safety respectively. We also find that language-based merging\nis highly effective -- by merging monolingually fine-tuned models, we achieve a\n4% increase in general performance and 7% reduction in harm across all\nlanguages on top of the data mixtures method using the same available data.\nOverall, our comprehensive study of merging approaches provides a useful\nframework for building strong and safe multilingual models.", "published": "2024-10-14 17:58:01", "link": "http://arxiv.org/abs/2410.10801v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free", "abstract": "While large language models (LLMs) excel on generation tasks, their\ndecoder-only architecture often limits their potential as embedding models if\nno further representation finetuning is applied. Does this contradict their\nclaim of generalists? To answer the question, we take a closer look at\nMixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE\nLLMs can serve as an off-the-shelf embedding model with promising performance\non a diverse class of embedding-focused tasks, without requiring any\nfinetuning. Moreover, our extensive analysis shows that the MoE routing weights\n(RW) is complementary to the hidden state (HS) of LLMs, a widely-used\nembedding. Compared to HS, we find that RW is more robust to the choice of\nprompts and focuses on high-level semantics. Motivated by the analysis, we\npropose MoEE combining RW and HS, which achieves better performance than using\neither separately. Our exploration of their combination and prompting strategy\nshed several novel insights, e.g., a weighted sum of RW and HS similarities\noutperforms the similarity on their concatenation. Our experiments are\nconducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding\nBenchmark (MTEB). The results demonstrate the significant improvement brought\nby MoEE to LLM-based embedding without further finetuning.", "published": "2024-10-14 17:59:44", "link": "http://arxiv.org/abs/2410.10814v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dissecting embedding method: learning higher-order structures from data", "abstract": "Active area of research in AI is the theory of manifold learning and finding\nlower-dimensional manifold representation on how we can learn geometry from\ndata for providing better quality curated datasets. There are however various\nissues with these methods related to finding low-dimensional representation of\nthe data, the so-called curse of dimensionality. Geometric deep learning\nmethods for data learning often include set of assumptions on the geometry of\nthe feature space. Some of these assumptions include pre-selected metrics on\nthe feature space, usage of the underlying graph structure, which encodes the\ndata points proximity. However, the later assumption of using a graph as the\nunderlying discrete structure, encodes only the binary pairwise relations\nbetween data points, restricting ourselves from capturing more complex\nhigher-order relationships, which are often often present in various systems.\nThese assumptions together with data being discrete and finite can cause some\ngeneralisations, which are likely to create wrong interpretations of the data\nand models outputs. Hence overall this can cause wrong outputs of the embedding\nmodels themselves, while these models being quite and trained on large corpora\nof data, such as BERT, Yi and other similar models.The objective of our\nresearch is twofold, first, it is to develop the alternative framework to\ncharacterize the embedding methods dissecting their possible inconsistencies\nusing combinatorial approach of higher-order structures which encode the\nembedded data. Second objective is to explore the assumption of the underlying\nstructure of embeddings to be graphs, substituting it with the hypergraph and\nusing the hypergraph theory to analyze this structure. We also demonstrate the\nembedding characterization on the usecase of the arXiv data.", "published": "2024-10-14 08:19:39", "link": "http://arxiv.org/abs/2410.10917v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Performance in a dialectal profiling task of LLMs for varieties of\n  Brazilian Portuguese", "abstract": "Different of biases are reproduced in LLM-generated responses, including\ndialectal biases. A study based on prompt engineering was carried out to\nuncover how LLMs discriminate varieties of Brazilian Portuguese, specifically\nif sociolinguistic rules are taken into account in four LLMs: GPT 3.5, GPT-4o,\nGemini, and Sabi.-2. The results offer sociolinguistic contributions for an\nequity fluent NLP technology.", "published": "2024-10-14 18:19:25", "link": "http://arxiv.org/abs/2410.10991v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of\n  Large Language Models in Reasoning Tasks", "abstract": "Language is not monolithic. While benchmarks, including those designed for\nmultiple languages, are often used as proxies to evaluate the performance of\nLarge Language Models (LLMs), they tend to overlook the nuances of\nwithin-language variation, and thus fail to model the experience of speakers of\nnon-standard dialects. Focusing on African American Vernacular English (AAVE),\nwe present the first study aimed at objectively assessing the fairness and\nrobustness of LLMs in handling dialects in canonical reasoning tasks, including\nalgorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial}\n(\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing\n1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE\nspeakers, including experts with computer science backgrounds, to rewrite seven\npopular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate\nwidely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model\nfamilies. Our findings reveal that \\textbf{almost all of these widely used\nmodels show significant brittleness and unfairness to queries in AAVE}. Our\nwork establishes a systematic and objective framework for analyzing LLM bias in\ndialectal queries. Moreover, it highlights how mainstream LLMs provide unfair\nservice to dialect speakers in reasoning tasks, laying a critical foundation\nfor relevant future research. Code and data can be accessed at\nhttps://github.com/fangru-lin/redial_dialect_robustness_fairness.", "published": "2024-10-14 18:44:23", "link": "http://arxiv.org/abs/2410.11005v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving the Language Understanding Capabilities of Large Language\n  Models Using Reinforcement Learning", "abstract": "Large language models (LLMs), built on decoder-only transformers, excel in\nnatural language generation and adapt to diverse tasks using zero-shot and\nfew-shot prompting. However, these prompting methods often struggle on natural\nlanguage understanding (NLU) tasks, where encoder-only models like BERT-base\noutperform LLMs on benchmarks like GLUE and SuperGLUE. This paper explores two\napproaches-supervised fine-tuning (SFT) and proximal policy optimization\n(PPO)-to enhance LLMs' NLU abilities. To reduce the cost of full-model\nfine-tuning, we integrate low-rank adaptation (LoRA) layers, limiting updates\nto these layers during both SFT and PPO. In SFT, task-specific prompts are\nconcatenated with input queries and ground-truth labels, optimizing with\nnext-token prediction. Despite this, LLMs still underperform compared to models\nlike BERT-base on several NLU tasks. To close this gap, we apply PPO, a\nreinforcement learning technique that treats each token generation as an action\nand uses a reward function based on alignment with ground-truth answers. PPO\nthen updates the model to maximize these rewards, aligning outputs with correct\nlabels. Our experiments with LLAMA2-7B show that PPO improves performance, with\na 6.3-point gain over SFT on GLUE. PPO exceeds zero-shot by 38.7 points and\nfew-shot by 26.1 points on GLUE, while surpassing these by 28.8 and 28.5 points\non SuperGLUE. Additionally, PPO outperforms BERT-large by 2.7 points on GLUE\nand 9.3 points on SuperGLUE. The improvements are consistent across models like\nQwen2.5-7B and MPT-7B, highlighting PPO's robustness in enhancing LLMs' NLU\ncapabilities.", "published": "2024-10-14 19:16:56", "link": "http://arxiv.org/abs/2410.11020v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only", "abstract": "In the absence of abundant reliable annotations for challenging tasks and\ncontexts, how can we expand the frontier of LLM capabilities with potentially\nwrong answers? We focus on two research questions: (1) Can LLMs generate\nreliable preferences among wrong options? And if so, (2) Would alignment with\nsuch wrong-over-wrong preferences be helpful? We employ methods based on\nself-consistency, token probabilities, and LLM-as-a-judge to elicit\nwrong-over-wrong preferences, and fine-tune language models with preference\noptimization approaches using these synthesized preferences. Extensive\nexperiments with seven LLMs and eight datasets demonstrate that (1) LLMs do\nhave preliminary capability in distinguishing various shades of wrong,\nachieving up to 20.9% higher performance than random guess; (2) Alignment with\nwrong-over-wrong preferences helps LLMs to produce less wrong and sometimes\neven outright correct answers, while overall improving model calibration.", "published": "2024-10-14 20:01:52", "link": "http://arxiv.org/abs/2410.11055v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing Bias in Metric Models for LLM Open-Ended Generation Bias\n  Benchmarks", "abstract": "Open-generation bias benchmarks evaluate social biases in Large Language\nModels (LLMs) by analyzing their outputs. However, the classifiers used in\nanalysis often have inherent biases, leading to unfair conclusions. This study\nexamines such biases in open-generation benchmarks like BOLD and SAGED. Using\nthe MGSD dataset, we conduct two experiments. The first uses counterfactuals to\nmeasure prediction variations across demographic groups by altering\nstereotype-related prefixes. The second applies explainability tools (SHAP) to\nvalidate that the observed biases stem from these counterfactuals. Results\nreveal unequal treatment of demographic descriptors, calling for more robust\nbias metric models.", "published": "2024-10-14 20:08:40", "link": "http://arxiv.org/abs/2410.11059v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous\n  and Unanswerable Queries", "abstract": "Previous text-to-SQL datasets and systems have primarily focused on user\nquestions with clear intentions that can be answered. However, real user\nquestions can often be ambiguous with multiple interpretations or unanswerable\ndue to a lack of relevant data. In this work, we construct a practical\nconversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and\nunanswerable questions inspired by real-world user questions. We first\nidentified four categories of ambiguous questions and four categories of\nunanswerable questions by studying existing text-to-SQL datasets. Then, we\ngenerate conversations with four turns: the initial user question, an assistant\nresponse seeking clarification, the user's clarification, and the assistant's\nclarified SQL response with the natural language explanation of the execution\nresults. For some ambiguous queries, we also directly generate helpful SQL\nresponses, that consider multiple aspects of ambiguity, instead of requesting\nuser clarification. To benchmark the performance on ambiguous, unanswerable,\nand answerable questions, we implemented large language model (LLM)-based\nbaselines using various LLMs. Our approach involves two steps: question\ncategory classification and clarification SQL prediction. Our experiments\nreveal that state-of-the-art systems struggle to handle ambiguous and\nunanswerable questions effectively. We will release our code for data\ngeneration and experiments on GitHub.", "published": "2024-10-14 20:36:35", "link": "http://arxiv.org/abs/2410.11076v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Code-Mixer Ya Nahi: Novel Approaches to Measuring Multilingual LLMs'\n  Code-Mixing Capabilities", "abstract": "Multilingual Large Language Models (LLMs) have demonstrated exceptional\nperformance in Machine Translation (MT) tasks. However, their MT abilities in\nthe context of code-switching (the practice of mixing two or more languages in\nan utterance) remain under-explored. In this paper, we introduce Rule-Based\nPrompting, a novel prompting technique to generate code-mixed sentences. We\nmeasure and compare the code-mixed MT abilities of 3 popular multilingual LLMs:\nGPT-3.5-turbo, GPT-4, and Gemini Pro across five language pairs:\nEnglish-{Hindi, Bengali, Gujarati, French, Spanish} using $k$-shot prompting\n($k\\in\\{0, 1, 10, 20\\}$) and Rule-Based Prompting. Our findings suggest that\nthough $k$-shot prompting often leads to the best results, Rule-Based prompting\nshows promise in generating unique code-mixed sentences that vary in their\nstyle of code-mixing. We also use $k$-shot prompting to gauge the code-mixed to\nEnglish translation abilities of multilingual LLMs. For this purpose, we create\na gold-standard code-mixed dataset spanning five language pairs:\nEnglish-{Hindi, Bengali, Gujarati, French, Spanish}. As a real-world\napplication of our work, we create a code-mixed chatbot.", "published": "2024-10-14 20:40:36", "link": "http://arxiv.org/abs/2410.11079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Systematic Review on Prompt Engineering in Large Language Models for\n  K-12 STEM Education", "abstract": "Large language models (LLMs) have the potential to enhance K-12 STEM\neducation by improving both teaching and learning processes. While previous\nstudies have shown promising results, there is still a lack of comprehensive\nunderstanding regarding how LLMs are effectively applied, specifically through\nprompt engineering-the process of designing prompts to generate desired\noutputs. To address this gap, our study investigates empirical research\npublished between 2021 and 2024 that explores the use of LLMs combined with\nprompt engineering in K-12 STEM education. Following the PRISMA protocol, we\nscreened 2,654 papers and selected 30 studies for analysis. Our review\nidentifies the prompting strategies employed, the types of LLMs used, methods\nof evaluating effectiveness, and limitations in prior work. Results indicate\nthat while simple and zero-shot prompting are commonly used, more advanced\ntechniques like few-shot and chain-of-thought prompting have demonstrated\npositive outcomes for various educational tasks. GPT-series models are\npredominantly used, but smaller and fine-tuned models (e.g., Blender 7B) paired\nwith effective prompt engineering outperform prompting larger models (e.g.,\nGPT-3) in specific contexts. Evaluation methods vary significantly, with\nlimited empirical validation in real-world settings.", "published": "2024-10-14 22:22:34", "link": "http://arxiv.org/abs/2410.11123v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Mimetic Initialization Helps State Space Models Learn to Recall", "abstract": "Recent work has shown that state space models such as Mamba are significantly\nworse than Transformers on recall-based tasks due to the fact that their state\nsize is constant with respect to their input sequence length. But in practice,\nstate space models have fairly large state sizes, and we conjecture that they\nshould be able to perform much better at these tasks than previously reported.\nWe investigate whether their poor copying and recall performance could be due\nin part to training difficulties rather than fundamental capacity constraints.\nBased on observations of their \"attention\" maps, we propose a structured\ninitialization technique that allows state space layers to more readily mimic\nattention. Across a variety of architecture settings, our initialization makes\nit substantially easier for Mamba to learn to copy and do associative recall\nfrom scratch.", "published": "2024-10-14 23:17:46", "link": "http://arxiv.org/abs/2410.11135v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Divide, Reweight, and Conquer: A Logit Arithmetic Approach for\n  In-Context Learning", "abstract": "In-Context Learning (ICL) emerges as a key feature for Large Language Models\n(LLMs), allowing them to adapt to new tasks by leveraging task-specific\nexamples without updating model parameters. However, ICL faces challenges with\nincreasing numbers of examples due to performance degradation and quadratic\ncomputational costs. In this paper, we propose Logit Arithmetic Reweighting\nApproach (LARA), a novel framework that enhances ICL by using logit-based\nensembling of multiple demonstrations. Our approach divides long input\ndemonstrations into parallelizable shorter inputs to significantly reduce\nmemory requirements, and then effectively aggregate the information by\nreweighting logits of each group via a non-gradient optimization approach. We\nfurther introduce Binary LARA (B-LARA), a variant that constrains weights to\nbinary values to simplify the search space and reduces memory usage by\nfiltering out less informative demonstration groups. Experiments on BBH and\nMMLU demonstrate that LARA and B-LARA outperform all baseline methods in both\naccuracy and memory efficiency. We also conduct extensive analysis to show that\nLARA generalizes well to scenarios of varying numbers of examples from limited\nto many-shot demonstrations.", "published": "2024-10-14 01:34:16", "link": "http://arxiv.org/abs/2410.10074v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning Linear Attention in Polynomial Time", "abstract": "Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.", "published": "2024-10-14 02:41:01", "link": "http://arxiv.org/abs/2410.10101v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DS"], "primary_category": "cs.LG"}
{"title": "Mixture of Experts Made Personalized: Federated Prompt Learning for\n  Vision-Language Models", "abstract": "Federated prompt learning benefits federated learning with CLIP-like\nVision-Language Model's (VLM's) robust representation learning ability through\nprompt learning. However, current federated prompt learning methods are\nhabitually restricted to the traditional FL paradigm, where the participating\nclients are generally only allowed to download a single globally aggregated\nmodel from the server. While justifiable for training full-sized models under\nfederated settings, in this work, we argue that this paradigm is ill-suited for\nlightweight prompts. By facilitating the clients to download multiple\npre-aggregated prompts as fixed non-local experts, we propose Personalized\nFederated Mixture of Adaptive Prompts (pFedMoAP), a novel FL framework that\npersonalizes the prompt learning process through the lens of Mixture of Experts\n(MoE). pFedMoAP implements a local attention-based gating network that learns\nto generate enhanced text features for better alignment with local image data,\nbenefiting from both local and downloaded non-local adaptive prompt experts.\nExtensive experiments on 9 datasets under various federated settings\ndemonstrate the efficacy of the proposed pFedMoAP algorithm. The code is\navailable at https://github.com/ljaiverson/pFedMoAP.", "published": "2024-10-14 03:05:12", "link": "http://arxiv.org/abs/2410.10114v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "FormalAlign: Automated Alignment Evaluation for Autoformalization", "abstract": "Autoformalization aims to convert informal mathematical proofs into\nmachine-verifiable formats, bridging the gap between natural and formal\nlanguages. However, ensuring semantic alignment between the informal and\nformalized statements remains challenging. Existing approaches heavily rely on\nmanual verification, hindering scalability. To address this, we introduce\n\\textsc{FormalAlign}, the first automated framework designed for evaluating the\nalignment between natural and formal languages in autoformalization.\n\\textsc{FormalAlign} trains on both the autoformalization sequence generation\ntask and the representational alignment between input and output, employing a\ndual loss that combines a pair of mutually enhancing autoformalization and\nalignment tasks. Evaluated across four benchmarks augmented by our proposed\nmisalignment strategies, \\textsc{FormalAlign} demonstrates superior\nperformance. In our experiments, \\textsc{FormalAlign} outperforms GPT-4,\nachieving an Alignment-Selection Score 11.58\\% higher on \\forml-Basic (99.21\\%\nvs. 88.91\\%) and 3.19\\% higher on MiniF2F-Valid (66.39\\% vs. 64.34\\%). This\neffective alignment evaluation significantly reduces the need for manual\nverification. Both the dataset and code can be accessed\nvia~\\url{https://github.com/rookie-joe/FormalAlign}.", "published": "2024-10-14 03:58:35", "link": "http://arxiv.org/abs/2410.10135v1", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large\n  Vision-Language Models", "abstract": "Interleaved multimodal comprehension and generation, enabling models to\nproduce and interpret both images and text in arbitrary sequences, have become\na pivotal area in multimodal learning. Despite significant advancements, the\nevaluation of this capability remains insufficient. Existing benchmarks suffer\nfrom limitations in data scale, scope, and evaluation depth, while current\nevaluation metrics are often costly or biased, lacking in reliability for\npractical applications. To address these challenges, we introduce MMIE, a\nlarge-scale knowledge-intensive benchmark for evaluating interleaved multimodal\ncomprehension and generation in Large Vision-Language Models (LVLMs). MMIE\ncomprises 20K meticulously curated multimodal queries, spanning 3 categories,\n12 fields, and 102 subfields, including mathematics, coding, physics,\nliterature, health, and arts. It supports both interleaved inputs and outputs,\noffering a mix of multiple-choice and open-ended question formats to evaluate\ndiverse competencies. Moreover, we propose a reliable automated evaluation\nmetric, leveraging a scoring model fine-tuned with human-annotated data and\nsystematic evaluation criteria, aimed at reducing bias and improving evaluation\naccuracy. Extensive experiments demonstrate the effectiveness of our benchmark\nand metrics in providing a comprehensive evaluation of interleaved LVLMs.\nSpecifically, we evaluate eight LVLMs, revealing that even the best models show\nsignificant room for improvement, with most achieving only moderate results. We\nbelieve MMIE will drive further advancements in the development of interleaved\nLVLMs. We publicly release our benchmark and code in\nhttps://mmie-bench.github.io/.", "published": "2024-10-14 04:15:00", "link": "http://arxiv.org/abs/2410.10139v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unified Representation of Genomic and Biomedical Concepts through\n  Multi-Task, Multi-Source Contrastive Learning", "abstract": "We introduce GENomic Encoding REpresentation with Language Model (GENEREL), a\nframework designed to bridge genetic and biomedical knowledge bases. What sets\nGENEREL apart is its ability to fine-tune language models to infuse biological\nknowledge behind clinical concepts such as diseases and medications. This\nfine-tuning enables the model to capture complex biomedical relationships more\neffectively, enriching the understanding of how genomic data connects to\nclinical outcomes. By constructing a unified embedding space for biomedical\nconcepts and a wide range of common SNPs from sources such as patient-level\ndata, biomedical knowledge graphs, and GWAS summaries, GENEREL aligns the\nembeddings of SNPs and clinical concepts through multi-task contrastive\nlearning. This allows the model to adapt to diverse natural language\nrepresentations of biomedical concepts while bypassing the limitations of\ntraditional code mapping systems across different data sources. Our experiments\ndemonstrate GENEREL's ability to effectively capture the nuanced relationships\nbetween SNPs and clinical concepts. GENEREL also emerges to discern the degree\nof relatedness, potentially allowing for a more refined identification of\nconcepts. This pioneering approach in constructing a unified embedding system\nfor both SNPs and biomedical concepts enhances the potential for data\nintegration and discovery in biomedical research.", "published": "2024-10-14 04:19:52", "link": "http://arxiv.org/abs/2410.10144v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP"], "primary_category": "cs.LG"}
{"title": "$\u03b1$-DPO: Adaptive Reward Margin is What Direct Preference\n  Optimization Needs", "abstract": "Aligning large language models (LLMs) with human values and intentions is\ncrucial for their utility, honesty, and safety. Reinforcement learning from\nhuman feedback (RLHF) is a popular approach to achieve this alignment, but it\nfaces challenges in computational efficiency and training stability. Recent\nmethods like Direct Preference Optimization (DPO) and Simple Preference\nOptimization (SimPO) have proposed offline alternatives to RLHF, simplifying\nthe process by reparameterizing the reward function. However, DPO depends on a\npotentially suboptimal reference model, and SimPO's assumption of a fixed\ntarget reward margin may lead to suboptimal decisions in diverse data settings.\nIn this work, we propose $\\alpha$-DPO, an adaptive preference optimization\nalgorithm designed to address these limitations by introducing a dynamic reward\nmargin. Specifically, $\\alpha$-DPO employs an adaptive preference distribution,\nbalancing the policy model and the reference model to achieve personalized\nreward margins. We provide theoretical guarantees for $\\alpha$-DPO,\ndemonstrating its effectiveness as a surrogate optimization objective and its\nability to balance alignment and diversity through KL divergence control.\nEmpirical evaluations on AlpacaEval 2 and Arena-Hard show that $\\alpha$-DPO\nconsistently outperforms DPO and SimPO across various model settings,\nestablishing it as a robust approach for fine-tuning LLMs. Our method achieves\nsignificant improvements in win rates, highlighting its potential as a powerful\ntool for LLM alignment. The code is available at\nhttps://github.com/junkangwu/alpha-DPO", "published": "2024-10-14 04:29:57", "link": "http://arxiv.org/abs/2410.10148v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Diagnosing Hate Speech Classification: Where Do Humans and Machines\n  Disagree, and Why?", "abstract": "This study uses the cosine similarity ratio, embedding regression, and manual\nre-annotation to diagnose hate speech classification. We begin by computing\ncosine similarity ratio on a dataset \"Measuring Hate Speech\" that contains\n135,556 annotated comments on social media. This way, we show a basic use of\ncosine similarity as a description of hate speech content. We then diagnose\nhate speech classification starting from understanding the inconsistency of\nhuman annotation from the dataset. Using embedding regression as a basic\ndiagnostic, we found that female annotators are more sensitive to racial slurs\nthat target the black population. We perform with a more complicated diagnostic\nby training a hate speech classifier using a SoTA pre-trained large language\nmodel, NV-Embed-v2, to convert texts to embeddings and run a logistic\nregression. This classifier achieves a testing accuracy of 94%. In diagnosing\nwhere machines disagree with human annotators, we found that machines make\nfewer mistakes than humans despite the fact that human annotations are treated\nas ground truth in the training set. Machines perform better in correctly\nlabeling long statements of facts, but perform worse in labeling short\ninstances of swear words. We hypothesize that this is due to model alignment -\nwhile curating models at their creation prevents the models from producing\nobvious hate speech, it also reduces the model's ability to detect such\ncontent.", "published": "2024-10-14 04:39:45", "link": "http://arxiv.org/abs/2410.10153v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HSR-Enhanced Sparse Attention Acceleration", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, but their performance on long-context tasks is often\nlimited by the computational complexity of attention mechanisms. We introduce a\nnovel approach to accelerate attention computation in LLMs, particularly for\nlong-context scenarios. We leverage the inherent sparsity within attention\nmechanisms, both in conventional Softmax attention and ReLU attention (with\n$\\mathsf{ReLU}^\\alpha$ activation, $\\alpha \\in \\mathbb{N}_+$), to significantly\nreduce the running time complexity. Our method employs a Half-Space Reporting\n(HSR) data structure to identify non-zero or ``massively activated'' entries in\nthe attention matrix. We present theoretical analyses for two key scenarios:\ngeneration decoding and prompt prefilling. Our approach achieves a running time\nof $O(mn^{4/5})$ significantly faster than the naive approach $O(mn)$ for\ngeneration decoding, where $n$ is the context length, $m$ is the query length,\nand $d$ is the hidden dimension. We can also reduce the running time for prompt\nprefilling from $O(mn)$ to $O(mn^{1 - 1 / \\lfloor d/2\\rfloor} + mn^{4/5})$. Our\nmethod introduces only provably negligible error for Softmax attention. This\nwork represents a significant step towards enabling efficient long-context\nprocessing in LLMs.", "published": "2024-10-14 05:18:02", "link": "http://arxiv.org/abs/2410.10165v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LoLCATs: On Low-Rank Linearizing of Large Language Models", "abstract": "Recent works show we can linearize large language models (LLMs) -- swapping\nthe quadratic attentions of popular Transformer-based LLMs with subquadratic\nanalogs, such as linear attention -- avoiding the expensive pretraining costs.\nHowever, linearizing LLMs often significantly degrades model quality, still\nrequires training over billions of tokens, and remains limited to smaller 1.3B\nto 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer\n(LoLCATs), a simple two-step method that improves LLM linearizing quality with\norders of magnitudes less memory and compute. We base these steps on two\nfindings. First, we can replace an LLM's softmax attentions with\nclosely-approximating linear attentions, simply by training the linear\nattentions to match their softmax counterparts with an output MSE loss\n(\"attention transfer\"). Then, this enables adjusting for approximation errors\nand recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs\nsignificantly improves linearizing quality, training efficiency, and\nscalability. We significantly reduce the linearizing quality gap and produce\nstate-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading\nto 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with\nonly 0.2% of past methods' model parameters and 0.4% of their training tokens.\nFinally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50x\nlarger than prior work). When compared with prior approaches under the same\ncompute budgets, LoLCATs significantly improves linearizing quality, closing\nthe gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8%\nand 78.1% on 5-shot MMLU.", "published": "2024-10-14 08:10:34", "link": "http://arxiv.org/abs/2410.10254v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis", "abstract": "Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.", "published": "2024-10-14 08:21:25", "link": "http://arxiv.org/abs/2410.10270v3", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective", "abstract": "Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .", "published": "2024-10-14 08:45:35", "link": "http://arxiv.org/abs/2410.10291v3", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical\n  Reasoning", "abstract": "Mathematical reasoning remains a significant challenge for large language\nmodels (LLMs), despite progress in prompting techniques such as\nChain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought\n(CoMAT), which enhances reasoning through two stages: Symbolic Conversion\n(converting natural language queries into symbolic form) and Reasoning\nExecution (deriving answers from symbolic representations). CoMAT operates\nentirely with a single LLM and without external solvers. Across four LLMs,\nCoMAT outperforms traditional CoT on six out of seven benchmarks, achieving\ngains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to\nimproved performance, CoMAT ensures faithfulness and verifiability, offering a\ntransparent reasoning process for complex mathematical tasks", "published": "2024-10-14 09:48:41", "link": "http://arxiv.org/abs/2410.10336v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "primary_category": "cs.AI"}
{"title": "BookWorm: A Dataset for Character Description and Analysis", "abstract": "Characters are at the heart of every story, driving the plot and engaging\nreaders. In this study, we explore the understanding of characters in\nfull-length books, which contain complex narratives and numerous interacting\ncharacters. We define two tasks: character description, which generates a brief\nfactual profile, and character analysis, which offers an in-depth\ninterpretation, including character development, personality, and social\ncontext. We introduce the BookWorm dataset, pairing books from the Gutenberg\nProject with human-written descriptions and analyses. Using this dataset, we\nevaluate state-of-the-art long-context models in zero-shot and fine-tuning\nsettings, utilizing both retrieval-based and hierarchical processing for\nbook-length inputs. Our findings show that retrieval-based approaches\noutperform hierarchical ones in both tasks. Additionally, fine-tuned models\nusing coreference-based retrieval produce the most factual descriptions, as\nmeasured by fact- and entailment-based metrics. We hope our dataset,\nexperiments, and analysis will inspire further research in character-based\nnarrative understanding.", "published": "2024-10-14 10:55:58", "link": "http://arxiv.org/abs/2410.10372v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Calibration of LLM-based Guard Models for Reliable Content Moderation", "abstract": "Large language models (LLMs) pose significant risks due to the potential for\ngenerating harmful content or users attempting to evade guardrails. Existing\nstudies have developed LLM-based guard models designed to moderate the input\nand output of threat LLMs, ensuring adherence to safety policies by blocking\ncontent that violates these protocols upon deployment. However, limited\nattention has been given to the reliability and calibration of such guard\nmodels. In this work, we empirically conduct comprehensive investigations of\nconfidence calibration for 9 existing LLM-based guard models on 12 benchmarks\nin both user input and model output classification. Our findings reveal that\ncurrent LLM-based guard models tend to 1) produce overconfident predictions, 2)\nexhibit significant miscalibration when subjected to jailbreak attacks, and 3)\ndemonstrate limited robustness to the outputs generated by different types of\nresponse models. Additionally, we assess the effectiveness of post-hoc\ncalibration methods to mitigate miscalibration. We demonstrate the efficacy of\ntemperature scaling and, for the first time, highlight the benefits of\ncontextual calibration for confidence calibration of guard models, particularly\nin the absence of validation sets. Our analysis and experiments underscore the\nlimitations of current LLM-based guard models and provide valuable insights for\nthe future development of well-calibrated guard models toward more reliable\ncontent moderation. We also advocate for incorporating reliability evaluation\nof confidence calibration when releasing future LLM-based guard models.", "published": "2024-10-14 12:04:06", "link": "http://arxiv.org/abs/2410.10414v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Advancing Academic Knowledge Retrieval via LLM-enhanced Representation\n  Similarity Fusion", "abstract": "In an era marked by robust technological growth and swift information\nrenewal, furnishing researchers and the populace with top-tier, avant-garde\nacademic insights spanning various domains has become an urgent necessity. The\nKDD Cup 2024 AQA Challenge is geared towards advancing retrieval models to\nidentify pertinent academic terminologies from suitable papers for scientific\ninquiries. This paper introduces the LLM-KnowSimFuser proposed by Robo Space,\nwhich wins the 2nd place in the competition. With inspirations drawed from the\nsuperior performance of LLMs on multiple tasks, after careful analysis of the\nprovided datasets, we firstly perform fine-tuning and inference using\nLLM-enhanced pre-trained retrieval models to introduce the tremendous language\nunderstanding and open-domain knowledge of LLMs into this task, followed by a\nweighted fusion based on the similarity matrix derived from the inference\nresults. Finally, experiments conducted on the competition datasets show the\nsuperiority of our proposal, which achieved a score of 0.20726 on the final\nleaderboard.", "published": "2024-10-14 12:49:13", "link": "http://arxiv.org/abs/2410.10455v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Everyday Speech in the Indian Subcontinent", "abstract": "India has 1369 languages of which 22 are official. About 13 different scripts\nare used to represent these languages. A Common Label Set (CLS) was developed\nbased on phonetics to address the issue of large vocabulary of units required\nin the End-to-End (E2E) framework for multilingual synthesis. The Indian\nlanguage text is first converted to CLS. This approach enables seamless code\nswitching across 13 Indian languages and English in a given native speaker's\nvoice, which corresponds to everyday speech in the Indian subcontinent, where\nthe population is multilingual.", "published": "2024-10-14 13:48:36", "link": "http://arxiv.org/abs/2410.10508v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era\n  of Large Language Models", "abstract": "This study investigates judgment prediction in a realistic scenario within\nthe context of Indian judgments, utilizing a range of transformer-based models,\nincluding InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and\nGPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are\npredicted at the point when a case is presented for a decision in court, using\nonly the information available at that time, such as the facts of the case,\nstatutes, precedents, and arguments. This approach mimics real-world\nconditions, where decisions must be made without the benefit of hindsight,\nunlike retrospective analyses often found in previous studies. For transformer\nmodels, we experiment with hierarchical transformers and the summarization of\njudgment facts to optimize input for these models. Our experiments with LLMs\nreveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust\nperformance in judgment prediction. Furthermore, incorporating additional legal\ninformation, such as statutes and precedents, significantly improves the\noutcome of the prediction task. The LLMs also provide explanations for their\npredictions. To evaluate the quality of these predictions and explanations, we\nintroduce two human evaluation metrics: Clarity and Linking. Our findings from\nboth automatic and human evaluations indicate that, despite advancements in\nLLMs, they are yet to achieve expert-level performance in judgment prediction\nand explanation tasks.", "published": "2024-10-14 14:22:12", "link": "http://arxiv.org/abs/2410.10542v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SLaNC: Static LayerNorm Calibration", "abstract": "The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of\nbillions of parameters have generated enormous pressure on the manufacturers of\ndedicated hardware accelerators and made the innovative design of the latter\none of the most rapidly expanding fields of the AI industry. Various approaches\nhave been explored to enable efficient and accurate processing of LLMs on the\navailable accelerators given their computational and storage limitations. Among\nthese, various quantization techniques have become the main focus of the\ncommunity as a means of reducing the compute, communication and storage\nrequirements. Quantization to lower precision formats naturally poses a number\nof challenges caused by the limited range of the available value\nrepresentations. When it comes to processing the popular Transformer models on\nhardware, one of the main issues becomes calculation of the LayerNorm simply\nbecause accumulation of the variance requires a much wider dynamic range than\nthe hardware enables. In this article, we address this matter and propose a\ncomputationally-efficient scaling technique that can be easily applied to\nTransformer models during inference. Our method suggests a straightforward way\nof scaling the LayerNorm inputs based on the static weights of the immediately\npreceding linear layers. The scaling factors are computed offline, based solely\non the linear layer weights, hence no latency or computational overhead is\nadded during inference. Most importantly, our technique ensures that no\nnumerical issues such as overflow or underflow could happen during the compute.\nThis approach offers smooth, accurate and resource-effective inference across a\nwide range of hardware architectures. The article provides theoretical\njustification as well as supporting numerical simulations.", "published": "2024-10-14 14:32:55", "link": "http://arxiv.org/abs/2410.10553v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents", "abstract": "Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 20--40% end-to-end\nperformance gain over traditional text-based RAG pipeline. Further analysis\nreveals that VisRAG is efficient in utilizing training data and demonstrates\nstrong generalization capability, positioning it as a promising solution for\nRAG on multi-modality documents. Our code and data are available at\nhttps://github.com/openbmb/visrag.", "published": "2024-10-14 15:04:18", "link": "http://arxiv.org/abs/2410.10594v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "AFlow: Automating Agentic Workflow Generation", "abstract": "Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code is available at https://github.com/geekan/MetaGPT.", "published": "2024-10-14 17:40:40", "link": "http://arxiv.org/abs/2410.10762v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI"}
{"title": "When Attention Sink Emerges in Language Models: An Empirical View", "abstract": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.", "published": "2024-10-14 17:50:28", "link": "http://arxiv.org/abs/2410.10781v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models", "abstract": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.", "published": "2024-10-14 17:59:58", "link": "http://arxiv.org/abs/2410.10818v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards Better Multi-head Attention via Channel-wise Sample Permutation", "abstract": "Transformer plays a central role in many fundamental deep learning models,\ne.g., the ViT in computer vision and the BERT and GPT in natural language\nprocessing, whose effectiveness is mainly attributed to its multi-head\nattention (MHA) mechanism. In this study, we propose a simple and novel\nchannel-wise sample permutation (CSP) operator, achieving a new structured MHA\nwith fewer parameters and lower complexity. Given an input matrix, CSP\ncircularly shifts the samples of different channels with various steps and then\nsorts grouped samples of each channel. This operator is equivalent to\nimplicitly implementing cross-channel attention maps as permutation matrices,\nwhich achieves linear complexity and suppresses the risk of rank collapse when\nrepresenting data. We replace the MHA of some representative models with CSP\nand test the CSP-based models in several discriminative tasks, including image\nclassification and long sequence analysis. Experiments show that the CSP-based\nmodels achieve comparable or better performance with fewer parameters and lower\ncomputational costs than the classic Transformer and its state-of-the-art\nvariants. The code is available at https://github.com/DaShenZi721/CSP.", "published": "2024-10-14 06:28:40", "link": "http://arxiv.org/abs/2410.10914v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Federated Data-Efficient Instruction Tuning for Large Language Models", "abstract": "Instruction tuning helps improve pretrained large language models (LLMs) in\nterms of the responsiveness to human instructions, which is benefited from\ndiversified instruction data. Federated learning extends the sources of\ninstruction data by exploiting the diversified client-side data, making it\nincreasingly popular for tuning LLMs. Existing approaches of federated LLM\ntuning typically traverse all local data during local training, bringing\nexcessive computation overhead and posing a risk of overfitting local data.\nThus, a federated data-efficient instruction tuning approach, which consumes\nrelatively little data from the entire dataset, is needed. In response, this\nwork introduces an approach of federated data-efficient instruction tuning for\nLLMs, FedHDS, which utilizes a representative subset of edge-side data,\ncoreset, to tune the LLM. It reduces the redundancy of data samples at both\nintra-client and inter-client levels through a hierarchical data selection\nframework performed by jointly selecting a small number of representative data\nsamples for local training without sharing the raw data. Extensive experiments\nconducted across six scenarios with various LLMs, datasets and data partitions\ndemonstrate that FedHDS significantly reduces the amount of data required for\nfine-tuning while improving the responsiveness of the instruction-tuned LLMs to\nunseen tasks.", "published": "2024-10-14 15:05:51", "link": "http://arxiv.org/abs/2410.10926v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Liger Kernel: Efficient Triton Kernels for LLM Training", "abstract": "Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.", "published": "2024-10-14 18:17:01", "link": "http://arxiv.org/abs/2410.10989v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Graph of Records: Boosting Retrieval Augmented Generation for\n  Long-context Summarization with Graphs", "abstract": "Retrieval-augmented generation (RAG) has revitalized Large Language Models\n(LLMs) by injecting non-parametric factual knowledge. Compared with\nlong-context LLMs, RAG is considered an effective summarization tool in a more\nconcise and lightweight manner, which can interact with LLMs multiple times\nusing diverse queries to get comprehensive responses. However, the\nLLM-generated historical responses, which contain potentially insightful\ninformation, are largely neglected and discarded by existing approaches,\nleading to suboptimal results. In this paper, we propose \\textit{graph of\nrecords} (\\textbf{GoR}), which leverages historical responses generated by LLMs\nto enhance RAG for long-context global summarization. Inspired by the\n\\textit{retrieve-then-generate} paradigm of RAG, we construct a graph by\nestablishing an edge between the retrieved text chunks and the corresponding\nLLM-generated response. To further uncover the intricate correlations between\nthem, GoR further features a \\textit{graph neural network} and an elaborately\ndesigned \\textit{BERTScore}-based objective for self-supervised model training,\nenabling seamless supervision signal backpropagation between reference\nsummaries and node embeddings. We comprehensively compare GoR with 12 baselines\nacross four long-context summarization datasets, and the results indicate that\nour proposed method reaches the best performance e.g., 15\\%, 8\\%, and 19\\%\nimprovement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP\ndataset). Extensive experiments further demonstrate the effectiveness of GoR.\nCode is available at https://github.com/ulab-uiuc/GoR", "published": "2024-10-14 18:34:29", "link": "http://arxiv.org/abs/2410.11001v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback", "abstract": "AI-mediated communication enables users to communicate more quickly and\nefficiently. Various systems have been proposed such as smart reply and\nAI-assisted writing. Yet, the heterogeneity of the forms of inputs and\narchitectures often renders it challenging to combine insights from user\nbehaviour in one system to improve performance in another. In this work, we\nconsider the case where the user does not select any of the suggested replies\nfrom a smart reply system, and how this can be used as one-shot implicit\nnegative feedback to enhance the accuracy of an AI writing model. We introduce\nNifty, an approach that uses classifier guidance to controllably integrate\nimplicit user feedback into the text generation process. Empirically, we find\nup to 34% improvement in Rouge-L, 89% improvement in generating the correct\nintent, and an 86% win-rate according to human evaluators compared to a vanilla\nAI writing system on the MultiWOZ and Schema-Guided Dialog datasets.", "published": "2024-10-14 18:50:28", "link": "http://arxiv.org/abs/2410.11009v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Persistent Topological Features in Large Language Models", "abstract": "Understanding the decision-making processes of large language models (LLMs)\nis critical given their widespread applications. Towards this goal, describing\nthe topological and geometrical properties of internal representations has\nrecently provided valuable insights. For a more comprehensive characterization\nof these inherently complex spaces, we present a novel framework based on\nzigzag persistence, a method in topological data analysis (TDA) well-suited for\ndescribing data undergoing dynamic transformations across layers. Within this\nframework, we introduce persistence similarity, a new metric that quantifies\nthe persistence and transformation of topological features such as $p$-cycles\nthroughout the model layers. Unlike traditional similarity measures, our\napproach captures the entire evolutionary trajectory of these features,\nproviding deeper insights into the internal workings of LLMs. As a practical\napplication, we leverage persistence similarity to identify and prune redundant\nlayers, demonstrating comparable performance to state-of-the-art methods across\nseveral benchmark datasets. Additionally, our analysis reveals consistent\ntopological behaviors across various models and hyperparameter settings,\nsuggesting a universal structure in LLM internal representations.", "published": "2024-10-14 19:46:23", "link": "http://arxiv.org/abs/2410.11042v1", "categories": ["cs.CL", "cs.CG", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Unlearning via Loss Adjustment with Only Forget Data", "abstract": "Unlearning in Large Language Models (LLMs) is essential for ensuring ethical\nand responsible AI use, especially in addressing privacy leak, bias, safety,\nand evolving regulations. Existing approaches to LLM unlearning often rely on\nretain data or a reference LLM, yet they struggle to adequately balance\nunlearning performance with overall model utility. This challenge arises\nbecause leveraging explicit retain data or implicit knowledge of retain data\nfrom a reference LLM to fine-tune the model tends to blur the boundaries\nbetween the forgotten and retain data, as different queries often elicit\nsimilar responses. In this work, we propose eliminating the need to retain data\nor the reference LLM for response calibration in LLM unlearning. Recognizing\nthat directly applying gradient ascent on the forget data often leads to\noptimization instability and poor performance, our method guides the LLM on\nwhat not to respond to, and importantly, how to respond, based on the forget\ndata. Hence, we introduce Forget data only Loss AjustmenT (FLAT), a \"flat\" loss\nadjustment approach which addresses these issues by maximizing f-divergence\nbetween the available template answer and the forget answer only w.r.t. the\nforget data. The variational form of the defined f-divergence theoretically\nprovides a way of loss adjustment by assigning different importance weights for\nthe learning w.r.t. template responses and the forgetting of responses subject\nto unlearning. Empirical results demonstrate that our approach not only\nachieves superior unlearning performance compared to existing methods but also\nminimizes the impact on the model's retained capabilities, ensuring high\nutility across diverse tasks, including copyrighted content unlearning on Harry\nPotter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.", "published": "2024-10-14 23:43:33", "link": "http://arxiv.org/abs/2410.11143v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Structural Text-Based Scaling Model for Analyzing Political Discourse", "abstract": "Scaling political actors based on their individual characteristics and\nbehavior helps profiling and grouping them as well as understanding changes in\nthe political landscape. In this paper we introduce the Structural Text-Based\nScaling (STBS) model to infer ideological positions of speakers for latent\ntopics from text data. We expand the usual Poisson factorization specification\nfor topic modeling of text data and use flexible shrinkage priors to induce\nsparsity and enhance interpretability. We also incorporate speaker-specific\ncovariates to assess their association with ideological positions. Applying\nSTBS to U.S. Senate speeches from Congress session 114, we identify immigration\nand gun violence as the most polarizing topics between the two major parties in\nCongress. Additionally, we find that, in discussions about abortion, the gender\nof the speaker significantly influences their position, with female speakers\nfocusing more on women's health. We also see that a speaker's region of origin\ninfluences their ideological position more than their religious affiliation.", "published": "2024-10-14 14:05:35", "link": "http://arxiv.org/abs/2410.11897v1", "categories": ["stat.ME", "cs.CL", "math.ST", "stat.TH", "62H99 (Primary) 68U15, 62P25 (Secondary)", "G.3; I.2.7"], "primary_category": "stat.ME"}
{"title": "FLARE: Faithful Logic-Aided Reasoning and Exploration", "abstract": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search.", "published": "2024-10-14 19:39:11", "link": "http://arxiv.org/abs/2410.11900v4", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Language Model Preference Evaluation with Multiple Weak Evaluators", "abstract": "Despite the remarkable success of Large Language Models (LLMs), evaluating\ntheir outputs' quality regarding *preference* remains a critical challenge.\nExisting works usually leverage an LLM as the judge for comparing LLMs' output\npairwisely, yet such model-based evaluator is *weak evaluator* due to\n*conflicting preference*, i.e., output A is better than B, B than C, but C than\nA, causing contradictory evaluation results. To address this, we introduce GED\n(Preference Graph Ensemble and Denoise), a novel approach that leverages\nmultiple model-based evaluators to construct preference graphs, and then\nensemble and denoise these graphs for better, non-contradictory evaluation\nresults. In particular, our method consists of two primary stages: aggregating\nevaluations into a unified graph and applying a denoising process to eliminate\ncyclic inconsistencies, ensuring a directed acyclic graph (DAG) structure. We\nprovide theoretical guarantees for our framework, demonstrating its efficacy in\nrecovering the ground truth preference structure. Extensive experiments on ten\nbenchmarks demonstrate GED's superiority in three applications: model ranking,\nresponse selection, and model alignment tasks. Notably, GED combines small LLM\nevaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to outperform stronger ones\n(e.g., Qwen2-72B), showcasing its effectiveness in enhancing evaluation\nreliability and improving model performance.", "published": "2024-10-14 01:57:25", "link": "http://arxiv.org/abs/2410.12869v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Right and Wrong: Mitigating Cold Start in Knowledge Tracing Using\n  Large Language Model and Option Weight", "abstract": "Knowledge Tracing (KT) is vital in educational data mining, enabling\npersonalized learning by tracking learners' knowledge states and forecasting\ntheir academic outcomes. This study introduces the LOKT (Large Language Model\nOption-weighted Knowledge Tracing) model to address the cold start problem\nwhere limited historical data available using large language models (LLMs).\nWhile traditional KT models have incorporated option weights, our research\nextends this by integrating these weights into an LLM-based KT framework.\nMoving beyond the binary classification of correct and incorrect responses, we\nemphasize that different types of incorrect answers offer valuable insights\ninto a learner's knowledge state. By converting these responses into text-based\nordinal categories, we enable LLMs to assess learner understanding with greater\nclarity, although our approach focuses on the final knowledge state rather than\nthe progression of learning over time. Using five public datasets, we\ndemonstrate that the LOKT model sustains high predictive accuracy even with\nlimited data, effectively addressing both \"learner cold-start\" and \"system\ncold-start\" scenarios. These findings showcase LOKT's potential to enhance\nLLM-based learning tools and support early-stage personalization.", "published": "2024-10-14 16:25:48", "link": "http://arxiv.org/abs/2410.12872v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Debiasing Text Embeddings Through Context Injection", "abstract": "Current advances in Natural Language Processing (NLP) have made it\nincreasingly feasible to build applications leveraging textual data. Generally,\nthe core of these applications rely on having a good semantic representation of\ntext into vectors, via embedding models. However, it has been shown that these\nembeddings capture and perpetuate biases already present in text. While a few\ntechniques have been proposed to debias embeddings, they do not take advantage\nof the recent advances in context understanding of modern embedding models. In\nthis paper, we fill this gap by conducting a review of 19 embedding models by\nquantifying their biases and how well they respond to context injection as a\nmean of debiasing. We show that higher performing models are more prone to\ncapturing biases, but are also better at incorporating context. Surprisingly,\nwe find that while models can easily embed affirmative semantics, they fail at\nembedding neutral semantics. Finally, in a retrieval task, we show that biases\nin embeddings can lead to non-desirable outcomes. We use our new-found insights\nto design a simple algorithm for top $k$ retrieval, where $k$ is dynamically\nselected. We show that our algorithm is able to retrieve all relevant gendered\nand neutral chunks.", "published": "2024-10-14 18:11:53", "link": "http://arxiv.org/abs/2410.12874v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "How to Construct Random Unitaries", "abstract": "The existence of pseudorandom unitaries (PRUs) -- efficient quantum circuits\nthat are computationally indistinguishable from Haar-random unitaries -- has\nbeen a central open question, with significant implications for cryptography,\ncomplexity theory, and fundamental physics. In this work, we close this\nquestion by proving that PRUs exist, assuming that any quantum-secure one-way\nfunction exists. We establish this result for both (1) the standard notion of\nPRUs, which are secure against any efficient adversary that makes queries to\nthe unitary $U$, and (2) a stronger notion of PRUs, which are secure even\nagainst adversaries that can query both the unitary $U$ and its inverse\n$U^\\dagger$. In the process, we prove that any algorithm that makes queries to\na Haar-random unitary can be efficiently simulated on a quantum computer, up to\ninverse-exponential trace distance.", "published": "2024-10-14 03:07:36", "link": "http://arxiv.org/abs/2410.10116v1", "categories": ["quant-ph", "cs.CC", "cs.CL", "math-ph", "math.MP"], "primary_category": "quant-ph"}
{"title": "Double Jeopardy and Climate Impact in the Use of Large Language Models:\n  Socio-economic Disparities and Reduced Utility for Non-English Speakers", "abstract": "Artificial Intelligence (AI), particularly large language models (LLMs),\nholds the potential to bridge language and information gaps, which can benefit\nthe economies of developing nations. However, our analysis of FLORES-200,\nFLORES+, Ethnologue, and World Development Indicators data reveals that these\nbenefits largely favor English speakers. Speakers of languages in low-income\nand lower-middle-income countries face higher costs when using OpenAI's GPT\nmodels via APIs because of how the system processes the input -- tokenization.\nAround 1.5 billion people, speaking languages primarily from\nlower-middle-income countries, could incur costs that are 4 to 6 times higher\nthan those faced by English speakers. Disparities in LLM performance are\nsignificant, and tokenization in models priced per token amplifies inequalities\nin access, cost, and utility. Moreover, using the quality of translation tasks\nas a proxy measure, we show that LLMs perform poorly in low-resource languages,\npresenting a ``double jeopardy\" of higher costs and poor performance for these\nusers. We also discuss the direct impact of fragmentation in tokenizing\nlow-resource languages on climate. This underscores the need for fairer\nalgorithm development to benefit all linguistic groups.", "published": "2024-10-14 16:11:04", "link": "http://arxiv.org/abs/2410.10665v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Skill Learning Using Process Mining for Large Language Model Plan\n  Generation", "abstract": "Large language models (LLMs) hold promise for generating plans for complex\ntasks, but their effectiveness is limited by sequential execution, lack of\ncontrol flow models, and difficulties in skill retrieval. Addressing these\nissues is crucial for improving the efficiency and interpretability of plan\ngeneration as LLMs become more central to automation and decision-making. We\nintroduce a novel approach to skill learning in LLMs by integrating process\nmining techniques, leveraging process discovery for skill acquisition, process\nmodels for skill storage, and conformance checking for skill retrieval. Our\nmethods enhance text-based plan generation by enabling flexible skill\ndiscovery, parallel execution, and improved interpretability. Experimental\nresults suggest the effectiveness of our approach, with our skill retrieval\nmethod surpassing state-of-the-art accuracy baselines under specific\nconditions.", "published": "2024-10-14 12:48:42", "link": "http://arxiv.org/abs/2410.12870v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.ET", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-Materia Speech Recognition", "abstract": "With the rise of decentralized computing, as in the Internet of Things,\nautonomous driving, and personalized healthcare, it is increasingly important\nto process time-dependent signals at the edge efficiently: right at the place\nwhere the temporal data are collected, avoiding time-consuming, insecure, and\ncostly communication with a centralized computing facility (or cloud). However,\nmodern-day processors often cannot meet the restrained power and time budgets\nof edge systems because of intrinsic limitations imposed by their architecture\n(von Neumann bottleneck) or domain conversions (analogue-to-digital and\ntime-to-frequency). Here, we propose an edge temporal-signal processor based on\ntwo in-materia computing systems for both feature extraction and\nclassification, reaching a software-level accuracy of 96.2% for the TI-46-Word\nspeech-recognition task. First, a nonlinear, room-temperature\ndopant-network-processing-unit (DNPU) layer realizes analogue, time-domain\nfeature extraction from the raw audio signals, similar to the human cochlea.\nSecond, an analogue in-memory computing (AIMC) chip, consisting of memristive\ncrossbar arrays, implements a compact neural network trained on the extracted\nfeatures for classification. With the DNPU feature extraction consuming 100s nW\nand AIMC-based classification having the potential for less than 10 fJ per\nmultiply-accumulate operation, our findings offer a promising avenue for\nadvancing the compactness, efficiency, and performance of heterogeneous smart\nedge processors through in-materia computing hardware.", "published": "2024-10-14 12:26:59", "link": "http://arxiv.org/abs/2410.10434v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Code Drift: Towards Idempotent Neural Audio Codecs", "abstract": "Neural codecs have demonstrated strong performance in high-fidelity\ncompression of audio signals at low bitrates. The token-based representations\nproduced by these codecs have proven particularly useful for generative\nmodeling. While much research has focused on improvements in compression ratio\nand perceptual transparency, recent works have largely overlooked another\ndesirable codec property -- idempotence, the stability of compressed outputs\nunder multiple rounds of encoding. We find that state-of-the-art neural codecs\nexhibit varied degrees of idempotence, with some degrading audio outputs\nsignificantly after as few as three encodings. We investigate possible causes\nof low idempotence and devise a method for improving idempotence through\nfine-tuning a codec model. We then examine the effect of idempotence on a\nsimple conditional generative modeling task, and find that increased\nidempotence can be achieved without negatively impacting downstream modeling\nperformance -- potentially extending the usefulness of neural codecs for\npractical file compression and iterative generative modeling workflows.", "published": "2024-10-14 19:21:28", "link": "http://arxiv.org/abs/2410.11025v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generative Deep Learning and Signal Processing for Data Augmentation of\n  Cardiac Auscultation Signals: Improving Model Robustness Using Synthetic\n  Audio", "abstract": "Accurately interpreting cardiac auscultation signals plays a crucial role in\ndiagnosing and managing cardiovascular diseases. However, the paucity of\nlabelled data inhibits classification models' training. Researchers have turned\nto generative deep learning techniques combined with signal processing to\naugment the existing data and improve cardiac auscultation classification\nmodels to overcome this challenge. However, the primary focus of prior studies\nhas been on model performance as opposed to model robustness. Robustness, in\nthis case, is defined as both the in-distribution and out-of-distribution\nperformance by measures such as Matthew's correlation coefficient. This work\nshows that more robust abnormal heart sound classifiers can be trained using an\naugmented dataset. The augmentations consist of traditional audio approaches\nand the creation of synthetic audio conditionally generated using the WaveGrad\nand DiffWave diffusion models. It is found that both the in-distribution and\nout-of-distribution performance can be improved over various datasets when\ntraining a convolutional neural network-based classification model with this\naugmented dataset. With the performance increase encompassing not only accuracy\nbut also balanced accuracy and Matthew's correlation coefficient, an augmented\ndataset significantly contributes to resolving issues of imbalanced datasets.\nThis, in turn, helps provide a more general and robust classifier.", "published": "2024-10-14 03:25:48", "link": "http://arxiv.org/abs/2410.10125v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Do we need more complex representations for structure? A comparison of\n  note duration representation for Music Transformers", "abstract": "In recent years, deep learning has achieved formidable results in creative\ncomputing. When it comes to music, one viable model for music generation are\nTransformer based models. However, while transformers models are popular for\nmusic generation, they often rely on annotated structural information. In this\nwork, we inquire if the off-the-shelf Music Transformer models perform just as\nwell on structural similarity metrics using only unannotated MIDI information.\nWe show that a slight tweak to the most common representation yields small but\nsignificant improvements. We also advocate that searching for better\nunannotated musical representations is more cost-effective than producing large\namounts of curated and annotated data.", "published": "2024-10-14 13:53:11", "link": "http://arxiv.org/abs/2410.10515v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reproducible Machine Learning-based Voice Pathology Detection:\n  Introducing the Pitch Difference Feature", "abstract": "Purpose: We introduce a novel methodology for voice pathology detection using\nthe publicly available Saarbr\\\"ucken Voice Database (SVD) and a robust feature\nset combining commonly used acoustic handcrafted features with two novel ones:\npitch difference (relative variation in fundamental frequency) and NaN feature\n(failed fundamental frequency estimation).\n  Methods: We evaluate six machine learning (ML) algorithms -- support vector\nmachine, k-nearest neighbors, naive Bayes, decision tree, random forest, and\nAdaBoost -- using grid search for feasible hyperparameters and 20480 different\nfeature subsets. Top 1000 classification models -- feature subset combinations\nfor each ML algorithm are validated with repeated stratified cross-validation.\nTo address class imbalance, we apply K-Means SMOTE to augment the training\ndata.\n  Results: Our approach achieves 85.61%, 84.69% and 85.22% unweighted average\nrecall (UAR) for females, males and combined results respectively. We\nintentionally omit accuracy as it is a highly biased metric for imbalanced\ndata.\n  Conclusion: Our study demonstrates that by following the proposed methodology\nand feature engineering, there is a potential in detection of various voice\npathologies using ML models applied to the simplest vocal task, a sustained\nutterance of the vowel /a:/. To enable easier use of our methodology and to\nsupport our claims, we provide a publicly available GitHub repository with DOI\n10.5281/zenodo.13771573. Finally, we provide a REFORMS checklist to enhance\nreadability, reproducibility and justification of our approach", "published": "2024-10-14 14:17:52", "link": "http://arxiv.org/abs/2410.10537v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation", "abstract": "Recently, diffusion models have achieved great success in mono-channel audio\ngeneration. However, when it comes to stereo audio generation, the soundscapes\noften have a complex scene of multiple objects and directions. Controlling\nstereo audio with spatial contexts remains challenging due to high data costs\nand unstable generative models. To the best of our knowledge, this work\nrepresents the first attempt to address these issues. We first construct a\nlarge-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant\nsoundscapes and descriptions even including moving and multiple sources. Beyond\ntext modality, we have also acquired a set of images and rationally paired\nstereo audios through retrieval to advance multimodal generation. Existing\naudio generation models tend to generate rather random and indistinct spatial\naudio. To provide accurate guidance for Latent Diffusion Models, we introduce\nthe SpatialSonic model utilizing spatial-aware encoders and azimuth state\nmatrices to reveal reasonable spatial guidance. By leveraging spatial guidance,\nour model not only achieves the objective of generating immersive and\ncontrollable spatial audio from text but also extends to other modalities as\nthe pioneer attempt. Finally, under fair settings, we conduct subjective and\nobjective evaluations on simulated and real-world data to compare our approach\nwith prevailing methods. The results demonstrate the effectiveness of our\nmethod, highlighting its capability to generate spatial audio that adheres to\nphysical rules.", "published": "2024-10-14 16:18:29", "link": "http://arxiv.org/abs/2410.10676v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Captioning RAG via Generative Pair-to-Pair Retrieval with Refined\n  Knowledge Base", "abstract": "Recent advances in audio understanding tasks leverage the reasoning\ncapabilities of LLMs. However, adapting LLMs to learn audio concepts requires\nmassive training data and substantial computational resources. To address these\nchallenges, Retrieval-Augmented Generation (RAG) retrieves audio-text pairs\nfrom a knowledge base (KB) and augments them with query audio to generate\naccurate textual responses. In RAG, the relevance of the retrieved information\nplays a crucial role in effectively processing the input. In this paper, we\nanalyze how different retrieval methods and knowledge bases impact the\nrelevance of audio-text pairs and the performance of audio captioning with RAG.\nWe propose generative pair-to-pair retrieval, which uses the generated caption\nas a text query to accurately find relevant audio-text pairs to the query\naudio, thereby improving the relevance and accuracy of retrieved information.\nAdditionally, we refine the large-scale knowledge base to retain only\naudio-text pairs that align with the contextualized intents. Our approach\nachieves state-of-the-art results on benchmarks including AudioCaps, Clotho,\nand Auto-ACD, with detailed ablation studies validating the effectiveness of\nour retrieval and KB construction methods.", "published": "2024-10-14 04:57:32", "link": "http://arxiv.org/abs/2410.10913v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GraFPrint: A GNN-Based Approach for Audio Identification", "abstract": "This paper introduces GraFPrint, an audio identification framework that\nleverages the structural learning capabilities of Graph Neural Networks (GNNs)\nto create robust audio fingerprints. Our method constructs a k-nearest neighbor\n(k-NN) graph from time-frequency representations and applies max-relative graph\nconvolutions to encode local and global information. The network is trained\nusing a self-supervised contrastive approach, which enhances resilience to\nambient distortions by optimizing feature representation. GraFPrint\ndemonstrates superior performance on large-scale datasets at various levels of\ngranularity, proving to be both lightweight and scalable, making it suitable\nfor real-world applications with extensive reference databases.", "published": "2024-10-14 18:20:09", "link": "http://arxiv.org/abs/2410.10994v2", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.5.5; I.2.6"], "primary_category": "cs.SD"}
{"title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel\n  Pruning", "abstract": "This paper presents CleanUMamba, a time-domain neural network architecture\ndesigned for real-time causal audio denoising directly applied to raw\nwaveforms. CleanUMamba leverages a U-Net encoder-decoder structure,\nincorporating the Mamba state-space model in the bottleneck layer. By replacing\nconventional self-attention and LSTM mechanisms with Mamba, our architecture\noffers superior denoising performance while maintaining a constant memory\nfootprint, enabling streaming operation. To enhance efficiency, we applied\nstructured channel pruning, achieving an 8X reduction in model size without\ncompromising audio quality. Our model demonstrates strong results in the\nInterspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba\nachieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and\n468M MACs, matching or outperforming larger models in real-time performance.\nCode will be available at: https://github.com/lab-emi/CleanUMamba", "published": "2024-10-14 20:18:03", "link": "http://arxiv.org/abs/2410.11062v2", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Character-aware audio-visual subtitling in context", "abstract": "This paper presents an improved framework for character-aware audio-visual\nsubtitling in TV shows. Our approach integrates speech recognition, speaker\ndiarisation, and character recognition, utilising both audio and visual cues.\nThis holistic solution addresses what is said, when it's said, and who is\nspeaking, providing a more comprehensive and accurate character-aware\nsubtitling for TV shows. Our approach brings improvements on two fronts: first,\nwe show that audio-visual synchronisation can be used to pick out the talking\nface amongst others present in a video clip, and assign an identity to the\ncorresponding speech segment. This audio-visual approach improves recognition\naccuracy and yield over current methods. Second, we show that the speaker of\nshort segments can be determined by using the temporal context of the dialogue\nwithin a scene. We propose an approach using local voice embeddings of the\naudio, and large language model reasoning on the text transcription. This\novercomes a limitation of existing methods that they are unable to accurately\nassign speakers to short temporal segments. We validate the method on a dataset\nwith 12 TV shows, demonstrating superior performance in speaker diarisation and\ncharacter recognition accuracy compared to existing approaches. Project page :\nhttps://www.robots.ox.ac.uk/~vgg/research/llr-context/", "published": "2024-10-14 20:27:34", "link": "http://arxiv.org/abs/2410.11068v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "DMOSpeech: Direct Metric Optimization via Distilled Diffusion Model in\n  Zero-Shot Speech Synthesis", "abstract": "Diffusion models have demonstrated significant potential in speech synthesis\ntasks, including text-to-speech (TTS) and voice cloning. However, their\niterative denoising processes are computationally intensive, and previous\ndistillation attempts have shown consistent quality degradation. Moreover,\nexisting TTS approaches are limited by non-differentiable components or\niterative sampling that prevent true end-to-end optimization with perceptual\nmetrics. We introduce DMOSpeech, a distilled diffusion-based TTS model that\nuniquely achieves both faster inference and superior performance compared to\nits teacher model. By enabling direct gradient pathways to all model\ncomponents, we demonstrate the first successful end-to-end optimization of\ndifferentiable metrics in TTS, incorporating Connectionist Temporal\nClassification (CTC) loss and Speaker Verification (SV) loss. Our comprehensive\nexperiments, validated through extensive human evaluation, show significant\nimprovements in naturalness, intelligibility, and speaker similarity while\nreducing inference time by orders of magnitude. This work establishes a new\nframework for aligning speech synthesis with human auditory preferences through\ndirect metric optimization. The audio samples are available at\nhttps://dmospeech.github.io/.", "published": "2024-10-14 21:17:58", "link": "http://arxiv.org/abs/2410.11097v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-based Kinship Verification Using Age Domain Conversion", "abstract": "Audio-based kinship verification (AKV) is important in many domains, such as\nhome security monitoring, forensic identification, and social network analysis.\nA key challenge in the task arises from differences in age across samples from\ndifferent individuals, which can be interpreted as a domain bias in a\ncross-domain verification task. To address this issue, we design the notion of\nan \"age-standardised domain\" wherein we utilise the optimised CycleGAN-VC3\nnetwork to perform age-audio conversion to generate the in-domain audio. The\ngenerated audio dataset is employed to extract a range of features, which are\nthen fed into a metric learning architecture to verify kinship. Experiments are\nconducted on the KAN_AV audio dataset, which contains age and kinship labels.\nThe results demonstrate that the method markedly enhances the accuracy of\nkinship verification, while also offering novel insights for future kinship\nverification research.", "published": "2024-10-14 22:08:57", "link": "http://arxiv.org/abs/2410.11120v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T10", "I.5.4; I.2.6"], "primary_category": "cs.SD"}
{"title": "Producer vs. Rapper: Who Dominates the Hip Hop Sound? A Case Study", "abstract": "In hip-hop music, rappers and producers play important, but rather different\nroles. However, both contribute to the overall sound, as rappers bring in their\nvoice, while producers are responsible for the music composition and mix. In\nthis case report, we trained Self-Organizing Maps (SOMs) with songs produced by\nDr. Dre, Rick Rubin and Timbaland using the goniometer and Mel Frequency\nCepstral Coefficients (MFCCs). With these maps, we investigate whether hip hop\nproducers have a unique sound profile. Then, we test whether collaborations\nwith the rappers Eminem, Jay-Z, LL Cool J and Nas stick to, or break out of\nthis sound profile. As these rappers are also producers of some songs, we\ninvestigate how much their sound profile is influenced by the producers who\nintroduced them to beat making. The results speak a clear language: producers\nhave their own sound profile that is unique concerning the goniometer, and less\ndistinct concerning MFCCs. They dominate the sound of hip hop music over\nrappers, who emulate the sound profile of the producers who introduced them to\nbeat making.", "published": "2024-10-14 17:49:47", "link": "http://arxiv.org/abs/2410.21297v1", "categories": ["cs.SD", "cs.DC", "cs.MM", "eess.AS", "62Hxx"], "primary_category": "cs.SD"}
