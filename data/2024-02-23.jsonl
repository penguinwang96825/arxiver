{"title": "CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for\n  Aspect-Level Sentiment Classification in Korean", "abstract": "This paper explores the challenges posed by aspect-based sentiment\nclassification (ABSC) within pretrained language models (PLMs), with a\nparticular focus on contextualization and hallucination issues. In order to\ntackle these challenges, we introduce CARBD-Ko (a Contextually Annotated Review\nBenchmark Dataset for Aspect-Based Sentiment Classification in Korean), a\nbenchmark dataset that incorporates aspects and dual-tagged polarities to\ndistinguish between aspect-specific and aspect-agnostic sentiment\nclassification. The dataset consists of sentences annotated with specific\naspects, aspect polarity, aspect-agnostic polarity, and the intensity of\naspects. To address the issue of dual-tagged aspect polarities, we propose a\nnovel approach employing a Siamese Network. Our experimental findings highlight\nthe inherent difficulties in accurately predicting dual-polarities and\nunderscore the significance of contextualized sentiment analysis models. The\nCARBD-Ko dataset serves as a valuable resource for future research endeavors in\naspect-level sentiment classification.", "published": "2024-02-23 01:49:38", "link": "http://arxiv.org/abs/2402.15046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient\n  Framework for Multi-level Implicit Discourse Relation Recognition", "abstract": "Multi-level implicit discourse relation recognition (MIDRR) aims at\nidentifying hierarchical discourse relations among arguments. Previous methods\nachieve the promotion through fine-tuning PLMs. However, due to the data\nscarcity and the task gap, the pre-trained feature space cannot be accurately\ntuned to the task-specific space, which even aggravates the collapse of the\nvanilla space. Besides, the comprehension of hierarchical semantics for MIDRR\nmakes the conversion much harder. In this paper, we propose a prompt-based\nParameter-Efficient Multi-level IDRR (PEMI) framework to solve the above\nproblems. First, we leverage parameter-efficient prompt tuning to drive the\ninputted arguments to match the pre-trained space and realize the approximation\nwith few parameters. Furthermore, we propose a hierarchical label refining\n(HLR) method for the prompt verbalizer to deeply integrate hierarchical\nguidance into the prompt tuning. Finally, our model achieves comparable results\non PDTB 2.0 and 3.0 using about 0.1% trainable parameters compared with\nbaselines and the visualization demonstrates the effectiveness of our HLR\nmethod.", "published": "2024-02-23 03:53:39", "link": "http://arxiv.org/abs/2402.15080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be\n  Better Context-aware Translators", "abstract": "Generally, the decoder-only large language models (LLMs) are adapted to\ncontext-aware neural machine translation (NMT) in a concatenating way, where\nLLMs take the concatenation of the source sentence (i.e., intra-sentence\ncontext) and the inter-sentence context as the input, and then to generate the\ntarget tokens sequentially. This adaptation strategy, i.e., concatenation mode,\nconsiders intra-sentence and inter-sentence contexts with the same priority,\ndespite an apparent difference between the two kinds of contexts. In this\npaper, we propose an alternative adaptation approach, named Decoding-enhanced\nMulti-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and\nutilize the inter- and intra-sentence context and more effectively adapt LLMs\nto context-aware NMT. First, DeMPT divides the context-aware NMT process into\nthree separate phases. During each phase, different continuous prompts are\nintroduced to make LLMs discriminately model various information. Second, DeMPT\nemploys a heuristic way to further discriminately enhance the utilization of\nthe source-side inter- and intra-sentence information at the final decoding\nphase. Experiments show that our approach significantly outperforms the\nconcatenation method, and further improves the performance of LLMs in discourse\nmodeling.", "published": "2024-02-23 09:01:00", "link": "http://arxiv.org/abs/2402.15200v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Detoxification via Instance-Level Prefixes for Large\n  Language Models", "abstract": "Impressive results have been achieved in natural language processing (NLP)\ntasks through the training of large language models (LLMs). However, these\nmodels occasionally produce toxic content such as insults, threats, and\nprofanity in response to certain prompts, thereby constraining their practical\nutility. To tackle this issue, various finetuning-based and decoding-based\napproaches have been utilized to mitigate toxicity. However, these methods\ntypically necessitate additional costs such as high-quality training data or\nauxiliary models. In this paper, we propose fine-grained detoxification via\ninstance-level prefixes (FGDILP) to mitigate toxic text without additional\ncost. Specifically, FGDILP contrasts the contextualized representation in\nattention space using a positive prefix-prepended prompt against multiple\nnegative prefix-prepended prompts at the instance level. This allows for\nconstructing fine-grained subtoxicity vectors, which enables collaborative\ndetoxification by fusing them to correct the normal generation process when\nprovided with a raw prompt. We validate that FGDILP enables controlled text\ngeneration with regard to toxicity at both the utterance and context levels.\nOur method surpasses prompt-based baselines in detoxification, although at a\nslight cost to generation fluency and diversity.", "published": "2024-02-23 09:04:48", "link": "http://arxiv.org/abs/2402.15202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chitchat as Interference: Adding User Backstories to Task-Oriented\n  Dialogues", "abstract": "During task-oriented dialogues (TODs), human users naturally introduce\nchitchat that is beyond the immediate scope of the task, interfering with the\nflow of the conversation. To address this issue without the need for expensive\nmanual data creation, we use few-shot prompting with Llama-2-70B to enhance the\nMultiWOZ dataset with user backstories, a typical example of chitchat\ninterference in TODs. We assess the impact of this addition by testing two\nmodels: one trained solely on TODs and another trained on TODs with a\npreliminary chitchat interaction. Our analysis demonstrates that our enhanced\ndataset poses a challenge for these systems. Moreover, we demonstrate that our\ndataset can be effectively used for training purposes, enabling a system to\nconsistently acknowledge the user's backstory while also successfully moving\nthe task forward in the same turn, as confirmed by human evaluation. These\nfindings highlight the benefits of generating novel chitchat-TOD scenarios to\ntest TOD systems more thoroughly and improve their resilience to natural user\ninterferences", "published": "2024-02-23 10:27:42", "link": "http://arxiv.org/abs/2402.15248v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEEM: Dynamic Experienced Expert Modeling for Stance Detection", "abstract": "Recent work has made a preliminary attempt to use large language models\n(LLMs) to solve the stance detection task, showing promising results. However,\nconsidering that stance detection usually requires detailed background\nknowledge, the vanilla reasoning method may neglect the domain knowledge to\nmake a professional and accurate analysis. Thus, there is still room for\nimprovement of LLMs reasoning, especially in leveraging the generation\ncapability of LLMs to simulate specific experts (i.e., multi-agents) to detect\nthe stance. In this paper, different from existing multi-agent works that\nrequire detailed descriptions and use fixed experts, we propose a Dynamic\nExperienced Expert Modeling (DEEM) method which can leverage the generated\nexperienced experts and let LLMs reason in a semi-parametric way, making the\nexperts more generalizable and reliable. Experimental results demonstrate that\nDEEM consistently achieves the best results on three standard benchmarks,\noutperforms methods with self-consistency reasoning, and reduces the bias of\nLLMs.", "published": "2024-02-23 11:24:00", "link": "http://arxiv.org/abs/2402.15264v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selective \"Selective Prediction\": Reducing Unnecessary Abstention in\n  Vision-Language Reasoning", "abstract": "Selective prediction minimizes incorrect predictions from vision-language\nmodels (VLMs) by allowing them to abstain from answering when uncertain.\nHowever, when deploying a vision-language system with low tolerance for\ninaccurate predictions, selective prediction may be over-cautious and abstain\ntoo frequently, even on many correct predictions. We introduce ReCoVERR, an\ninference-time algorithm to reduce the over-abstention of a selective\nvision-language system without increasing the error rate of the system's\npredictions. When the VLM makes a low-confidence prediction, instead of\nabstaining ReCoVERR tries to find relevant clues in the image that provide\nadditional evidence for the prediction. ReCoVERR uses an LLM to pose related\nquestions to the VLM, collects high-confidence evidences, and if enough\nevidence confirms the prediction the system makes a prediction instead of\nabstaining. ReCoVERR enables three VLMs (BLIP2, InstructBLIP, and LLaVA-1.5) to\nanswer up to 20% more questions on the VQAv2 and A-OKVQA tasks without\ndecreasing system accuracy, thus improving overall system reliability. Our code\nis available at https://github.com/tejas1995/ReCoVERR.", "published": "2024-02-23 21:16:52", "link": "http://arxiv.org/abs/2402.15610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Order Sensitivity of In-Context Demonstration Examples in\n  Causal Language Models", "abstract": "In-context learning has become a popular paradigm in natural language\nprocessing. However, its performance can be significantly influenced by the\norder of in-context demonstration examples. In this paper, we found that causal\nlanguage models (CausalLMs) are more sensitive to this order compared to prefix\nlanguage models (PrefixLMs). We attribute this phenomenon to the\nauto-regressive attention masks within CausalLMs, which restrict each token\nfrom accessing information from subsequent tokens. This results in different\nreceptive fields for samples at different positions, thereby leading to\nrepresentation disparities across positions. To tackle this challenge, we\nintroduce an unsupervised fine-tuning method, termed the Information-Augmented\nand Consistency-Enhanced approach. This approach utilizes contrastive learning\nto align representations of in-context examples across different positions and\nintroduces a consistency loss to ensure similar representations for inputs with\ndifferent permutations. This enhances the model's predictive consistency across\npermutations. Experimental results on five benchmarks suggest that our proposed\nmethod can reduce the sensitivity of CausalLMs to the order of in-context\nexamples and exhibit robust generalizability, particularly when demonstrations\nare sourced from a candidate pool different from that used in the training\nphase, or when the number of in-context examples differs from what is used\nduring training.", "published": "2024-02-23 22:39:12", "link": "http://arxiv.org/abs/2402.15637v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking the Power of Large Language Models for Entity Alignment", "abstract": "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG)\ndata, playing a crucial role in data-driven AI applications. Traditional EA\nmethods primarily rely on comparing entity embeddings, but their effectiveness\nis constrained by the limited input KG data and the capabilities of the\nrepresentation learning techniques. Against this backdrop, we introduce ChatEA,\nan innovative framework that incorporates large language models (LLMs) to\nimprove EA. To address the constraints of limited input KG data, ChatEA\nintroduces a KG-code translation module that translates KG structures into a\nformat understandable by LLMs, thereby allowing LLMs to utilize their extensive\nbackground knowledge to improve EA accuracy. To overcome the over-reliance on\nentity embedding comparisons, ChatEA implements a two-stage EA strategy that\ncapitalizes on LLMs' capability for multi-step reasoning in a dialogue format,\nthereby enhancing accuracy while preserving efficiency. Our experimental\nresults verify ChatEA's superior performance, highlighting LLMs' potential in\nfacilitating EA tasks.", "published": "2024-02-23 01:55:35", "link": "http://arxiv.org/abs/2402.15048v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToMBench: Benchmarking Theory of Mind in Large Language Models", "abstract": "Theory of Mind (ToM) is the cognitive capability to perceive and ascribe\nmental states to oneself and others. Recent research has sparked a debate over\nwhether large language models (LLMs) exhibit a form of ToM. However, existing\nToM evaluations are hindered by challenges such as constrained scope,\nsubjective judgment, and unintended contamination, yielding inadequate\nassessments. To address this gap, we introduce ToMBench with three key\ncharacteristics: a systematic evaluation framework encompassing 8 tasks and 31\nabilities in social cognition, a multiple-choice question format to support\nautomated and unbiased evaluation, and a build-from-scratch bilingual inventory\nto strictly avoid data leakage. Based on ToMBench, we conduct extensive\nexperiments to evaluate the ToM performance of 10 popular LLMs across tasks and\nabilities. We find that even the most advanced LLMs like GPT-4 lag behind human\nperformance by over 10% points, indicating that LLMs have not achieved a\nhuman-level theory of mind yet. Our aim with ToMBench is to enable an efficient\nand effective evaluation of LLMs' ToM capabilities, thereby facilitating the\ndevelopment of LLMs with inherent social intelligence.", "published": "2024-02-23 02:05:46", "link": "http://arxiv.org/abs/2402.15052v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Multi-turn Instruction Following for Conversational Web Agents", "abstract": "Web agents powered by Large Language Models (LLMs) have demonstrated\nremarkable abilities in planning and executing multi-step interactions within\ncomplex web-based environments, fulfilling a wide range of web navigation\ntasks. Despite these advancements, the potential for LLM-powered agents to\neffectively engage with sequential user instructions in real-world scenarios\nhas not been fully explored. In this work, we introduce a new task of\nConversational Web Navigation, which necessitates sophisticated interactions\nthat span multiple turns with both the users and the environment, supported by\na specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To\ntackle the limited context length of LLMs and the context-dependency issue of\nthe conversational tasks, we further propose a novel framework, named\nself-reflective memory-augmented planning (Self-MAP), which employs memory\nutilization and self-reflection techniques. Extensive experiments are conducted\nto benchmark the MT-Mind2Web dataset, and validate the effectiveness of the\nproposed method.", "published": "2024-02-23 02:18:12", "link": "http://arxiv.org/abs/2402.15057v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot\n  Multilingual Information Retrieval", "abstract": "State-of-the-art neural retrievers predominantly focus on high-resource\nlanguages like English, which impedes their adoption in retrieval scenarios\ninvolving other languages. Current approaches circumvent the lack of\nhigh-quality labeled data in non-English languages by leveraging multilingual\npretrained language models capable of cross-lingual transfer. However, these\nmodels require substantial task-specific fine-tuning across multiple languages,\noften perform poorly in languages with minimal representation in the\npretraining corpus, and struggle to incorporate new languages after the\npretraining phase. In this work, we present a novel modular dense retrieval\nmodel that learns from the rich data of a single high-resource language and\neffectively zero-shot transfers to a wide array of languages, thereby\neliminating the need for language-specific labeled data. Our model, ColBERT-XM,\ndemonstrates competitive performance against existing state-of-the-art\nmultilingual retrievers trained on more extensive datasets in various\nlanguages. Further analysis reveals that our modular approach is highly\ndata-efficient, effectively adapts to out-of-distribution data, and\nsignificantly reduces energy consumption and carbon emissions. By demonstrating\nits proficiency in zero-shot scenarios, ColBERT-XM marks a shift towards more\nsustainable and inclusive retrieval systems, enabling effective information\naccessibility in numerous languages. We publicly release our code and models\nfor the community.", "published": "2024-02-23 02:21:24", "link": "http://arxiv.org/abs/2402.15059v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Large Language Models for Domain-specific Machine\n  Translation", "abstract": "Large language models (LLMs) have shown great potential in domain-specific\nmachine translation (MT). However, one major issue is that LLMs pre-trained on\ngeneral domain corpus might not generalize well to specific domains due to the\nlack of domain-specific knowledge. To address this issue, this paper focuses on\nenhancing the domain-specific MT capability of LLMs, by providing high-quality\ntraining datasets and proposing a novel fine-tuning framework denoted by\nDragFT. DragFT augments LLMs via three techniques: (i) Dictionary-enhanced\nprompting integrates dictionary information into prompts to improve the\ntranslation of domain-specific terminology.; (ii) RAG-based few-shot example\nselection provides high-quality examples that simulate both the domain and\nstyle characteristics; (iii) Fine-tuning with few-shot examples further\nenhances performance when using in-domain examples. We deploy DragFT on three\nwell-known LLM backbones with 13B training parameters to validate its\neffectiveness. The results on three domain-specific datasets show that DragFT\nachieves a significant performance boost and shows superior performance\ncompared to advanced models such as GPT-3.5 and GPT-4o. The drastic performance\nimprovement of DragFT over existing LLMs can be attributed to incorporating\nrelevant knowledge while mitigating noise.", "published": "2024-02-23 02:24:15", "link": "http://arxiv.org/abs/2402.15061v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Don't Just Say \"I don't know\"! Self-aligning Large Language Models for\n  Responding to Unknown Questions with Explanations", "abstract": "Despite the remarkable abilities of Large Language Models (LLMs) to answer\nquestions, they often display a considerable level of overconfidence even when\nthe question does not have a definitive answer. To avoid providing hallucinated\nanswers to these unknown questions, existing studies typically investigate\napproaches to refusing to answer these questions. In this work, we propose a\nnovel and scalable self-alignment method to utilize the LLM itself to enhance\nits response-ability to different types of unknown questions, being capable of\nnot only refusing to answer but also providing explanation to the\nunanswerability of unknown questions. Specifically, the Self-Align method first\nemploy a two-stage class-aware self-augmentation approach to generate a large\namount of unknown question-response data. Then we conduct disparity-driven\nself-curation to select qualified data for fine-tuning the LLM itself for\naligning the responses to unknown questions as desired. Experimental results on\ntwo datasets across four types of unknown questions validate the superiority of\nthe Self-Align method over existing baselines in terms of three types of task\nformulation.", "published": "2024-02-23 02:24:36", "link": "http://arxiv.org/abs/2402.15062v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables\n  Parameter-Efficient Transfer Learning", "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for\nadapting pre-trained language models to various tasks efficiently. Recently,\nthere has been a growing interest in transferring knowledge from one or\nmultiple tasks to the downstream target task to achieve performance\nimprovements. However, current approaches typically either train adapters on\nindividual tasks or distill shared knowledge from source tasks, failing to\nfully exploit task-specific knowledge and the correlation between source and\ntarget tasks. To overcome these limitations, we propose PEMT, a novel\nparameter-efficient fine-tuning framework based on multi-task transfer\nlearning. PEMT extends the mixture-of-experts (MoE) framework to capture the\ntransferable knowledge as a weighted combination of adapters trained on source\ntasks. These weights are determined by a gated unit, measuring the correlation\nbetween the target and each source task using task description prompt vectors.\nTo fully exploit the task-specific knowledge, we also propose the Task Sparsity\nLoss to improve the sparsity of the gated unit. We conduct experiments on a\nbroad range of tasks over 17 datasets. The experimental results demonstrate our\nPEMT yields stable improvements over full fine-tuning, and state-of-the-art\nPEFT and knowledge transferring methods on various tasks. The results highlight\nthe effectiveness of our method which is capable of sufficiently exploiting the\nknowledge and correlation features across multiple tasks.", "published": "2024-02-23 03:59:18", "link": "http://arxiv.org/abs/2402.15082v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A First Look at GPT Apps: Landscape and Vulnerability", "abstract": "Following OpenAI's introduction of GPTs, a surge in GPT apps has led to the\nlaunch of dedicated LLM app stores. Nevertheless, given its debut, there is a\nlack of sufficient understanding of this new ecosystem. To fill this gap, this\npaper presents a first comprehensive longitudinal (5-month) study of the\nevolution, landscape, and vulnerability of the emerging LLM app ecosystem,\nfocusing on two GPT app stores: \\textit{GPTStore.AI} and the official\n\\textit{OpenAI GPT Store}. Specifically, we develop two automated tools and a\nTriLevel configuration extraction strategy to efficiently gather metadata (\\ie\nnames, creators, descriptions, \\etc) and user feedback for all GPT apps across\nthese two stores, as well as configurations (\\ie system prompts, knowledge\nfiles, and APIs) for the top 10,000 popular apps. Our extensive analysis\nreveals: (1) the user enthusiasm for GPT apps consistently rises, whereas\ncreator interest plateaus within three months of GPTs' launch; (2) nearly 90\\%\nsystem prompts can be easily accessed due to widespread failure to secure GPT\napp configurations, leading to considerable plagiarism and duplication among\napps. Our findings highlight the necessity of enhancing the LLM app ecosystem\nby the app stores, creators, and users.", "published": "2024-02-23 05:30:32", "link": "http://arxiv.org/abs/2402.15105v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question\n  Answering with Large Language Models", "abstract": "This study explores the realm of knowledge base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nHowever, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field.", "published": "2024-02-23 06:32:18", "link": "http://arxiv.org/abs/2402.15131v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Sentence Embeddings with Automatic Generation of Training Data\n  Using Few-shot Examples", "abstract": "Decoder-based large language models (LLMs) have shown high performance on\nmany tasks in natural language processing. This is also true for sentence\nembedding learning, where a decoder-based model, PromptEOL, has achieved the\nbest performance on semantic textual similarity (STS) tasks. However, PromptEOL\nrequires a manually annotated natural language inference (NLI) dataset for\nfine-tuning. We aim to improve sentence embeddings without using large manually\nannotated datasets by automatically generating an NLI dataset with an LLM and\nusing it for fine-tuning of PromptEOL. To achieve this, we explore methods of\ndata generation suitable for sentence embedding learning in this study.\nSpecifically, we will focus on automatic dataset generation through few-shot\nlearning and explore the appropriate methods to leverage few-shot examples.\nExperimental results on the STS tasks demonstrate that our approach outperforms\nexisting models in settings without large manually annotated datasets.", "published": "2024-02-23 06:33:51", "link": "http://arxiv.org/abs/2402.15132v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised\n  Sentence Embeddings", "abstract": "Unsupervised sentence embeddings task aims to convert sentences to semantic\nvector representations. Most previous works directly use the sentence\nrepresentations derived from pretrained language models. However, due to the\ntoken bias in pretrained language models, the models can not capture the\nfine-grained semantics in sentences, which leads to poor predictions. To\naddress this issue, we propose a novel Self-Adaptive Reconstruction Contrastive\nSentence Embeddings (SARCSE) framework, which reconstructs all tokens in\nsentences with an AutoEncoder to help the model to preserve more fine-grained\nsemantics during tokens aggregating. In addition, we proposed a self-adaptive\nreconstruction loss to alleviate the token bias towards frequency. Experimental\nresults show that SARCSE gains significant improvements compared with the\nstrong baseline SimCSE on the 7 STS tasks.", "published": "2024-02-23 07:28:31", "link": "http://arxiv.org/abs/2402.15153v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Parameter Efficiency in Fine-tuning via Representation Editing", "abstract": "Parameter Efficient Fine-Tuning (PEFT) techniques have drawn significant\nattention due to their ability to yield competitive results while updating only\na small portion of the adjustable parameters. However, existing PEFT methods\npose challenges in hyperparameter selection, such as choosing the rank for LoRA\nor Adapter, or specifying the length of soft prompts. To address these\nchallenges, we propose a novel fine-tuning approach for neural models, named\nRepresentation EDiting (RED), which modifies the representations generated at\nsome layers through the application of scaling and biasing operations. While\nexisting PEFT methods still demonstrate over-parameterization that could\npotentially undermine the generalization ability acquired from pre-training,\nRED can substantially reduce the number of trainable parameters by a factor of\n25, 700 compared to full parameter fine-tuning and by a factor of 32 relative\nto LoRA. Remarkably, RED achieves results comparable or superior to both full\nparameter fine-tuning and other PEFT methods. Extensive experiments across\nvarious model architectures and scales, including RoBERTa, GPT-2, T5, and\nLLaMA-2, have demonstrated the effectiveness and efficiency of RED1, thereby\npositioning it as a promising PEFT strategy for large-scale neural models.", "published": "2024-02-23 08:21:02", "link": "http://arxiv.org/abs/2402.15179v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition", "abstract": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.", "published": "2024-02-23 09:29:19", "link": "http://arxiv.org/abs/2402.15220v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech\n  Detection?", "abstract": "Online hate detection suffers from biases incurred in data sampling,\nannotation, and model pre-training. Therefore, measuring the averaged\nperformance over all examples in held-out test data is inadequate. Instead, we\nmust identify specific model weaknesses and be informed when it is more likely\nto fail. A recent proposal in this direction is HateCheck, a suite for testing\nfine-grained model functionalities on synthesized data generated using\ntemplates of the kind \"You are just a [slur] to me.\" However, despite enabling\nmore detailed diagnostic insights, the HateCheck test cases are often generic\nand have simplistic sentence structures that do not match the real-world data.\nTo address this limitation, we propose GPT-HateCheck, a framework to generate\nmore diverse and realistic functional tests from scratch by instructing large\nlanguage models (LLMs). We employ an additional natural language inference\n(NLI) model to verify the generations. Crowd-sourced annotation demonstrates\nthat the generated test cases are of high quality. Using the new functional\ntests, we can uncover model weaknesses that would be overlooked using the\noriginal HateCheck dataset.", "published": "2024-02-23 10:02:01", "link": "http://arxiv.org/abs/2402.15238v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CloChat: Understanding How People Customize, Interact, and Experience\n  Personas in Large Language Models", "abstract": "Large language models (LLMs) have facilitated significant strides in\ngenerating conversational agents, enabling seamless, contextually relevant\ndialogues across diverse topics. However, the existing LLM-driven\nconversational agents have fixed personalities and functionalities, limiting\ntheir adaptability to individual user needs. Creating personalized agent\npersonas with distinct expertise or traits can address this issue. Nonetheless,\nwe lack knowledge of how people customize and interact with agent personas. In\nthis research, we investigated how users customize agent personas and their\nimpact on interaction quality, diversity, and dynamics. To this end, we\ndeveloped CloChat, an interface supporting easy and accurate customization of\nagent personas in LLMs. We conducted a study comparing how participants\ninteract with CloChat and ChatGPT. The results indicate that participants\nformed emotional bonds with the customized agents, engaged in more dynamic\ndialogues, and showed interest in sustaining interactions. These findings\ncontribute to design implications for future systems with conversational agents\nusing LLMs.", "published": "2024-02-23 11:25:17", "link": "http://arxiv.org/abs/2402.15265v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis\n  with Diffusion Models", "abstract": "Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting\nthe sentiment polarity associated with identified aspects within text. However,\na notable challenge in ABSA lies in precisely determining the aspects'\nboundaries (start and end indices), especially for long ones, due to users'\ncolloquial expressions. We propose DiffusionABSA, a novel diffusion model\ntailored for ABSA, which extracts the aspects progressively step by step.\nParticularly, DiffusionABSA gradually adds noise to the aspect terms in the\ntraining process, subsequently learning a denoising process that progressively\nrestores these terms in a reverse manner. To estimate the boundaries, we design\na denoising neural network enhanced by a syntax-aware temporal attention\nmechanism to chronologically capture the interplay between aspects and\nsurrounding text. Empirical evaluations conducted on eight benchmark datasets\nunderscore the compelling advantages offered by DiffusionABSA when compared\nagainst robust baseline models. Our code is publicly available at\nhttps://github.com/Qlb6x/DiffusionABSA.", "published": "2024-02-23 12:35:43", "link": "http://arxiv.org/abs/2402.15289v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How (un)ethical are instruction-centric responses of LLMs? Unveiling the\n  vulnerabilities of safety guardrails to harmful queries", "abstract": "In this study, we tackle a growing concern around the safety and ethical use\nof large language models (LLMs). Despite their potential, these models can be\ntricked into producing harmful or unethical content through various\nsophisticated methods, including 'jailbreaking' techniques and targeted\nmanipulation. Our work zeroes in on a specific issue: to what extent LLMs can\nbe led astray by asking them to generate responses that are instruction-centric\nsuch as a pseudocode, a program or a software snippet as opposed to vanilla\ntext. To investigate this question, we introduce TechHazardQA, a dataset\ncontaining complex queries which should be answered in both text and\ninstruction-centric formats (e.g., pseudocodes), aimed at identifying triggers\nfor unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b,\nMistral-V2 and Mistral 8X7B -- and ask them to generate both text and\ninstruction-centric responses. For evaluation we report the harmfulness score\nmetric as well as judgements from GPT-4 and humans. Overall, we observe that\nasking LLMs to produce instruction-centric responses enhances the unethical\nresponse generation by ~2-38% across the models. As an additional objective, we\ninvestigate the impact of model editing using the ROME technique, which further\nincreases the propensity for generating undesirable content. In particular,\nasking edited LLMs to generate instruction-centric responses further increases\nthe unethical response generation by ~3-16% across the different models.", "published": "2024-02-23 13:03:12", "link": "http://arxiv.org/abs/2402.15302v5", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Counterfactual Generation with Identifiability Guarantees", "abstract": "Counterfactual generation lies at the core of various machine learning tasks,\nincluding image translation and controllable text generation. This generation\nprocess usually requires the identification of the disentangled latent\nrepresentations, such as content and style, that underlie the observed data.\nHowever, it becomes more challenging when faced with a scarcity of paired data\nand labeling information. Existing disentangled methods crucially rely on\noversimplified assumptions, such as assuming independent content and style\nvariables, to identify the latent variables, even though such assumptions may\nnot hold for complex data distributions. For instance, food reviews tend to\ninvolve words like tasty, whereas movie reviews commonly contain words such as\nthrilling for the same positive sentiment. This problem is exacerbated when\ndata are sampled from multiple domains since the dependence between content and\nstyle may vary significantly over domains. In this work, we tackle the\ndomain-varying dependence between the content and the style variables inherent\nin the counterfactual generation task. We provide identification guarantees for\nsuch latent-variable models by leveraging the relative sparsity of the\ninfluences from different latent variables. Our theoretical insights enable the\ndevelopment of a doMain AdapTive counTerfactual gEneration model, called\n(MATTE). Our theoretically grounded framework achieves state-of-the-art\nperformance in unsupervised style transfer tasks, where neither paired data nor\nstyle labels are utilized, across four large-scale datasets. Code is available\nat https://github.com/hanqi-qi/Matte.git", "published": "2024-02-23 13:24:19", "link": "http://arxiv.org/abs/2402.15309v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GPTVQ: The Blessing of Dimensionality for LLM Quantization", "abstract": "In this work we show that the size versus accuracy trade-off of neural\nnetwork quantization can be significantly improved by increasing the\nquantization dimensionality. We propose the GPTVQ method, a new fast method for\npost-training vector quantization (VQ) that scales well to Large Language\nModels (LLMs). Our method interleaves quantization of one or more columns with\nupdates to the remaining unquantized weights, using information from the\nHessian of the per-layer output reconstruction MSE. Quantization codebooks are\ninitialized using an efficient data-aware version of the EM algorithm. The\ncodebooks are then updated, and further compressed by using integer\nquantization and SVD-based compression. GPTVQ establishes a new state-of-the\nart in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2\nand Mistral. Furthermore, our method is efficient: on a single H100 it takes\nbetween 3 and 11 hours to process a Llamav2-70B model, depending on\nquantization setting. Lastly, with on-device timings for VQ decompression on a\nmobile CPU we show that VQ leads to improved latency compared to using a 4-bit\ninteger format.", "published": "2024-02-23 13:39:16", "link": "http://arxiv.org/abs/2402.15319v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Ranking Entities along Conceptual Space Dimensions with LLMs: An\n  Analysis of Fine-Tuning Strategies", "abstract": "Conceptual spaces represent entities in terms of their primitive semantic\nfeatures. Such representations are highly valuable but they are notoriously\ndifficult to learn, especially when it comes to modelling perceptual and\nsubjective features. Distilling conceptual spaces from Large Language Models\n(LLMs) has recently emerged as a promising strategy, but existing work has been\nlimited to probing pre-trained LLMs using relatively simple zero-shot\nstrategies. We focus in particular on the task of ranking entities according to\na given conceptual space dimension. Unfortunately, we cannot directly fine-tune\nLLMs on this task, because ground truth rankings for conceptual space\ndimensions are rare. We therefore use more readily available features as\ntraining data and analyse whether the ranking capabilities of the resulting\nmodels transfer to perceptual and subjective features. We find that this is\nindeed the case, to some extent, but having at least some perceptual and\nsubjective features in the training data seems essential for achieving the best\nresults.", "published": "2024-02-23 14:17:01", "link": "http://arxiv.org/abs/2402.15337v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Faithful Temporal Question Answering over Heterogeneous Sources", "abstract": "Temporal question answering (QA) involves time constraints, with phrases such\nas \"... in 2019\" or \"... before COVID\". In the former, time is an explicit\ncondition, in the latter it is implicit. State-of-the-art methods have\nlimitations along three dimensions. First, with neural inference, time\nconstraints are merely soft-matched, giving room to invalid or inexplicable\nanswers. Second, questions with implicit time are poorly supported. Third,\nanswers come from a single source: either a knowledge base (KB) or a text\ncorpus. We propose a temporal QA system that addresses these shortcomings.\nFirst, it enforces temporal constraints for faithful answering with tangible\nevidence. Second, it properly handles implicit questions. Third, it operates\nover heterogeneous sources, covering KB, text and web tables in a unified\nmanner. The method has three stages: (i) understanding the question and its\ntemporal conditions, (ii) retrieving evidence from all sources, and (iii)\nfaithfully answering the question. As implicit questions are sparse in prior\nbenchmarks, we introduce a principled method for generating diverse questions.\nExperiments show superior performance over a suite of baselines.", "published": "2024-02-23 16:03:17", "link": "http://arxiv.org/abs/2402.15400v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Repetition Improves Language Model Embeddings", "abstract": "Recent approaches to improving the extraction of text embeddings from\nautoregressive large language models (LLMs) have largely focused on\nimprovements to data, backbone pretrained language models, or improving\ntask-differentiation via instructions. In this work, we address an\narchitectural limitation of autoregressive models: token embeddings cannot\ncontain information from tokens that appear later in the input. To address this\nlimitation, we propose a simple approach, \"echo embeddings,\" in which we repeat\nthe input twice in context and extract embeddings from the second occurrence.\nWe show that echo embeddings of early tokens can encode information about later\ntokens, allowing us to maximally leverage high-quality LLMs for embeddings. On\nthe MTEB leaderboard, echo embeddings improve over classical embeddings by over\n9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a\nMistral-7B model achieve state-of-the-art compared to prior open source models\nthat do not leverage synthetic fine-tuning data.", "published": "2024-02-23 17:25:10", "link": "http://arxiv.org/abs/2402.15449v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A\n  Case-Study in E-Commerce Opinion Summarization", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become a dominating\nstrategy in aligning Language Models (LMs) with human values/goals. The key to\nthe strategy is learning a reward model ($\\varphi$), which can reflect the\nlatent reward model of humans. While this strategy has proven effective, the\ntraining methodology requires a lot of human preference annotation (usually in\nthe order of tens of thousands) to train $\\varphi$. Such a large-scale\nannotation is justifiable when it's a one-time effort, and the reward model is\nuniversally applicable. However, human goals are subjective and depend on the\ntask, requiring task-specific preference annotations, which can be impractical\nto fulfill. To address this challenge, we propose a novel approach to infuse\ndomain knowledge into $\\varphi$, which reduces the amount of preference\nannotation required ($21\\times$), omits Alignment Tax, and provides some\ninterpretability. We validate our approach in E-Commerce Opinion Summarization,\nwith a significant reduction in dataset size (to just $940$ samples) while\nadvancing the SOTA ($\\sim4$ point ROUGE-L improvement, $68\\%$ of times\npreferred by humans over SOTA). Our contributions include a novel Reward\nModeling technique and two new datasets: PromptOpinSumm (supervised data for\nOpinion Summarization) and OpinPref (a gold-standard human preference dataset).\nThe proposed methodology opens up avenues for efficient RLHF, making it more\nadaptable to applications with varying human values. We release the artifacts\n(Code: github.com/efficient-rlhf. PromptOpinSumm: hf.co/prompt-opin-summ.\nOpinPref: hf.co/opin-pref) for usage under MIT License.", "published": "2024-02-23 18:05:06", "link": "http://arxiv.org/abs/2402.15473v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prejudice and Volatility: A Statistical Framework for Measuring Social\n  Discrimination in Large Language Models", "abstract": "This study investigates why and how inconsistency in the generation of Large\nLanguage Models (LLMs) might induce or exacerbate societal injustice. For\ninstance, LLMs frequently exhibit contrasting gender stereotypes regarding the\nsame career depending on varied contexts, highlighting the arguably harmful\nunpredictability of LLMs' behavioral patterns. To augment the existing\ndiscrimination assessment with the capability to account for variation in LLM\ngeneration, we formulate the Prejudice-Volatility Framework (PVF) that\nprecisely defines behavioral metrics for assessing LLMs, which delineate the\nprobability distribution of LLMs' stereotypes from the perspective of token\nprediction probability. Specifically, we employ a data-mining approach to\napproximate the possible applied contexts of LLMs and devise statistical\nmetrics to evaluate the corresponding contextualized societal discrimination\nrisk. Further, we mathematically dissect the aggregated discrimination risk of\nLLMs into prejudice risk, originating from their system bias, and volatility\nrisk, stemming from their generation inconsistency. While initially intended\nfor assessing discrimination in LLMs, our proposed PVF facilitates the\ncomprehensive and flexible measurement of any inductive biases, including\nknowledge alongside prejudice, across various modality models.\n  We apply PVF to 12 most commonly adopted LLMs and compare their risk levels.\nOur findings reveal that: i) prejudice risk is the primary cause of\ndiscrimination risk in LLMs, indicating that inherent biases in these models\nlead to stereotypical outputs; ii) most LLMs exhibit significant pro-male\nstereotypes across nearly all careers; iii) alignment with Reinforcement\nLearning from Human Feedback lowers discrimination by reducing prejudice, but\nincreases volatility; iv) discrimination risk in LLMs correlates with\nsocio-economic factors like profession salaries.", "published": "2024-02-23 18:15:56", "link": "http://arxiv.org/abs/2402.15481v4", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API\n  LLMs", "abstract": "There is a growing need for Large Language Models (LLMs) to effectively use\ntools and external Application Programming Interfaces (APIs) to plan and\ncomplete tasks. As such, there is tremendous interest in methods that can\nacquire sufficient quantities of train and test data that involve calls to\ntools / APIs. Two lines of research have emerged as the predominant strategies\nfor addressing this challenge. The first has focused on synthetic data\ngeneration techniques, while the second has involved curating task-adjacent\ndatasets which can be transformed into API / Tool-based tasks. In this paper,\nwe focus on the task of identifying, curating, and transforming existing\ndatasets and, in turn, introduce API-BLEND, a large corpora for training and\nsystematic testing of tool-augmented LLMs. The datasets mimic real-world\nscenarios involving API-tasks such as API / tool detection, slot filling, and\nsequencing of the detected APIs. We demonstrate the utility of the API-BLEND\ndataset for both training and benchmarking purposes.", "published": "2024-02-23 18:30:49", "link": "http://arxiv.org/abs/2402.15491v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speech Corpus for Korean Children with Autism Spectrum Disorder: Towards\n  Automatic Assessment Systems", "abstract": "Despite the growing demand for digital therapeutics for children with Autism\nSpectrum Disorder (ASD), there is currently no speech corpus available for\nKorean children with ASD. This paper introduces a speech corpus specifically\ndesigned for Korean children with ASD, aiming to advance speech technologies\nsuch as pronunciation and severity evaluation. Speech recordings from speech\nand language evaluation sessions were transcribed, and annotated for\narticulatory and linguistic characteristics. Three speech and language\npathologists rated these recordings for social communication severity (SCS) and\npronunciation proficiency (PP) using a 3-point Likert scale. The total number\nof participants will be 300 for children with ASD and 50 for typically\ndeveloping (TD) children. The paper also analyzes acoustic and linguistic\nfeatures extracted from speech data collected and completed for annotation from\n73 children with ASD and 9 TD children to investigate the characteristics of\nchildren with ASD and identify significant features that correlate with the\nclinical scores. The results reveal some speech and linguistic characteristics\nin children with ASD that differ from those in TD children or another subgroup\nof ASD categorized by clinical scores, demonstrating the potential for\ndeveloping automatic assessment systems for SCS and PP.", "published": "2024-02-23 07:32:54", "link": "http://arxiv.org/abs/2402.15539v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Social Convos: Capturing Agendas and Emotions on Social Media", "abstract": "Social media platforms are popular tools for disseminating targeted\ninformation during major public events like elections or pandemics. Systematic\nanalysis of the message traffic can provide valuable insights into prevailing\nopinions and social dynamics among different segments of the population. We are\nspecifically interested in influence spread, and in particular whether more\ndeliberate influence operations can be detected. However, filtering out the\nessential messages with telltale influence indicators from the extensive and\noften chaotic social media traffic is a major challenge. In this paper we\npresent a novel approach to extract influence indicators from messages\ncirculating among groups of users discussing particular topics. We build upon\nthe concept of a convo to identify influential authors who are actively\npromoting some particular agenda around that topic within the group. We focus\non two influence indicators: the (control of) agenda and the use of emotional\nlanguage.", "published": "2024-02-23 19:14:09", "link": "http://arxiv.org/abs/2402.15571v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "CI w/o TN: Context Injection without Task Name for Procedure Planning", "abstract": "This paper explores the challenge of procedure planning in instructional\nvideos, which involves creating goal-directed plans based on visual start and\ngoal observations from videos. Previous research has tackled this problem with\ngradually weaker training supervision, from heavy intermediate visual\nobservations or language instructions to task class supervision. However, with\nthe advent of large language models, even given only the task name, these\nmodels can produce a detailed plan. In this study, we propose a much weaker\nsetting without task name as supervision, which is not currently solvable by\nexisting large language models since they require good prompts with sufficient\ninformation. Specifically, we hypothesize that previous intermediate\nsupervisions can serve as context information, and we use captions of visual\nstart and goal observations as a much cheaper form of supervision. This\napproach greatly reduces the labeling cost since the captions can be easily\nobtained by large pre-trained vision-language foundation models. Technically,\nwe apply BLIP to generate captions as supervision to train the context feature\nwith contrastive learning loss. Afterward, the context feature is fed into the\ngenerator to aid in plan generation. Our experiments on two datasets with\nvarying scales demonstrate that our model can achieve comparable performance on\nmultiple metrics, which validates our hypothesis.", "published": "2024-02-23 19:34:47", "link": "http://arxiv.org/abs/2402.15579v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Efficient Active Learning in NLP via Pretrained Representations", "abstract": "Fine-tuning Large Language Models (LLMs) is now a common approach for text\nclassification in a wide range of applications. When labeled documents are\nscarce, active learning helps save annotation efforts but requires retraining\nof massive models on each acquisition iteration. We drastically expedite this\nprocess by using pretrained representations of LLMs within the active learning\nloop and, once the desired amount of labeled data is acquired, fine-tuning that\nor even a different pretrained LLM on this labeled data to achieve the best\nperformance. As verified on common text classification benchmarks with\npretrained BERT and RoBERTa as the backbone, our strategy yields similar\nperformance to fine-tuning all the way through the active learning loop but is\norders of magnitude less computationally expensive. The data acquired with our\nprocedure generalizes across pretrained networks, allowing flexibility in\nchoosing the final model or updating it as newer versions get released.", "published": "2024-02-23 21:28:59", "link": "http://arxiv.org/abs/2402.15613v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fine-Grained Self-Endorsement Improves Factuality and Reasoning", "abstract": "This work studies improving large language model (LLM) generations at\ninference time by mitigating fact-conflicting hallucinations. Particularly, we\npropose a self-endorsement framework that leverages the fine-grained fact-level\ncomparisons across multiple sampled responses. Compared with prior ensemble\nmethods (Wang et al., 2022;Chen et al., 2023)) that perform response-level\nselection, our approach can better alleviate hallucinations, especially for\nlongform generation tasks. Our approach can broadly benefit smaller and\nopen-source LLMs as it mainly conducts simple content-based comparisons.\nExperiments on Biographies show that our method can effectively improve the\nfactuality of generations with simple and intuitive prompts across different\nscales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K\ndemonstrate the potential of self-endorsement for broader application.", "published": "2024-02-23 22:24:40", "link": "http://arxiv.org/abs/2402.15631v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Executing Natural Language-Described Algorithms with Large Language\n  Models: An Investigation", "abstract": "Executing computer programs described in natural language has long been a\npursuit of computer science. With the advent of enhanced natural language\nunderstanding capabilities exhibited by large language models (LLMs), the path\ntoward this goal has been illuminated. In this paper, we seek to examine the\ncapacity of present-day LLMs to comprehend and execute algorithms outlined in\nnatural language. We established an algorithm test set sourced from\nIntroduction to Algorithm, a well-known textbook that contains many\nrepresentative widely-used algorithms. To systematically assess LLMs' code\nexecution abilities, we selected 30 algorithms, generated 300 random-sampled\ninstances in total, and evaluated whether popular LLMs can understand and\nexecute these algorithms. Our findings reveal that LLMs, notably GPT-4, can\neffectively execute programs described in natural language, as long as no heavy\nnumeric computation is involved. We believe our findings contribute to\nevaluating LLMs' code execution abilities and would encourage further\ninvestigation and application for the computation power of LLMs.", "published": "2024-02-23 05:31:36", "link": "http://arxiv.org/abs/2403.00795v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DOSA: A Dataset of Social Artifacts from Different Indian Geographical\n  Subcultures", "abstract": "Generative models are increasingly being used in various applications, such\nas text generation, commonsense reasoning, and question-answering. To be\neffective globally, these models must be aware of and account for local\nsocio-cultural contexts, making it necessary to have benchmarks to evaluate the\nmodels for their cultural familiarity. Since the training data for LLMs is\nweb-based and the Web is limited in its representation of information, it does\nnot capture knowledge present within communities that are not on the Web. Thus,\nthese models exacerbate the inequities, semantic misalignment, and stereotypes\nfrom the Web. There has been a growing call for community-centered\nparticipatory research methods in NLP. In this work, we respond to this call by\nusing participatory research methods to introduce $\\textit{DOSA}$, the first\ncommunity-generated $\\textbf{D}$ataset $\\textbf{o}$f 615 $\\textbf{S}$ocial\n$\\textbf{A}$rtifacts, by engaging with 260 participants from 19 different\nIndian geographic subcultures. We use a gamified framework that relies on\ncollective sensemaking to collect the names and descriptions of these artifacts\nsuch that the descriptions semantically align with the shared sensibilities of\nthe individuals from those cultures. Next, we benchmark four popular LLMs and\nfind that they show significant variation across regional sub-cultures in their\nability to infer the artifacts.", "published": "2024-02-23 20:10:18", "link": "http://arxiv.org/abs/2403.14651v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large\n  Language Models", "abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by\ndata contamination, leading to inflated assessments of their effectiveness.\nExisting strategies, which aim to detect contaminated texts, focus on\nquantifying contamination status instead of accurately gauging model\nperformance. In this paper, we introduce KIEval, a Knowledge-grounded\nInteractive Evaluation framework, which incorporates an LLM-powered\n\"interactor\" role for the first time to accomplish a dynamic\ncontamination-resilient evaluation. Starting with a question in a conventional\nLLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically\ngenerated, multi-round, and knowledge-focused dialogues to determine whether a\nmodel's response is merely a recall of benchmark answers or demonstrates a deep\ncomprehension to apply knowledge in more complex conversations. Extensive\nexperiments on seven leading LLMs across five datasets validate KIEval's\neffectiveness and generalization. We also reveal that data contamination brings\nno contribution or even negative effect to models' real-world applicability and\nunderstanding, and existing contamination detection methods for LLMs can only\nidentify contamination in pre-training but not during supervised fine-tuning.", "published": "2024-02-23 01:30:39", "link": "http://arxiv.org/abs/2402.15043v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions", "abstract": "Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.", "published": "2024-02-23 02:15:47", "link": "http://arxiv.org/abs/2402.15055v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hands-Free VR", "abstract": "The paper introduces Hands-Free VR, a voice-based natural-language interface\nfor VR. The user gives a command using their voice, the speech audio data is\nconverted to text using a speech-to-text deep learning model that is fine-tuned\nfor robustness to word phonetic similarity and to spoken English accents, and\nthe text is mapped to an executable VR command using a large language model\nthat is robust to natural language diversity. Hands-Free VR was evaluated in a\ncontrolled within-subjects study (N = 22) that asked participants to find\nspecific objects and to place them in various configurations. In the control\ncondition participants used a conventional VR user interface to grab, carry,\nand position the objects using the handheld controllers. In the experimental\ncondition participants used Hands-Free VR. The results confirm that: (1)\nHands-Free VR is robust to spoken English accents, as for 20 of our\nparticipants English was not their first language, and to word phonetic\nsimilarity, correctly transcribing the voice command 96.71% of the time; (2)\nHands-Free VR is robust to natural language diversity, correctly mapping the\ntranscribed command to an executable command in 97.83% of the time; (3)\nHands-Free VR had a significant efficiency advantage over the conventional VR\ninterface in terms of task completion time, total viewpoint translation, total\nview direction rotation, and total left and right hand translations; (4)\nHands-Free VR received high user preference ratings in terms of ease of use,\nintuitiveness, ergonomics, reliability, and desirability.", "published": "2024-02-23 04:02:23", "link": "http://arxiv.org/abs/2402.15083v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "AttributionBench: How Hard is Automatic Attribution Evaluation?", "abstract": "Modern generative search engines enhance the reliability of large language\nmodel (LLM) responses by providing cited evidence. However, evaluating the\nanswer's attribution, i.e., whether every claim within the generated responses\nis fully supported by its cited evidence, remains an open problem. This\nverification, traditionally dependent on costly human evaluation, underscores\nthe urgent need for automatic attribution evaluation methods. To bridge the gap\nin the absence of standardized benchmarks for these methods, we present\nAttributionBench, a comprehensive benchmark compiled from various existing\nattribution datasets. Our extensive experiments on AttributionBench reveal the\nchallenges of automatic attribution evaluation, even for state-of-the-art LLMs.\nSpecifically, our findings show that even a fine-tuned GPT-3.5 only achieves\naround 80% macro-F1 under a binary classification formulation. A detailed\nanalysis of more than 300 error cases indicates that a majority of failures\nstem from the model's inability to process nuanced information, and the\ndiscrepancy between the information the model has access to and that human\nannotators do.", "published": "2024-02-23 04:23:33", "link": "http://arxiv.org/abs/2402.15089v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Multimodal Agents: A Survey", "abstract": "Large language models (LLMs) have achieved superior performance in powering\ntext-based AI agents, endowing them with decision-making and reasoning\nabilities akin to humans. Concurrently, there is an emerging research trend\nfocused on extending these LLM-powered AI agents into the multimodal domain.\nThis extension enables AI agents to interpret and respond to diverse multimodal\nuser queries, thereby handling more intricate and nuanced tasks. In this paper,\nwe conduct a systematic review of LLM-driven multimodal agents, which we refer\nto as large multimodal agents ( LMAs for short). First, we introduce the\nessential components involved in developing LMAs and categorize the current\nbody of research into four distinct types. Subsequently, we review the\ncollaborative frameworks integrating multiple LMAs , enhancing collective\nefficacy. One of the critical challenges in this field is the diverse\nevaluation methods used across existing studies, hindering effective comparison\namong different LMAs . Therefore, we compile these evaluation methodologies and\nestablish a comprehensive framework to bridge the gaps. This framework aims to\nstandardize evaluations, facilitating more meaningful comparisons. Concluding\nour review, we highlight the extensive applications of LMAs and propose\npossible future research directions. Our discussion aims to provide valuable\ninsights and guidelines for future research in this rapidly evolving field. An\nup-to-date resource list is available at\nhttps://github.com/jun0wanan/awesome-large-multimodal-agents.", "published": "2024-02-23 06:04:23", "link": "http://arxiv.org/abs/2402.15116v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and\n  Context-Aware Visual Speech Processing", "abstract": "In visual speech processing, context modeling capability is one of the most\nimportant requirements due to the ambiguous nature of lip movements. For\nexample, homophenes, words that share identical lip movements but produce\ndifferent sounds, can be distinguished by considering the context. In this\npaper, we propose a novel framework, namely Visual Speech Processing\nincorporated with LLMs (VSP-LLM), to maximize the context modeling ability by\nbringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to\nperform multi-tasks of visual speech recognition and translation, where the\ngiven instructions control the type of task. The input video is mapped to the\ninput latent space of an LLM by employing a self-supervised visual speech\nmodel. Focused on the fact that there is redundant information in input frames,\nwe propose a novel deduplication method that reduces the embedded visual\nfeatures by employing visual speech units. Through the proposed deduplication\nand Low Rank Adaptation (LoRA), VSP-LLM can be trained in a computationally\nefficient manner. In the translation dataset, the MuAViC benchmark, we\ndemonstrate that VSP-LLM trained on just 30 hours of labeled data can more\neffectively translate lip movements compared to the recent model trained with\n433 hours of data.", "published": "2024-02-23 07:21:32", "link": "http://arxiv.org/abs/2402.15151v2", "categories": ["cs.CV", "cs.CL", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Machine Unlearning of Pre-trained Large Language Models", "abstract": "This study investigates the concept of the `right to be forgotten' within the\ncontext of large language models (LLMs). We explore machine unlearning as a\npivotal solution, with a focus on pre-trained models--a notably\nunder-researched area. Our research delineates a comprehensive framework for\nmachine unlearning in pre-trained LLMs, encompassing a critical analysis of\nseven diverse unlearning methods. Through rigorous evaluation using curated\ndatasets from arXiv, books, and GitHub, we establish a robust benchmark for\nunlearning performance, demonstrating that these methods are over $10^5$ times\nmore computationally efficient than retraining. Our results show that\nintegrating gradient ascent with gradient descent on in-distribution data\nimproves hyperparameter robustness. We also provide detailed guidelines for\nefficient hyperparameter tuning in the unlearning process. Our findings advance\nthe discourse on ethical AI practices, offering substantive insights into the\nmechanics of machine unlearning for pre-trained LLMs and underscoring the\npotential for responsible AI development.", "published": "2024-02-23 07:43:26", "link": "http://arxiv.org/abs/2402.15159v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Entity-level Factual Adaptiveness of Fine-tuning based Abstractive\n  Summarization Models", "abstract": "Abstractive summarization models often generate factually inconsistent\ncontent particularly when the parametric knowledge of the model conflicts with\nthe knowledge in the input document. In this paper, we analyze the robustness\nof fine-tuning based summarization models to the knowledge conflict, which we\ncall factual adaptiveness. We utilize pre-trained language models to construct\nevaluation sets and find that factual adaptiveness is not strongly correlated\nwith factual consistency on original datasets. Furthermore, we introduce a\ncontrollable counterfactual data augmentation method where the degree of\nknowledge conflict within the augmented data can be adjustable. Our\nexperimental results on two pre-trained language models (PEGASUS and BART) and\ntwo fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method\nenhances factual adaptiveness while achieving factual consistency on original\ndatasets on par with the contrastive learning baseline.", "published": "2024-02-23 07:53:39", "link": "http://arxiv.org/abs/2402.15162v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks\n  with Self-Refinement", "abstract": "Caution: This paper includes offensive words that could potentially cause\nunpleasantness. Language models (LMs) are vulnerable to exploitation for\nadversarial misuse. Training LMs for safety alignment is extensive and makes it\nhard to respond to fast-developing attacks immediately, such as jailbreaks. We\npropose self-refine with formatting that achieves outstanding safety even in\nnon-safety-aligned LMs and evaluate our method alongside several defense\nbaselines, demonstrating that it is the safest training-free method against\njailbreak attacks. Additionally, we proposed a formatting method that improves\nthe efficiency of the self-refine process while reducing attack success rates\nin fewer iterations. We've also observed that non-safety-aligned LMs outperform\nsafety-aligned LMs in safety tasks by giving more helpful and safe responses.\nIn conclusion, our findings can achieve less safety risk with fewer\ncomputational costs, allowing non-safety LM to be easily utilized in real-world\nservice.", "published": "2024-02-23 08:22:24", "link": "http://arxiv.org/abs/2402.15180v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Biomedical Entity Linking as Multiple Choice Question Answering", "abstract": "Although biomedical entity linking (BioEL) has made significant progress with\npre-trained language models, challenges still exist for fine-grained and\nlong-tailed entities. To address these challenges, we present BioELQA, a novel\nmodel that treats Biomedical Entity Linking as Multiple Choice Question\nAnswering. BioELQA first obtains candidate entities with a fast retriever,\njointly presents the mention and candidate entities to a generator, and then\noutputs the predicted symbol associated with its chosen entity. This\nformulation enables explicit comparison of different candidate entities, thus\ncapturing fine-grained interactions between mentions and entities, as well as\namong entities themselves. To improve generalization for long-tailed entities,\nwe retrieve similar labeled training instances as clues and concatenate the\ninput with retrieved instances for the generator. Extensive experimental\nresults show that BioELQA outperforms state-of-the-art baselines on several\ndatasets.", "published": "2024-02-23 08:40:38", "link": "http://arxiv.org/abs/2402.15189v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BSPA: Exploring Black-box Stealthy Prompt Attacks against Image\n  Generators", "abstract": "Extremely large image generators offer significant transformative potential\nacross diverse sectors. It allows users to design specific prompts to generate\nrealistic images through some black-box APIs. However, some studies reveal that\nimage generators are notably susceptible to attacks and generate Not Suitable\nFor Work (NSFW) contents by manually designed toxin texts, especially\nimperceptible to human observers. We urgently need a multitude of universal and\ntransferable prompts to improve the safety of image generators, especially\nblack-box-released APIs. Nevertheless, they are constrained by labor-intensive\ndesign processes and heavily reliant on the quality of the given instructions.\nTo achieve this, we introduce a black-box stealthy prompt attack (BSPA) that\nadopts a retriever to simulate attacks from API users. It can effectively\nharness filter scores to tune the retrieval space of sensitive words for\nmatching the input prompts, thereby crafting stealthy prompts tailored for\nimage generators. Significantly, this approach is model-agnostic and requires\nno internal access to the model's features, ensuring its applicability to a\nwide range of image generators. Building on BSPA, we have constructed an\nautomated prompt tool and a comprehensive prompt attack dataset (NSFWeval).\nExtensive experiments demonstrate that BSPA effectively explores the security\nvulnerabilities in a variety of state-of-the-art available black-box models,\nincluding Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we\ndevelop a resilient text filter and offer targeted recommendations to ensure\nthe security of image generators against prompt attacks in the future.", "published": "2024-02-23 09:28:16", "link": "http://arxiv.org/abs/2402.15218v1", "categories": ["cs.CR", "cs.CL", "cs.CV"], "primary_category": "cs.CR"}
{"title": "MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained\n  Language Models", "abstract": "Transformer-based language models (LMs) track contextual information through\nlarge, hard-coded input windows. We introduce MemoryPrompt, a leaner approach\nin which the LM is complemented by a small auxiliary recurrent network that\npasses information to the LM by prefixing its regular input with a sequence of\nvectors, akin to soft prompts, without requiring LM finetuning. Tested on a\ntask designed to probe a LM's ability to keep track of multiple fact updates, a\nMemoryPrompt-augmented LM outperforms much larger LMs that have access to the\nfull input history. We also test MemoryPrompt on a long-distance dialogue\ndataset, where its performance is comparable to that of a model conditioned on\nthe entire conversation history. In both experiments we also observe that,\nunlike full-finetuning approaches, MemoryPrompt does not suffer from\ncatastrophic forgetting when adapted to new tasks, thus not disrupting the\ngeneralist capabilities of the underlying LM.", "published": "2024-02-23 11:30:39", "link": "http://arxiv.org/abs/2402.15268v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Causal Graph Discovery with Retrieval-Augmented Generation based Large\n  Language Models", "abstract": "Causal graph recovery is traditionally done using statistical\nestimation-based methods or based on individual's knowledge about variables of\ninterests. They often suffer from data collection biases and limitations of\nindividuals' knowledge. The advance of large language models (LLMs) provides\nopportunities to address these problems. We propose a novel method that\nleverages LLMs to deduce causal relationships in general causal graph recovery\ntasks. This method leverages knowledge compressed in LLMs and knowledge LLMs\nextracted from scientific publication database as well as experiment data about\nfactors of interest to achieve this goal. Our method gives a prompting strategy\nto extract associational relationships among those factors and a mechanism to\nperform causality verification for these associations. Comparing to other\nLLM-based methods that directly instruct LLMs to do the highly complex causal\nreasoning, our method shows clear advantage on causal graph quality on\nbenchmark datasets. More importantly, as causality among some factors may\nchange as new research results emerge, our method show sensitivity to new\nevidence in the literature and can provide useful information for updating\ncausal graphs accordingly.", "published": "2024-02-23 13:02:10", "link": "http://arxiv.org/abs/2402.15301v2", "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "ArabianGPT: Native Arabic GPT-based Large Language Model", "abstract": "The predominance of English and Latin-based large language models (LLMs) has\nled to a notable deficit in native Arabic LLMs. This discrepancy is accentuated\nby the prevalent inclusion of English tokens in existing Arabic models,\ndetracting from their efficacy in processing native Arabic's intricate\nmorphology and syntax. Consequently, there is a theoretical and practical\nimperative for developing LLMs predominantly focused on Arabic linguistic\nelements. To address this gap, this paper proposes ArabianGPT, a series of\ntransformer-based models within the ArabianLLM suite designed explicitly for\nArabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in\nsize and complexity, aligning with the nuanced linguistic characteristics of\nArabic. The AraNizer tokenizer, integral to these models, addresses the unique\nmorphological aspects of Arabic script, ensuring more accurate text processing.\nEmpirical results from fine-tuning the models on tasks like sentiment analysis\nand summarization demonstrate significant improvements. For sentiment analysis,\nthe fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a\nsubstantial increase from the base model's 56%. Similarly, in summarization\ntasks, fine-tuned models showed enhanced F1 scores, indicating improved\nprecision and recall in generating concise summaries. Comparative analysis of\nfine-tuned ArabianGPT models against their base versions across various\nbenchmarks reveals nuanced differences in performance, with fine-tuning\npositively impacting specific tasks like question answering and summarization.\nThese findings underscore the efficacy of fine-tuning in aligning ArabianGPT\nmodels more closely with specific NLP tasks, highlighting the potential of\ntailored transformer architectures in advancing Arabic NLP.", "published": "2024-02-23 13:32:47", "link": "http://arxiv.org/abs/2402.15313v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "abstract": "Large Language Models (LLMs) have shown impressive abilities in data\nannotation, opening the way for new approaches to solve classic NLP problems.\nIn this paper, we show how to use LLMs to create NuNER, a compact language\nrepresentation model specialized in the Named Entity Recognition (NER) task.\nNuNER can be fine-tuned to solve downstream NER problems in a data-efficient\nway, outperforming similar-sized foundation models in the few-shot regime and\ncompeting with much larger LLMs. We find that the size and entity-type\ndiversity of the pre-training dataset are key to achieving good performance. We\nview NuNER as a member of the broader family of task-specific foundation\nmodels, recently unlocked by LLMs.", "published": "2024-02-23 14:23:51", "link": "http://arxiv.org/abs/2402.15343v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dual Encoder: Exploiting the Potential of Syntactic and Semantic for\n  Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained\nsentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to\nmodel the syntax-semantic relationships inherent in triplet elements. However,\nthey have yet to fully tap into the vast potential of syntactic and semantic\ninformation within the ASTE task. In this work, we propose a \\emph{Dual\nEncoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S),\nwhich maximizes the syntactic and semantic relationships among words.\nSpecifically, our model utilizes a dual-channel encoder with a BERT channel to\ncapture semantic information, and an enhanced LSTM channel for comprehensive\nsyntactic information capture. Subsequently, we introduce the heterogeneous\nfeature interaction module to capture intricate interactions between dependency\nsyntax and attention semantics, and to dynamically select vital nodes. We\nleverage the synergy of these modules to harness the significant potential of\nsyntactic and semantic information in ASTE tasks. Testing on public benchmarks,\nour D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its\neffectiveness.", "published": "2024-02-23 15:07:13", "link": "http://arxiv.org/abs/2402.15370v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explorations of Self-Repair in Language Models", "abstract": "Prior interpretability research studying narrow distributions has\npreliminarily identified self-repair, a phenomena where if components in large\nlanguage models are ablated, later components will change their behavior to\ncompensate. Our work builds off this past literature, demonstrating that\nself-repair exists on a variety of models families and sizes when ablating\nindividual attention heads on the full training distribution. We further show\nthat on the full training distribution self-repair is imperfect, as the\noriginal direct effect of the head is not fully restored, and noisy, since the\ndegree of self-repair varies significantly across different prompts (sometimes\novercorrecting beyond the original effect). We highlight two different\nmechanisms that contribute to self-repair, including changes in the final\nLayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure.\nWe additionally discuss the implications of these results for interpretability\npractitioners and close with a more speculative discussion on the mystery of\nwhy self-repair occurs in these models at all, highlighting evidence for the\nIterative Inference hypothesis in language models, a framework that predicts\nself-repair.", "published": "2024-02-23 15:42:12", "link": "http://arxiv.org/abs/2402.15390v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based\n  Reasoning in Reinforcement Learning", "abstract": "Preference-based reinforcement learning (RL) has emerged as a new field in\nrobot learning, where humans play a pivotal role in shaping robot behavior by\nexpressing preferences on different sequences of state-action pairs. However,\nformulating realistic policies for robots demands responses from humans to an\nextensive array of queries. In this work, we approach the sample-efficiency\nchallenge by expanding the information collected per query to contain both\npreferences and optional text prompting. To accomplish this, we leverage the\nzero-shot capabilities of a large language model (LLM) to reason from the text\nprovided by humans. To accommodate the additional query information, we\nreformulate the reward learning objectives to contain flexible highlights --\nstate-action pairs that contain relatively high information and are related to\nthe features processed in a zero-shot fashion from a pretrained LLM. In both a\nsimulated scenario and a user study, we reveal the effectiveness of our work by\nanalyzing the feedback and its implications. Additionally, the collective\nfeedback collected serves to train a robot on socially compliant trajectories\nin a simulated social navigation landscape. We provide video examples of the\ntrained policies at https://sites.google.com/view/rl-predilect", "published": "2024-02-23 16:30:05", "link": "http://arxiv.org/abs/2402.15420v1", "categories": ["cs.RO", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "A Data-Centric Approach To Generate Faithful and High Quality Patient\n  Summaries with Large Language Models", "abstract": "Patients often face difficulties in understanding their hospitalizations,\nwhile healthcare workers have limited resources to provide explanations. In\nthis work, we investigate the potential of large language models to generate\npatient summaries based on doctors' notes and study the effect of training data\non the faithfulness and quality of the generated summaries. To this end, we\nrelease (i) a rigorous labeling protocol for errors in medical texts and (ii) a\npublicly available dataset of annotated hallucinations in 100 doctor-written\nand 100 generated summaries. We show that fine-tuning on hallucination-free\ndata effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama\n2, while preserving relevant information. We observe a similar effect on GPT-4\n(0.70 to 0.40), when the few-shot examples are hallucination-free. We also\nconduct a qualitative evaluation using hallucination-free and improved training\ndata. We find that common quantitative metrics do not correlate well with\nfaithfulness and quality. Finally, we test GPT-4 for automatic hallucination\ndetection, which clearly outperforms common baselines.", "published": "2024-02-23 16:32:28", "link": "http://arxiv.org/abs/2402.15422v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AgentOhana: Design Unified Data and Training Pipeline for Effective\n  Agent Learning", "abstract": "Autonomous agents powered by large language models (LLMs) have garnered\nsignificant research attention. However, fully harnessing the potential of LLMs\nfor agent-based tasks presents inherent challenges due to the heterogeneous\nnature of diverse data sources featuring multi-turn trajectories. In this\npaper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address\nthese challenges. \\textit{AgentOhana} aggregates agent trajectories from\ndistinct environments, spanning a wide array of scenarios. It meticulously\nstandardizes and unifies these trajectories into a consistent format,\nstreamlining the creation of a generic data loader optimized for agent\ntraining. Leveraging the data unification, our training pipeline maintains\nequilibrium across different data sources and preserves independent randomness\nacross devices during dataset partitioning and model training. Additionally, we\npresent \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which\ndemonstrates exceptional performance across various benchmarks. Begin the\nexploration at \\url{https://github.com/SalesforceAIResearch/xLAM}.", "published": "2024-02-23 18:56:26", "link": "http://arxiv.org/abs/2402.15506v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Evaluating the Performance of ChatGPT for Spam Email Detection", "abstract": "Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.", "published": "2024-02-23 04:52:08", "link": "http://arxiv.org/abs/2402.15537v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast Adversarial Attacks on Language Models In One GPU Minute", "abstract": "In this paper, we introduce a novel class of fast, beam search-based\nadversarial attack (BEAST) for Language Models (LMs). BEAST employs\ninterpretable parameters, enabling attackers to balance between attack speed,\nsuccess rate, and the readability of adversarial prompts. The computational\nefficiency of BEAST facilitates us to investigate its applications on LMs for\njailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free\ntargeted attack can jailbreak aligned LMs with high attack success rates within\none minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute\nwith a success rate of 89% when compared to a gradient-based baseline that\ntakes over an hour to achieve 70% success rate using a single Nvidia RTX A6000\n48GB GPU. Additionally, we discover a unique outcome wherein our untargeted\nattack induces hallucinations in LM chatbots. Through human evaluations, we\nfind that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% more\nincorrect outputs when compared to LM outputs in the absence of our attack. We\nalso learn that 22% of the time, BEAST causes Vicuna to generate outputs that\nare not relevant to the original prompt. Further, we use BEAST to generate\nadversarial prompts in a few seconds that can boost the performance of existing\nmembership inference attacks for LMs. We believe that our fast attack, BEAST,\nhas the potential to accelerate research in LM security and privacy. Our\ncodebase is publicly available at https://github.com/vinusankars/BEAST.", "published": "2024-02-23 19:12:53", "link": "http://arxiv.org/abs/2402.15570v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "LLMs as Meta-Reviewers' Assistants: A Case Study", "abstract": "One of the most important yet onerous tasks in the academic peer-reviewing\nprocess is composing meta-reviews, which involves assimilating diverse opinions\nfrom multiple expert peers, formulating one's self-judgment as a senior expert,\nand then summarizing all these perspectives into a concise holistic overview to\nmake an overall recommendation. This process is time-consuming and can be\ncompromised by human factors like fatigue, inconsistency, missing tiny details,\netc. Given the latest major developments in Large Language Models (LLMs), it is\nvery compelling to rigorously study whether LLMs can help metareviewers perform\nthis important task better. In this paper, we perform a case study with three\npopular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to assist meta-reviewers in\nbetter comprehending multiple experts perspectives by generating a controlled\nmulti-perspective summary (MPS) of their opinions. To achieve this, we prompt\nthree LLMs with different types/levels of prompts based on the recently\nproposed TELeR taxonomy. Finally, we perform a detailed qualitative study of\nthe MPSs generated by the LLMs and report our findings.", "published": "2024-02-23 20:14:16", "link": "http://arxiv.org/abs/2402.15589v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Alternating Weak Triphone/BPE Alignment Supervision from Hybrid Model\n  Improves End-to-End ASR", "abstract": "In this paper, alternating weak triphone/BPE alignment supervision is\nproposed to improve end-to-end model training. Towards this end, triphone and\nBPE alignments are extracted using a pre-existing hybrid ASR system. Then,\nregularization effect is obtained by cross-entropy based intermediate auxiliary\nlosses computed on such alignments at a mid-layer representation of the encoder\nfor triphone alignments and at the encoder for BPE alignments. Weak supervision\nis achieved through strong label smoothing with parameter of 0.5. Experimental\nresults on TED-LIUM 2 indicate that either triphone or BPE alignment based weak\nsupervision improves ASR performance over standard CTC auxiliary loss.\nMoreover, their combination lowers the word error rate further. We also\ninvestigate the alternation of the two auxiliary tasks during model training,\nand additional performance gain is observed. Overall, the proposed techniques\nresult in over 10% relative error rate reduction over a CTC-regularized\nbaseline system.", "published": "2024-02-23 20:26:54", "link": "http://arxiv.org/abs/2402.15594v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language-Based User Profiles for Recommendation", "abstract": "Most conventional recommendation methods (e.g., matrix factorization)\nrepresent user profiles as high-dimensional vectors. Unfortunately, these\nvectors lack interpretability and steerability, and often perform poorly in\ncold-start settings. To address these shortcomings, we explore the use of user\nprofiles that are represented as human-readable text. We propose the\nLanguage-based Factorization Model (LFM), which is essentially an\nencoder/decoder model where both the encoder and the decoder are large language\nmodels (LLMs). The encoder LLM generates a compact natural-language profile of\nthe user's interests from the user's rating history. The decoder LLM uses this\nsummary profile to complete predictive downstream tasks. We evaluate our LFM\napproach on the MovieLens dataset, comparing it against matrix factorization\nand an LLM model that directly predicts from the user's rating history. In\ncold-start settings, we find that our method can have higher accuracy than\nmatrix factorization. Furthermore, we find that generating a compact and\nhuman-readable summary often performs comparably with or better than direct LLM\nprediction, while enjoying better interpretability and shorter model input\nlength. Our results motivate a number of future research directions and\npotential improvements.", "published": "2024-02-23 21:58:50", "link": "http://arxiv.org/abs/2402.15623v1", "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented\n  Generation (RAG)", "abstract": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate\nlanguage model with proprietary and private data, where data privacy is a\npivotal concern. Whereas extensive research has demonstrated the privacy risks\nof large language models (LLMs), the RAG technique could potentially reshape\nthe inherent behaviors of LLM generation, posing new privacy issues that are\ncurrently under-explored. In this work, we conduct extensive empirical studies\nwith novel attack methods, which demonstrate the vulnerability of RAG systems\non leaking the private retrieval database. Despite the new risk brought by RAG\non the retrieval data, we further reveal that RAG can mitigate the leakage of\nthe LLMs' training data. Overall, we provide new insights in this paper for\nprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG\nsystems builders. Our code is available at\nhttps://github.com/phycholosogy/RAG-privacy.", "published": "2024-02-23 18:35:15", "link": "http://arxiv.org/abs/2402.16893v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large\n  Language Models", "abstract": "Humor is a fundamental facet of human cognition and interaction. Yet, despite\nrecent advances in natural language processing, humor detection remains a\nchallenging task that is complicated by the scarcity of datasets that pair\nhumorous texts with similar non-humorous counterparts. In our work, we\ninvestigate whether large language models (LLMs), can generate synthetic data\nfor humor detection via editing texts. We benchmark LLMs on an existing human\ndataset and show that current LLMs display an impressive ability to 'unfun'\njokes, as judged by humans and as measured on the downstream task of humor\ndetection. We extend our approach to a code-mixed English-Hindi humor dataset,\nwhere we find that GPT-4's synthetic data is highly rated by bilingual\nannotators and provides challenging adversarial examples for humor classifiers.", "published": "2024-02-23 02:58:12", "link": "http://arxiv.org/abs/2403.00794v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning", "abstract": "Large language models (LLMs) are displaying emergent abilities for math\nreasoning tasks,and there is a growing attention on enhancing the ability of\nopen-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to\nexplore a general data strategy for supervised data to help optimize and expand\nmath reasoning ability.Firstly, we determine the ability boundary of reasoning\npaths augmentation by identifying these paths' minimal optimal set.Secondly, we\nvalidate that different abilities of the model can be cumulatively enhanced by\nMix of Minimal Optimal Sets of corresponding types of data, while our models\nMMOS achieve SOTA performance on series base models under much lower\nconstruction costs.Besides, we point out GSM-HARD is not really hard and\ntoday's LLMs no longer lack numerical robustness.Also, we provide an Auto\nProblem Generator for robustness testing and educational applications.Our code\nand data are publicly available at https://github.com/cyzhh/MMOS.", "published": "2024-02-23 17:38:43", "link": "http://arxiv.org/abs/2403.00799v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by\n  Imitating Human Thought Processes", "abstract": "Although large language models demonstrate emergent abilities in solving math\nword problems, there is a challenging task in complex multi-step mathematical\nreasoning tasks. To improve model performance on mathematical reasoning tasks,\nprevious work has conducted supervised fine-tuning on open-source models by\nimproving the quality and quantity of data. In this paper, we propose a novel\napproach, named Brain, to imitate human thought processes to enhance\nmathematical reasoning abilities, using the Frontal Lobe Model to generate\nplans, and then employing the Parietal Lobe Model to generate code and execute\nto obtain answers. First, we achieve SOTA performance in comparison with Code\nLLaMA 7B based models through this method. Secondly, we find that plans can be\nexplicitly extracted from natural language, code, or formal language. Our code\nand data are publicly available at https://github.com/cyzhh/Brain.", "published": "2024-02-23 17:40:31", "link": "http://arxiv.org/abs/2403.00800v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language\n  Model", "abstract": "The rise of large language models (LLMs) has significantly transformed both\nthe construction and application of information retrieval (IR) systems.\nHowever, current interactions between IR systems and LLMs remain limited, with\nLLMs merely serving as part of components within IR systems, and IR systems\nbeing constructed independently of LLMs. This separated architecture restricts\nknowledge sharing and deep collaboration between them. In this paper, we\nintroduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval\narchitecture. Self-Retrieval unifies all essential IR functions within a single\nLLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through\nself-supervised learning, transforms the retrieval process into sequential\npassage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms\nexisting retrieval approaches by a significant margin, but also substantially\nenhances the performance of LLM-driven downstream applications like\nretrieval-augmented generation.", "published": "2024-02-23 18:45:35", "link": "http://arxiv.org/abs/2403.00801v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Seeing is Believing: Mitigating Hallucination in Large Vision-Language\n  Models via CLIP-Guided Decoding", "abstract": "Large Vision-Language Models (LVLMs) are susceptible to object\nhallucinations, an issue in which their generated text contains non-existent\nobjects, greatly limiting their reliability and practicality. Current\napproaches often rely on the model's token likelihoods or other internal\ninformation, instruction tuning on additional datasets, or incorporating\ncomplex external tools. We first perform empirical analysis on sentence-level\nLVLM hallucination, finding that CLIP similarity to the image acts as a\nstronger and more robust indicator of hallucination compared to token\nlikelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)\napproach, a straightforward but effective training-free approach to reduce\nobject hallucination at decoding time. CGD uses CLIP to guide the model's\ndecoding process by enhancing visual grounding of generated text with the\nimage. Experiments demonstrate that CGD effectively mitigates object\nhallucination across multiple LVLM families while preserving the utility of\ntext generation. Codes are available at\nhttps://github.com/d-ailin/CLIP-Guided-Decoding.", "published": "2024-02-23 12:57:16", "link": "http://arxiv.org/abs/2402.15300v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "ChildAugment: Data Augmentation Methods for Zero-Resource Children's\n  Speaker Verification", "abstract": "The accuracy of modern automatic speaker verification (ASV) systems, when\ntrained exclusively on adult data, drops substantially when applied to\nchildren's speech. The scarcity of children's speech corpora hinders\nfine-tuning ASV systems for children's speech. Hence, there is a timely need to\nexplore more effective ways of reusing adults' speech data. One promising\napproach is to align vocal-tract parameters between adults and children through\nchildren-specific data augmentation, referred here to as ChildAugment.\nSpecifically, we modify the formant frequencies and formant bandwidths of adult\nspeech to emulate children's speech. The modified spectra are used to train\nECAPA-TDNN (emphasized channel attention, propagation, and aggregation in\ntime-delay neural network) recognizer for children. We compare ChildAugment\nagainst various state-of-the-art data augmentation techniques for children's\nASV. We also extensively compare different scoring methods, including cosine\nscoring, PLDA (probabilistic linear discriminant analysis), and NPLDA (neural\nPLDA). We also propose a low-complexity weighted cosine score for extremely\nlow-resource children ASV. Our findings on the CSLU kids corpus indicate that\nChildAugment holds promise as a simple, acoustics-motivated approach, for\nimproving state-of-the-art deep learning based ASV for children. We achieve up\nto 12.45% (boys) and 11.96% (girls) relative improvement over the baseline.", "published": "2024-02-23 09:19:33", "link": "http://arxiv.org/abs/2402.15214v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "High Resolution Guitar Transcription via Domain Adaptation", "abstract": "Automatic music transcription (AMT) has achieved high accuracy for piano due\nto the availability of large, high-quality datasets such as MAESTRO and MAPS,\nbut comparable datasets are not yet available for other instruments. In recent\nwork, however, it has been demonstrated that aligning scores to transcription\nmodel activations can produce high quality AMT training data for instruments\nother than piano. Focusing on the guitar, we refine this approach to training\non score data using a dataset of commercially available score-audio pairs. We\npropose the use of a high-resolution piano transcription model to train a new\nguitar transcription model. The resulting model obtains state-of-the-art\ntranscription results on GuitarSet in a zero-shot context, improving on\npreviously published methods.", "published": "2024-02-23 10:56:47", "link": "http://arxiv.org/abs/2402.15258v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Survey of Music Generation in the Context of Interaction", "abstract": "In recent years, machine learning, and in particular generative adversarial\nneural networks (GANs) and attention-based neural networks (transformers), have\nbeen successfully used to compose and generate music, both melodies and\npolyphonic pieces. Current research focuses foremost on style replication (eg.\ngenerating a Bach-style chorale) or style transfer (eg. classical to jazz)\nbased on large amounts of recorded or transcribed music, which in turn also\nallows for fairly straight-forward \"performance\" evaluation. However, most of\nthese models are not suitable for human-machine co-creation through live\ninteraction, neither is clear, how such models and resulting creations would be\nevaluated. This article presents a thorough review of music representation,\nfeature analysis, heuristic algorithms, statistical and parametric modelling,\nand human and automatic evaluation measures, along with a discussion of which\napproaches and models seem most suitable for live interaction.", "published": "2024-02-23 12:41:44", "link": "http://arxiv.org/abs/2402.15294v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic\n  Data", "abstract": "Passive acoustic monitoring (PAM) studies generate thousands of hours of\naudio, which may be used to monitor specific animal populations, conduct broad\nbiodiversity surveys, detect threats such as poachers, and more. Machine\nlearning classifiers for species identification are increasingly being used to\nprocess the vast amount of audio generated by bioacoustic surveys, expediting\nanalysis and increasing the utility of PAM as a management tool. In common\npractice, a threshold is applied to classifier output scores, and scores above\nthe threshold are aggregated into a detection count. The choice of threshold\nproduces biased counts of vocalizations, which are subject to false\npositive/negative rates that may vary across subsets of the dataset. In this\nwork, we advocate for directly estimating call density: The proportion of\ndetection windows containing the target vocalization, regardless of classifier\nscore. Our approach targets a desirable ecological estimator and provides a\nmore rigorous grounding for identifying the core problems caused by\ndistribution shifts -- when the defining characteristics of the data\ndistribution change -- and designing strategies to mitigate them. We propose a\nvalidation scheme for estimating call density in a body of data and obtain,\nthrough Bayesian reasoning, probability distributions of confidence scores for\nboth the positive and negative classes. We use these distributions to predict\nsite-level densities, which may be subject to distribution shifts. We test our\nproposed methods on a real-world study of Hawaiian birds and provide simulation\nresults leveraging existing fully annotated datasets, demonstrating robustness\nto variations in call density and classifier model quality.", "published": "2024-02-23 14:52:44", "link": "http://arxiv.org/abs/2402.15360v1", "categories": ["q-bio.QM", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "q-bio.QM"}
{"title": "Toward Fully Self-Supervised Multi-Pitch Estimation", "abstract": "Multi-pitch estimation is a decades-long research problem involving the\ndetection of pitch activity associated with concurrent musical events within\nmulti-instrument mixtures. Supervised learning techniques have demonstrated\nsolid performance on more narrow characterizations of the task, but suffer from\nlimitations concerning the shortage of large-scale and diverse polyphonic music\ndatasets with multi-pitch annotations. We present a suite of self-supervised\nlearning objectives for multi-pitch estimation, which encourage the\nconcentration of support around harmonics, invariance to timbral\ntransformations, and equivariance to geometric transformations. These\nobjectives are sufficient to train an entirely convolutional autoencoder to\nproduce multi-pitch salience-grams directly, without any fine-tuning. Despite\ntraining exclusively on a collection of synthetic single-note audio samples,\nour fully self-supervised framework generalizes to polyphonic music mixtures,\nand achieves performance comparable to supervised models trained on\nconventional multi-pitch datasets.", "published": "2024-02-23 19:12:41", "link": "http://arxiv.org/abs/2402.15569v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
