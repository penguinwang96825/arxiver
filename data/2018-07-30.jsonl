{"title": "Leveraging Medical Sentiment to Understand Patients Health on Social\n  Media", "abstract": "The unprecedented growth of Internet users in recent years has resulted in an\nabundance of unstructured information in the form of social media text. A large\npercentage of this population is actively engaged in health social networks to\nshare health-related information. In this paper, we address an important and\ntimely topic by analyzing the users' sentiments and emotions w.r.t their\nmedical conditions. Towards this, we examine users on popular medical forums\n(Patient.info,dailystrength.org), where they post on important topics such as\nasthma, allergy, depression, and anxiety. First, we provide a benchmark setup\nfor the task by crawling the data, and further define the sentiment specific\nfine-grained medical conditions (Recovered, Exist, Deteriorate, and Other). We\npropose an effective architecture that uses a Convolutional Neural Network\n(CNN) as a data-driven feature extractor and a Support Vector Machine (SVM) as\na classifier. We further develop a sentiment feature which is sensitive to the\nmedical context. Here, we show that the use of medical sentiment feature along\nwith extracted features from CNN improves the model performance. In addition to\nour dataset, we also evaluate our approach on the benchmark \"CLEF eHealth 2014\"\ncorpora and show that our model outperforms the state-of-the-art techniques.", "published": "2018-07-30 04:59:43", "link": "http://arxiv.org/abs/1807.11172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Neural Machine Translation using Word Embedding-based Loss", "abstract": "In neural machine translation (NMT), the computational cost at the output\nlayer increases with the size of the target-side vocabulary. Using a\nlimited-size vocabulary instead may cause a significant decrease in translation\nquality. This trade-off is derived from a softmax-based loss function that\nhandles in-dictionary words independently, in which word similarity is not\nconsidered. In this paper, we propose a novel NMT loss function that includes\nword similarity in forms of distances in a word embedding space. The proposed\nloss function encourages an NMT decoder to generate words close to their\nreferences in the embedding space; this helps the decoder to choose similar\nacceptable words when the actual best candidates are not included in the\nvocabulary due to its size limitation. In experiments using ASPEC\nJapanese-to-English and IWSLT17 English-to-French data sets, the proposed\nmethod showed improvements against a standard NMT baseline in both datasets;\nespecially with IWSLT17 En-Fr, it achieved up to +1.72 in BLEU and +1.99 in\nMETEOR. When the target-side vocabulary was very limited to 1,000 words, the\nproposed method demonstrated a substantial gain, +1.72 in METEOR with ASPEC\nJa-En.", "published": "2018-07-30 08:11:52", "link": "http://arxiv.org/abs/1807.11219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning for Interactive Neural Machine Translation of Data\n  Streams", "abstract": "We study the application of active learning techniques to the translation of\nunbounded data streams via interactive neural machine translation. The main\nidea is to select, from an unbounded stream of source sentences, those worth to\nbe supervised by a human agent. The user will interactively translate those\nsamples. Once validated, these data is useful for adapting the neural machine\ntranslation model.\n  We propose two novel methods for selecting the samples to be validated. We\nexploit the information from the attention mechanism of a neural machine\ntranslation system. Our experiments show that the inclusion of active learning\ntechniques into this pipeline allows to reduce the effort required during the\nprocess, while increasing the quality of the translation system. Moreover, it\nenables to balance the human effort required for achieving a certain\ntranslation quality. Moreover, our neural system outperforms classical\napproaches by a large margin.", "published": "2018-07-30 09:11:26", "link": "http://arxiv.org/abs/1807.11243v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graphene: Semantically-Linked Propositions in Open Information\n  Extraction", "abstract": "We present an Open Information Extraction (IE) approach that uses a\ntwo-layered transformation stage consisting of a clausal disembedding layer and\na phrasal disembedding layer, together with rhetorical relation identification.\nIn that way, we convert sentences that present a complex linguistic structure\ninto simplified, syntactically sound sentences, from which we can extract\npropositions that are represented in a two-layered hierarchy in the form of\ncore relational tuples and accompanying contextual information which are\nsemantically linked via rhetorical relations. In a comparative evaluation, we\ndemonstrate that our reference implementation Graphene outperforms\nstate-of-the-art Open IE systems in the construction of correct n-ary\npredicate-argument structures. Moreover, we show that existing Open IE\napproaches can benefit from the transformation process of our framework.", "published": "2018-07-30 10:31:52", "link": "http://arxiv.org/abs/1807.11276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "News Article Teaser Tweets and How to Generate Them", "abstract": "In this work, we define the task of teaser generation and provide an\nevaluation benchmark and baseline systems for the process of generating\nteasers. A teaser is a short reading suggestion for an article that is\nillustrative and includes curiosity-arousing elements to entice potential\nreaders to read particular news items. Teasers are one of the main vehicles for\ntransmitting news to social media users. We compile a novel dataset of teasers\nby systematically accumulating tweets and selecting those that conform to the\nteaser definition. We have compared a number of neural abstractive\narchitectures on the task of teaser generation and the overall best performing\nsystem is See et al.(2017)'s seq2seq with pointer network.", "published": "2018-07-30 19:16:09", "link": "http://arxiv.org/abs/1807.11535v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UH-PRHLT at SemEval-2016 Task 3: Combining Lexical and Semantic-based\n  Features for Community Question Answering", "abstract": "In this work we describe the system built for the three English subtasks of\nthe SemEval 2016 Task 3 by the Department of Computer Science of the University\nof Houston (UH) and the Pattern Recognition and Human Language Technology\n(PRHLT) research center - Universitat Polit`ecnica de Val`encia: UH-PRHLT. Our\nsystem represents instances by using both lexical and semantic-based similarity\nmeasures between text pairs. Our semantic features include the use of\ndistributed representations of words, knowledge graphs generated with the\nBabelNet multilingual semantic network, and the FrameNet lexical database.\nExperimental results outperform the random and Google search engine baselines\nin the three English subtasks. Our approach obtained the highest results of\nsubtask B compared to the other task participants.", "published": "2018-07-30 21:14:25", "link": "http://arxiv.org/abs/1807.11584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Doubly Attentive Transformer Machine Translation", "abstract": "In this paper a doubly attentive transformer machine translation model\n(DATNMT) is presented in which a doubly-attentive transformer decoder normally\njoins spatial visual features obtained via pretrained convolutional neural\nnetworks, conquering any gap between image captioning and translation. In this\nframework, the transformer decoder figures out how to take care of\nsource-language words and parts of an image freely by methods for two separate\nattention components in an Enhanced Multi-Head Attention Layer of doubly\nattentive transformer, as it generates words in the target language. We find\nthat the proposed model can effectively exploit not just the scarce multimodal\nmachine translation data, but also large general-domain text-only machine\ntranslation corpora, or image-text image captioning corpora. The experimental\nresults show that the proposed doubly-attentive transformer-decoder performs\nbetter than a single-decoder transformer model, and gives the state-of-the-art\nresults in the English-German multimodal machine translation task.", "published": "2018-07-30 23:13:55", "link": "http://arxiv.org/abs/1807.11605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "YouTube AV 50K: An Annotated Corpus for Comments in Autonomous Vehicles", "abstract": "With one billion monthly viewers, and millions of users discussing and\nsharing opinions, comments below YouTube videos are rich sources of data for\nopinion mining and sentiment analysis. We introduce the YouTube AV 50K dataset,\na freely-available collections of more than 50,000 YouTube comments and\nmetadata below autonomous vehicle (AV)-related videos. We describe its creation\nprocess, its content and data format, and discuss its possible usages.\nEspecially, we do a case study of the first self-driving car fatality to\nevaluate the dataset, and show how we can use this dataset to better understand\npublic attitudes toward self-driving cars and public reactions to the accident.\nFuture developments of the dataset are also discussed.", "published": "2018-07-30 08:28:44", "link": "http://arxiv.org/abs/1807.11227v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation by Adversarial Learning for Robust Speech\n  Recognition", "abstract": "In this paper, we investigate the use of adversarial learning for\nunsupervised adaptation to unseen recording conditions, more specifically,\nsingle microphone far-field speech. We adapt neural networks based acoustic\nmodels trained with close-talk clean speech to the new recording conditions\nusing untranscribed adaptation data. Our experimental results on Italian\nSPEECON data set show that our proposed method achieves 19.8% relative word\nerror rate (WER) reduction compared to the unadapted models. Furthermore, this\nadaptation method is beneficial even when performed on data from another\nlanguage (i.e. French) giving 12.6% relative WER reduction.", "published": "2018-07-30 11:00:59", "link": "http://arxiv.org/abs/1807.11284v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The impact of imbalanced training data on machine learning for author\n  name disambiguation", "abstract": "In supervised machine learning for author name disambiguation, negative\ntraining data are often dominantly larger than positive training data. This\npaper examines how the ratios of negative to positive training data can affect\nthe performance of machine learning algorithms to disambiguate author names in\nbibliographic records. On multiple labeled datasets, three classifiers -\nLogistic Regression, Na\\\"ive Bayes, and Random Forest - are trained through\nrepresentative features such as coauthor names, and title words extracted from\nthe same training data but with various positive-negative training data ratios.\nResults show that increasing negative training data can improve disambiguation\nperformance but with a few percent of performance gains and sometimes degrade\nit. Logistic Regression and Na\\\"ive Bayes learn optimal disambiguation models\neven with a base ratio (1:1) of positive and negative training data. Also, the\nperformance improvement by Random Forest tends to quickly saturate roughly\nafter 1:10 ~ 1:15. These findings imply that contrary to the common practice\nusing all training data, name disambiguation algorithms can be trained using\npart of negative training data without degrading much disambiguation\nperformance while increasing computational efficiency. This study calls for\nmore attention from author name disambiguation scholars to methods for machine\nlearning from imbalanced data.", "published": "2018-07-30 14:29:27", "link": "http://arxiv.org/abs/1808.00525v2", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Audio segmentation based on melodic style with hand-crafted features and\n  with convolutional neural networks", "abstract": "We investigate methods for the automatic labeling of the taan section, a\nprominent structural component of the Hindustani Khayal vocal concert. The taan\ncontains improvised raga-based melody rendered in the highly distinctive style\nof rapid pitch and energy modulations of the voice. We propose computational\nfeatures that capture these specific high-level characteristics of the singing\nvoice in the polyphonic context. The extracted local features are used to\nachieve classification at the frame level via a trained multilayer perceptron\n(MLP) network, followed by grouping and segmentation based on novelty\ndetection. We report high accuracies with reference to musician annotated taan\nsections across artists and concerts. We also compare the performance obtained\nby the compact specialized features with frame-level classification via a\nconvolutional neural network (CNN) operating directly on audio spectrogram\npatches for the same task. While the relatively simple architecture we\nexperiment with does not quite attain the classification accuracy of the\nhand-crafted features, it provides for a performance well above chance with\ninteresting insights about the ability of the network to learn discriminative\nfeatures effectively from labeled data.", "published": "2018-07-30 01:34:20", "link": "http://arxiv.org/abs/1807.11138v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DCASE 2018 Challenge - Task 5: Monitoring of domestic activities based\n  on multi-channel acoustics", "abstract": "The DCASE 2018 Challenge consists of five tasks related to automatic\nclassification and detection of sound events and scenes. This paper presents\nthe setup of Task 5 which includes the description of the task, dataset and the\nbaseline system. In this task, it is investigated to which extent multi-channel\nacoustic recordings are beneficial for the purpose of classifying domestic\nactivities. The goal is to exploit spectral and spatial cues independent of\nsensor location using multi-channel audio. For this purpose we provided a\ndevelopment and evaluation dataset which are derivatives of the SINS database\nand contain domestic activities recorded by multiple microphone arrays. The\nbaseline system, based on a Neural Network architecture using convolutional and\ndense layer(s), is intended to lower the hurdle to participate the challenge\nand to provide a reference performance.", "published": "2018-07-30 09:15:34", "link": "http://arxiv.org/abs/1807.11246v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Harmonic-Percussive Source Separation with Deep Neural Networks and\n  Phase Recovery", "abstract": "Harmonic/percussive source separation (HPSS) consists in separating the\npitched instruments from the percussive parts in a music mixture. In this\npaper, we propose to apply the recently introduced Masker-Denoiser with twin\nnetworks (MaD TwinNet) system to this task. MaD TwinNet is a deep learning\narchitecture that has reached state-of-the-art results in monaural singing\nvoice separation. Herein, we propose to apply it to HPSS by using it to\nestimate the magnitude spectrogram of the percussive source. Then, we retrieve\nthe complex-valued short-time Fourier transform of the sources by means of a\nphase recovery algorithm, which minimizes the reconstruction error and enforces\nthe phase of the harmonic part to follow a sinusoidal phase model. Experiments\nconducted on realistic music mixtures show that this novel separation system\noutperforms the previous state-of-the art kernel additive model approach.", "published": "2018-07-30 11:51:59", "link": "http://arxiv.org/abs/1807.11298v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lead Sheet Generation and Arrangement by Conditional Generative\n  Adversarial Network", "abstract": "Research on automatic music generation has seen great progress due to the\ndevelopment of deep neural networks. However, the generation of\nmulti-instrument music of arbitrary genres still remains a challenge. Existing\nresearch either works on lead sheets or multi-track piano-rolls found in MIDIs,\nbut both musical notations have their limits. In this work, we propose a new\ntask called lead sheet arrangement to avoid such limits. A new recurrent\nconvolutional generative model for the task is proposed, along with three new\nsymbolic-domain harmonic features to facilitate learning from unpaired lead\nsheets and MIDIs. Our model can generate lead sheets and their arrangements of\neight-bar long. Audio samples of the generated result can be found at\nhttps://drive.google.com/open?id=1c0FfODTpudmLvuKBbc23VBCgQizY6-Rk", "published": "2018-07-30 03:48:04", "link": "http://arxiv.org/abs/1807.11161v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Encoder-Decoder Models for Unsupervised Learning of Controllable\n  Speech Synthesis", "abstract": "Generating versatile and appropriate synthetic speech requires control over\nthe output expression separate from the spoken text. Important non-textual\nspeech variation is seldom annotated, in which case output control must be\nlearned in an unsupervised fashion. In this paper, we perform an in-depth study\nof methods for unsupervised learning of control in statistical speech\nsynthesis. For example, we show that popular unsupervised training heuristics\ncan be interpreted as variational inference in certain autoencoder models. We\nadditionally connect these models to VQ-VAEs, another, recently-proposed class\nof deep variational autoencoders, which we show can be derived from a very\nsimilar mathematical argument. The implications of these new probabilistic\ninterpretations are discussed. We illustrate the utility of the various\napproaches with an application to acoustic modelling for emotional speech\nsynthesis, where the unsupervised methods for learning expression control\n(without access to emotional labels) are found to give results that in many\naspects match or surpass the previous best supervised approach.", "published": "2018-07-30 17:59:28", "link": "http://arxiv.org/abs/1807.11470v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML", "62F99", "I.2.7; G.3"], "primary_category": "eess.AS"}
