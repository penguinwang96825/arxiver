{"title": "Using Similarity Measures to Select Pretraining Data for NER", "abstract": "Word vectors and Language Models (LMs) pretrained on a large amount of\nunlabelled data can dramatically improve various Natural Language Processing\n(NLP) tasks. However, the measure and impact of similarity between pretraining\ndata and target task data are left to intuition. We propose three\ncost-effective measures to quantify different aspects of similarity between\nsource pretraining and target task data. We demonstrate that these measures are\ngood predictors of the usefulness of pretrained models for Named Entity\nRecognition (NER) over 30 data pairs. Results also suggest that pretrained LMs\nare more effective and more predictable than pretrained word vectors, but\npretrained word vectors are better when pretraining data is dissimilar.", "published": "2019-04-01 06:45:45", "link": "http://arxiv.org/abs/1904.00585v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discontinuous Constituency Parsing with a Stack-Free Transition System\n  and a Dynamic Oracle", "abstract": "We introduce a novel transition system for discontinuous constituency\nparsing. Instead of storing subtrees in a stack --i.e. a data structure with\nlinear-time sequential access-- the proposed system uses a set of parsing\nitems, with constant-time random access. This change makes it possible to\nconstruct any discontinuous constituency tree in exactly $4n - 2$ transitions\nfor a sentence of length $n$. At each parsing step, the parser considers every\nitem in the set to be combined with a focus item and to construct a new\nconstituent in a bottom-up fashion. The parsing strategy is based on the\nassumption that most syntactic structures can be parsed incrementally and that\nthe set --the memory of the parser-- remains reasonably small on average.\nMoreover, we introduce a provably correct dynamic oracle for the new transition\nsystem, and present the first experiments in discontinuous constituency parsing\nusing a dynamic oracle. Our parser obtains state-of-the-art results on three\nEnglish and German discontinuous treebanks.", "published": "2019-04-01 07:49:19", "link": "http://arxiv.org/abs/1904.00615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Machine Translation with Embedding Prediction", "abstract": "Multimodal machine translation is an attractive application of neural machine\ntranslation (NMT). It helps computers to deeply understand visual objects and\ntheir relations with natural languages. However, multimodal NMT systems suffer\nfrom a shortage of available training data, resulting in poor performance for\ntranslating rare words. In NMT, pretrained word embeddings have been shown to\nimprove NMT of low-resource domains, and a search-based approach is proposed to\naddress the rare word problem. In this study, we effectively combine these two\napproaches in the context of multimodal NMT and explore how we can take full\nadvantage of pretrained word embeddings to better translate rare words. We\nreport overall performance improvements of 1.24 METEOR and 2.49 BLEU and\nachieve an improvement of 7.67 F-score for rare word translation.", "published": "2019-04-01 08:47:40", "link": "http://arxiv.org/abs/1904.00639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing Musical Entities in User-generated Content", "abstract": "Recognizing Musical Entities is important for Music Information Retrieval\n(MIR) since it can improve the performance of several tasks such as music\nrecommendation, genre classification or artist similarity. However, most entity\nrecognition systems in the music domain have concentrated on formal texts (e.g.\nartists' biographies, encyclopedic articles, etc.), ignoring rich and noisy\nuser-generated content. In this work, we present a novel method to recognize\nmusical entities in Twitter content generated by users following a classical\nmusic radio channel. Our approach takes advantage of both formal radio schedule\nand users' tweets to improve entity recognition. We instantiate several machine\nlearning algorithms to perform entity recognition combining task-specific and\ncorpus-based features. We also show how to improve recognition results by\njointly considering formal and user-generated content", "published": "2019-04-01 09:10:14", "link": "http://arxiv.org/abs/1904.00648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Interchangeability in Word Embedding Models", "abstract": "Nearest neighbors in word embedding models are commonly observed to be\nsemantically similar, but the relations between them can vary greatly. We\ninvestigate the extent to which word embedding models preserve syntactic\ninterchangeability, as reflected by distances between word vectors, and the\neffect of hyper-parameters---context window size in particular. We use part of\nspeech (POS) as a proxy for syntactic interchangeability, as generally\nspeaking, words with the same POS are syntactically valid in the same contexts.\nWe also investigate the relationship between interchangeability and similarity\nas judged by commonly-used word similarity benchmarks, and correlate the result\nwith the performance of word embedding models on these benchmarks. Our results\nwill inform future research and applications in the selection of word embedding\nmodel, suggesting a principle for an appropriate selection of the context\nwindow size parameter depending on the use-case.", "published": "2019-04-01 09:49:16", "link": "http://arxiv.org/abs/1904.00669v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human\n  Needs", "abstract": "To make machines better understand sentiments, research needs to move from\npolarity identification to understanding the reasons that underlie the\nexpression of sentiment. Categorizing the goals or needs of humans is one way\nto explain the expression of sentiment in text. Humans are good at\nunderstanding situations described in natural language and can easily connect\nthem to the character's psychological needs using commonsense knowledge. We\npresent a novel method to extract, rank, filter and select multi-hop relation\npaths from a commonsense knowledge resource to interpret the expression of\nsentiment in terms of their underlying human needs. We efficiently integrate\nthe acquired knowledge paths in a neural model that interfaces context\nrepresentations with knowledge using a gated attention mechanism. We assess the\nmodel's performance on a recently published dataset for categorizing human\nneeds. Selectively integrating knowledge paths boosts performance and\nestablishes a new state-of-the-art. Our model offers interpretability through\nthe learned attention map over commonsense knowledge paths. Human evaluation\nhighlights the relevance of the encoded knowledge.", "published": "2019-04-01 09:57:54", "link": "http://arxiv.org/abs/1904.00676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic text summarization: What has been done and what has to be done", "abstract": "Summaries are important when it comes to process huge amounts of information.\nTheir most important benefit is saving time, which we do not have much\nnowadays. Therefore, a summary must be short, representative and readable.\nGenerating summaries automatically can be beneficial for humans, since it can\nsave time and help selecting relevant documents. Automatic summarization and,\nin particular, Automatic text summarization (ATS) is not a new research field;\nIt was known since the 50s. Since then, researchers have been active to find\nthe perfect summarization method. In this article, we will discuss different\nworks in automatic summarization, especially the recent ones. We will present\nsome problems and limits which prevent works to move forward. Most of these\nchallenges are much more related to the nature of processed languages. These\nchallenges are interesting for academics and developers, as a path to follow in\nthis field.", "published": "2019-04-01 10:22:42", "link": "http://arxiv.org/abs/1904.00688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Abbreviation Disambiguation Contextual disambiguation using\n  word embeddings", "abstract": "Abbreviations often have several distinct meanings, often making their use in\ntext ambiguous. Expanding them to their intended meaning in context is\nimportant for Machine Reading tasks such as document search, recommendation and\nquestion answering. Existing approaches mostly rely on manually labeled\nexamples of abbreviations and their correct long-forms. Such data sets are\ncostly to create and result in trained models with limited applicability and\nflexibility. Importantly, most current methods must be subjected to a full\nempirical evaluation in order to understand their limitations, which is\ncumbersome in practice.\n  In this paper, we present an entirely unsupervised abbreviation\ndisambiguation method (called UAD) that picks up abbreviation definitions from\nunstructured text. Creating distinct tokens per meaning, we learn context\nrepresentations as word vectors. We demonstrate how to further boost\nabbreviation disambiguation performance by obtaining better context\nrepresentations using additional unstructured text. Our method is the first\nabbreviation disambiguation approach with a transparent model that allows\nperformance analysis without requiring full-scale evaluation, making it highly\nrelevant for real-world deployments.\n  In our thorough empirical evaluation, UAD achieves high performance on large\nreal-world data sets from different domains and outperforms both baseline and\nstate-of-the-art methods. UAD scales well and supports thousands of\nabbreviations with multiple different meanings within a single model.\n  In order to spur more research into abbreviation disambiguation, we publish a\nnew data set, that we also use in our experiments.", "published": "2019-04-01 15:59:18", "link": "http://arxiv.org/abs/1904.00929v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Interpretation: Predicting Untranslated Terminology in\n  Simultaneous Interpretation", "abstract": "Simultaneous interpretation, the translation of speech from one language to\nanother in real-time, is an inherently difficult and strenuous task. One of the\ngreatest challenges faced by interpreters is the accurate translation of\ndifficult terminology like proper names, numbers, or other entities.\nIntelligent computer-assisted interpreting (CAI) tools that could analyze the\nspoken word and detect terms likely to be untranslated by an interpreter could\nreduce translation error and improve interpreter performance. In this paper, we\npropose a task of predicting which terminology simultaneous interpreters will\nleave untranslated, and examine methods that perform this task using supervised\nsequence taggers. We describe a number of task-specific features explicitly\ndesigned to indicate when an interpreter may struggle with translating a word.\nExperimental results on a newly-annotated version of the NAIST Simultaneous\nTranslation Corpus (Shimizu et al., 2014) indicate the promise of our proposed\nmethod.", "published": "2019-04-01 16:03:59", "link": "http://arxiv.org/abs/1904.00930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Stop in Structured Prediction for Neural Machine Translation", "abstract": "Beam search optimization resolves many issues in neural machine translation.\nHowever, this method lacks principled stopping criteria and does not learn how\nto stop during training, and the model naturally prefers the longer hypotheses\nduring the testing time in practice since they use the raw score instead of the\nprobability-based score. We propose a novel ranking method which enables an\noptimal beam search stopping criteria. We further introduce a structured\nprediction loss function which penalizes suboptimal finished candidates\nproduced by beam search during training. Experiments of neural machine\ntranslation on both synthetic data and real languages (German-to-English and\nChinese-to-English) demonstrate our proposed methods lead to better length and\nBLEU score.", "published": "2019-04-01 18:01:08", "link": "http://arxiv.org/abs/1904.01032v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "abstract": "fairseq is an open-source sequence modeling toolkit that allows researchers\nand developers to train custom models for translation, summarization, language\nmodeling, and other text generation tasks. The toolkit is based on PyTorch and\nsupports distributed training across multiple GPUs and machines. We also\nsupport fast mixed-precision training and inference on modern GPUs. A demo\nvideo can be found at https://www.youtube.com/watch?v=OtgDdWtHvto", "published": "2019-04-01 18:05:02", "link": "http://arxiv.org/abs/1904.01038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAWS: Paraphrase Adversaries from Word Scrambling", "abstract": "Existing paraphrase identification datasets lack sentence pairs that have\nhigh lexical overlap without being paraphrases. Models trained on such data\nfail to distinguish pairs like flights from New York to Florida and flights\nfrom Florida to New York. This paper introduces PAWS (Paraphrase Adversaries\nfrom Word Scrambling), a new dataset with 108,463 well-formed paraphrase and\nnon-paraphrase pairs with high lexical overlap. Challenging pairs are generated\nby controlled word swapping and back translation, followed by fluency and\nparaphrase judgments by human raters. State-of-the-art models trained on\nexisting datasets have dismal performance on PAWS (<40% accuracy); however,\nincluding PAWS training data for these models improves their accuracy to 85%\nwhile maintaining performance on existing tasks. In contrast, models that do\nnot capture non-local contextual information fail even with PAWS training\nexamples. As such, PAWS provides an effective instrument for driving further\nprogress on models that better exploit structure, context, and pairwise\ncomparisons.", "published": "2019-04-01 22:21:14", "link": "http://arxiv.org/abs/1904.01130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Multi-Speaker Neural Text-to-Speech Systems using\n  Speaker-Imbalanced Speech Corpora", "abstract": "When the available data of a target speaker is insufficient to train a high\nquality speaker-dependent neural text-to-speech (TTS) system, we can combine\ndata from multiple speakers and train a multi-speaker TTS model instead. Many\nstudies have shown that neural multi-speaker TTS model trained with a small\namount data from multiple speakers combined can generate synthetic speech with\nbetter quality and stability than a speaker-dependent one. However when the\namount of data from each speaker is highly unbalanced, the best approach to\nmake use of the excessive data remains unknown. Our experiments showed that\nsimply combining all available data from every speaker to train a multi-speaker\nmodel produces better than or at least similar performance to its\nspeaker-dependent counterpart. Moreover by using an ensemble multi-speaker\nmodel, in which each subsystem is trained on a subset of available data, we can\nfurther improve the quality of the synthetic speech especially for\nunderrepresented speakers whose training data is limited.", "published": "2019-04-01 12:39:05", "link": "http://arxiv.org/abs/1904.00771v2", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes", "abstract": "Training large deep neural networks on massive datasets is computationally\nvery challenging. There has been recent surge in interest in using large batch\nstochastic optimization methods to tackle this issue. The most prominent\nalgorithm in this line of research is LARS, which by employing layerwise\nadaptive learning rates trains ResNet on ImageNet in a few minutes. However,\nLARS performs poorly for attention models like BERT, indicating that its\nperformance gains are not consistent across tasks. In this paper, we first\nstudy a principled layerwise adaptation strategy to accelerate training of deep\nneural networks using large mini-batches. Using this strategy, we develop a new\nlayerwise adaptive large batch optimization technique called LAMB; we then\nprovide convergence analysis of LAMB as well as LARS, showing convergence to a\nstationary point in general nonconvex settings. Our empirical results\ndemonstrate the superior performance of LAMB across various tasks such as BERT\nand ResNet-50 training with very little hyperparameter tuning. In particular,\nfor BERT training, our optimizer enables use of very large batch sizes of 32868\nwithout any degradation of performance. By increasing the batch size to the\nmemory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to\njust 76 minutes (Table 1). The LAMB implementation is available at\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "published": "2019-04-01 16:53:35", "link": "http://arxiv.org/abs/1904.00962v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sentiment analysis with genetically evolved Gaussian kernels", "abstract": "Sentiment analysis consists of evaluating opinions or statements from the\nanalysis of text. Among the methods used to estimate the degree in which a text\nexpresses a given sentiment, are those based on Gaussian Processes. However,\ntraditional Gaussian Processes methods use a predefined kernel with\nhyperparameters that can be tuned but whose structure can not be adapted. In\nthis paper, we propose the application of Genetic Programming for evolving\nGaussian Process kernels that are more precise for sentiment analysis. We use\nuse a very flexible representation of kernels combined with a multi-objective\napproach that simultaneously considers two quality metrics and the\ncomputational time spent by the kernels. Our results show that the algorithm\ncan outperform Gaussian Processes with traditional kernels for some of the\nsentiment analysis tasks considered.", "published": "2019-04-01 17:28:35", "link": "http://arxiv.org/abs/1904.00977v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual neTworks", "abstract": "We present JHU's system submission to the ASVspoof 2019 Challenge:\nAnti-Spoofing with Squeeze-Excitation and Residual neTworks (ASSERT).\nAnti-spoofing has gathered more and more attention since the inauguration of\nthe ASVspoof Challenges, and ASVspoof 2019 dedicates to address attacks from\nall three major types: text-to-speech, voice conversion, and replay. Built upon\nprevious research work on Deep Neural Network (DNN), ASSERT is a pipeline for\nDNN-based approach to anti-spoofing. ASSERT has four components: feature\nengineering, DNN models, network optimization and system combination, where the\nDNN models are variants of squeeze-excitation and residual networks. We\nconducted an ablation study of the effectiveness of each component on the\nASVspoof 2019 corpus, and experimental results showed that ASSERT obtained more\nthan 93% and 17% relative improvements over the baseline systems in the two\nsub-challenges in ASVspooof 2019, ranking ASSERT one of the top performing\nsystems. Code and pretrained models will be made publicly available.", "published": "2019-04-01 21:47:00", "link": "http://arxiv.org/abs/1904.01120v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Benchmarking Approximate Inference Methods for Neural Structured\n  Prediction", "abstract": "Exact structured inference with neural network scoring functions is\ncomputationally challenging but several methods have been proposed for\napproximating inference. One approach is to perform gradient descent with\nrespect to the output structure directly (Belanger and McCallum, 2016). Another\napproach, proposed recently, is to train a neural network (an \"inference\nnetwork\") to perform inference (Tu and Gimpel, 2018). In this paper, we compare\nthese two families of inference methods on three sequence labeling datasets. We\nchoose sequence labeling because it permits us to use exact inference as a\nbenchmark in terms of speed, accuracy, and search error. Across datasets, we\ndemonstrate that inference networks achieve a better speed/accuracy/search\nerror trade-off than gradient descent, while also being faster than exact\ninference at similar accuracy levels. We find further benefit by combining\ninference networks and gradient descent, using the former to provide a warm\nstart for the latter.", "published": "2019-04-01 23:08:05", "link": "http://arxiv.org/abs/1904.01138v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contrastive Predictive Coding Based Feature for Automatic Speaker\n  Verification", "abstract": "This thesis describes our ongoing work on Contrastive Predictive Coding (CPC)\nfeatures for speaker verification. CPC is a recently proposed representation\nlearning framework based on predictive coding and noise contrastive estimation.\nWe focus on incorporating CPC features into the standard automatic speaker\nverification systems, and we present our methods, experiments, and analysis.\nThis thesis also details necessary background knowledge in past and recent work\non automatic speaker verification systems, conventional speech features, and\nthe motivation and techniques behind CPC.", "published": "2019-04-01 23:54:08", "link": "http://arxiv.org/abs/1904.01575v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adaptation of Hierarchical Structured Models for Speech Act Recognition\n  in Asynchronous Conversation", "abstract": "We address the problem of speech act recognition (SAR) in asynchronous\nconversations (forums, emails). Unlike synchronous conversations (e.g.,\nmeetings, phone), asynchronous domains lack large labeled datasets to train an\neffective SAR model. In this paper, we propose methods to effectively leverage\nabundant unlabeled conversational data and the available labeled data from\nsynchronous domains. We carry out our research in three main steps. First, we\nintroduce a neural architecture based on hierarchical LSTMs and conditional\nrandom fields (CRF) for SAR, and show that our method outperforms existing\nmethods when trained on in-domain data only. Second, we improve our initial SAR\nmodels by semi-supervised learning in the form of pretrained word embeddings\nlearned from a large unlabeled conversational corpus. Finally, we employ\nadversarial training to improve the results further by leveraging the labeled\ndata from synchronous domains and by explicitly modeling the distributional\nshift in two domains.", "published": "2019-04-01 04:57:25", "link": "http://arxiv.org/abs/1904.04021v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Twitter Sentiment Analysis using Distributed Word and Sentence\n  Representation", "abstract": "An important part of the information gathering and data analysis is to find\nout what people think about, either a product or an entity. Twitter is an\nopinion rich social networking site. The posts or tweets from this data can be\nused for mining people's opinions. The recent surge of activity in this area\ncan be attributed to the computational treatment of data, which made opinion\nextraction and sentiment analysis easier. This paper classifies tweets into\npositive and negative sentiments, but instead of using traditional methods or\npreprocessing text data here we use the distributed representations of words\nand sentences to classify the tweets. We use Long Short Term Memory (LSTM)\nNetworks, Convolutional Neural Networks (CNNs) and Artificial Neural Networks.\nThe first two are used on Distributed Representation of words while the latter\nis used on the distributed representation of sentences. This paper achieves\naccuracies as high as 81%. It also suggests the best and optimal ways for\ncreating distributed representations of words for sentiment analysis, out of\nthe available methods.", "published": "2019-04-01 17:46:54", "link": "http://arxiv.org/abs/1904.12580v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Room Geometry Estimation from Room Impulse Responses using Convolutional\n  Neural Networks", "abstract": "We describe a new method to estimate the geometry of a room given room\nimpulse responses. The method utilises convolutional neural networks to\nestimate the room geometry and uses the mean square error as the loss function.\nIn contrast to existing methods, we do not require the position or distance of\nsources or receivers in the room. The method can be used with only a single\nroom impulse response between one source and one receiver for room geometry\nestimation. The proposed estimation method can achieve an average of six\ncentimetre accuracy. In addition, the proposed method is shown to be\ncomputationally efficient compared to state-of-the-art methods.", "published": "2019-04-01 14:13:13", "link": "http://arxiv.org/abs/1904.00869v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
