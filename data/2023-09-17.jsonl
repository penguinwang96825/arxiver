{"title": "A Benchmark for Text Expansion: Datasets, Metrics, and Baselines", "abstract": "This work presents a new task of Text Expansion (TE), which aims to insert\nfine-grained modifiers into proper locations of the plain text to concretize or\nvivify human writings. Different from existing insertion-based writing\nassistance tasks, TE requires the model to be more flexible in both locating\nand generation, and also more cautious in keeping basic semantics. We leverage\nfour complementary approaches to construct a dataset with 12 million\nautomatically generated instances and 2K human-annotated references for both\nEnglish and Chinese. To facilitate automatic evaluation, we design various\nmetrics from multiple perspectives. In particular, we propose Info-Gain to\neffectively measure the informativeness of expansions, which is an important\nquality dimension in TE. On top of a pre-trained text-infilling model, we build\nboth pipelined and joint Locate&Infill models, which demonstrate the\nsuperiority over the Text2Text baselines, especially in expansion\ninformativeness. Experiments verify the feasibility of the TE task and point\nout potential directions for future research toward better automatic text\nexpansion.", "published": "2023-09-17 07:54:38", "link": "http://arxiv.org/abs/2309.09198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Social Discourse to Measure Check-worthiness of Claims for\n  Fact-checking", "abstract": "The expansion of online social media platforms has led to a surge in online\ncontent consumption. However, this has also paved the way for disseminating\nfalse claims and misinformation. As a result, there is an escalating demand for\na substantial workforce to sift through and validate such unverified claims.\nCurrently, these claims are manually verified by fact-checkers. Still, the\nvolume of online content often outweighs their potency, making it difficult for\nthem to validate every single claim in a timely manner. Thus, it is critical to\ndetermine which assertions are worth fact-checking and prioritize claims that\nrequire immediate attention. Multiple factors contribute to determining whether\na claim necessitates fact-checking, encompassing factors such as its factual\ncorrectness, potential impact on the public, the probability of inciting\nhatred, and more. Despite several efforts to address claim check-worthiness, a\nsystematic approach to identify these factors remains an open challenge. To\nthis end, we introduce a new task of fine-grained claim check-worthiness, which\nunderpins all of these factors and provides probable human grounds for\nidentifying a claim as check-worthy. We present CheckIt, a manually annotated\nlarge Twitter dataset for fine-grained claim check-worthiness. We benchmark our\ndataset against a unified approach, CheckMate, that jointly determines whether\na claim is check-worthy and the factors that led to that conclusion. We compare\nour suggested system with several baseline systems. Finally, we report a\nthorough analysis of results and human assessment, validating the efficacy of\nintegrating check-worthiness factors in detecting claims worth fact-checking.", "published": "2023-09-17 13:42:41", "link": "http://arxiv.org/abs/2309.09274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OWL: A Large Language Model for IT Operations", "abstract": "With the rapid development of IT operations, it has become increasingly\ncrucial to efficiently manage and analyze large volumes of data for practical\napplications. The techniques of Natural Language Processing (NLP) have shown\nremarkable capabilities for various tasks, including named entity recognition,\nmachine translation and dialogue systems. Recently, Large Language Models\n(LLMs) have achieved significant improvements across various NLP downstream\ntasks. However, there is a lack of specialized LLMs for IT operations. In this\npaper, we introduce the OWL, a large language model trained on our collected\nOWL-Instruct dataset with a wide range of IT-related information, where the\nmixture-of-adapter strategy is proposed to improve the parameter-efficient\ntuning across different domains or tasks. Furthermore, we evaluate the\nperformance of our OWL on the OWL-Bench established by us and open IT-related\nbenchmarks. OWL demonstrates superior performance results on IT tasks, which\noutperforms existing models by significant margins. Moreover, we hope that the\nfindings of our work will provide more insights to revolutionize the techniques\nof IT operations with specialized LLMs.", "published": "2023-09-17 15:19:29", "link": "http://arxiv.org/abs/2309.09298v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoAM: An End-To-End Neural Model for Automatic and Universal Argument\n  Mining", "abstract": "Argument mining is to analyze argument structure and extract important\nargument information from unstructured text. An argument mining system can help\npeople automatically gain causal and logical information behind the text. As\nargumentative corpus gradually increases, like more people begin to argue and\ndebate on social media, argument mining from them is becoming increasingly\ncritical. However, argument mining is still a big challenge in natural language\ntasks due to its difficulty, and relative techniques are not mature. For\nexample, research on non-tree argument mining needs to be done more. Most works\njust focus on extracting tree structure argument information. Moreover, current\nmethods cannot accurately describe and capture argument relations and do not\npredict their types. In this paper, we propose a novel neural model called\nAutoAM to solve these problems. We first introduce the argument component\nattention mechanism in our model. It can capture the relevant information\nbetween argument components, so our model can better perform argument mining.\nOur model is a universal end-to-end framework, which can analyze argument\nstructure without constraints like tree structure and complete three subtasks\nof argument mining in one model. The experiment results show that our model\noutperforms the existing works on several metrics in two public datasets.", "published": "2023-09-17 15:26:21", "link": "http://arxiv.org/abs/2309.09300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance of the Pre-Trained Large Language Model GPT-4 on Automated\n  Short Answer Grading", "abstract": "Automated Short Answer Grading (ASAG) has been an active area of\nmachine-learning research for over a decade. It promises to let educators grade\nand give feedback on free-form responses in large-enrollment courses in spite\nof limited availability of human graders. Over the years, carefully trained\nmodels have achieved increasingly higher levels of performance. More recently,\npre-trained Large Language Models (LLMs) emerged as a commodity, and an\nintriguing question is how a general-purpose tool without additional training\ncompares to specialized models. We studied the performance of GPT-4 on the\nstandard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in\naddition to the standard task of grading the alignment of the student answer\nwith a reference answer, we also investigated withholding the reference answer.\nWe found that overall, the performance of the pre-trained general-purpose GPT-4\nLLM is comparable to hand-engineered models, but worse than pre-trained LLMs\nthat had specialized training.", "published": "2023-09-17 18:04:34", "link": "http://arxiv.org/abs/2309.09338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language models are susceptible to incorrect patient self-diagnosis in\n  medical applications", "abstract": "Large language models (LLMs) are becoming increasingly relevant as a\npotential tool for healthcare, aiding communication between clinicians,\nresearchers, and patients. However, traditional evaluations of LLMs on medical\nexam questions do not reflect the complexity of real patient-doctor\ninteractions. An example of this complexity is the introduction of patient\nself-diagnosis, where a patient attempts to diagnose their own medical\nconditions from various sources. While the patient sometimes arrives at an\naccurate conclusion, they more often are led toward misdiagnosis due to the\npatient's over-emphasis on bias validating information. In this work we present\na variety of LLMs with multiple-choice questions from United States medical\nboard exams which are modified to include self-diagnostic reports from\npatients. Our findings highlight that when a patient proposes incorrect\nbias-validating information, the diagnostic accuracy of LLMs drop dramatically,\nrevealing a high susceptibility to errors in self-diagnosis.", "published": "2023-09-17 19:56:39", "link": "http://arxiv.org/abs/2309.09362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embrace Divergence for Richer Insights: A Multi-document Summarization\n  Benchmark and a Case Study on Summarizing Diverse Information from News\n  Articles", "abstract": "Previous research in multi-document news summarization has typically\nconcentrated on collating information that all sources agree upon. However, the\nsummarization of diverse information dispersed across multiple articles about\nan event remains underexplored. In this paper, we propose a new task of\nsummarizing diverse information encountered in multiple news articles\nencompassing the same event. To facilitate this task, we outlined a data\ncollection schema for identifying diverse information and curated a dataset\nnamed DiverseSumm. The dataset includes 245 news stories, with each story\ncomprising 10 news articles and paired with a human-validated reference. Next,\nto enable consistent automatic evaluation, we conducted a comprehensive\nanalysis to pinpoint the position and verbosity biases when utilizing Large\nLanguage Model (LLM)-based metrics for evaluating the coverage and faithfulness\nof summaries. Through correlation analyses, we outline the best practices for\neffectively using automatic LLM-based metrics on the DiverseSumm dataset.\nFinally, we study how LLMs summarize multiple news articles by analyzing which\ntype of diverse information LLMs are capable of identifying. Our analyses\nsuggest that despite the extraordinary capabilities of LLMs in single-document\nsummarization, the proposed task remains a complex challenge for them mainly\ndue to their limited coverage, with GPT-4 only able to cover under 40% of the\ndiverse information on average.", "published": "2023-09-17 20:28:17", "link": "http://arxiv.org/abs/2309.09369v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Decoding Improves Reasoning in Large Language Models", "abstract": "We demonstrate that Contrastive Decoding -- a simple, computationally light,\nand training-free text generation method proposed by Li et al 2022 -- achieves\nlarge out-of-the-box improvements over greedy decoding on a variety of\nreasoning tasks. Originally shown to improve the perceived quality of long-form\ntext generation, Contrastive Decoding searches for strings that maximize a\nweighted difference in likelihood between strong and weak models. We show that\nContrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM\n2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA\n2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in\naddition to improvements on a collection of other tasks. Analysis suggests that\nContrastive Decoding improves over existing methods by preventing some abstract\nreasoning errors, as well as by avoiding simpler modes such as copying sections\nof the input during chain-of-thought. Overall, Contrastive Decoding outperforms\nnucleus sampling for long-form generation and greedy decoding for reasoning\ntasks, making it a powerful general purpose method for generating text from\nlanguage models.", "published": "2023-09-17 00:29:32", "link": "http://arxiv.org/abs/2309.09117v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "How much can ChatGPT really help Computational Biologists in\n  Programming?", "abstract": "ChatGPT, a recently developed product by openAI, is successfully leaving its\nmark as a multi-purpose natural language based chatbot. In this paper, we are\nmore interested in analyzing its potential in the field of computational\nbiology. A major share of work done by computational biologists these days\ninvolve coding up bioinformatics algorithms, analyzing data, creating\npipelining scripts and even machine learning modeling and feature extraction.\nThis paper focuses on the potential influence (both positive and negative) of\nChatGPT in the mentioned aspects with illustrative examples from different\nperspectives. Compared to other fields of computer science, computational\nbiology has - (1) less coding resources, (2) more sensitivity and bias issues\n(deals with medical data) and (3) more necessity of coding assistance (people\nfrom diverse background come to this field). Keeping such issues in mind, we\ncover use cases such as code writing, reviewing, debugging, converting,\nrefactoring and pipelining using ChatGPT from the perspective of computational\nbiologists in this paper.", "published": "2023-09-17 01:36:02", "link": "http://arxiv.org/abs/2309.09126v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Can Large Language Models Understand Real-World Complex Instructions?", "abstract": "Large language models (LLMs) can understand human instructions, showing their\npotential for pragmatic applications beyond traditional NLP tasks. However,\nthey still struggle with complex instructions, which can be either complex task\ndescriptions that require multiple tasks and constraints, or complex input that\ncontains long context, noise, heterogeneous information and multi-turn format.\nDue to these features, LLMs often ignore semantic constraints from task\ndescriptions, generate incorrect formats, violate length or sample count\nconstraints, and be unfaithful to the input text. Existing benchmarks are\ninsufficient to assess LLMs' ability to understand complex instructions, as\nthey are close-ended and simple. To bridge this gap, we propose CELLO, a\nbenchmark for evaluating LLMs' ability to follow complex instructions\nsystematically. We design eight features for complex instructions and construct\na comprehensive evaluation dataset from real-world scenarios. We also establish\nfour criteria and develop corresponding metrics, as current ones are\ninadequate, biased or too strict and coarse-grained. We compare the performance\nof representative Chinese-oriented and English-oriented models in following\ncomplex instructions through extensive experiments. Resources of CELLO are\npublicly available at https://github.com/Abbey4799/CELLO.", "published": "2023-09-17 04:18:39", "link": "http://arxiv.org/abs/2309.09150v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Code quality assessment using transformers", "abstract": "Automatically evaluate the correctness of programming assignments is rather\nstraightforward using unit and integration tests. However, programming tasks\ncan be solved in multiple ways, many of which, although correct, are inelegant.\nFor instance, excessive branching, poor naming or repetitiveness make the code\nhard to understand and maintain. These subjective qualities of code are hard to\nautomatically assess using current techniques. In this work we investigate the\nuse of CodeBERT to automatically assign quality score to Java code. We\nexperiment with different models and training paradigms. We explore the\naccuracy of the models on a novel dataset for code quality assessment. Finally,\nwe assess the quality of the predictions using saliency maps. We find that code\nquality to some extent is predictable and that transformer based models using\ntask adapted pre-training can solve the task more efficiently than other\ntechniques.", "published": "2023-09-17 12:59:59", "link": "http://arxiv.org/abs/2309.09264v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "How People Perceive The Dynamic Zero-COVID Policy: A Retrospective\n  Analysis From The Perspective of Appraisal Theory", "abstract": "The Dynamic Zero-COVID Policy in China spanned three years and diverse\nemotional responses have been observed at different times. In this paper, we\nretrospectively analyzed public sentiments and perceptions of the policy,\nespecially regarding how they evolved over time, and how they related to\npeople's lived experiences. Through sentiment analysis of 2,358 collected Weibo\nposts, we identified four representative points, i.e., policy initialization,\nsharp sentiment change, lowest sentiment score, and policy termination, for an\nin-depth discourse analysis through the lens of appraisal theory. In the end,\nwe reflected on the evolving public sentiments toward the Dynamic Zero-COVID\nPolicy and proposed implications for effective epidemic prevention and control\nmeasures for future crises.", "published": "2023-09-17 17:05:18", "link": "http://arxiv.org/abs/2309.09324v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Mitigating Shortcuts in Language Models with Soft Label Encoding", "abstract": "Recent research has shown that large language models rely on spurious\ncorrelations in the data for natural language understanding (NLU) tasks. In\nthis work, we aim to answer the following research question: Can we reduce\nspurious correlations by modifying the ground truth labels of the training\ndata? Specifically, we propose a simple yet effective debiasing framework,\nnamed Soft Label Encoding (SoftLE). We first train a teacher model with hard\nlabels to determine each sample's degree of relying on shortcuts. We then add\none dummy class to encode the shortcut degree, which is used to smooth other\ndimensions in the ground truth label to generate soft labels. This new ground\ntruth label is used to train a more robust student model. Extensive experiments\non two NLU benchmark tasks demonstrate that SoftLE significantly improves\nout-of-distribution generalization while maintaining satisfactory\nin-distribution accuracy.", "published": "2023-09-17 21:18:02", "link": "http://arxiv.org/abs/2309.09380v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\n  Language Models in 167 Languages", "abstract": "The driving factors behind the development of large language models (LLMs)\nwith impressive learning capabilities are their colossal model sizes and\nextensive training datasets. Along with the progress in natural language\nprocessing, LLMs have been frequently made accessible to the public to foster\ndeeper investigation and applications. However, when it comes to training\ndatasets for these LLMs, especially the recent state-of-the-art models, they\nare often not fully disclosed. Creating training data for high-performing LLMs\ninvolves extensive cleaning and deduplication to ensure the necessary level of\nquality. The lack of transparency for training data has thus hampered research\non attributing and addressing hallucination and bias issues in LLMs, hindering\nreplication efforts and further advancements in the community. These challenges\nbecome even more pronounced in multilingual learning scenarios, where the\navailable multilingual text datasets are often inadequately collected and\ncleaned. Consequently, there is a lack of open-source and readily usable\ndataset to effectively train LLMs in multiple languages. To overcome this\nissue, we present CulturaX, a substantial multilingual dataset with 6.3\ntrillion tokens in 167 languages, tailored for LLM development. Our dataset\nundergoes meticulous cleaning and deduplication through a rigorous pipeline of\nmultiple stages to accomplish the best quality for model training, including\nlanguage identification, URL-based filtering, metric-based cleaning, document\nrefinement, and data deduplication. CulturaX is fully released to the public in\nHuggingFace to facilitate research and advancements in multilingual LLMs:\nhttps://huggingface.co/datasets/uonlp/CulturaX.", "published": "2023-09-17 23:49:10", "link": "http://arxiv.org/abs/2309.09400v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting covariate drift in text data using document embeddings and\n  dimensionality reduction", "abstract": "Detecting covariate drift in text data is essential for maintaining the\nreliability and performance of text analysis models. In this research, we\ninvestigate the effectiveness of different document embeddings, dimensionality\nreduction techniques, and drift detection methods for identifying covariate\ndrift in text data. We explore three popular document embeddings: term\nfrequency-inverse document frequency (TF-IDF) using Latent semantic\nanalysis(LSA) for dimentionality reduction and Doc2Vec, and BERT embeddings,\nwith and without using principal component analysis (PCA) for dimensionality\nreduction. To quantify the divergence between training and test data\ndistributions, we employ the Kolmogorov-Smirnov (KS) statistic and the Maximum\nMean Discrepancy (MMD) test as drift detection methods. Experimental results\ndemonstrate that certain combinations of embeddings, dimensionality reduction\ntechniques, and drift detection methods outperform others in detecting\ncovariate drift. Our findings contribute to the advancement of reliable text\nanalysis models by providing insights into effective approaches for addressing\ncovariate drift in text data.", "published": "2023-09-17 07:34:57", "link": "http://arxiv.org/abs/2309.10000v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SplitEE: Early Exit in Deep Neural Networks with Split Computing", "abstract": "Deep Neural Networks (DNNs) have drawn attention because of their outstanding\nperformance on various tasks. However, deploying full-fledged DNNs in\nresource-constrained devices (edge, mobile, IoT) is difficult due to their\nlarge size. To overcome the issue, various approaches are considered, like\noffloading part of the computation to the cloud for final inference (split\ncomputing) or performing the inference at an intermediary layer without passing\nthrough all layers (early exits). In this work, we propose combining both\napproaches by using early exits in split computing. In our approach, we decide\nup to what depth of DNNs computation to perform on the device (splitting layer)\nand whether a sample can exit from this layer or need to be offloaded. The\ndecisions are based on a weighted combination of accuracy, computational, and\ncommunication costs. We develop an algorithm named SplitEE to learn an optimal\npolicy. Since pre-trained DNNs are often deployed in new domains where the\nground truths may be unavailable and samples arrive in a streaming fashion,\nSplitEE works in an online and unsupervised setup. We extensively perform\nexperiments on five different datasets. SplitEE achieves a significant cost\nreduction ($>50\\%$) with a slight drop in accuracy ($<2\\%$) as compared to the\ncase when all samples are inferred at the final layer. The anonymized source\ncode is available at\n\\url{https://anonymous.4open.science/r/SplitEE_M-B989/README.md}.", "published": "2023-09-17 07:48:22", "link": "http://arxiv.org/abs/2309.09195v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Model-based Subsampling for Knowledge Graph Completion", "abstract": "Subsampling is effective in Knowledge Graph Embedding (KGE) for reducing\noverfitting caused by the sparsity in Knowledge Graph (KG) datasets. However,\ncurrent subsampling approaches consider only frequencies of queries that\nconsist of entities and their relations. Thus, the existing subsampling\npotentially underestimates the appearance probabilities of infrequent queries\neven if the frequencies of their entities or relations are high. To address\nthis problem, we propose Model-based Subsampling (MBS) and Mixed Subsampling\n(MIX) to estimate their appearance probabilities through predictions of KGE\nmodels. Evaluation results on datasets FB15k-237, WN18RR, and YAGO3-10 showed\nthat our proposed subsampling methods actually improved the KG completion\nperformances for popular KGE models, RotatE, TransE, HAKE, ComplEx, and\nDistMult.", "published": "2023-09-17 15:12:50", "link": "http://arxiv.org/abs/2309.09296v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Few-Shot Approach to Dysarthric Speech Intelligibility Level\n  Classification Using Transformers", "abstract": "Dysarthria is a speech disorder that hinders communication due to\ndifficulties in articulating words. Detection of dysarthria is important for\nseveral reasons as it can be used to develop a treatment plan and help improve\na person's quality of life and ability to communicate effectively. Much of the\nliterature focused on improving ASR systems for dysarthric speech. The\nobjective of the current work is to develop models that can accurately classify\nthe presence of dysarthria and also give information about the intelligibility\nlevel using limited data by employing a few-shot approach using a transformer\nmodel. This work also aims to tackle the data leakage that is present in\nprevious studies. Our whisper-large-v2 transformer model trained on a subset of\nthe UASpeech dataset containing medium intelligibility level patients achieved\nan accuracy of 85%, precision of 0.92, recall of 0.8 F1-score of 0.85, and\nspecificity of 0.91. Experimental results also demonstrate that the model\ntrained using the 'words' dataset performed better compared to the model\ntrained on the 'letters' and 'digits' dataset. Moreover, the multiclass model\nachieved an accuracy of 67%.", "published": "2023-09-17 17:23:41", "link": "http://arxiv.org/abs/2309.09329v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\n  Large-Language-Model", "abstract": "Despite the plethora of telehealth applications to assist home-based older\nadults and healthcare providers, basic messaging and phone calls are still the\nmost common communication methods, which suffer from limited availability,\ninformation loss, and process inefficiencies. One promising solution to\nfacilitate patient-provider communication is to leverage large language models\n(LLMs) with their powerful natural conversation and summarization capability.\nHowever, there is a limited understanding of LLMs' role during the\ncommunication. We first conducted two interview studies with both older adults\n(N=10) and healthcare providers (N=9) to understand their needs and\nopportunities for LLMs in patient-provider asynchronous communication. Based on\nthe insights, we built an LLM-powered communication system, Talk2Care, and\ndesigned interactive components for both groups: (1) For older adults, we\nleveraged the convenience and accessibility of voice assistants (VAs) and built\nan LLM-powered VA interface for effective information collection. (2) For\nhealth providers, we built an LLM-based dashboard to summarize and present\nimportant health information based on older adults' conversations with the VA.\nWe further conducted two user studies with older adults and providers to\nevaluate the usability of the system. The results showed that Talk2Care could\nfacilitate the communication process, enrich the health information collected\nfrom older adults, and considerably save providers' efforts and time. We\nenvision our work as an initial exploration of LLMs' capability in the\nintersection of healthcare and interpersonal communication.", "published": "2023-09-17 19:46:03", "link": "http://arxiv.org/abs/2309.09357v5", "categories": ["cs.CL", "cs.AI", "cs.HC", "68U35", "H.5.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Augmenting text for spoken language understanding with Large Language\n  Models", "abstract": "Spoken semantic parsing (SSP) involves generating machine-comprehensible\nparses from input speech. Training robust models for existing application\ndomains represented in training data or extending to new domains requires\ncorresponding triplets of speech-transcript-semantic parse data, which is\nexpensive to obtain. In this paper, we address this challenge by examining\nmethods that can use transcript-semantic parse data (unpaired text) without\ncorresponding speech. First, when unpaired text is drawn from existing textual\ncorpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are compared as ways\nto generate speech representations for unpaired text. Experiments on the STOP\ndataset show that unpaired text from existing and new domains improves\nperformance by 2% and 30% in absolute Exact Match (EM) respectively. Second, we\nconsider the setting when unpaired text is not available in existing textual\ncorpora. We propose to prompt Large Language Models (LLMs) to generate unpaired\ntext for existing and new domains. Experiments show that examples and words\nthat co-occur with intents can be used to generate unpaired text with Llama\n2.0. Using the generated text with JAT and TTS for spoken semantic parsing\nimproves EM on STOP by 1.4% and 2.6% absolute for existing and new domains\nrespectively.", "published": "2023-09-17 22:25:34", "link": "http://arxiv.org/abs/2309.09390v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Do Large GPT Models Discover Moral Dimensions in Language\n  Representations? A Topological Study Of Sentence Embeddings", "abstract": "As Large Language Models are deployed within Artificial Intelligence systems,\nthat are increasingly integrated with human society, it becomes more important\nthan ever to study their internal structures. Higher level abilities of LLMs\nsuch as GPT-3.5 emerge in large part due to informative language\nrepresentations they induce from raw text data during pre-training on trillions\nof words. These embeddings exist in vector spaces of several thousand\ndimensions, and their processing involves mapping between multiple vector\nspaces, with total number of parameters on the order of trillions. Furthermore,\nthese language representations are induced by gradient optimization, resulting\nin a black box system that is hard to interpret. In this paper, we take a look\nat the topological structure of neuronal activity in the \"brain\" of Chat-GPT's\nfoundation language model, and analyze it with respect to a metric representing\nthe notion of fairness. We develop a novel approach to visualize GPT's moral\ndimensions. We first compute a fairness metric, inspired by social psychology\nliterature, to identify factors that typically influence fairness assessments\nin humans, such as legitimacy, need, and responsibility. Subsequently, we\nsummarize the manifold's shape using a lower-dimensional simplicial complex,\nwhose topology is derived from this metric. We color it with a heat map\nassociated with this fairness metric, producing human-readable visualizations\nof the high-dimensional sentence manifold. Our results show that sentence\nembeddings based on GPT-3.5 can be decomposed into two submanifolds\ncorresponding to fair and unfair moral judgments. This indicates that GPT-based\nlanguage models develop a moral dimension within their representation spaces\nand induce an understanding of fairness during their training process.", "published": "2023-09-17 23:38:39", "link": "http://arxiv.org/abs/2309.09397v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A novel approach to measuring the scope of patent claims based on\n  probabilities obtained from (large) language models", "abstract": "This work proposes to measure the scope of a patent claim as the reciprocal\nof self-information contained in this claim. Self-information is calculated\nbased on a probability of occurrence of the claim, where this probability is\nobtained from a language model. Grounded in information theory, this approach\nis based on the assumption that an unlikely concept is more informative than a\nusual concept, insofar as it is more surprising. In turn, the more surprising\nthe information required to define the claim, the narrower its scope. Seven\nlanguage models are considered, ranging from simplest models (each word or\ncharacter has an identical probability) to intermediate models (based on\naverage word or character frequencies), to large language models (LLMs) such as\nGPT2 and davinci-002. Remarkably, when using the simplest language models to\ncompute the probabilities, the scope becomes proportional to the reciprocal of\nthe number of words or characters involved in the claim, a metric already used\nin previous works. Application is made to multiple series of patent claims\ndirected to distinct inventions, where each series consists of claims devised\nto have a gradually decreasing scope. The performance of the language models is\nthen assessed through several ad hoc tests. The LLMs outperform models based on\nword and character frequencies, which themselves outdo the simplest models\nbased on word or character counts. Interestingly, however, the character count\nappears to be a more reliable indicator than the word count.", "published": "2023-09-17 16:50:07", "link": "http://arxiv.org/abs/2309.10003v4", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT", "math.PR", "62P99, 60E05", "I.2.m; G.3"], "primary_category": "cs.CL"}
{"title": "Zero- and Few-shot Sound Event Localization and Detection", "abstract": "Sound event localization and detection (SELD) systems estimate\ndirection-of-arrival (DOA) and temporal activation for sets of target classes.\nNeural network (NN)-based SELD systems have performed well in various sets of\ntarget classes, but they only output the DOA and temporal activation of preset\nclasses trained before inference. To customize target classes after training,\nwe tackle zero- and few-shot SELD tasks, in which we set new classes with a\ntext sample or a few audio samples. While zero-shot sound classification tasks\nare achievable by embedding from contrastive language-audio pretraining (CLAP),\nzero-shot SELD tasks require assigning an activity and a DOA to each embedding,\nespecially in overlapping cases. To tackle the assignment problem in\noverlapping cases, we propose an embed-ACCDOA model, which is trained to output\ntrack-wise CLAP embedding and corresponding activity-coupled Cartesian\ndirection-of-arrival (ACCDOA). In our experimental evaluations on zero- and\nfew-shot SELD tasks, the embed-ACCDOA model showed better location-dependent\nscores than a straightforward combination of the CLAP audio encoder and a DOA\nestimation model. Moreover, the proposed combination of the embed-ACCDOA model\nand CLAP audio encoder with zero- or few-shot samples performed comparably to\nan official baseline system trained with complete train data in an evaluation\ndataset.", "published": "2023-09-17 09:29:15", "link": "http://arxiv.org/abs/2309.09223v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by\n  Natural Language Prompts", "abstract": "Style voice conversion aims to transform the style of source speech to a\ndesired style according to real-world application demands. However, the current\nstyle voice conversion approach relies on pre-defined labels or reference\nspeech to control the conversion process, which leads to limitations in style\ndiversity or falls short in terms of the intuitive and interpretability of\nstyle representation. In this study, we propose PromptVC, a novel style voice\nconversion approach that employs a latent diffusion model to generate a style\nvector driven by natural language prompts. Specifically, the style vector is\nextracted by a style encoder during training, and then the latent diffusion\nmodel is trained independently to sample the style vector from noise, with this\nprocess being conditioned on natural language prompts. To improve style\nexpressiveness, we leverage HuBERT to extract discrete tokens and replace them\nwith the K-Means center embedding to serve as the linguistic content, which\nminimizes residual style information. Additionally, we deduplicate the same\ndiscrete token and employ a differentiable duration predictor to re-predict the\nduration of each token, which can adapt the duration of the same linguistic\ncontent to different styles. The subjective and objective evaluation results\ndemonstrate the effectiveness of our proposed system.", "published": "2023-09-17 12:58:27", "link": "http://arxiv.org/abs/2309.09262v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sound Source Distance Estimation in Diverse and Dynamic Acoustic\n  Conditions", "abstract": "Localizing a moving sound source in the real world involves determining its\ndirection-of-arrival (DOA) and distance relative to a microphone. Advancements\nin DOA estimation have been facilitated by data-driven methods optimized with\nlarge open-source datasets with microphone array recordings in diverse\nenvironments. In contrast, estimating a sound source's distance remains\nunderstudied. Existing approaches assume recordings by non-coincident\nmicrophones to use methods that are susceptible to differences in room\nreverberation. We present a CRNN able to estimate the distance of moving sound\nsources across multiple datasets featuring diverse rooms, outperforming a\nrecently-published approach. We also characterize our model's performance as a\nfunction of sound source distance and different training losses. This analysis\nreveals optimal training using a loss that weighs model errors as an inverse\nfunction of the sound source true distance. Our study is the first to\ndemonstrate that sound source distance estimation can be performed across\ndiverse acoustic conditions using deep learning.", "published": "2023-09-17 14:49:08", "link": "http://arxiv.org/abs/2309.09288v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Quantised End-to-End ASR Models via Personalisation", "abstract": "Recent end-to-end automatic speech recognition (ASR) models have become\nincreasingly larger, making them particularly challenging to be deployed on\nresource-constrained devices. Model quantisation is an effective solution that\nsometimes causes the word error rate (WER) to increase. In this paper, a novel\nstrategy of personalisation for a quantised model (PQM) is proposed, which\ncombines speaker adaptive training (SAT) with model quantisation to improve the\nperformance of heavily compressed models. Specifically, PQM uses a 4-bit\nNormalFloat Quantisation (NF4) approach for model quantisation and low-rank\nadaptation (LoRA) for SAT. Experiments have been performed on the LibriSpeech\nand the TED-LIUM 3 corpora. Remarkably, with a 7x reduction in model size and\n1% additional speaker-specific parameters, 15.1% and 23.3% relative WER\nreductions were achieved on quantised Whisper and Conformer-based\nattention-based encoder-decoder ASR models respectively, comparing to the\noriginal full precision models.", "published": "2023-09-17 02:35:21", "link": "http://arxiv.org/abs/2309.09136v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding\n  with Sequence-to-Sequence Architecture", "abstract": "We propose a novel neural speaker diarization system using memory-aware\nmulti-speaker embedding with sequence-to-sequence architecture (NSD-MS2S),\nwhich integrates the strengths of memory-aware multi-speaker embedding (MA-MSE)\nand sequence-to-sequence (Seq2Seq) architecture, leading to improvement in both\nefficiency and performance. Next, we further decrease the memory occupation of\ndecoding by incorporating input features fusion and then employ a multi-head\nattention mechanism to capture features at different levels. NSD-MS2S achieved\na macro diarization error rate (DER) of 15.9% on the CHiME-7 EVAL set, which\nsignifies a relative improvement of 49% over the official baseline system, and\nis the key technique for us to achieve the best performance for the main track\nof CHiME-7 DASR Challenge. Additionally, we introduce a deep interactive module\n(DIM) in MA-MSE module to better retrieve a cleaner and more discriminative\nmulti-speaker embedding, enabling the current model to outperform the system we\nused in the CHiME-7 DASR Challenge. Our code will be available at\nhttps://github.com/liyunlongaaa/NSD-MS2S.", "published": "2023-09-17 07:08:06", "link": "http://arxiv.org/abs/2309.09180v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Speech Inversion Through Self-Supervised Embeddings and\n  Enhanced Tract Variables", "abstract": "The performance of deep learning models depends significantly on their\ncapacity to encode input features efficiently and decode them into meaningful\noutputs. Better input and output representation has the potential to boost\nmodels' performance and generalization. In the context of\nacoustic-to-articulatory speech inversion (SI) systems, we study the impact of\nutilizing speech representations acquired via self-supervised learning (SSL)\nmodels, such as HuBERT compared to conventional acoustic features.\nAdditionally, we investigate the incorporation of novel tract variables (TVs)\nthrough an improved geometric transformation model. By combining these two\napproaches, we improve the Pearson product-moment correlation (PPMC) scores\nwhich evaluate the accuracy of TV estimation of the SI system from 0.7452 to\n0.8141, a 6.9% increase. Our findings underscore the profound influence of rich\nfeature representations from SSL models and improved geometric transformations\nwith target TVs on the enhanced functionality of SI systems.", "published": "2023-09-17 09:18:04", "link": "http://arxiv.org/abs/2309.09220v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Continuous Modeling of the Denoising Process for Speech Enhancement\n  Based on Deep Learning", "abstract": "In this paper, we explore a continuous modeling approach for\ndeep-learning-based speech enhancement, focusing on the denoising process. We\nuse a state variable to indicate the denoising process. The starting state is\nnoisy speech and the ending state is clean speech. The noise component in the\nstate variable decreases with the change of the state index until the noise\ncomponent is 0. During training, a UNet-like neural network learns to estimate\nevery state variable sampled from the continuous denoising process. In testing,\nwe introduce a controlling factor as an embedding, ranging from zero to one, to\nthe neural network, allowing us to control the level of noise reduction. This\napproach enables controllable speech enhancement and is adaptable to various\napplication scenarios. Experimental results indicate that preserving a small\namount of noise in the clean target benefits speech enhancement, as evidenced\nby improvements in both objective speech measures and automatic speech\nrecognition performance.", "published": "2023-09-17 13:27:11", "link": "http://arxiv.org/abs/2309.09270v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
