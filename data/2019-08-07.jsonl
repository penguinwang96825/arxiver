{"title": "Fast and Accurate Capitalization and Punctuation for Automatic Speech\n  Recognition Using Transformer and Chunk Merging", "abstract": "In recent years, studies on automatic speech recognition (ASR) have shown\noutstanding results that reach human parity on short speech segments. However,\nthere are still difficulties in standardizing the output of ASR such as\ncapitalization and punctuation restoration for long-speech transcription. The\nproblems obstruct readers to understand the ASR output semantically and also\ncause difficulties for natural language processing models such as NER, POS and\nsemantic parsing. In this paper, we propose a method to restore the punctuation\nand capitalization for long-speech ASR transcription. The method is based on\nTransformer models and chunk merging that allows us to (1), build a single\nmodel that performs punctuation and capitalization in one go, and (2), perform\ndecoding in parallel while improving the prediction accuracy. Experiments on\nBritish National Corpus showed that the proposed approach outperforms existing\nmethods in both accuracy and decoding speed.", "published": "2019-08-07 00:04:58", "link": "http://arxiv.org/abs/1908.02404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ab Antiquo: Neural Proto-language Reconstruction", "abstract": "Historical linguists have identified regularities in the process of historic\nsound change. The comparative method utilizes those regularities to reconstruct\nproto-words based on observed forms in daughter languages. Can this process be\nefficiently automated? We address the task of proto-word reconstruction, in\nwhich the model is exposed to cognates in contemporary daughter languages, and\nhas to predict the proto word in the ancestor language. We provide a novel\ndataset for this task, encompassing over 8,000 comparative entries, and show\nthat neural sequence models outperform conventional methods applied to this\ntask so far. Error analysis reveals variability in the ability of neural model\nto capture different phonological changes, correlating with the complexity of\nthe changes. Analysis of learned embeddings reveals the models learn\nphonologically meaningful generalizations, corresponding to well-attested\nphonological shifts documented by historical linguistics.", "published": "2019-08-07 08:03:08", "link": "http://arxiv.org/abs/1908.02477v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding-based system for the Text part of CALL v3 shared task", "abstract": "This paper presents a scoring system that has shown the top result on the\ntext subset of CALL v3 shared task. The presented system is based on text\nembeddings, namely NNLM~\\cite{nnlm} and BERT~\\cite{Bert}. The distinguishing\nfeature of the given approach is that it does not rely on the reference grammar\nfile for scoring. The model is compared against approaches that use the grammar\nfile and proves the possibility to achieve similar and even higher results\nwithout a predefined set of correct answers.\n  The paper describes the model itself and the data preparation process that\nplayed a crucial role in the model training.", "published": "2019-08-07 09:44:31", "link": "http://arxiv.org/abs/1908.02505v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Simple and Effective Approach for Fine Tuning Pre-trained Word\n  Embeddings for Improved Text Classification", "abstract": "This work presents a new and simple approach for fine-tuning pretrained word\nembeddings for text classification tasks. In this approach, the class in which\na term appears, acts as an additional contextual variable during the fine\ntuning process, and contributes to the final word vector for that term. As a\nresult, words that are used distinctively within a particular class, will bear\nvectors that are closer to each other in the embedding space and will be more\ndiscriminative towards that class. To validate this novel approach, it was\napplied to three Arabic and two English datasets that have been previously used\nfor text classification tasks such as sentiment analysis and emotion detection.\nIn the vast majority of cases, the results obtained using the proposed\napproach, improved considerably.", "published": "2019-08-07 12:32:33", "link": "http://arxiv.org/abs/1908.02579v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text mining policy: Classifying forest and landscape restoration policy\n  agenda with neural information retrieval", "abstract": "Dozens of countries have committed to restoring the ecological functionality\nof 350 million hectares of land by 2030. In order to achieve such wide-scale\nimplementation of restoration, the values and priorities of multi-sectoral\nstakeholders must be aligned and integrated with national level commitments and\nother development agenda. Although misalignment across scales of policy and\nbetween stakeholders are well known barriers to implementing restoration,\nfast-paced policy making in multi-stakeholder environments complicates the\nmonitoring and analysis of governance and policy. In this work, we assess the\npotential of machine learning to identify restoration policy agenda across\ndiverse policy documents. An unsupervised neural information retrieval\narchitecture is introduced that leverages transfer learning and word embeddings\nto create high-dimensional representations of paragraphs. Policy agenda labels\nare recast as information retrieval queries in order to classify policies with\na cosine similarity threshold between paragraphs and query embeddings. This\napproach achieves a 0.83 F1-score measured across 14 policy agenda in 31 policy\ndocuments in Malawi, Kenya, and Rwanda, indicating that automated text mining\ncan provide reliable, generalizable, and efficient analyses of restoration\npolicy.", "published": "2019-08-07 02:58:24", "link": "http://arxiv.org/abs/1908.02425v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TinySearch -- Semantics based Search Engine using Bert Embeddings", "abstract": "Existing search engines use keyword matching or tf-idf based matching to map\nthe query to the web-documents and rank them. They also consider other factors\nsuch as page rank, hubs-and-authority scores, knowledge graphs to make the\nresults more meaningful. However, the existing search engines fail to capture\nthe meaning of query when it becomes large and complex. BERT, introduced by\nGoogle in 2018, provides embeddings for words as well as sentences. In this\npaper, I have developed a semantics-oriented search engine using neural\nnetworks and BERT embeddings that can search for query and rank the documents\nin the order of the most meaningful to least meaningful. The results shows\nimprovement over one existing search engine for complex queries for given set\nof documents.", "published": "2019-08-07 06:02:17", "link": "http://arxiv.org/abs/1908.02451v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Debiasing Embeddings for Reduced Gender Bias in Text Classification", "abstract": "(Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can\ninherit gender bias from the data they were trained on. We investigate how this\nbias affects downstream classification tasks, using the case study of\noccupation classification (De-Arteaga et al.,2019). We show that traditional\ntechniques for debiasing embeddings can actually worsen the bias of the\ndownstream classifier by providing a less noisy channel for communicating\ngender information. With a relatively minor adjustment, however, we show how\nthese same techniques can be used to simultaneously reduce bias and maintain\nhigh classification accuracy.", "published": "2019-08-07 19:46:11", "link": "http://arxiv.org/abs/1908.02810v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Hitchhiker's Guide to LDA", "abstract": "Latent Dirichlet Allocation (LDA) model is a famous model in the topic model\nfield, it has been studied for years due to its extensive application value in\nindustry and academia. However, the mathematical derivation of LDA model is\nchallenging and difficult, which makes it difficult for the beginners to learn.\nTo help the beginners in learning LDA, this book analyzes the mathematical\nderivation of LDA in detail, and it also introduces all the knowledge\nbackground to make it easy for beginners to understand. Thus, this book\ncontains the author's unique insights. It should be noted that this book is\nwritten in Chinese.", "published": "2019-08-07 03:59:19", "link": "http://arxiv.org/abs/1908.03142v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Audio-visual Speech Enhancement Using Conditional Variational\n  Auto-Encoders", "abstract": "Variational auto-encoders (VAEs) are deep generative latent variable models\nthat can be used for learning the distribution of complex data. VAEs have been\nsuccessfully used to learn a probabilistic prior over speech signals, which is\nthen used to perform speech enhancement. One advantage of this generative\napproach is that it does not require pairs of clean and noisy speech signals at\ntraining. In this paper, we propose audio-visual variants of VAEs for\nsingle-channel and speaker-independent speech enhancement. We develop a\nconditional VAE (CVAE) where the audio speech generative process is conditioned\non visual information of the lip region. At test time, the audio-visual speech\ngenerative model is combined with a noise model based on nonnegative matrix\nfactorization, and speech enhancement relies on a Monte Carlo\nexpectation-maximization algorithm. Experiments are conducted with the recently\npublished NTCD-TIMIT dataset as well as the GRID corpus. The results confirm\nthat the proposed audio-visual CVAE effectively fuses audio and visual\ninformation, and it improves the speech enhancement performance compared with\nthe audio-only VAE model, especially when the speech signal is highly corrupted\nby noise. We also show that the proposed unsupervised audio-visual speech\nenhancement approach outperforms a state-of-the-art supervised deep learning\nmethod.", "published": "2019-08-07 12:38:32", "link": "http://arxiv.org/abs/1908.02590v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervised Attention Model for Weakly Labeled Audio Event\n  Classification", "abstract": "We describe a novel weakly labeled Audio Event Classification approach based\non a self-supervised attention model. The weakly labeled framework is used to\neliminate the need for expensive data labeling procedure and self-supervised\nattention is deployed to help a model distinguish between relevant and\nirrelevant parts of a weakly labeled audio clip in a more effective manner\ncompared to prior attention models. We also propose a highly effective strongly\nsupervised attention model when strong labels are available. This model also\nserves as an upper bound for the self-supervised model. The performances of the\nmodel with self-supervised attention training are comparable to the strongly\nsupervised one which is trained using strong labels. We show that our\nself-supervised attention method is especially beneficial for short audio\nevents. We achieve 8.8% and 17.6% relative mean average precision improvements\nover the current state-of-the-art systems for SL-DCASE-17 and balanced\nAudioSet.", "published": "2019-08-07 23:48:34", "link": "http://arxiv.org/abs/1908.02876v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Pitch-Synchronous Single Frequency Filtering Spectrogram for Speech\n  Emotion Recognition", "abstract": "Convolutional neural networks (CNN) are widely used for speech emotion\nrecognition (SER). In such cases, the short time fourier transform (STFT)\nspectrogram is the most popular choice for representing speech, which is fed as\ninput to the CNN. However, the uncertainty principles of the short-time Fourier\ntransform prevent it from capturing time and frequency resolutions\nsimultaneously. On the other hand, the recently proposed single frequency\nfiltering (SFF) spectrogram promises to be a better alternative because it\ncaptures both time and frequency resolutions simultaneously. In this work, we\nexplore the SFF spectrogram as an alternative representation of speech for SER.\nWe have modified the SFF spectrogram by taking the average of the amplitudes of\nall the samples between two successive glottal closure instants (GCI)\nlocations. The duration between two successive GCI locations gives the pitch,\nmotivating us to name the modified SFF spectrogram as pitch-synchronous SFF\nspectrogram. The GCI locations were detected using zero frequency filtering\napproach. The proposed pitch-synchronous SFF spectrogram produced accuracy\nvalues of 63.95% (unweighted) and 70.4% (weighted) on the IEMOCAP dataset.\nThese correspond to an improvement of +7.35% (unweighted) and +4.3% (weighted)\nover state-of-the-art result on the STFT sepctrogram using CNN. Specially, the\nproposed method recognized 22.7% of the happy emotion samples correctly,\nwhereas this number was 0% for state-of-the-art results. These results also\npromise a much wider use of the proposed pitch-synchronous SFF spectrogram for\nother speech-based applications.", "published": "2019-08-07 11:49:58", "link": "http://arxiv.org/abs/1908.03054v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Viterbi Extraction tutorial with Hidden Markov Toolkit", "abstract": "An algorithm used to extract HMM parameters is revisited. Most parts of the\nextraction process are taken from implemented Hidden Markov Toolkit (HTK)\nprogram under name HInit. The algorithm itself shows a few variations compared\nto another domain of implementations. The HMM model is introduced briefly based\non the theory of Discrete Time Markov Chain. We schematically outline the\nViterbi method implemented in HTK. Iterative definition of the method which is\nready to be implemented in computer programs is reviewed. We also illustrate\nthe method calculation precisely using manual calculation and extensive\ngraphical illustration. The distribution of observation probability used is\nsimply independent Gaussians r.v.s. The purpose of the content is not to\njustify the performance or accuracy of the method applied in a specific area.\nThis writing merely to describe how the algorithm is performed. The whole\ncontent should enlighten the audience the insight of the Viterbi Extraction\nmethod used by HTK.", "published": "2019-08-07 03:38:22", "link": "http://arxiv.org/abs/1908.03143v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Understanding Optical Music Recognition", "abstract": "For over 50 years, researchers have been trying to teach computers to read\nmusic notation, referred to as Optical Music Recognition (OMR). However, this\nfield is still difficult to access for new researchers, especially those\nwithout a significant musical background: few introductory materials are\navailable, and furthermore the field has struggled with defining itself and\nbuilding a shared terminology. In this tutorial, we address these shortcomings\nby (1) providing a robust definition of OMR and its relationship to related\nfields, (2) analyzing how OMR inverts the music encoding process to recover the\nmusical notation and the musical semantics from documents, (3) proposing a\ntaxonomy of OMR, with most notably a novel taxonomy of applications.\nAdditionally, we discuss how deep learning affects modern OMR research, as\nopposed to the traditional pipeline. Based on this work, the reader should be\nable to attain a basic understanding of OMR: its objectives, its inherent\nstructure, its relationship to other fields, the state of the art, and the\nresearch opportunities it affords.", "published": "2019-08-07 08:37:16", "link": "http://arxiv.org/abs/1908.03608v3", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
