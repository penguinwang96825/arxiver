{"title": "Leveraging Few-Shot Data Augmentation and Waterfall Prompting for\n  Response Generation", "abstract": "This paper discusses our approaches for task-oriented conversational\nmodelling using subjective knowledge, with a particular emphasis on response\ngeneration. Our methodology was shaped by an extensive data analysis that\nevaluated key factors such as response length, sentiment, and dialogue acts\npresent in the provided dataset. We used few-shot learning to augment the data\nwith newly generated subjective knowledge items and present three approaches\nfor DSTC11: (1) task-specific model exploration, (2) incorporation of the most\nfrequent question into all generated responses, and (3) a waterfall prompting\ntechnique using a combination of both GPT-3 and ChatGPT.", "published": "2023-08-02 11:04:27", "link": "http://arxiv.org/abs/2308.01080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Machine Translation through Prompt Engineering: An\n  Investigation into ChatGPT's Customizability", "abstract": "This paper explores the influence of integrating the purpose of the\ntranslation and the target audience into prompts on the quality of translations\nproduced by ChatGPT. Drawing on previous translation studies, industry\npractices, and ISO standards, the research underscores the significance of the\npre-production phase in the translation process. The study reveals that the\ninclusion of suitable prompts in large-scale language models like ChatGPT can\nyield flexible translations, a feat yet to be realized by conventional Machine\nTranslation (MT). The research scrutinizes the changes in translation quality\nwhen prompts are used to generate translations that meet specific conditions.\nThe evaluation is conducted from a practicing translator's viewpoint, both\nsubjectively and qualitatively, supplemented by the use of OpenAI's word\nembedding API for cosine similarity calculations. The findings suggest that the\nintegration of the purpose and target audience into prompts can indeed modify\nthe generated translations, generally enhancing the translation quality by\nindustry standards. The study also demonstrates the practical application of\nthe \"good translation\" concept, particularly in the context of marketing\ndocuments and culturally dependent idioms.", "published": "2023-08-02 19:11:04", "link": "http://arxiv.org/abs/2308.01391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text\n  using Transformer Ensembles", "abstract": "This paper describes the solutions submitted by the UPB team to the\nAuTexTification shared task, featured as part of IberLEF-2023. Our team\nparticipated in the first subtask, identifying text documents produced by large\nlanguage models instead of humans. The organizers provided a bilingual dataset\nfor this subtask, comprising English and Spanish texts covering multiple\ndomains, such as legal texts, social media posts, and how-to articles. We\nexperimented mostly with deep learning models based on Transformers, as well as\ntraining techniques such as multi-task learning and virtual adversarial\ntraining to obtain better results. We submitted three runs, two of which\nconsisted of ensemble models. Our best-performing model achieved macro\nF1-scores of 66.63% on the English dataset and 67.10% on the Spanish dataset.", "published": "2023-08-02 20:08:59", "link": "http://arxiv.org/abs/2308.01408v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Feature-aware conditional GAN for category text generation", "abstract": "Category text generation receives considerable attentions since it is\nbeneficial for various natural language processing tasks. Recently, the\ngenerative adversarial network (GAN) has attained promising performance in text\ngeneration, attributed to its adversarial training process. However, there are\nseveral issues in text GANs, including discreteness, training instability, mode\ncollapse, lack of diversity and controllability etc. To address these issues,\nthis paper proposes a novel GAN framework, the feature-aware conditional GAN\n(FA-GAN), for controllable category text generation. In FA-GAN, the generator\nhas a sequence-to-sequence structure for improving sentence diversity, which\nconsists of three encoders including a special feature-aware encoder and a\ncategory-aware encoder, and one relational-memory-core-based decoder with the\nGumbel SoftMax activation function. The discriminator has an additional\ncategory classification head. To generate sentences with specified categories,\nthe multi-class classification loss is supplemented in the adversarial\ntraining. Comprehensive experiments have been conducted, and the results show\nthat FA-GAN consistently outperforms 10 state-of-the-art text generation\napproaches on 6 text classification datasets. The case study demonstrates that\nthe synthetic sentences generated by FA-GAN can match the required categories\nand are aware of the features of conditioned sentences, with good readability,\nfluency, and text authenticity.", "published": "2023-08-02 04:43:54", "link": "http://arxiv.org/abs/2308.00939v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Teaching Smaller Language Models To Generalise To Unseen Compositional\n  Questions", "abstract": "We equip a smaller Language Model to generalise to answering challenging\ncompositional questions that have not been seen in training. To do so we\npropose a combination of multitask supervised pretraining on up to 93 tasks\ndesigned to instill diverse reasoning abilities, and a dense retrieval system\nthat aims to retrieve a set of evidential paragraph fragments. Recent progress\nin question-answering has been achieved either through prompting methods\nagainst very large pretrained Language Models in zero or few-shot fashion, or\nby fine-tuning smaller models, sometimes in conjunction with information\nretrieval. We focus on the less explored question of the extent to which\nzero-shot generalisation can be enabled in smaller models with retrieval\nagainst a corpus within which sufficient information to answer a particular\nquestion may not exist. We establish strong baselines in this setting for\ndiverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and\nARC-DA), and show that performance can be significantly improved by adding\nretrieval-augmented training datasets which are designed to expose our models\nto a variety of heuristic reasoning strategies such as weighing partial\nevidence or ignoring an irrelevant context.", "published": "2023-08-02 05:00:12", "link": "http://arxiv.org/abs/2308.00946v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chat Translation Error Detection for Assisting Cross-lingual\n  Communications", "abstract": "In this paper, we describe the development of a communication support system\nthat detects erroneous translations to facilitate crosslingual communications\ndue to the limitations of current machine chat translation methods. We trained\nan error detector as the baseline of the system and constructed a new\nJapanese-English bilingual chat corpus, BPersona-chat, which comprises\nmultiturn colloquial chats augmented with crowdsourced quality ratings. The\nerror detector can serve as an encouraging foundation for more advanced\nerroneous translation detection systems.", "published": "2023-08-02 09:38:29", "link": "http://arxiv.org/abs/2308.01044v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ADS-Cap: A Framework for Accurate and Diverse Stylized Captioning with\n  Unpaired Stylistic Corpora", "abstract": "Generating visually grounded image captions with specific linguistic styles\nusing unpaired stylistic corpora is a challenging task, especially since we\nexpect stylized captions with a wide variety of stylistic patterns. In this\npaper, we propose a novel framework to generate Accurate and Diverse Stylized\nCaptions (ADS-Cap). Our ADS-Cap first uses a contrastive learning module to\nalign the image and text features, which unifies paired factual and unpaired\nstylistic corpora during the training process. A conditional variational\nauto-encoder is then used to automatically memorize diverse stylistic patterns\nin latent space and enhance diversity through sampling. We also design a simple\nbut effective recheck module to boost style accuracy by filtering\nstyle-specific captions. Experimental results on two widely used stylized image\ncaptioning datasets show that regarding consistency with the image, style\naccuracy and diversity, ADS-Cap achieves outstanding performances compared to\nvarious baselines. We finally conduct extensive analyses to understand the\neffectiveness of our method. Our code is available at\nhttps://github.com/njucckevin/ADS-Cap.", "published": "2023-08-02 13:33:20", "link": "http://arxiv.org/abs/2308.01143v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Arithmetic with Language Models: from Memorization to Computation", "abstract": "A better understanding of the emergent computation and problem-solving\ncapabilities of recent large language models is of paramount importance to\nfurther improve them and broaden their applicability. This work investigates\nhow a language model, trained to predict the next token, can perform arithmetic\ncomputations generalizing beyond training data. Binary addition and\nmultiplication constitute a good testbed for this purpose, since they require a\nvery small vocabulary and exhibit relevant input/output discontinuities making\nsmooth input interpolation ineffective for novel data. We successfully trained\na light language model to learn these tasks and ran a number of experiments to\ninvestigate the extrapolation capabilities and internal information processing.\nOur findings support the hypothesis that the language model works as an\nEncoding-Regression-Decoding machine where the computation takes place in the\nvalue space once the input token representation is mapped to an appropriate\ninternal representation.", "published": "2023-08-02 13:58:37", "link": "http://arxiv.org/abs/2308.01154v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Grounded Image Text Matching with Mismatched Relation Reasoning", "abstract": "This paper introduces Grounded Image Text Matching with Mismatched Relation\n(GITM-MR), a novel visual-linguistic joint task that evaluates the relation\nunderstanding capabilities of transformer-based pre-trained models. GITM-MR\nrequires a model to first determine if an expression describes an image, then\nlocalize referred objects or ground the mismatched parts of the text. We\nprovide a benchmark for evaluating pre-trained models on this task, with a\nfocus on the challenging settings of limited data and out-of-distribution\nsentence lengths. Our evaluation demonstrates that pre-trained models lack data\nefficiency and length generalization ability. To address this, we propose the\nRelation-sensitive Correspondence Reasoning Network (RCRN), which incorporates\nrelation-aware reasoning via bi-directional message propagation guided by\nlanguage structure. RCRN can be interpreted as a modular program and delivers\nstrong performance in both length generalization and data efficiency.", "published": "2023-08-02 15:44:36", "link": "http://arxiv.org/abs/2308.01236v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evaluating Instruction-Tuned Large Language Models on Code Comprehension\n  and Generation", "abstract": "In this work, we evaluate 10 open-source instructed LLMs on four\nrepresentative code comprehension and generation tasks. We have the following\nmain findings. First, for the zero-shot setting, instructed LLMs are very\ncompetitive on code comprehension and generation tasks and sometimes even\nbetter than small SOTA models specifically fine-tuned on each downstream task.\nWe also find that larger instructed LLMs are not always better on code-related\ntasks. Second, for the few-shot setting, we find that adding demonstration\nexamples substantially helps instructed LLMs perform better on most code\ncomprehension and generation tasks; however, the examples would sometimes\ninduce unstable or even worse performance. Furthermore, we find widely-used\nBM25-based shot selection strategy significantly outperforms the basic random\nselection or fixed selection only on generation problems. Third, for the\nfine-tuning setting, we find that fine-tuning could further improve the model\nperformance on downstream code comprehension and generation tasks compared to\nthe zero-shot/one-shot performance. In addition, after being fine-tuned on the\nsame downstream task dataset, instructed LLMs outperform both the small SOTA\nmodels and similar-scaled LLMs without instruction tuning. Based on our\nfindings, we further present practical implications on model and usage\nrecommendation, performance and cost trade-offs, and future direction.", "published": "2023-08-02 15:54:22", "link": "http://arxiv.org/abs/2308.01240v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in\n  Large Language Models", "abstract": "Without proper safeguards, large language models will readily follow\nmalicious instructions and generate toxic content. This risk motivates safety\nefforts such as red-teaming and large-scale feedback learning, which aim to\nmake models both helpful and harmless. However, there is a tension between\nthese two objectives, since harmlessness requires models to refuse to comply\nwith unsafe prompts, and thus not be helpful. Recent anecdotal evidence\nsuggests that some models may have struck a poor balance, so that even clearly\nsafe prompts are refused if they use similar language to unsafe prompts or\nmention sensitive topics. In this paper, we introduce a new test suite called\nXSTest to identify such eXaggerated Safety behaviours in a systematic way.\nXSTest comprises 250 safe prompts across ten prompt types that well-calibrated\nmodels should not refuse to comply with, and 200 unsafe prompts as contrasts\nthat models, for most applications, should refuse. We describe XSTest's\ncreation and composition, and then use the test suite to highlight systematic\nfailure modes in state-of-the-art language models as well as more general\nchallenges in building safer language models.", "published": "2023-08-02 16:30:40", "link": "http://arxiv.org/abs/2308.01263v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the psychology of LLMs' Moral and Legal Reasoning", "abstract": "Large language models (LLMs) exhibit expert-level performance in tasks across\na wide range of different domains. Ethical issues raised by LLMs and the need\nto align future versions makes it important to know how state of the art models\nreason about moral and legal issues. In this paper, we employ the methods of\nexperimental psychology to probe into this question. We replicate eight studies\nfrom the experimental literature with instances of Google's Gemini Pro,\nAnthropic's Claude 2.1, OpenAI's GPT-4, and Meta's Llama 2 Chat 70b. We find\nthat alignment with human responses shifts from one experiment to another, and\nthat models differ amongst themselves as to their overall alignment, with GPT-4\ntaking a clear lead over all other models we tested. Nonetheless, even when\nLLM-generated responses are highly correlated to human responses, there are\nstill systematic differences, with a tendency for models to exaggerate effects\nthat are present among humans, in part by reducing variance. This recommends\ncaution with regards to proposals of replacing human participants with current\nstate-of-the-art LLMs in psychological research and highlights the need for\nfurther research about the distinctive aspects of machine psychology.", "published": "2023-08-02 16:36:58", "link": "http://arxiv.org/abs/2308.01264v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?", "abstract": "Large language models (LLMs) such as ChatGPT are increasingly being used for\nvarious use cases, including text content generation at scale. Although\ndetection methods for such AI-generated text exist already, we investigate\nChatGPT's performance as a detector on such AI-generated text, inspired by\nworks that use ChatGPT as a data labeler or annotator. We evaluate the\nzero-shot performance of ChatGPT in the task of human-written vs. AI-generated\ntext detection, and perform experiments on publicly available datasets. We\nempirically investigate if ChatGPT is symmetrically effective in detecting\nAI-generated or human-written text. Our findings provide insight on how ChatGPT\nand similar LLMs may be leveraged in automated detection pipelines by simply\nfocusing on solving a specific aspect of the problem and deriving the rest from\nthat solution. All code and data is available at\nhttps://github.com/AmritaBh/ChatGPT-as-Detector.", "published": "2023-08-02 17:11:37", "link": "http://arxiv.org/abs/2308.01284v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?", "abstract": "A hallmark of intelligence is the ability to use a familiar domain to make\ninferences about a less familiar domain, known as analogical reasoning. In this\narticle, we delve into the performance of Large Language Models (LLMs) in\ndealing with progressively complex analogies expressed in unstructured text. We\ndiscuss analogies at four distinct levels of complexity: lexical analogies,\nsyntactic analogies, semantic analogies, and pragmatic analogies. As the\nanalogies become more complex, they require increasingly extensive, diverse\nknowledge beyond the textual content, unlikely to be found in the lexical\nco-occurrence statistics that power LLMs. To address this, we discuss the\nnecessity of employing Neuro-symbolic AI techniques that combine statistical\nand symbolic AI, informing the representation of unstructured text to highlight\nand augment relevant content, provide abstraction and guide the mapping\nprocess. Our knowledge-informed approach maintains the efficiency of LLMs while\npreserving the ability to explain analogies for pedagogical applications.", "published": "2023-08-02 21:13:38", "link": "http://arxiv.org/abs/2308.01936v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Industrial Memories: Exploring the Findings of Government Inquiries with\n  Neural Word Embedding and Machine Learning", "abstract": "We present a text mining system to support the exploration of large volumes\nof text detailing the findings of government inquiries. Despite their\nhistorical significance and potential societal impact, key findings of\ninquiries are often hidden within lengthy documents and remain inaccessible to\nthe general public. We transform the findings of the Irish government's inquiry\ninto industrial schools and through the use of word embedding, text\nclassification and visualisation, present an interactive web-based platform\nthat enables the exploration of the text to uncover new historical insights.", "published": "2023-08-02 10:39:11", "link": "http://arxiv.org/abs/2308.02556v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SALTTS: Leveraging Self-Supervised Speech Representations for improved\n  Text-to-Speech Synthesis", "abstract": "While FastSpeech2 aims to integrate aspects of speech such as pitch, energy,\nand duration as conditional inputs, it still leaves scope for richer\nrepresentations. As a part of this work, we leverage representations from\nvarious Self-Supervised Learning (SSL) models to enhance the quality of the\nsynthesized speech. In particular, we pass the FastSpeech2 encoder's\nlength-regulated outputs through a series of encoder layers with the objective\nof reconstructing the SSL representations. In the SALTTS-parallel\nimplementation, the representations from this second encoder are used for an\nauxiliary reconstruction loss with the SSL features. The SALTTS-cascade\nimplementation, however, passes these representations through the decoder in\naddition to having the reconstruction loss. The richness of speech\ncharacteristics from the SSL features reflects in the output speech quality,\nwith the objective and subjective evaluation measures of the proposed approach\noutperforming the baseline FastSpeech2.", "published": "2023-08-02 08:59:52", "link": "http://arxiv.org/abs/2308.01018v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Beyond Generic: Enhancing Image Captioning with Real-World Knowledge\n  using Vision-Language Pre-Training Model", "abstract": "Current captioning approaches tend to generate correct but \"generic\"\ndescriptions that lack real-world knowledge, e.g., named entities and\ncontextual information. Considering that Vision-Language Pre-Training (VLP)\nmodels master massive such knowledge from large-scale web-harvested data, it is\npromising to utilize the generalizability of VLP models to incorporate\nknowledge into image descriptions. However, using VLP models faces challenges:\nzero-shot inference suffers from knowledge hallucination that leads to\nlow-quality descriptions, but the generic bias in downstream task fine-tuning\nhinders the VLP model from expressing knowledge. To address these concerns, we\npropose a simple yet effective method called Knowledge-guided Replay\n(K-Replay), which enables the retention of pre-training knowledge during\nfine-tuning. Our approach consists of two parts: (1) a knowledge prediction\ntask on automatically collected replay exemplars to continuously awaken the VLP\nmodel's memory about knowledge, thus preventing the model from collapsing into\nthe generic pattern; (2) a knowledge distillation constraint to improve the\nfaithfulness of generated descriptions hence alleviating the knowledge\nhallucination. To evaluate knowledge-enhanced descriptions, we construct a\nnovel captioning benchmark KnowCap, containing knowledge of landmarks, famous\nbrands, special foods and movie characters. Experimental results show that our\napproach effectively incorporates knowledge into descriptions, outperforming\nstrong VLP baseline by 20.9 points (78.7->99.6) in CIDEr score and 20.5\npercentage points (34.0%->54.5%) in knowledge recognition accuracy. Our code\nand data is available at https://github.com/njucckevin/KnowCap.", "published": "2023-08-02 13:09:57", "link": "http://arxiv.org/abs/2308.01126v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Global Hierarchical Neural Networks using Hierarchical Softmax", "abstract": "This paper presents a framework in which hierarchical softmax is used to\ncreate a global hierarchical classifier. The approach is applicable for any\nclassification task where there is a natural hierarchy among classes. We show\nempirical results on four text classification datasets. In all datasets the\nhierarchical softmax improved on the regular softmax used in a flat classifier\nin terms of macro-F1 and macro-recall. In three out of four datasets\nhierarchical softmax achieved a higher micro-accuracy and macro-precision.", "published": "2023-08-02 15:12:56", "link": "http://arxiv.org/abs/2308.01210v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Do Multilingual Language Models Think Better in English?", "abstract": "Translate-test is a popular technique to improve the performance of\nmultilingual language models. This approach works by translating the input into\nEnglish using an external machine translation system, and running inference\nover the translated input. However, these improvements can be attributed to the\nuse of a separate translation system, which is typically trained on large\namounts of parallel data not seen by the language model. In this work, we\nintroduce a new approach called self-translate, which overcomes the need of an\nexternal translation system by leveraging the few-shot translation capabilities\nof multilingual language models. Experiments over 5 tasks show that\nself-translate consistently outperforms direct inference, demonstrating that\nlanguage models are unable to leverage their full multilingual potential when\nprompted in non-English languages. Our code is available at\nhttps://github.com/juletx/self-translate.", "published": "2023-08-02 15:29:22", "link": "http://arxiv.org/abs/2308.01223v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PerceptionCLIP: Visual Classification by Inferring and Conditioning on\n  Contexts", "abstract": "Vision-language models like CLIP are widely used in zero-shot image\nclassification due to their ability to understand various visual concepts and\nnatural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better\nperformance is still an open question. This paper draws inspiration from the\nhuman visual perception process: when classifying an object, humans first infer\ncontextual attributes (e.g., background and orientation) which help separate\nthe foreground object from the background, and then classify the object based\non this information. Inspired by it, we observe that providing CLIP with\ncontextual attributes improves zero-shot image classification and mitigates\nreliance on spurious features. We also observe that CLIP itself can reasonably\ninfer the attributes from an image. With these observations, we propose a\ntraining-free, two-step zero-shot classification method PerceptionCLIP. Given\nan image, it first infers contextual attributes (e.g., background) and then\nperforms object classification conditioning on them. Our experiments show that\nPerceptionCLIP achieves better generalization, group robustness, and\ninteroperability. Our code is available at\nhttps://github.com/umd-huang-lab/perceptionCLIP", "published": "2023-08-02 17:57:25", "link": "http://arxiv.org/abs/2308.01313v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like\n  Models at All Scales", "abstract": "ChatGPT-like models have revolutionized various applications in artificial\nintelligence, from summarization and coding to translation, matching or even\nsurpassing human performance. However, the current landscape lacks an\naccessible, efficient, and cost-effective end-to-end RLHF (Reinforcement\nLearning with Human Feedback) training pipeline for these powerful models,\nparticularly when training at the scale of billions of parameters. This paper\nintroduces DeepSpeed-Chat, a novel system that democratizes RLHF training,\nmaking it accessible to the AI community. DeepSpeed-Chat offers three key\ncapabilities: an easy-to-use training and inference experience for ChatGPT-like\nmodels, a DeepSpeed-RLHF pipeline that replicates the training pipeline from\nInstructGPT, and a robust DeepSpeed-RLHF system that combines various\noptimizations for training and inference in a unified way. The system delivers\nunparalleled efficiency and scalability, enabling training of models with\nhundreds of billions of parameters in record time and at a fraction of the\ncost. With this development, DeepSpeed-Chat paves the way for broader access to\nadvanced RLHF training, even for data scientists with limited resources,\nthereby fostering innovation and further development in the field of AI.", "published": "2023-08-02 18:49:57", "link": "http://arxiv.org/abs/2308.01320v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Careful Whisper -- leveraging advances in automatic speech recognition\n  for robust and interpretable aphasia subtype classification", "abstract": "This paper presents a fully automated approach for identifying speech\nanomalies from voice recordings to aid in the assessment of speech impairments.\nBy combining Connectionist Temporal Classification (CTC) and\nencoder-decoder-based automatic speech recognition models, we generate rich\nacoustic and clean transcripts. We then apply several natural language\nprocessing methods to extract features from these transcripts to produce\nprototypes of healthy speech. Basic distance measures from these prototypes\nserve as input features for standard machine learning classifiers, yielding\nhuman-level accuracy for the distinction between recordings of people with\naphasia and a healthy control group. Furthermore, the most frequently occurring\naphasia types can be distinguished with 90% accuracy. The pipeline is directly\napplicable to other diseases and languages, showing promise for robustly\nextracting diagnostic speech biomarkers.", "published": "2023-08-02 15:53:59", "link": "http://arxiv.org/abs/2308.01327v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Empirical Translation Process Research: Past and Possible Future\n  Perspectives", "abstract": "Over the past four decades, efforts have been made to develop and evaluate\nmodels for Empirical Translation Process Research (TPR), yet a comprehensive\nframework remains elusive. This article traces the evolution of empirical TPR\nwithin the CRITT TPR-DB tradition and proposes the Free Energy Principle (FEP)\nand Active Inference (AIF) as a framework for modeling deeply embedded\ntranslation processes. It introduces novel approaches for quantifying\nfundamental concepts of Relevance Theory (relevance, s-mode, i-mode), and\nestablishes their relation to the Monitor Model, framing relevance maximization\nas a special case of minimizing free energy. FEP/AIF provides a mathematically\nrigorous foundation that enables modeling of deep temporal architectures in\nwhich embedded translation processes unfold on different timelines. This\nframework opens up exciting prospects for future research in predictive TPR,\nlikely to enrich our comprehension of human translation processes, and making\nvaluable contributions to the wider realm of translation studies and the design\nof cognitive architectures.", "published": "2023-08-02 18:22:49", "link": "http://arxiv.org/abs/2308.01368v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Reverse Stable Diffusion: What prompt was used to generate this image?", "abstract": "Text-to-image diffusion models have recently attracted the interest of many\nresearchers, and inverting the diffusion process can play an important role in\nbetter understanding the generative process and how to engineer prompts in\norder to obtain the desired images. To this end, we study the task of\npredicting the prompt embedding given an image generated by a generative\ndiffusion model. We consider a series of white-box and black-box models (with\nand without access to the weights of the diffusion network) to deal with the\nproposed task. We propose a novel learning framework comprising a joint prompt\nregression and multi-label vocabulary classification objective that generates\nimproved prompts. To further improve our method, we employ a curriculum\nlearning procedure that promotes the learning of image-prompt pairs with lower\nlabeling noise (i.e. that are better aligned). We conduct experiments on the\nDiffusionDB data set, predicting text prompts from images generated by Stable\nDiffusion. In addition, we make an interesting discovery: training a diffusion\nmodel on the prompt generation task can make the model generate images that are\nmuch better aligned with the input prompts, when the model is directly reused\nfor text-to-image generation. Our code is publicly available for download at\nhttps://github.com/CroitoruAlin/Reverse-Stable-Diffusion.", "published": "2023-08-02 23:39:29", "link": "http://arxiv.org/abs/2308.01472v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MultiEM: Efficient and Effective Unsupervised Multi-Table Entity\n  Matching", "abstract": "Entity Matching (EM), which aims to identify all entity pairs referring to\nthe same real-world entity from relational tables, is one of the most important\ntasks in real-world data management systems. Due to the labeling process of EM\nbeing extremely labor-intensive, unsupervised EM is more applicable than\nsupervised EM in practical scenarios. Traditional unsupervised EM assumes that\nall entities come from two tables; however, it is more common to match entities\nfrom multiple tables in practical applications, that is, multi-table entity\nmatching (multi-table EM). Unfortunately, effective and efficient unsupervised\nmulti-table EM remains under-explored. To fill this gap, this paper formally\nstudies the problem of unsupervised multi-table entity matching and proposes an\neffective and efficient solution, termed as MultiEM. MultiEM is a parallelable\npipeline of enhanced entity representation, table-wise hierarchical merging,\nand density-based pruning. Extensive experimental results on six real-world\nbenchmark datasets demonstrate the superiority of MultiEM in terms of\neffectiveness and efficiency.", "published": "2023-08-02 11:39:19", "link": "http://arxiv.org/abs/2308.01927v1", "categories": ["cs.DB", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for\n  Predicting Drug-Review Satisfaction", "abstract": "The objective of this study is to develop natural language processing (NLP)\nmodels that can analyze patients' drug reviews and accurately classify their\nsatisfaction levels as positive, neutral, or negative. Such models would reduce\nthe workload of healthcare professionals and provide greater insight into\npatients' quality of life, which is a critical indicator of treatment\neffectiveness. To achieve this, we implemented and evaluated several\nclassification models, including a BERT base model, Bio+Clinical BERT, and a\nsimpler CNN. Results indicate that the medical domain-specific Bio+Clinical\nBERT model significantly outperformed the general domain base BERT model,\nachieving macro f1 and recall score improvement of 11%, as shown in Table 2.\nFuture research could explore how to capitalize on the specific strengths of\neach model. Bio+Clinical BERT excels in overall performance, particularly with\nmedical jargon, while the simpler CNN demonstrates the ability to identify\ncrucial words and accurately classify sentiment in texts with conflicting\nsentiments.", "published": "2023-08-02 20:01:38", "link": "http://arxiv.org/abs/2308.03782v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Music De-limiter Networks via Sample-wise Gain Inversion", "abstract": "The loudness war, an ongoing phenomenon in the music industry characterized\nby the increasing final loudness of music while reducing its dynamic range, has\nbeen a controversial topic for decades. Music mastering engineers have used\nlimiters to heavily compress and make music louder, which can induce ear\nfatigue and hearing loss in listeners. In this paper, we introduce music\nde-limiter networks that estimate uncompressed music from heavily compressed\nsignals. Inspired by the principle of a limiter, which performs sample-wise\ngain reduction of a given signal, we propose the framework of sample-wise gain\ninversion (SGI). We also present the musdb-XL-train dataset, consisting of 300k\nsegments created by applying a commercial limiter plug-in for training\nreal-world friendly de-limiter networks. Our proposed de-limiter network\nachieves excellent performance with a scale-invariant source-to-distortion\nratio (SI-SDR) of 24.0 dB in reconstructing musdb-HQ from musdb-XL data, a\nlimiter-applied version of musdb-HQ. The training data, codes, and model\nweights are available in our repository\n(https://github.com/jeonchangbin49/De-limiter).", "published": "2023-08-02 14:51:22", "link": "http://arxiv.org/abs/2308.01187v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Inaudible Adversarial Perturbation: Manipulating the Recognition of User\n  Speech in Real Time", "abstract": "Automatic speech recognition (ASR) systems have been shown to be vulnerable\nto adversarial examples (AEs). Recent success all assumes that users will not\nnotice or disrupt the attack process despite the existence of music/noise-like\nsounds and spontaneous responses from voice assistants. Nonetheless, in\npractical user-present scenarios, user awareness may nullify existing attack\nattempts that launch unexpected sounds or ASR usage. In this paper, we seek to\nbridge the gap in existing research and extend the attack to user-present\nscenarios. We propose VRIFLE, an inaudible adversarial perturbation (IAP)\nattack via ultrasound delivery that can manipulate ASRs as a user speaks. The\ninherent differences between audible sounds and ultrasounds make IAP delivery\nface unprecedented challenges such as distortion, noise, and instability. In\nthis regard, we design a novel ultrasonic transformation model to enhance the\ncrafted perturbation to be physically effective and even survive long-distance\ndelivery. We further enable VRIFLE's robustness by adopting a series of\naugmentation on user and real-world variations during the generation process.\nIn this way, VRIFLE features an effective real-time manipulation of the ASR\noutput from different distances and under any speech of users, with an\nalter-and-mute strategy that suppresses the impact of user disruption. Our\nextensive experiments in both digital and physical worlds verify VRIFLE's\neffectiveness under various configurations, robustness against six kinds of\ndefenses, and universality in a targeted manner. We also show that VRIFLE can\nbe delivered with a portable attack device and even everyday-life loudspeakers.", "published": "2023-08-02 09:32:17", "link": "http://arxiv.org/abs/2308.01040v3", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion", "abstract": "Deep generative models can generate high-fidelity audio conditioned on\nvarious types of representations (e.g., mel-spectrograms, Mel-frequency\nCepstral Coefficients (MFCC)). Recently, such models have been used to\nsynthesize audio waveforms conditioned on highly compressed representations.\nAlthough such methods produce impressive results, they are prone to generate\naudible artifacts when the conditioning is flawed or imperfect. An alternative\nmodeling approach is to use diffusion models. However, these have mainly been\nused as speech vocoders (i.e., conditioned on mel-spectrograms) or generating\nrelatively low sampling rate signals. In this work, we propose a high-fidelity\nmulti-band diffusion-based framework that generates any type of audio modality\n(e.g., speech, music, environmental sounds) from low-bitrate discrete\nrepresentations. At equal bit rate, the proposed approach outperforms\nstate-of-the-art generative techniques in terms of perceptual quality. Training\nand, evaluation code, along with audio samples, are available on the\nfacebookresearch/audiocraft Github page.", "published": "2023-08-02 22:14:29", "link": "http://arxiv.org/abs/2308.02560v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
