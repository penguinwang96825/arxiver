{"title": "Integrative Semantic Dependency Parsing via Efficient Large-scale\n  Feature Selection", "abstract": "Semantic parsing, i.e., the automatic derivation of meaning representation\nsuch as an instantiated predicate-argument structure for a sentence, plays a\ncritical role in deep processing of natural language. Unlike all other top\nsystems of semantic dependency parsing that have to rely on a pipeline\nframework to chain up a series of submodels each specialized for a specific\nsubtask, the one presented in this article integrates everything into one\nmodel, in hopes of achieving desirable integrity and practicality for real\napplications while maintaining a competitive performance. This integrative\napproach tackles semantic parsing as a word pair classification problem using a\nmaximum entropy classifier. We leverage adaptive pruning of argument candidates\nand large-scale feature selection engineering to allow the largest feature\nspace ever in use so far in this field, it achieves a state-of-the-art\nperformance on the evaluation data set for CoNLL-2008 shared task, on top of\nall but one top pipeline system, confirming its feasibility and effectiveness.", "published": "2014-01-23 16:45:39", "link": "http://arxiv.org/abs/1401.6050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Bengali Multiword Expressions using Semantic Clustering", "abstract": "One of the key issues in both natural language understanding and generation\nis the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge\nproblem to the precise language processing due to their idiosyncratic nature\nand diversity in lexical, syntactical and semantic properties. The semantics of\na MWE cannot be expressed after combining the semantics of its constituents.\nTherefore, the formalism of semantic clustering is often viewed as an\ninstrument for extracting MWEs especially for resource constraint languages\nlike Bengali. The present semantic clustering approach contributes to locate\nclusters of the synonymous noun tokens present in the document. These clusters\nin turn help measure the similarity between the constituent words of a\npotentially candidate phrase using a vector space model and judge the\nsuitability of this phrase to be a MWE. In this experiment, we apply the\nsemantic clustering approach for noun-noun bigram MWEs, though it can be\nextended to any types of MWEs. In parallel, the well known statistical models,\nnamely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),\nSignificance function are also employed to extract MWEs from the Bengali\ncorpus. The comparative evaluation shows that the semantic clustering approach\noutperforms all other competing statistical models. As a by-product of this\nexperiment, we have started developing a standard lexicon in Bengali that\nserves as a productive Bengali linguistic thesaurus.", "published": "2014-01-23 19:03:18", "link": "http://arxiv.org/abs/1401.6122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Aggregation by Joint Modeling of Aspects and Values", "abstract": "We present a model for aggregation of product review snippets by joint aspect\nidentification and sentiment analysis. Our model simultaneously identifies an\nunderlying set of ratable aspects presented in the reviews of a product (e.g.,\nsushi and miso for a Japanese restaurant) and determines the corresponding\nsentiment of each aspect. This approach directly enables discovery of\nhighly-rated or inconsistent aspects of a product. Our generative model admits\nan efficient variational mean-field inference algorithm. It is also easily\nextensible, and we describe several modifications and their effects on model\nstructure and inference. We test our model on two tasks, joint aspect\nidentification and sentiment analysis on a set of Yelp reviews and aspect\nidentification alone on a set of medical summaries. We evaluate the performance\nof the model on aspect identification, sentiment analysis, and per-word\nlabeling accuracy. We demonstrate that our model outperforms applicable\nbaselines by a considerable margin, yielding up to 32% relative error reduction\non aspect identification and up to 20% relative error reduction on sentiment\nanalysis.", "published": "2014-01-23 02:48:07", "link": "http://arxiv.org/abs/1401.6422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Statistical Machine Translation for a Resource-Poor Language\n  Using Related Resource-Rich Languages", "abstract": "We propose a novel language-independent approach for improving machine\ntranslation for resource-poor languages by exploiting their similarity to\nresource-rich ones. More precisely, we improve the translation from a\nresource-poor source language X_1 into a resource-rich language Y given a\nbi-text containing a limited number of parallel sentences for X_1-Y and a\nlarger bi-text for X_2-Y for some resource-rich language X_2 that is closely\nrelated to X_1. This is achieved by taking advantage of the opportunities that\nvocabulary overlap and similarities between the languages X_1 and X_2 in\nspelling, word order, and syntax offer: (1) we improve the word alignments for\nthe resource-poor language, (2) we further augment it with additional\ntranslation options, and (3) we take care of potential spelling differences\nthrough appropriate transliteration. The evaluation for Indonesian- >English\nusing Malay and for Spanish -> English using Portuguese and pretending Spanish\nis resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,\nrespectively, which is an improvement over the best rivaling approaches, while\nusing much less additional data. Overall, our method cuts the amount of\nnecessary \"real training data by a factor of 2--5.", "published": "2014-01-23 02:42:12", "link": "http://arxiv.org/abs/1401.6876v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference\n  in Natural Language Processing", "abstract": "Dual decomposition, and more generally Lagrangian relaxation, is a classical\nmethod for combinatorial optimization; it has recently been applied to several\ninference problems in natural language processing (NLP). This tutorial gives an\noverview of the technique. We describe example algorithms, describe formal\nguarantees for the method, and describe practical issues in implementing the\nalgorithms. While our examples are predominantly drawn from the NLP literature,\nthe material should be of general relevance to inference problems in machine\nlearning. A central theme of this tutorial is that Lagrangian relaxation is\nnaturally applied in conjunction with a broad class of combinatorial\nalgorithms, allowing inference in models that go significantly beyond previous\nwork on Lagrangian relaxation for inference in graphical models.", "published": "2014-01-23 02:50:15", "link": "http://arxiv.org/abs/1405.5208v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Unsupervised Learning of Temporal Relations between Events", "abstract": "Automatic extraction of temporal relations between event pairs is an\nimportant task for several natural language processing applications such as\nQuestion Answering, Information Extraction, and Summarization. Since most\nexisting methods are supervised and require large corpora, which for many\nlanguages do not exist, we have concentrated our efforts to reduce the need for\nannotated data as much as possible. This paper presents two different\nalgorithms towards this goal. The first algorithm is a weakly supervised\nmachine learning approach for classification of temporal relations between\nevents. In the first stage, the algorithm learns a general classifier from an\nannotated corpus. Then, inspired by the hypothesis of \"one type of temporal\nrelation per discourse, it extracts useful information from a cluster of\ntopically related documents. We show that by combining the global information\nof such a cluster with local decisions of a general classifier, a bootstrapping\ncross-document classifier can be built to extract temporal relations between\nevents. Our experiments show that without any additional annotated data, the\naccuracy of the proposed algorithm is higher than that of several previous\nsuccessful systems. The second proposed method for temporal relation extraction\nis based on the expectation maximization (EM) algorithm. Within EM, we used\ndifferent techniques such as a greedy best-first search and integer linear\nprogramming for temporal inconsistency removal. We think that the experimental\nresults of our EM based algorithm, as a first step toward a fully unsupervised\ntemporal relation extraction method, is encouraging.", "published": "2014-01-23 02:50:50", "link": "http://arxiv.org/abs/1401.6427v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reasoning about Meaning in Natural Language with Compact Closed\n  Categories and Frobenius Algebras", "abstract": "Compact closed categories have found applications in modeling quantum\ninformation protocols by Abramsky-Coecke. They also provide semantics for\nLambek's pregroup algebras, applied to formalizing the grammatical structure of\nnatural language, and are implicit in a distributional model of word meaning\nbased on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh\nused the product category of pregroups with vector spaces and provided a\ndistributional model of meaning for sentences. We recast this theory in terms\nof strongly monoidal functors and advance it via Frobenius algebras over vector\nspaces. The former are used to formalize topological quantum field theories by\nAtiyah and Baez-Dolan, and the latter are used to model classical data in\nquantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us\nto work in a single space in which meanings of words, phrases, and sentences of\nany structure live. Hence we can compare meanings of different language\nconstructs and enhance the applicability of the theory. We report on\nexperimental results on a number of language tasks and verify the theoretical\npredictions.", "published": "2014-01-23 14:20:58", "link": "http://arxiv.org/abs/1401.5980v1", "categories": ["cs.CL", "cs.AI", "math.CT"], "primary_category": "cs.CL"}
