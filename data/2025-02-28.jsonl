{"title": "A Minor-Testing Approach for Coordinated Motion Planning with Sliding Robots", "abstract": "We study a variant of the Coordinated Motion Planning problem on undirected\ngraphs, referred to herein as the \\textsc{Coordinated Sliding-Motion Planning}\n(CSMP) problem. In this variant, we are given an undirected graph $G$, $k$\nrobots $R_1,\\dots,R_k$ positioned on distinct vertices of $G$, $p\\leq k$\ndistinct destination vertices for robots $R_1,\\dots,R_p$, and $\\ell \\in\n\\mathbb{N}$. The problem is to decide if there is a serial schedule of at most\n$\\ell$ moves (i.e., of makespan $\\ell$) such that at the end of the schedule\neach robot with a destination reaches it, where a robot's move is a free path\n(unoccupied by any robots) from its current position to an unoccupied vertex.\nThe problem is known to be NP-hard even on full grids. It has been studied in\nseveral contexts, including coin movement and reconfiguration problems, with\nrespect to feasibility, complexity, and approximation. Geometric variants of\nthe problem, in which congruent geometric-shape robots (e.g., unit\ndisk/squares) slide or translate in the Euclidean plane, have also been studied\nextensively. We investigate the parameterized complexity of CSMP with respect\nto two parameters: the number $k$ of robots and the makespan $\\ell$. As our\nfirst result, we present a fixed-parameter algorithm for CSMP parameterized by\n$k$. For our second result, we present a fixed-parameter algorithm\nparameterized by $\\ell$ for the special case of CSMP in which only a single\nrobot has a destination and the graph is planar, which we prove to be\nNP-complete. A crucial new ingredient for both of our results is that the\nsolution admits a succinct representation as a small labeled topological minor\nof the input graph.", "published": "2025-02-28 15:56:42", "link": "http://arxiv.org/abs/2502.21175v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Polynomial-Size Enumeration Kernelizations for Long Path Enumeration", "abstract": "Enumeration kernelization for parameterized enumeration problems was defined\nby Creignou et al. [Theory Comput. Syst. 2017] and was later refined by\nGolovach et al. [J. Comput. Syst. Sci. 2022, STACS 2021] to polynomial-delay\nenumeration kernelization. We consider ENUM LONG-PATH, the enumeration variant\nof the Long-Path problem, from the perspective of enumeration kernelization.\nFormally, given an undirected graph G and an integer k, the objective of ENUM\nLONG-PATH is to enumerate all paths of G having exactly k vertices. We consider\nthe structural parameters vertex cover number, dissociation number, and\ndistance to clique and provide polynomial-delay enumeration kernels of\npolynomial size for each of these parameters.", "published": "2025-02-28 15:48:14", "link": "http://arxiv.org/abs/2502.21164v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "4-tangrams are 4-avoidable", "abstract": "A tangram is a word in which every letter occurs an even number of times.\nThus it can be cut into parts that can be arranged into two identical words.\nThe \\emph{cut number} of a tangram is the minimum number of required cuts in\nthis process. Tangrams with cut number one corresponds to squares. For $k\\ge1$,\nlet $t(k)$ denote the minimum size of an alphabet over which an infinite word\navoids tangrams with cut number at most~$k$. The existence of infinite ternary\nsquare-free words shows that $t(1)=t(2)=3$. We show that $t(3)=t(4)=4$,\nanswering a question from D\\k{e}bski, Grytczuk, Pawlik, Przyby\\l{}o, and\n\\'Sleszy\\'nska-Nowak.", "published": "2025-02-28 06:48:13", "link": "http://arxiv.org/abs/2502.20774v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Using quantile time series and historical simulation to forecast financial risk multiple steps ahead", "abstract": "A method for quantile-based, semi-parametric historical simulation estimation\nof multiple step ahead Value-at-Risk (VaR) and Expected Shortfall (ES) models\nis developed. It uses the quantile loss function, analogous to how the\nquasi-likelihood is employed by standard historical simulation methods. The\nreturns data are scaled by the estimated quantile series, then resampling is\nemployed to estimate the forecast distribution one and multiple steps ahead,\nallowing tail risk forecasting. The proposed method is applicable to any data\nor model where the relationship between VaR and ES does not change over time\nand can be extended to allow a measurement equation incorporating realized\nmeasures, thus including Realized GARCH and Realized CAViaR type models. Its\nfinite sample properties, and its comparison with existing historical\nsimulation methods, are evaluated via a simulation study. A forecasting study\nassesses the relative accuracy of the 1% and 2.5% VaR and ES one-day-ahead and\nten-day-ahead forecasting results for the proposed class of models compared to\nseveral competitors.", "published": "2025-02-28 11:48:35", "link": "http://arxiv.org/abs/2502.20978v2", "categories": ["q-fin.ST", "q-fin.CP", "q-fin.RM"], "primary_category": "q-fin.ST"}
{"title": "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced Finite Difference Estimators", "abstract": "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.", "published": "2025-02-28 08:05:54", "link": "http://arxiv.org/abs/2502.20819v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "q-fin.CP", "stat.ML", "90-05", "I.6.1; I.6.6"], "primary_category": "math.OC"}
{"title": "Strong Solutions and Quantization-Based Numerical Schemes for a Class of Non-Markovian Volatility Models", "abstract": "We investigate a class of non-Markovian processes that hold particular\nrelevance in the realm of mathematical finance. This family encompasses\npath-dependent volatility models, including those pioneered by [Platen and\nRendek, 2018] and, more recently, by [Guyon and Lekeufack, 2023], as well as an\nextension of the framework proposed by [Blanc et al., 2017]. Our study unfolds\nin two principal phases. In the first phase, we introduce a functional\nquantization scheme based on an extended version of the Lamperti transformation\nthat we propose to handle the presence of a memory term incorporated into the\ndiffusion coefficient. For scenarios involving a Brownian integral in the\ndiffusion term, we propose alternative numerical schemes that leverage the\npower of marginal recursive quantization. In the second phase, we study the\nproblem of existence and uniqueness of a strong solution for the SDEs related\nto the examples that motivate our study, in order to provide a theoretical\nbasis to correctly apply the proposed numerical schemes.", "published": "2025-02-28 23:21:44", "link": "http://arxiv.org/abs/2503.00243v1", "categories": ["q-fin.MF", "60F10, 91G99, 91B25"], "primary_category": "q-fin.MF"}
{"title": "Short-Rate Derivatives in a Higher-for-Longer Environment", "abstract": "We introduce a class of short-rate models that exhibit a ``higher for\nlonger'' phenomenon. Specifically, the short-rate is modeled as a general\ntime-homogeneous one-factor Markov diffusion on a finite interval. The lower\nendpoint is assumed to be regular, exit or natural according to boundary\nclassification while the upper endpoint is assumed to be regular with absorbing\nbehavior. In this setting, we give an explicit expression for price of a\nzero-coupon bond (as well as more general interest rate derivatives) in terms\nof the transition density of the short-rate under a new probability measure,\nand the solution of a non-linear ordinary differential equation (ODE). We then\nnarrow our focus to a class of models for which the transition density and ODE\ncan be solved explicitly. For models within this class, we provide conditions\nunder which the lower endpoint is regular, exit and natural. Finally, we study\ntwo specific models -- one in which the lower endpoint is exit and another in\nwhich the lower endpoint is natural. In these two models, we give an explicit\nsolution of transition density of the short-rate as a (generalized)\neigenfunction expansion. We provide plots of the transition density,\n(generalized) eigenfunctions, bond prices and the associated yield curve.", "published": "2025-02-28 17:27:51", "link": "http://arxiv.org/abs/2502.21252v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Chronologically Consistent Large Language Models", "abstract": "Large language models are increasingly used in social sciences, but their\ntraining data can introduce lookahead bias and training leakage. A good\nchronologically consistent language model requires efficient use of training\ndata to maintain accuracy despite time-restricted data. Here, we overcome this\nchallenge by training a suite of chronologically consistent large language\nmodels, ChronoBERT and ChronoGPT, which incorporate only the text data that\nwould have been available at each point in time. Despite this strict temporal\nconstraint, our models achieve strong performance on natural language\nprocessing benchmarks, outperforming or matching widely used models (e.g.,\nBERT), and remain competitive with larger open-weight models. Lookahead bias is\nmodel and application-specific because even if a chronologically consistent\nlanguage model has poorer language comprehension, a regression or prediction\nmodel applied on top of the language model can compensate. In an asset pricing\napplication predicting next-day stock returns from financial news, we find that\nChronoBERT's real-time outputs achieve a Sharpe ratio comparable to\nstate-of-the-art models, indicating that lookahead bias is modest. Our results\ndemonstrate a scalable, practical framework to mitigate training leakage,\nensuring more credible backtests and predictions across finance and other\nsocial science domains.", "published": "2025-02-28 16:25:50", "link": "http://arxiv.org/abs/2502.21206v2", "categories": ["q-fin.GN", "q-fin.TR"], "primary_category": "q-fin.GN"}
{"title": "Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning", "abstract": "Large language models (LLMs) can exhibit advanced reasoning yet still\ngenerate incorrect answers. We hypothesize that such errors frequently stem\nfrom spurious beliefs, propositions the model internally considers true but are\nincorrect. To address this, we propose a method to rectify the belief space by\nsuppressing these spurious beliefs while simultaneously enhancing true ones,\nthereby enabling more reliable inferences. Our approach first identifies the\nbeliefs that lead to incorrect or correct answers by prompting the model to\ngenerate textual explanations, using our Forward-Backward Beam Search (FBBS).\nWe then apply unlearning to suppress the identified spurious beliefs and\nenhance the true ones, effectively rectifying the model's belief space.\nEmpirical results on multiple QA datasets and LLMs show that our method\ncorrects previously misanswered questions without harming overall model\nperformance. Furthermore, our approach yields improved generalization on unseen\ndata, suggesting that rectifying a model's belief space is a promising\ndirection for mitigating errors and enhancing overall reliability.", "published": "2025-02-28 00:57:45", "link": "http://arxiv.org/abs/2502.20620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prediction of Item Difficulty for Reading Comprehension Items by\n  Creation of Annotated Item Repository", "abstract": "Prediction of item difficulty based on its text content is of substantial\ninterest. In this paper, we focus on the related problem of recovering\nIRT-based difficulty when the data originally reported item p-value (percent\ncorrect responses). We model this item difficulty using a repository of reading\npassages and student data from US standardized tests from New York and Texas\nfor grades 3-8 spanning the years 2017-23. This repository is annotated with\nmeta-data on (1) linguistic features of the reading items, (2) test features of\nthe passage, and (3) context features. A penalized regression prediction model\nwith all these features can predict item difficulty with RMSE 0.52 compared to\nbaseline RMSE of 0.92, and with a correlation of 0.77 between true and\npredicted difficulty. We supplement these features with embeddings from LLMs\n(ModernBERT, BERT, and LlAMA), which marginally improve item difficulty\nprediction. When models use only item linguistic features or LLM embeddings,\nprediction performance is similar, which suggests that only one of these\nfeature categories may be required. This item difficulty prediction model can\nbe used to filter and categorize reading items and will be made publicly\navailable for use by other stakeholders.", "published": "2025-02-28 02:42:13", "link": "http://arxiv.org/abs/2502.20663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations in Large Vision-Language Models by Adaptively\n  Constraining Information Flow", "abstract": "Large vision-language models show tremendous potential in understanding\nvisual information through human languages. However, they are prone to suffer\nfrom object hallucination, i.e., the generated image descriptions contain\nobjects that do not exist in the image. In this paper, we reveal that object\nhallucination can be attributed to overconfidence in irrelevant visual features\nwhen soft visual tokens map to the LLM's word embedding space. Specifically, by\nfiguring out the semantic similarity between visual tokens and LLM's word\nembedding, we observe that the smoothness of similarity distribution strongly\ncorrelates with the emergence of object hallucinations. To mitigate\nhallucinations, we propose using the Variational Information Bottleneck (VIB)\nto alleviate overconfidence by introducing stochastic noise, facilitating the\nconstraining of irrelevant information. Furthermore, we propose an\nentropy-based noise-controlling strategy to enable the injected noise to be\nadaptively constrained regarding the smoothness of the similarity distribution.\nWe adapt the proposed AdaVIB across distinct model architectures. Experimental\nresults demonstrate that the proposed AdaVIB mitigates object hallucinations by\neffectively alleviating the overconfidence in irrelevant visual features, with\nconsistent improvements on two object hallucination benchmarks.", "published": "2025-02-28 05:56:23", "link": "http://arxiv.org/abs/2502.20750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue\n  Agents", "abstract": "Large Language Models (LLMs) have made remarkable advances in role-playing\ndialogue agents, demonstrating their utility in character simulations. However,\nit remains challenging for these agents to balance character portrayal utility\nwith content safety because this essential character simulation often comes\nwith the risk of generating unsafe content. To address this issue, we first\nconduct a systematic exploration of the safety-utility trade-off across\nmultiple LLMs. Our analysis reveals that risk scenarios created by villain\ncharacters and user queries (referred to as risk coupling) contribute to this\ntrade-off. Building on this, we propose a novel Adaptive Dynamic\nMulti-Preference (ADMP) method, which dynamically adjusts safety-utility\npreferences based on the degree of risk coupling and guides the model to\ngenerate responses biased toward utility or safety. We further introduce\nCoupling Margin Sampling (CMS) into coupling detection to enhance the model's\nability to handle high-risk scenarios. Experimental results demonstrate that\nour approach improves safety metrics while maintaining utility.", "published": "2025-02-28 06:18:50", "link": "http://arxiv.org/abs/2502.20757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GraphCheck: Multi-Path Fact-Checking with Entity-Relationship Graphs", "abstract": "Automated fact-checking aims to assess the truthfulness of text based on\nrelevant evidence, yet verifying complex claims requiring multi-hop reasoning\nremains a significant challenge. We propose GraphCheck, a novel framework that\nconverts claims into entity-relationship graphs for comprehensive verification.\nBy identifying relation between explicit entities and latent entities across\nmultiple paths, GraphCheck enhances the adaptability and robustness of\nverification. Furthermore, we introduce DP-GraphCheck, a two-stage variant that\nimproves performance by incorporating direct prompting as an initial filtering\nstep. Experiments on the HOVER and EX-FEVER datasets show that our approach\noutperforms existing methods, particularly in multi-hop reasoning tasks.\nFurthermore, our two-stage framework generalizes well to other fact-checking\npipelines, demonstrating its versatility.", "published": "2025-02-28 07:06:19", "link": "http://arxiv.org/abs/2502.20785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Thought Matters: Improving Long-Context Language Models with\n  Reasoning Path Supervision", "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the\nchallenge of handling long-context tasks, where models need to reason over\nextensive input contexts to aggregate target information. While\nChain-of-Thought (CoT) prompting has shown promise for multi-step reasoning,\nits effectiveness for long-context scenarios remains underexplored. Through\nsystematic investigation across diverse tasks, we demonstrate that CoT's\nbenefits generalize across most long-context scenarios and amplify with\nincreasing context length. Motivated by this critical observation, we propose\nLongRePS, a process-supervised framework that teaches models to generate\nhigh-quality reasoning paths for enhanced long-context performance. Our\nframework incorporates a self-sampling mechanism to bootstrap reasoning paths\nand a novel quality assessment protocol specifically designed for long-context\nscenarios. Experimental results on various long-context benchmarks demonstrate\nthe effectiveness of our approach, achieving significant improvements over\noutcome supervision baselines on both in-domain tasks (+13.6/+3.8 points for\nLLaMA/Qwen on MuSiQue) and cross-domain generalization (+9.3/+8.1 points on\naverage across diverse QA tasks). Our code, data and trained models are made\npublic to facilitate future research.", "published": "2025-02-28 07:15:12", "link": "http://arxiv.org/abs/2502.20790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plan2Align: Predictive Planning Based Test-Time Preference Alignment in\n  Paragraph-Level Machine Translation", "abstract": "Machine Translation (MT) has been predominantly designed for sentence-level\ntranslation using transformer-based architectures. While next-token prediction\nbased Large Language Models (LLMs) demonstrate strong capabilities in long-text\ntranslation, non-extensive language models often suffer from omissions and\nsemantic inconsistencies when processing paragraphs. Existing preference\nalignment methods improve sentence-level translation but fail to ensure\ncoherence over extended contexts due to the myopic nature of next-token\ngeneration. We introduce Plan2Align, a test-time alignment framework that\ntreats translation as a predictive planning problem, adapting Model Predictive\nControl to iteratively refine translation outputs. Experiments on WMT24\nDiscourse-Level Literary Translation show that Plan2Align significantly\nimproves paragraph-level translation, achieving performance surpassing or on\npar with the existing training-time and test-time alignment methods on\nLLaMA-3.1 8B.", "published": "2025-02-28 07:24:33", "link": "http://arxiv.org/abs/2502.20795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Substitute Components for Compositional Generalization", "abstract": "Despite the rising prevalence of neural language models, recent empirical\nevidence suggests their deficiency in compositional generalization. One of the\ncurrent de-facto solutions to this problem is compositional data augmentation,\nwhich aims to introduce additional compositional inductive bias. However,\nexisting handcrafted augmentation strategies offer limited improvement when\nsystematic generalization of neural language models requires multi-grained\ncompositional bias (i.e., not limited to either lexical or structural biases\nalone) or when training sentences have an imbalanced difficulty distribution.\nTo address these challenges, we first propose a novel compositional\naugmentation strategy called Component Substitution (CompSub), which enables\nmulti-grained composition of substantial substructures across the entire\ntraining set. Furthermore, we introduce the Learning Component Substitution\n(LCS) framework. This framework empowers the learning of component substitution\nprobabilities in CompSub in an end-to-end manner by maximizing the loss of\nneural language models, thereby prioritizing challenging compositions with\nelusive concepts and novel contexts. We extend the key ideas of CompSub and LCS\nto the recently emerging in-context learning scenarios of pre-trained large\nlanguage models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot\ncompositional generalization of state-of-the-art (SOTA) LLMs. Theoretically, we\nprovide insights into why applying our algorithms to language models can\nimprove compositional generalization performance. Empirically, our results on\nfour standard compositional generalization benchmarks(SCAN, COGS, GeoQuery, and\nCOGS-QL) demonstrate the superiority of CompSub, LCS, and LCS-ICL, with\nimprovements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively.", "published": "2025-02-28 08:30:47", "link": "http://arxiv.org/abs/2502.20834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Power of Personality: A Human Simulation Perspective to Investigate\n  Large Language Model Agents", "abstract": "Large language models (LLMs) excel in both closed tasks (including\nproblem-solving, and code generation) and open tasks (including creative\nwriting), yet existing explanations for their capabilities lack connections to\nreal-world human intelligence. To fill this gap, this paper systematically\ninvestigates LLM intelligence through the lens of ``human simulation'',\naddressing three core questions: (1) How do personality traits affect\nproblem-solving in closed tasks? (2) How do traits shape creativity in open\ntasks? (3) How does single-agent performance influence multi-agent\ncollaboration? By assigning Big Five personality traits to LLM agents and\nevaluating their performance in single- and multi-agent settings, we reveal\nthat specific traits significantly influence reasoning accuracy (closed tasks)\nand creative output (open tasks). Furthermore, multi-agent systems exhibit\ncollective intelligence distinct from individual capabilities, driven by\ndistinguishing combinations of personalities. We demonstrate that LLMs\ninherently simulate human behavior through next-token prediction, mirroring\nhuman language, decision-making, and collaborative dynamics.", "published": "2025-02-28 09:01:39", "link": "http://arxiv.org/abs/2502.20859v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Language Models Understand Honorific Systems in Javanese?", "abstract": "The Javanese language features a complex system of honorifics that vary\naccording to the social status of the speaker, listener, and referent. Despite\nits cultural and linguistic significance, there has been limited progress in\ndeveloping a comprehensive corpus to capture these variations for natural\nlanguage processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a\ncarefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh\nBasa, the Javanese speech etiquette framework that dictates the choice of words\nand phrases based on social hierarchy and context. Using Unggah-Ungguh, we\nassess the ability of language models (LMs) to process various levels of\nJavanese honorifics through classification and machine translation tasks. To\nfurther evaluate cross-lingual LMs, we conduct machine translation experiments\nbetween Javanese (at specific honorific levels) and Indonesian. Additionally,\nwe explore whether LMs can generate contextually appropriate Javanese\nhonorifics in conversation tasks, where the honorific usage should align with\nthe social role and contextual cues. Our findings indicate that current LMs\nstruggle with most honorific levels, exhibitinga bias toward certain honorific\ntiers.", "published": "2025-02-28 09:05:35", "link": "http://arxiv.org/abs/2502.20864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Benchmarking LLMs for Zero-Shot Dependency Parsing", "abstract": "While LLMs excel in zero-shot tasks, their performance in linguistic\nchallenges like syntactic parsing has been less scrutinized. This paper studies\nstate-of-the-art open-weight LLMs on the task by comparing them to baselines\nthat do not have access to the input sentence, including baselines that have\nnot been used in this context such as random projective trees or optimal linear\narrangements. The results show that most of the tested LLMs cannot outperform\nthe best uninformed baselines, with only the newest and largest versions of\nLLaMA doing so for most languages, and still achieving rather low performance.\nThus, accurate zero-shot syntactic parsing is not forthcoming with open LLMs.", "published": "2025-02-28 09:08:57", "link": "http://arxiv.org/abs/2502.20866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProBench: Benchmarking Large Language Models in Competitive Programming", "abstract": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging,\nlarge language models (LLMs) have entered a new phase of development. However,\nexisting benchmarks for coding evaluation are gradually inadequate to assess\nthe capability of advanced LLMs in code reasoning. To bridge the gap for\nhigh-level code reasoning assessment, we propose ProBench to benchmark LLMs in\ncompetitive programming, drawing inspiration from the International Collegiate\nProgramming Contest. ProBench collects a comprehensive set of competitive\nprogramming problems from Codeforces, Luogu, and Nowcoder platforms during the\nperiod from July to December 2024, obtaining real test results through online\nsubmissions to ensure the fairness and accuracy of the evaluation. We establish\na unified problem attribute system, including difficulty grading and algorithm\ntagging. With carefully collected and annotated data in ProBench, we\nsystematically assess 9 latest LLMs in competitive programming across multiple\ndimensions, including thought chain analysis, error type diagnosis, and\nreasoning depth evaluation. Experimental results show that QwQ-32B-Preview\nachieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38,\nsuggesting that models trained with specialized reasoning tasks significantly\noutperform general-purpose models (even larger than reasoning-oriented models)\nin programming. Further analysis also reveals key areas for programming\ncapability enhancement, e.g., algorithm adaptability and reasoning sufficiency,\nproviding important insights for the future development of reasoning models.", "published": "2025-02-28 09:12:42", "link": "http://arxiv.org/abs/2502.20868v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Demographics: Fine-tuning Large Language Models to Predict\n  Individuals' Subjective Text Perceptions", "abstract": "People naturally vary in their annotations for subjective questions and some\nof this variation is thought to be due to the person's sociodemographic\ncharacteristics. LLMs have also been used to label data, but recent work has\nshown that models perform poorly when prompted with sociodemographic\nattributes, suggesting limited inherent sociodemographic knowledge. Here, we\nask whether LLMs can be trained to be accurate sociodemographic models of\nannotator variation. Using a curated dataset of five tasks with standardized\nsociodemographics, we show that models do improve in sociodemographic prompting\nwhen trained but that this performance gain is largely due to models learning\nannotator-specific behaviour rather than sociodemographic patterns. Across all\ntasks, our results suggest that models learn little meaningful connection\nbetween sociodemographics and annotation, raising doubts about the current use\nof LLMs for simulating sociodemographic variation and behaviour.", "published": "2025-02-28 09:53:42", "link": "http://arxiv.org/abs/2502.20897v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A database to support the evaluation of gender biases in GPT-4o output", "abstract": "The widespread application of Large Language Models (LLMs) involves ethical\nrisks for users and societies. A prominent ethical risk of LLMs is the\ngeneration of unfair language output that reinforces or exacerbates harm for\nmembers of disadvantaged social groups through gender biases (Weidinger et al.,\n2022; Bender et al., 2021; Kotek et al., 2023). Hence, the evaluation of the\nfairness of LLM outputs with respect to such biases is a topic of rising\ninterest. To advance research in this field, promote discourse on suitable\nnormative bases and evaluation methodologies, and enhance the reproducibility\nof related studies, we propose a novel approach to database construction. This\napproach enables the assessment of gender-related biases in LLM-generated\nlanguage beyond merely evaluating their degree of neutralization.", "published": "2025-02-28 09:54:13", "link": "http://arxiv.org/abs/2502.20898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Evaluation of Meter and Rhyme in Russian Generative and\n  Human-Authored Poetry", "abstract": "Generative poetry systems require effective tools for data engineering and\nautomatic evaluation, particularly to assess how well a poem adheres to\nversification rules, such as the correct alternation of stressed and unstressed\nsyllables and the presence of rhymes.\n  In this work, we introduce the Russian Poetry Scansion Tool library designed\nfor stress mark placement in Russian-language syllabo-tonic poetry, rhyme\ndetection, and identification of defects of poeticness. Additionally, we\nrelease RIFMA -- a dataset of poem fragments spanning various genres and forms,\nannotated with stress marks. This dataset can be used to evaluate the\ncapability of modern large language models to accurately place stress marks in\npoetic texts.\n  The published resources provide valuable tools for researchers and\npractitioners in the field of creative generative AI, facilitating advancements\nin the development and evaluation of generative poetry systems.", "published": "2025-02-28 10:39:07", "link": "http://arxiv.org/abs/2502.20931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play\n  Fine-Tuning of LLMs", "abstract": "Role-playing enables large language models (LLMs) to engage users in\nimmersive and personalized interactions, but it also introduces significant\nsafety risks. Existing role-play fine-tuning techniques improve role\nadaptability but may degrade safety performance, particularly for villainous\ncharacters. In this work, we conduct the first comprehensive assessment of\nrole-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.\nOur experiments reveal that role-play fine-tuning leads to a noticeable decline\nin safety performance, with safety risks varying based on character traits. To\ntackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a\nnovel method designed to balance role-playing capabilities and safety.\nExtensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and\nQwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms\nstate-of-the-art baselines under both LoRA and full-parameter fine-tuning\nsettings. Our findings highlight the necessity of role-adaptive safety measures\nand provide insights into mitigating role-specific safety risks in role-playing\nLLMs.", "published": "2025-02-28 11:31:27", "link": "http://arxiv.org/abs/2502.20968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?", "abstract": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic.", "published": "2025-02-28 11:37:52", "link": "http://arxiv.org/abs/2502.20973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Set-Theoretic Compositionality of Sentence Embeddings", "abstract": "Sentence encoders play a pivotal role in various NLP tasks; hence, an\naccurate evaluation of their compositional properties is paramount. However,\nexisting evaluation methods predominantly focus on goal task-specific\nperformance. This leaves a significant gap in understanding how well sentence\nembeddings demonstrate fundamental compositional properties in a\ntask-independent context. Leveraging classical set theory, we address this gap\nby proposing six criteria based on three core \"set-like\"\ncompositions/operations: \\textit{TextOverlap}, \\textit{TextDifference}, and\n\\textit{TextUnion}. We systematically evaluate $7$ classical and $9$ Large\nLanguage Model (LLM)-based sentence encoders to assess their alignment with\nthese criteria. Our findings show that SBERT consistently demonstrates set-like\ncompositional properties, surpassing even the latest LLMs. Additionally, we\nintroduce a new dataset of ~$192$K samples designed to facilitate future\nbenchmarking efforts on set-like compositionality of sentence embeddings.", "published": "2025-02-28 11:40:34", "link": "http://arxiv.org/abs/2502.20975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capability Localization: Capabilities Can be Localized rather than\n  Individual Knowledge", "abstract": "Large scale language models have achieved superior performance in tasks\nrelated to natural language processing, however, it is still unclear how model\nparameters affect performance improvement. Previous studies assumed that\nindividual knowledge is stored in local parameters, and the storage form of\nindividual knowledge is dispersed parameters, parameter layers, or parameter\nchains, which are not unified. We found through fidelity and reliability\nevaluation experiments that individual knowledge cannot be localized.\nAfterwards, we constructed a dataset for decoupling experiments and discovered\nthe potential for localizing data commonalities. To further reveal this\nphenomenon, this paper proposes a Commonality Neuron Localization (CNL) method,\nwhich successfully locates commonality neurons and achieves a neuron overlap\nrate of 96.42% on the GSM8K dataset. Finally, we have demonstrated through\ncross data experiments that commonality neurons are a collection of capability\nneurons that possess the capability to enhance performance. Our code is\navailable at https://github.com/nlpkeg/Capability-Neuron-Localization.", "published": "2025-02-28 12:22:13", "link": "http://arxiv.org/abs/2502.20992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in\n  Persuasive Dialogues", "abstract": "The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\ninteractions. Recent research has emerged to evaluate whether Large Language\nModels (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM\nin LLMs, existing benchmarks focus predominantly on physical perception with\nprinciples guided by the Sally-Anne test in synthetic stories and\nconversations, failing to capture the complex psychological activities of\nmental states in real-life social interactions. To mitigate this gap, we\npropose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of\nLLMs in persuasive dialogues. Our framework introduces two categories of\nquestions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees), and (2) ToM Application,\nevaluating whether LLMs can take advantage of inferred mental states to select\neffective persuasion strategies (e.g., emphasize rarity) and evaluate the\neffectiveness of persuasion strategies. Experiments across eight\nstate-of-the-art LLMs reveal that while models excel on multiple questions,\nthey struggle to answer questions that need tracking the dynamics and shifts of\nmental states and understanding the mental states in the whole dialogue\ncomprehensively. Our aim with PersuasiveToM is to allow an effective evaluation\nof the ToM reasoning ability of LLMs with more focus on complex psychological\nactivities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM.", "published": "2025-02-28 13:04:04", "link": "http://arxiv.org/abs/2502.21017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CODI: Compressing Chain-of-Thought into Continuous Space via\n  Self-Distillation", "abstract": "Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling\nstep-by-step reasoning in natural language. However, the language space may be\nsuboptimal for reasoning. While implicit CoT methods attempt to enable\nreasoning without explicit CoT tokens, they have consistently lagged behind\nexplicit CoT method in task performance. We propose CODI (Continuous\nChain-of-Thought via Self-Distillation), a novel framework that distills CoT\ninto a continuous space, where a shared model acts as both teacher and student,\njointly learning explicit and implicit CoT while aligning their hidden\nactivation on the token generating the final answer. CODI is the first implicit\nCoT method to match explicit CoT's performance on GSM8k while achieving 3.1x\ncompression, surpassing the previous state-of-the-art by 28.2% in accuracy.\nFurthermore, CODI demonstrates scalability, robustness, and generalizability to\nmore complex CoT datasets. Additionally, CODI retains interpretability by\ndecoding its continuous thoughts, making its reasoning process transparent. Our\nfindings establish implicit CoT as not only a more efficient but a powerful\nalternative to explicit CoT.", "published": "2025-02-28 14:07:48", "link": "http://arxiv.org/abs/2502.21074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating patient cohorts from electronic health records using two-step\n  retrieval-augmented text-to-SQL generation", "abstract": "Clinical cohort definition is crucial for patient recruitment and\nobservational studies, yet translating inclusion/exclusion criteria into SQL\nqueries remains challenging and manual. We present an automated system\nutilizing large language models that combines criteria parsing, two-level\nretrieval augmented generation with specialized knowledge bases, medical\nconcept standardization, and SQL generation to retrieve patient cohorts with\npatient funnels. The system achieves 0.75 F1-score in cohort identification on\nEHR data, effectively capturing complex temporal and logical relationships.\nThese results demonstrate the feasibility of automated cohort generation for\nepidemiological research.", "published": "2025-02-28 14:46:02", "link": "http://arxiv.org/abs/2502.21107v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Linguistic Diversity on Social Media", "abstract": "This chapter explores the efficacy of using social media data to examine\nchanging linguistic behaviour of a place. We focus our investigation on\nAotearoa New Zealand where official statistics from the census is the only\nsource of language use data. We use published census data as the ground truth\nand the social media sub-corpus from the Corpus of Global Language Use as our\nalternative data source. We use place as the common denominator between the two\ndata sources. We identify the language conditions of each tweet in the social\nmedia data set and validated our results with two language identification\nmodels. We then compare levels of linguistic diversity at national, regional,\nand local geographies. The results suggest that social media language data has\nthe possibility to provide a rich source of spatial and temporal insights on\nthe linguistic profile of a place. We show that social media is sensitive to\ndemographic and sociopolitical changes within a language and at low-level\nregional and local geographies.", "published": "2025-02-28 16:56:34", "link": "http://arxiv.org/abs/2502.21224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring internal access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.", "published": "2025-02-28 17:09:08", "link": "http://arxiv.org/abs/2502.21239v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Token-level Ensembling of Models with Different Vocabularies", "abstract": "Model ensembling is a technique to combine the predicted distributions of two\nor more models, often leading to improved robustness and performance. For\nensembling in text generation, the next token's probability distribution is\nderived from a weighted sum of the distributions of each individual model. This\nrequires the underlying models to share the same subword vocabulary, limiting\nthe applicability of ensembling, since many open-sourced models have distinct\nvocabularies. In research settings, experimentation or upgrades to vocabularies\nmay introduce multiple vocabulary sizes. This paper proposes an inference-time\nonly algorithm that allows for ensembling models with different vocabularies,\nwithout the need to learn additional parameters or alter the underlying models.\nInstead, the algorithm ensures that tokens generated by the ensembled models\n\\textit{agree} in their surface form. We apply this technique to combinations\nof traditional encoder-decoder models and decoder-only LLMs and evaluate on\nmachine translation. In addition to expanding to model pairs that were\npreviously incapable of token-level ensembling, our algorithm frequently\nimproves translation performance over either model individually.", "published": "2025-02-28 17:41:27", "link": "http://arxiv.org/abs/2502.21265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset With\n  Faithfulness Based on Causal Theory of Mind", "abstract": "Persuasive dialogue plays a pivotal role in human communication, influencing\nvarious domains. Recent persuasive dialogue datasets often fail to align with\nreal-world interpersonal interactions, leading to unfaithful representations.\nFor instance, unrealistic scenarios may arise, such as when the persuadee\nexplicitly instructs the persuader on which persuasion strategies to employ,\nwith each of the persuadee's questions corresponding to a specific strategy for\nthe persuader to follow. This issue can be attributed to a violation of the\n\"Double Blind\" condition, where critical information is fully shared between\nparticipants. In actual human interactions, however, key information such as\nthe mental state of the persuadee and the persuasion strategies of the\npersuader is not directly accessible. The persuader must infer the persuadee's\nmental state using Theory of Mind capabilities and construct arguments that\nalign with the persuadee's motivations. To address this gap, we introduce\nToMMA, a novel multi-agent framework for dialogue generation that is guided by\ncausal Theory of Mind. This framework ensures that information remains\nundisclosed between agents, preserving \"double-blind\" conditions, while causal\nToM directs the persuader's reasoning, enhancing alignment with human-like\npersuasion dynamics. Consequently, we present CToMPersu, a multi-domain,\nmulti-turn persuasive dialogue dataset that tackles both double-blind and\nlogical coherence issues, demonstrating superior performance across multiple\nmetrics and achieving better alignment with real human dialogues. Our dataset\nand prompts are available at https://github.com/DingyiZhang/ToMMA-CToMPersu .", "published": "2025-02-28 18:28:16", "link": "http://arxiv.org/abs/2502.21297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary\n  Recommendations", "abstract": "Large Language Models (LLMs) effectively leverage common-sense knowledge for\ngeneral reasoning, yet they struggle with personalized reasoning when tasked\nwith interpreting multifactor personal data. This limitation restricts their\napplicability in domains that require context-aware decision-making tailored to\nindividuals. This paper introduces Personalized Causal Graph Reasoning as an\nagentic framework that enhances LLM reasoning by incorporating personal causal\ngraphs derived from data of individuals. These graphs provide a foundation that\nguides the LLM's reasoning process. We evaluate it on a case study on\nnutrient-oriented dietary recommendations, which requires personal reasoning\ndue to the implicit unique dietary effects. We propose a counterfactual\nevaluation to estimate the efficiency of LLM-recommended foods for glucose\nmanagement. Results demonstrate that the proposed method efficiently provides\npersonalized dietary recommendations to reduce average glucose iAUC across\nthree time windows, which outperforms the previous approach. LLM-as-a-judge\nevaluation results indicate that our proposed method enhances personalization\nin the reasoning process.", "published": "2025-02-28 19:25:04", "link": "http://arxiv.org/abs/2503.00134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCORE: Systematic COnsistency and Robustness Evaluation for Large\n  Language Models", "abstract": "Typical evaluations of Large Language Models (LLMs) report a single metric\nper dataset, often representing the model's best-case performance under\ncarefully selected settings. Unfortunately, this approach overlooks model\nrobustness and reliability in real-world applications. For instance, simple\nparaphrasing of prompts on the MMLU-Pro dataset causes accuracy fluctuations of\nup to 10\\%, while reordering answer choices in the AGIEval dataset results in\naccuracy differences of up to 6.1\\%. While some studies discuss issues with LLM\nrobustness, there is no unified or centralized framework for evaluating the\nrobustness of language models. To address this gap and consolidate existing\nresearch on model robustness, we present SCORE ($\\mathbf{S}$ystematic\n$\\mathbf{CO}$nsistency and $\\mathbf{R}$obustness $\\mathbf{E}$valuation), a\ncomprehensive framework for non-adversarial evaluation of LLMs. The SCORE\nframework evaluates models by repeatedly testing them on the same benchmarks in\nvarious setups to give a realistic estimate of their accuracy and consistency.\nWe release the code publicly and start an LLM robustness leaderboard to\nfacilitate further development and research.", "published": "2025-02-28 19:27:29", "link": "http://arxiv.org/abs/2503.00137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Uncertainty Estimation Methods on Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks. However, these models could offer biased, hallucinated, or\nnon-factual responses camouflaged by their fluency and realistic appearance.\nUncertainty estimation is the key method to address this challenge. While\nresearch efforts in uncertainty estimation are ramping up, there is a lack of\ncomprehensive and dedicated surveys on LLM uncertainty estimation. This survey\npresents four major avenues of LLM uncertainty estimation. Furthermore, we\nperform extensive experimental evaluations across multiple methods and\ndatasets. At last, we provide critical and promising future directions for LLM\nuncertainty estimation.", "published": "2025-02-28 20:38:39", "link": "http://arxiv.org/abs/2503.00172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Llamarine: Open-source Maritime Industry-specific Large Language Model", "abstract": "Large Language Models (LLMs) have demonstrated substantial potential in\naddressing complex reasoning tasks, yet their general-purpose nature often\nlimits their effectiveness in specialized domains such as maritime navigation.\nTo bridge this gap, we introduce Llamarine, the first open-source LLM designed\nspecifically for maritime navigation. Llamarine 1.0 is developed through\ncontinued pretraining and fine-tuning on a high-quality corpus comprising\nmaritime textbooks, research publications, and web text from Wikipedia. This\ndomain-specific training enables the model to acquire expert-level knowledge in\nnavigational principles, collision avoidance, route optimization, and\nregulatory compliance. Our key contributions include (a) the curation of a\ncomprehensive maritime dataset from authoritative sources, ensuring depth and\nreliability in the model's knowledge base; (b) the development of a\nfoundational model capable of reasoning about complex navigational challenges\nwith greater accuracy than general-purpose LLMs; and (c) the establishment of a\nbenchmark to evaluate performance in maritime-specific decision-making tasks.\nExperimental results demonstrate that Llamarine outperforms both\ngeneral-purpose and commercial LLMs in critical navigation-related tasks, such\nas trajectory planning, risk assessment, and compliance with maritime\nregulations. By providing an open-source foundation model trained exclusively\non high-quality maritime literature, Llamarine paves the way for AI-driven\nadvancements in maritime safety, efficiency, and operational decision-making.", "published": "2025-02-28 21:39:22", "link": "http://arxiv.org/abs/2503.00203v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoencoder-Based Framework to Capture Vocabulary Quality in NLP", "abstract": "Linguistic richness is essential for advancing natural language processing\n(NLP), as dataset characteristics often directly influence model performance.\nHowever, traditional metrics such as Type-Token Ratio (TTR), Vocabulary\nDiversity (VOCD), and Measure of Lexical Text Diversity (MTLD) do not\nadequately capture contextual relationships, semantic richness, and structural\ncomplexity. In this paper, we introduce an autoencoder-based framework that\nuses neural network capacity as a proxy for vocabulary richness, diversity, and\ncomplexity, enabling a dynamic assessment of the interplay between vocabulary\nsize, sentence structure, and contextual depth. We validate our approach on two\ndistinct datasets: the DIFrauD dataset, which spans multiple domains of\ndeceptive and fraudulent text, and the Project Gutenberg dataset, representing\ndiverse languages, genres, and historical periods. Experimental results\nhighlight the robustness and adaptability of our method, offering practical\nguidance for dataset curation and NLP model design. By enhancing traditional\nvocabulary evaluation, our work fosters the development of more context-aware,\nlinguistically adaptive NLP systems.", "published": "2025-02-28 21:45:28", "link": "http://arxiv.org/abs/2503.00209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\u00c0 la recherche du sens perdu: your favourite LLM might have more to\n  say than you can understand", "abstract": "We report a peculiar observation that LLMs can assign hidden meanings to\nsequences that seem visually incomprehensible to humans: for example, a\nnonsensical phrase consisting of Byzantine musical symbols is recognized by\ngpt-4o as \"say abracadabra\". Moreover, some models can communicate using these\nsequences.\n  Some of these meanings are hypothesized to partly originate in the massive\nspurious correlations due to BPE tokenization. We systematically evaluate the\npresence of such abilities in a wide range of models: Claude-3.5 Haiku,\nClaude-3.5 Sonnet (New and Old), Claude-3.7 Sonnet, gpt-4o mini, gpt-4o,\no1-mini, Llama-3.3 70B, DeepSeek-R1-Distill-Lllama 70B, Qwen2.5 1.5B, Qwen2.5\n32B, Phi-3.5 mini, GigaChat-Max, Vikhr-Llama-3.2 1B.\n  We argue that this observation might have far-reaching consequences for both\nsafety and security of the modern and future LLMs and systems that employ them.\nAs an illustration, we show that applying this method in combination with\nsimple templates is sufficient to jailbreak previous generation models, with\nASR = 0.4 on gpt-4o mini.\n  Our code and data artifacts are available at\nhttps://github.com/L3G5/llm-hidden-meanings", "published": "2025-02-28 22:18:23", "link": "http://arxiv.org/abs/2503.00224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Building Interpretable Rule-Based\n  Data-to-Text Systems", "abstract": "We introduce a simple approach that uses a large language model (LLM) to\nautomatically implement a fully interpretable rule-based data-to-text system in\npure Python. Experimental evaluation on the WebNLG dataset showed that such a\nconstructed system produces text of better quality (according to the BLEU and\nBLEURT metrics) than the same LLM prompted to directly produce outputs, and\nproduces fewer hallucinations than a BART language model fine-tuned on the same\ndata. Furthermore, at runtime, the approach generates text in a fraction of the\nprocessing time required by neural approaches, using only a single CPU", "published": "2025-02-28 00:23:55", "link": "http://arxiv.org/abs/2502.20609v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continuous Adversarial Text Representation Learning for Affective\n  Recognition", "abstract": "While pre-trained language models excel at semantic understanding, they often\nstruggle to capture nuanced affective information critical for affective\nrecognition tasks. To address these limitations, we propose a novel framework\nfor enhancing emotion-aware embeddings in transformer-based models. Our\napproach introduces a continuous valence-arousal labeling system to guide\ncontrastive learning, which captures subtle and multi-dimensional emotional\nnuances more effectively. Furthermore, we employ a dynamic token perturbation\nmechanism, using gradient-based saliency to focus on sentiment-relevant tokens,\nimproving model sensitivity to emotional cues. The experimental results\ndemonstrate that the proposed framework outperforms existing methods, achieving\nup to 15.5% improvement in the emotion classification benchmark, highlighting\nthe importance of employing continuous labels. This improvement demonstrates\nthat the proposed framework is effective in affective representation learning\nand enables precise and contextually relevant emotional understanding.", "published": "2025-02-28 00:29:09", "link": "http://arxiv.org/abs/2502.20613v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LexRAG: Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal\n  Consultation Conversation", "abstract": "Retrieval-augmented generation (RAG) has proven highly effective in improving\nlarge language models (LLMs) across various domains. However, there is no\nbenchmark specifically designed to assess the effectiveness of RAG in the legal\ndomain, which restricts progress in this area. To fill this gap, we propose\nLexRAG, the first benchmark to evaluate RAG systems for multi-turn legal\nconsultations. LexRAG consists of 1,013 multi-turn dialogue samples and 17,228\ncandidate legal articles. Each sample is annotated by legal experts and\nconsists of five rounds of progressive questioning. LexRAG includes two key\ntasks: (1) Conversational knowledge retrieval, requiring accurate retrieval of\nrelevant legal articles based on multi-turn context. (2) Response generation,\nfocusing on producing legally sound answers. To ensure reliable\nreproducibility, we develop LexiT, a legal RAG toolkit that provides a\ncomprehensive implementation of RAG system components tailored for the legal\ndomain. Additionally, we introduce an LLM-as-a-judge evaluation pipeline to\nenable detailed and effective assessment. Through experimental analysis of\nvarious LLMs and retrieval methods, we reveal the key limitations of existing\nRAG systems in handling legal consultation conversations. LexRAG establishes a\nnew benchmark for the practical application of RAG systems in the legal domain,\nwith its code and data available at https://github.com/CSHaitao/LexRAG.", "published": "2025-02-28 01:46:32", "link": "http://arxiv.org/abs/2502.20640v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Fine-tuning BERT with Bidirectional LSTM for Fine-grained Movie Reviews\n  Sentiment Analysis", "abstract": "Sentiment Analysis (SA) is instrumental in understanding peoples viewpoints\nfacilitating social media monitoring recognizing products and brands and\ngauging customer satisfaction. Consequently SA has evolved into an active\nresearch domain within Natural Language Processing (NLP). Many approaches\noutlined in the literature devise intricate frameworks aimed at achieving high\naccuracy, focusing exclusively on either binary sentiment classification or\nfine-grained sentiment classification. In this paper our objective is to\nfine-tune the pre-trained BERT model with Bidirectional LSTM (BiLSTM) to\nenhance both binary and fine-grained SA specifically for movie reviews. Our\napproach involves conducting sentiment classification for each review followed\nby computing the overall sentiment polarity across all reviews. We present our\nfindings on binary classification as well as fine-grained classification\nutilizing benchmark datasets. Additionally we implement and assess two accuracy\nimprovement techniques Synthetic Minority Oversampling Technique (SMOTE) and\nNLP Augmenter (NLPAUG) to bolster the models generalization in fine-grained\nsentiment classification. Finally a heuristic algorithm is employed to\ncalculate the overall polarity of predicted reviews from the BERT+BiLSTM output\nvector. Our approach performs comparably with state-of-the-art (SOTA)\ntechniques in both classifications. For instance in binary classification we\nachieve 97.67% accuracy surpassing the leading SOTA model\nNB-weighted-BON+dv-cosine by 0.27% on the renowned IMDb dataset. Conversely for\nfive-class classification on SST-5 while the top SOTA model\nRoBERTa+large+Self-explaining attains 55.5% accuracy our model achieves 59.48%\naccuracy surpassing the BERT-large baseline by 3.6%.", "published": "2025-02-28 03:30:48", "link": "http://arxiv.org/abs/2502.20682v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ProAI: Proactive Multi-Agent Conversational AI with Structured Knowledge\n  Base for Psychiatric Diagnosis", "abstract": "Most LLM-driven conversational AI systems operate reactively, responding to\nuser prompts without guiding the interaction. Most LLM-driven conversational AI\nsystems operate reactively, responding to user prompts without guiding the\ninteraction. However, many real-world applications-such as psychiatric\ndiagnosis, consulting, and interviews-require AI to take a proactive role,\nasking the right questions and steering conversations toward specific\nobjectives. Using mental health differential diagnosis as an application\ncontext, we introduce ProAI, a goal-oriented, proactive conversational AI\nframework. ProAI integrates structured knowledge-guided memory, multi-agent\nproactive reasoning, and a multi-faceted evaluation strategy, enabling LLMs to\nengage in clinician-style diagnostic reasoning rather than simple response\ngeneration. Through simulated patient interactions, user experience assessment,\nand professional clinical validation, we demonstrate that ProAI achieves up to\n83.3% accuracy in mental disorder differential diagnosis while maintaining\nprofessional and empathetic interaction standards. These results highlight the\npotential for more reliable, adaptive, and goal-driven AI diagnostic\nassistants, advancing LLMs beyond reactive dialogue systems.", "published": "2025-02-28 03:45:39", "link": "http://arxiv.org/abs/2502.20689v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Retrieval Backward Attention without Additional Training: Enhance\n  Embeddings of Large Language Models via Repetition", "abstract": "Language models can be viewed as functions that embed text into Euclidean\nspace, where the quality of the embedding vectors directly determines model\nperformance, training such neural networks involves various uncertainties. This\npaper focuses on improving the performance of pre-trained language models in\nzero-shot settings through a simple and easily implementable method. We propose\na novel backward attention mechanism to enhance contextual information\nencoding. Evaluated on the Chinese Massive Text Embedding Benchmark (C-MTEB),\nour approach achieves significant improvements across multiple tasks, providing\nvaluable insights for advancing zero-shot learning capabilities.", "published": "2025-02-28 05:19:18", "link": "http://arxiv.org/abs/2502.20726v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepSolution: Boosting Complex Engineering Solution Design via\n  Tree-based Exploration and Bi-point Thinking", "abstract": "Designing solutions for complex engineering challenges is crucial in human\nproduction activities. However, previous research in the retrieval-augmented\ngeneration (RAG) field has not sufficiently addressed tasks related to the\ndesign of complex engineering solutions. To fill this gap, we introduce a new\nbenchmark, SolutionBench, to evaluate a system's ability to generate complete\nand feasible solutions for engineering problems with multiple complex\nconstraints. To further advance the design of complex engineering solutions, we\npropose a novel system, SolutionRAG, that leverages the tree-based exploration\nand bi-point thinking mechanism to generate reliable solutions. Extensive\nexperimental results demonstrate that SolutionRAG achieves state-of-the-art\n(SOTA) performance on the SolutionBench, highlighting its potential to enhance\nthe automation and reliability of complex engineering solution design in\nreal-world applications.", "published": "2025-02-28 05:23:10", "link": "http://arxiv.org/abs/2502.20730v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Teach-to-Reason with Scoring: Self-Explainable Rationale-Driven\n  Multi-Trait Essay Scoring", "abstract": "Multi-trait automated essay scoring (AES) systems provide a fine-grained\nevaluation of an essay's diverse aspects. While they excel in scoring, prior\nsystems fail to explain why specific trait scores are assigned. This lack of\ntransparency leaves instructors and learners unconvinced of the AES outputs,\nhindering their practical use. To address this, we propose a self-explainable\nRationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME\nleverages the reasoning capabilities of large language models (LLMs) by\ndistilling them into a smaller yet effective scorer. This more manageable\nstudent model is optimized to sequentially generate a trait score followed by\nthe corresponding rationale, thereby inherently learning to select a more\njustifiable score by considering the subsequent rationale during training. Our\nfindings indicate that while LLMs underperform in direct AES tasks, they excel\nin rationale generation when provided with precise numerical scores. Thus,\nRaDME integrates the superior reasoning capacities of LLMs into the robust\nscoring accuracy of an optimized smaller model. Extensive experiments\ndemonstrate that RaDME achieves both accurate and adequate reasoning while\nsupporting high-quality multi-trait scoring, significantly enhancing the\ntransparency of AES.", "published": "2025-02-28 05:54:23", "link": "http://arxiv.org/abs/2502.20748v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient\n  Long-Sequence Inference", "abstract": "Large language models (LLMs) encounter computational challenges during\nlong-sequence inference, especially in the attention pre-filling phase, where\nthe complexity grows quadratically with the prompt length. Previous efforts to\nmitigate these challenges have relied on fixed sparse attention patterns or\nidentifying sparse attention patterns based on limited cases. However, these\nmethods lacked the flexibility to efficiently adapt to varying input demands.\nIn this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling\nmechanism that dynamically adjusts sparse attention patterns and computational\nbudget in real-time to meet the specific requirements of each input and\nattention head. The flexibility of our method is demonstrated through two key\ninnovations: 1) Query-Aware Sparse Pattern Determination: By measuring\nJensen-Shannon divergence, this component adaptively switches between\nquery-specific diverse attention patterns and predefined attention patterns. 2)\nCumulative-Attention Based Index Selection: This component dynamically selects\nquery-key indexes to be computed based on different attention patterns,\nensuring the sum of attention scores meets a predefined threshold. FlexPrefill\nadaptively optimizes the sparse pattern and sparse ratio of each attention head\nbased on the prompt, enhancing efficiency in long-sequence inference tasks.\nExperimental results show significant improvements in both speed and accuracy\nover prior methods, providing a more flexible and efficient solution for LLM\ninference.", "published": "2025-02-28 06:34:53", "link": "http://arxiv.org/abs/2502.20766v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Pilot Empirical Study on When and How to Use Knowledge Graphs as\n  Retrieval Augmented Generation", "abstract": "The integration of Knowledge Graphs (KGs) into the Retrieval Augmented\nGeneration (RAG) framework has attracted significant interest, with early\nstudies showing promise in mitigating hallucinations and improving model\naccuracy. However, a systematic understanding and comparative analysis of the\nrapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the\nfoundation for systematically answering the question of when and how to use\nKG-RAG by analyzing their performance in various application scenarios\nassociated with different technical configurations. After outlining the mind\nmap using KG-RAG framework and summarizing its popular pipeline, we conduct a\npilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG\nmethods across 7 datasets in diverse scenarios, analyzing the impact of 9\nKG-RAG configurations in combination with 17 LLMs. Our results underscore the\ncritical role of appropriate application conditions and optimal configurations\nof KG-RAG components.", "published": "2025-02-28 08:53:08", "link": "http://arxiv.org/abs/2502.20854v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MAMUT: A Novel Framework for Modifying Mathematical Formulas for the\n  Generation of Specialized Datasets for Language Model Training", "abstract": "Mathematical formulas are a fundamental and widely used component in various\nscientific fields, serving as a universal language for expressing complex\nconcepts and relationships. While state-of-the-art transformer models excel in\nprocessing and understanding natural language, they encounter challenges with\nmathematical notation, which involves a complex structure and diverse\nrepresentations. This study focuses on the development of specialized training\ndatasets to enhance the encoding of mathematical content. We introduce Math\nMutator (MAMUT), a framework capable of generating equivalent and falsified\nversions of a given mathematical formula in LaTeX notation, effectively\ncapturing the mathematical variety in notation of the same concept. Based on\nMAMUT, we have generated four large mathematical datasets containing diverse\nnotation, which can be used to train language models with enhanced mathematical\nembeddings.", "published": "2025-02-28 08:53:42", "link": "http://arxiv.org/abs/2502.20855v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation", "abstract": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.", "published": "2025-02-28 11:52:02", "link": "http://arxiv.org/abs/2502.20984v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Merging Clinical Knowledge into Large Language Models for Medical\n  Research and Applications: A Survey", "abstract": "Clinical knowledge is the collection of information learned from studies on\nthe causes, prognosis, diagnosis, and treatment of diseases. This type of\nknowledge can improve curing performances, and promote physical health. With\nthe emergence of large language models (LLMs), medical artificial intelligence\n(medical AI), which aims to apply academic medical AI systems to real-world\nmedical scenarios, has entered a new age of development, resulting in excellent\nworks such as DoctorGPT and Pangu-Drug from academic and industrial researches.\nHowever, the field lacks a comprehensive compendium and comparison of building\nmedical AI systems from academia and industry. Therefore, this survey focuses\non the building paradigms of medical AI systems including the use of clinical\ndatabases, datasets, training pipelines, integrating medical knowledge graphs,\nsystem applications, and evaluation systems. We hope that this survey can help\nrelevant practical researchers understand the current performance of academic\nmodels in various fields of healthcare, as well as the potential problems and\nfuture directions for implementing these scientific achievements.", "published": "2025-02-28 12:00:51", "link": "http://arxiv.org/abs/2502.20988v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "TempRetriever: Fusion-based Temporal Dense Passage Retrieval for\n  Time-Sensitive Questions", "abstract": "Temporal awareness is crucial in many information retrieval tasks,\nparticularly in scenarios where the relevance of documents depends on their\nalignment with the query's temporal context. Traditional approaches such as\nBM25 and Dense Passage Retrieval (DPR) focus on lexical or semantic similarity\nbut tend to neglect the temporal alignment between queries and documents, which\nis essential for time-sensitive tasks like temporal question answering (TQA).\nWe propose TempRetriever, a novel extension of DPR that explicitly incorporates\ntemporal information by embedding both the query date and document timestamp\ninto the retrieval process. This allows retrieving passages that are not only\ncontextually relevant but also aligned with the temporal intent of queries. We\nevaluate TempRetriever on two large-scale datasets ArchivalQA and\nChroniclingAmericaQA demonstrating its superiority over baseline retrieval\nmodels across multiple metrics. TempRetriever achieves a 6.63\\% improvement in\nTop-1 retrieval accuracy and a 3.79\\% improvement in NDCG@10 compared to the\nstandard DPR on ArchivalQA. Similarly, for ChroniclingAmericaQA, TempRetriever\nexhibits a 9.56\\% improvement in Top-1 retrieval accuracy and a 4.68\\%\nimprovement in NDCG@10. We also propose a novel, time-based negative sampling\nstrategy which further enhances retrieval performance by addressing temporal\nmisalignment during training. Our results underline the importance of temporal\naspects in dense retrieval systems and establish a new benchmark for time-aware\npassage retrieval.", "published": "2025-02-28 13:06:25", "link": "http://arxiv.org/abs/2502.21024v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs", "abstract": "Recent advances in large language models (LLMs) have popularized the\nchain-of-thought (CoT) paradigm, in which models produce explicit reasoning\nsteps in natural language. Although this approach improves interpretability and\nfacilitates external auditing, it may not represent the most computationally\nefficient method for internal reasoning. In contrast, human cognition relies on\nimplicit mental representations that recall past sensory and episodic\ninformation without requiring complete verbalization. In this paper, we propose\na framework that integrates implicit mental representations into the internal\nreasoning processes of LLMs. Preliminary experiments indicate that\nincorporating an Implicit Memory Module (IMM) into a simple GPT model yields a\nreduction of between 35% and 57% in final training loss compared to a regular\nGPT baseline. The addition of an explicit interpretability channel (e.g., a\nchain-of-thought decoder) is straightforward to implement within this approach.\nWe outline theoretical foundations, propose technical mechanisms to scale the\nmemory module, and discuss how these ideas may lead to more efficient and\nrobust reasoning, with optional future extensions for explicit auditability.", "published": "2025-02-28 13:22:29", "link": "http://arxiv.org/abs/2502.21030v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured\n  Data with Text and Relational Information", "abstract": "Large language models (LLMs) have shown impressive abilities in answering\nquestions across various domains, but they often encounter hallucination issues\non questions that require professional and up-to-date knowledge. To address\nthis limitation, retrieval-augmented generation (RAG) techniques have been\nproposed, which retrieve relevant information from external sources to inform\ntheir responses. However, existing RAG methods typically focus on a single type\nof external data, such as vectorized text database or knowledge graphs, and\ncannot well handle real-world questions on semi-structured data containing both\ntext and relational information. To bridge this gap, we introduce PASemiQA, a\nnovel approach that jointly leverages text and relational information in\nsemi-structured data to answer questions. PASemiQA first generates a plan to\nidentify relevant text and relational information to answer the question in\nsemi-structured data, and then uses an LLM agent to traverse the\nsemi-structured data and extract necessary information. Our empirical results\ndemonstrate the effectiveness of PASemiQA across different semi-structured\ndatasets from various domains, showcasing its potential to improve the accuracy\nand reliability of question answering systems on semi-structured data.", "published": "2025-02-28 14:26:47", "link": "http://arxiv.org/abs/2502.21087v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Re-evaluating Theory of Mind evaluation in large language models", "abstract": "The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from \"pure\" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition.", "published": "2025-02-28 14:36:57", "link": "http://arxiv.org/abs/2502.21098v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual\n  Knowledge Transfer", "abstract": "To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in.", "published": "2025-02-28 16:59:30", "link": "http://arxiv.org/abs/2502.21228v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Emerging Concepts in Large Corpora", "abstract": "We introduce a new method to identify emerging concepts in large text\ncorpora. By analyzing changes in the heatmaps of the underlying embedding\nspace, we are able to detect these concepts with high accuracy shortly after\nthey originate, in turn outperforming common alternatives. We further\ndemonstrate the utility of our approach by analyzing speeches in the U.S.\nSenate from 1941 to 2015. Our results suggest that the minority party is more\nactive in introducing new concepts into the Senate discourse. We also identify\nspecific concepts that closely correlate with the Senators' racial, ethnic, and\ngender identities. An implementation of our method is publicly available.", "published": "2025-02-28 18:59:15", "link": "http://arxiv.org/abs/2502.21315v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models", "abstract": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.", "published": "2025-02-28 18:59:54", "link": "http://arxiv.org/abs/2502.21321v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "AnnoCaseLaw: A Richly-Annotated Dataset For Benchmarking Explainable\n  Legal Judgment Prediction", "abstract": "Legal systems worldwide continue to struggle with overwhelming caseloads,\nlimited judicial resources, and growing complexities in legal proceedings.\nArtificial intelligence (AI) offers a promising solution, with Legal Judgment\nPrediction (LJP) -- the practice of predicting a court's decision from the case\nfacts -- emerging as a key research area. However, existing datasets often\nformulate the task of LJP unrealistically, not reflecting its true difficulty.\nThey also lack high-quality annotation essential for legal reasoning and\nexplainability. To address these shortcomings, we introduce AnnoCaseLaw, a\nfirst-of-its-kind dataset of 471 meticulously annotated U.S. Appeals Court\nnegligence cases. Each case is enriched with comprehensive, expert-labeled\nannotations that highlight key components of judicial decision making, along\nwith relevant legal concepts. Our dataset lays the groundwork for more\nhuman-aligned, explainable LJP models. We define three legally relevant tasks:\n(1) judgment prediction; (2) concept identification; and (3) automated case\nannotation, and establish a performance baseline using industry-leading large\nlanguage models (LLMs). Our results demonstrate that LJP remains a formidable\ntask, with application of legal precedent proving particularly difficult. Code\nand data are available at https://github.com/anonymouspolar1/annocaselaw.", "published": "2025-02-28 19:14:48", "link": "http://arxiv.org/abs/2503.00128v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Palm: A Culturally Inclusive and Linguistically Diverse Dataset for\n  Arabic LLMs", "abstract": "As large language models (LLMs) become increasingly integrated into daily\nlife, ensuring their cultural sensitivity and inclusivity is paramount. We\nintroduce our dataset, a year-long community-driven project covering all 22\nArab countries. The dataset includes instructions (input, response pairs) in\nboth Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20\ndiverse topics. Built by a team of 44 researchers across the Arab world, all of\nwhom are authors of this paper, our dataset offers a broad, inclusive\nperspective. We use our dataset to evaluate the cultural and dialectal\ncapabilities of several frontier LLMs, revealing notable limitations. For\ninstance, while closed-source LLMs generally exhibit strong performance, they\nare not without flaws, and smaller open-source models face greater challenges.\nMoreover, certain countries (e.g., Egypt, the UAE) appear better represented\nthan others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code,\nand data for reproducibility are publicly available.", "published": "2025-02-28 19:59:13", "link": "http://arxiv.org/abs/2503.00151v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jawaher: A Multidialectal Dataset of Arabic Proverbs for LLM\n  Benchmarking", "abstract": "Recent advancements in instruction fine-tuning, alignment methods such as\nreinforcement learning from human feedback (RLHF), and optimization techniques\nlike direct preference optimization (DPO) have significantly enhanced the\nadaptability of large language models (LLMs) to user preferences. However,\ndespite these innovations, many LLMs continue to exhibit biases toward Western,\nAnglo-centric, or American cultures, with performance on English data\nconsistently surpassing that of other languages. This reveals a persistent\ncultural gap in LLMs, which complicates their ability to accurately process\nculturally rich and diverse figurative language such as proverbs. To address\nthis, we introduce Jawaher, a benchmark designed to assess LLMs' capacity to\ncomprehend and interpret Arabic proverbs. Jawaher includes proverbs from\nvarious Arabic dialects, along with idiomatic translations and explanations.\nThrough extensive evaluations of both open- and closed-source models, we find\nthat while LLMs can generate idiomatically accurate translations, they struggle\nwith producing culturally nuanced and contextually relevant explanations. These\nfindings highlight the need for ongoing model refinement and dataset expansion\nto bridge the cultural gap in figurative language processing.", "published": "2025-02-28 22:28:00", "link": "http://arxiv.org/abs/2503.00231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CoSMoEs: Compact Sparse Mixture of Experts", "abstract": "Sparse Mixture of Expert (MoE) models are popular foundational architectures\nat large scale, however, under-explored at smaller sizes. Here, we show how to\nenable Compact Sparse Mixture of Experts (CoSMoEs) for on-device inference.\nSpecifically, we tackle the three main on-device dimensions: Quality, Memory\nand Latency. Along the quality axis, we show that in a fair evaluation\n(removing confounding factors) MoE architectures outperform FLOP-aligned dense\nmodels at on-device scale. We introduce weight-decomposed experts, further\nimproving the MoE model performance. Regarding model memory and latency, we\nsignificantly improve model offloading efficiency and, in turn, reduce model\ninference latency.", "published": "2025-02-28 23:25:11", "link": "http://arxiv.org/abs/2503.00245v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Empirical Analysis of LLMs for Countering Misinformation", "abstract": "While Large Language Models (LLMs) can amplify online misinformation, they\nalso show promise in tackling misinformation. In this paper, we empirically\nstudy the capabilities of three LLMs -- ChatGPT, Gemini, and Claude -- in\ncountering political misinformation. We implement a two-step, chain-of-thought\nprompting approach, where models first identify credible sources for a given\nclaim and then generate persuasive responses. Our findings suggest that models\nstruggle to ground their responses in real news sources, and tend to prefer\nciting left-leaning sources. We also observe varying degrees of response\ndiversity among models. Our findings highlight concerns about using LLMs for\nfact-checking through only prompt-engineering, emphasizing the need for more\nrobust guardrails. Our results have implications for both researchers and\nnon-technical users.", "published": "2025-02-28 07:12:03", "link": "http://arxiv.org/abs/2503.01902v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Anthropomorphic Conversational AI Part I: A Practical Framework", "abstract": "Large language models (LLMs), due to their advanced natural language\ncapabilities, have seen significant success in applications where the user\ninterface is usually a conversational artificial intelligence (AI) agent and\nengages the user through multi-round conversations. However, many scenarios\nrequire the agents to exhibit stronger social and conversational intelligence\nand demonstrate more human-like (anthropomorphic) reactions. This is an aspect\nthat foundational LLMs have yet to fully address such that a single call of\nfoundational models might be insufficient.\n  To bridge this gap, we propose a two-stage solution. In this work, we focus\non the first stage, introducing a multi-module framework designed to replicate\nthe key aspects of human intelligence involved in conversations. This framework\ncomprises thinking modules for reasoning, resource modules for managing\nknowledge and external information, and response modules for generating\ncontextually appropriate interactions. With all the modules cooperating, the\nframework would empower the agents to provide a better human-like conversation\nexperience. In the second stage of our approach, these conversational data,\nafter filtering and labeling, can serve as training and testing data for\nreinforcement learning, enabling AI to better capture human preferences. This\nstage is left for future work.\n  In our experiments, volunteers engaged in over 3000 rounds of conversation\nwith the same AI character powered by a standalone LLM and our framework which\nintegrates the same LLM. A separate group of evaluators rated the conversation\nsamples, revealing that our framework significantly enhanced the social and\nconversational intelligence, even without fine-tuning the LLM.", "published": "2025-02-28 03:18:39", "link": "http://arxiv.org/abs/2503.04787v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ext2Gen: Alignment through Unified Extraction and Generation for Robust\n  Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) enhances LLMs by integrating external\nknowledge, but generation remains fragile due to the uncertain placement of\nrelevant chunks and retrieval-induced information overload, leading to\nhallucinations. We propose Ext2Gen, a novel extract-then-generate model that\nenhances RAG robustness by first extracting query-relevant sentences before\ngenerating answers. To optimize this model, we employ preference alignment\nthrough pairwise feedback learning, enabling the model to generate robust\nanswers regardless of variations in retrieval results. Extensive experiments\ndemonstrate that Ext2Gen effectively identifies query-relevant sentences with\nhigh precision and recall, leading to highly reliable answers. Furthermore,\ndeploying our model in a RAG environment reveals that it not only boosts the\nperformance of the base LLM but also synergizes with advanced retrieval\nstrategies like query expansion. The model is available at\nhttps://huggingface.co/DISLab/Ext2Gen-8B-R2.", "published": "2025-02-28 06:46:53", "link": "http://arxiv.org/abs/2503.04789v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SuperRAG: Beyond RAG with Layout-Aware Graph Modeling", "abstract": "This paper introduces layout-aware graph modeling for multimodal RAG.\nDifferent from traditional RAG methods that mostly deal with flat text chunks,\nthe proposed method takes into account the relationship of multimodalities by\nusing a graph structure. To do that, a graph modeling structure is defined\nbased on document layout parsing. The structure of an input document is\nretained with the connection of text chunks, tables, and figures. This\nrepresentation allows the method to handle complex questions that require\ninformation from multimodalities. To confirm the efficiency of the graph\nmodeling, a flexible RAG pipeline is developed using robust components.\nExperimental results on four benchmark test sets confirm the contribution of\nthe layout-aware modeling for performance improvement of the RAG pipeline.", "published": "2025-02-28 09:05:49", "link": "http://arxiv.org/abs/2503.04790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NutriGen: Personalized Meal Plan Generator Leveraging Large Language\n  Models to Enhance Dietary and Nutritional Adherence", "abstract": "Maintaining a balanced diet is essential for overall health, yet many\nindividuals struggle with meal planning due to nutritional complexity, time\nconstraints, and lack of dietary knowledge. Personalized food recommendations\ncan help address these challenges by tailoring meal plans to individual\npreferences, habits, and dietary restrictions. However, existing dietary\nrecommendation systems often lack adaptability, fail to consider real-world\nconstraints such as food ingredient availability, and require extensive user\ninput, making them impractical for sustainable and scalable daily use. To\naddress these limitations, we introduce NutriGen, a framework based on large\nlanguage models (LLM) designed to generate personalized meal plans that align\nwith user-defined dietary preferences and constraints. By building a\npersonalized nutrition database and leveraging prompt engineering, our approach\nenables LLMs to incorporate reliable nutritional references like the USDA\nnutrition database while maintaining flexibility and ease-of-use. We\ndemonstrate that LLMs have strong potential in generating accurate and\nuser-friendly food recommendations, addressing key limitations in existing\ndietary recommendation systems by providing structured, practical, and scalable\nmeal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve\nthe lowest percentage errors of 1.55\\% and 3.68\\%, respectively, producing meal\nplans that closely align with user-defined caloric targets while minimizing\ndeviation and improving precision. Additionally, we compared the performance of\nDeepSeek V3 against several established models to evaluate its potential in\npersonalized nutrition planning.", "published": "2025-02-28 00:05:49", "link": "http://arxiv.org/abs/2502.20601v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Consistency Evaluation of News Article Summaries Generated by Large (and\n  Small) Language Models", "abstract": "Text summarizing is a critical Natural Language Processing (NLP) task with\napplications ranging from information retrieval to content generation. Large\nLanguage Models (LLMs) have shown remarkable promise in generating fluent\nabstractive summaries but they can produce hallucinated details not grounded in\nthe source text. Regardless of the method of generating a summary, high quality\nautomated evaluations remain an open area of investigation. This paper embarks\non an exploration of text summarization with a diverse set of techniques,\nincluding TextRank, BART, Mistral-7B-Instruct, and OpenAI GPT-3.5-Turbo. The\ngenerated summaries are evaluated using traditional metrics such as the\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) Score and\nBidirectional Encoder Representations from Transformers (BERT) Score, as well\nas LLM-powered evaluation methods that directly assess a generated summary's\nconsistency with the source text. We introduce a meta evaluation score which\ndirectly assesses the performance of the LLM evaluation system (prompt +\nmodel). We find that that all summarization models produce consistent summaries\nwhen tested on the XL-Sum dataset, exceeding the consistency of the reference\nsummaries.", "published": "2025-02-28 01:58:17", "link": "http://arxiv.org/abs/2502.20647v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Automatic database description generation for Text-to-SQL", "abstract": "In the context of the Text-to-SQL task, table and column descriptions are\ncrucial for bridging the gap between natural language and database schema. This\nreport proposes a method for automatically generating effective database\ndescriptions when explicit descriptions are unavailable. The proposed method\nemploys a dual-process approach: a coarse-to-fine process, followed by a\nfine-to-coarse process. The coarse-to-fine approach leverages the inherent\nknowledge of LLM to guide the understanding process from databases to tables\nand finally to columns. This approach provides a holistic understanding of the\ndatabase structure and ensures contextual alignment. Conversely, the\nfine-to-coarse approach starts at the column level, offering a more accurate\nand nuanced understanding when stepping back to the table level. Experimental\nresults on the Bird benchmark indicate that using descriptions generated by the\nproposed improves SQL generation accuracy by 0.93\\% compared to not using\ndescriptions, and achieves 37\\% of human-level performance. The source code is\npublicly available at https://github.com/XGenerationLab/XiYan-DBDescGen.", "published": "2025-02-28 02:23:06", "link": "http://arxiv.org/abs/2502.20657v1", "categories": ["cs.AI", "cs.CL", "cs.DB", "I.2; H.2"], "primary_category": "cs.AI"}
{"title": "Disentangling Feature Structure: A Mathematically Provable Two-Stage\n  Training Dynamics in Transformers", "abstract": "Transformers may exhibit two-stage training dynamics during the real-world\ntraining process. For instance, when training GPT-2 on the Counterfact dataset,\nthe answers progress from syntactically incorrect to syntactically correct to\nsemantically correct. However, existing theoretical analyses hardly account for\nthis two-stage phenomenon. In this paper, we theoretically demonstrate how such\ntwo-stage training dynamics occur in transformers. Specifically, we analyze the\ndynamics of transformers using feature learning techniques under in-context\nlearning regimes, based on a disentangled two-type feature structure. Such\ndisentanglement of feature structure is general in practice, e.g., natural\nlanguages contain syntax and semantics, and proteins contain primary and\nsecondary structures. To our best known, this is the first rigorous result\nregarding a two-stage optimization process in transformers. Additionally, a\ncorollary indicates that such a two-stage process is closely related to the\nspectral properties of the attention weights, which accords well with empirical\nfindings.", "published": "2025-02-28 03:27:24", "link": "http://arxiv.org/abs/2502.20681v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JAM: Controllable and Responsible Text Generation via Causal Reasoning\n  and Latent Vector Manipulation", "abstract": "While large language models (LLMs) have made significant strides in\ngenerating coherent and contextually relevant text, they often function as\nopaque black boxes, trained on vast unlabeled datasets with statistical\nobjectives, lacking an interpretable framework for responsible control. In this\npaper, we introduce JAM (Just A Move), a novel framework that interprets and\ncontrols text generation by integrating cause-effect analysis within the latent\nspace of LLMs. Based on our observations, we uncover the inherent causality in\nLLM generation, which is critical for producing responsible and realistic\noutputs. Moreover, we explore latent vectors as fundamental components in LLM\narchitectures, aiming to understand and manipulate them for more effective and\nefficient controllable text generation. We evaluate our framework using a range\nof tools, including the HHH criteria, toxicity reduction benchmarks, and GPT-4\nalignment measures. Our results show that JAM achieves up to a 22% improvement\nover previous Controllable Text Generation (CTG) methods across multiple\nquantitative metrics and human-centric evaluations. Furthermore, JAM\ndemonstrates greater computational efficiency compared to other CTG methods.\nThese results highlight the effectiveness and efficiency of JAM for responsible\nand realistic text generation, paving the way for more interpretable and\ncontrollable models.", "published": "2025-02-28 03:31:48", "link": "http://arxiv.org/abs/2502.20684v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T30, 68T35, 68T37, 68T50", "I.2.0; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Structured Preference Optimization for Vision-Language Long-Horizon Task\n  Planning", "abstract": "Existing methods for vision-language task planning excel in short-horizon\ntasks but often fall short in complex, long-horizon planning within dynamic\nenvironments. These challenges primarily arise from the difficulty of\neffectively training models to produce high-quality reasoning processes for\nlong-horizon tasks. To address this, we propose Structured Preference\nOptimization (SPO), which aims to enhance reasoning and action selection in\nlong-horizon task planning through structured preference evaluation and\noptimized training strategies. Specifically, SPO introduces: 1)\nPreference-Based Scoring and Optimization, which systematically evaluates\nreasoning chains based on task relevance, visual grounding, and historical\nconsistency; and 2) Curriculum-Guided Training, where the model progressively\nadapts from simple to complex tasks, improving its generalization ability in\nlong-horizon scenarios and enhancing reasoning robustness. To advance research\nin vision-language long-horizon task planning, we introduce ExtendaBench, a\ncomprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat\n2.0, categorized into ultra-short, short, medium, and long tasks. Experimental\nresults demonstrate that SPO significantly improves reasoning quality and final\ndecision accuracy, outperforming prior methods on long-horizon tasks and\nunderscoring the effectiveness of preference-driven optimization in\nvision-language task planning. Specifically, SPO achieves a +5.98% GCR and\n+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement\nin Habitat over the best-performing baselines.", "published": "2025-02-28 05:47:34", "link": "http://arxiv.org/abs/2502.20742v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Acquiring Grounded Representations of Words with Situated Interactive\n  Instruction", "abstract": "We present an approach for acquiring grounded representations of words from\nmixed-initiative, situated interactions with a human instructor. The work\nfocuses on the acquisition of diverse types of knowledge including perceptual,\nsemantic, and procedural knowledge along with learning grounded meanings.\nInteractive learning allows the agent to control its learning by requesting\ninstructions about unknown concepts, making learning efficient. Our approach\nhas been instantiated in Soar and has been evaluated on a table-top robotic arm\ncapable of manipulating small objects.", "published": "2025-02-28 06:04:52", "link": "http://arxiv.org/abs/2502.20754v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Collective Reasoning Among LLMs A Framework for Answer Validation\n  Without Ground Truth", "abstract": "We present a collaborative framework where multiple large language models,\nnamely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and\nGemini-1.5-Flash, work together to generate and respond to complex PhD-level\nprobability questions in the absence of definitive ground truth. This study\nexplores how inter-model consensus enhances response reliability and serves as\na proxy for assessing the quality of generated questions. To quantify agreement\nand consistency, we employ statistical methods including chi-square tests,\nFleiss' Kappa, and confidence interval analysis, measuring both response\nprecision and question clarity. Our findings highlight that Claude and Gemini\ngenerate well-structured and less ambiguous questions, leading to higher\ninter-model agreement. This is reflected in their narrower confidence intervals\nand stronger alignment with answering models. Conversely, LLaMA demonstrates\nincreased variability and lower reliability in question formulation, as\nindicated by broader confidence intervals and reduced consensus rates. These\nresults suggest that multi-model collaboration not only enhances the\nreliability of responses but also provides a valuable framework for assessing\nand improving question quality in the absence of explicit ground truth. This\nresearch offers meaningful insights into optimizing AI-driven reasoning through\ncollaborative large-language model interactions.", "published": "2025-02-28 06:20:52", "link": "http://arxiv.org/abs/2502.20758v1", "categories": ["stat.AP", "cs.AI", "cs.CL"], "primary_category": "stat.AP"}
{"title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large\n  Language Models from a Neuroscience Perspective", "abstract": "Large language models (LLMs) often exhibit abrupt emergent behavior, whereby\nnew abilities arise at certain points during their training. This phenomenon,\ncommonly referred to as a ''phase transition'', remains poorly understood. In\nthis study, we conduct an integrative analysis of such phase transitions by\nexamining three interconnected perspectives: the similarity between LLMs and\nthe human brain, the internal states of LLMs, and downstream task performance.\nWe propose a novel interpretation for the learning dynamics of LLMs that vary\nin both training data and architecture, revealing that three phase transitions\ncommonly emerge across these models during training: (1) alignment with the\nentire brain surges as LLMs begin adhering to task instructions Brain Alignment\nand Instruction Following, (2) unexpectedly, LLMs diverge from the brain during\na period in which downstream task accuracy temporarily stagnates Brain\nDetachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs\nbecome capable of solving the downstream tasks Brain Realignment and\nConsolidation. These findings illuminate the underlying mechanisms of phase\ntransitions in LLMs, while opening new avenues for interdisciplinary research\nbridging AI and neuroscience.", "published": "2025-02-28 06:59:04", "link": "http://arxiv.org/abs/2502.20779v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical\n  Hallucination in Vision-Language Models", "abstract": "The increasing use of vision-language models (VLMs) in healthcare\napplications presents great challenges related to hallucinations, in which the\nmodels may generate seemingly plausible results that are in fact incorrect.\nSuch hallucinations can jeopardize clinical decision making, potentially\nharming the diagnosis and treatments. In this work, we propose MedHallTune, a\nlarge-scale benchmark designed specifically to evaluate and mitigate\nhallucinations in medical VLMs. Comprising over 100,000 images and 1,000,000\ninstruction pairs, MedHallTune includes both hallucination and\nnon-hallucination samples, each with ground-truth annotations. We conduct a\ncomprehensive evaluation of current medical and general VLMs using MedHallTune,\nassessing their performance across key metrics, including clinical accuracy,\nrelevance, detail level, and risk level. The experimental results show that\nfine-tuning with MedHallTune successfully improves the ability of several\nexisting models to manage hallucinations and boost their zero-shot performance\non downstream visual-question-answering (VQA) tasks, making them more reliable\nfor practical medical applications. Our work contributes to the development of\nmore trustworthy VLMs. Codes and dataset will be available at\n\\href{https://github.com/russellyq/MedHallTune}{MedHallTune}.", "published": "2025-02-28 06:59:49", "link": "http://arxiv.org/abs/2502.20780v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "HAIC: Improving Human Action Understanding and Generation with Better\n  Captions for Multi-modal Large Language Models", "abstract": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in\nvideo understanding. However, their performance on videos involving human\nactions is still limited by the lack of high-quality data. To address this, we\nintroduce a two-stage data annotation pipeline. First, we design strategies to\naccumulate videos featuring clear human actions from the Internet. Second,\nvideos are annotated in a standardized caption format that uses human\nattributes to distinguish individuals and chronologically details their actions\nand interactions. Through this pipeline, we curate two datasets, namely\nHAICTrain and HAICBench. \\textbf{HAICTrain} comprises 126K video-caption pairs\ngenerated by Gemini-Pro and verified for training purposes. Meanwhile,\n\\textbf{HAICBench} includes 500 manually annotated video-caption pairs and\n1,400 QA pairs, for a comprehensive evaluation of human action understanding.\nExperimental results demonstrate that training with HAICTrain not only\nsignificantly enhances human understanding abilities across 4 benchmarks, but\ncan also improve text-to-video generation results. Both the HAICTrain and\nHAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.", "published": "2025-02-28 07:53:40", "link": "http://arxiv.org/abs/2502.20811v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability\n  Identifiable?", "abstract": "As AI systems are used in high-stakes applications, ensuring interpretability\nis crucial. Mechanistic Interpretability (MI) aims to reverse-engineer neural\nnetworks by extracting human-understandable algorithms to explain their\nbehavior. This work examines a key question: for a given behavior, and under\nMI's criteria, does a unique explanation exist? Drawing on identifiability in\nstatistics, where parameters are uniquely inferred under specific assumptions,\nwe explore the identifiability of MI explanations.\n  We identify two main MI strategies: (1) \"where-then-what,\" which isolates a\ncircuit replicating model behavior before interpreting it, and (2)\n\"what-then-where,\" which starts with candidate algorithms and searches for\nneural activation subspaces implementing them, using causal alignment.\n  We test both strategies on Boolean functions and small multi-layer\nperceptrons, fully enumerating candidate explanations. Our experiments reveal\nsystematic non-identifiability: multiple circuits can replicate behavior, a\ncircuit can have multiple interpretations, several algorithms can align with\nthe network, and one algorithm can align with different subspaces.\n  Is uniqueness necessary? A pragmatic approach may require only predictive and\nmanipulability standards. If uniqueness is essential for understanding,\nstricter criteria may be needed. We also reference the inner interpretability\nframework, which validates explanations through multiple criteria. This work\ncontributes to defining explanation standards in AI.", "published": "2025-02-28 10:13:54", "link": "http://arxiv.org/abs/2502.20914v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense\n  Retrieval", "abstract": "We present WebFAQ, a large-scale collection of open-domain question answering\ndatasets derived from FAQ-style schema.org annotations. In total, the data\ncollection consists of 96 million natural question-answer (QA) pairs across 75\nlanguages, including 47 million (49%) non-English samples. WebFAQ further\nserves as the foundation for 20 monolingual retrieval benchmarks with a total\nsize of 11.2 million QA pairs (5.9 million non-English). These datasets are\ncarefully curated through refined filtering and near-duplicate detection,\nyielding high-quality resources for training and evaluating multilingual dense\nretrieval models. To empirically confirm WebFAQ's efficacy, we use the\ncollected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through\nthis process of dataset-specific fine-tuning, the model achieves significant\nretrieval performance gains, which generalize - beyond WebFAQ - to other\nmultilingual retrieval benchmarks evaluated in zero-shot setting. Last but not\nleast, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora\nspanning over 1000 language pairs using state-of-the-art bitext mining and\nautomated LLM-assessed translation evaluation. Due to our advanced, automated\nmethod of bitext dataset generation, the resulting bilingual corpora\ndemonstrate higher translation quality compared to similar datasets. WebFAQ and\nall associated resources are publicly available on GitHub and HuggingFace.", "published": "2025-02-28 10:46:52", "link": "http://arxiv.org/abs/2502.20936v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Transforming Tuberculosis Care: Optimizing Large Language Models For\n  Enhanced Clinician-Patient Communication", "abstract": "Tuberculosis (TB) is the leading cause of death from an infectious disease\nglobally, with the highest burden in low- and middle-income countries. In these\nregions, limited healthcare access and high patient-to-provider ratios impede\neffective patient support, communication, and treatment completion. To bridge\nthis gap, we propose integrating a specialized Large Language Model into an\nefficacious digital adherence technology to augment interactive communication\nwith treatment supporters. This AI-powered approach, operating within a\nhuman-in-the-loop framework, aims to enhance patient engagement and improve TB\ntreatment outcomes.", "published": "2025-02-28 17:05:13", "link": "http://arxiv.org/abs/2502.21236v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "RuCCoD: Towards Automated ICD Coding in Russian", "abstract": "This study investigates the feasibility of automating clinical coding in\nRussian, a language with limited biomedical resources. We present a new dataset\nfor ICD coding, which includes diagnosis fields from electronic health records\n(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD\ncodes. This dataset serves as a benchmark for several state-of-the-art models,\nincluding BERT, LLaMA with LoRA, and RAG, with additional experiments examining\ntransfer learning across domains (from PubMed abstracts to medical diagnosis)\nand terminologies (from UMLS concepts to ICD codes). We then apply the\nbest-performing model to label an in-house EHR dataset containing patient\nhistories from 2017 to 2021. Our experiments, conducted on a carefully curated\ntest set, demonstrate that training with the automated predicted codes leads to\na significant improvement in accuracy compared to manually annotated data from\nphysicians. We believe our findings offer valuable insights into the potential\nfor automating clinical coding in resource-limited languages like Russian,\nwhich could enhance clinical efficiency and data accuracy in these contexts.", "published": "2025-02-28 17:40:24", "link": "http://arxiv.org/abs/2502.21263v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "FANformer: Improving Large Language Models Through Effective Periodicity\n  Modeling", "abstract": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)\ninto attention mechanism to achieve efficient periodicity modeling, by\nmodifying the feature projection process of attention mechanism. Extensive\nexperimental results on language modeling show that FANformer consistently\noutperforms Transformer when scaling up model size and training tokens,\nunderscoring its superior learning efficiency. To further validate the\neffectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.\nFANformer-1B exhibits marked improvements on downstream tasks compared to\nopen-source LLMs with similar model parameters or training tokens. The results\nposition FANformer as an effective and promising architecture for advancing\nLLMs.", "published": "2025-02-28 18:52:24", "link": "http://arxiv.org/abs/2502.21309v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InspireMusic: Integrating Super Resolution and Large Language Model for\n  High-Fidelity Long-Form Music Generation", "abstract": "We introduce InspireMusic, a framework integrated super resolution and large\nlanguage model for high-fidelity long-form music generation. A unified\nframework generates high-fidelity music, songs, and audio, which incorporates\nan autoregressive transformer with a super-resolution flow-matching model. This\nframework enables the controllable generation of high-fidelity long-form music\nat a higher sampling rate from both text and audio prompts. Our model differs\nfrom previous approaches, as we utilize an audio tokenizer with one codebook\nthat contains richer semantic information, thereby reducing training costs and\nenhancing efficiency. This combination enables us to achieve high-quality audio\ngeneration with long-form coherence of up to $8$ minutes. Then, an\nautoregressive transformer model based on Qwen 2.5 predicts audio tokens. Next,\nwe employ a super-resolution flow-matching model to generate high-sampling rate\naudio with fine-grained details learned from an acoustic codec model.\nComprehensive experiments show that the InspireMusic-1.5B-Long model has a\ncomparable performance to recent top-tier open-source systems, including\nMusicGen and Stable Audio 2.0, on subjective and objective evaluations. The\ncode and pre-trained models are released at\nhttps://github.com/FunAudioLLM/InspireMusic.", "published": "2025-02-28 09:58:25", "link": "http://arxiv.org/abs/2503.00084v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rethinking LLM Bias Probing Using Lessons from the Social Sciences", "abstract": "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.", "published": "2025-02-28 16:53:18", "link": "http://arxiv.org/abs/2503.00093v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Evaluation of LLMs-based Hidden States as Author Representations for\n  Psychological Human-Centered NLP Tasks", "abstract": "Like most of NLP, models for human-centered NLP tasks -- tasks attempting to\nassess author-level information -- predominantly use representations derived\nfrom hidden states of Transformer-based LLMs. However, what component of the LM\nis used for the representation varies widely. Moreover, there is a need for\nHuman Language Models (HuLMs) that implicitly model the author and provide a\nuser-level hidden state. Here, we systematically evaluate different ways of\nrepresenting documents and users using different LM and HuLM architectures to\npredict task outcomes as both dynamically changing states and averaged\ntrait-like user-level attributes of valence, arousal, empathy, and distress. We\nfind that representing documents as an average of the token hidden states\nperforms the best generally. Further, while a user-level hidden state itself is\nrarely the best representation, we find its inclusion in the model strengthens\ntoken or document embeddings used to derive document- and user-level\nrepresentations resulting in best performances.", "published": "2025-02-28 19:10:06", "link": "http://arxiv.org/abs/2503.00124v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PreMind: Multi-Agent Video Understanding for Advanced Indexing of\n  Presentation-style Videos", "abstract": "In recent years, online lecture videos have become an increasingly popular\nresource for acquiring new knowledge. Systems capable of effectively\nunderstanding/indexing lecture videos are thus highly desirable, enabling\ndownstream tasks like question answering to help users efficiently locate\nspecific information within videos. This work proposes PreMind, a novel\nmulti-agent multimodal framework that leverages various large models for\nadvanced understanding/indexing of presentation-style videos. PreMind first\nsegments videos into slide-presentation segments using a Vision-Language Model\n(VLM) to enhance modern shot-detection techniques. Each segment is then\nanalyzed to generate multimodal indexes through three key steps: (1) extracting\nslide visual content, (2) transcribing speech narratives, and (3) consolidating\nthese visual and speech contents into an integrated understanding. Three\ninnovative mechanisms are also proposed to improve performance: leveraging\nprior lecture knowledge to refine visual understanding, detecting/correcting\nspeech transcription errors using a VLM, and utilizing a critic agent for\ndynamic iterative self-reflection in vision analysis. Compared to traditional\nvideo indexing methods, PreMind captures rich, reliable multimodal information,\nallowing users to search for details like abbreviations shown only on slides.\nSystematic evaluations on the public LPM dataset and an internal enterprise\ndataset are conducted to validate PreMind's effectiveness, supported by\ndetailed analyses.", "published": "2025-02-28 20:17:48", "link": "http://arxiv.org/abs/2503.00162v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.CV"}
{"title": "Zero-Shot and Efficient Clarification Need Prediction in Conversational\n  Search", "abstract": "Clarification need prediction (CNP) is a key task in conversational search,\naiming to predict whether to ask a clarifying question or give an answer to the\ncurrent user query. However, current research on CNP suffers from the issues of\nlimited CNP training data and low efficiency. In this paper, we propose a\nzero-shot and efficient CNP framework (Zef-CNP), in which we first prompt large\nlanguage models (LLMs) in a zero-shot manner to generate two sets of synthetic\nqueries: ambiguous and specific (unambiguous) queries. We then use the\ngenerated queries to train efficient CNP models. Zef-CNP eliminates the need\nfor human-annotated clarification-need labels during training and avoids the\nuse of LLMs with high query latency at query time. To further improve the\ngeneration quality of synthetic queries, we devise a topic-, information-need-,\nand query-aware chain-of-thought (CoT) prompting strategy (TIQ-CoT). Moreover,\nwe enhance TIQ-CoT with counterfactual query generation (CoQu), which guides\nLLMs first to generate a specific/ambiguous query and then sequentially\ngenerate its corresponding ambiguous/specific query. Experimental results show\nthat Zef-CNP achieves superior CNP effectiveness and efficiency compared with\nzero- and few-shot LLM-based CNP predictors.", "published": "2025-02-28 20:49:18", "link": "http://arxiv.org/abs/2503.00179v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Steering Dialogue Dynamics for Robustness against Multi-turn\n  Jailbreaking Attacks", "abstract": "Large language models (LLMs) are highly vulnerable to jailbreaking attacks,\nwherein adversarial prompts are designed to elicit harmful responses. While\nexisting defenses effectively mitigate single-turn attacks by detecting and\nfiltering unsafe inputs, they fail against multi-turn jailbreaks that exploit\ncontextual drift over multiple interactions, gradually leading LLMs away from\nsafe behavior. To address this challenge, we propose a safety steering\nframework grounded in safe control theory, ensuring invariant safety in\nmulti-turn dialogues. Our approach models the dialogue with LLMs using\nstate-space representations and introduces a novel neural barrier function\n(NBF) to detect and filter harmful queries emerging from evolving contexts\nproactively. Our method achieves invariant safety at each turn of dialogue by\nlearning a safety predictor that accounts for adversarial queries, preventing\npotential context drift toward jailbreaks. Extensive experiments under multiple\nLLMs show that our NBF-based safety steering outperforms safety alignment\nbaselines, offering stronger defenses against multi-turn jailbreaks while\nmaintaining a better trade-off between safety and helpfulness under different\nmulti-turn jailbreak methods. Our code is available at\nhttps://github.com/HanjiangHu/NBF-LLM .", "published": "2025-02-28 21:10:03", "link": "http://arxiv.org/abs/2503.00187v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PsychBench: A comprehensive and professional benchmark for evaluating\n  the performance of LLM-assisted psychiatric clinical practice", "abstract": "The advent of Large Language Models (LLMs) offers potential solutions to\naddress problems such as shortage of medical resources and low diagnostic\nconsistency in psychiatric clinical practice. Despite this potential, a robust\nand comprehensive benchmarking framework to assess the efficacy of LLMs in\nauthentic psychiatric clinical environments is absent. This has impeded the\nadvancement of specialized LLMs tailored to psychiatric applications. In\nresponse to this gap, by incorporating clinical demands in psychiatry and\nclinical data, we proposed a benchmarking system, PsychBench, to evaluate the\npractical performance of LLMs in psychiatric clinical settings. We conducted a\ncomprehensive quantitative evaluation of 16 LLMs using PsychBench, and\ninvestigated the impact of prompt design, chain-of-thought reasoning, input\ntext length, and domain-specific knowledge fine-tuning on model performance.\nThrough detailed error analysis, we identified strengths and potential\nlimitations of the existing models and suggested directions for improvement.\nSubsequently, a clinical reader study involving 60 psychiatrists of varying\nseniority was conducted to further explore the practical benefits of existing\nLLMs as supportive tools for psychiatrists of varying seniority. Through the\nquantitative and reader evaluation, we show that while existing models\ndemonstrate significant potential, they are not yet adequate as decision-making\ntools in psychiatric clinical practice. The reader study further indicates\nthat, as an auxiliary tool, LLM could provide particularly notable support for\njunior psychiatrists, effectively enhancing their work efficiency and overall\nclinical quality. To promote research in this area, we will make the dataset\nand evaluation framework publicly available, with the hope of advancing the\napplication of LLMs in psychiatric clinical settings.", "published": "2025-02-28 12:17:41", "link": "http://arxiv.org/abs/2503.01903v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "AgroLLM: Connecting Farmers and Agricultural Practices through Large\n  Language Models for Enhanced Knowledge Transfer and Practical Application", "abstract": "AgroLLM is an AI-powered chatbot designed to enhance knowledge-sharing and\neducation in agriculture using Large Language Models (LLMs) and a\nRetrieval-Augmented Generation (RAG) framework. By using a comprehensive\nopen-source agricultural database, AgroLLM provides accurate, contextually\nrelevant responses while reducing incorrect information retrieval. The system\nutilizes the FAISS vector database for efficient similarity searches, ensuring\nrapid access to agricultural knowledge. A comparative study of three advanced\nmodels: Gemini 1.5 Flash, ChatGPT-4o Mini, and Mistral-7B-Instruct-v0.2 was\nconducted to evaluate performance across four key agricultural domains:\nAgriculture and Life Sciences, Agricultural Management, Agriculture and\nForestry, and Agriculture Business. Key evaluation metrics included embedding\nquality, search efficiency, and response relevance. Results indicated that\nChatGPT-4o Mini with RAG achieved the highest accuracy at 93%. Continuous\nfeedback mechanisms enhance response quality, making AgroLLM a benchmark\nAI-driven educational tool for farmers, researchers, and professionals,\npromoting informed decision-making and improved agricultural practices.", "published": "2025-02-28 04:13:18", "link": "http://arxiv.org/abs/2503.04788v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emergent Abilities in Large Language Models: A Survey", "abstract": "Large Language Models (LLMs) are leading a new technological revolution as\none of the most promising research streams toward artificial general\nintelligence. The scaling of these models, accomplished by increasing the\nnumber of parameters and the magnitude of the training datasets, has been\nlinked to various so-called emergent abilities that were previously unobserved.\nThese emergent abilities, ranging from advanced reasoning and in-context\nlearning to coding and problem-solving, have sparked an intense scientific\ndebate: Are they truly emergent, or do they simply depend on external factors,\nsuch as training dynamics, the type of problems, or the chosen metric? What\nunderlying mechanism causes them? Despite their transformative potential,\nemergent abilities remain poorly understood, leading to misconceptions about\ntheir definition, nature, predictability, and implications. In this work, we\nshed light on emergent abilities by conducting a comprehensive review of the\nphenomenon, addressing both its scientific underpinnings and real-world\nconsequences. We first critically analyze existing definitions, exposing\ninconsistencies in conceptualizing emergent abilities. We then explore the\nconditions under which these abilities appear, evaluating the role of scaling\nlaws, task complexity, pre-training loss, quantization, and prompting\nstrategies. Our review extends beyond traditional LLMs and includes Large\nReasoning Models (LRMs), which leverage reinforcement learning and\ninference-time search to amplify reasoning and self-reflection. However,\nemergence is not inherently positive. As AI systems gain autonomous reasoning\ncapabilities, they also develop harmful behaviors, including deception,\nmanipulation, and reward hacking. We highlight growing concerns about safety\nand governance, emphasizing the need for better evaluation frameworks and\nregulatory oversight.", "published": "2025-02-28 01:20:01", "link": "http://arxiv.org/abs/2503.05788v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hybrid Retrieval for Hallucination Mitigation in Large Language Models:\n  A Comparative Analysis", "abstract": "Large Language Models (LLMs) excel in language comprehension and generation\nbut are prone to hallucinations, producing factually incorrect or unsupported\noutputs. Retrieval Augmented Generation (RAG) systems address this issue by\ngrounding LLM responses with external knowledge. This study evaluates the\nrelationship between retriever effectiveness and hallucination reduction in\nLLMs using three retrieval approaches: sparse retrieval based on BM25 keyword\nsearch, dense retrieval using semantic search with Sentence Transformers, and a\nproposed hybrid retrieval module. The hybrid module incorporates query\nexpansion and combines the results of sparse and dense retrievers through a\ndynamically weighted Reciprocal Rank Fusion score. Using the HaluBench dataset,\na benchmark for hallucinations in question answering tasks, we assess retrieval\nperformance with metrics such as mean average precision and normalised\ndiscounted cumulative gain, focusing on the relevance of the top three\nretrieved documents. Results show that the hybrid retriever achieves better\nrelevance scores, outperforming both sparse and dense retrievers. Further\nevaluation of LLM-generated answers against ground truth using metrics such as\naccuracy, hallucination rate, and rejection rate reveals that the hybrid\nretriever achieves the highest accuracy on fails, the lowest hallucination\nrate, and the lowest rejection rate. These findings highlight the hybrid\nretriever's ability to enhance retrieval relevance, reduce hallucination rates,\nand improve LLM reliability, emphasising the importance of advanced retrieval\ntechniques in mitigating hallucinations and improving response accuracy.", "published": "2025-02-28 10:13:33", "link": "http://arxiv.org/abs/2504.05324v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Optimizing Large Language Models for ESG Activity Detection in Financial\n  Texts", "abstract": "The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques.", "published": "2025-02-28 14:52:25", "link": "http://arxiv.org/abs/2502.21112v1", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.AI"}
{"title": "JiTTER: Jigsaw Temporal Transformer for Event Reconstruction for\n  Self-Supervised Sound Event Detection", "abstract": "Sound event detection (SED) has significantly benefited from self-supervised\nlearning (SSL) approaches, particularly masked audio transformer for SED\n(MAT-SED), which leverages masked block prediction to reconstruct missing audio\nsegments. However, while effective in capturing global dependencies, masked\nblock prediction disrupts transient sound events and lacks explicit enforcement\nof temporal order, making it less suitable for fine-grained event boundary\ndetection. To address these limitations, we propose JiTTER (Jigsaw Temporal\nTransformer for Event Reconstruction), an SSL framework designed to enhance\ntemporal modeling in transformer-based SED. JiTTER introduces a hierarchical\ntemporal shuffle reconstruction strategy, where audio sequences are randomly\nshuffled at both the block-level and frame-level, forcing the model to\nreconstruct the correct temporal order. This pretraining objective encourages\nthe model to learn both global event structures and fine-grained transient\ndetails, improving its ability to detect events with sharp onset-offset\ncharacteristics. Additionally, we incorporate noise injection during block\nshuffle, providing a subtle perturbation mechanism that further regularizes\nfeature learning and enhances model robustness. Experimental results on the\nDESED dataset demonstrate that JiTTER outperforms MAT-SED, achieving a 5.89%\nimprovement in PSDS, highlighting the effectiveness of explicit temporal\nreasoning in SSL-based SED. Our findings suggest that structured temporal\nreconstruction tasks, rather than simple masked prediction, offer a more\neffective pretraining paradigm for sound event representation learning.", "published": "2025-02-28 08:55:20", "link": "http://arxiv.org/abs/2502.20857v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Weakly Supervised Multiple Instance Learning for Whale Call Detection\n  and Localization in Long-Duration Passive Acoustic Monitoring", "abstract": "Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates\nvast data, but deep learning often requires precise annotations and short\nsegments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for\nwhale call detection and localization using only bag-level labels. Our\ndual-stream model processes 2-30 minute audio segments, leveraging spectral and\ntemporal features with attention-based instance selection. Tests on Antarctic\nwhale data show longer contexts improve classification (F1: 0.8-0.9) while\nmedium instances ensure localization precision (0.65-0.70). This suggests MIL\ncan enhance scalable marine monitoring. Code:\nhttps://github.com/Ragib-Amin-Nihal/DSMIL-Loc", "published": "2025-02-28 08:34:12", "link": "http://arxiv.org/abs/2502.20838v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep learning-based filtering of cross-spectral matrices using\n  generative adversarial networks", "abstract": "In this paper, we present a deep-learning method to filter out effects such\nas ambient noise, reflections, or source directivity from microphone array data\nrepresented as cross-spectral matrices. Specifically, we focus on a generative\nadversarial network (GAN) architecture designed to transform fixed-size\ncross-spectral matrices. Theses models were trained using sound pressure\nsimulations of varying complexity developed for this purpose. Based on the\nresults from applying these methods in a hyperparameter optimization of an\nauto-encoding task, we trained the optimized model to perform five distinct\ntransformation tasks derived from different complexities inherent in our sound\npressure simulations.", "published": "2025-02-28 14:34:43", "link": "http://arxiv.org/abs/2502.21097v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
