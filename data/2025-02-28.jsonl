{"title": "A Minor-Testing Approach for Coordinated Motion Planning with Sliding Robots", "abstract": "We study a variant of the Coordinated Motion Planning problem on undirected\ngraphs, referred to herein as the \\textsc{Coordinated Sliding-Motion Planning}\n(CSMP) problem. In this variant, we are given an undirected graph $G$, $k$\nrobots $R_1,\\dots,R_k$ positioned on distinct vertices of $G$, $p\\leq k$\ndistinct destination vertices for robots $R_1,\\dots,R_p$, and $\\ell \\in\n\\mathbb{N}$. The problem is to decide if there is a serial schedule of at most\n$\\ell$ moves (i.e., of makespan $\\ell$) such that at the end of the schedule\neach robot with a destination reaches it, where a robot's move is a free path\n(unoccupied by any robots) from its current position to an unoccupied vertex.\nThe problem is known to be NP-hard even on full grids. It has been studied in\nseveral contexts, including coin movement and reconfiguration problems, with\nrespect to feasibility, complexity, and approximation. Geometric variants of\nthe problem, in which congruent geometric-shape robots (e.g., unit\ndisk/squares) slide or translate in the Euclidean plane, have also been studied\nextensively. We investigate the parameterized complexity of CSMP with respect\nto two parameters: the number $k$ of robots and the makespan $\\ell$. As our\nfirst result, we present a fixed-parameter algorithm for CSMP parameterized by\n$k$. For our second result, we present a fixed-parameter algorithm\nparameterized by $\\ell$ for the special case of CSMP in which only a single\nrobot has a destination and the graph is planar, which we prove to be\nNP-complete. A crucial new ingredient for both of our results is that the\nsolution admits a succinct representation as a small labeled topological minor\nof the input graph.", "published": "2025-02-28 15:56:42", "link": "http://arxiv.org/abs/2502.21175v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "Polynomial-Size Enumeration Kernelizations for Long Path Enumeration", "abstract": "Enumeration kernelization for parameterized enumeration problems was defined\nby Creignou et al. [Theory Comput. Syst. 2017] and was later refined by\nGolovach et al. [J. Comput. Syst. Sci. 2022, STACS 2021] to polynomial-delay\nenumeration kernelization. We consider ENUM LONG-PATH, the enumeration variant\nof the Long-Path problem, from the perspective of enumeration kernelization.\nFormally, given an undirected graph G and an integer k, the objective of ENUM\nLONG-PATH is to enumerate all paths of G having exactly k vertices. We consider\nthe structural parameters vertex cover number, dissociation number, and\ndistance to clique and provide polynomial-delay enumeration kernels of\npolynomial size for each of these parameters.", "published": "2025-02-28 15:48:14", "link": "http://arxiv.org/abs/2502.21164v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "4-tangrams are 4-avoidable", "abstract": "A tangram is a word in which every letter occurs an even number of times.\nThus it can be cut into parts that can be arranged into two identical words.\nThe \\emph{cut number} of a tangram is the minimum number of required cuts in\nthis process. Tangrams with cut number one corresponds to squares. For $k\\ge1$,\nlet $t(k)$ denote the minimum size of an alphabet over which an infinite word\navoids tangrams with cut number at most~$k$. The existence of infinite ternary\nsquare-free words shows that $t(1)=t(2)=3$. We show that $t(3)=t(4)=4$,\nanswering a question from D\\k{e}bski, Grytczuk, Pawlik, Przyby\\l{}o, and\n\\'Sleszy\\'nska-Nowak.", "published": "2025-02-28 06:48:13", "link": "http://arxiv.org/abs/2502.20774v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Using quantile time series and historical simulation to forecast financial risk multiple steps ahead", "abstract": "A method for quantile-based, semi-parametric historical simulation estimation\nof multiple step ahead Value-at-Risk (VaR) and Expected Shortfall (ES) models\nis developed. It uses the quantile loss function, analogous to how the\nquasi-likelihood is employed by standard historical simulation methods. The\nreturns data are scaled by the estimated quantile series, then resampling is\nemployed to estimate the forecast distribution one and multiple steps ahead,\nallowing tail risk forecasting. The proposed method is applicable to any data\nor model where the relationship between VaR and ES does not change over time\nand can be extended to allow a measurement equation incorporating realized\nmeasures, thus including Realized GARCH and Realized CAViaR type models. Its\nfinite sample properties, and its comparison with existing historical\nsimulation methods, are evaluated via a simulation study. A forecasting study\nassesses the relative accuracy of the 1% and 2.5% VaR and ES one-day-ahead and\nten-day-ahead forecasting results for the proposed class of models compared to\nseveral competitors.", "published": "2025-02-28 11:48:35", "link": "http://arxiv.org/abs/2502.20978v2", "categories": ["q-fin.ST", "q-fin.CP", "q-fin.RM"], "primary_category": "q-fin.ST"}
{"title": "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced Finite Difference Estimators", "abstract": "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.", "published": "2025-02-28 08:05:54", "link": "http://arxiv.org/abs/2502.20819v1", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "q-fin.CP", "stat.ML", "90-05", "I.6.1; I.6.6"], "primary_category": "math.OC"}
{"title": "Strong Solutions and Quantization-Based Numerical Schemes for a Class of Non-Markovian Volatility Models", "abstract": "We investigate a class of non-Markovian processes that hold particular\nrelevance in the realm of mathematical finance. This family encompasses\npath-dependent volatility models, including those pioneered by [Platen and\nRendek, 2018] and, more recently, by [Guyon and Lekeufack, 2023], as well as an\nextension of the framework proposed by [Blanc et al., 2017]. Our study unfolds\nin two principal phases. In the first phase, we introduce a functional\nquantization scheme based on an extended version of the Lamperti transformation\nthat we propose to handle the presence of a memory term incorporated into the\ndiffusion coefficient. For scenarios involving a Brownian integral in the\ndiffusion term, we propose alternative numerical schemes that leverage the\npower of marginal recursive quantization. In the second phase, we study the\nproblem of existence and uniqueness of a strong solution for the SDEs related\nto the examples that motivate our study, in order to provide a theoretical\nbasis to correctly apply the proposed numerical schemes.", "published": "2025-02-28 23:21:44", "link": "http://arxiv.org/abs/2503.00243v1", "categories": ["q-fin.MF", "60F10, 91G99, 91B25"], "primary_category": "q-fin.MF"}
{"title": "Short-Rate Derivatives in a Higher-for-Longer Environment", "abstract": "We introduce a class of short-rate models that exhibit a ``higher for\nlonger'' phenomenon. Specifically, the short-rate is modeled as a general\ntime-homogeneous one-factor Markov diffusion on a finite interval. The lower\nendpoint is assumed to be regular, exit or natural according to boundary\nclassification while the upper endpoint is assumed to be regular with absorbing\nbehavior. In this setting, we give an explicit expression for price of a\nzero-coupon bond (as well as more general interest rate derivatives) in terms\nof the transition density of the short-rate under a new probability measure,\nand the solution of a non-linear ordinary differential equation (ODE). We then\nnarrow our focus to a class of models for which the transition density and ODE\ncan be solved explicitly. For models within this class, we provide conditions\nunder which the lower endpoint is regular, exit and natural. Finally, we study\ntwo specific models -- one in which the lower endpoint is exit and another in\nwhich the lower endpoint is natural. In these two models, we give an explicit\nsolution of transition density of the short-rate as a (generalized)\neigenfunction expansion. We provide plots of the transition density,\n(generalized) eigenfunctions, bond prices and the associated yield curve.", "published": "2025-02-28 17:27:51", "link": "http://arxiv.org/abs/2502.21252v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Chronologically Consistent Large Language Models", "abstract": "Large language models are increasingly used in social sciences, but their\ntraining data can introduce lookahead bias and training leakage. A good\nchronologically consistent language model requires efficient use of training\ndata to maintain accuracy despite time-restricted data. Here, we overcome this\nchallenge by training a suite of chronologically consistent large language\nmodels, ChronoBERT and ChronoGPT, which incorporate only the text data that\nwould have been available at each point in time. Despite this strict temporal\nconstraint, our models achieve strong performance on natural language\nprocessing benchmarks, outperforming or matching widely used models (e.g.,\nBERT), and remain competitive with larger open-weight models. Lookahead bias is\nmodel and application-specific because even if a chronologically consistent\nlanguage model has poorer language comprehension, a regression or prediction\nmodel applied on top of the language model can compensate. In an asset pricing\napplication predicting next-day stock returns from financial news, we find that\nChronoBERT's real-time outputs achieve a Sharpe ratio comparable to\nstate-of-the-art models, indicating that lookahead bias is modest. Our results\ndemonstrate a scalable, practical framework to mitigate training leakage,\nensuring more credible backtests and predictions across finance and other\nsocial science domains.", "published": "2025-02-28 16:25:50", "link": "http://arxiv.org/abs/2502.21206v2", "categories": ["q-fin.GN", "q-fin.TR"], "primary_category": "q-fin.GN"}
