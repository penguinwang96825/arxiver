{"title": "Using Interlinear Glosses as Pivot in Low-Resource Multilingual Machine\n  Translation", "abstract": "We demonstrate a new approach to Neural Machine Translation (NMT) for\nlow-resource languages using a ubiquitous linguistic resource, Interlinear\nGlossed Text (IGT). IGT represents a non-English sentence as a sequence of\nEnglish lemmas and morpheme labels. As such, it can serve as a pivot or\ninterlingua for NMT. Our contribution is four-fold. Firstly, we pool IGT for\n1,497 languages in ODIN (54,545 glosses) and 70,918 glosses in Arapaho and\ntrain a gloss-to-target NMT system from IGT to English, with a BLEU score of\n25.94. We introduce a multilingual NMT model that tags all glossed text with\ngloss-source language tags and train a universal system with shared attention\nacross 1,497 languages. Secondly, we use the IGT gloss-to-target translation as\na key step in an English-Turkish MT system trained on only 865 lines from ODIN.\nThirdly, we we present five metrics for evaluating extremely low-resource\ntranslation when BLEU is no longer sufficient and evaluate the Turkish\nlow-resource system using BLEU and also using accuracy of matching nouns,\nverbs, agreement, tense, and spurious repetition, showing large improvements.", "published": "2019-11-07 01:45:33", "link": "http://arxiv.org/abs/1911.02709v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making the Best Use of Review Summary for Sentiment Analysis", "abstract": "Sentiment analysis provides a useful overview of customer review contents.\nMany review websites allow a user to enter a summary in addition to a full\nreview. Intuitively, summary information may give additional benefit for review\nsentiment analysis. In this paper, we conduct a study to exploit methods for\nbetter use of summary information. We start by finding out that the sentimental\nsignal distribution of a review and that of its corresponding summary are in\nfact complementary to each other. We thus explore various architectures to\nbetter guide the interactions between the two and propose a\nhierarchically-refined review-centric attention model. Empirical results show\nthat our review-centric model can make better use of user-written summaries for\nreview sentiment analysis, and is also more effective compared to existing\nmethods when the user summary is replaced with summary generated by an\nautomatic summarization system.", "published": "2019-11-07 01:46:54", "link": "http://arxiv.org/abs/1911.02711v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Knowledge Distillation in Non-autoregressive Machine\n  Translation", "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of\noutput tokens in parallel, achieving substantial improvements in generation\nspeed compared to autoregressive models. Existing NAT models usually rely on\nthe technique of knowledge distillation, which creates the training data from a\npretrained autoregressive model for better performance. Knowledge distillation\nis empirically useful, leading to large gains in accuracy for NAT models, but\nthe reason for this success has, as of yet, been unclear. In this paper, we\nfirst design systematic experiments to investigate why knowledge distillation\nis crucial to NAT training. We find that knowledge distillation can reduce the\ncomplexity of data sets and help NAT to model the variations in the output\ndata. Furthermore, a strong correlation is observed between the capacity of an\nNAT model and the optimal complexity of the distilled data for the best\ntranslation quality. Based on these findings, we further propose several\napproaches that can alter the complexity of data sets to improve the\nperformance of NAT models. We achieve the state-of-the-art performance for the\nNAT-based models, and close the gap with the autoregressive baseline on WMT14\nEn-De benchmark.", "published": "2019-11-07 02:47:26", "link": "http://arxiv.org/abs/1911.02727v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SubCharacter Chinese-English Neural Machine Translation with Wubi\n  encoding", "abstract": "Neural machine translation (NMT) is one of the best methods for understanding\nthe differences in semantic rules between two languages. Especially for\nIndo-European languages, subword-level models have achieved impressive results.\nHowever, when the translation task involves Chinese, semantic granularity\nremains at the word and character level, so there is still need more\nfine-grained translation model of Chinese. In this paper, we introduce a simple\nand effective method for Chinese translation at the sub-character level. Our\napproach uses the Wubi method to translate Chinese into English; byte-pair\nencoding (BPE) is then applied. Our method for Chinese-English translation\neliminates the need for a complicated word segmentation algorithm during\npreprocessing. Furthermore, our method allows for sub-character-level neural\ntranslation based on recurrent neural network (RNN) architecture, without\npreprocessing. The empirical results show that for Chinese-English translation\ntasks, our sub-character-level model has a comparable BLEU score to the subword\nmodel, despite having a much smaller vocabulary. Additionally, the small\nvocabulary is highly advantageous for NMT model compression.", "published": "2019-11-07 03:13:26", "link": "http://arxiv.org/abs/1911.02737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transition-Based Deep Input Linearization", "abstract": "Traditional methods for deep NLG adopt pipeline approaches comprising stages\nsuch as constructing syntactic input, predicting function words, linearizing\nthe syntactic input and generating the surface forms. Though easier to\nvisualize, pipeline approaches suffer from error propagation. In addition,\ninformation available across modules cannot be leveraged by all modules. We\nconstruct a transition-based model to jointly perform linearization, function\nword prediction and morphological generation, which considerably improves upon\nthe accuracy compared to a pipelined baseline system. On a standard deep input\nlinearization shared task, our system achieves the best results reported so\nfar.", "published": "2019-11-07 08:58:02", "link": "http://arxiv.org/abs/1911.02808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Pre-trained Chinese Character Representation with Word-aligned\n  Attention", "abstract": "Most Chinese pre-trained models take character as the basic unit and learn\nrepresentation according to character's external contexts, ignoring the\nsemantics expressed in the word, which is the smallest meaningful utterance in\nChinese. Hence, we propose a novel word-aligned attention to exploit explicit\nword information, which is complementary to various character-based Chinese\npre-trained language models. Specifically, we devise a pooling mechanism to\nalign the character-level attention to the word level and propose to alleviate\nthe potential issue of segmentation error propagation by multi-source\ninformation fusion. As a result, word and character information are explicitly\nintegrated at the fine-tuning procedure. Experimental results on five Chinese\nNLP benchmark tasks demonstrate that our model could bring another significant\ngain over several pre-trained models.", "published": "2019-11-07 09:50:21", "link": "http://arxiv.org/abs/1911.02821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Grammatical Error Correction with Machine Translation Pairs", "abstract": "We propose a novel data synthesis method to generate diverse error-corrected\nsentence pairs for improving grammatical error correction, which is based on a\npair of machine translation models of different qualities (i.e., poor and\ngood). The poor translation model resembles the ESL (English as a second\nlanguage) learner and tends to generate translations of low quality in terms of\nfluency and grammatical correctness, while the good translation model generally\ngenerates fluent and grammatically correct translations. We build the poor and\ngood translation model with phrase-based statistical machine translation model\nwith decreased language model weight and neural machine translation model\nrespectively. By taking the pair of their translations of the same sentences in\na bridge language as error-corrected sentence pairs, we can construct unlimited\npseudo parallel data. Our approach is capable of generating diverse\nfluency-improving patterns without being limited by the pre-defined rule set\nand the seed error-corrected data. Experimental results demonstrate the\neffectiveness of our approach and show that it can be combined with other\nsynthetic data sources to yield further improvements.", "published": "2019-11-07 10:08:04", "link": "http://arxiv.org/abs/1911.02825v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit Pairwise Word Interaction Modeling Improves Pretrained\n  Transformers for English Semantic Similarity Tasks", "abstract": "In English semantic similarity tasks, classic word embedding-based approaches\nexplicitly model pairwise \"interactions\" between the word representations of a\nsentence pair. Transformer-based pretrained language models disregard this\nnotion, instead modeling pairwise word interactions globally and implicitly\nthrough their self-attention mechanism. In this paper, we hypothesize that\nintroducing an explicit, constrained pairwise word interaction mechanism to\npretrained language models improves their effectiveness on semantic similarity\ntasks. We validate our hypothesis using BERT on four tasks in semantic textual\nsimilarity and answer sentence selection. We demonstrate consistent\nimprovements in quality by adding an explicit pairwise word interaction module\nto BERT.", "published": "2019-11-07 10:59:40", "link": "http://arxiv.org/abs/1911.02847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency and Span, Cross-Style Semantic Role Labeling on PropBank and\n  NomBank", "abstract": "The latest developments in neural semantic role labeling (SRL) have shown\ngreat performance improvements with both the dependency and span\nformalisms/styles. Although the two styles share many similarities in\nlinguistic meaning and computation, most previous studies focus on a single\nstyle. In this paper, we define a new cross-style semantic role label\nconvention and propose a new cross-style joint optimization model designed\naround the most basic linguistic meaning of a semantic role, providing a\nsolution to make the results of the two styles more comparable and allowing\nboth formalisms of SRL to benefit from their natural connections in both\nlinguistics and computation. Our model learns a general semantic argument\nstructure and is capable of outputting in either style. Additionally, we\npropose a syntax-aided method to uniformly enhance the learning of both\ndependency and span representations. Experiments show that the proposed methods\nare effective on both span and dependency SRL benchmarks.", "published": "2019-11-07 11:02:39", "link": "http://arxiv.org/abs/1911.02851v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dice Loss for Data-imbalanced NLP Tasks", "abstract": "Many NLP tasks such as tagging and machine reading comprehension are faced\nwith the severe data imbalance issue: negative examples significantly outnumber\npositive examples, and the huge number of background examples (or easy-negative\nexamples) overwhelms the training. The most commonly used cross entropy (CE)\ncriteria is actually an accuracy-oriented objective, and thus creates a\ndiscrepancy between training and test: at training time, each training instance\ncontributes equally to the objective function, while at test time F1 score\nconcerns more about positive examples. In this paper, we propose to use dice\nloss in replacement of the standard cross-entropy objective for data-imbalanced\nNLP tasks. Dice loss is based on the Sorensen-Dice coefficient or Tversky\nindex, which attaches similar importance to false positives and false\nnegatives, and is more immune to the data-imbalance issue. To further alleviate\nthe dominating influence from easy-negative examples in training, we propose to\nassociate training examples with dynamically adjusted weights to deemphasize\neasy-negative examples.Theoretical analysis shows that this strategy narrows\ndown the gap between the F1 score in evaluation and the dice loss in training.\nWith the proposed training objective, we observe significant performance boost\non a wide range of data imbalanced NLP tasks. Notably, we are able to achieve\nSOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA\nresults on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity\nrecognition task; along with competitive results on the tasks of machine\nreading comprehension and paraphrase identification.", "published": "2019-11-07 11:14:05", "link": "http://arxiv.org/abs/1911.02855v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The LIG system for the English-Czech Text Translation Task of IWSLT 2019", "abstract": "In this paper, we present our submission for the English to Czech Text\nTranslation Task of IWSLT 2019. Our system aims to study how pre-trained\nlanguage models, used as input embeddings, can improve a specialized machine\ntranslation system trained on few data. Therefore, we implemented a\nTransformer-based encoder-decoder neural system which is able to use the output\nof a pre-trained language model as input embeddings, and we compared its\nperformance under three configurations: 1) without any pre-trained language\nmodel (constrained), 2) using a language model trained on the monolingual parts\nof the allowed English-Czech data (constrained), and 3) using a language model\ntrained on a large quantity of external monolingual data (unconstrained). We\nused BERT as external pre-trained language model (configuration 3), and BERT\narchitecture for training our own language model (configuration 2). Regarding\nthe training data, we trained our MT system on a small quantity of parallel\ntext: one set only consists of the provided MuST-C corpus, and the other set\nconsists of the MuST-C corpus and the News Commentary corpus from WMT. We\nobserved that using the external pre-trained BERT improves the scores of our\nsystem by +0.8 to +1.5 of BLEU on our development set, and +0.97 to +1.94 of\nBLEU on the test set. However, using our own language model trained only on the\nallowed parallel data seems to improve the machine translation performances\nonly when the system is trained on the smallest dataset.", "published": "2019-11-07 13:40:54", "link": "http://arxiv.org/abs/1911.02898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformation of Dense and Sparse Text Representations", "abstract": "Sparsity is regarded as a desirable property of representations, especially\nin terms of explanation. However, its usage has been limited due to the gap\nwith dense representations. Most NLP research progresses in recent years are\nbased on dense representations. Thus the desirable property of sparsity cannot\nbe leveraged. Inspired by Fourier Transformation, in this paper, we propose a\nnovel Semantic Transformation method to bridge the dense and sparse spaces,\nwhich can facilitate the NLP research to shift from dense space to sparse space\nor to jointly use both spaces. The key idea of the proposed approach is to use\na Forward Transformation to transform dense representations to sparse\nrepresentations. Then some useful operations in the sparse space can be\nperformed over the sparse representations, and the sparse representations can\nbe used directly to perform downstream tasks such as text classification and\nnatural language inference. Then, a Backward Transformation can also be carried\nout to transform those processed sparse representations to dense\nrepresentations. Experiments using classification tasks and natural language\ninference task show that the proposed Semantic Transformation is effective.", "published": "2019-11-07 14:06:36", "link": "http://arxiv.org/abs/1911.02914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Can BERT Help Lexical Semantics Tasks?", "abstract": "Contextualized embeddings such as BERT can serve as strong input\nrepresentations to NLP tasks, outperforming their static embeddings\ncounterparts such as skip-gram, CBOW and GloVe. However, such embeddings are\ndynamic, calculated according to a sentence-level context, which limits their\nuse in lexical semantics tasks. We address this issue by making use of dynamic\nembeddings as word representations in training static embeddings, thereby\nleveraging their strong representation power for disambiguating context\ninformation. Results show that this method leads to improvements over\ntraditional static embeddings on a range of lexical semantics tasks, obtaining\nthe best reported results on seven datasets.", "published": "2019-11-07 14:23:05", "link": "http://arxiv.org/abs/1911.02929v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTs of a feather do not generalize together: Large variability in\n  generalization across models with similar test set performance", "abstract": "If the same neural network architecture is trained multiple times on the same\ndataset, will it make similar linguistic generalizations across runs? To study\nthis question, we fine-tuned 100 instances of BERT on the Multi-genre Natural\nLanguage Inference (MNLI) dataset and evaluated them on the HANS dataset, which\nevaluates syntactic generalization in natural language inference. On the MNLI\ndevelopment set, the behavior of all instances was remarkably consistent, with\naccuracy ranging between 83.6% and 84.8%. In stark contrast, the same models\nvaried widely in their generalization performance. For example, on the simple\ncase of subject-object swap (e.g., determining that \"the doctor visited the\nlawyer\" does not entail \"the lawyer visited the doctor\"), accuracy ranged from\n0.00% to 66.2%. Such variation is likely due to the presence of many local\nminima that are equally attractive to a low-bias learner such as a neural\nnetwork; decreasing the variability may therefore require models with stronger\ninductive biases.", "published": "2019-11-07 16:20:40", "link": "http://arxiv.org/abs/1911.02969v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SIMMC: Situated Interactive Multi-Modal Conversational Data Collection\n  And Evaluation Platform", "abstract": "As digital virtual assistants become ubiquitous, it becomes increasingly\nimportant to understand the situated behaviour of users as they interact with\nthese assistants. To this end, we introduce SIMMC, an extension to ParlAI for\nmulti-modal conversational data collection and system evaluation. SIMMC\nsimulates an immersive setup, where crowd workers are able to interact with\nenvironments constructed in AI Habitat or Unity while engaging in a\nconversation. The assistant in SIMMC can be a crowd worker or Artificial\nIntelligent (AI) agent. This enables both (i) a multi-player / Wizard of Oz\nsetting for data collection, or (ii) a single player mode for model / system\nevaluation. We plan to open-source a situated conversational data-set collected\non this platform for the Conversational AI research community.", "published": "2019-11-07 00:52:38", "link": "http://arxiv.org/abs/1911.02690v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Domain Neural Machine Translation with Word-Level Adaptive\n  Layer-wise Domain Mixing", "abstract": "Many multi-domain neural machine translation (NMT) models achieve knowledge\ntransfer by enforcing one encoder to learn shared embedding across domains.\nHowever, this design lacks adaptation to individual domains. To overcome this\nlimitation, we propose a novel multi-domain NMT model using individual modules\nfor each domain, on which we apply word-level, adaptive and layer-wise domain\nmixing. We first observe that words in a sentence are often related to multiple\ndomains. Hence, we assume each word has a domain proportion, which indicates\nits domain preference. Then word representations are obtained by mixing their\nembedding in individual domains based on their domain proportions. We show this\ncan be achieved by carefully designing multi-head dot-product attention modules\nfor different domains, and eventually taking weighted averages of their\nparameters by word-level layer-wise domain proportions. Through this, we can\nachieve effective domain knowledge sharing, and capture fine-grained\ndomain-specific knowledge as well. Our experiments show that our proposed model\noutperforms existing ones in several NMT tasks.", "published": "2019-11-07 00:54:06", "link": "http://arxiv.org/abs/1911.02692v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounded Conversation Generation as Guided Traverses in Commonsense\n  Knowledge Graphs", "abstract": "Human conversations naturally evolve around related concepts and scatter to\nmulti-hop concepts. This paper presents a new conversation generation model,\nConceptFlow, which leverages commonsense knowledge graphs to explicitly model\nconversation flows. By grounding conversations to the concept space,\nConceptFlow represents the potential conversation flow as traverses in the\nconcept space along commonsense relations. The traverse is guided by graph\nattentions in the concept graph, moving towards more meaningful directions in\nthe concept space, in order to generate more semantic and informative\nresponses. Experiments on Reddit conversations demonstrate ConceptFlow's\neffectiveness over previous knowledge-aware conversation models and GPT-2 based\nmodels while using 70% fewer parameters, confirming the advantage of explicit\nmodeling conversation structures. All source codes of this work are available\nat https://github.com/thunlp/ConceptFlow.", "published": "2019-11-07 01:40:39", "link": "http://arxiv.org/abs/1911.02707v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Porous Lattice-based Transformer Encoder for Chinese NER", "abstract": "Incorporating lattices into character-level Chinese named entity recognition\nis an effective method to exploit explicit word information. Recent works\nextend recurrent and convolutional neural networks to model lattice inputs.\nHowever, due to the DAG structure or the variable-sized potential word set for\nlattice inputs, these models prevent the convenient use of batched computation,\nresulting in serious inefficient. In this paper, we propose a porous\nlattice-based transformer encoder for Chinese named entity recognition, which\nis capable to better exploit the GPU parallelism and batch the computation\nowing to the mask mechanism in transformer. We first investigate the\nlattice-aware self-attention coupled with relative position representations to\nexplore effective word information in the lattice structure. Besides, to\nstrengthen the local dependencies among neighboring tokens, we propose a novel\nporous structure during self-attentional computation processing, in which every\ntwo non-neighboring tokens are connected through a shared pivot node.\nExperimental results on four datasets show that our model performs up to 9.47\ntimes faster than state-of-the-art models, while is roughly on a par with its\nperformance. The source code of this paper can be obtained from\nhttps://github.com/xxx/xxx.", "published": "2019-11-07 02:58:17", "link": "http://arxiv.org/abs/1911.02733v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Query-bag Matching with Mutual Coverage for Information-seeking\n  Conversations in E-commerce", "abstract": "Information-seeking conversation system aims at satisfying the information\nneeds of users through conversations. Text matching between a user query and a\npre-collected question is an important part of the information-seeking\nconversation in E-commerce. In the practical scenario, a sort of questions\nalways correspond to a same answer. Naturally, these questions can form a bag.\nLearning the matching between user query and bag directly may improve the\nconversation performance, denoted as query-bag matching. Inspired by such\nopinion, we propose a query-bag matching model which mainly utilizes the mutual\ncoverage between query and bag and measures the degree of the content in the\nquery mentioned by the bag, and vice verse. In addition, the learned bag\nrepresentation in word level helps find the main points of a bag in a fine\ngrade and promotes the query-bag matching performance. Experiments on two\ndatasets show the effectiveness of our model.", "published": "2019-11-07 04:11:03", "link": "http://arxiv.org/abs/1911.02747v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "S2ORC: The Semantic Scholar Open Research Corpus", "abstract": "We introduce S2ORC, a large corpus of 81.1M English-language academic papers\nspanning many academic disciplines. The corpus consists of rich metadata, paper\nabstracts, resolved bibliographic references, as well as structured full text\nfor 8.1M open access papers. Full text is annotated with automatically-detected\ninline mentions of citations, figures, and tables, each linked to their\ncorresponding paper objects. In S2ORC, we aggregate papers from hundreds of\nacademic publishers and digital archives into a unified source, and create the\nlargest publicly-available collection of machine-readable academic text to\ndate. We hope this resource will facilitate research and development of tools\nand tasks for text mining over academic text.", "published": "2019-11-07 07:34:43", "link": "http://arxiv.org/abs/1911.02782v3", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "CROWN: Conversational Passage Ranking by Reasoning over Word Networks", "abstract": "Information needs around a topic cannot be satisfied in a single turn; users\ntypically ask follow-up questions referring to the same theme and a system must\nbe capable of understanding the conversational context of a request to retrieve\ncorrect answers. In this paper, we present our submission to the TREC\nConversational Assistance Track 2019, in which such a conversational setting is\nexplored. We propose a simple unsupervised method for conversational passage\nranking by formulating the passage score for a query as a combination of\nsimilarity and coherence. To be specific, passages are preferred that contain\nwords semantically similar to the words used in the question, and where such\nwords appear close by. We built a word-proximity network (WPN) from a large\ncorpus, where words are nodes and there is an edge between two nodes if they\nco-occur in the same passages in a statistically significant way, within a\ncontext window. Our approach, named CROWN, improved nDCG scores over a provided\nIndri baseline on the CAsT training data. On the evaluation data for CAsT, our\nbest run submission achieved above-average performance with respect to AP@5 and\nnDCG@1000.", "published": "2019-11-07 11:02:21", "link": "http://arxiv.org/abs/1911.02850v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Improving Joint Training of Inference Networks and Structured Prediction\n  Energy Networks", "abstract": "Deep energy-based models are powerful, but pose challenges for learning and\ninference (Belanger and McCallum, 2016). Tu and Gimpel (2018) developed an\nefficient framework for energy-based models by training \"inference networks\" to\napproximate structured inference instead of using gradient descent. However,\ntheir alternating optimization approach suffers from instabilities during\ntraining, requiring additional loss terms and careful hyperparameter tuning. In\nthis paper, we contribute several strategies to stabilize and improve this\njoint training of energy functions and inference networks for structured\nprediction. We design a compound objective to jointly train both cost-augmented\nand test-time inference networks along with the energy function. We propose\njoint parameterizations for the inference networks that encourage them to\ncapture complementary functionality during learning. We empirically validate\nour strategies on two sequence labeling tasks, showing easier paths to strong\nperformance than prior work, as well as further improvements with global energy\nterms.", "published": "2019-11-07 13:26:07", "link": "http://arxiv.org/abs/1911.02891v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextualized Sparse Representations for Real-Time Open-Domain Question\n  Answering", "abstract": "Open-domain question answering can be formulated as a phrase retrieval\nproblem, in which we can expect huge scalability and speed benefit but often\nsuffer from low accuracy due to the limitation of existing phrase\nrepresentation models. In this paper, we aim to improve the quality of each\nphrase embedding by augmenting it with a contextualized sparse representation\n(Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g.,\ntf-idf) or directly learned (only few thousand dimensions), we leverage\nrectified self-attention to indirectly learn sparse vectors in n-gram\nvocabulary space. By augmenting the previous phrase retrieval model (Seo et\nal., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.\nOur CuratedTREC score is even better than the best known retrieve & read model\nwith at least 45x faster inference speed.", "published": "2019-11-07 13:34:54", "link": "http://arxiv.org/abs/1911.02896v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Blockwise Self-Attention for Long Document Understanding", "abstract": "We present BlockBERT, a lightweight and efficient BERT model for better\nmodeling long-distance dependencies. Our model extends BERT by introducing\nsparse block structures into the attention matrix to reduce both memory\nconsumption and training/inference time, which also enables attention heads to\ncapture either short- or long-range contextual information. We conduct\nexperiments on language model pre-training and several benchmark question\nanswering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1%\nless memory and 12.0-25.1% less time to learn the model. During testing,\nBlockBERT saves 27.8% inference time, while having comparable and sometimes\nbetter prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "published": "2019-11-07 16:35:53", "link": "http://arxiv.org/abs/1911.02972v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DCA: Diversified Co-Attention towards Informative Live Video Commenting", "abstract": "We focus on the task of Automatic Live Video Commenting (ALVC), which aims to\ngenerate real-time video comments with both video frames and other viewers'\ncomments as inputs. A major challenge in this task is how to properly leverage\nthe rich and diverse information carried by video and text. In this paper, we\naim to collect diversified information from video and text for informative\ncomment generation. To achieve this, we propose a Diversified Co-Attention\n(DCA) model for this task. Our model builds bidirectional interactions between\nvideo frames and surrounding comments from multiple perspectives via metric\nlearning, to collect a diversified and informative context for comment\ngeneration. We also propose an effective parameter orthogonalization technique\nto avoid excessive overlap of information learned from different perspectives.\nResults show that our approach outperforms existing methods in the ALVC task,\nachieving new state-of-the-art results.", "published": "2019-11-07 03:28:38", "link": "http://arxiv.org/abs/1911.02739v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework", "abstract": "Text-to-speech synthesis (TTS) has witnessed rapid progress in recent years,\nwhere neural methods became capable of producing audios with high naturalness.\nHowever, these efforts still suffer from two types of latencies: (a) the {\\em\ncomputational latency} (synthesizing time), which grows linearly with the\nsentence length even with parallel approaches, and (b) the {\\em input latency}\nin scenarios where the input text is incrementally generated (such as in\nsimultaneous translation, dialog generation, and assistive technologies). To\nreduce these latencies, we devise the first neural incremental TTS approach\nbased on the recently proposed prefix-to-prefix framework. We synthesize speech\nin an online fashion, playing a segment of audio while generating the next,\nresulting in an $O(1)$ rather than $O(n)$ latency.", "published": "2019-11-07 04:22:54", "link": "http://arxiv.org/abs/1911.02750v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Teacher-Student Training for Robust Tacotron-based TTS", "abstract": "While neural end-to-end text-to-speech (TTS) is superior to conventional\nstatistical methods in many ways, the exposure bias problem in the\nautoregressive models remains an issue to be resolved. The exposure bias\nproblem arises from the mismatch between the training and inference process,\nthat results in unpredictable performance for out-of-domain test data at\nrun-time. To overcome this, we propose a teacher-student training scheme for\nTacotron-based TTS by introducing a distillation loss function in addition to\nthe feature loss function. We first train a Tacotron2-based TTS model by always\nproviding natural speech frames to the decoder, that serves as a teacher model.\nWe then train another Tacotron2-based model as a student model, of which the\ndecoder takes the predicted speech frames as input, similar to how the decoder\nworks during run-time inference. With the distillation loss, the student model\nlearns the output probabilities from the teacher model, that is called\nknowledge distillation. Experiments show that our proposed training scheme\nconsistently improves the voice quality for out-of-domain test data both in\nChinese and English systems.", "published": "2019-11-07 10:47:30", "link": "http://arxiv.org/abs/1911.02839v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SENSE: Semantically Enhanced Node Sequence Embedding", "abstract": "Effectively capturing graph node sequences in the form of vector embeddings\nis critical to many applications. We achieve this by (i) first learning vector\nembeddings of single graph nodes and (ii) then composing them to compactly\nrepresent node sequences. Specifically, we propose SENSE-S (Semantically\nEnhanced Node Sequence Embedding - for Single nodes), a skip-gram based novel\nembedding mechanism, for single graph nodes that co-learns graph structure as\nwell as their textual descriptions. We demonstrate that SENSE-S vectors\nincrease the accuracy of multi-label classification tasks by up to 50% and\nlink-prediction tasks by up to 78% under a variety of scenarios using real\ndatasets. Based on SENSE-S, we next propose generic SENSE to compute composite\nvectors that represent a sequence of nodes, where preserving the node order is\nimportant. We prove that this approach is efficient in embedding node\nsequences, and our experiments on real data confirm its high accuracy in node\norder decoding.", "published": "2019-11-07 16:21:40", "link": "http://arxiv.org/abs/1911.02970v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Probing Contextualized Sentence Representations with Visual Awareness", "abstract": "We present a universal framework to model contextualized sentence\nrepresentations with visual awareness that is motivated to overcome the\nshortcomings of the multimodal parallel data with manual annotations. For each\nsentence, we first retrieve a diversity of images from a shared cross-modal\nembedding space, which is pre-trained on a large-scale of text-image pairs.\nThen, the texts and images are respectively encoded by transformer encoder and\nconvolutional neural network. The two sequences of representations are further\nfused by a simple and effective attention layer. The architecture can be easily\napplied to text-only natural language processing tasks without manually\nannotating multimodal parallel corpora. We apply the proposed method on three\ntasks, including neural machine translation, natural language inference and\nsequence labeling and experimental results verify the effectiveness.", "published": "2019-11-07 16:34:31", "link": "http://arxiv.org/abs/1911.02971v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Neural Networks Learn Symbolic Rewriting?", "abstract": "This work investigates if the current neural architectures are adequate for\nlearning symbolic rewriting. Two kinds of data sets are proposed for this\nresearch -- one based on automated proofs and the other being a synthetic set\nof polynomial terms. The experiments with use of the current neural machine\ntranslation models are performed and its results are discussed. Ideas for\nextending this line of research are proposed, and its relevance is motivated.", "published": "2019-11-07 11:22:44", "link": "http://arxiv.org/abs/1911.04873v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Microsoft Research Asia's Systems for WMT19", "abstract": "We Microsoft Research Asia made submissions to 11 language directions in the\nWMT19 news translation tasks. We won the first place for 8 of the 11 directions\nand the second place for the other three. Our basic systems are built on\nTransformer, back translation and knowledge distillation. We integrate several\nof our rececent techniques to enhance the baseline systems: multi-agent dual\nlearning (MADL), masked sequence-to-sequence pre-training (MASS), neural\narchitecture optimization (NAO), and soft contextual data augmentation (SCA).", "published": "2019-11-07 03:55:53", "link": "http://arxiv.org/abs/1911.06191v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multi-domain Dialogue State Tracking as Dynamic Knowledge Graph Enhanced\n  Question Answering", "abstract": "Multi-domain dialogue state tracking (DST) is a critical component for\nconversational AI systems. The domain ontology (i.e., specification of domains,\nslots, and values) of a conversational AI system is generally incomplete,\nmaking the capability for DST models to generalize to new slots, values, and\ndomains during inference imperative. In this paper, we propose to model\nmulti-domain DST as a question answering problem, referred to as Dialogue State\nTracking via Question Answering (DSTQA). Within DSTQA, each turn generates a\nquestion asking for the value of a (domain, slot) pair, thus making it\nnaturally extensible to unseen domains, slots, and values. Additionally, we use\na dynamically-evolving knowledge graph to explicitly learn relationships\nbetween (domain, slot) pairs. Our model has a 5.80% and 12.21% relative\nimprovement over the current state-of-the-art model on MultiWOZ 2.0 and\nMultiWOZ 2.1 datasets, respectively. Additionally, our model consistently\noutperforms the state-of-the-art model in domain adaptation settings. (Code is\nreleased at https://github.com/alexa/dstqa )", "published": "2019-11-07 10:00:16", "link": "http://arxiv.org/abs/1911.06192v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Mask-dependent Phase Estimation for Monaural Speaker Separation", "abstract": "Speaker separation refers to isolating speech of interest in a multi-talker\nenvironment. Most methods apply real-valued Time-Frequency (T-F) masks to the\nmixture Short-Time Fourier Transform (STFT) to reconstruct the clean speech.\nHence there is an unavoidable mismatch between the phase of the reconstruction\nand the original phase of the clean speech. In this paper, we propose a simple\nyet effective phase estimation network that predicts the phase of the clean\nspeech based on a T-F mask predicted by a chimera++ network. To overcome the\nlabel-permutation problem for both the T-F mask and the phase, we propose a\nmask-dependent permutation invariant training (PIT) criterion to select the\nphase signal based on the loss from the T-F mask prediction. We also propose an\nInverse Mask Weighted Loss Function for phase prediction to focus the model on\nthe T-F regions in which the phase is more difficult to predict. Results on the\nWSJ0-2mix dataset show that the phase estimation network achieves comparable\nperformance to models that use iterative phase reconstruction or end-to-end\ntime-domain loss functions, but in a more straightforward manner.", "published": "2019-11-07 04:07:25", "link": "http://arxiv.org/abs/1911.02746v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Accounting for Physics Uncertainty in Ultrasonic Wave Propagation using\n  Deep Learning", "abstract": "Ultrasonic guided waves are commonly used to localize structural damage in\ninfrastructures such as buildings, airplanes, bridges. Damage localization can\nbe viewed as an inverse problem. Physical model based techniques are popular\nfor guided wave based damage localization. The performance of these techniques\ndepend on the degree of faithfulness with which the physical model describes\nwave propagation. External factors such as environmental variations and random\nnoise are a source of uncertainty in wave propagation. The physical modeling of\nuncertainty in an inverse problem is still a challenging problem. In this work,\nwe propose a deep learning based model for robust damage localization in\npresence of uncertainty. Wave data with uncertainty is simulated to reflect\nvariations due to external factors and Gaussian noise is added to reflect\nrandom noise in the environment. After evaluating the localization error on\ntest data with uncertainty, we observe that the deep learning model trained\nwith uncertainty can learn robust representations. The approach shows potential\nfor dealing with uncertainty in physical science problems using deep learning\nmodels.", "published": "2019-11-07 03:44:18", "link": "http://arxiv.org/abs/1911.02743v1", "categories": ["cs.LG", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Change your singer: a transfer learning generative adversarial framework\n  for song to song conversion", "abstract": "Have you ever wondered how a song might sound if performed by a different\nartist? In this work, we propose SCM-GAN, an end-to-end non-parallel song\nconversion system powered by generative adversarial and transfer learning that\nallows users to listen to a selected target singer singing any song. SCM-GAN\nfirst separates songs into vocals and instrumental music using a U-Net network,\nthen converts the vocal segments to the target singer using advanced\nCycleGAN-VC, before merging the converted vocals with their corresponding\nbackground music. SCM-GAN is first initialized with feature representations\nlearned from a state-of-the-art voice-to-voice conversion and then trained on a\ndataset of non-parallel songs. Furthermore, SCM-GAN is evaluated against a set\nof metrics including global variance GV and modulation spectra MS on the 24\nMel-cepstral coefficients (MCEPs). Transfer learning improves the GV by 35% and\nthe MS by 13% on average. A subjective comparison is conducted to test the user\nsatisfaction with the quality and the naturalness of the conversion. Results\nshow above par similarity between SCM-GAN's output and the target (70\\% on\naverage) as well as great naturalness of the converted songs.", "published": "2019-11-07 14:32:43", "link": "http://arxiv.org/abs/1911.02933v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
