{"title": "Contextual Mood Analysis with Knowledge Graph Representation for Hindi\n  Song Lyrics in Devanagari Script", "abstract": "Lyrics play a significant role in conveying the song's mood and are\ninformation to understand and interpret music communication. Conventional\nnatural language processing approaches use translation of the Hindi text into\nEnglish for analysis. This approach is not suitable for lyrics as it is likely\nto lose the inherent intended contextual meaning. Thus, the need was identified\nto develop a system for Devanagari text analysis. The data set of 300 song\nlyrics with equal distribution in five different moods is used for the\nexperimentation. The proposed system performs contextual mood analysis of Hindi\nsong lyrics in Devanagari text format. The contextual analysis is stored as a\nknowledge base, updated using an incremental learning approach with new data.\nContextual knowledge graph with moods and associated important contextual terms\nprovides the graphical representation of the lyric data set used. The testing\nresults show 64% accuracy for the mood prediction. This work can be easily\nextended to applications related to Hindi literary work such as summarization,\nindexing, contextual retrieval, context-based classification and grouping of\ndocuments.", "published": "2021-08-16 07:44:20", "link": "http://arxiv.org/abs/2108.06947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MobIE: A German Dataset for Named Entity Recognition, Entity Linking and\n  Relation Extraction in the Mobility Domain", "abstract": "We present MobIE, a German-language dataset, which is human-annotated with 20\ncoarse- and fine-grained entity types and entity linking information for\ngeographically linkable entities. The dataset consists of 3,232 social media\ntexts and traffic reports with 91K tokens, and contains 20.5K annotated\nentities, 13.1K of which are linked to a knowledge base. A subset of the\ndataset is human-annotated with seven mobility-related, n-ary relation types,\nwhile the remaining documents are annotated using a weakly-supervised labeling\napproach implemented with the Snorkel framework. To the best of our knowledge,\nthis is the first German-language dataset that combines annotations for NER, EL\nand RE, and thus can be used for joint and multi-task learning of these\nfundamental information extraction tasks. We make MobIE public at\nhttps://github.com/dfki-nlp/mobie.", "published": "2021-08-16 08:21:50", "link": "http://arxiv.org/abs/2108.06955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Single Example Can Improve Zero-Shot Data Generation", "abstract": "Sub-tasks of intent classification, such as robustness to distribution shift,\nadaptation to specific user groups and personalization, out-of-domain\ndetection, require extensive and flexible datasets for experiments and\nevaluation. As collecting such datasets is time- and labor-consuming, we\npropose to use text generation methods to gather datasets. The generator should\nbe trained to generate utterances that belong to the given intent. We explore\ntwo approaches to generating task-oriented utterances. In the zero-shot\napproach, the model is trained to generate utterances from seen intents and is\nfurther used to generate utterances for intents unseen during training. In the\none-shot approach, the model is presented with a single utterance from a test\nintent. We perform a thorough automatic, and human evaluation of the dataset\ngenerated utilizing two proposed approaches. Our results reveal that the\nattributes of the generated data are close to original test sets, collected via\ncrowd-sourcing.", "published": "2021-08-16 09:43:26", "link": "http://arxiv.org/abs/2108.06991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning for Massively Parallel Translation of Constrained Text\n  into Low Resource Languages", "abstract": "We translate a closed text that is known in advance and available in many\nlanguages into a new and severely low resource language. Most human translation\nefforts adopt a portion-based approach to translate consecutive pages/chapters\nin order, which may not suit machine translation. We compare the portion-based\napproach that optimizes coherence of the text locally with the random sampling\napproach that increases coverage of the text globally. Our results show that\nthe random sampling approach performs better. When training on a seed corpus of\n~1,000 lines from the Bible and testing on the rest of the Bible (~30,000\nlines), random sampling gives a performance gain of +11.0 BLEU using English as\na simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a\nMayan language. Furthermore, we compare three ways of updating machine\ntranslation models with increasing amount of human post-edited data through\niterations. We find that adding newly post-edited data to training after\nvocabulary update without self-supervision performs the best. We propose an\nalgorithm for human and machine to work together seamlessly to translate a\nclosed text into a severely low resource language.", "published": "2021-08-16 14:49:50", "link": "http://arxiv.org/abs/2108.07127v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Partially Supervised Named Entity Recognition via the Expected Entity\n  Ratio Loss", "abstract": "We study learning named entity recognizers in the presence of missing entity\nannotations. We approach this setting as tagging with latent variables and\npropose a novel loss, the Expected Entity Ratio, to learn models in the\npresence of systematically missing tags. We show that our approach is both\ntheoretically sound and empirically useful. Experimentally, we find that it\nmeets or exceeds performance of strong and state-of-the-art baselines across a\nvariety of languages, annotation scenarios, and amounts of labeled data. In\nparticular, we find that it significantly outperforms the previous\nstate-of-the-art methods from Mayhew et al. (2019) and Li et al. (2021) by\n+12.7 and +2.3 F1 score in a challenging setting with only 1,000 biased\nannotations, averaged across 7 datasets. We also show that, when combined with\nour approach, a novel sparse annotation scheme outperforms exhaustive\nannotation for modest annotation budgets.", "published": "2021-08-16 16:53:39", "link": "http://arxiv.org/abs/2108.07216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BloomNet: A Robust Transformer based model for Bloom's Learning Outcome\n  Classification", "abstract": "Bloom taxonomy is a common paradigm for categorizing educational learning\nobjectives into three learning levels: cognitive, affective, and psychomotor.\nFor the optimization of educational programs, it is crucial to design course\nlearning outcomes (CLOs) according to the different cognitive levels of Bloom\nTaxonomy. Usually, administrators of the institutions manually complete the\ntedious work of mapping CLOs and examination questions to Bloom taxonomy\nlevels. To address this issue, we propose a transformer-based model named\nBloomNet that captures linguistic as well semantic information to classify the\ncourse learning outcomes (CLOs). We compare BloomNet with a diverse set of\nbasic as well as strong baselines and we observe that our model performs better\nthan all the experimented baselines. Further, we also test the generalization\ncapability of BloomNet by evaluating it on different distributions which our\nmodel does not encounter during training and we observe that our model is less\nsusceptible to distribution shift compared to the other considered models. We\nsupport our findings by performing extensive result analysis. In ablation study\nwe observe that on explicitly encapsulating the linguistic information along\nwith semantic information improves the model on IID (independent and\nidentically distributed) performance as well as OOD (out-of-distribution)\ngeneralization capability.", "published": "2021-08-16 17:31:44", "link": "http://arxiv.org/abs/2108.07249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An NLP approach to quantify dynamic salience of predefined topics in a\n  text corpus", "abstract": "The proliferation of news media available online simultaneously presents a\nvaluable resource and significant challenge to analysts aiming to profile and\nunderstand social and cultural trends in a geographic location of interest.\nWhile an abundance of news reports documenting significant events, trends, and\nresponses provides a more democratized picture of the social characteristics of\na location, making sense of an entire corpus to extract significant trends is a\nsteep challenge for any one analyst or team. Here, we present an approach using\nnatural language processing techniques that seeks to quantify how a set of\npre-defined topics of interest change over time across a large corpus of text.\nWe found that, given a predefined topic, we can identify and rank sets of\nterms, or n-grams, that map to those topics and have usage patterns that\ndeviate from a normal baseline. Emergence, disappearance, or significant\nvariations in n-gram usage present a ground-up picture of a topic's dynamic\nsalience within a corpus of interest.", "published": "2021-08-16 21:00:06", "link": "http://arxiv.org/abs/2108.07345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering Filipino Disaster-Related Tweets Using Incremental and\n  Density-Based Spatiotemporal Algorithm with Support Vector Machines for Needs\n  Assessment", "abstract": "Social media has played a huge part on how people get informed and\ncommunicate with one another. It has helped people express their needs due to\ndistress especially during disasters. Because posts made through it are\npublicly accessible by default, Twitter is among the most helpful social media\nsites in times of disaster. With this, the study aims to assess the needs\nexpressed during calamities by Filipinos on Twitter. Data were gathered and\nclassified as either disaster-related or unrelated with the use of Na\\\"ive\nBayes classifier. After this, the disaster-related tweets were clustered per\ndisaster type using Incremental Clustering Algorithm, and then sub-clustered\nbased on the location and time of the tweet using Density-based Spatiotemporal\nClustering Algorithm. Lastly, using Support Vector Machines, the tweets were\nclassified according to the expressed need, such as shelter, rescue, relief,\ncash, prayer, and others. After conducting the study, results showed that the\nIncremental Clustering Algorithm and Density-Based Spatiotemporal Clustering\nAlgorithm were able to cluster the tweets with f-measure scores of 47.20% and\n82.28% respectively. Also, the Na\\\"ive Bayes and Support Vector Machines were\nable to classify with an average f-measure score of 97% and an average accuracy\nof 77.57% respectively.", "published": "2021-08-16 01:38:15", "link": "http://arxiv.org/abs/2108.06853v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Effective System for Multi-format Information Extraction", "abstract": "The multi-format information extraction task in the 2021 Language and\nIntelligence Challenge is designed to comprehensively evaluate information\nextraction from different dimensions. It consists of an multiple slots relation\nextraction subtask and two event extraction subtasks that extract events from\nboth sentence-level and document-level. Here we describe our system for this\nmulti-format information extraction competition task. Specifically, for the\nrelation extraction subtask, we convert it to a traditional triple extraction\ntask and design a voting based method that makes full use of existing models.\nFor the sentence-level event extraction subtask, we convert it to a NER task\nand use a pointer labeling based method for extraction. Furthermore,\nconsidering the annotated trigger information may be helpful for event\nextraction, we design an auxiliary trigger recognition model and use the\nmulti-task learning mechanism to integrate the trigger features into the event\nextraction model. For the document-level event extraction subtask, we design an\nEncoder-Decoder based method and propose a Transformer-alike decoder.\nFinally,our system ranks No.4 on the test set leader-board of this multi-format\ninformation extraction task, and its F1 scores for the subtasks of relation\nextraction, event extractions of sentence-level and document-level are 79.887%,\n85.179%, and 70.828% respectively. The codes of our model are available at\n{https://github.com/neukg/MultiIE}.", "published": "2021-08-16 08:25:17", "link": "http://arxiv.org/abs/2108.06957v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Effective Non-Autoregressive Model for Spoken Language Understanding", "abstract": "Spoken Language Understanding (SLU), a core component of the task-oriented\ndialogue system, expects a shorter inference latency due to the impatience of\nhumans. Non-autoregressive SLU models clearly increase the inference speed but\nsuffer uncoordinated-slot problems caused by the lack of sequential dependency\ninformation among each slot chunk. To gap this shortcoming, in this paper, we\npropose a novel non-autoregressive SLU model named Layered-Refine Transformer,\nwhich contains a Slot Label Generation (SLG) task and a Layered Refine\nMechanism (LRM). SLG is defined as generating the next slot label with the\ntoken sequence and generated slot labels. With SLG, the non-autoregressive\nmodel can efficiently obtain dependency information during training and spend\nno extra time in inference. LRM predicts the preliminary SLU results from\nTransformer's middle states and utilizes them to guide the final prediction.\nExperiments on two public datasets indicate that our model significantly\nimproves SLU performance (1.5\\% on Overall accuracy) while substantially speed\nup (more than 10 times) the inference process over the state-of-the-art\nbaseline.", "published": "2021-08-16 10:26:57", "link": "http://arxiv.org/abs/2108.07005v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and\n  Intra-modal Knowledge Integration", "abstract": "Vision-and-language pretraining (VLP) aims to learn generic multimodal\nrepresentations from massive image-text pairs. While various successful\nattempts have been proposed, learning fine-grained semantic alignments between\nimage-text pairs plays a key role in their approaches. Nevertheless, most\nexisting VLP approaches have not fully utilized the intrinsic knowledge within\nthe image-text pairs, which limits the effectiveness of the learned alignments\nand further restricts the performance of their models. To this end, we\nintroduce a new VLP method called ROSITA, which integrates the cross- and\nintra-modal knowledge in a unified scene graph to enhance the semantic\nalignments. Specifically, we introduce a novel structural knowledge masking\n(SKM) strategy to use the scene graph structure as a priori to perform masked\nlanguage (region) modeling, which enhances the semantic alignments by\neliminating the interference information within and across modalities.\nExtensive ablation studies and comprehensive analysis verifies the\neffectiveness of ROSITA in semantic alignments. Pretrained with both in-domain\nand out-of-domain datasets, ROSITA significantly outperforms existing\nstate-of-the-art VLP methods on three typical vision-and-language tasks over\nsix benchmark datasets.", "published": "2021-08-16 13:16:58", "link": "http://arxiv.org/abs/2108.07073v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MMChat: Multi-Modal Chat Dataset on Social Media", "abstract": "Incorporating multi-modal contexts in conversation is important for\ndeveloping more engaging dialogue systems. In this work, we explore this\ndirection by introducing MMChat: a large-scale Chinese multi-modal dialogue\ncorpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous\ncorpora that are crowd-sourced or collected from fictitious movies, MMChat\ncontains image-grounded dialogues collected from real conversations on social\nmedia, in which the sparsity issue is observed. Specifically, image-initiated\ndialogues in common communications may deviate to some non-image-grounded\ntopics as the conversation proceeds. To better investigate this issue, we\nmanually annotate 100K dialogues from MMChat and further filter the corpus\naccordingly, which yields MMChat-hf. We develop a benchmark model to address\nthe sparsity issue in dialogue generation tasks by adapting the attention\nrouting mechanism on image features. Experiments demonstrate the usefulness of\nincorporating image features and the effectiveness of handling the sparsity of\nimage features.", "published": "2021-08-16 15:27:49", "link": "http://arxiv.org/abs/2108.07154v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Generative Relation Linking for Question Answering over Knowledge Bases", "abstract": "Relation linking is essential to enable question answering over knowledge\nbases. Although there are various efforts to improve relation linking\nperformance, the current state-of-the-art methods do not achieve optimal\nresults, therefore, negatively impacting the overall end-to-end question\nanswering performance. In this work, we propose a novel approach for relation\nlinking framing it as a generative problem facilitating the use of pre-trained\nsequence-to-sequence models. We extend such sequence-to-sequence models with\nthe idea of infusing structured data from the target knowledge base, primarily\nto enable these models to handle the nuances of the knowledge base. Moreover,\nwe train the model with the aim to generate a structured output consisting of a\nlist of argument-relation pairs, enabling a knowledge validation step. We\ncompared our method against the existing relation linking systems on four\ndifferent datasets derived from DBpedia and Wikidata. Our method reports large\nimprovements over the state-of-the-art while using a much simpler model that\ncan be easily adapted to different knowledge bases.", "published": "2021-08-16 20:33:43", "link": "http://arxiv.org/abs/2108.07337v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IsoScore: Measuring the Uniformity of Embedding Space Utilization", "abstract": "The recent success of distributed word representations has led to an\nincreased interest in analyzing the properties of their spatial distribution.\nSeveral studies have suggested that contextualized word embedding models do not\nisotropically project tokens into vector space. However, current methods\ndesigned to measure isotropy, such as average random cosine similarity and the\npartition score, have not been thoroughly analyzed and are not appropriate for\nmeasuring isotropy. We propose IsoScore: a novel tool that quantifies the\ndegree to which a point cloud uniformly utilizes the ambient vector space.\nUsing rigorously designed tests, we demonstrate that IsoScore is the only tool\navailable in the literature that accurately measures how uniformly distributed\nvariance is across dimensions in vector space. Additionally, we use IsoScore to\nchallenge a number of recent conclusions in the NLP literature that have been\nderived using brittle metrics of isotropy. We caution future studies from using\nexisting tools to measure isotropy in contextualized embedding space as\nresulting conclusions will be misleading or altogether inaccurate.", "published": "2021-08-16 20:58:54", "link": "http://arxiv.org/abs/2108.07344v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reusable Templates and Guides For Documenting Datasets and Models for\n  Natural Language Processing and Generation: A Case Study of the HuggingFace\n  and GEM Data and Model Cards", "abstract": "Developing documentation guidelines and easy-to-use templates for datasets\nand models is a challenging task, especially given the variety of backgrounds,\nskills, and incentives of the people involved in the building of natural\nlanguage processing (NLP) tools. Nevertheless, the adoption of standard\ndocumentation practices across the field of NLP promotes more accessible and\ndetailed descriptions of NLP datasets and models, while supporting researchers\nand developers in reflecting on their work. To help with the standardization of\ndocumentation, we present two case studies of efforts that aim to develop\nreusable documentation templates -- the HuggingFace data card, a general\npurpose card for datasets in NLP, and the GEM benchmark data and model cards\nwith a focus on natural language generation. We describe our process for\ndeveloping these templates, including the identification of relevant\nstakeholder groups, the definition of a set of guiding principles, the use of\nexisting templates as our foundation, and iterative revisions based on\nfeedback.", "published": "2021-08-16 23:15:09", "link": "http://arxiv.org/abs/2108.07374v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Hybrid deep learning methods for phenotype prediction from clinical\n  notes", "abstract": "Identifying patient cohorts from clinical notes in secondary electronic\nhealth records is a fundamental task in clinical information management.\nHowever, with the growing number of clinical notes, it becomes challenging to\nanalyze the data manually for phenotype detection. Automatic extraction of\nclinical concepts would helps to identify the patient phenotypes correctly.\nThis paper proposes a novel hybrid model for automatically extracting patient\nphenotypes using natural language processing and deep learning models to\ndetermine the patient phenotypes without dictionaries and human intervention.\nThe model is based on a neural bidirectional sequence model (BiLSTM or BiGRU)\nand a CNN layer for phenotypes identification. An extra CNN layer is run\nparallel to the hybrid model to extract more features related to each\nphenotype. We used pre-trained embeddings such as FastText and Word2vec\nseparately as the input layers to evaluate other embedding's performance.\nExperimental results using MIMIC III database in internal comparison\ndemonstrate that the proposed model achieved significant performance\nimprovement over existing models. The enhanced version of our model with an\nextra CNN layer obtained a relatively higher F1-score than the original hybrid\nmodel. We also showed that BiGRU layer with FastText embedding had better\nperformance than BiLSTM layer to identify patient phenotypes.", "published": "2021-08-16 05:57:28", "link": "http://arxiv.org/abs/2108.10682v3", "categories": ["cs.CL", "cs.IR", "68T50, 68T07", "J.3; I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "Misleading the Covid-19 vaccination discourse on Twitter: An exploratory\n  study of infodemic around the pandemic", "abstract": "In this work, we collect a moderate-sized representative corpus of tweets\n(200,000 approx.) pertaining Covid-19 vaccination spanning over a period of\nseven months (September 2020 - March 2021). Following a Transfer Learning\napproach, we utilize the pre-trained Transformer-based XLNet model to classify\ntweets as Misleading or Non-Misleading and validate against a random subset of\nresults manually. We build on this to study and contrast the characteristics of\ntweets in the corpus that are misleading in nature against non-misleading ones.\nThis exploratory analysis enables us to design features (such as sentiments,\nhashtags, nouns, pronouns, etc) that can, in turn, be exploited for classifying\ntweets as (Non-)Misleading using various ML models in an explainable manner.\nSpecifically, several ML models are employed for prediction, with up to 90%\naccuracy, and the importance of each feature is explained using SHAP\nExplainable AI (XAI) tool. While the thrust of this work is principally\nexploratory analysis in order to obtain insights on the online discourse on\nCovid-19 vaccination, we conclude the paper by outlining how these insights\nprovide the foundations for a more actionable approach to mitigate\nmisinformation. The curated dataset and code is made available (Github\nrepository) so that the research community at large can reproduce, compare\nagainst, or build upon this work.", "published": "2021-08-16 17:02:18", "link": "http://arxiv.org/abs/2108.10735v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Deep Natural Language Processing for LinkedIn Search", "abstract": "Many search systems work with large amounts of natural language data, e.g.,\nsearch queries, user profiles, and documents. Building a successful search\nsystem requires a thorough understanding of textual data semantics, where deep\nlearning based natural language processing techniques (deep NLP) can be of\ngreat help. In this paper, we introduce a comprehensive study for applying deep\nNLP techniques to five representative tasks in search systems: query intent\nprediction (classification), query tagging (sequential tagging), document\nranking (ranking), query auto completion (language modeling), and query\nsuggestion (sequence to sequence). We also introduce BERT pre-training as a\nsixth task that can be applied to many of the other tasks. Through the model\ndesign and experiments of the six tasks, readers can find answers to four\nimportant questions: (1). When is deep NLP helpful/not helpful in search\nsystems? (2). How to address latency challenges? (3). How to ensure model\nrobustness? This work builds on existing efforts of LinkedIn search, and is\ntested at scale on LinkedIn's commercial search engines. We believe our\nexperiences can provide useful insights for the industry and research\ncommunities.", "published": "2021-08-16 23:37:33", "link": "http://arxiv.org/abs/2108.13300v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "AutoChart: A Dataset for Chart-to-Text Generation Task", "abstract": "The analytical description of charts is an exciting and important research\narea with many applications in academia and industry. Yet, this challenging\ntask has received limited attention from the computational linguistics research\ncommunity. This paper proposes \\textsf{AutoChart}, a large dataset for the\nanalytical description of charts, which aims to encourage more research into\nthis important area. Specifically, we offer a novel framework that generates\nthe charts and their analytical description automatically. We conducted\nextensive human and machine evaluations on the generated charts and\ndescriptions and demonstrate that the generated texts are informative,\ncoherent, and relevant to the corresponding charts.", "published": "2021-08-16 05:01:46", "link": "http://arxiv.org/abs/2108.06897v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Who's Waldo? Linking People Across Text and Images", "abstract": "We present a task and benchmark dataset for person-centric visual grounding,\nthe problem of linking between people named in a caption and people pictured in\nan image. In contrast to prior work in visual grounding, which is predominantly\nobject-based, our new task masks out the names of people in captions in order\nto encourage methods trained on such image-caption pairs to focus on contextual\ncues (such as rich interactions between multiple people), rather than learning\nassociations between names and appearances. To facilitate this task, we\nintroduce a new dataset, Who's Waldo, mined automatically from image-caption\ndata on Wikimedia Commons. We propose a Transformer-based method that\noutperforms several strong baselines on this task, and are releasing our data\nto the research community to spur work on contextual models that consider both\nvision and language.", "published": "2021-08-16 17:36:49", "link": "http://arxiv.org/abs/2108.07253v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Investigation of lightweight acoustic curtains for mid-to-high frequency\n  noise insulations", "abstract": "The continuous surge of environmental noise levels has become a vital\nchallenge for humanity. Earlier studies have reported that prolonged exposure\nto loud noise may cause auditory and non-auditory disorders. Therefore, there\nis a growing demand for suitable noise barriers. Herein, we have investigated\nseveral commercially available curtain fabrics' acoustic performance,\npotentially used for sound insulation purposes. Thorough experimental\ninvestigations have been performed on PVC coated polyester fabrics' acoustical\nperformances and 100 % pure PVC sheets. The PVC-coated polyester fabric\nexhibited better sound insulation properties, particularly in the mid-to-high\nfrequency range (600-1600 Hz) with a transmission loss of about 11 to 22 dB,\nwhile insertion loss of > 10 dB has been achieved. Also, the acoustic\nperformance of multi-layer curtains has been investigated. These multi-layer\ncurtains have shown superior acoustic properties to that of single-layer\nacoustic curtains.", "published": "2021-08-16 05:35:01", "link": "http://arxiv.org/abs/2108.10683v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Convolutive Prediction for Reverberant Speech Separation", "abstract": "We investigate the effectiveness of convolutive prediction, a novel\nformulation of linear prediction for speech dereverberation, for speaker\nseparation in reverberant conditions. The key idea is to first use a deep\nneural network (DNN) to estimate the direct-path signal of each speaker, and\nthen identify delayed and decayed copies of the estimated direct-path signal.\nSuch copies are likely due to reverberation, and can be directly removed for\ndereverberation or used as extra features for another DNN to perform better\ndereverberation and separation. To identify such copies, we solve a linear\nregression problem per frequency efficiently in the time-frequency (T-F) domain\nto estimate the underlying room impulse response (RIR). In the multi-channel\nextension, we perform minimum variance distortionless response (MVDR)\nbeamforming on the outputs of convolutive prediction. The beamforming and\ndereverberation results are used as extra features for a second DNN to perform\nbetter separation and dereverberation. State-of-the-art results are obtained on\nthe SMS-WSJ corpus.", "published": "2021-08-16 16:05:46", "link": "http://arxiv.org/abs/2108.07194v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolutive Prediction for Monaural Speech Dereverberation and\n  Noisy-Reverberant Speaker Separation", "abstract": "A promising approach for speech dereverberation is based on supervised\nlearning, where a deep neural network (DNN) is trained to predict the direct\nsound from noisy-reverberant speech. This data-driven approach is based on\nleveraging prior knowledge of clean speech patterns and seldom explicitly\nexploits the linear-filter structure in reverberation, i.e., that reverberation\nresults from a linear convolution between a room impulse response (RIR) and a\ndry source signal. In this work, we propose to exploit this linear-filter\nstructure within a deep learning based monaural speech dereverberation\nframework. The key idea is to first estimate the direct-path signal of the\ntarget speaker using a DNN and then identify signals that are decayed and\ndelayed copies of the estimated direct-path signal, as these can be reliably\nconsidered as reverberation. They can be either directly removed for\ndereverberation, or used as extra features for another DNN to perform better\ndereverberation. To identify the copies, we estimate the underlying filter (or\nRIR) by efficiently solving a linear regression problem per frequency in the\ntime-frequency domain. We then modify the proposed algorithm for speaker\nseparation in reverberant and noisy-reverberant conditions. State-of-the-art\nspeech dereverberation and speaker separation results are obtained on the\nREVERB, SMS-WSJ, and WHAMR! datasets.", "published": "2021-08-16 23:52:05", "link": "http://arxiv.org/abs/2108.07376v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GC-TTS: Few-shot Speaker Adaptation with Geometric Constraints", "abstract": "Few-shot speaker adaptation is a specific Text-to-Speech (TTS) system that\naims to reproduce a novel speaker's voice with a few training data. While\nnumerous attempts have been made to the few-shot speaker adaptation system,\nthere is still a gap in terms of speaker similarity to the target speaker\ndepending on the amount of data. To bridge the gap, we propose GC-TTS which\nachieves high-quality speaker adaptation with significantly improved speaker\nsimilarity. Specifically, we leverage two geometric constraints to learn\ndiscriminative speaker representations. Here, a TTS model is pre-trained for\nbase speakers with a sufficient amount of data, and then fine-tuned for novel\nspeakers on a few minutes of data with two geometric constraints. Two geometric\nconstraints enable the model to extract discriminative speaker embeddings from\nlimited data, which leads to the synthesis of intelligible speech. We discuss\nand verify the effectiveness of GC-TTS by comparing it with popular and\nessential methods. The experimental results demonstrate that GC-TTS generates\nhigh-quality speech from only a few minutes of training data, outperforming\nstandard techniques in terms of speaker similarity to the target speaker.", "published": "2021-08-16 04:25:31", "link": "http://arxiv.org/abs/2108.06890v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language-Independent Approach for Automatic Computation of Vowel\n  Articulation Features in Dysarthric Speech Assessment", "abstract": "Imprecise vowel articulation can be observed in people with Parkinson's\ndisease (PD). Acoustic features measuring vowel articulation have been\ndemonstrated to be effective indicators of PD in its assessment. Standard\nclinical vowel articulation features of vowel working space area (VSA), vowel\narticulation index (VAI) and formants centralization ratio (FCR), are derived\nthe first two formants of the three corner vowels /a/, /i/ and /u/.\nConventionally, manual annotation of the corner vowels from speech data is\nrequired before measuring vowel articulation. This process is time-consuming.\nThe present work aims to reduce human effort in clinical analysis of PD speech\nby proposing an automatic pipeline for vowel articulation assessment. The\nmethod is based on automatic corner vowel detection using a language universal\nphoneme recognizer, followed by statistical analysis of the formant data. The\napproach removes the restrictions of prior knowledge of speaking content and\nthe language in question. Experimental results on a Finnish PD speech corpus\ndemonstrate the efficacy and reliability of the proposed automatic method in\nderiving VAI, VSA, FCR and F2i/F2u (the second formant ratio for vowels /i/ and\n/u/). The automatically computed parameters are shown to be highly correlated\nwith features computed with manual annotations of corner vowels. In addition,\nautomatically and manually computed vowel articulation features have comparable\ncorrelations with experts' ratings on speech intelligibility, voice impairment\nand overall severity of communication disorder. Language-independence of the\nproposed approach is further validated on a Spanish PD database, PC-GITA, as\nwell as on TORGO corpus of English dysarthric speech.", "published": "2021-08-16 07:35:52", "link": "http://arxiv.org/abs/2108.06943v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "NIST SRE CTS Superset: A large-scale dataset for telephony speaker\n  recognition", "abstract": "This document provides a brief description of the National Institute of\nStandards and Technology (NIST) speaker recognition evaluation (SRE)\nconversational telephone speech (CTS) Superset. The CTS Superset has been\ncreated in an attempt to provide the research community with a large-scale\ndataset along with uniform metadata that can be used to effectively train and\ndevelop telephony (narrowband) speaker recognition systems. It contains a large\nnumber of telephony speech segments from more than 6800 speakers with speech\ndurations distributed uniformly in the [10s, 60s] range. The segments have been\nextracted from the source corpora used to compile prior SRE datasets\n(SRE1996-2012), including the Greybeard corpus as well as the Switchboard and\nMixer series collected by the Linguistic Data Consortium (LDC). In addition to\nthe brief description, we also report speaker recognition results on the NIST\n2020 CTS Speaker Recognition Challenge, obtained using a system trained with\nthe CTS Superset. The results will serve as a reference baseline for the\nchallenge.", "published": "2021-08-16 14:39:23", "link": "http://arxiv.org/abs/2108.07118v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Precision and accuracy of acoustic gunshot location in an urban\n  environment", "abstract": "The muzzle blast caused by the discharge of a firearm generates a loud,\nimpulsive sound that propagates away from the shooter in all directions. The\nlocation of the source can be computed from time-of-arrival measurements of the\nmuzzle blast on multiple acoustic sensors at known locations, a technique known\nas multilateration. The multilateration problem is considerably simplified by\nassuming straight-line propagation in a homogeneous medium, a model for which\nthere are multiple published solutions. Live-fire tests of the ShotSpotter\ngunshot location system in Pittsburgh, PA were analyzed off-line under several\nalgorithms and geometric constraints to evaluate the accuracy of acoustic\nmultilateration in a forensic context. Best results were obtained using the\nalgorithm due to Mathias, Leonari and Galati under a two-dimensional geometric\nconstraint. Multilateration on random subsets of the participating sensor array\nshow that 96% of shots can be located to an accuracy of 15 m or better when six\nor more sensors participate in the solution.", "published": "2021-08-16 23:54:57", "link": "http://arxiv.org/abs/2108.07377v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
