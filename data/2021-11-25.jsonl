{"title": "Probabilistic Impact Score Generation using Ktrain-BERT to Identify Hate\n  Words from Twitter Discussions", "abstract": "Social media has seen a worrying rise in hate speech in recent times.\nBranching to several distinct categories of cyberbullying, gender\ndiscrimination, or racism, the combined label for such derogatory content can\nbe classified as toxic content in general. This paper presents experimentation\nwith a Keras wrapped lightweight BERT model to successfully identify hate\nspeech and predict probabilistic impact score for the same to extract the\nhateful words within sentences. The dataset used for this task is the Hate\nSpeech and Offensive Content Detection (HASOC 2021) data from FIRE 2021 in\nEnglish. Our system obtained a validation accuracy of 82.60%, with a maximum\nF1-Score of 82.68%. Subsequently, our predictive cases performed significantly\nwell in generating impact scores for successful identification of the hate\ntweets as well as the hateful words from tweet pools.", "published": "2021-11-25 06:35:49", "link": "http://arxiv.org/abs/2111.12939v2", "categories": ["cs.CL", "15-04", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Near-Zero-Shot Suggestion Mining with a Little Help from WordNet", "abstract": "In this work, we explore the constructive side of online reviews: advice,\ntips, requests, and suggestions that users provide about goods, venues,\nservices, and other items of interest. To reduce training costs and annotation\nefforts needed to build a classifier for a specific label set, we present and\nevaluate several entailment-based zero-shot approaches to suggestion\nclassification in a label-fully-unseen fashion. In particular, we introduce the\nstrategy of assigning target class labels to sentences in English language with\nuser intentions, which significantly improves prediction quality. The proposed\nstrategies are evaluated with a comprehensive experimental study that validated\nour results both quantitatively and qualitatively.", "published": "2021-11-25 07:40:45", "link": "http://arxiv.org/abs/2111.12956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Korean Pretrained Language Models: A Survey on Three\n  Years of Progress", "abstract": "With the advent of Transformer, which was used in translation models in 2017,\nattention-based architectures began to attract attention. Furthermore, after\nthe emergence of BERT, which strengthened the NLU-specific encoder part, which\nis a part of the Transformer, and the GPT architecture, which strengthened the\nNLG-specific decoder part, various methodologies, data, and models for learning\nthe Pretrained Language Model began to appear. Furthermore, in the past three\nyears, various Pretrained Language Models specialized for Korean have appeared.\nIn this paper, we intend to numerically and qualitatively compare and analyze\nvarious Korean PLMs released to the public.", "published": "2021-11-25 16:37:24", "link": "http://arxiv.org/abs/2112.03014v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Less is More: Generating Grounded Navigation Instructions from Landmarks", "abstract": "We study the automatic generation of navigation instructions from 360-degree\nimages captured on indoor routes. Existing generators suffer from poor visual\ngrounding, causing them to rely on language priors and hallucinate objects. Our\nMARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a\nfirst stage landmark detector and a second stage generator -- a multimodal,\nmultilingual, multitask encoder-decoder. To train it, we bootstrap grounded\nlandmark annotations on top of the Room-across-Room (RxR) dataset. Using text\nparsers, weak supervision from RxR's pose traces, and a multilingual image-text\nencoder trained on 1.8b images, we identify 971k English, Hindi and Telugu\nlandmark descriptions and ground them to specific regions in panoramas. On\nRoom-to-Room, human wayfinders obtain success rates (SR) of 71% following\nMARKY-MT5's instructions, just shy of their 75% SR following human instructions\n-- and well above SRs with other generators. Evaluations on RxR's longer,\ndiverse paths obtain 61-64% SRs on three languages. Generating such\nhigh-quality navigation instructions in novel environments is a step towards\nconversational navigation tools and could facilitate larger-scale training of\ninstruction-following agents.", "published": "2021-11-25 02:20:12", "link": "http://arxiv.org/abs/2111.12872v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TunBERT: Pretrained Contextualized Text Representation for Tunisian\n  Dialect", "abstract": "Pretrained contextualized text representation models learn an effective\nrepresentation of a natural language to make it machine understandable. After\nthe breakthrough of the attention mechanism, a new generation of pretrained\nmodels have been proposed achieving good performances since the introduction of\nthe Transformer. Bidirectional Encoder Representations from Transformers (BERT)\nhas become the state-of-the-art model for language understanding. Despite their\nsuccess, most of the available models have been trained on Indo-European\nlanguages however similar research for under-represented languages and dialects\nremains sparse.\n  In this paper, we investigate the feasibility of training monolingual\nTransformer-based language models for under represented languages, with a\nspecific focus on the Tunisian dialect. We evaluate our language model on\nsentiment analysis task, dialect identification task and reading comprehension\nquestion-answering task. We show that the use of noisy web crawled data instead\nof structured data (Wikipedia, articles, etc.) is more convenient for such\nnon-standardized language. Moreover, results indicate that a relatively small\nweb crawled dataset leads to performances that are as good as those obtained\nusing larger datasets. Finally, our best performing TunBERT model reaches or\nimproves the state-of-the-art in all three downstream tasks. We release the\nTunBERT pretrained model and the datasets used for fine-tuning.", "published": "2021-11-25 15:49:50", "link": "http://arxiv.org/abs/2111.13138v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identification of Bias Against People with Disabilities in Sentiment\n  Analysis and Toxicity Detection Models", "abstract": "Sociodemographic biases are a common problem for natural language processing,\naffecting the fairness and integrity of its applications. Within sentiment\nanalysis, these biases may undermine sentiment predictions for texts that\nmention personal attributes that unbiased human readers would consider neutral.\nSuch discrimination can have great consequences in the applications of\nsentiment analysis both in the public and private sectors. For example,\nincorrect inferences in applications like online abuse and opinion analysis in\nsocial media platforms can lead to unwanted ramifications, such as wrongful\ncensoring, towards certain populations. In this paper, we address the\ndiscrimination against people with disabilities, PWD, done by sentiment\nanalysis and toxicity classification models. We provide an examination of\nsentiment and toxicity analysis models to understand in detail how they\ndiscriminate PWD. We present the Bias Identification Test in Sentiments (BITS),\na corpus of 1,126 sentences designed to probe sentiment analysis models for\nbiases in disability. We use this corpus to demonstrate statistically\nsignificant biases in four widely used sentiment analysis tools (TextBlob,\nVADER, Google Cloud Natural Language API and DistilBERT) and two toxicity\nanalysis models trained to predict toxic comments on Jigsaw challenges (Toxic\ncomment classification and Unintended Bias in Toxic comments). The results show\nthat all exhibit strong negative biases on sentences that mention disability.\nWe publicly release BITS Corpus for others to identify potential biases against\ndisability in any sentiment analysis tools and also to update the corpus to be\nused as a test for other sociodemographic variables as well.", "published": "2021-11-25 21:44:18", "link": "http://arxiv.org/abs/2111.13259v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "New Approaches to Long Document Summarization: Fourier Transform Based\n  Attention in a Transformer Model", "abstract": "In this work, we extensively redesign the newly introduced method of token\nmixing using Fourier Transforms (FNET) to replace the computationally expensive\nself-attention mechanism in a full transformer implementation on a long\ndocument summarization task (> 512 tokens). As a baseline, we also carried out\nlong document summarization using established methods such as Longformer and\nBig Bird transformer models that are capable of processing over 8000 tokens and\nare currently the state of the art methods for these type of problems. The\noriginal FNET paper implemented this in an encoder only architecture while\nabstractive summarization requires both an encoder and a decoder. Since such a\npretrained transformer model does not currently exist in the public domain, we\ndecided to implement a full transformer based on this Fourier token mixing\napproach in an encoder/decoder architecture which we trained starting with\nGlove embeddings for the individual words in the corpus. We investigated a\nnumber of different extensions to the original FNET architecture and evaluated\nthem on their Rouge F1-score performance on a summarization task. All\nmodifications showed better performance on the summarization task than when\nusing the original FNET encoder in a transformer architecture.", "published": "2021-11-25 18:03:41", "link": "http://arxiv.org/abs/2111.15473v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiPD: Disruptive event Prediction Dataset from Twitter", "abstract": "Riots and protests, if gone out of control, can cause havoc in a country. We\nhave seen examples of this, such as the BLM movement, climate strikes, CAA\nMovement, and many more, which caused disruption to a large extent. Our motive\nbehind creating this dataset was to use it to develop machine learning systems\nthat can give its users insight into the trending events going on and alert\nthem about the events that could lead to disruption in the nation. If any event\nstarts going out of control, it can be handled and mitigated by monitoring it\nbefore the matter escalates. This dataset collects tweets of past or ongoing\nevents known to have caused disruption and labels these tweets as 1. We also\ncollect tweets that are considered non-eventful and label them as 0 so that\nthey can also be used to train a classification system. The dataset contains\n94855 records of unique events and 168706 records of unique non-events, thus\ngiving the total dataset 263561 records. We extract multiple features from the\ntweets, such as the user's follower count and the user's location, to\nunderstand the impact and reach of the tweets. This dataset might be useful in\nvarious event related machine learning problems such as event classification,\nevent recognition, and so on.", "published": "2021-11-25 13:16:21", "link": "http://arxiv.org/abs/2111.15629v1", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Global alignment for relation extraction in Microbiology", "abstract": "We investigate a method to extract relations from texts based on global\nalignment and syntactic information. Combined with SVM, this method is shown to\nhave a performance comparable or even better than LSTM on two RE tasks.", "published": "2021-11-25 10:19:05", "link": "http://arxiv.org/abs/2112.02097v1", "categories": ["q-bio.OT", "cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "q-bio.OT"}
{"title": "Does constituency analysis enhance domain-specific pre-trained BERT\n  models for relation extraction?", "abstract": "Recently many studies have been conducted on the topic of relation\nextraction. The DrugProt track at BioCreative VII provides a manually-annotated\ncorpus for the purpose of the development and evaluation of relation extraction\nsystems, in which interactions between chemicals and genes are studied. We\ndescribe the ensemble system that we used for our submission, which combines\npredictions of fine-tuned bioBERT, sciBERT and const-bioBERT models by majority\nvoting. We specifically tested the contribution of syntactic information to\nrelation extraction with BERT. We observed that adding constituentbased\nsyntactic information to BERT improved precision, but decreased recall, since\nrelations rarely seen in the train set were less likely to be predicted by BERT\nmodels in which the syntactic information is infused. Our code is available\nonline [https://github.com/Maple177/drugprot-relation-extraction].", "published": "2021-11-25 10:27:10", "link": "http://arxiv.org/abs/2112.02955v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Polyphonic Sound Event Detection Using Capsule Neural Network on\n  Multi-Type-Multi-Scale Time-Frequency Representation", "abstract": "The challenges of polyphonic sound event detection (PSED) stem from the\ndetection of multiple overlapping events in a time series. Recent efforts\nexploit Deep Neural Networks (DNNs) on Time-Frequency Representations (TFRs) of\naudio clips as model inputs to mitigate such issues. However, existing\nsolutions often rely on a single type of TFR, which causes under-utilization of\ninput features. To this end, we propose a novel PSED framework, which\nincorporates Multi-Type-Multi-Scale TFRs. Our key insight is that: TFRs, which\nare of different types or in different scales, can reveal acoustics patterns in\na complementary manner, so that the overlapped events can be best extracted by\ncombining different TFRs. Moreover, our framework design applies a novel\napproach, to adaptively fuse different models and TFRs symbiotically. Hence,\nthe overall performance can be significantly improved. We quantitatively\nexamine the benefits of our framework by using Capsule Neural Networks, a\nstate-of-the-art approach for PSED. The experimental results show that our\nmethod achieves a reduction of 7\\% in error rate compared with the\nstate-of-the-art solutions on the TUT-SED 2016 dataset.", "published": "2021-11-25 02:10:46", "link": "http://arxiv.org/abs/2111.12869v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A novel time delay estimation algorithm of acoustic pyrometry for\n  furnace", "abstract": "Acoustic pyrometry is a non-contact measurement technology for monitoring\nfurnace combustion reaction, diagnosing energy loss due to incomplete\ncombustion and ensuring safe production. The accuracy of time of flight (TOF)\nestimation of an acoustic pyrometry directly affects the authenticity of\nfurnace temperature measurement. In this paper presented is a novel TOF (i.e.\ntime delay) estimation algorithm based on digital lock-in filtering (DLF)\nalgorithm. In this research, the time-frequency relationship between the first\nharmonic of the acoustic signal and the moment of characteristic frequency\napplied is established through the digital lock-in and low-pass filtering\ntechniques. The accurate estimation of TOF is obtained by extracting and\ncomparing the temporal relationship of the characteristic frequency occurrence\nbetween received and source acoustic signals. The computational error analysis\nindicates that the accuracy of the proposed algorithm is better than that of\nthe classical generalized cross-correlation (GCC) algorithm, and the\ncomputational effort is significantly reduced to half of that the GCC can\noffer. It can be confirmed that with this method, the temperature measurement\nin furnaces can be improved in terms of computational effort and accuracy,\nwhich are vital parameters in furnace combustion control. It provides a new\nidea of time delay estimation with the utilization of acoustic pyrometry for\nfurnace.", "published": "2021-11-25 03:16:41", "link": "http://arxiv.org/abs/2111.12884v2", "categories": ["physics.ins-det", "cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "physics.ins-det"}
{"title": "V2C: Visual Voice Cloning", "abstract": "Existing Voice Cloning (VC) tasks aim to convert a paragraph text to a speech\nwith desired voice specified by a reference audio. This has significantly\nboosted the development of artificial speech applications. However, there also\nexist many scenarios that cannot be well reflected by these VC tasks, such as\nmovie dubbing, which requires the speech to be with emotions consistent with\nthe movie plots. To fill this gap, in this work we propose a new task named\nVisual Voice Cloning (V2C), which seeks to convert a paragraph of text to a\nspeech with both desired voice specified by a reference audio and desired\nemotion specified by a reference video. To facilitate research in this field,\nwe construct a dataset, V2C-Animation, and propose a strong baseline based on\nexisting state-of-the-art (SoTA) VC techniques. Our dataset contains 10,217\nanimated movie clips covering a large variety of genres (e.g., Comedy, Fantasy)\nand emotions (e.g., happy, sad). We further design a set of evaluation metrics,\nnamed MCD-DTW-SL, which help evaluate the similarity between ground-truth\nspeeches and the synthesised ones. Extensive experimental results show that\neven SoTA VC methods cannot generate satisfying speeches for our V2C task. We\nhope the proposed new task together with the constructed dataset and evaluation\nmetric will facilitate the research in the field of voice cloning and the\nbroader vision-and-language community.", "published": "2021-11-25 03:35:18", "link": "http://arxiv.org/abs/2111.12890v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A-Muze-Net: Music Generation by Composing the Harmony based on the\n  Generated Melody", "abstract": "We present a method for the generation of Midi files of piano music. The\nmethod models the right and left hands using two networks, where the left hand\nis conditioned on the right hand. This way, the melody is generated before the\nharmony. The Midi is represented in a way that is invariant to the musical\nscale, and the melody is represented, for the purpose of conditioning the\nharmony, by the content of each bar, viewed as a chord. Finally, notes are\nadded randomly, based on this chord representation, in order to enrich the\ngenerated audio. Our experiments show a significant improvement over the state\nof the art for training on such datasets, and demonstrate the contribution of\neach of the novel components.", "published": "2021-11-25 09:45:53", "link": "http://arxiv.org/abs/2111.12986v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
