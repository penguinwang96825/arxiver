{"title": "Computation of Graph Polynomials via Tree Decomposition: Theory, Algorithms, and Python Implementation", "abstract": "Graph polynomials encode fundamental combinatorial invariants of graphs.\nTheir computation is investigated using tree and path decomposition frameworks,\nwith formal definitions of treewidth, k-trees, and pathwidth establishing the\nstructural basis for algorithmic efficiency. Explicit algorithms are\nconstructed for each polynomial, leveraging decomposition order and state\ntransformation mappings to enable tractable computation on graphs of bounded\ntreewidth. Python implementations validate the methods, and computational\ncomplexity is analyzed with respect to sparse and k-degenerate graph classes.\nThese results advance decomposition-based approaches for polynomial computation\nin algebraic graph theory.", "published": "2025-09-20 21:37:16", "link": "http://arxiv.org/abs/2509.16816v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "Discrepancy And Fair Division For Non-Additive Valuations", "abstract": "We extend the notion of combinatorial discrepancy to \\emph{non-additive}\nfunctions. Our main result is an upper bound of $O(\\sqrt{n \\log(nk)})$ on the\nnon-additive $k$-color discrepancy when $k$ is a prime power. We demonstrate\ntwo applications of this result to problems in fair division. First, we\nestablish a bound for a consensus halving problem, where fairness is measured\nby the minimum number of items that must be transferred between the two parts\nto eliminate envy. Second, we improve the upper bound on the total subsidy\nrequired to achieve an envy-free allocation when the number of agents is a\nprime power, obtaining an $O(n \\sqrt{n \\log n})$ bound. This constitutes the\nfirst known subquadratic guarantee in this setting.", "published": "2025-09-20 20:26:13", "link": "http://arxiv.org/abs/2509.16802v1", "categories": ["cs.GT", "cs.DM"], "primary_category": "cs.GT"}
{"title": "Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook", "abstract": "Technology-enhanced learning environments often help students retrieve\nrelevant learning content for questions arising during self-paced study. Large\nlanguage models (LLMs) have emerged as novel aids for information retrieval\nduring learning. While LLMs are effective for general-purpose\nquestion-answering, they typically lack alignment with the domain knowledge of\nspecific course materials such as textbooks and slides. We investigate\nRetrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced\nRAG approach, for page-level question answering in an undergraduate mathematics\ntextbook. While RAG has been effective for retrieving discrete, contextually\nrelevant passages, GraphRAG may excel in modeling interconnected concepts and\nhierarchical knowledge structures. We curate a dataset of 477 question-answer\npairs, each tied to a distinct textbook page. We then compare the standard\nembedding-based RAG methods to GraphRAG for evaluating both retrieval\naccuracy-whether the correct page is retrieved-and generated answer quality via\nF1 scores. Our findings show that embedding-based RAG achieves higher retrieval\naccuracy and better F1 scores compared to GraphRAG, which tends to retrieve\nexcessive and sometimes irrelevant content due to its entity-based structure.\nWe also explored re-ranking the retrieved pages with LLM and observed mixed\nresults, including performance drop and hallucinations when dealing with larger\ncontext windows. Overall, this study highlights both the promises and\nchallenges of page-level retrieval systems in educational contexts, emphasizing\nthe need for more refined retrieval methods to build reliable AI tutoring\nsolutions in providing reference page numbers.", "published": "2025-09-20 19:06:49", "link": "http://arxiv.org/abs/2509.16780v1", "categories": ["cs.IR", "cs.AI", "cs.HC"], "primary_category": "cs.IR"}
{"title": "The Role of Vocabularies in Learning Sparse Representations for Ranking", "abstract": "Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for\neffective semantic 1st stage matching while enjoying the efficiency of inverted\nindices. A recent work on learning SPLADE models with expanded vocabularies\n(ESPLADE) was proposed to represent queries and documents into a sparse space\nof custom vocabulary which have different levels of vocabularic granularity.\nWithin this effort, however, there have not been many studies on the role of\nvocabulary in SPLADE models and their relationship to retrieval efficiency and\neffectiveness.\n  To study this, we construct BERT models with 100K-sized output vocabularies,\none initialized with the ESPLADE pretraining method and one initialized\nrandomly. After finetune on real-world search click logs, we applied logit\nscore-based queries and documents pruning to max size for further balancing\nefficiency. The experimental result in our evaluation set shows that, when\npruning is applied, the two models are effective compared to the 32K-sized\nnormal SPLADE model in the computational budget under the BM25. And the ESPLADE\nmodels are more effective than the random vocab model, while having a similar\nretrieval cost.\n  The result indicates that the size and pretrained weight of output\nvocabularies play the role of configuring the representational specification\nfor queries, documents, and their interactions in the retrieval engine, beyond\ntheir original meaning and purposes in NLP. These findings can provide a new\nroom for improvement for LSR by identifying the importance of representational\nspecification from vocabulary configuration for efficient and effective\nretrieval.", "published": "2025-09-20 10:44:26", "link": "http://arxiv.org/abs/2509.16621v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Learn to Rank Risky Investors: A Case Study of Predicting Retail Traders' Behaviour and Profitability", "abstract": "Identifying risky traders with high profits in financial markets is crucial\nfor market makers, such as trading exchanges, to ensure effective risk\nmanagement through real-time decisions on regulation compliance and hedging.\nHowever, capturing the complex and dynamic behaviours of individual traders\nposes significant challenges. Traditional classification and anomaly detection\nmethods often establish a fixed risk boundary, failing to account for this\ncomplexity and dynamism. To tackle this issue, we propose a profit-aware risk\nranker (PA-RiskRanker) that reframes the problem of identifying risky traders\nas a ranking task using Learning-to-Rank (LETOR) algorithms. Our approach\nfeatures a Profit-Aware binary cross entropy (PA-BCE) loss function and a\ntransformer-based ranker enhanced with a self-cross-trader attention pipeline.\nThese components effectively integrate profit and loss (P&L) considerations\ninto the training process while capturing intra- and inter-trader\nrelationships. Our research critically examines the limitations of existing\ndeep learning-based LETOR algorithms in trading risk management, which often\noverlook the importance of P&L in financial scenarios. By prioritising P&L, our\nmethod improves risky trader identification, achieving an 8.4% increase in F1\nscore compared to state-of-the-art (SOTA) ranking models like Rankformer.\nAdditionally, it demonstrates a 10%-17% increase in average profit compared to\nall benchmark models.", "published": "2025-09-20 10:41:13", "link": "http://arxiv.org/abs/2509.16616v1", "categories": ["cs.CE", "cs.IR"], "primary_category": "cs.CE"}
{"title": "Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence", "abstract": "Background: Evidence synthesis facilitates evidence-based medicine. Without\ninformation retrieval techniques, this task is impossible due to the vast and\nexpanding literature. Objective: Building on prior work, this study evaluates\nan information retrieval-driven workflow to enhance the efficiency,\ntransparency, and reproducibility of systematic reviews. We use endometriosis\nrecurrence as an ideal case due to its complex and ambiguous literature.\nMethods: Our hybrid approach integrates PRISMA guidelines with computational\ntechniques. We applied semi-automated deduplication to efficiently filter\nrecords before manual screening. This workflow synthesized evidence from\nrandomised controlled trials on the efficacy of a subclass of\ngonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method\naddressed unit-of-analysis errors in multi-arm trials. Results: Our workflow\nefficiently reduced the screening workload. It took only 11 days to fetch and\nfilter 812 records. Seven RCTs were eligible, providing evidence from 841\npatients in 4 countries. The pooled random-effects model yielded a Risk Ratio\n(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity\n($I^2=0.00\\%$, $\\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.\nSensitivity analyses and bias assessments supported the robustness of our\nfindings. Conclusion: This study demonstrates an information-retrieval-driven\nworkflow for medical evidence synthesis. Our approach yields valuable clinical\nresults while providing a framework for accelerating the systematic review\nprocess. It bridges the gap between clinical research and computer science and\ncan be generalized to other complex systematic reviews.", "published": "2025-09-20 09:50:18", "link": "http://arxiv.org/abs/2509.16599v1", "categories": ["cs.CL", "cs.IR", "stat.AP", "stat.ME", "H.3.3; I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning", "abstract": "Human mobility forecasting is important for applications such as\ntransportation planning, urban management, and personalized recommendations.\nHowever, existing methods often fail to generalize to unseen users or locations\nand struggle to capture dynamic intent due to limited labeled data and the\ncomplexity of mobility patterns. We propose ZHMF, a framework for zero-shot\nhuman mobility forecasting that combines a semantic enhanced retrieval and\nreflection mechanism with a hierarchical language model based reasoning system.\nThe task is reformulated as a natural language question answering paradigm.\nLeveraging LLMs semantic understanding of user histories and context, our\napproach handles previously unseen prediction scenarios. We further introduce a\nhierarchical reflection mechanism for iterative reasoning and refinement by\ndecomposing forecasting into an activity level planner and a location level\nselector, enabling collaborative modeling of long term user intentions and\nshort term contextual preferences. Experiments on standard human mobility\ndatasets show that our approach outperforms existing models. Ablation studies\nreveal the contribution of each module, and case studies illustrate how the\nmethod captures user intentions and adapts to diverse contextual scenarios.", "published": "2025-09-20 08:46:38", "link": "http://arxiv.org/abs/2509.16578v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models", "abstract": "Millions of people openly share mental health struggles on social media,\nproviding rich data for early detection of conditions such as depression,\nbipolar disorder, etc. However, most prior Natural Language Processing (NLP)\nresearch has focused on single-disorder identification, leaving a gap in\nunderstanding the efficacy of advanced NLP techniques for distinguishing among\nmultiple mental health conditions. In this work, we present a large-scale\ncomparative study of state-of-the-art transformer versus Long Short-Term Memory\n(LSTM)-based models to classify mental health posts into exclusive categories\nof mental health conditions. We first curate a large dataset of Reddit posts\nspanning six mental health conditions and a control group, using rigorous\nfiltering and statistical exploratory analysis to ensure annotation quality. We\nthen evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,\nALBERT, and ELECTRA) against several LSTM variants (with or without attention,\nusing contextual or static embeddings) under identical conditions. Experimental\nresults show that transformer models consistently outperform the alternatives,\nwith RoBERTa achieving 91-99% F1-scores and accuracies across all classes.\nNotably, attention-augmented LSTMs with BERT embeddings approach transformer\nperformance (up to 97% F1-score) while training 2-3.5 times faster, whereas\nLSTMs using static embeddings fail to learn useful signals. These findings\nrepresent the first comprehensive benchmark for multi-class mental health\ndetection, offering practical guidance on model selection and highlighting an\naccuracy-efficiency trade-off for real-world deployment of mental health NLP\nsystems.", "published": "2025-09-20 05:41:59", "link": "http://arxiv.org/abs/2509.16542v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Long document summarization using page specific target text alignment and distilling page importance", "abstract": "The rapid growth of textual data across news, legal, medical, and scientific\ndomains is becoming a challenge for efficiently accessing and understanding\nlarge volumes of content. It is increasingly complex for users to consume and\nextract meaningful information efficiently. Thus, raising the need for\nsummarization. Unlike short document summarization, long document abstractive\nsummarization is resource-intensive, and very little literature is present in\nthis direction. BART is a widely used efficient sequence-to-sequence\n(seq-to-seq) model. However, when it comes to summarizing long documents, the\nlength of the context window limits its capabilities. We proposed a model\ncalled PTS (Page-specific Target-text alignment Summarization) that extends the\nseq-to-seq method for abstractive summarization by dividing the source document\ninto several pages. PTS aligns each page with the relevant part of the target\nsummary for better supervision. Partial summaries are generated for each page\nof the document. We proposed another model called PTSPI (Page-specific\nTarget-text alignment Summarization with Page Importance), an extension to PTS\nwhere an additional layer is placed before merging the partial summaries into\nthe final summary. This layer provides dynamic page weightage and explicit\nsupervision to focus on the most informative pages. We performed experiments on\nthe benchmark dataset and found that PTSPI outperformed the SOTA by 6.32\\% in\nROUGE-1 and 8.08\\% in ROUGE-2 scores.", "published": "2025-09-20 05:05:34", "link": "http://arxiv.org/abs/2509.16539v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Randomized Space-Time Sampling for Affine Graph Dynamical Systems", "abstract": "This paper investigates the problem of dynamical sampling for graph signals\ninfluenced by a constant source term. We consider signals evolving over time\naccording to a linear dynamical system on a graph, where both the initial state\nand the source term are bandlimited. We introduce two random space-time\nsampling regimes and analyze the conditions under which stable recovery is\nachievable. While our framework extends recent work on homogeneous dynamics, it\naddresses a fundamentally different setting where the evolution includes a\nconstant source term. This results in a non-orthogonal-diagonalizable system\nmatrix, rendering classical spectral techniques inapplicable and introducing\nnew challenges in sampling design, stability analysis, and joint recovery of\nboth the initial state and the forcing term. A key component of our analysis is\nthe spectral graph weighted coherence, which characterizes the interplay\nbetween the sampling distribution and the graph structure. We establish\nsampling complexity bounds ensuring stable recovery via the Restricted Isometry\nProperty (RIP), and develop a robust recovery algorithm with provable error\nguarantees. The effectiveness of our method is validated through extensive\nexperiments on both synthetic and real-world datasets.", "published": "2025-09-20 21:43:05", "link": "http://arxiv.org/abs/2509.16818v1", "categories": ["math.NA", "cs.IT", "cs.LG", "cs.NA", "cs.SY", "eess.SY", "math.IT"], "primary_category": "math.NA"}
{"title": "Machine Learning in Near-Field Communication for 6G: A Survey", "abstract": "6G wireless communication networks are expected to use extremely large-scale\nantenna arrays (ELAAs) to support higher throughput, massive connectivity, and\nimproved system performance. ELAAs would fundamentally alter wave\ncharacteristics, transforming them from plane waves into spherical waves,\nthereby operating in the near field. Near-field communications (NFC) offer\nunique advantages to enhance system performance, but also present significant\nchallenges in channel modeling, computational complexity, and beamforming\ndesign. The use of machine learning (ML) is emerging as a powerful approach to\ntackle such challenges and has the capabilities to enable intelligent, secure,\nand efficient 6G wireless communications. In this survey, we discuss ML-driven\napproaches for NFC. We first outline the fundamental concepts of NFC and ML. We\nthen discuss ML applications in channel estimation, beamforming design, and\nsecurity enhancement. We also highlight key challenges (e.g., data privacy and\ncomputational overhead). Finally, we discuss open issues and future directions\nto emphasize the role of advanced ML techniques in near-field system design.", "published": "2025-09-20 15:22:17", "link": "http://arxiv.org/abs/2509.16723v1", "categories": ["cs.ET", "cs.IT", "math.IT"], "primary_category": "cs.ET"}
{"title": "6DMA-Assisted Secure Wireless Communications", "abstract": "Six-dimensional movable antenna (6DMA) has been widely studied for capacity\nenhancement, but its potential for physical layer security (PLS) remains\nlargely unexplored. By adjusting both three-dimensional (3D) positions and 3D\nrotations of distributed antenna surfaces, 6DMA can increase spatial degrees of\nfreedom (DoFs). The extra DoFs enable dynamic shaping of legitimate channels\nand suppresses eavesdropping channels, thereby offering unique advantages in\nenhancing secrecy performance. Motivated by this, this letter proposes a novel\n6DMA-assisted secure wireless communication system, where the base station (BS)\nis equipped with 6DMA to enhance secrecy performance. Specifically, to\nsimultaneously serve multiple legitimate users and counter cooperative\ninterception by multiple eavesdroppers (Eves), we formulate a sum secrecy rate\n(SSR) maximization problem by jointly optimizing the transmit and artificial\nnoise (AN) beamformers, as well as the 3D positions and 3D rotations of antenna\nsurfaces. To solve this non-convex problem, we propose an alternating\noptimization (AO) algorithm that decomposes the original problem into two\nsubproblems and solves them iteratively to obtain a high-quality suboptimal\nsolution. Simulation results demonstrate the superior secrecy performance over\npartially movable and conventional fixed-position antenna systems.", "published": "2025-09-20 13:51:32", "link": "http://arxiv.org/abs/2509.16698v1", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT"], "primary_category": "eess.SY"}
{"title": "Near-Field Channel Estimation with ELAA Modular Arrays Under Hardware Impairments", "abstract": "Extremely large-scale antenna arrays (ELAAs) enable high spatial resolution\nand multiplexing, especially for user equipments (UEs) in the radiative\nnear-field. To reduce hardware cost, modular ELAA architectures with\ndistributed baseband units (BBUs) are gaining traction. This paper addresses\nnear-field line-of-sight (LOS) channel estimation under low noise amplifier\n(LNA)-induced hardware impairments in such modular systems. We propose\ncomputationally efficient estimators that exploit the array geometry and\nconstant-modulus structure of near-field LOS channels, including a novel\ntwo-dimensional (2D) discrete Fourier transform (DFT) masking technique that\nimproves estimation accuracy and significantly reduces fronthaul signaling.\nNumerical results show that the proposed methods significantly outperform the\nconventional least squares (LS) method.", "published": "2025-09-20 13:31:35", "link": "http://arxiv.org/abs/2509.16688v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "A New Class of Analog Precoding for Multi-Antenna Multi-User Communications over High-Frequency Bands", "abstract": "A network relying on a large antenna-array-aided base station is designed for\ndelivering multiple information streams to multi-antenna users over\nhigh-frequency bands such as the millimeter-wave and sub-Terahertz bands. The\nstate-of-the-art analog precoder (AP) dissipates excessive circuit power due to\nits reliance on a large number of phase shifters. To mitigate the power\nconsumption, we propose a novel AP relying on a controlled number of phase\nshifters. Within this new AP framework, we design a hybrid precoder (HP) for\nmaximizing the users' minimum throughput, which poses a computationally\nchallenging problem of large-scale, nonsmooth mixed discrete-continuous\nlog-determinant optimization. To tackle this challenge, we develop an algorithm\nwhich iterates through solving convex problems to generate a sequence of HPs\nthat converges to the max-min solution. We also introduce a new framework of\nsmooth optimization termed soft max-min throughput optimization. Additionally,\nwe develop another algorithm, which iterates by evaluating closed-form\nexpressions to generate a sequence of HPs that converges to the soft max-min\nsolution. Simulation results reveal that the HP soft max-min solution\napproaches the Pareto-optimal solution constructed for simultaneously\noptimizing both the minimum throughput and sum-throughput. Explicitly, it\nachieves a minimum throughput similar to directly maximizing the users' minimum\nthroughput and it also attains a sum-throughput similar to directly maximizing\nthe sum-throughput.", "published": "2025-09-20 11:15:36", "link": "http://arxiv.org/abs/2509.16634v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Holographic Multi-User Multi-Stream Beamforming Maintaining Rate-Fairness", "abstract": "We present the first investigation into the transmission of multi-stream\ninformation from a base station equipped with reconfigurable holographic\nsurfaces (RHS) to multiple users with the aid of multi-antenna arrays. Building\nupon this, we propose the joint design of RHS and baseband beamformers that\nenables multi-stream delivery at fair rates across all users. Specifically, we\nfirst introduce a max-min rate optimization approach, which aims for maximizing\nthe minimum rate for all users through iterative solutions of quadratic\nproblems. To reduce complexity, we then propose a surrogate-based optimization\napproach that offers a low-complexity design alternative relying on closed-form\nupdates. Our simulations show that the surrogate-based approach achieves nearly\nthe same minimum rate as max-min optimization, while delivering sum-rates\ncomparable to those of sum-rate maximization, overcoming the rate-fairness\ndeficiency typical of the latter.", "published": "2025-09-20 10:26:39", "link": "http://arxiv.org/abs/2509.16612v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Robust Sparse Subspace Tracking from Corrupted Data Observations", "abstract": "Subspace tracking is a fundamental problem in signal processing, where the\ngoal is to estimate and track the underlying subspace that spans a sequence of\ndata streams over time. In high-dimensional settings, data samples are often\ncorrupted by non-Gaussian noises and may exhibit sparsity. This paper explores\nthe alpha divergence for sparse subspace estimation and tracking, offering\nrobustness to data corruption. The proposed method outperforms the\nstate-of-the-art robust subspace tracking methods while achieving a low\ncomputational complexity and memory storage. Several experiments are conducted\nto demonstrate its effectiveness in robust subspace tracking and\ndirection-of-arrival (DOA) estimation.", "published": "2025-09-20 09:17:17", "link": "http://arxiv.org/abs/2509.16585v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Towards Transparent and Incentive-Compatible Collaboration in Decentralized LLM Multi-Agent Systems: A Blockchain-Driven Approach", "abstract": "Large Language Models (LLMs) have enabled the emergence of autonomous agents\ncapable of complex reasoning, planning, and interaction. However, coordinating\nsuch agents at scale remains a fundamental challenge, particularly in\ndecentralized environments where communication lacks transparency and agent\nbehavior cannot be shaped through centralized incentives. We propose a\nblockchain-based framework that enables transparent agent registration,\nverifiable task allocation, and dynamic reputation tracking through smart\ncontracts. The core of our design lies in two mechanisms: a matching\nscore-based task allocation protocol that evaluates agents by reputation,\ncapability match, and workload; and a behavior-shaping incentive mechanism that\nadjusts agent behavior via feedback on performance and reward. Our\nimplementation integrates GPT-4 agents with Solidity contracts and\ndemonstrates, through 50-round simulations, strong task success rates, stable\nutility distribution, and emergent agent specialization. The results underscore\nthe potential for trustworthy, incentive-compatible multi-agent coordination in\nopen environments.", "published": "2025-09-20 16:00:24", "link": "http://arxiv.org/abs/2509.16736v1", "categories": ["cs.MA", "cs.CR"], "primary_category": "cs.MA"}
{"title": "Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning", "abstract": "In networked multi-agent reinforcement learning (Networked-MARL),\ndecentralized agents must act under local observability and constrained\ncommunication over fixed physical graphs. Existing methods often assume static\nneighborhoods, limiting adaptability to dynamic or heterogeneous environments.\nWhile centralized frameworks can learn dynamic graphs, their reliance on global\nstate access and centralized infrastructure is impractical in real-world\ndecentralized systems. We propose a stochastic graph-based policy for\nNetworked-MARL, where each agent conditions its decision on a sampled subgraph\nover its local physical neighborhood. Building on this formulation, we\nintroduce BayesG, a decentralized actor-framework that learns sparse,\ncontext-aware interaction structures via Bayesian variational inference. Each\nagent operates over an ego-graph and samples a latent communication mask to\nguide message passing and policy computation. The variational distribution is\ntrained end-to-end alongside the policy using an evidence lower bound (ELBO)\nobjective, enabling agents to jointly learn both interaction topology and\ndecision-making strategies. BayesG outperforms strong MARL baselines on\nlarge-scale traffic control tasks with up to 167 agents, demonstrating superior\nscalability, efficiency, and performance.", "published": "2025-09-20 10:09:37", "link": "http://arxiv.org/abs/2509.16606v1", "categories": ["cs.MA", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Spectral Analysis of the Weighted Frobenius Objective", "abstract": "We analyze a weighted Frobenius loss for approximating symmetric positive\ndefinite matrices in the context of preconditioning iterative solvers. Unlike\nthe standard Frobenius norm, the weighted loss penalizes error components\nassociated with small eigenvalues of the system matrix more strongly. Our\nanalysis reveals that each eigenmode is scaled by the corresponding square of\nits eigenvalue, and that, under a fixed error budget, the loss is minimized\nonly when the error is confined to the direction of the largest eigenvalue.\nThis provides a rigorous explanation of why minimizing the weighted loss\nnaturally suppresses low-frequency components, which can be a desirable\nstrategy for the conjugate gradient method. The analysis is independent of the\nspecific approximation scheme or sparsity pattern, and applies equally to\nincomplete factorizations, algebraic updates, and learning-based constructions.\nNumerical experiments confirm the predictions of the theory, including an\nillustration where sparse factors are trained by a direct gradient updates to\nIC(0) factor entries, i.e., no trained neural network model is used.", "published": "2025-09-20 19:15:24", "link": "http://arxiv.org/abs/2509.16783v1", "categories": ["math.NA", "cs.LG", "cs.NA"], "primary_category": "math.NA"}
{"title": "Fast and accurate computation of classical Gaussian quadratures", "abstract": "Algorithms for computing classical Gaussian quadrature rules (Gauss-Jacobi,\nGauss-Laguerre, and Gauss-Hermite) are presented, based on globally convergent\nfourth-order iterative methods and asymptotic approximations, which are applied\nin complementary regions of the parameter space. The combination of these\napproaches results in methods that surpass previous algorithms in terms of\nspeed, accuracy, and computational range (practically unrestricted). The\nGauss-Radau and Gauss-Lobatto variants are also considered, along with the\ncomputation of the associated barycentric weights. Arbitrary accuracy\nalgorithms are also provided for the symmetric cases (Gauss-Gegenbauer and\nGauss-Hermite).", "published": "2025-09-20 14:58:52", "link": "http://arxiv.org/abs/2509.16716v1", "categories": ["math.NA", "cs.MS", "cs.NA", "math.CA", "65D32, 33B20, 41A60, 65G10, 65G50 65D32, 33B20, 41A60, 65G10, 65G50\n  65D32, 33B20, 41A60, 65G10, 65G50"], "primary_category": "math.NA"}
{"title": "Computational Modeling of Selective Capture Mechanisms in Conduction System Pacing", "abstract": "CSP is gaining clinical significance owing to its ability to restore a\nphysiological activation sequence in the ventricles. While His bundle pacing\n(HBP) producing the most physiological activation is preferable, due to implant\ncomplications the selective activation of the LBB by left bundle branch area\npacing (LBBAP) is considered an alternative, offering both a simpler implant\nand a physiological activation sequence. However, the physical mechanisms\nfacilitating selective activation of the LBB remain poorly understood. We\ndeveloped a structurally and biophysically detailed computer model of the IVS\nand LBB to quantitatively elucidate the role of lead position, orientation and\npolarity in achieving optimal s-LBBP thresholds, using a geometrically detailed\nmodel of a clinically widely used CSP lead. A deep implant within the LV\nsub-endocardium ensuring a direct contact between electrode and LBB is key for\neffective s-LBBP. For low strength s-LBBP is feasible, but capturing the LBB in\nits entirety could only be achieved using higher strengths that led to\nnon-selective left bundle branch pacing (ns-LBBP). Switching the tip polarity\nto anodal was not beneficial, requiring higher strengths to activate the LBB.\nLead orientation relative to the LBB bundles was found to influence the s-LBBP\ncapture threshold and the number of synchronously activating bundles. The model\nexplains the impedance trends that are clinically observed when advancing the\ntip through the IVS into the LBB region, as well as sudden impedance drops\nassociated with implant complications such as septal perforation or lead\ndislodgement. Quantitative consistence with clinically observed trends support\nmodel credibility, and indicate that simulation may offer an effective approach\nfor guiding the design of improved CSP leads, facilitating a selective and\nsynchronous activation of the entire LBB.", "published": "2025-09-20 11:09:35", "link": "http://arxiv.org/abs/2509.16631v1", "categories": ["math.NA", "cs.NA", "physics.med-ph"], "primary_category": "math.NA"}
{"title": "DoubleGen: Debiased Generative Modeling of Counterfactuals", "abstract": "Generative models for counterfactual outcomes face two key sources of bias.\nConfounding bias arises when approaches fail to account for systematic\ndifferences between those who receive the intervention and those who do not.\nMisspecification bias arises when methods attempt to address confounding\nthrough estimation of an auxiliary model, but specify it incorrectly. We\nintroduce DoubleGen, a doubly robust framework that modifies generative\nmodeling training objectives to mitigate these biases. The new objectives rely\non two auxiliaries -- a propensity and outcome model -- and successfully\naddress confounding bias even if only one of them is correct. We provide\nfinite-sample guarantees for this robustness property. We further establish\nconditions under which DoubleGen achieves oracle optimality -- matching the\nconvergence rates standard approaches would enjoy if interventional data were\navailable -- and minimax rate optimality. We illustrate DoubleGen with three\nexamples: diffusion models, flow matching, and autoregressive language models.", "published": "2025-09-20 23:42:04", "link": "http://arxiv.org/abs/2509.16842v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "System-Level Uncertainty Quantification with Multiple Machine Learning Models: A Theoretical Framework", "abstract": "ML models have errors when used for predictions. The errors are unknown but\ncan be quantified by model uncertainty. When multiple ML models are trained\nusing the same training points, their model uncertainties may be statistically\ndependent. In reality, model inputs are also random with input uncertainty. The\neffects of these types of uncertainty must be considered in decision-making and\ndesign. This study develops a theoretical framework that generates the joint\ndistribution of multiple ML predictions given the joint distribution of model\nuncertainties and the joint distribution of model inputs. The strategy is to\ndecouple the coupling between the two types of uncertainty and transform them\nas independent random variables. The framework lays a foundation for numerical\nalgorithm development for various specific applications.", "published": "2025-09-20 12:34:05", "link": "http://arxiv.org/abs/2509.16663v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Conditional Multidimensional Scaling with Incomplete Conditioning Data", "abstract": "Conditional multidimensional scaling seeks for a low-dimensional\nconfiguration from pairwise dissimilarities, in the presence of other known\nfeatures. By taking advantage of available data of the known features,\nconditional multidimensional scaling improves the estimation quality of the\nlow-dimensional configuration and simplifies knowledge discovery tasks.\nHowever, existing conditional multidimensional scaling methods require full\ndata of the known features, which may not be always attainable due to time,\ncost, and other constraints. This paper proposes a conditional multidimensional\nscaling method that can learn the low-dimensional configuration when there are\nmissing values in the known features. The method can also impute the missing\nvalues, which provides additional insights of the problem. Computer codes of\nthis method are maintained in the cml R package on CRAN.", "published": "2025-09-20 11:06:12", "link": "http://arxiv.org/abs/2509.16627v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs", "abstract": "Recent advances have significantly improved our understanding of the sample\ncomplexity of learning in average-reward Markov decision processes (AMDPs)\nunder the generative model. However, much less is known about the constrained\naverage-reward MDP (CAMDP), where policies must satisfy long-run average\nconstraints. In this work, we address this gap by studying the sample\ncomplexity of learning an $\\epsilon$-optimal policy in CAMDPs under a\ngenerative model. We propose a model-based algorithm that operates under two\nsettings: (i) relaxed feasibility, which allows small constraint violations,\nand (ii) strict feasibility, where the output policy satisfies the constraint.\nWe show that our algorithm achieves sample complexities of\n$\\tilde{O}\\left(\\frac{S A (B+H)}{ \\epsilon^2}\\right)$ and $\\tilde{O}\n\\left(\\frac{S A (B+H)}{\\epsilon^2 \\zeta^2} \\right)$ under the relaxed and\nstrict feasibility settings, respectively. Here, $\\zeta$ is the Slater constant\nindicating the size of the feasible region, $H$ is the span bound of the bias\nfunction, and $B$ is the transient time bound. Moreover, a matching lower bound\nof $\\tilde{\\Omega}\\left(\\frac{S A (B+H)}{ \\epsilon^2\\zeta^2}\\right)$ for the\nstrict feasibility case is established, thus providing the first\nminimax-optimal bounds for CAMDPs. Our results close the theoretical gap in\nunderstanding the complexity of constrained average-reward MDPs.", "published": "2025-09-20 09:19:42", "link": "http://arxiv.org/abs/2509.16586v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology", "abstract": "According to the U.S. National Institutes of Health, more than 3.4 million\nchildren experience speech disorders that require clinical intervention. The\nnumber of speech-language pathologists (SLPs) is roughly 20 times fewer than\nthe number of affected children, highlighting a significant gap in children's\ncare and a pressing need for technological support that improves the\nproductivity of SLPs. State-of-the-art multimodal language models (MLMs) show\npromise for supporting SLPs, but their use remains underexplored largely due to\na limited understanding of their performance in high-stakes clinical settings.\nTo address this gap, we collaborate with domain experts to develop a taxonomy\nof real-world use cases of MLMs in speech-language pathologies. Building on\nthis taxonomy, we introduce the first comprehensive benchmark for evaluating\nMLM across five core use cases, each containing 1,000 manually annotated data\npoints. This benchmark includes robustness and sensitivity tests under various\nsettings, including background noise, speaker gender, and accent. Our\nevaluation of 15 state-of-the-art MLMs reveals that no single model\nconsistently outperforms others across all tasks. Notably, we find systematic\ndisparities, with models performing better on male speakers, and observe that\nchain-of-thought prompting can degrade performance on classification tasks with\nlarge label spaces and narrow decision boundaries. Furthermore, we study\nfine-tuning MLMs on domain-specific data, achieving improvements of over 30%\ncompared to base models. These findings highlight both the potential and\nlimitations of current MLMs for speech-language pathology applications,\nunderscoring the need for further research and targeted development.", "published": "2025-09-20 18:10:30", "link": "http://arxiv.org/abs/2509.16765v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Feature Selection via Graph Topology Inference for Soundscape Emotion Recognition", "abstract": "Research on soundscapes has shifted the focus of environmental acoustics from\nnoise levels to the perception of sounds, incorporating contextual factors.\nSoundscape emotion recognition (SER) models perception using a set of features,\nwith arousal and valence commonly regarded as sufficient descriptors of affect.\nIn this work, we blend \\emph{graph learning} techniques with a novel\n\\emph{information criterion} to develop a feature selection framework for SER.\nSpecifically, we estimate a sparse graph representation of feature relations\nusing linear structural equation models (SEM) tailored to the widely used\nEmo-Soundscapes dataset. The resulting graph captures the relations between\ninput features and the two emotional outputs. To determine the appropriate\nlevel of sparsity, we propose a novel \\emph{generalized elbow detector}, which\nprovides both a point estimate and an uncertainty interval. We conduct an\nextensive evaluation of our methods, including visualizations of the inferred\nrelations. While several of our findings align with previous studies, the graph\nrepresentation also reveals a strong connection between arousal and valence,\nchallenging common SER assumptions.", "published": "2025-09-20 17:52:20", "link": "http://arxiv.org/abs/2509.16760v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition: Dysarthric Case Studies", "abstract": "State-of-the-art automatic speech recognition (ASR) models like Whisper,\nperform poorly on atypical speech, such as that produced by individuals with\ndysarthria. Past works for atypical speech have mostly investigated fully\npersonalized (or idiosyncratic) models, but modeling strategies that can both\ngeneralize and handle idiosyncracy could be more effective for capturing\natypical speech. To investigate this, we compare four strategies: (a)\n$\\textit{normative}$ models trained on typical speech (no personalization), (b)\n$\\textit{idiosyncratic}$ models completely personalized to individuals, (c)\n$\\textit{dysarthric-normative}$ models trained on other dysarthric speakers,\nand (d) $\\textit{dysarthric-idiosyncratic}$ models which combine strategies by\nfirst modeling normative patterns before adapting to individual speech. In this\ncase study, we find the dysarthric-idiosyncratic model performs better than\nidiosyncratic approach while requiring less than half as much personalized data\n(36.43 WER with 128 train size vs 36.99 with 256). Further, we found that\ntuning the speech encoder alone (as opposed to the LM decoder) yielded the best\nresults reducing word error rate from 71% to 32% on average. Our findings\nhighlight the value of leveraging both normative (cross-speaker) and\nidiosyncratic (speaker-specific) patterns to improve ASR for underrepresented\nspeech populations.", "published": "2025-09-20 15:04:33", "link": "http://arxiv.org/abs/2509.16718v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "QASTAnet: A DNN-based Quality Metric for Spatial Audio", "abstract": "In the development of spatial audio technologies, reliable and shared methods\nfor evaluating audio quality are essential. Listening tests are currently the\nstandard but remain costly in terms of time and resources. Several models\npredicting subjective scores have been proposed, but they do not generalize\nwell to real-world signals. In this paper, we propose QASTAnet (Quality\nAssessment for SpaTial Audio network), a new metric based on a deep neural\nnetwork, specialized on spatial audio (ambisonics and binaural). As training\ndata is scarce, we aim for the model to be trainable with a small amount of\ndata. To do so, we propose to rely on expert modeling of the low-level auditory\nsystem and use a neurnal network to model the high-level cognitive function of\nthe quality judgement. We compare its performance to two reference metrics on a\nwide range of content types (speech, music, ambiance, anechoic, reverberated)\nand focusing on codec artifacts. Results demonstrate that QASTAnet overcomes\nthe aforementioned limitations of the existing methods. The strong correlation\nbetween the proposed metric prediction and subjective scores makes it a good\ncandidate for comparing codecs in their development.", "published": "2025-09-20 14:57:09", "link": "http://arxiv.org/abs/2509.16715v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Reverse Attention for Lightweight Speech Enhancement on Edge Devices", "abstract": "This paper introduces a lightweight deep learning model for real-time speech\nenhancement, designed to operate efficiently on resource-constrained devices.\nThe proposed model leverages a compact architecture that facilitates rapid\ninference without compromising performance. Key contributions include infusing\nsoft attention-based attention gates in the U-Net architecture which is known\nto perform well for segmentation tasks and is optimized for GPUs. Experimental\nevaluations demonstrate that the model achieves competitive speech quality and\nintelligibility metrics, such as PESQ and Word Error Rates (WER), improving the\nperformance of similarly sized baseline models. We are able to achieve a 6.24%\nWER improvement and a 0.64 PESQ score improvement over un-enhanced waveforms.", "published": "2025-09-20 14:31:23", "link": "http://arxiv.org/abs/2509.16705v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection", "abstract": "Audio grounding, or speech-driven open-set object detection, aims to localize\nand identify objects directly from speech, enabling generalization beyond\npredefined categories. This task is crucial for applications like human-robot\ninteraction where textual input is impractical. However, progress in this\ndomain faces a fundamental bottleneck from the scarcity of large-scale, paired\naudio-image data, and is further constrained by previous methods that rely on\nindirect, text-mediated pipelines. In this paper, we introduce Speech-to-See\n(Speech2See), an end-to-end approach built on a pre-training and fine-tuning\nparadigm. Specifically, in the pre-training stage, we design a Query-Guided\nSemantic Aggregation module that employs learnable queries to condense\nredundant speech embeddings into compact semantic representations. During\nfine-tuning, we incorporate a parameter-efficient Mixture-of-LoRA-Experts\n(MoLE) architecture to achieve deeper and more nuanced cross-modal adaptation.\nExtensive experiments show that Speech2See achieves robust and adaptable\nperformance across multiple benchmarks, demonstrating its strong generalization\nability and broad applicability.", "published": "2025-09-20 12:43:46", "link": "http://arxiv.org/abs/2509.16670v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the de-duplication of the Lakh MIDI dataset", "abstract": "A large-scale dataset is essential for training a well-generalized\ndeep-learning model. Most such datasets are collected via scraping from various\ninternet sources, inevitably introducing duplicated data. In the symbolic music\ndomain, these duplicates often come from multiple user arrangements and\nmetadata changes after simple editing. However, despite critical issues such as\nunreliable training evaluation from data leakage during random splitting,\ndataset duplication has not been extensively addressed in the MIR community.\nThis study investigates the dataset duplication issues regarding Lakh MIDI\nDataset (LMD), one of the largest publicly available sources in the symbolic\nmusic domain. To find and evaluate the best retrieval method for duplicated\ndata, we employed the Clean MIDI subset of the LMD as a benchmark test set, in\nwhich different versions of the same songs are grouped together. We first\nevaluated rule-based approaches and previous symbolic music retrieval models\nfor de-duplication and also investigated with a contrastive learning-based BERT\nmodel with various augmentations to find duplicate files. As a result, we\npropose three different versions of the filtered list of LMD, which filters out\nat least 38,134 samples in the most conservative settings among 178,561 files.", "published": "2025-09-20 12:31:30", "link": "http://arxiv.org/abs/2509.16662v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval", "abstract": "This report presents the AISTAT team's submission to the language-based audio\nretrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder\narchitecture, where audio and text modalities are encoded separately, and their\nrepresentations are aligned using contrastive learning. Drawing inspiration\nfrom methodologies of the previous year's challenge, we implemented a\ndistillation approach and leveraged large language models (LLMs) for effective\ndata augmentation techniques, including back-translation and LLM mix.\nAdditionally, we incorporated clustering to introduce an auxiliary\nclassification task for further finetuning. Our best single system achieved a\nmAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on\nthe Clotho development test split.", "published": "2025-09-20 11:53:18", "link": "http://arxiv.org/abs/2509.16649v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing", "abstract": "Diffusion-based large language models (DLLMs) have recently attracted growing\ninterest as an alternative to autoregressive decoders. In this work, we present\nan empirical study on using the diffusion-based large language model LLaDA for\nautomatic speech recognition (ASR). We first investigate its use as an external\ndeliberation-based processing module for Whisper-LLaMA transcripts. By\nleveraging the bidirectional attention and denoising capabilities of LLaDA, we\nexplore random masking, low-confidence masking, and semi-autoregressive\nstrategies, showing that Whisper-LLaDA substantially reduces WER compared with\nthe baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER\non test-clean/test-other, representing a 12.3% relative improvement over the\nWhisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA\nwithout acoustic features fails to improve accuracy, highlighting the\nimportance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA\nas a standalone decoder for ASR with diffusion-based and semi-autoregressive\ndecoding. Most experimental configurations achieve faster inference than the\nWhisper-LLaMA baseline, although recognition accuracy is slightly lower. These\nfindings offer an empirical view of diffusion-based LLMs for ASR and point to\npromising directions for improvements.", "published": "2025-09-20 10:48:06", "link": "http://arxiv.org/abs/2509.16622v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Octave-based Multi-Resolution CQT Architecture for Diffusion-based Audio Generation", "abstract": "This paper introduces MR-CQTdiff, a novel neural-network architecture for\ndiffusion-based audio generation that leverages a multi-resolution Constant-$Q$\nTransform (C$Q$T). The proposed architecture employs an efficient, invertible\nCQT framework that adjusts the time-frequency resolution on an octave-by-octave\nbasis. This design addresses the issue of low temporal resolution at lower\nfrequencies, enabling more flexible and expressive audio generation. We conduct\nan evaluation using the Fr\\'echet Audio Distance (FAD) metric across various\narchitectures and two datasets. Experimental results demonstrate that\nMR-CQTdiff achieves state-of-the-art audio quality, outperforming competing\narchitectures.", "published": "2025-09-20 09:57:37", "link": "http://arxiv.org/abs/2509.16603v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Barwise Section Boundary Detection in Symbolic Music Using Convolutional Neural Networks", "abstract": "Current methods for Music Structure Analysis (MSA) focus primarily on audio\ndata. While symbolic music can be synthesized into audio and analyzed using\nexisting MSA techniques, such an approach does not exploit symbolic music's\nrich explicit representation of pitch, timing, and instrumentation. A key\nsubproblem of MSA is section boundary detection-determining whether a given\npoint in time marks the transition between musical sections. In this paper, we\nstudy automatic section boundary detection for symbolic music. First, we\nintroduce a human-annotated MIDI dataset for section boundary detection,\nconsisting of metadata from 6134 MIDI files that we manually curated from the\nLakh MIDI dataset. Second, we train a deep learning model to classify the\npresence of section boundaries within a fixed-length musical window. Our data\nrepresentation involves a novel encoding scheme based on synthesized overtones\nto encode arbitrary MIDI instrumentations into 3-channel piano rolls. Our model\nachieves an F1 score of 0.77, improving over the analogous audio-based\nsupervised learning approach and the unsupervised block-matching segmentation\n(CBM) audio approach by 0.22 and 0.31, respectively. We release our dataset,\ncode, and models.", "published": "2025-09-20 07:52:08", "link": "http://arxiv.org/abs/2509.16566v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Etude: Piano Cover Generation with a Three-Stage Approach -- Extract, strucTUralize, and DEcode", "abstract": "Piano cover generation aims to automatically transform a pop song into a\npiano arrangement. While numerous deep learning approaches have been proposed,\nexisting models often fail to maintain structural consistency with the original\nsong, likely due to the absence of beat-aware mechanisms or the difficulty of\nmodeling complex rhythmic patterns. Rhythmic information is crucial, as it\ndefines structural similarity (e.g., tempo, BPM) and directly impacts the\noverall quality of the generated music.\n  In this paper, we introduce Etude, a three-stage architecture consisting of\nExtract, strucTUralize, and DEcode stages. By pre-extracting rhythmic\ninformation and applying a novel, simplified REMI-based tokenization, our model\nproduces covers that preserve proper song structure, enhance fluency and\nmusical dynamics, and support highly controllable generation through style\ninjection. Subjective evaluations with human listeners show that Etude\nsubstantially outperforms prior models, achieving a quality level comparable to\nthat of human composers.", "published": "2025-09-20 04:06:43", "link": "http://arxiv.org/abs/2509.16522v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TF-CorrNet: Leveraging Spatial Correlation for Continuous Speech Separation", "abstract": "In general, multi-channel source separation has utilized inter-microphone\nphase differences (IPDs) concatenated with magnitude information in\ntime-frequency domain, or real and imaginary components stacked along the\nchannel axis. However, the spatial information of a sound source is\nfundamentally contained in the differences between microphones, specifically in\nthe correlation between them, while the power of each microphone also provides\nvaluable information about the source spectrum, which is why the magnitude is\nalso included. Therefore, we propose a network that directly leverages a\ncorrelation input with phase transform (PHAT)-beta to estimate the separation\nfilter. In addition, the proposed TF-CorrNet processes the features alternately\nacross time and frequency axes as a dual-path strategy in terms of spatial\ninformation. Furthermore, we add a spectral module to model source-related\ndirect time-frequency patterns for improved speech separation. Experimental\nresults demonstrate that the proposed TF-CorrNet effectively separates the\nspeech sounds, showing high performance with a low computational cost in the\nLibriCSS dataset.", "published": "2025-09-20 00:42:41", "link": "http://arxiv.org/abs/2509.16481v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Harmonic Summation-Based Robust Pitch Estimation in Noisy and Reverberant Environments", "abstract": "Accurate pitch estimation is essential for numerous speech processing\napplications, yet it remains challenging in high-distortion environments. This\npaper proposes a robust pitch estimation method that delivers robust pitch\nestimates in challenging noise environments. Our approach computes the\nNormalized Average Magnitude Difference Function (NAMDF), transforms it into a\nlikelihood function, and generates probabilistic pitch states for frames at\neach sample shift. To enhance noise robustness, we aggregate likelihood values\nacross integer multiples of the pitch period and neighboring frames.\nFurthermore, we introduce a simple yet effective continuity constraint in the\nViterbi algorithm to refine pitch selection among multiple candidates.\nExperimental results show that our method consistently achieves lower Gross\nPitch Error (GPE) and Voicing Decision Error (VDE) across various SNR levels,\noutperforming existing methods in both noisy and reverberant conditions.", "published": "2025-09-20 00:33:42", "link": "http://arxiv.org/abs/2509.16480v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tolerance of Phase Noise in Single-Carrier M-Ary QAM Terahertz Wireless Communications", "abstract": "Terahertz wireless communications offer abundant untapped spectrum and are\nregarded as a promising playground for next-generation high-throughput links.\nYet oscillator phase noise becomes the dominant impairment at such high\nfrequencies, severely limiting the reliability of high-order QAM transmission.\nHere, phase noise is reconstructed from measured spectra and embedded into a\nsingle-carrier link model to evaluate its impact. Distinct distortion\nmechanisms are identified, with slow common phase error and instantaneous phase\njitter, where the latter remains as the residual impairment after carrier phase\nrecovery. We further adopt 3{\\sigma} error criterion that maps residual\ndistortions onto the constellation, offering a clear and practical indicator of\nsystem robustness. The results indicate that modest improvements in oscillator\nstability translate into significant BER gains without proportional power\nincrease. These findings provide intuitive tolerance of phase noise in M-QAM\nsystems and emphasizes the importance of integrating low-noise photonic\noscillators.", "published": "2025-09-20 23:50:38", "link": "http://arxiv.org/abs/2509.16843v1", "categories": ["physics.optics", "eess.SP"], "primary_category": "physics.optics"}
{"title": "Data-Driven Two-Stage IRS-Aided Sumrate Maximization with Inexact Precoding", "abstract": "We propose iZoSGA, a data-driven learning algorithm for joint passive\nlong-term intelligent reflective surface (IRS)-aided beamforming and active\nshort-term precoding in wireless networks. iZoSGA is based on a zeroth-order\nstochastic quasigradient ascent methodology designed for tackling two-stage\nnonconvex stochastic programs with continuous uncertainty and objective\nfunctions with \"black-box\" terms, and where second-stage optimization is\ninexact. As such, iZoSGA utilizes inexact precoding oracles, enabling practical\nimplementation when short-term (e.g., WMMSE-based) beamforming is solved\napproximately. The proposed method is agnostic to channel models or statistics,\nand applies to arbitrary IRS/network configurations. We prove non-asymptotic\nconvergence of iZoSGA to a neighborhood of a stationary solution of the\noriginal exact problem under minimal assumptions. Our numerics confirm the\nefficacy iZoSGA in several \"inexact regimes\", enabling passive yet fully\neffective IRS operation in diverse and realistic IRS-aided scenarios.", "published": "2025-09-20 18:53:04", "link": "http://arxiv.org/abs/2509.16776v1", "categories": ["eess.SP", "math.OC"], "primary_category": "eess.SP"}
{"title": "Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees", "abstract": "Discrete diffusion models have recently gained significant prominence in\napplications involving natural language and graph data. A key factor\ninfluencing their effectiveness is the efficiency of discretized samplers.\nAmong these, $\\tau$-leaping samplers have become particularly popular due to\ntheir empirical success. However, existing theoretical analyses of\n$\\tau$-leaping often rely on somewhat restrictive and difficult-to-verify\nregularity assumptions, and their convergence bounds contain quadratic\ndependence on the vocabulary size. In this work, we introduce a new analytical\napproach for discrete diffusion models that removes the need for such\nassumptions. For the standard $\\tau$-leaping method, we establish convergence\nguarantees in KL divergence that scale linearly with vocabulary size, improving\nupon prior results with quadratic dependence. Our approach is also more broadly\napplicable: it provides the first convergence guarantees for other widely used\nsamplers, including the Euler method and Tweedie $\\tau$-leaping. Central to our\napproach is a novel technique based on differential inequalities, offering a\nmore flexible alternative to the traditional Girsanov change-of-measure\nmethods. This technique may also be of independent interest for the analysis of\nother stochastic processes.", "published": "2025-09-20 17:42:29", "link": "http://arxiv.org/abs/2509.16756v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Affine Frequency Division Multiplexing for Communication and Channel Sounding: Requirements, Challenges, and Key Technologies", "abstract": "Channel models are crucial for theoretical analysis, performance evaluation,\nand deployment of wireless communication systems. Traditional channel sounding\nsystems are insufficient for handling the dynamic changes of channels in the\nnext-generation space-air-ground-sea integrated networks (SAGSIN), which often\nresults in outdated channel models that fail to provide reliable prior\ninformation for communication systems. To address this challenge, this paper\nproposes an integrated channel sounding and communication (ICSC) method as a\npractical solution. Unlike orthogonal frequency division multiplexing, affine\nfrequency division multiplexing (AFDM) provides a full delay-Doppler\nrepresentation of the channel, achieving optimal diversity in time-frequency\ndoubly dispersive channels and effectively addressing the aforementioned\nchallenges. Thus, we investigate the fundamental principles of AFDM, showing\nhow it enables simultaneous communication and channel sounding, and explore key\nperformance metrics for both functionalities. We also clarify the distinction\nand relationship between channel sounding, estimation, tracking and scatterer\nsensing. Additionally, several potential application scenarios for AFDM-ICSC\nare explored. Finally, we highlight the key challenges in implementing\nAFDM-ICSC, outline future research directions, and provide valuable insights\nfor the continued development of this technology.", "published": "2025-09-20 11:46:37", "link": "http://arxiv.org/abs/2509.16643v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Fusing Spectral Correlation Density Imaging with Deep Learning for Intelligent Fault Diagnosis in Rotating Machinery", "abstract": "Bearing fault diagnosis in rotating machinery is critical for ensuring\noperational reliability, therefore early fault detection is essential to avoid\ncatastrophic failures and expensive emergency repairs. Traditional methods like\nFast Fourier Transform (FFT) often fail to capture the complex, non-stationary\nnature of vibration signals. This study leverages the cyclostationary\nproperties of vibration data through Spectral Correlation Density (SCD) images\nto enhance fault detection and apply deep learning for classification. Using a\npublicly available dataset with bearing faults seeded in two distinct housings\n(A and B) under varying load conditions (0 Nm, 2 Nm, 4 Nm), we processed\nvibration signals into 2D SCD images to reveal fault-specific periodicities,\nsuch as broadband spectra (2000--8000 Hz) for larger faults. Three\nconvolutional neural network (CNN) models, Custom CNN, ResNet152V2, and\nEfficientNetB0, were developed to classify seven bearing conditions. The custom\nCNN achieved the highest accuracies of 96.58\\% and 94.95\\% on Housing A and B,\nrespectively, followed by ResNet152V2 at 96.49\\% and 95.35\\%, and\nEfficientNetB0 at 94.16\\% and 91.65\\%, respectively. The models' high\naccuracies across different housings demonstrate a robust solution suitable for\ncost-effective condition monitoring deployable near sensing platforms,\ncontributing to applied machine learning for edge intelligence and showcasing\neffective signal processing strategies for handling complex, potentially\nlarge-scale vibration data.", "published": "2025-09-20 08:58:08", "link": "http://arxiv.org/abs/2509.16580v1", "categories": ["eess.SP", "cs.CV"], "primary_category": "eess.SP"}
{"title": "Learned Digital Codes for Over-the-Air Federated Learning", "abstract": "Federated edge learning (FEEL) enables distributed model training across\nwireless devices without centralising raw data, but deployment is constrained\nby the wireless uplink. A promising direction is over-the-air (OTA)\naggregation, which merges communication with computation. Existing digital OTA\nmethods can achieve either strong convergence or robustness to noise, but\nstruggle to achieve both simultaneously, limiting performance in low\nsignal-to-noise ratios (SNRs) where many IoT devices operate. This work\nproposes a learnt digital OTA framework that extends reliable operation into\nlow-SNR conditions while maintaining the same uplink overhead as\nstate-of-the-art. The proposed method combines an unrolled decoder with a\njointly learnt unsourced random access codebook. Results show an extension of\nreliable operation by more than 7 dB, with improved global model convergence\nacross all SNR levels, highlighting the potential of learning-based design for\nFEEL.", "published": "2025-09-20 08:43:42", "link": "http://arxiv.org/abs/2509.16577v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Bearing-only Tracking using Towed Sensor-Array with Non-Gaussian Measurement Noise Statistics", "abstract": "Passive bearing-only tracking (BOT) estimates the target states by utilising\nnoisy bearing measurements captured by a sensor array. The sensor array is\noften towed behind the ship, using a long flexible cable to reduce interference\nfrom the own-ship's inherent noises. This forms a towed cable sensor-array\nsystem (TCSAS). During BOT, the tow-ship has to perform a manoeuvre to make the\ntracking system observable. Such a manoeuvre destabilises the TCSAS, thus\nmaking its exact location unknown \\emph{w.r.t.} tow-ship. However, it is very\ncrucial to know the exact location of the towed sensor-array to perform\nefficient and reliable target state estimation. The existing BOT approaches\nperform TMA during own-ship manoeuvre either by pausing the measurement\nupdation step of the estimation algorithm or assuming a fixed aft position for\nthe towed sensor-array. These assumptions lead to unreliable state estimation.\nTo address this, we propose a dynamic model for TCSAS, using a lumped mass\napproach, which will provide the location of the sensor array during the\nown-ship manoeuvre. This location will be fed to the state estimation\nalgorithm. The dynamic of TCSAS in 3D space is obtained by solving the\nequations obtained from the moment balance condition and quasi-static\nequilibrium condition at the lumped mass points. Moreover, the bearing data\ncaptured by the towed sensor-array is corrupted with non-Gaussian noise. It is\nhandled using the maximum correntropy criterion based Kalman filter with a\nkernel bandwidth selection technique, proposed in this paper. The proposed\nsensor-array dynamic model is verified for a real-world BOT engagement\nscenario.", "published": "2025-09-20 07:59:19", "link": "http://arxiv.org/abs/2509.16570v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Advancing Accessible Hand-Arm Vibration Safety Monitoring: ISO-Compliance with Wearable Sensors and Transfer Functions", "abstract": "Field workers are frequently exposed to hazardous vibrations, increasing the\nrisk of Hand-Arm Vibration Syndrome (HAVS) and other long-term health problems.\nISO 5349-1 provides guidelines for measuring vibration exposure. However, this\nstandard was established in controlled conditions using high-quality\naccelerometers directly attached to power tool handles. This study investigates\nan alternative, wearable sensor-based data collection process and develops an\nerror-minimization transfer function that derives values comparable to ISO\nbenchmarks for safety monitoring. Experiments are performed with subjects\nhammer drilling into concrete while vibrations are measured using three\naccelerometers at different sampling frequencies. The transfer function maps\nvibration data across sensor positions by accounting for damping effects. The\nfindings indicate a significant reduction in acceleration between the palm and\nupper arm, highlight the impact of sampling frequency on data accuracy, and\nenable accurate comparison of true hand-arm vibration levels with existing\nstandard limits to allow accessible, real-time, and cost-effective HAVS\nprevention.", "published": "2025-09-20 04:54:13", "link": "http://arxiv.org/abs/2509.16536v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
