{"title": "Neural Models for Sequence Chunking", "abstract": "Many natural language understanding (NLU) tasks, such as shallow parsing\n(i.e., text chunking) and semantic slot filling, require the assignment of\nrepresentative labels to the meaningful chunks in a sentence. Most of the\ncurrent deep neural network (DNN) based methods consider these tasks as a\nsequence labeling problem, in which a word, rather than a chunk, is treated as\nthe basic unit for labeling. These chunks are then inferred by the standard IOB\n(Inside-Outside-Beginning) labels. In this paper, we propose an alternative\napproach by investigating the use of DNN for sequence chunking, and propose\nthree neural models so that each chunk can be treated as a complete unit for\nlabeling. Experimental results show that the proposed neural sequence chunking\nmodels can achieve start-of-the-art performance on both the text chunking and\nslot filling tasks.", "published": "2017-01-15 11:08:28", "link": "http://arxiv.org/abs/1701.04027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialog Context Language Modeling with Recurrent Neural Networks", "abstract": "In this work, we propose contextual language models that incorporate dialog\nlevel discourse information into language modeling. Previous works on\ncontextual language model treat preceding utterances as a sequence of inputs,\nwithout considering dialog interactions. We design recurrent neural network\n(RNN) based contextual language models that specially track the interactions\nbetween speakers in a dialog. Experiment results on Switchboard Dialog Act\nCorpus show that the proposed model outperforms conventional single turn based\nRNN language model by 3.3% on perplexity. The proposed models also demonstrate\nadvantageous performance over other competitive contextual language models.", "published": "2017-01-15 15:10:29", "link": "http://arxiv.org/abs/1701.04056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Copy-Augmented Sequence-to-Sequence Architecture Gives Good\n  Performance on Task-Oriented Dialogue", "abstract": "Task-oriented dialogue focuses on conversational agents that participate in\nuser-initiated dialogues on domain-specific topics. In contrast to chatbots,\nwhich simply seek to sustain open-ended meaningful discourse, existing\ntask-oriented agents usually explicitly model user intent and belief states.\nThis paper examines bypassing such an explicit representation by depending on a\nlatent neural embedding of state and learning selective attention to dialogue\nhistory together with copying to incorporate relevant prior context. We\ncomplement recent work by showing the effectiveness of simple\nsequence-to-sequence neural architectures with a copy mechanism. Our model\noutperforms more complex memory-augmented models by 7% in per-response\ngeneration and is on par with the current state-of-the-art on DSTC2.", "published": "2017-01-15 10:38:17", "link": "http://arxiv.org/abs/1701.04024v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Birth of Collective Memories: Analyzing Emerging Entities in Text\n  Streams", "abstract": "We study how collective memories are formed online. We do so by tracking\nentities that emerge in public discourse, that is, in online text streams such\nas social media and news streams, before they are incorporated into Wikipedia,\nwhich, we argue, can be viewed as an online place for collective memory. By\ntracking how entities emerge in public discourse, i.e., the temporal patterns\nbetween their first mention in online text streams and subsequent incorporation\ninto collective memory, we gain insights into how the collective remembrance\nprocess happens online. Specifically, we analyze nearly 80,000 entities as they\nemerge in online text streams before they are incorporated into Wikipedia. The\nonline text streams we use for our analysis comprise of social media and news\nstreams, and span over 579 million documents in a timespan of 18 months. We\ndiscover two main emergence patterns: entities that emerge in a \"bursty\"\nfashion, i.e., that appear in public discourse without a precedent, blast into\nactivity and transition into collective memory. Other entities display a\n\"delayed\" pattern, where they appear in public discourse, experience a period\nof inactivity, and then resurface before transitioning into our cultural\ncollective memory.", "published": "2017-01-15 13:34:43", "link": "http://arxiv.org/abs/1701.04039v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DyNet: The Dynamic Neural Network Toolkit", "abstract": "We describe DyNet, a toolkit for implementing neural network models based on\ndynamic declaration of network structure. In the static declaration strategy\nthat is used in toolkits like Theano, CNTK, and TensorFlow, the user first\ndefines a computation graph (a symbolic representation of the computation), and\nthen examples are fed into an engine that executes this computation and\ncomputes its derivatives. In DyNet's dynamic declaration strategy, computation\ngraph construction is mostly transparent, being implicitly constructed by\nexecuting procedural code that computes the network outputs, and the user is\nfree to use different network structures for each input. Dynamic declaration\nthus facilitates the implementation of more complicated network architectures,\nand DyNet is specifically designed to allow users to implement their models in\na way that is idiomatic in their preferred programming language (C++ or\nPython). One challenge with dynamic declaration is that because the symbolic\ncomputation graph is defined anew for every training example, its construction\nmust have low overhead. To achieve this, DyNet has an optimized C++ backend and\nlightweight graph representation. Experiments show that DyNet's speeds are\nfaster than or comparable with static declaration toolkits, and significantly\nfaster than Chainer, another dynamic declaration toolkit. DyNet is released\nopen-source under the Apache 2.0 license and available at\nhttp://github.com/clab/dynet.", "published": "2017-01-15 01:53:23", "link": "http://arxiv.org/abs/1701.03980v1", "categories": ["stat.ML", "cs.CL", "cs.MS"], "primary_category": "stat.ML"}
