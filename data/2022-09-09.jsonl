{"title": "An Analysis of Deep Reinforcement Learning Agents for Text-based Games", "abstract": "Text-based games(TBG) are complex environments which allow users or computer\nagents to make textual interactions and achieve game goals.In TBG agent design\nand training process, balancing the efficiency and performance of the agent\nmodels is a major challenge. Finding TBG agent deep learning modules'\nperformance in standardized environments, and testing their performance among\ndifferent evaluation types is also important for TBG agent research. We\nconstructed a standardized TBG agent with no hand-crafted rules, formally\ncategorized TBG evaluation types, and analyzed selected methods in our\nenvironment.", "published": "2022-09-09 03:36:06", "link": "http://arxiv.org/abs/2209.04105v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaxMatch-Dropout: Subword Regularization for WordPiece", "abstract": "We present a subword regularization method for WordPiece, which uses a\nmaximum matching algorithm for tokenization. The proposed method,\nMaxMatch-Dropout, randomly drops words in a search using the maximum matching\nalgorithm. It realizes finetuning with subword regularization for popular\npretrained language models such as BERT-base. The experimental results\ndemonstrate that MaxMatch-Dropout improves the performance of text\nclassification and machine translation tasks as well as other subword\nregularization methods. Moreover, we provide a comparative analysis of subword\nregularization methods: subword regularization with SentencePiece (Unigram),\nBPE-Dropout, and MaxMatch-Dropout.", "published": "2022-09-09 05:41:26", "link": "http://arxiv.org/abs/2209.04126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting to Non-Centered Languages for Zero-shot Multilingual\n  Translation", "abstract": "Multilingual neural machine translation can translate unseen language pairs\nduring training, i.e. zero-shot translation. However, the zero-shot translation\nis always unstable. Although prior works attributed the instability to the\ndomination of central language, e.g. English, we supplement this viewpoint with\nthe strict dependence of non-centered languages. In this work, we propose a\nsimple, lightweight yet effective language-specific modeling method by adapting\nto non-centered languages and combining the shared information and the\nlanguage-specific information to counteract the instability of zero-shot\ntranslation. Experiments with Transformer on IWSLT17, Europarl, TED talks, and\nOPUS-100 datasets show that our method not only performs better than strong\nbaselines in centered data conditions but also can easily fit non-centered data\nconditions. By further investigating the layer attribution, we show that our\nproposed method can disentangle the coupled representation in the correct\ndirection.", "published": "2022-09-09 06:34:12", "link": "http://arxiv.org/abs/2209.04138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-grained Label Refinement Network with Dependency Structures for\n  Joint Intent Detection and Slot Filling", "abstract": "Slot filling and intent detection are two fundamental tasks in the field of\nnatural language understanding. Due to the strong correlation between these two\ntasks, previous studies make efforts on modeling them with multi-task learning\nor designing feature interaction modules to improve the performance of each\ntask. However, none of the existing approaches consider the relevance between\nthe structural information of sentences and the label semantics of two tasks.\nThe intent and semantic components of a utterance are dependent on the\nsyntactic elements of a sentence. In this paper, we investigate a multi-grained\nlabel refinement network, which utilizes dependency structures and label\nsemantic embeddings. Considering to enhance syntactic representations, we\nintroduce the dependency structures of sentences into our model by graph\nattention layer. To capture the semantic dependency between the syntactic\ninformation and task labels, we combine the task specific features with\ncorresponding label embeddings by attention mechanism. The experimental results\ndemonstrate that our model achieves the competitive performance on two public\ndatasets.", "published": "2022-09-09 07:27:38", "link": "http://arxiv.org/abs/2209.04156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deriving dynamical systems for language based on the Tolerance Principle", "abstract": "In this research note, I derive explicit dynamical systems for language\nwithin an acquisition-driven framework (Niyogi \\& Berwick, 1997; Niyogi, 2006)\nassuming that children/learners follow the Tolerance Principle (Yang, 2016) to\ndetermine whether a rule is productive during the process of language\nacquisition. I consider different theoretical parameters such as population\nsize (finite vs. infinite) and the number of previous generations that provide\nlearners with data. Multiple simulations of the dynamics obtained here and\napplications to diacrhonic language data are in preparation, so they are not\nincluded in this first note.", "published": "2022-09-09 11:49:55", "link": "http://arxiv.org/abs/2209.04261v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "F-coref: Fast, Accurate and Easy to Use Coreference Resolution", "abstract": "We introduce fastcoref, a python package for fast, accurate, and easy-to-use\nEnglish coreference resolution. The package is pip-installable, and allows two\nmodes: an accurate mode based on the LingMess architecture, providing\nstate-of-the-art coreference accuracy, and a substantially faster model,\nF-coref, which is the focus of this work. F-coref allows to process 2.8K\nOntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the\nLingMess model, and to 12 minutes of the popular AllenNLP coreference model)\nwith only a modest drop in accuracy. The fast speed is achieved through a\ncombination of distillation of a compact model from the LingMess model, and an\nefficient batching implementation using a technique we call leftover batching.\nOur code is available at https://github.com/shon-otmazgin/fastcoref", "published": "2022-09-09 12:52:28", "link": "http://arxiv.org/abs/2209.04280v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Readability Assessment of German Sentences with Transformer\n  Ensembles", "abstract": "Reliable methods for automatic readability assessment have the potential to\nimpact a variety of fields, ranging from machine translation to self-informed\nlearning. Recently, large language models for the German language (such as\nGBERT and GPT-2-Wechsel) have become available, allowing to develop Deep\nLearning based approaches that promise to further improve automatic readability\nassessment. In this contribution, we studied the ability of ensembles of\nfine-tuned GBERT and GPT-2-Wechsel models to reliably predict the readability\nof German sentences. We combined these models with linguistic features and\ninvestigated the dependence of prediction performance on ensemble size and\ncomposition. Mixed ensembles of GBERT and GPT-2-Wechsel performed better than\nensembles of the same size consisting of only GBERT or GPT-2-Wechsel models.\nOur models were evaluated in the GermEval 2022 Shared Task on Text Complexity\nAssessment on data of German sentences. On out-of-sample data, our best\nensemble achieved a root mean squared error of 0.435.", "published": "2022-09-09 13:47:55", "link": "http://arxiv.org/abs/2209.04299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ranking-Enhanced Unsupervised Sentence Representation Learning", "abstract": "Unsupervised sentence representation learning has progressed through\ncontrastive learning and data augmentation methods such as dropout masking.\nDespite this progress, sentence encoders are still limited to using only an\ninput sentence when predicting its semantic vector. In this work, we show that\nthe semantic meaning of a sentence is also determined by nearest-neighbor\nsentences that are similar to the input sentence. Based on this finding, we\npropose a novel unsupervised sentence encoder, RankEncoder. RankEncoder\npredicts the semantic vector of an input sentence by leveraging its\nrelationship with other sentences in an external corpus, as well as the input\nsentence itself. We evaluate RankEncoder on semantic textual benchmark\ndatasets. From the experimental results, we verify that 1) RankEncoder achieves\n80.07% Spearman's correlation, a 1.1% absolute improvement compared to the\nprevious state-of-the-art performance, 2) RankEncoder is universally applicable\nto existing unsupervised sentence embedding methods, and 3) RankEncoder is\nspecifically effective for predicting the similarity scores of similar sentence\npairs.", "published": "2022-09-09 14:45:16", "link": "http://arxiv.org/abs/2209.04333v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trigger Warnings: Bootstrapping a Violence Detector for FanFiction", "abstract": "We present the first dataset and evaluation results on a newly defined\ncomputational task of trigger warning assignment. Labeled corpus data has been\ncompiled from narrative works hosted on Archive of Our Own (AO3), a well-known\nfanfiction site. In this paper, we focus on the most frequently assigned\ntrigger type--violence--and define a document-level binary classification task\nof whether or not to assign a violence trigger warning to a fanfiction,\nexploiting warning labels provided by AO3 authors. SVM and BERT models trained\nin four evaluation setups on the corpora we compiled yield $F_1$ results\nranging from 0.585 to 0.798, proving the violence trigger warning assignment to\nbe a doable, however, non-trivial task.", "published": "2022-09-09 17:27:03", "link": "http://arxiv.org/abs/2209.04409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Simplification of College Admissions Instructions: A Professionally\n  Simplified and Verified Corpus", "abstract": "Access to higher education is critical for minority populations and emergent\nbilingual students. However, the language used by higher education institutions\nto communicate with prospective students is often too complex; concretely, many\ninstitutions in the US publish admissions application instructions far above\nthe average reading level of a typical high school graduate, often near the\n13th or 14th grade level. This leads to an unnecessary barrier between students\nand access to higher education. This work aims to tackle this challenge via\ntext simplification. We present PSAT (Professionally Simplified Admissions\nTexts), a dataset with 112 admissions instructions randomly selected from\nhigher education institutions across the US. These texts are then\nprofessionally simplified, and verified and accepted by subject-matter experts\nwho are full-time employees in admissions offices at various institutions.\nAdditionally, PSAT comes with manual alignments of 1,883 original-simplified\nsentence pairs. The result is a first-of-its-kind corpus for the evaluation and\nfine-tuning of text simplification systems in a high-stakes genre distinct from\nexisting simplification resources.", "published": "2022-09-09 21:12:52", "link": "http://arxiv.org/abs/2209.04529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Alignment of Multi-Task Feature and Label Spaces for Emotion Cause\n  Pair Extraction", "abstract": "Emotion cause pair extraction (ECPE), as one of the derived subtasks of\nemotion cause analysis (ECA), shares rich inter-related features with emotion\nextraction (EE) and cause extraction (CE). Therefore EE and CE are frequently\nutilized as auxiliary tasks for better feature learning, modeled via multi-task\nlearning (MTL) framework by prior works to achieve state-of-the-art (SoTA) ECPE\nresults. However, existing MTL-based methods either fail to simultaneously\nmodel the specific features and the interactive feature in between, or suffer\nfrom the inconsistency of label prediction. In this work, we consider\naddressing the above challenges for improving ECPE by performing two alignment\nmechanisms with a novel A^2Net model. We first propose a feature-task alignment\nto explicitly model the specific emotion-&cause-specific features and the\nshared interactive feature. Besides, an inter-task alignment is implemented, in\nwhich the label distance between the ECPE and the combinations of EE&CE are\nlearned to be narrowed for better label consistency. Evaluations of benchmarks\nshow that our methods outperform current best-performing systems on all ECA\nsubtasks. Further analysis proves the importance of our proposed alignment\nmechanisms for the task.", "published": "2022-09-09 04:06:27", "link": "http://arxiv.org/abs/2209.04112v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Pre-trained Models with Text Structure Knowledge for Question\n  Generation", "abstract": "Today the pre-trained language models achieve great success for question\ngeneration (QG) task and significantly outperform traditional\nsequence-to-sequence approaches. However, the pre-trained models treat the\ninput passage as a flat sequence and are thus not aware of the text structure\nof input passage. For QG task, we model text structure as answer position and\nsyntactic dependency, and propose answer localness modeling and syntactic mask\nattention to address these limitations. Specially, we present localness\nmodeling with a Gaussian bias to enable the model to focus on answer-surrounded\ncontext, and propose a mask attention mechanism to make the syntactic structure\nof input passage accessible in question generation process. Experiments on\nSQuAD dataset show that our proposed two modules improve performance over the\nstrong pre-trained model ProphetNet, and combing them together achieves very\ncompetitive results with the state-of-the-art pre-trained model.", "published": "2022-09-09 08:33:47", "link": "http://arxiv.org/abs/2209.04179v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric\n  View", "abstract": "Multi-Document Scientific Summarization (MDSS) aims to produce coherent and\nconcise summaries for clusters of topic-relevant scientific papers. This task\nrequires precise understanding of paper content and accurate modeling of\ncross-paper relationships. Knowledge graphs convey compact and interpretable\nstructured information for documents, which makes them ideal for content\nmodeling and relationship modeling. In this paper, we present KGSum, an MDSS\nmodel centred on knowledge graphs during both the encoding and decoding\nprocess. Specifically, in the encoding process, two graph-based modules are\nproposed to incorporate knowledge graph information into paper encoding, while\nin the decoding process, we propose a two-stage decoder by first generating\nknowledge graph information of summary in the form of descriptive sentences,\nfollowed by generating the final summary. Empirical results show that the\nproposed architecture brings substantial improvements over baselines on the\nMulti-Xscience dataset.", "published": "2022-09-09 14:20:59", "link": "http://arxiv.org/abs/2209.04319v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Activity report analysis with automatic single or multispan answer\n  extraction", "abstract": "In the era of loT (Internet of Things) we are surrounded by a plethora of Al\nenabled devices that can transcribe images, video, audio, and sensors signals\ninto text descriptions. When such transcriptions are captured in activity\nreports for monitoring, life logging and anomaly detection applications, a user\nwould typically request a summary or ask targeted questions about certain\nsections of the report they are interested in. Depending on the context and the\ntype of question asked, a question answering (QA) system would need to\nautomatically determine whether the answer covers single-span or multi-span\ntext components. Currently available QA datasets primarily focus on single span\nresponses only (such as SQuAD[4]) or contain a low proportion of examples with\nmultiple span answers (such as DROP[3]). To investigate automatic selection of\nsingle/multi-span answers in the use case described, we created a new smart\nhome environment dataset comprised of questions paired with single-span or\nmulti-span answers depending on the question and context queried. In addition,\nwe propose a RoBERTa[6]-based multiple span extraction question answering\n(MSEQA) model returning the appropriate answer span for a given question. Our\nexperiments show that the proposed model outperforms state-of-the-art QA models\non our dataset while providing comparable performance on published individual\nsingle/multi-span task datasets.", "published": "2022-09-09 06:33:29", "link": "http://arxiv.org/abs/2209.09316v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "T-NER: An All-Round Python Library for Transformer-based Named Entity\n  Recognition", "abstract": "Language model (LM) pretraining has led to consistent improvements in many\nNLP downstream tasks, including named entity recognition (NER). In this paper,\nwe present T-NER (Transformer-based Named Entity Recognition), a Python library\nfor NER LM finetuning. In addition to its practical utility, T-NER facilitates\nthe study and investigation of the cross-domain and cross-lingual\ngeneralization ability of LMs finetuned on NER. Our library also provides a web\napp where users can get model predictions interactively for arbitrary text,\nwhich facilitates qualitative model evaluation for non-expert programmers. We\nshow the potential of the library by compiling nine public NER datasets into a\nunified format and evaluating the cross-domain and cross-lingual performance\nacross the datasets. The results from our initial experiments show that\nin-domain performance is generally competitive across datasets. However,\ncross-domain generalization is challenging even with a large pretrained LM,\nwhich has nevertheless capacity to learn domain-specific features if fine-tuned\non a combined dataset. To facilitate future research, we also release all our\nLM checkpoints via the Hugging Face model hub.", "published": "2022-09-09 15:00:38", "link": "http://arxiv.org/abs/2209.12616v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MICO: Selective Search with Mutual Information Co-training", "abstract": "In contrast to traditional exhaustive search, selective search first clusters\ndocuments into several groups before all the documents are searched\nexhaustively by a query, to limit the search executed within one group or only\na few groups. Selective search is designed to reduce the latency and\ncomputation in modern large-scale search systems. In this study, we propose\nMICO, a Mutual Information CO-training framework for selective search with\nminimal supervision using the search logs. After training, MICO does not only\ncluster the documents, but also routes unseen queries to the relevant clusters\nfor efficient retrieval. In our empirical experiments, MICO significantly\nimproves the performance on multiple metrics of selective search and\noutperforms a number of existing competitive baselines.", "published": "2022-09-09 16:26:52", "link": "http://arxiv.org/abs/2209.04378v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "PoxVerifi: An Information Verification System to Combat Monkeypox\n  Misinformation", "abstract": "Following recent outbreaks, monkeypox-related misinformation continues to\nrapidly spread online. This negatively impacts response strategies and\ndisproportionately harms LGBTQ+ communities in the short-term, and ultimately\nundermines the overall effectiveness of public health responses. In an attempt\nto combat monkeypox-related misinformation, we present PoxVerifi, an\nopen-source, extensible tool that provides a comprehensive approach to\nassessing the accuracy of monkeypox related claims. Leveraging information from\nexisting fact checking sources and published World Health Organization (WHO)\ninformation, we created an open-sourced corpus of 225 rated monkeypox claims.\nAdditionally, we trained an open-sourced BERT-based machine learning model for\nspecifically classifying monkeypox information, which achieved 96%\ncross-validation accuracy. PoxVerifi is a Google Chrome browser extension\ndesigned to empower users to navigate through monkeypox-related misinformation.\nSpecifically, PoxVerifi provides users with a comprehensive toolkit to assess\nthe veracity of headlines on any webpage across the Internet without having to\nvisit an external site. Users can view an automated accuracy review from our\ntrained machine learning model, a user-generated accuracy review based on\ncommunity-member votes, and have the ability to see similar, vetted, claims.\nBesides PoxVerifi's comprehensive approach to claim-testing, our platform\nprovides an efficient and accessible method to crowdsource accuracy ratings on\nmonkeypox related-claims, which can be aggregated to create new labeled\nmisinformation datasets.", "published": "2022-09-09 02:50:47", "link": "http://arxiv.org/abs/2209.09300v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "DeID-VC: Speaker De-identification via Zero-shot Pseudo Voice Conversion", "abstract": "The widespread adoption of speech-based online services raises security and\nprivacy concerns regarding the data that they use and share. If the data were\ncompromised, attackers could exploit user speech to bypass speaker verification\nsystems or even impersonate users. To mitigate this, we propose DeID-VC, a\nspeaker de-identification system that converts a real speaker to pseudo\nspeakers, thus removing or obfuscating the speaker-dependent attributes from a\nspoken voice. The key components of DeID-VC include a Variational Autoencoder\n(VAE) based Pseudo Speaker Generator (PSG) and a voice conversion Autoencoder\n(AE) under zero-shot settings. With the help of PSG, DeID-VC can assign unique\npseudo speakers at speaker level or even at utterance level. Also, two novel\nlearning objectives are added to bridge the gap between training and inference\nof zero-shot voice conversion. We present our experimental results with word\nerror rate (WER) and equal error rate (EER), along with three subjective\nmetrics to evaluate the generated output of DeID-VC. The result shows that our\nmethod substantially improved intelligibility (WER 10% lower) and\nde-identification effectiveness (EER 5% higher) compared to our baseline. Code\nand listening demo: https://github.com/a43992899/DeID-VC", "published": "2022-09-09 21:13:08", "link": "http://arxiv.org/abs/2209.04530v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Streaming Target-Speaker ASR with Neural Transducer", "abstract": "Although recent advances in deep learning technology have boosted automatic\nspeech recognition (ASR) performance in the single-talker case, it remains\ndifficult to recognize multi-talker speech in which many voices overlap. One\nconventional approach to tackle this problem is to use a cascade of a speech\nseparation or target speech extraction front-end with an ASR back-end. However,\nthe extra computation costs of the front-end module are a critical barrier to\nquick response, especially for streaming ASR. In this paper, we propose a\ntarget-speaker ASR (TS-ASR) system that implicitly integrates the target speech\nextraction functionality within a streaming end-to-end (E2E) ASR system, i.e.\nrecurrent neural network-transducer (RNNT). Our system uses a similar idea as\nadopted for target speech extraction, but implements it directly at the level\nof the encoder of RNNT. This allows TS-ASR to be realized without placing extra\ncomputation costs on the front-end. Note that this study presents two major\ndifferences between prior studies on E2E TS-ASR; we investigate streaming\nmodels and base our study on Conformer models, whereas prior studies used\nRNN-based systems and considered only offline processing. We confirm in\nexperiments that our TS-ASR achieves comparable recognition performance with\nconventional cascade systems in the offline setting, while reducing computation\ncosts and realizing streaming TS-ASR.", "published": "2022-09-09 08:21:41", "link": "http://arxiv.org/abs/2209.04175v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Analytics-based Human Trafficking Detection Framework for\n  Autonomous Vehicles", "abstract": "Human trafficking is a universal problem, persistent despite numerous efforts\nto combat it globally. Individuals of any age, race, ethnicity, sex, gender\nidentity, sexual orientation, nationality, immigration status, cultural\nbackground, religion, socioeconomic class, and education can be a victim of\nhuman trafficking. With the advancements in technology and the introduction of\nautonomous vehicles (AVs), human traffickers will adopt new ways to transport\nvictims, which could accelerate the growth of organized human trafficking\nnetworks, which can make the detection of trafficking in persons more\nchallenging for law enforcement agencies. The objective of this study is to\ndevelop an innovative audio analytics-based human trafficking detection\nframework for autonomous vehicles. The primary contributions of this study are\nto: (i) define four non-trivial, feasible, and realistic human trafficking\nscenarios for AVs; (ii) create a new and comprehensive audio dataset related to\nhuman trafficking with five classes i.e., crying, screaming, car door banging,\ncar noise, and conversation; and (iii) develop a deep 1-D Convolution Neural\nNetwork (CNN) architecture for audio data classification related to human\ntrafficking. We have also conducted a case study using the new audio dataset\nand evaluated the audio classification performance of the deep 1-D CNN. Our\nanalyses reveal that the deep 1-D CNN can distinguish sound coming from a human\ntrafficking victim from a non-human trafficking sound with an accuracy of 95%,\nwhich proves the efficacy of our framework.", "published": "2022-09-09 01:06:50", "link": "http://arxiv.org/abs/2209.04071v1", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Improving the Environmental Perception of Autonomous Vehicles using Deep\n  Learning-based Audio Classification", "abstract": "Sense of hearing is crucial for autonomous vehicles (AVs) to better perceive\nits surrounding environment. Although visual sensors of an AV, such as camera,\nlidar, and radar, help to see its surrounding environment, an AV cannot see\nbeyond those sensors line of sight. On the other hand, an AV s sense of hearing\ncannot be obstructed by line of sight. For example, an AV can identify an\nemergency vehicle s siren through audio classification even though the\nemergency vehicle is not within the line of sight of the AV. Thus, auditory\nperception is complementary to the camera, lidar, and radar-based perception\nsystems. This paper presents a deep learning-based robust audio classification\nframework aiming to achieve improved environmental perception for AVs. The\npresented framework leverages a deep Convolution Neural Network (CNN) to\nclassify different audio classes. UrbanSound8k, an urban environment dataset,\nis used to train and test the developed framework. Seven audio classes i.e.,\nair conditioner, car horn, children playing, dog bark, engine idling, gunshot,\nand siren, are identified from the UrbanSound8k dataset because of their\nrelevancy related to AVs. Our framework can classify different audio classes\nwith 97.82% accuracy. Moreover, the audio classification accuracies with all\nten classes are presented, which proves that our framework performed better in\nthe case of AV-related sounds compared to the existing audio classification\nframeworks.", "published": "2022-09-09 01:23:13", "link": "http://arxiv.org/abs/2209.04075v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prediction method of Soundscape Impressions using Environmental Sounds\n  and Aerial Photographs", "abstract": "We investigate an method for quantifying city characteristics based on\nimpressions of a sound environment. The quantification of the city\ncharacteristics will be beneficial to government policy planning, tourism\nprojects, etc. In this study, we try to predict two soundscape impressions,\nmeaning pleasantness and eventfulness, using sound data collected by the\ncloud-sensing method. The collected sounds comprise meta information of\nrecording location using Global Positioning System. Furthermore, the soundscape\nimpressions and sound-source features are separately assigned to the\ncloud-sensing sounds by assessments defined using Swedish Soundscape-Quality\nProtocol, assessing the quality of the acoustic environment. The prediction\nmodels are built using deep neural networks with multi-layer perceptron for the\ninput of 10-second sound and the aerial photographs of its location. An\nacoustic feature comprises equivalent noise level and outputs of octave-band\nfilters every second, and statistics of them in 10~s. An image feature is\nextracted from an aerial photograph using ResNet-50 and autoencoder\narchitecture. We perform comparison experiments to demonstrate the benefit of\neach feature. As a result of the comparison, aerial photographs and\nsound-source features are efficient to predict impression information.\nAdditionally, even if the sound-source features are predicted using acoustic\nand image features, the features also show fine results to predict the\nsoundscape impression close to the result of oracle sound-source features.", "published": "2022-09-09 01:41:23", "link": "http://arxiv.org/abs/2209.04077v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Audio-Visual embedding for Person Verification in the Wild", "abstract": "It has already been observed that audio-visual embedding is more robust than\nuni-modality embedding for person verification. Here, we proposed a novel\naudio-visual strategy that considers aggregators from a fusion perspective.\nFirst, we introduced weight-enhanced attentive statistics pooling for the first\ntime in face verification. We find that a strong correlation exists between\nmodalities during pooling, so joint attentive pooling is proposed which\ncontains cycle consistency to learn the implicit inter-frame weight. Finally,\neach modality is fused with a gated attention mechanism to gain robust\naudio-visual embedding. All the proposed models are trained on the VoxCeleb2\ndev dataset and the best system obtains 0.18%, 0.27%, and 0.49% EER on three\nofficial trial lists of VoxCeleb1 respectively, which is to our knowledge the\nbest-published results for person verification.", "published": "2022-09-09 02:29:47", "link": "http://arxiv.org/abs/2209.04093v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Overlapped speech and gender detection with WavLM pre-trained features", "abstract": "This article focuses on overlapped speech and gender detection in order to\nstudy interactions between women and men in French audiovisual media (Gender\nEquality Monitoring project). In this application context, we need to\nautomatically segment the speech signal according to speakers gender, and to\nidentify when at least two speakers speak at the same time. We propose to use\nWavLM model which has the advantage of being pre-trained on a huge amount of\nspeech data, to build an overlapped speech detection (OSD) and a gender\ndetection (GD) systems. In this study, we use two different corpora. The DIHARD\nIII corpus which is well adapted for the OSD task but lack gender information.\nThe ALLIES corpus fits with the project application context. Our best OSD\nsystem is a Temporal Convolutional Network (TCN) with WavLM pre-trained\nfeatures as input, which reaches a new state-of-the-art F1-score performance on\nDIHARD. A neural GD is trained with WavLM inputs on a gender balanced subset of\nthe French broadcast news ALLIES data, and obtains an accuracy of 97.9%. This\nwork opens new perspectives for human science researchers regarding the\ndifferences of representation between women and men in French media.", "published": "2022-09-09 08:00:47", "link": "http://arxiv.org/abs/2209.04167v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Semi-Supervised Algorithm for Improving the Consistency of\n  Crowdsourced Datasets: The COVID-19 Case Study on Respiratory Disorder\n  Classification", "abstract": "Cough audio signal classification is a potentially useful tool in screening\nfor respiratory disorders, such as COVID-19. Since it is dangerous to collect\ndata from patients with such contagious diseases, many research teams have\nturned to crowdsourcing to quickly gather cough sound data, as it was done to\ngenerate the COUGHVID dataset. The COUGHVID dataset enlisted expert physicians\nto diagnose the underlying diseases present in a limited number of uploaded\nrecordings. However, this approach suffers from potential mislabeling of the\ncoughs, as well as notable disagreement between experts. In this work, we use a\nsemi-supervised learning (SSL) approach to improve the labeling consistency of\nthe COUGHVID dataset and the robustness of COVID-19 versus healthy cough sound\nclassification. First, we leverage existing SSL expert knowledge aggregation\ntechniques to overcome the labeling inconsistencies and sparsity in the\ndataset. Next, our SSL approach is used to identify a subsample of re-labeled\nCOUGHVID audio samples that can be used to train or augment future cough\nclassification models. The consistency of the re-labeled data is demonstrated\nin that it exhibits a high degree of class separability, 3x higher than that of\nthe user-labeled data, despite the expert label inconsistency present in the\noriginal dataset. Furthermore, the spectral differences in the user-labeled\naudio segments are amplified in the re-labeled data, resulting in significantly\ndifferent power spectral densities between healthy and COVID-19 coughs, which\ndemonstrates both the increased consistency of the new dataset and its\nexplainability from an acoustic perspective. Finally, we demonstrate how the\nre-labeled dataset can be used to train a cough classifier. This SSL approach\ncan be used to combine the medical knowledge of several experts to improve the\ndatabase consistency for any diagnostic classification task.", "published": "2022-09-09 15:44:26", "link": "http://arxiv.org/abs/2209.04360v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Longitudinal Acoustic Speech Tracking Following Pediatric Traumatic\n  Brain Injury", "abstract": "Recommendations for common outcome measures following pediatric traumatic\nbrain injury (TBI) support the integration of instrumental measurements\nalongside perceptual assessment in recovery and treatment plans. A\ncomprehensive set of sensitive, robust and non-invasive measurements is\ntherefore essential in assessing variations in speech characteristics over time\nfollowing pediatric TBI. In this article, we study the changes in the acoustic\nspeech patterns of a pediatric cohort of ten subjects diagnosed with severe\nTBI. We extract a diverse set of both well-known and novel acoustic features\nfrom child speech recorded throughout the year after the child produced\nintelligible words. These features are analyzed individually and by speech\nsubsystem, within-subject and across the cohort. As a group, older children\nexhibit highly significant (p<0.01) increases in pitch variation and phoneme\ndiversity, shortened pause length, and steadying articulation rate variability.\nYounger children exhibit similar steadied rate variability alongside an\nincrease in formant-based articulation complexity. Correlation analysis of the\nfeature set with age and comparisons to normative developmental data confirm\nthat age at injury plays a significant role in framing the recovery trajectory.\nNearly all speech features significantly change (p<0.05) for the cohort as a\nwhole, confirming that acoustic measures supplementing perceptual assessment\nare needed to identify efficacious treatment targets for speech therapy\nfollowing TBI.", "published": "2022-09-09 17:18:41", "link": "http://arxiv.org/abs/2209.04406v1", "categories": ["q-bio.NC", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Reconstructing the Dynamic Directivity of Unconstrained Speech", "abstract": "This article presents a method for estimating and reconstructing the spatial\nenergy distribution pattern of natural speech, which is crucial for achieving\nrealistic vocal presence in virtual communication settings. The method\ncomprises two stages. First, recordings of speech captured by a real, static\nmicrophone array are used to create an egocentric virtual array that tracks the\nmovement of the speaker over time. This virtual array is used to measure and\nencode the high-resolution directivity pattern of the speech signal as it\nevolves dynamically with natural speech and movement. In the second stage, the\nencoded directivity representation is utilized to train a machine learning\nmodel that can estimate the full, dynamic directivity pattern given a limited\nset of speech signals, such as those recorded using the microphones on a\nhead-mounted display. Our results show that neural networks can accurately\nestimate the full directivity pattern of natural, unconstrained speech from\nlimited information. The proposed method for estimating and reconstructing the\nspatial energy distribution pattern of natural speech, along with the\nevaluation of various machine learning models and training paradigms, provides\nan important contribution to the development of realistic vocal presence in\nvirtual communication settings.", "published": "2022-09-09 18:06:46", "link": "http://arxiv.org/abs/2209.04473v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MATT: A Multiple-instance Attention Mechanism for Long-tail Music Genre\n  Classification", "abstract": "Imbalanced music genre classification is a crucial task in the Music\nInformation Retrieval (MIR) field for identifying the long-tail, data-poor\ngenre based on the related music audio segments, which is very prevalent in\nreal-world scenarios. Most of the existing models are designed for\nclass-balanced music datasets, resulting in poor performance in accuracy and\ngeneralization when identifying the music genres at the tail of the\ndistribution. Inspired by the success of introducing Multi-instance Learning\n(MIL) in various classification tasks, we propose a novel mechanism named\nMulti-instance Attention (MATT) to boost the performance for identifying tail\nclasses. Specifically, we first construct the bag-level datasets by generating\nthe album-artist pair bags. Second, we leverage neural networks to encode the\nmusic audio segments. Finally, under the guidance of a multi-instance attention\nmechanism, the neural network-based models could select the most informative\ngenre to match the given music segment. Comprehensive experimental results on a\nlarge-scale music genre benchmark dataset with long-tail distribution\ndemonstrate MATT significantly outperforms other state-of-the-art baselines.", "published": "2022-09-09 03:52:44", "link": "http://arxiv.org/abs/2209.04109v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Defend Data Poisoning Attacks on Voice Authentication", "abstract": "With the advances in deep learning, speaker verification has achieved very\nhigh accuracy and is gaining popularity as a type of biometric authentication\noption in many scenes of our daily life, especially the growing market of web\nservices. Compared to traditional passwords, \"vocal passwords\" are much more\nconvenient as they relieve people from memorizing different passwords. However,\nnew machine learning attacks are putting these voice authentication systems at\nrisk. Without a strong security guarantee, attackers could access legitimate\nusers' web accounts by fooling the deep neural network (DNN) based voice\nrecognition models. In this paper, we demonstrate an easy-to-implement data\npoisoning attack to the voice authentication system, which can hardly be\ncaptured by existing defense mechanisms. Thus, we propose a more robust defense\nmethod, called Guardian, which is a convolutional neural network-based\ndiscriminator. The Guardian discriminator integrates a series of novel\ntechniques including bias reduction, input augmentation, and ensemble learning.\nOur approach is able to distinguish about 95% of attacked accounts from normal\naccounts, which is much more effective than existing approaches with only 60%\naccuracy.", "published": "2022-09-09 22:48:35", "link": "http://arxiv.org/abs/2209.04547v2", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
