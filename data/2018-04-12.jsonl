{"title": "Training a Ranking Function for Open-Domain Question Answering", "abstract": "In recent years, there have been amazing advances in deep learning methods\nfor machine reading. In machine reading, the machine reader has to extract the\nanswer from the given ground truth paragraph. Recently, the state-of-the-art\nmachine reading models achieve human level performance in SQuAD which is a\nreading comprehension-style question answering (QA) task. The success of\nmachine reading has inspired researchers to combine information retrieval with\nmachine reading to tackle open-domain QA. However, these systems perform poorly\ncompared to reading comprehension-style QA because it is difficult to retrieve\nthe pieces of paragraphs that contain the answer to the question. In this\nstudy, we propose two neural network rankers that assign scores to different\npassages based on their likelihood of containing the answer to a given\nquestion. Additionally, we analyze the relative importance of semantic\nsimilarity and word level relevance matching in open-domain QA.", "published": "2018-04-12 00:25:45", "link": "http://arxiv.org/abs/1804.04264v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Capsule Network-based Embedding Model for Search Personalization", "abstract": "Search personalization aims to tailor search results to each specific user\nbased on the user's personal interests and preferences (i.e., the user\nprofile). Recent research approaches to search personalization by modelling the\npotential 3-way relationship between the submitted query, the user and the\nsearch results (i.e., documents). That relationship is then used to personalize\nthe search results to that user. In this paper, we introduce a novel embedding\nmodel based on capsule network, which recently is a breakthrough in deep\nlearning, to model the 3-way relationships for search personalization. In the\nmodel, each user (submitted query or returned document) is embedded by a vector\nin the same vector space. The 3-way relationship is described as a triple of\n(query, user, document) which is then modeled as a 3-column matrix containing\nthe three embedding vectors. After that, the 3-column matrix is fed into a deep\nlearning architecture to re-rank the search results returned by a basis ranker.\nExperimental results on query logs from a commercial web search engine show\nthat our model achieves better performances than the basis ranker as well as\nstrong search personalization baselines.", "published": "2018-04-12 00:36:53", "link": "http://arxiv.org/abs/1804.04266v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Amobee at SemEval-2018 Task 1: GRU Neural Network with a CNN Attention\n  Mechanism for Sentiment Classification", "abstract": "This paper describes the participation of Amobee in the shared sentiment\nanalysis task at SemEval 2018. We participated in all the English sub-tasks and\nthe Spanish valence tasks. Our system consists of three parts: training\ntask-specific word embeddings, training a model consisting of\ngated-recurrent-units (GRU) with a convolution neural network (CNN) attention\nmechanism and training stacking-based ensembles for each of the sub-tasks. Our\nalgorithm reached 3rd and 1st places in the valence ordinal classification\nsub-tasks in English and Spanish, respectively.", "published": "2018-04-12 09:04:50", "link": "http://arxiv.org/abs/1804.04380v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning Multilingual Embeddings for Cross-Lingual Information Retrieval\n  in the Presence of Topically Aligned Corpora", "abstract": "Cross-lingual information retrieval is a challenging task in the absence of\naligned parallel corpora. In this paper, we address this problem by considering\ntopically aligned corpora designed for evaluating an IR setup. To emphasize, we\nneither use any sentence-aligned corpora or document-aligned corpora, nor do we\nuse any language specific resources such as dictionary, thesaurus, or grammar\nrules. Instead, we use an embedding into a common space and learn word\ncorrespondences directly from there. We test our proposed approach for\nbilingual IR on standard FIRE datasets for Bangla, Hindi and English. The\nproposed method is superior to the state-of-the-art method not only for IR\nevaluation measures but also in terms of time requirements. We extend our\nmethod successfully to the trilingual setting.", "published": "2018-04-12 12:46:08", "link": "http://arxiv.org/abs/1804.04475v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "EventKG: A Multilingual Event-Centric Temporal Knowledge Graph", "abstract": "One of the key requirements to facilitate semantic analytics of information\nregarding contemporary and historical events on the Web, in the news and in\nsocial media is the availability of reference knowledge repositories containing\ncomprehensive representations of events and temporal relations. Existing\nknowledge graphs, with popular examples including DBpedia, YAGO and Wikidata,\nfocus mostly on entity-centric information and are insufficient in terms of\ntheir coverage and completeness with respect to events and temporal relations.\nEventKG presented in this paper is a multilingual event-centric temporal\nknowledge graph that addresses this gap. EventKG incorporates over 690 thousand\ncontemporary and historical events and over 2.3 million temporal relations\nextracted from several large-scale knowledge graphs and semi-structured sources\nand makes them available through a canonical representation.", "published": "2018-04-12 14:12:48", "link": "http://arxiv.org/abs/1804.04526v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "The Voice Conversion Challenge 2018: Promoting Development of Parallel\n  and Nonparallel Methods", "abstract": "We present the Voice Conversion Challenge 2018, designed as a follow up to\nthe 2016 edition with the aim of providing a common framework for evaluating\nand comparing different state-of-the-art voice conversion (VC) systems. The\nobjective of the challenge was to perform speaker conversion (i.e. transform\nthe vocal identity) of a source speaker to a target speaker while maintaining\nlinguistic information. As an update to the previous challenge, we considered\nboth parallel and non-parallel data to form the Hub and Spoke tasks,\nrespectively. A total of 23 teams from around the world submitted their\nsystems, 11 of them additionally participated in the optional Spoke task. A\nlarge-scale crowdsourced perceptual evaluation was then carried out to rate the\nsubmitted converted speech in terms of naturalness and similarity to the target\nspeaker identity. In this paper, we present a brief summary of the\nstate-of-the-art techniques for VC, followed by a detailed explanation of the\nchallenge tasks and the results that were obtained.", "published": "2018-04-12 00:14:10", "link": "http://arxiv.org/abs/1804.04262v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Sound Event Detection and Time-Frequency Segmentation from Weakly\n  Labelled Data", "abstract": "Sound event detection (SED) aims to detect when and recognize what sound\nevents happen in an audio clip. Many supervised SED algorithms rely on strongly\nlabelled data which contains the onset and offset annotations of sound events.\nHowever, many audio tagging datasets are weakly labelled, that is, only the\npresence of the sound events is known, without knowing their onset and offset\nannotations. In this paper, we propose a time-frequency (T-F) segmentation\nframework trained on weakly labelled data to tackle the sound event detection\nand separation problem. In training, a segmentation mapping is applied on a T-F\nrepresentation, such as log mel spectrogram of an audio clip to obtain T-F\nsegmentation masks of sound events. The T-F segmentation masks can be used for\nseparating the sound events from the background scenes in the time-frequency\ndomain. Then a classification mapping is applied on the T-F segmentation masks\nto estimate the presence probabilities of the sound events. We model the\nsegmentation mapping using a convolutional neural network and the\nclassification mapping using a global weighted rank pooling (GWRP). In SED,\npredicted onset and offset times can be obtained from the T-F segmentation\nmasks. As a byproduct, separated waveforms of sound events can be obtained from\nthe T-F segmentation masks. We remixed the DCASE 2018 Task 1 acoustic scene\ndata with the DCASE 2018 Task 2 sound events data. When mixing under 0 dB, the\nproposed method achieved F1 scores of 0.534, 0.398 and 0.167 in audio tagging,\nframe-wise SED and event-wise SED, outperforming the fully connected deep\nneural network baseline of 0.331, 0.237 and 0.120, respectively. In T-F\nsegmentation, we achieved an F1 score of 0.218, where previous methods were not\nable to do T-F segmentation.", "published": "2018-04-12 20:20:29", "link": "http://arxiv.org/abs/1804.04715v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Global SNR Estimation of Speech Signals using Entropy and Uncertainty\n  Estimates from Dropout Networks", "abstract": "This paper demonstrates two novel methods to estimate the global SNR of\nspeech signals. In both methods, Deep Neural Network-Hidden Markov Model\n(DNN-HMM) acoustic model used in speech recognition systems is leveraged for\nthe additional task of SNR estimation. In the first method, the entropy of the\nDNN-HMM output is computed. Recent work on bayesian deep learning has shown\nthat a DNN-HMM trained with dropout can be used to estimate model uncertainty\nby approximating it as a deep Gaussian process. In the second method, this\napproximation is used to obtain model uncertainty estimates. Noise specific\nregressors are used to predict the SNR from the entropy and model uncertainty.\nThe DNN-HMM is trained on GRID corpus and tested on different noise profiles\nfrom the DEMAND noise database at SNR levels ranging from -10 dB to 30 dB.", "published": "2018-04-12 07:15:20", "link": "http://arxiv.org/abs/1804.04353v1", "categories": ["eess.AS", "cs.AI", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
