{"title": "Moment Matching Training for Neural Machine Translation: A Preliminary\n  Study", "abstract": "In previous works, neural sequence models have been shown to improve\nsignificantly if external prior knowledge can be provided, for instance by\nallowing the model to access the embeddings of explicit features during both\ntraining and inference. In this work, we propose a different point of view on\nhow to incorporate prior knowledge in a principled way, using a moment matching\nframework. In this approach, the standard local cross-entropy training of the\nsequential model is combined with a moment matching training mode that\nencourages the equality of the expectations of certain predefined features\nbetween the model distribution and the empirical distribution. In particular,\nwe show how to derive unbiased estimates of some stochastic gradients that are\ncentral to the training, and compare our framework with a formally related one:\npolicy gradient training in reinforcement learning, pointing out some important\ndifferences in terms of the kinds of prior assumptions in both approaches. Our\ninitial results are promising, showing the effectiveness of our proposed\nframework.", "published": "2018-12-24 05:29:19", "link": "http://arxiv.org/abs/1812.09836v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
