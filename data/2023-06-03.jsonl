{"title": "GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and\n  Linguistic Evaluation", "abstract": "We present GENTLE, a new mixed-genre English challenge corpus totaling 17K\ntokens and consisting of 8 unusual text types for out-of domain evaluation:\ndictionary entries, esports commentaries, legal documents, medical notes,\npoetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually\nannotated for a variety of popular NLP tasks, including syntactic dependency\nparsing, entity recognition, coreference resolution, and discourse parsing. We\nevaluate state-of-the-art NLP systems on GENTLE and find severe degradation for\nat least some genres in their performance on all tasks, which indicates\nGENTLE's utility as an evaluation dataset for NLP systems.", "published": "2023-06-03 00:20:15", "link": "http://arxiv.org/abs/2306.01966v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive\n  Statements", "abstract": "Warning: This paper contains content that may be offensive or upsetting.\nUnderstanding the harms and offensiveness of statements requires reasoning\nabout the social and situational context in which statements are made. For\nexample, the utterance \"your English is very good\" may implicitly signal an\ninsult when uttered by a white man to a non-white colleague, but uttered by an\nESL teacher to their student would be interpreted as a genuine compliment. Such\ncontextual factors have been largely ignored by previous approaches to toxic\nlanguage detection. We introduce COBRA frames, the first context-aware\nformalism for explaining the intents, reactions, and harms of offensive or\nbiased statements grounded in their social and situational context. We create\nCOBRACORPUS, a dataset of 33k potentially offensive statements paired with\nmachine-generated contexts and free-text explanations of offensiveness, implied\nbiases, speaker intents, and listener reactions. To study the contextual\ndynamics of offensiveness, we train models to generate COBRA explanations, with\nand without access to the context. We find that explanations by\ncontext-agnostic models are significantly worse than by context-aware ones,\nespecially in situations where the context inverts the statement's\noffensiveness (29% accuracy drop). Our work highlights the importance and\nfeasibility of contextualized NLP by modeling social factors.", "published": "2023-06-03 02:47:24", "link": "http://arxiv.org/abs/2306.01985v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ACI-BENCH: a Novel Ambient Clinical Intelligence Dataset for\n  Benchmarking Automatic Visit Note Generation", "abstract": "Recent immense breakthroughs in generative models such as in GPT4 have\nprecipitated re-imagined ubiquitous usage of these models in all applications.\nOne area that can benefit by improvements in artificial intelligence (AI) is\nhealthcare. The note generation task from doctor-patient encounters, and its\nassociated electronic medical record documentation, is one of the most arduous\ntime-consuming tasks for physicians. It is also a natural prime potential\nbeneficiary to advances in generative models. However with such advances,\nbenchmarking is more critical than ever. Whether studying model weaknesses or\ndeveloping new evaluation metrics, shared open datasets are an imperative part\nof understanding the current state-of-the-art. Unfortunately as clinic\nencounter conversations are not routinely recorded and are difficult to\nethically share due to patient confidentiality, there are no sufficiently large\nclinic dialogue-note datasets to benchmark this task. Here we present the\nAmbient Clinical Intelligence Benchmark (ACI-BENCH) corpus, the largest dataset\nto date tackling the problem of AI-assisted note generation from visit\ndialogue. We also present the benchmark performances of several common\nstate-of-the-art approaches.", "published": "2023-06-03 06:42:17", "link": "http://arxiv.org/abs/2306.02022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of translation on biomedical information extraction from\n  real-life clinical notes", "abstract": "The objective of our study is to determine whether using English tools to\nextract and normalize French medical concepts on translations provides\ncomparable performance to French models trained on a set of annotated French\nclinical notes. We compare two methods: a method involving French language\nmodels and a method involving English language models. For the native French\nmethod, the Named Entity Recognition (NER) and normalization steps are\nperformed separately. For the translated English method, after the first\ntranslation step, we compare a two-step method and a terminology-oriented\nmethod that performs extraction and normalization at the same time. We used\nFrench, English and bilingual annotated datasets to evaluate all steps (NER,\nnormalization and translation) of our algorithms. Concerning the results, the\nnative French method performs better than the translated English one with a\nglobal f1 score of 0.51 [0.47;0.55] against 0.39 [0.34;0.44] and 0.38\n[0.36;0.40] for the two English methods tested. In conclusion, despite the\nrecent improvement of the translation models, there is a significant\nperformance difference between the two approaches in favor of the native French\nmethod which is more efficient on French medical texts, even with few annotated\ndocuments.", "published": "2023-06-03 07:48:00", "link": "http://arxiv.org/abs/2306.02042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conflicts, Villains, Resolutions: Towards models of Narrative Media\n  Framing", "abstract": "Despite increasing interest in the automatic detection of media frames in\nNLP, the problem is typically simplified as single-label classification and\nadopts a topic-like view on frames, evading modelling the broader\ndocument-level narrative. In this work, we revisit a widely used\nconceptualization of framing from the communication sciences which explicitly\ncaptures elements of narratives, including conflict and its resolution, and\nintegrate it with the narrative framing of key entities in the story as heroes,\nvictims or villains. We adapt an effective annotation paradigm that breaks a\ncomplex annotation task into a series of simpler binary questions, and present\nan annotated data set of English news articles, and a case study on the framing\nof climate change in articles from news outlets across the political spectrum.\nFinally, we explore automatic multi-label prediction of our frames with\nsupervised and semi-supervised approaches, and present a novel retrieval-based\nmethod which is both effective and transparent in its predictions. We conclude\nwith a discussion of opportunities and challenges for future work on\ndocument-level models of narrative framing.", "published": "2023-06-03 08:50:13", "link": "http://arxiv.org/abs/2306.02052v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending an Event-type Ontology: Adding Verbs and Classes Using\n  Fine-tuned LLMs Suggestions", "abstract": "In this project, we have investigated the use of advanced machine learning\nmethods, specifically fine-tuned large language models, for pre-annotating data\nfor a lexical extension task, namely adding descriptive words (verbs) to an\nexisting (but incomplete, as of yet) ontology of event types. Several research\nquestions have been focused on, from the investigation of a possible heuristics\nto provide at least hints to annotators which verbs to include and which are\noutside the current version of the ontology, to the possible use of the\nautomatic scores to help the annotators to be more efficient in finding a\nthreshold for identifying verbs that cannot be assigned to any existing class\nand therefore they are to be used as seeds for a new class. We have also\ncarefully examined the correlation of the automatic scores with the human\nannotation. While the correlation turned out to be strong, its influence on the\nannotation proper is modest due to its near linearity, even though the mere\nfact of such pre-annotation leads to relatively short annotation times.", "published": "2023-06-03 14:57:47", "link": "http://arxiv.org/abs/2306.02130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TART: Improved Few-shot Text Classification Using Task-Adaptive\n  Reference Transformation", "abstract": "Meta-learning has emerged as a trending technique to tackle few-shot text\nclassification and achieve state-of-the-art performance. However, the\nperformance of existing approaches heavily depends on the inter-class variance\nof the support set. As a result, it can perform well on tasks when the\nsemantics of sampled classes are distinct while failing to differentiate\nclasses with similar semantics. In this paper, we propose a novel Task-Adaptive\nReference Transformation (TART) network, aiming to enhance the generalization\nby transforming the class prototypes to per-class fixed reference points in\ntask-adaptive metric spaces. To further maximize divergence between transformed\nprototypes in task-adaptive metric spaces, TART introduces a discriminative\nreference regularization among transformed prototypes. Extensive experiments\nare conducted on four benchmark datasets and our method demonstrates clear\nsuperiority over the state-of-the-art models in all the datasets. In\nparticular, our model surpasses the state-of-the-art method by 7.4% and 5.4% in\n1-shot and 5-shot classification on the 20 Newsgroups dataset, respectively.", "published": "2023-06-03 18:38:02", "link": "http://arxiv.org/abs/2306.02175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FlairNLP at SemEval-2023 Task 6b: Extraction of Legal Named Entities\n  from Legal Texts using Contextual String Embeddings", "abstract": "Indian court legal texts and processes are essential towards the integrity of\nthe judicial system and towards maintaining the social and political order of\nthe nation. Due to the increase in number of pending court cases, there is an\nurgent need to develop tools to automate many of the legal processes with the\nknowledge of artificial intelligence. In this paper, we employ knowledge\nextraction techniques, specially the named entity extraction of legal entities\nwithin court case judgements. We evaluate several state of the art\narchitectures in the realm of sequence labeling using models trained on a\ncurated dataset of legal texts. We observe that a Bi-LSTM model trained on\nFlair Embeddings achieves the best results, and we also publish the BIO\nformatted dataset as part of this paper.", "published": "2023-06-03 19:38:04", "link": "http://arxiv.org/abs/2306.02182v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stubborn Lexical Bias in Data and Models", "abstract": "In NLP, recent work has seen increased focus on spurious correlations between\nvarious features and labels in training data, and how these influence model\nbehavior. However, the presence and effect of such correlations are typically\nexamined feature by feature. We investigate the cumulative impact on a model of\nmany such intersecting features. Using a new statistical method, we examine\nwhether such spurious patterns in data appear in models trained on the data. We\nselect two tasks -- natural language inference and duplicate-question detection\n-- for which any unigram feature on its own should ideally be uninformative,\nwhich gives us a large pool of automatically extracted features with which to\nexperiment. The large size of this pool allows us to investigate the\nintersection of features spuriously associated with (potentially different)\nlabels. We then apply an optimization approach to *reweight* the training data,\nreducing thousands of spurious correlations, and examine how doing so affects\nmodels trained on the reweighted data. Surprisingly, though this method can\nsuccessfully reduce lexical biases in the training data, we still find strong\nevidence of corresponding bias in the trained models, including worsened bias\nfor slightly more complex features (bigrams). We close with discussion about\nthe implications of our results on what it means to \"debias\" training data, and\nhow issues of data quality can affect model bias.", "published": "2023-06-03 20:12:27", "link": "http://arxiv.org/abs/2306.02190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Context Alignment and Answer-Context Dependencies for Effective\n  Answer Sentence Selection", "abstract": "Answer sentence selection (AS2) in open-domain question answering finds\nanswer for a question by ranking candidate sentences extracted from web\ndocuments. Recent work exploits answer context, i.e., sentences around a\ncandidate, by incorporating them as additional input string to the Transformer\nmodels to improve the correctness scoring. In this paper, we propose to improve\nthe candidate scoring by explicitly incorporating the dependencies between\nquestion-context and answer-context into the final representation of a\ncandidate. Specifically, we use Optimal Transport to compute the question-based\ndependencies among sentences in the passage where the answer is extracted from.\nWe then represent these dependencies as edges in a graph and use Graph\nConvolutional Network to derive the representation of a candidate, a node in\nthe graph. Our proposed model achieves significant improvements on popular AS2\nbenchmarks, i.e., WikiQA and WDRASS, obtaining new state-of-the-art on all\nbenchmarks.", "published": "2023-06-03 20:59:19", "link": "http://arxiv.org/abs/2306.02196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in\n  Sentiment Analysis", "abstract": "Emotion arcs capture how an individual (or a population) feels over time.\nThey are widely used in industry and research; however, there is little work on\nevaluating the automatically generated arcs. This is because of the difficulty\nof establishing the true (gold) emotion arc. Our work, for the first time,\nsystematically and quantitatively evaluates automatically generated emotion\narcs. We also compare two common ways of generating emotion arcs:\nMachine-Learning (ML) models and Lexicon-Only (LexO) methods. By running\nexperiments on 18 diverse datasets in 9 languages, we show that despite being\nmarkedly poor at instance level emotion classification, LexO methods are highly\naccurate at generating emotion arcs when aggregating information from hundreds\nof instances. We also show, through experiments on six indigenous African\nlanguages, as well as Arabic, and Spanish, that automatic translations of\nEnglish emotion lexicons can be used to generate high-quality emotion arcs in\nless-resource languages. This opens up avenues for work on emotions in\nlanguages from around the world; which is crucial for commerce, public policy,\nand health research in service of speakers often left behind. Code and\nresources: https://github.com/dteodore/EmotionArcs", "published": "2023-06-03 23:34:55", "link": "http://arxiv.org/abs/2306.02213v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey on Relation Extraction: Recent Advances and New\n  Frontiers", "abstract": "Relation extraction (RE) involves identifying the relations between entities\nfrom underlying content. RE serves as the foundation for many natural language\nprocessing (NLP) and information retrieval applications, such as knowledge\ngraph completion and question answering. In recent years, deep neural networks\nhave dominated the field of RE and made noticeable progress. Subsequently, the\nlarge pre-trained language models have taken the state-of-the-art RE to a new\nlevel. This survey provides a comprehensive review of existing deep learning\ntechniques for RE. First, we introduce RE resources, including datasets and\nevaluation metrics. Second, we propose a new taxonomy to categorize existing\nworks from three perspectives, i.e., text representation, context encoding, and\ntriplet prediction. Third, we discuss several important challenges faced by RE\nand summarize potential techniques to tackle these challenges. Finally, we\noutline some promising future directions and prospects in this field. This\nsurvey is expected to facilitate researchers' collaborative efforts to address\nthe challenges of real-world RE systems.", "published": "2023-06-03 08:39:25", "link": "http://arxiv.org/abs/2306.02051v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Conditional Generative Chatbot using Transformer Model", "abstract": "A Chatbot serves as a communication tool between a human user and a machine\nto achieve an appropriate answer based on the human input. In more recent\napproaches, a combination of Natural Language Processing and sequential models\nare used to build a generative Chatbot. The main challenge of these models is\ntheir sequential nature, which leads to less accurate results. To tackle this\nchallenge, in this paper, a novel architecture is proposed using conditional\nWasserstein Generative Adversarial Networks and a transformer model for answer\ngeneration in Chatbots. While the generator of the proposed model consists of a\nfull transformer model to generate an answer, the discriminator includes only\nthe encoder part of a transformer model followed by a classifier. To the best\nof our knowledge, this is the first time that a generative Chatbot is proposed\nusing the embedded transformer in both generator and discriminator models.\nRelying on the parallel computing of the transformer model, the results of the\nproposed model on the Cornell Movie-Dialog corpus and the Chit-Chat datasets\nconfirm the superiority of the proposed model compared to state-of-the-art\nalternatives using different evaluation metrics.", "published": "2023-06-03 10:35:04", "link": "http://arxiv.org/abs/2306.02074v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Utilizing ChatGPT to Enhance Clinical Trial Enrollment", "abstract": "Clinical trials are a critical component of evaluating the effectiveness of\nnew medical interventions and driving advancements in medical research.\nTherefore, timely enrollment of patients is crucial to prevent delays or\npremature termination of trials. In this context, Electronic Health Records\n(EHRs) have emerged as a valuable tool for identifying and enrolling eligible\nparticipants. In this study, we propose an automated approach that leverages\nChatGPT, a large language model, to extract patient-related information from\nunstructured clinical notes and generate search queries for retrieving\npotentially eligible clinical trials. Our empirical evaluation, conducted on\ntwo benchmark retrieval collections, shows improved retrieval performance\ncompared to existing approaches when several general-purposed and task-specific\nprompts are used. Notably, ChatGPT-generated queries also outperform\nhuman-generated queries in terms of retrieval performance. These findings\nhighlight the potential use of ChatGPT to enhance clinical trial enrollment\nwhile ensuring the quality of medical service and minimizing direct risks to\npatients.", "published": "2023-06-03 10:54:23", "link": "http://arxiv.org/abs/2306.02077v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Incorporating Deep Syntactic and Semantic Knowledge for Chinese Sequence\n  Labeling with GCN", "abstract": "Recently, it is quite common to integrate Chinese sequence labeling results\nto enhance syntactic and semantic parsing. However, little attention has been\npaid to the utility of hierarchy and structure information encoded in syntactic\nand semantic features for Chinese sequence labeling tasks. In this paper, we\npropose a novel framework to encode syntactic structure features and semantic\ninformation for Chinese sequence labeling tasks with graph convolutional\nnetworks (GCN). Experiments on five benchmark datasets, including Chinese word\nsegmentation and part-of-speech tagging, demonstrate that our model can\neffectively improve the performance of Chinese labeling tasks.", "published": "2023-06-03 10:56:44", "link": "http://arxiv.org/abs/2306.02078v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Human Activity Recognition through Two-stage Prompting with\n  ChatGPT", "abstract": "Wearable sensor devices, which offer the advantage of recording daily objects\nused by a person while performing an activity, enable the feasibility of\nunsupervised Human Activity Recognition (HAR). Unfortunately, previous\nunsupervised approaches using the usage sequence of objects usually require a\nproper description of activities manually prepared by humans. Instead, we\nleverage the knowledge embedded in a Large Language Model (LLM) of ChatGPT.\nBecause the sequence of objects robustly characterizes the activity identity,\nit is possible that ChatGPT already learned the association between activities\nand objects from existing contexts. However, previous prompt engineering for\nChatGPT exhibits limited generalization ability when dealing with a list of\nwords (i.e., sequence of objects) due to the similar weighting assigned to each\nword in the list. In this study, we propose a two-stage prompt engineering,\nwhich first guides ChatGPT to generate activity descriptions associated with\nobjects while emphasizing important objects for distinguishing similar\nactivities; then outputs activity classes and explanations for enhancing the\ncontexts that are helpful for HAR. To the best of our knowledge, this is the\nfirst study that utilizes ChatGPT to recognize activities using objects in an\nunsupervised manner. We conducted our approach on three datasets and\ndemonstrated the state-of-the-art performance.", "published": "2023-06-03 15:41:59", "link": "http://arxiv.org/abs/2306.02140v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "LDEB -- Label Digitization with Emotion Binarization and Machine\n  Learning for Emotion Recognition in Conversational Dialogues", "abstract": "Emotion recognition in conversations (ERC) is vital to the advancements of\nconversational AI and its applications. Therefore, the development of an\nautomated ERC model using the concepts of machine learning (ML) would be\nbeneficial. However, the conversational dialogues present a unique problem\nwhere each dialogue depicts nested emotions that entangle the association\nbetween the emotional feature descriptors and emotion type (or label). This\nentanglement that can be multiplied with the presence of data paucity is an\nobstacle for a ML model. To overcome this problem, we proposed a novel approach\ncalled Label Digitization with Emotion Binarization (LDEB) that disentangles\nthe twists by utilizing the text normalization and 7-bit digital encoding\ntechniques and constructs a meaningful feature space for a ML model to be\ntrained. We also utilized the publicly available dataset called the\nFETA-DailyDialog dataset for feature learning and developed a hierarchical ERC\nmodel using random forest (RF) and artificial neural network (ANN) classifiers.\nSimulations showed that the ANN-based ERC model was able to predict emotion\nwith the best accuracy and precision scores of about 74% and 76%, respectively.\nSimulations also showed that the ANN-model could reach a training accuracy\nscore of about 98% with 60 epochs. On the other hand, the RF-based ERC model\nwas able to predict emotions with the best accuracy and precision scores of\nabout 78% and 75%, respectively.", "published": "2023-06-03 20:37:46", "link": "http://arxiv.org/abs/2306.02193v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Span Identification of Epistemic Stance-Taking in Academic Written\n  English", "abstract": "Responding to the increasing need for automated writing evaluation (AWE)\nsystems to assess language use beyond lexis and grammar (Burstein et al.,\n2016), we introduce a new approach to identify rhetorical features of stance in\nacademic English writing. Drawing on the discourse-analytic framework of\nengagement in the Appraisal analysis (Martin & White, 2005), we manually\nannotated 4,688 sentences (126,411 tokens) for eight rhetorical stance\ncategories (e.g., PROCLAIM, ATTRIBUTION) and additional discourse elements. We\nthen report an experiment to train machine learning models to identify and\ncategorize the spans of these stance expressions. The best-performing model\n(RoBERTa + LSTM) achieved macro-averaged F1 of .7208 in the span identification\nof stance-taking expressions, slightly outperforming the intercoder reliability\nestimates before adjudication (F1 = .6629).", "published": "2023-06-03 07:32:25", "link": "http://arxiv.org/abs/2306.02038v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MultiLegalPile: A 689GB Multilingual Legal Corpus", "abstract": "Large, high-quality datasets are crucial for training Large Language Models\n(LLMs). However, so far, there are few datasets available for specialized\ncritical domains such as law and the available ones are often only for the\nEnglish language. We curate and release MultiLegalPile, a 689GB corpus in 24\nlanguages from 17 jurisdictions. The MultiLegalPile corpus, which includes\ndiverse legal data sources with varying licenses, allows for pretraining NLP\nmodels under fair use, with more permissive licenses for the Eurlex Resources\nand Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer\nmultilingually, and 24 monolingual models on each of the language-specific\nsubsets and evaluate them on LEXTREME. Additionally, we evaluate the English\nand multilingual models on LexGLUE. Our multilingual models set a new SotA on\nLEXTREME and our English models on LexGLUE. We release the dataset, the trained\nmodels, and all of the code under the most open possible licenses.", "published": "2023-06-03 10:10:38", "link": "http://arxiv.org/abs/2306.02069v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "primary_category": "cs.CL"}
{"title": "Benchmarking Robustness of Adaptation Methods on Pre-trained\n  Vision-Language Models", "abstract": "Various adaptation methods, such as LoRA, prompts, and adapters, have been\nproposed to enhance the performance of pre-trained vision-language models in\nspecific domains. The robustness of these adaptation methods against\ndistribution shifts have not been studied. In this study, we assess the\nrobustness of 11 widely-used adaptation methods across 4 vision-language\ndatasets under multimodal corruptions. Concretely, we introduce 7 benchmark\ndatasets, including 96 visual and 87 textual corruptions, to investigate the\nrobustness of different adaptation methods, the impact of available adaptation\nexamples, and the influence of trainable parameter size during adaptation. Our\nanalysis reveals that: 1) Adaptation methods are more sensitive to text\ncorruptions than visual corruptions. 2) Full fine-tuning does not consistently\nprovide the highest robustness; instead, adapters can achieve better robustness\nwith comparable clean performance. 3) Contrary to expectations, our findings\nindicate that increasing the number of adaptation data and parameters does not\nguarantee enhanced robustness; instead it results in even lower robustness. We\nhope this study could benefit future research in the development of robust\nmultimodal adaptation methods. The benchmark, code, and dataset used in this\nstudy can be accessed at https://adarobustness.github.io .", "published": "2023-06-03 11:05:04", "link": "http://arxiv.org/abs/2306.02080v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Advancing African-Accented Speech Recognition: Epistemic\n  Uncertainty-Driven Data Selection for Generalizable ASR Models", "abstract": "Accents play a pivotal role in shaping human communication, enhancing our\nability to convey and comprehend messages with clarity and cultural nuance.\nWhile there has been significant progress in Automatic Speech Recognition\n(ASR), African-accented English ASR has been understudied due to a lack of\ntraining datasets, which are often expensive to create and demand colossal\nhuman labor. Combining several active learning paradigms and the core-set\napproach, we propose a new multi-rounds adaptation process that uses epistemic\nuncertainty to automate the annotation process, significantly reducing the\nassociated costs and human labor. This novel method streamlines data annotation\nand strategically selects data samples contributing most to model uncertainty,\nenhancing training efficiency. We define a new U-WER metric to track model\nadaptation to hard accents. We evaluate our approach across several domains,\ndatasets, and high-performing speech models. Our results show that our approach\nleads to a 27\\% WER relative average improvement while requiring on average\n45\\% less data than established baselines. Our approach also improves\nout-of-distribution generalization for very low-resource accents, demonstrating\nits viability for building generalizable ASR models in the context of accented\nAfrican ASR. We open-source the code here:\nhttps://github.com/bonaventuredossou/active_learning_african_asr.", "published": "2023-06-03 13:11:37", "link": "http://arxiv.org/abs/2306.02105v6", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Table and Image Generation for Investigating Knowledge of Entities in\n  Pre-trained Vision and Language Models", "abstract": "In this paper, we propose a table and image generation task to verify how the\nknowledge about entities acquired from natural language is retained in Vision &\nLanguage (V&L) models. This task consists of two parts: the first is to\ngenerate a table containing knowledge about an entity and its related image,\nand the second is to generate an image from an entity with a caption and a\ntable containing related knowledge of the entity. In both tasks, the model must\nknow the entities used to perform the generation properly. We created the\nWikipedia Table and Image Generation (WikiTIG) dataset from about 200,000\ninfoboxes in English Wikipedia articles to perform the proposed tasks. We\nevaluated the performance on the tasks with respect to the above research\nquestion using the V&L model OFA, which has achieved state-of-the-art results\nin multiple tasks. Experimental results show that OFA forgets part of its\nentity knowledge by pre-training as a complement to improve the performance of\nimage related tasks.", "published": "2023-06-03 14:01:54", "link": "http://arxiv.org/abs/2306.02115v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Acoustic Word Embeddings for Untranscribed Target Languages with\n  Continued Pretraining and Learned Pooling", "abstract": "Acoustic word embeddings are typically created by training a pooling function\nusing pairs of word-like units. For unsupervised systems, these are mined using\nk-nearest neighbor (KNN) search, which is slow. Recently, mean-pooled\nrepresentations from a pre-trained self-supervised English model were suggested\nas a promising alternative, but their performance on target languages was not\nfully competitive. Here, we explore improvements to both approaches: we use\ncontinued pre-training to adapt the self-supervised model to the target\nlanguage, and we use a multilingual phone recognizer (MPR) to mine phone n-gram\npairs for training the pooling function. Evaluating on four languages, we show\nthat both methods outperform a recent approach on word discrimination.\nMoreover, the MPR method is orders of magnitude faster than KNN, and is highly\ndata efficient. We also show a small improvement from performing learned\npooling on top of the continued pre-trained representations.", "published": "2023-06-03 16:44:21", "link": "http://arxiv.org/abs/2306.02153v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with\n  Prompts", "abstract": "Large language models (LLMs) have gained considerable attention for\nArtificial Intelligence Generated Content (AIGC), particularly with the\nemergence of ChatGPT. However, the direct adaptation of continuous speech to\nLLMs that process discrete tokens remains an unsolved challenge, hindering the\napplication of LLMs for speech generation. The advanced speech LMs are in the\ncorner, as that speech signals encapsulate a wealth of information, including\nspeaker and emotion, beyond textual data alone. Prompt tuning has demonstrated\nnotable gains in parameter efficiency and competitive performance on some\nspeech classification tasks. However, the extent to which prompts can\neffectively elicit generation tasks from speech LMs remains an open question.\nIn this paper, we present pioneering research that explores the application of\nprompt tuning to stimulate speech LMs for various generation tasks, within a\nunified framework called SpeechGen, with around 10M trainable parameters. The\nproposed unified framework holds great promise for efficiency and\neffectiveness, particularly with the imminent arrival of advanced speech LMs,\nwhich will significantly enhance the capabilities of the framework. The code\nand demos of SpeechGen will be available on the project website:\n\\url{https://ga642381.github.io/SpeechPrompt/speechgen}", "published": "2023-06-03 22:35:27", "link": "http://arxiv.org/abs/2306.02207v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Word-Level Explanations for Analyzing Bias in Text-to-Image Models", "abstract": "Text-to-image models take a sentence (i.e., prompt) and generate images\nassociated with this input prompt. These models have created award wining-art,\nvideos, and even synthetic datasets. However, text-to-image (T2I) models can\ngenerate images that underrepresent minorities based on race and sex. This\npaper investigates which word in the input prompt is responsible for bias in\ngenerated images. We introduce a method for computing scores for each word in\nthe prompt; these scores represent its influence on biases in the model's\noutput. Our method follows the principle of \\emph{explaining by removing},\nleveraging masked language models to calculate the influence scores. We perform\nexperiments on Stable Diffusion to demonstrate that our method identifies the\nreplication of societal stereotypes in generated images.", "published": "2023-06-03 21:39:07", "link": "http://arxiv.org/abs/2306.05500v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-shot Class-incremental Audio Classification Using Stochastic\n  Classifier", "abstract": "It is generally assumed that number of classes is fixed in current audio\nclassification methods, and the model can recognize pregiven classes only. When\nnew classes emerge, the model needs to be retrained with adequate samples of\nall classes. If new classes continually emerge, these methods will not work\nwell and even infeasible. In this study, we propose a method for fewshot\nclass-incremental audio classification, which continually recognizes new\nclasses and remember old ones. The proposed model consists of an embedding\nextractor and a stochastic classifier. The former is trained in base session\nand frozen in incremental sessions, while the latter is incrementally expanded\nin all sessions. Two datasets (NS-100 and LS-100) are built by choosing samples\nfrom audio corpora of NSynth and LibriSpeech, respectively. Results show that\nour method exceeds four baseline ones in average accuracy and performance\ndropping rate. Code is at https://github.com/vinceasvp/meta-sc.", "published": "2023-06-03 08:59:01", "link": "http://arxiv.org/abs/2306.02053v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Low-Complexity Acoustic Scene Classification Using Data Augmentation and\n  Lightweight ResNet", "abstract": "We present a work on low-complexity acoustic scene classification (ASC) with\nmultiple devices, namely the subtask A of Task 1 of the DCASE2021 challenge.\nThis subtask focuses on classifying audio samples of multiple devices with a\nlow-complexity model, where two main difficulties need to be overcome. First,\nthe audio samples are recorded by different devices, and there is mismatch of\nrecording devices in audio samples. We reduce the negative impact of the\nmismatch of recording devices by using some effective strategies, including\ndata augmentation (e.g., mix-up, spectrum correction, pitch shift), usages of\nmulti-patch network structure and channel attention. Second, the model size\nshould be smaller than a threshold (e.g., 128 KB required by the DCASE2021\nchallenge). To meet this condition, we adopt a ResNet with both depthwise\nseparable convolution and channel attention as the backbone network, and\nperform model compression. In summary, we propose a low-complexity ASC method\nusing data augmentation and a lightweight ResNet. Evaluated on the official\ndevelopment and evaluation datasets, our method obtains classification accuracy\nscores of 71.6% and 66.7%, respectively; and obtains Log-loss scores of 1.038\nand 1.136, respectively. Our final model size is 110.3 KB which is smaller than\nthe maximum of 128 KB.", "published": "2023-06-03 09:05:29", "link": "http://arxiv.org/abs/2306.02054v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BEDRF: Bidirectional Edge Diffraction Response Function for Interactive\n  Sound Propagation", "abstract": "We introduce bidirectional edge diffraction response function (BEDRF), a new\napproach to model wave diffraction around edges with path tracing. The\ndiffraction part of the wave is expressed as an integration on path space, and\nthe wave-edge interaction is expressed using only the localized information\naround points on the edge similar to a bidirectional scattering distribution\nfunction (BSDF) for visual rendering. For an infinite single wedge, our model\ngenerates the same result as the analytic solution. Our approach can be easily\nintegrated into interactive geometric sound propagation algorithms that use\npath tracing to compute specular and diffuse reflections. Our resulting\npropagation algorithm can approximate complex wave propagation phenomena\ninvolving high-order diffraction, and is able to handle dynamic, deformable\nobjects and moving sources and listeners. We highlight the performance of our\napproach in different scenarios to generate smooth auralization.", "published": "2023-06-03 01:06:54", "link": "http://arxiv.org/abs/2306.01974v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Why We Should Report the Details in Subjective Evaluation of TTS More\n  Rigorously", "abstract": "This paper emphasizes the importance of reporting experiment details in\nsubjective evaluations and demonstrates how such details can significantly\nimpact evaluation results in the field of speech synthesis. Through an analysis\nof 80 papers presented at INTERSPEECH 2022, we find a lack of thorough\nreporting on critical details such as evaluator recruitment and filtering,\ninstructions and payments, and the geographic and linguistic backgrounds of\nevaluators. To illustrate the effect of these details on evaluation outcomes,\nwe conducted mean opinion score (MOS) tests on three well-known TTS systems\nunder different evaluation settings and we obtain at least three distinct\nrankings of TTS models. We urge the community to report experiment details in\nsubjective evaluations to improve the reliability and interpretability of\nexperimental results.", "published": "2023-06-03 07:52:11", "link": "http://arxiv.org/abs/2306.02044v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SGEM: Test-Time Adaptation for Automatic Speech Recognition via\n  Sequential-Level Generalized Entropy Minimization", "abstract": "Automatic speech recognition (ASR) models are frequently exposed to data\ndistribution shifts in many real-world scenarios, leading to erroneous\npredictions. To tackle this issue, an existing test-time adaptation (TTA)\nmethod has recently been proposed to adapt the pre-trained ASR model on\nunlabeled test instances without source data. Despite decent performance gain,\nthis work relies solely on naive greedy decoding and performs adaptation across\ntimesteps at a frame level, which may not be optimal given the sequential\nnature of the model output. Motivated by this, we propose a novel TTA\nframework, dubbed SGEM, for general ASR models. To treat the sequential output,\nSGEM first exploits beam search to explore candidate output logits and selects\nthe most plausible one. Then, it utilizes generalized entropy minimization and\nnegative sampling as unsupervised objectives to adapt the model. SGEM achieves\nstate-of-the-art performance for three mainstream ASR models under various\ndomain shifts.", "published": "2023-06-03 02:27:08", "link": "http://arxiv.org/abs/2306.01981v4", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
