{"title": "Not All Attention Is All You Need", "abstract": "Beyond the success story of pre-trained language models (PrLMs) in recent\nnatural language processing, they are susceptible to over-fitting due to\nunusual large model size. To this end, dropout serves as a therapy. However,\nexisting methods like random-based, knowledge-based and search-based dropout\nare more general but less effective onto self-attention based models, which are\nbroadly chosen as the fundamental architecture of PrLMs. In this paper, we\npropose a novel dropout method named AttendOut to let self-attention empowered\nPrLMs capable of more robust task-specific tuning. We demonstrate that\nstate-of-the-art models with elaborate training design may achieve much\nstronger results. We verify the universality of our approach on extensive\nnatural language processing tasks.", "published": "2021-04-10 06:24:52", "link": "http://arxiv.org/abs/2104.04692v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fool Me Twice: Entailment from Wikipedia Gamification", "abstract": "We release FoolMeTwice (FM2 for short), a large dataset of challenging\nentailment pairs collected through a fun multi-player game. Gamification\nencourages adversarial examples, drastically lowering the number of examples\nthat can be solved using \"shortcuts\" compared to other popular entailment\ndatasets. Players are presented with two tasks. The first task asks the player\nto write a plausible claim based on the evidence from a Wikipedia page. The\nsecond one shows two plausible claims written by other players, one of which is\nfalse, and the goal is to identify it before the time runs out. Players \"pay\"\nto see clues retrieved from the evidence pool: the more evidence the player\nneeds, the harder the claim. Game-play between motivated players leads to\ndiverse strategies for crafting claims, such as temporal inference and\ndiverting to unrelated evidence, and results in higher quality data for the\nentailment and evidence retrieval tasks. We open source the dataset and the\ngame code.", "published": "2021-04-10 09:58:40", "link": "http://arxiv.org/abs/2104.04725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLI Data Sanity Check: Assessing the Effect of Data Corruption on Model\n  Performance", "abstract": "Pre-trained neural language models give high performance on natural language\ninference (NLI) tasks. But whether they actually understand the meaning of the\nprocessed sequences remains unclear. We propose a new diagnostics test suite\nwhich allows to assess whether a dataset constitutes a good testbed for\nevaluating the models' meaning understanding capabilities. We specifically\napply controlled corruption transformations to widely used benchmarks (MNLI and\nANLI), which involve removing entire word classes and often lead to\nnon-sensical sentence pairs. If model accuracy on the corrupted data remains\nhigh, then the dataset is likely to contain statistical biases and artefacts\nthat guide prediction. Inversely, a large decrease in model accuracy indicates\nthat the original dataset provides a proper challenge to the models' reasoning\ncapabilities. Hence, our proposed controls can serve as a crash test for\ndeveloping high quality data for NLI tasks.", "published": "2021-04-10 12:28:07", "link": "http://arxiv.org/abs/2104.04751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UTNLP at SemEval-2021 Task 5: A Comparative Analysis of Toxic Span\n  Detection using Attention-based, Named Entity Recognition, and Ensemble\n  Models", "abstract": "Detecting which parts of a sentence contribute to that sentence's toxicity --\nrather than providing a sentence-level verdict of hatefulness -- would increase\nthe interpretability of models and allow human moderators to better understand\nthe outputs of the system. This paper presents our team's, UTNLP, methodology\nand results in the SemEval-2021 shared task 5 on toxic spans detection. We test\nmultiple models and contextual embeddings and report the best setting out of\nall. The experiments start with keyword-based models and are followed by\nattention-based, named entity-based, transformers-based, and ensemble models.\nOur best approach, an ensemble model, achieves an F1 of 0.684 in the\ncompetition's evaluation phase.", "published": "2021-04-10 13:56:03", "link": "http://arxiv.org/abs/2104.04770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation Learning for Weakly Supervised Relation Extraction", "abstract": "Recent years have seen rapid development in Information Extraction, as well\nas its subtask, Relation Extraction. Relation Extraction is able to detect\nsemantic relations between entities in sentences. Currently, many efficient\napproaches have been applied to relation extraction tasks. Supervised learning\napproaches especially have good performance. However, there are still many\ndifficult challenges. One of the most serious problems is that manually labeled\ndata is difficult to acquire. In most cases, limited data for supervised\napproaches equals lousy performance. Thus here, under the situation with only\nlimited training data, we focus on how to improve the performance of our\nsupervised baseline system with unsupervised pre-training. Feature is one of\nthe key components in improving the supervised approaches. Traditional\napproaches usually apply hand-crafted features, which require expert knowledge\nand expensive human labor. However, this type of feature might suffer from data\nsparsity: when the training set size is small, the model parameters might be\npoorly estimated. In this thesis, we present several novel unsupervised\npre-training models to learn the distributed text representation features,\nwhich are encoded with rich syntactic-semantic patterns of relation\nexpressions. The experiments have demonstrated that this type of feature,\ncombine with the traditional hand-crafted features, could improve the\nperformance of the logistic classification model for relation extraction,\nespecially on the classification of relations with only minor training\ninstances.", "published": "2021-04-10 12:22:25", "link": "http://arxiv.org/abs/2105.00815v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on\n  Dataset and Prompt Collections", "abstract": "Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.", "published": "2021-04-10 02:57:22", "link": "http://arxiv.org/abs/2104.04670v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser", "abstract": "Given a database schema, Text-to-SQL aims to translate a natural language\nquestion into the corresponding SQL query. Under the setup of cross-domain,\ntraditional semantic parsing models struggle to adapt to unseen database\nschemas. To improve the model generalization capability for rare and unseen\nschemas, we propose a new architecture, ShadowGNN, which processes schemas at\nabstract and semantic levels. By ignoring names of semantic items in databases,\nabstract schemas are exploited in a well-designed graph projection neural\nnetwork to obtain delexicalized representation of question and schema. Based on\nthe domain-independent representations, a relation-aware transformer is\nutilized to further extract logical linking between question and schema.\nFinally, a SQL decoder with context-free grammar is applied. On the challenging\nText-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms\nstate-of-the-art models. When the annotated data is extremely limited (only\n10\\% training set), ShadowGNN gets over absolute 5\\% performance gain, which\nshows its powerful generalization ability. Our implementation will be\nopen-sourced at \\url{https://github.com/WowCZ/shadowgnn}.", "published": "2021-04-10 05:48:28", "link": "http://arxiv.org/abs/2104.04689v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing", "abstract": "Meta-learning, or learning to learn, is a technique that can help to overcome\nresource scarcity in cross-lingual NLP problems, by enabling fast adaptation to\nnew tasks. We apply model-agnostic meta-learning (MAML) to the task of\ncross-lingual dependency parsing. We train our model on a diverse set of\nlanguages to learn a parameter initialization that can adapt quickly to new\nlanguages. We find that meta-learning with pre-training can significantly\nimprove upon the performance of language transfer and standard supervised\nlearning baselines for a variety of unseen, typologically diverse, and\nlow-resource languages, in a few-shot learning setup.", "published": "2021-04-10 11:10:16", "link": "http://arxiv.org/abs/2104.04736v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FreSaDa: A French Satire Data Set for Cross-Domain Satire Detection", "abstract": "In this paper, we introduce FreSaDa, a French Satire Data Set, which is\ncomposed of 11,570 articles from the news domain. In order to avoid reporting\nunreasonably high accuracy rates due to the learning of characteristics\nspecific to publication sources, we divided our samples into training,\nvalidation and test, such that the training publication sources are distinct\nfrom the validation and test publication sources. This gives rise to a\ncross-domain (cross-source) satire detection task. We employ two classification\nmethods as baselines for our new data set, one based on low-level features\n(character n-grams) and one based on high-level features (average of CamemBERT\nword embeddings). As an additional contribution, we present an unsupervised\ndomain adaptation method based on regarding the pairwise similarities (given by\nthe dot product) between the training samples and the validation samples as\nfeatures. By including these domain-specific features, we attain significant\nimprovements for both character n-grams and CamemBERT embeddings.", "published": "2021-04-10 18:21:53", "link": "http://arxiv.org/abs/2104.04828v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FRAKE: Fusional Real-time Automatic Keyword Extraction", "abstract": "Keyword extraction is the process of identifying the words or phrases that\nexpress the main concepts of text to the best of one's ability. Electronic\ninfrastructure creates a considerable amount of text every day and at all\ntimes. This massive volume of documents makes it practically impossible for\nhuman resources to study and manage them. Nevertheless, the need for these\ndocuments to be accessed efficiently and effectively is evident in numerous\npurposes. A blog, news article, or technical note is considered a relatively\nlong text since the reader aims to learn the subject based on keywords or\ntopics. Our approach consists of a combination of two models: graph centrality\nfeatures and textural features. The proposed method has been used to extract\nthe best keyword among the candidate keywords with an optimal combination of\ngraph centralities, such as degree, betweenness, eigenvector, closeness\ncentrality and etc, and textural, such as Casing, Term position, Term frequency\nnormalization, Term different sentence, Part Of Speech tagging. There have also\nbeen attempts to distinguish keywords from candidate phrases and consider them\non separate keywords. For evaluating the proposed method, seven datasets were\nused: Semeval2010, SemEval2017, Inspec, fao30, Thesis100, pak2018, and\nWikinews, with results reported as Precision, Recall, and F- measure. Our\nproposed method performed much better in terms of evaluation metrics in all\nreviewed datasets compared with available methods in literature. An approximate\n16.9% increase was witnessed in F-score metric and this was much more for the\nInspec in English datasets and WikiNews in forgone languages.", "published": "2021-04-10 18:30:17", "link": "http://arxiv.org/abs/2104.04830v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On migration to Perpetual Enterprise System", "abstract": "This document describes a pragmatic approach on how to migrate an\norganisation computer system towards a new system that could evolve forever,\naddresses the whole organisation and it is integrated.\n  Governance aspects are as important, if not more, than purely technical IT\naspects: human resources, call for tenders, and similar. Migration implies that\none is not starting from a green field.", "published": "2021-04-10 19:20:55", "link": "http://arxiv.org/abs/2104.04844v4", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Identifying and Categorizing Offensive Language in Social Media", "abstract": "Offensive language is pervasive in social media. Individuals frequently take\nadvantage of the perceived anonymity of computer-mediated communication, using\nthis to engage in behavior that many of them would not consider in real life.\nThe automatic identification of offensive content online is an important task\nthat has gained more attention in recent years. This task can be modeled as a\nsupervised classification problem in which systems are trained using a dataset\ncontaining posts that are annotated with respect to the presence of some\nform(s) of abusive or offensive content. The objective of this study is to\nprovide a description of a classification system built for SemEval-2019 Task 6:\nOffensEval. This system classifies a tweet as either offensive or not offensive\n(Sub-task A) and further classifies offensive tweets into categories (Sub-tasks\nB \\& C). We trained machine learning and deep learning models along with data\npreprocessing and sampling techniques to come up with the best results. Models\ndiscussed include Naive Bayes, SVM, Logistic Regression, Random Forest and\nLSTM.", "published": "2021-04-10 22:53:43", "link": "http://arxiv.org/abs/2104.04871v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal\n  Procrustes Analysis", "abstract": "Knowledge Graph Embeddings (KGEs) have been intensively explored in recent\nyears due to their promise for a wide range of applications. However, existing\nstudies focus on improving the final model performance without acknowledging\nthe computational cost of the proposed approaches, in terms of execution time\nand environmental impact. This paper proposes a simple yet effective KGE\nframework which can reduce the training time and carbon footprint by orders of\nmagnitudes compared with state-of-the-art approaches, while producing\ncompetitive performance. We highlight three technical innovations: full batch\nlearning via relational matrices, closed-form Orthogonal Procrustes Analysis\nfor KGEs, and non-negative-sampling training. In addition, as the first KGE\nmethod whose entity embeddings also store full relation information, our\ntrained models encode rich semantics and are highly interpretable.\nComprehensive experiments and ablation studies involving 13 strong baselines\nand two standard datasets verify the effectiveness and efficiency of our\nalgorithm.", "published": "2021-04-10 03:55:45", "link": "http://arxiv.org/abs/2104.04676v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute\n  Representation Learning", "abstract": "While relation extraction is an essential task in knowledge acquisition and\nrepresentation, and new-generated relations are common in the real world, less\neffort is made to predict unseen relations that cannot be observed at the\ntraining stage. In this paper, we formulate the zero-shot relation extraction\nproblem by incorporating the text description of seen and unseen relations. We\npropose a novel multi-task learning model, zero-shot BERT (ZS-BERT), to\ndirectly predict unseen relations without hand-crafted attribute labeling and\nmultiple pairwise classifications. Given training instances consisting of input\nsentences and the descriptions of their relations, ZS-BERT learns two functions\nthat project sentences and relation descriptions into an embedding space by\njointly minimizing the distances between them and classifying seen relations.\nBy generating the embeddings of unseen relations and new-coming sentences based\non such two functions, we use nearest neighbor search to obtain the prediction\nof unseen relations. Experiments conducted on two well-known datasets exhibit\nthat ZS-BERT can outperform existing methods by at least 13.54\\% improvement on\nF1 score.", "published": "2021-04-10 06:53:41", "link": "http://arxiv.org/abs/2104.04697v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with\n  Pre-trained Language Models for Toxic Spans Detection", "abstract": "This paper describes our system for SemEval-2021 Task 5 on Toxic Spans\nDetection. We developed ensemble models using BERT-based neural architectures\nand post-processing to combine tokens into spans. We evaluated several\npre-trained language models using various ensemble techniques for toxic span\nidentification and achieved sizable improvements over our baseline fine-tuned\nBERT models. Finally, our system obtained a F1-score of 67.55% on test data.", "published": "2021-04-10 11:27:32", "link": "http://arxiv.org/abs/2104.04739v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7; I.7.m; H.3.3"], "primary_category": "cs.CL"}
{"title": "Imperfect also Deserves Reward: Multi-Level and Sequential Reward\n  Modeling for Better Dialog Management", "abstract": "For task-oriented dialog systems, training a Reinforcement Learning (RL)\nbased Dialog Management module suffers from low sample efficiency and slow\nconvergence speed due to the sparse rewards in RL.To solve this problem, many\nstrategies have been proposed to give proper rewards when training RL, but\ntheir rewards lack interpretability and cannot accurately estimate the\ndistribution of state-action pairs in real dialogs. In this paper, we propose a\nmulti-level reward modeling approach that factorizes a reward into a\nthree-level hierarchy: domain, act, and slot. Based on inverse adversarial\nreinforcement learning, our designed reward model can provide more accurate and\nexplainable reward signals for state-action pairs.Extensive evaluations show\nthat our approach can be applied to a wide range of reinforcement\nlearning-based dialog systems and significantly improves both the performance\nand the speed of convergence.", "published": "2021-04-10 12:20:23", "link": "http://arxiv.org/abs/2104.04748v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive Transformer-based End-to-end ASR using BERT", "abstract": "Transformer-based models have led to significant innovation in classical and\npractical subjects as varied as speech processing, natural language processing,\nand computer vision. On top of the Transformer, attention-based end-to-end\nautomatic speech recognition (ASR) models have recently become popular.\nSpecifically, non-autoregressive modeling, which boasts fast inference and\nperformance comparable to conventional autoregressive methods, is an emerging\nresearch topic. In the context of natural language processing, the\nbidirectional encoder representations from Transformers (BERT) model has\nreceived widespread attention, partially due to its ability to infer\ncontextualized word representations and to enable superior performance for\ndownstream tasks while needing only simple fine-tuning. Motivated by the\nsuccess, we intend to view speech recognition as a downstream task of BERT,\nthus an ASR system is expected to be deduced by performing fine-tuning.\nConsequently, to not only inherit the advantages of non-autoregressive ASR\nmodels but also enjoy the benefits of a pre-trained language model (e.g.,\nBERT), we propose a non-autoregressive Transformer-based end-to-end ASR model\nbased on BERT. We conduct a series of experiments on the AISHELL-1 dataset that\ndemonstrate competitive or superior results for the model when compared to\nstate-of-the-art ASR systems.", "published": "2021-04-10 16:22:17", "link": "http://arxiv.org/abs/2104.04805v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sentiment-based Candidate Selection for NMT", "abstract": "The explosion of user-generated content (UGC)--e.g. social media posts,\ncomments, and reviews--has motivated the development of NLP applications\ntailored to these types of informal texts. Prevalent among these applications\nhave been sentiment analysis and machine translation (MT). Grounded in the\nobservation that UGC features highly idiomatic, sentiment-charged language, we\npropose a decoder-side approach that incorporates automatic sentiment scoring\ninto the MT candidate selection process. We train separate English and Spanish\nsentiment classifiers, then, using n-best candidates generated by a baseline MT\nmodel with beam search, select the candidate that minimizes the absolute\ndifference between the sentiment score of the source sentence and that of the\ntranslation, and perform a human evaluation to assess the produced\ntranslations. Unlike previous work, we select this minimally divergent\ntranslation by considering the sentiment scores of the source sentence and\ntranslation on a continuous interval, rather than using e.g. binary\nclassification, allowing for more fine-grained selection of translation\ncandidates. The results of human evaluations show that, in comparison to the\nopen-source MT baseline model on top of which our sentiment-based pipeline is\nbuilt, our pipeline produces more accurate translations of colloquial,\nsentiment-heavy source texts.", "published": "2021-04-10 19:01:52", "link": "http://arxiv.org/abs/2104.04840v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Boundary and Context Aware Training for CIF-based Non-Autoregressive\n  End-to-end ASR", "abstract": "Continuous integrate-and-fire (CIF) based models, which use a soft and\nmonotonic alignment mechanism, have been well applied in non-autoregressive\n(NAR) speech recognition with competitive performance compared with other NAR\nmethods. However, such an alignment learning strategy may suffer from an\nerroneous acoustic boundary estimation, severely hindering the convergence\nspeed as well as the system performance. In this paper, we propose a boundary\nand context aware training approach for CIF based NAR models. Firstly, the\nconnectionist temporal classification (CTC) spike information is utilized to\nguide the learning of acoustic boundaries in the CIF. Besides, an additional\ncontextual decoder is introduced behind the CIF decoder, aiming to capture the\nlinguistic dependencies within a sentence. Finally, we adopt a recently\nproposed Conformer architecture to improve the capacity of acoustic modeling.\nExperiments on the open-source Mandarin AISHELL-1 corpus show that the proposed\nmethod achieves a comparable character error rates (CERs) of 4.9% with only\n1/24 latency compared with a state-of-the-art autoregressive (AR) Conformer\nmodel. Futhermore, when evaluating on an internal 7500 hours Mandarin corpus,\nour model still outperforms other NAR methods and even reaches the AR Conformer\nmodel on a challenging real-world noisy test set.", "published": "2021-04-10 07:42:27", "link": "http://arxiv.org/abs/2104.04702v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unified Source-Filter GAN: Unified Source-filter Network Based On\n  Factorization of Quasi-Periodic Parallel WaveGAN", "abstract": "We propose a unified approach to data-driven source-filter modeling using a\nsingle neural network for developing a neural vocoder capable of generating\nhigh-quality synthetic speech waveforms while retaining flexibility of the\nsource-filter model to control their voice characteristics. Our proposed\nnetwork called unified source-filter generative adversarial networks (uSFGAN)\nis developed by factorizing quasi-periodic parallel WaveGAN (QPPWG), one of the\nneural vocoders based on a single neural network, into a source excitation\ngeneration network and a vocal tract resonance filtering network by\nadditionally implementing a regularization loss. Moreover, inspired by neural\nsource filter (NSF), only a sinusoidal waveform is additionally used as the\nsimplest clue to generate a periodic source excitation waveform while\nminimizing the effect of approximations in the source filter model. The\nexperimental results demonstrate that uSFGAN outperforms conventional neural\nvocoders, such as QPPWG and NSF in both speech quality and pitch\ncontrollability.", "published": "2021-04-10 02:38:26", "link": "http://arxiv.org/abs/2104.04668v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
