{"title": "True or False: Does the Deep Learning Model Learn to Detect Rumors?", "abstract": "It is difficult for humans to distinguish the true and false of rumors, but\ncurrent deep learning models can surpass humans and achieve excellent accuracy\non many rumor datasets. In this paper, we investigate whether deep learning\nmodels that seem to perform well actually learn to detect rumors. We evaluate\nmodels on their generalization ability to out-of-domain examples by fine-tuning\nBERT-based models on five real-world datasets and evaluating against all test\nsets. The experimental results indicate that the generalization ability of the\nmodels on other unseen datasets are unsatisfactory, even common-sense rumors\ncannot be detected. Moreover, we found through experiments that models take\nshortcuts and learn absurd knowledge when the rumor datasets have serious data\npitfalls. This means that simple modifications to the rumor text based on\nspecific rules will lead to inconsistent model predictions. To more\nrealistically evaluate rumor detection models, we proposed a new evaluation\nmethod called paired test (PairT), which requires models to correctly predict a\npair of test samples at the same time. Furthermore, we make recommendations on\nhow to better create rumor dataset and evaluate rumor detection model at the\nend of this paper.", "published": "2021-12-01 02:59:21", "link": "http://arxiv.org/abs/2112.00245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wiki to Automotive: Understanding the Distribution Shift and its impact\n  on Named Entity Recognition", "abstract": "While transfer learning has become a ubiquitous technique used across Natural\nLanguage Processing (NLP) tasks, it is often unable to replicate the\nperformance of pre-trained models on text of niche domains like Automotive. In\nthis paper we aim to understand the main characteristics of the distribution\nshift with automotive domain text (describing technical functionalities such as\nCruise Control) and attempt to explain the potential reasons for the gap in\nperformance. We focus on performing the Named Entity Recognition (NER) task as\nit requires strong lexical, syntactic and semantic understanding by the model.\nOur experiments with 2 different encoders, namely BERT-Base-Uncased and\nSciBERT-Base-Scivocab-Uncased have lead to interesting findings that showed: 1)\nThe performance of SciBERT is better than BERT when used for automotive domain,\n2) Fine-tuning the language models with automotive domain text did not make\nsignificant improvements to the NER performance, 3) The distribution shift is\nchallenging as it is characterized by lack of repeating contexts, sparseness of\nentities, large number of Out-Of-Vocabulary (OOV) words and class overlap due\nto domain specific nuances.", "published": "2021-12-01 05:13:47", "link": "http://arxiv.org/abs/2112.00283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP Research and Resources at DaSciM, Ecole Polytechnique", "abstract": "DaSciM (Data Science and Mining) part of LIX at Ecole Polytechnique,\nestablished in 2013 and since then producing research results in the area of\nlarge scale data analysis via methods of machine and deep learning. The group\nhas been specifically active in the area of NLP and text mining with\ninteresting results at methodological and resources level. Here follow our\ndifferent contributions of interest to the AFIA community.", "published": "2021-12-01 15:34:39", "link": "http://arxiv.org/abs/2112.00566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CO-STAR: Conceptualisation of Stereotypes for Analysis and Reasoning", "abstract": "Warning: this paper contains material which may be offensive or upsetting.\n  While much of recent work has focused on the detection of hate speech and\novertly offensive content, very little research has explored the more subtle\nbut equally harmful language in the form of implied stereotypes. This is a\nchallenging domain, made even more so by the fact that humans often struggle to\nunderstand and reason about stereotypes. We build on existing literature and\npresent CO-STAR (COnceptualisation of STereotypes for Analysis and Reasoning),\na novel framework which encodes the underlying concepts of implied stereotypes.\nWe also introduce the CO-STAR training data set, which contains just over 12K\nstructured annotations of implied stereotypes and stereotype\nconceptualisations, and achieve state-of-the-art results after training and\nmanual evaluation. The CO-STAR models are, however, limited in their ability to\nunderstand more complex and subtly worded stereotypes, and our research\nmotivates future work in developing models with more sophisticated methods for\nencoding common-sense knowledge.", "published": "2021-12-01 20:39:04", "link": "http://arxiv.org/abs/2112.00819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-oriented Language Pre-training with Adaptive Hybrid Masking and\n  Optimal Transport Alignment", "abstract": "Motivated by the success of pre-trained language models such as BERT in a\nbroad range of natural language processing (NLP) tasks, recent research efforts\nhave been made for adapting these models for different application domains.\nAlong this line, existing domain-oriented models have primarily followed the\nvanilla BERT architecture and have a straightforward use of the domain corpus.\nHowever, domain-oriented tasks usually require accurate understanding of domain\nphrases, and such fine-grained phrase-level knowledge is hard to be captured by\nexisting pre-training scheme. Also, the word co-occurrences guided semantic\nlearning of pre-training models can be largely augmented by entity-level\nassociation knowledge. But meanwhile, by doing so there is a risk of\nintroducing noise due to the lack of groundtruth word-level alignment. To\naddress the above issues, we provide a generalized domain-oriented approach,\nwhich leverages auxiliary domain knowledge to improve the existing pre-training\nframework from two aspects. First, to preserve phrase knowledge effectively, we\nbuild a domain phrase pool as auxiliary training tool, meanwhile we introduce\nAdaptive Hybrid Masked Model to incorporate such knowledge. It integrates two\nlearning modes, word learning and phrase learning, and allows them to switch\nbetween each other. Second, we introduce Cross Entity Alignment to leverage\nentity association as weak supervision to augment the semantic learning of\npre-trained models. To alleviate the potential noise in this process, we\nintroduce an interpretable Optimal Transport based approach to guide alignment\nlearning. Experiments on four domain-oriented tasks demonstrate the superiority\nof our framework.", "published": "2021-12-01 15:47:01", "link": "http://arxiv.org/abs/2112.03024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive Model with Structural Loss for Language-based Abductive\n  Reasoning", "abstract": "The abductive natural language inference task ($\\alpha$NLI) is proposed to\ninfer the most plausible explanation between the cause and the event. In the\n$\\alpha$NLI task, two observations are given, and the most plausible hypothesis\nis asked to pick out from the candidates. Existing methods model the relation\nbetween each candidate hypothesis separately and penalize the inference network\nuniformly. In this paper, we argue that it is unnecessary to distinguish the\nreasoning abilities among correct hypotheses; and similarly, all wrong\nhypotheses contribute the same when explaining the reasons of the observations.\nTherefore, we propose to group instead of ranking the hypotheses and design a\nstructural loss called ``joint softmax focal loss'' in this paper. Based on the\nobservation that the hypotheses are generally semantically related, we have\ndesigned a novel interactive language model aiming at exploiting the rich\ninteraction among competing hypotheses. We name this new model for $\\alpha$NLI:\nInteractive Model with Structural Loss (IMSL). The experimental results show\nthat our IMSL has achieved the highest performance on the RoBERTa-large\npretrained model, with ACC and AUC results increased by about 1\\% and 5\\%\nrespectively.", "published": "2021-12-01 05:21:07", "link": "http://arxiv.org/abs/2112.00284v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NER-BERT: A Pre-trained Model for Low-Resource Entity Tagging", "abstract": "Named entity recognition (NER) models generally perform poorly when large\ntraining datasets are unavailable for low-resource domains. Recently,\npre-training a large-scale language model has become a promising direction for\ncoping with the data scarcity issue. However, the underlying discrepancies\nbetween the language modeling and NER task could limit the models' performance,\nand pre-training for the NER task has rarely been studied since the collected\nNER datasets are generally small or large but with low quality. In this paper,\nwe construct a massive NER corpus with a relatively high quality, and we\npre-train a NER-BERT model based on the created dataset. Experimental results\nshow that our pre-trained model can significantly outperform BERT as well as\nother strong baselines in low-resource scenarios across nine diverse domains.\nMoreover, a visualization of entity representations further indicates the\neffectiveness of NER-BERT for categorizing a variety of entities.", "published": "2021-12-01 10:45:02", "link": "http://arxiv.org/abs/2112.00405v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seeking Sinhala Sentiment: Predicting Facebook Reactions of Sinhala\n  Posts", "abstract": "The Facebook network allows its users to record their reactions to text via a\ntypology of emotions. This network, taken at scale, is therefore a prime data\nset of annotated sentiment data. This paper uses millions of such reactions,\nderived from a decade worth of Facebook post data centred around a Sri Lankan\ncontext, to model an eye of the beholder approach to sentiment detection for\nonline Sinhala textual content. Three different sentiment analysis models are\nbuilt, taking into account a limited subset of reactions, all reactions, and\nanother that derives a positive/negative star rating value. The efficacy of\nthese models in capturing the reactions of the observers are then computed and\ndiscussed. The analysis reveals that binary classification of reactions, for\nSinhala content, is significantly more accurate than the other approaches.\nFurthermore, the inclusion of the like reaction hinders the capability of\naccurately predicting other reactions.", "published": "2021-12-01 13:05:05", "link": "http://arxiv.org/abs/2112.00468v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Weakly-Supervised Video Object Grounding via Causal Intervention", "abstract": "We target at the task of weakly-supervised video object grounding (WSVOG),\nwhere only video-sentence annotations are available during model learning. It\naims to localize objects described in the sentence to visual regions in the\nvideo, which is a fundamental capability needed in pattern analysis and machine\nlearning. Despite the recent progress, existing methods all suffer from the\nsevere problem of spurious association, which will harm the grounding\nperformance. In this paper, we start from the definition of WSVOG and pinpoint\nthe spurious association from two aspects: (1) the association itself is not\nobject-relevant but extremely ambiguous due to weak supervision, and (2) the\nassociation is unavoidably confounded by the observational bias when taking the\nstatistics-based matching strategy in existing methods. With this in mind, we\ndesign a unified causal framework to learn the deconfounded object-relevant\nassociation for more accurate and robust video object grounding. Specifically,\nwe learn the object-relevant association by causal intervention from the\nperspective of video data generation process. To overcome the problems of\nlacking fine-grained supervision in terms of intervention, we propose a novel\nspatial-temporal adversarial contrastive learning paradigm. To further remove\nthe accompanying confounding effect within the object-relevant association, we\npursue the true causality by conducting causal intervention via backdoor\nadjustment. Finally, the deconfounded object-relevant association is learned\nand optimized under a unified causal framework in an end-to-end manner.\nExtensive experiments on both IID and OOD testing sets of three benchmarks\ndemonstrate its accurate and robust grounding performance against\nstate-of-the-arts.", "published": "2021-12-01 13:13:03", "link": "http://arxiv.org/abs/2112.00475v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-sentence\n  Dependency Graph", "abstract": "We target the task of cross-lingual Machine Reading Comprehension (MRC) in\nthe direct zero-shot setting, by incorporating syntactic features from\nUniversal Dependencies (UD), and the key features we use are the syntactic\nrelations within each sentence. While previous work has demonstrated effective\nsyntax-guided MRC models, we propose to adopt the inter-sentence syntactic\nrelations, in addition to the rudimentary intra-sentence relations, to further\nutilize the syntactic dependencies in the multi-sentence input of the MRC task.\nIn our approach, we build the Inter-Sentence Dependency Graph (ISDG) connecting\ndependency trees to form global syntactic relations across sentences. We then\npropose the ISDG encoder that encodes the global dependency graph, addressing\nthe inter-sentence relations via both one-hop and multi-hop dependency paths\nexplicitly. Experiments on three multilingual MRC datasets (XQuAD, MLQA,\nTyDiQA-GoldP) show that our encoder that is only trained on English is able to\nimprove the zero-shot performance on all 14 test sets covering 8 languages,\nwith up to 3.8 F1 / 5.2 EM improvement on-average, and 5.2 F1 / 11.2 EM on\ncertain languages. Further analysis shows the improvement can be attributed to\nthe attention on the cross-linguistically consistent syntactic path.", "published": "2021-12-01 13:58:39", "link": "http://arxiv.org/abs/2112.00503v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Empirical evaluation of shallow and deep learning classifiers for Arabic\n  sentiment analysis", "abstract": "This work presents a detailed comparison of the performance of deep learning\nmodels such as convolutional neural networks (CNN), long short-term memory\n(LSTM), gated recurrent units (GRU), their hybrids, and a selection of shallow\nlearning classifiers for sentiment analysis of Arabic reviews. Additionally,\nthe comparison includes state-of-the-art models such as the transformer\narchitecture and the araBERT pre-trained model. The datasets used in this study\nare multi-dialect Arabic hotel and book review datasets, which are some of the\nlargest publicly available datasets for Arabic reviews. Results showed deep\nlearning outperforming shallow learning for binary and multi-label\nclassification, in contrast with the results of similar work reported in the\nliterature. This discrepancy in outcome was caused by dataset size as we found\nit to be proportional to the performance of deep learning models. The\nperformance of deep and shallow learning techniques was analyzed in terms of\naccuracy and F1 score. The best performing shallow learning technique was\nRandom Forest followed by Decision Tree, and AdaBoost. The deep learning models\nperformed similarly using a default embedding layer, while the transformer\nmodel performed best when augmented with araBERT.", "published": "2021-12-01 14:45:43", "link": "http://arxiv.org/abs/2112.00534v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.5"], "primary_category": "cs.CL"}
{"title": "DPRK-BERT: The Supreme Language Model", "abstract": "Deep language models have achieved remarkable success in the NLP domain. The\nstandard way to train a deep language model is to employ unsupervised learning\nfrom scratch on a large unlabeled corpus. However, such large corpora are only\navailable for widely-adopted and high-resource languages and domains. This\nstudy presents the first deep language model, DPRK-BERT, for the DPRK language.\nWe achieve this by compiling the first unlabeled corpus for the DPRK language\nand fine-tuning a preexisting the ROK language model. We compare the proposed\nmodel with existing approaches and show significant improvements on two DPRK\ndatasets. We also present a cross-lingual version of this model which yields\nbetter generalization across the two Korean languages. Finally, we provide\nvarious NLP tools related to the DPRK language that would foster future\nresearch.", "published": "2021-12-01 15:36:13", "link": "http://arxiv.org/abs/2112.00567v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Systematic Generalization with Edge Transformers", "abstract": "Recent research suggests that systematic generalization in natural language\nunderstanding remains a challenge for state-of-the-art neural models such as\nTransformers and Graph Neural Networks. To tackle this challenge, we propose\nEdge Transformer, a new model that combines inspiration from Transformers and\nrule-based symbolic AI. The first key idea in Edge Transformers is to associate\nvector states with every edge, that is, with every pair of input nodes -- as\nopposed to just every node, as it is done in the Transformer model. The second\nmajor innovation is a triangular attention mechanism that updates edge\nrepresentations in a way that is inspired by unification from logic\nprogramming. We evaluate Edge Transformer on compositional generalization\nbenchmarks in relational reasoning, semantic parsing, and dependency parsing.\nIn all three settings, the Edge Transformer outperforms Relation-aware,\nUniversal and classical Transformer baselines.", "published": "2021-12-01 15:50:45", "link": "http://arxiv.org/abs/2112.00578v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Building astroBERT, a language model for Astronomy & Astrophysics", "abstract": "The existing search tools for exploring the NASA Astrophysics Data System\n(ADS) can be quite rich and empowering (e.g., similar and trending operators),\nbut researchers are not yet allowed to fully leverage semantic search. For\nexample, a query for \"results from the Planck mission\" should be able to\ndistinguish between all the various meanings of Planck (person, mission,\nconstant, institutions and more) without further clarification from the user.\nAt ADS, we are applying modern machine learning and natural language processing\ntechniques to our dataset of recent astronomy publications to train astroBERT,\na deeply contextual language model based on research at Google. Using\nastroBERT, we aim to enrich the ADS dataset and improve its discoverability,\nand in particular we are developing our own named entity recognition tool. We\npresent here our preliminary results and lessons learned.", "published": "2021-12-01 16:01:46", "link": "http://arxiv.org/abs/2112.00590v1", "categories": ["cs.CL", "astro-ph.IM"], "primary_category": "cs.CL"}
{"title": "Object-aware Video-language Pre-training for Retrieval", "abstract": "Recently, by introducing large-scale dataset and strong transformer network,\nvideo-language pre-training has shown great success especially for retrieval.\nYet, existing video-language transformer models do not explicitly fine-grained\nsemantic align. In this work, we present Object-aware Transformers, an\nobject-centric approach that extends video-language transformer to incorporate\nobject representations. The key idea is to leverage the bounding boxes and\nobject tags to guide the training process. We evaluate our model on three\nstandard sub-tasks of video-text matching on four widely used benchmarks. We\nalso provide deep analysis and detailed ablation about the proposed method. We\nshow clear improvement in performance across all tasks and datasets considered,\ndemonstrating the value of a model that incorporates object representations\ninto a video-language architecture. The code will be released at\n\\url{https://github.com/FingerRec/OA-Transformer}.", "published": "2021-12-01 17:06:39", "link": "http://arxiv.org/abs/2112.00656v6", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Controlling Conditional Language Models without Catastrophic Forgetting", "abstract": "Machine learning is shifting towards general-purpose pretrained generative\nmodels, trained in a self-supervised manner on large amounts of data, which can\nthen be applied to solve a large number of tasks. However, due to their generic\ntraining methodology, these models often fail to meet some of the downstream\nrequirements (e.g., hallucinations in abstractive summarization or style\nviolations in code generation). This raises the important question of how to\nadapt pre-trained generative models to meet all requirements without destroying\ntheir general capabilities (\"catastrophic forgetting\"). Recent work has\nproposed to solve this problem by representing task-specific requirements\nthrough energy-based models (EBMs) and approximating these EBMs using\ndistributional policy gradients (DPG). Despite its effectiveness, this approach\nis however limited to unconditional distributions. In this paper, we extend DPG\nto conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on\nfour different control objectives across three tasks (translation,\nsummarization and code generation) and two pretrained models (T5 and GPT-Neo).\nOur results show that fine-tuning using CDPG robustly moves these pretrained\nmodels closer towards meeting control objectives and -- in contrast with\nbaseline approaches -- does not result in catastrophic forgetting.", "published": "2021-12-01 19:24:05", "link": "http://arxiv.org/abs/2112.00791v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Iconary: A Pictionary-Based Game for Testing Multimodal Communication\n  with Drawings and Text", "abstract": "Communicating with humans is challenging for AIs because it requires a shared\nunderstanding of the world, complex semantics (e.g., metaphors or analogies),\nand at times multi-modal gestures (e.g., pointing with a finger, or an arrow in\na diagram). We investigate these challenges in the context of Iconary, a\ncollaborative game of drawing and guessing based on Pictionary, that poses a\nnovel challenge for the research community. In Iconary, a Guesser tries to\nidentify a phrase that a Drawer is drawing by composing icons, and the Drawer\niteratively revises the drawing to help the Guesser in response. This\nback-and-forth often uses canonical scenes, visual metaphor, or icon\ncompositions to express challenging words, making it an ideal test for mixing\nlanguage and visual/symbolic communication in AI. We propose models to play\nIconary and train them on over 55,000 games between human players. Our models\nare skillful players and are able to employ world knowledge in language models\nto play with words unseen during training. Elite human players outperform our\nmodels, particularly at the drawing task, leaving an important gap for future\nresearch to address. We release our dataset, code, and evaluation setup as a\nchallenge to the community at http://www.github.com/allenai/iconary.", "published": "2021-12-01 19:41:03", "link": "http://arxiv.org/abs/2112.00800v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A General Language Assistant as a Laboratory for Alignment", "abstract": "Given the broad capabilities of large language models, it should be possible\nto work towards a general-purpose, text-based assistant that is aligned with\nhuman values, meaning that it is helpful, honest, and harmless. As an initial\nforay in this direction we study simple baseline techniques and evaluations,\nsuch as prompting. We find that the benefits from modest interventions increase\nwith model size, generalize to a variety of alignment evaluations, and do not\ncompromise the performance of large models. Next we investigate scaling trends\nfor several training objectives relevant to alignment, comparing imitation\nlearning, binary discrimination, and ranked preference modeling. We find that\nranked preference modeling performs much better than imitation learning, and\noften scales more favorably with model size. In contrast, binary discrimination\ntypically performs and scales very similarly to imitation learning. Finally we\nstudy a `preference model pre-training' stage of training, with the goal of\nimproving sample efficiency when finetuning on human preferences.", "published": "2021-12-01 22:24:34", "link": "http://arxiv.org/abs/2112.00861v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards More Robust Natural Language Understanding", "abstract": "Natural Language Understanding (NLU) is a branch of Natural Language\nProcessing (NLP) that uses intelligent computer software to understand texts\nthat encode human knowledge. Recent years have witnessed notable progress\nacross various NLU tasks with deep learning techniques, especially with\npretrained language models. Besides proposing more advanced model\narchitectures, constructing more reliable and trustworthy datasets also plays a\nhuge role in improving NLU systems, without which it would be impossible to\ntrain a decent NLU model. It's worth noting that the human ability of\nunderstanding natural language is flexible and robust. On the contrary, most of\nexisting NLU systems fail to achieve desirable performance on out-of-domain\ndata or struggle on handling challenging items (e.g., inherently ambiguous\nitems, adversarial items) in the real world. Therefore, in order to have NLU\nmodels understand human language more effectively, it is expected to prioritize\nthe study on robust natural language understanding. In this thesis, we deem\nthat NLU systems are consisting of two components: NLU models and NLU datasets.\nAs such, we argue that, to achieve robust NLU, the model architecture/training\nand the dataset are equally important. Specifically, we will focus on three NLU\ntasks to illustrate the robustness problem in different NLU tasks and our\ncontributions (i.e., novel models and new datasets) to help achieve more robust\nnatural language understanding. Moving forward, the ultimate goal for robust\nnatural language understanding is to build NLU models which can behave humanly.\nThat is, it's expected that robust NLU systems are capable to transfer the\nknowledge from training corpus to unseen documents more reliably and survive\nwhen encountering challenging items even if the system doesn't know a priori of\nusers' inputs.", "published": "2021-12-01 17:27:19", "link": "http://arxiv.org/abs/2112.02992v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigation of Training Label Error Impact on RNN-T", "abstract": "In this paper, we propose an approach to quantitatively analyze impacts of\ndifferent training label errors to RNN-T based ASR models. The result shows\ndeletion errors are more harmful than substitution and insertion label errors\nin RNN-T training data. We also examined label error impact mitigation\napproaches on RNN-T and found that, though all the methods mitigate the\nlabel-error-caused degradation to some extent, they could not remove the\nperformance gap between the models trained with and without the presence of\nlabel errors. Based on the analysis results, we suggest to design data\npipelines for RNN-T with higher priority on reducing deletion label errors. We\nalso find that ensuring high-quality training labels remains important, despite\nof the existence of the label error mitigation approaches.", "published": "2021-12-01 08:57:39", "link": "http://arxiv.org/abs/2112.00350v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CLIPstyler: Image Style Transfer with a Single Text Condition", "abstract": "Existing neural style transfer methods require reference style images to\ntransfer texture information of style images to content images. However, in\nmany practical situations, users may not have reference style images but still\nbe interested in transferring styles by just imagining them. In order to deal\nwith such applications, we propose a new framework that enables a style\ntransfer `without' a style image, but only with a text description of the\ndesired style. Using the pre-trained text-image embedding model of CLIP, we\ndemonstrate the modulation of the style of content images only with a single\ntext condition. Specifically, we propose a patch-wise text-image matching loss\nwith multiview augmentations for realistic texture transfer. Extensive\nexperimental results confirmed the successful image style transfer with\nrealistic textures that reflect semantic query texts.", "published": "2021-12-01 09:48:53", "link": "http://arxiv.org/abs/2112.00374v3", "categories": ["cs.CV", "cs.CL", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Exploration into Translation-Equivariant Image Quantization", "abstract": "This is an exploratory study that discovers the current image quantization\n(vector quantization) do not satisfy translation equivariance in the quantized\nspace due to aliasing. Instead of focusing on anti-aliasing, we propose a\nsimple yet effective way to achieve translation-equivariant image quantization\nby enforcing orthogonality among the codebook embeddings. To explore the\nadvantages of translation-equivariant image quantization, we conduct three\nproof-of-concept experiments with a carefully controlled dataset: (1)\ntext-to-image generation, where the quantized image indices are the target to\npredict, (2) image-to-text generation, where the quantized image indices are\ngiven as a condition, (3) using a smaller training set to analyze sample\nefficiency. From the strictly controlled experiments, we empirically verify\nthat the translation-equivariant image quantizer improves not only sample\nefficiency but also the accuracy over VQGAN up to +11.9% in text-to-image\ngeneration and +3.9% in image-to-text generation.", "published": "2021-12-01 10:08:24", "link": "http://arxiv.org/abs/2112.00384v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "STEM: Unsupervised STructural EMbedding for Stance Detection", "abstract": "Stance detection is an important task, supporting many downstream tasks such\nas discourse parsing and modeling the propagation of fake news, rumors, and\nscience denial. In this paper, we propose a novel framework for stance\ndetection. Our framework is unsupervised and domain-independent. Given a claim\nand a multi-participant discussion - we construct the interaction network from\nwhich we derive topological embedding for each speaker. These speaker embedding\nenjoy the following property: speakers with the same stance tend to be\nrepresented by similar vectors, while antipodal vectors represent speakers with\nopposing stances. These embedding are then used to divide the speakers into\nstance-partitions. We evaluate our method on three different datasets from\ndifferent platforms. Our method outperforms or is comparable with supervised\nmodels while providing confidence levels for its output. Furthermore, we\ndemonstrate how the structural embedding relate to the valence expressed by the\nspeakers. Finally, we discuss some limitations inherent to the framework.", "published": "2021-12-01 18:43:00", "link": "http://arxiv.org/abs/2112.00712v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Deliberation of Streaming RNN-Transducer by Non-autoregressive Decoding", "abstract": "We propose to deliberate the hypothesis alignment of a streaming RNN-T model\nwith the previously proposed Align-Refine non-autoregressive decoding method\nand its improved versions. The method performs a few refinement steps, where\neach step shares a transformer decoder that attends to both text features\n(extracted from alignments) and audio features, and outputs complete updated\nalignments. The transformer decoder is trained with the CTC loss which\nfacilitates parallel greedy decoding, and performs full-context attention to\ncapture label dependencies. We improve Align-Refine by introducing cascaded\nencoder that captures more audio context before refinement, and alignment\naugmentation which enforces learning label dependency. We show that,\nconditioned on hypothesis alignments of a streaming RNN-T model, our method\nobtains significantly more accurate recognition results than the first-pass\nRNN-T, with only small amount of model parameters.", "published": "2021-12-01 01:34:28", "link": "http://arxiv.org/abs/2112.11442v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Environmental Sound Extraction Using Onomatopoeic Words", "abstract": "An onomatopoeic word, which is a character sequence that phonetically\nimitates a sound, is effective in expressing characteristics of sound such as\nduration, pitch, and timbre. We propose an environmental-sound-extraction\nmethod using onomatopoeic words to specify the target sound to be extracted. By\nthis method, we estimate a time-frequency mask from an input mixture\nspectrogram and an onomatopoeic word using a U-Net architecture, then extract\nthe corresponding target sound by masking the spectrogram. Experimental results\nindicate that the proposed method can extract only the target sound\ncorresponding to the onomatopoeic word and performs better than conventional\nmethods that use sound-event classes to specify the target sound.", "published": "2021-12-01 01:18:06", "link": "http://arxiv.org/abs/2112.00209v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PoseKernelLifter: Metric Lifting of 3D Human Pose using Sound", "abstract": "Reconstructing the 3D pose of a person in metric scale from a single view\nimage is a geometrically ill-posed problem. For example, we can not measure the\nexact distance of a person to the camera from a single view image without\nadditional scene assumptions (e.g., known height). Existing learning based\napproaches circumvent this issue by reconstructing the 3D pose up to scale.\nHowever, there are many applications such as virtual telepresence, robotics,\nand augmented reality that require metric scale reconstruction. In this paper,\nwe show that audio signals recorded along with an image, provide complementary\ninformation to reconstruct the metric 3D pose of the person.\n  The key insight is that as the audio signals traverse across the 3D space,\ntheir interactions with the body provide metric information about the body's\npose. Based on this insight, we introduce a time-invariant transfer function\ncalled pose kernel -- the impulse response of audio signals induced by the body\npose. The main properties of the pose kernel are that (1) its envelope highly\ncorrelates with 3D pose, (2) the time response corresponds to arrival time,\nindicating the metric distance to the microphone, and (3) it is invariant to\nchanges in the scene geometry configurations. Therefore, it is readily\ngeneralizable to unseen scenes. We design a multi-stage 3D CNN that fuses audio\nand visual signals and learns to reconstruct 3D pose in a metric scale. We show\nthat our multi-modal method produces accurate metric reconstruction in real\nworld scenes, which is not possible with state-of-the-art lifting approaches\nincluding parametric mesh regression and depth regression.", "published": "2021-12-01 01:34:56", "link": "http://arxiv.org/abs/2112.00216v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Score Transformer: Generating Musical Score from Note-level\n  Representation", "abstract": "In this paper, we explore the tokenized representation of musical scores\nusing the Transformer model to automatically generate musical scores. Thus far,\nsequence models have yielded fruitful results with note-level (MIDI-equivalent)\nsymbolic representations of music. Although the note-level representations can\ncomprise sufficient information to reproduce music aurally, they cannot contain\nadequate information to represent music visually in terms of notation. Musical\nscores contain various musical symbols (e.g., clef, key signature, and notes)\nand attributes (e.g., stem direction, beam, and tie) that enable us to visually\ncomprehend musical content. However, automated estimation of these elements has\nyet to be comprehensively addressed. In this paper, we first design score token\nrepresentation corresponding to the various musical elements. We then train the\nTransformer model to transcribe note-level representation into appropriate\nmusic notation. Evaluations of popular piano scores show that the proposed\nmethod significantly outperforms existing methods on all 12 musical aspects\nthat were investigated. We also explore an effective notation-level token\nrepresentation to work with the model and determine that our proposed\nrepresentation produces the steadiest results.", "published": "2021-12-01 09:08:01", "link": "http://arxiv.org/abs/2112.00355v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Predicting lexical skills from oral reading with acoustic measures", "abstract": "Literacy assessment is an important activity for education administrators\nacross the globe. Typically achieved in a school setting by testing a child's\noral reading, it is intensive in human resources. While automatic speech\nrecognition (ASR) is a potential solution to the problem, it tends to be\ncomputationally expensive for hand-held devices apart from needing language and\naccent-specific speech for training. In this work, we propose a system to\npredict the word-decoding skills of a student based on simple acoustic features\nderived from the recording. We first identify a meaningful categorization of\nword-decoding skills by analyzing a manually transcribed data set of children's\noral reading recordings. Next the automatic prediction of the category is\nattempted with the proposed acoustic features. Pause statistics, syllable rate\nand spectral and intensity dynamics are found to be reliable indicators of\nspecific types of oral reading deficits, providing useful feedback by\ndiscriminating the different characteristics of beginning readers. This\ncomputationally simple and language-agnostic approach is found to provide a\nperformance close to that obtained using a language dependent ASR that required\nconsiderable tuning of its parameters.", "published": "2021-12-01 16:39:31", "link": "http://arxiv.org/abs/2112.00635v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Semi-supervised music emotion recognition using noisy student training\n  and harmonic pitch class profiles", "abstract": "We present Mirable's submission to the 2021 Emotions and Themes in Music\nchallenge. In this work, we intend to address the question: can we leverage\nsemi-supervised learning techniques on music emotion recognition? With that, we\nexperiment with noisy student training, which has improved model performance in\nthe image classification domain. As the noisy student method requires a strong\nteacher model, we further delve into the factors including (i) input training\nlength and (ii) complementary music representations to further boost the\nperformance of the teacher model. For (i), we find that models trained with\nshort input length perform better in PR-AUC, whereas those trained with long\ninput length perform better in ROC-AUC. For (ii), we find that using harmonic\npitch class profiles (HPCP) consistently improve tagging performance, which\nsuggests that harmonic representation is useful for music emotion tagging.\nFinally, we find that noisy student method only improves tagging results for\nthe case of long training length. Additionally, we find that ensembling\nrepresentations trained with different training lengths can improve tagging\nresults significantly, which suggest a possible direction to explore\nincorporating multiple temporal resolutions in the network architecture for\nfuture work.", "published": "2021-12-01 18:25:51", "link": "http://arxiv.org/abs/2112.00702v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
