{"title": "Best-$k$ Search Algorithm for Neural Text Generation", "abstract": "Modern natural language generation paradigms require a good decoding strategy\nto obtain quality sequences out of the model. Beam search yields high-quality\nbut low diversity outputs; stochastic approaches suffer from high variance and\nsometimes low quality, but the outputs tend to be more natural and creative. In\nthis work, we propose a deterministic search algorithm balancing both quality\nand diversity. We first investigate the vanilla best-first search (BFS)\nalgorithm and then propose the Best-$k$ Search algorithm. Inspired by BFS, we\ngreedily expand the top $k$ nodes, instead of only the first node, to boost\nefficiency and diversity. Upweighting recently discovered nodes accompanied by\nheap pruning ensures the completeness of the search procedure. Experiments on\nfour NLG tasks, including question generation, commonsense generation, text\nsummarization, and translation, show that best-$k$ search yields more diverse\nand natural outputs compared to strong baselines, while our approach maintains\nhigh text quality. The proposed algorithm is parameter-free, lightweight,\nefficient, and easy to use.", "published": "2022-11-22 00:26:13", "link": "http://arxiv.org/abs/2211.11924v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BotSIM: An End-to-End Bot Simulation Framework for Commercial\n  Task-Oriented Dialog Systems", "abstract": "We present BotSIM, a data-efficient end-to-end Bot SIMulation toolkit for\ncommercial text-based task-oriented dialog (TOD) systems. BotSIM consists of\nthree major components: 1) a Generator that can infer semantic-level dialog\nacts and entities from bot definitions and generate user queries via\nmodel-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to\nsimulate conversations with the dialog agents; 3) a Remediator to analyze the\nsimulated conversations, visualize the bot health reports and provide\nactionable remediation suggestions for bot troubleshooting and improvement. We\ndemonstrate BotSIM's effectiveness in end-to-end evaluation, remediation and\nmulti-intent dialog generation via case studies on two commercial bot\nplatforms. BotSIM's \"generation-simulation-remediation\" paradigm accelerates\nthe end-to-end bot evaluation and iteration process by: 1) reducing manual test\ncases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU\nand end-to-end performance via extensive dialog simulation; 3) improving the\nbot troubleshooting process with actionable suggestions. A demo of our system\ncan be found at https://tinyurl.com/mryu74cd and a demo video at\nhttps://youtu.be/qLi5iSoly30. We have open-sourced the toolkit at\nhttps://github.com/salesforce/botsim", "published": "2022-11-22 03:34:36", "link": "http://arxiv.org/abs/2211.11982v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArzEn-ST: A Three-way Speech Translation Corpus for Code-Switched\n  Egyptian Arabic - English", "abstract": "We present our work on collecting ArzEn-ST, a code-switched Egyptian Arabic -\nEnglish Speech Translation Corpus. This corpus is an extension of the ArzEn\nspeech corpus, which was collected through informal interviews with bilingual\nspeakers. In this work, we collect translations in both directions, monolingual\nEgyptian Arabic and monolingual English, forming a three-way speech translation\ncorpus. We make the translation guidelines and corpus publicly available. We\nalso report results for baseline systems for machine translation and speech\ntranslation tasks. We believe this is a valuable resource that can motivate and\nfacilitate further research studying the code-switching phenomenon from a\nlinguistic perspective and can be used to train and evaluate NLP systems.", "published": "2022-11-22 04:37:14", "link": "http://arxiv.org/abs/2211.12000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HaRiM$^+$: Evaluating Summary Quality with Hallucination Risk", "abstract": "One of the challenges of developing a summarization model arises from the\ndifficulty in measuring the factual inconsistency of the generated text. In\nthis study, we reinterpret the decoder overconfidence-regularizing objective\nsuggested in (Miao et al., 2021) as a hallucination risk measurement to better\nestimate the quality of generated summaries. We propose a reference-free\nmetric, HaRiM+, which only requires an off-the-shelf summarization model to\ncompute the hallucination risk based on token likelihoods. Deploying it\nrequires no additional training of models or ad-hoc modules, which usually need\nalignment to human judgments. For summary-quality estimation, HaRiM+ records\nstate-of-the-art correlation to human judgment on three summary-quality\nannotation sets: FRANK, QAGS, and SummEval. We hope that our work, which merits\nthe use of summarization models, facilitates the progress of both automated\nevaluation and generation of summary.", "published": "2022-11-22 09:36:41", "link": "http://arxiv.org/abs/2211.12118v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Dataset for Biomedical Keyphrase Generation", "abstract": "Keyphrase generation is the task consisting in generating a set of words or\nphrases that highlight the main topics of a document. There are few datasets\nfor keyphrase generation in the biomedical domain and they do not meet the\nexpectations in terms of size for training generative models. In this paper, we\nintroduce kp-biomed, the first large-scale biomedical keyphrase generation\ndataset with more than 5M documents collected from PubMed abstracts. We train\nand release several generative models and conduct a series of experiments\nshowing that using large scale datasets improves significantly the performances\nfor present and absent keyphrase generation. The dataset is available under\nCC-BY-NC v4.0 license at https://huggingface.co/ datasets/taln-ls2n/kpbiomed.", "published": "2022-11-22 09:53:23", "link": "http://arxiv.org/abs/2211.12124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Converge to the Truth: Factual Error Correction via Iterative\n  Constrained Editing", "abstract": "Given a possibly false claim sentence, how can we automatically correct it\nwith minimal editing? Existing methods either require a large number of pairs\nof false and corrected claims for supervised training or do not handle well\nerrors spanning over multiple tokens within an utterance. In this paper, we\npropose VENCE, a novel method for factual error correction (FEC) with minimal\nedits. VENCE formulates the FEC problem as iterative sampling editing actions\nwith respect to a target density function. We carefully design the target\nfunction with predicted truthfulness scores from an offline trained fact\nverification model. VENCE samples the most probable editing positions based on\nback-calculated gradients of the truthfulness score concerning input tokens and\nthe editing actions using a distantly-supervised language model (T5).\nExperiments on a public dataset show that VENCE improves the well-adopted SARI\nmetric by 5.3 (or a relative improvement of 11.8%) over the previous best\ndistantly-supervised methods.", "published": "2022-11-22 10:03:13", "link": "http://arxiv.org/abs/2211.12130v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Causality Identification with Causal News Corpus -- Shared Task 3,\n  CASE 2022", "abstract": "The Event Causality Identification Shared Task of CASE 2022 involved two\nsubtasks working on the Causal News Corpus. Subtask 1 required participants to\npredict if a sentence contains a causal relation or not. This is a supervised\nbinary classification task. Subtask 2 required participants to identify the\nCause, Effect and Signal spans per causal sentence. This could be seen as a\nsupervised sequence labeling task. For both subtasks, participants uploaded\ntheir predictions for a held-out test set, and ranking was done based on binary\nF1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes\nthe work of the 17 teams that submitted their results to our competition and 12\nsystem description papers that were received. The best F1 scores achieved for\nSubtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing\napproaches involved pre-trained language models fine-tuned to the targeted\ntask. We further discuss these approaches and analyze errors across\nparticipants' systems in this paper.", "published": "2022-11-22 10:34:09", "link": "http://arxiv.org/abs/2211.12154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PESE: Event Structure Extraction using Pointer Network based\n  Encoder-Decoder Architecture", "abstract": "The task of event extraction (EE) aims to find the events and event-related\nargument information from the text and represent them in a structured format.\nMost previous works try to solve the problem by separately identifying multiple\nsubstructures and aggregating them to get the complete event structure. The\nproblem with the methods is that it fails to identify all the interdependencies\namong the event participants (event-triggers, arguments, and roles). In this\npaper, we represent each event record in a unique tuple format that contains\ntrigger phrase, trigger type, argument phrase, and corresponding role\ninformation. Our proposed pointer network-based encoder-decoder model generates\nan event tuple in each time step by exploiting the interactions among event\nparticipants and presenting a truly end-to-end solution to the EE task. We\nevaluate our model on the ACE2005 dataset, and experimental results demonstrate\nthe effectiveness of our model by achieving competitive performance compared to\nthe state-of-the-art methods.", "published": "2022-11-22 10:36:56", "link": "http://arxiv.org/abs/2211.12157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GDPR Compliant Collection of Therapist-Patient-Dialogues", "abstract": "According to the Global Burden of Disease list provided by the World Health\nOrganization (WHO), mental disorders are among the most debilitating\ndisorders.To improve the diagnosis and the therapy effectiveness in recent\nyears, researchers have tried to identify individual biomarkers. Gathering\nneurobiological data however, is costly and time-consuming. Another potential\nsource of information, which is already part of the clinical routine, are\ntherapist-patient dialogues. While there are some pioneering works\ninvestigating the role of language as predictors for various therapeutic\nparameters, for example patient-therapist alliance, there are no large-scale\nstudies. A major obstacle to conduct these studies is the availability of\nsizeable datasets, which are needed to train machine learning models. While\nthese conversations are part of the daily routine of clinicians, gathering them\nis usually hindered by various ethical (purpose of data usage), legal (data\nprivacy) and technical (data formatting) limitations. Some of these limitations\nare particular to the domain of therapy dialogues, like the increased\ndifficulty in anonymisation, or the transcription of the recordings. In this\npaper, we elaborate on the challenges we faced in starting our collection of\ntherapist-patient dialogues in a psychiatry clinic under the General Data\nPrivacy Regulation of the European Union with the goal to use the data for\nNatural Language Processing (NLP) research. We give an overview of each step in\nour procedure and point out the potential pitfalls to motivate further research\nin this field.", "published": "2022-11-22 15:51:10", "link": "http://arxiv.org/abs/2211.12360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HyperTuning: Toward Adapting Large Language Models without\n  Back-propagation", "abstract": "Fine-tuning large language models for different tasks can be costly and\ninefficient, and even methods that reduce the number of tuned parameters still\nrequire full gradient-based optimization. We propose HyperTuning, a novel\napproach to model adaptation that uses a hypermodel to generate task-specific\nparameters for a fixed downstream model. We demonstrate a simple setup for\nhypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or\nLoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5\nin two stages: first, hyperpretraining with a modified conditional language\nmodeling objective that trains a hypermodel to generate parameters; second,\nmulti-task fine-tuning (MTF) on a large number of diverse language tasks. We\nevaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and\nshow that it can effectively generate parameters for unseen tasks. Moreover, we\nshow that using hypermodel-generated parameters as initializations for further\nparameter-efficient fine-tuning improves performance. HyperTuning can thus be a\nflexible and efficient way to leverage large language models for diverse\ndownstream applications.", "published": "2022-11-22 18:52:25", "link": "http://arxiv.org/abs/2211.12485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Backdoor Attack and Defense in Natural Language Processing", "abstract": "Deep learning is becoming increasingly popular in real-life applications,\nespecially in natural language processing (NLP). Users often choose training\noutsourcing or adopt third-party data and models due to data and computation\nresources being limited. In such a situation, training data and models are\nexposed to the public. As a result, attackers can manipulate the training\nprocess to inject some triggers into the model, which is called backdoor\nattack. Backdoor attack is quite stealthy and difficult to be detected because\nit has little inferior influence on the model's performance for the clean\nsamples. To get a precise grasp and understanding of this problem, in this\npaper, we conduct a comprehensive review of backdoor attacks and defenses in\nthe field of NLP. Besides, we summarize benchmark datasets and point out the\nopen issues to design credible systems to defend against backdoor attacks.", "published": "2022-11-22 02:35:12", "link": "http://arxiv.org/abs/2211.11958v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Linear Interpolation In Parameter Space is Good Enough for Fine-Tuned\n  Language Models", "abstract": "The simplest way to obtain continuous interpolation between two points in\nhigh dimensional space is to draw a line between them. While previous works\nfocused on the general connectivity between model parameters, we explored\nlinear interpolation for parameters of pre-trained models after fine-tuning.\nSurprisingly, we could perform linear interpolation without a performance drop\nin intermediate points for fine-tuned models. For controllable text generation,\nsuch interpolation could be seen as moving a model towards or against the\ndesired text attribute (e.g., positive sentiment), which could be used as\ngrounds for further methods for controllable text generation without inference\nspeed overhead.", "published": "2022-11-22 08:49:22", "link": "http://arxiv.org/abs/2211.12092v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coreference Resolution through a seq2seq Transition-Based System", "abstract": "Most recent coreference resolution systems use search algorithms over\npossible spans to identify mentions and resolve coreference. We instead present\na coreference resolution system that uses a text-to-text (seq2seq) paradigm to\npredict mentions and links jointly. We implement the coreference system as a\ntransition system and use multilingual T5 as an underlying language model. We\nobtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score\nfor English (a 2.3 higher F1-score than previous work (Dobrovolskii, 2021))\nusing only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than\nprevious work) and 74.3 F1-score for Chinese (+5.3). In addition we use the\nSemEval-2010 data sets for experiments in the zero-shot setting, a few-shot\nsetting, and supervised setting using all available training data. We get\nsubstantially higher zero-shot F1-scores for 3 out of 4 languages than previous\napproaches and significantly exceed previous supervised state-of-the-art\nresults for all five tested languages.", "published": "2022-11-22 10:17:50", "link": "http://arxiv.org/abs/2211.12142v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simplicity Bias in Transformers and their Ability to Learn Sparse\n  Boolean Functions", "abstract": "Despite the widespread success of Transformers on NLP tasks, recent works\nhave found that they struggle to model several formal languages when compared\nto recurrent models. This raises the question of why Transformers perform well\nin practice and whether they have any properties that enable them to generalize\nbetter than recurrent models. In this work, we conduct an extensive empirical\nstudy on Boolean functions to demonstrate the following: (i) Random\nTransformers are relatively more biased towards functions of low sensitivity.\n(ii) When trained on Boolean functions, both Transformers and LSTMs prioritize\nlearning functions of low sensitivity, with Transformers ultimately converging\nto functions of lower sensitivity. (iii) On sparse Boolean functions which have\nlow sensitivity, we find that Transformers generalize near perfectly even in\nthe presence of noisy labels whereas LSTMs overfit and achieve poor\ngeneralization accuracy. Overall, our results provide strong quantifiable\nevidence that suggests differences in the inductive biases of Transformers and\nrecurrent models which may help explain Transformer's effective generalization\nperformance despite relatively limited expressiveness.", "published": "2022-11-22 15:10:48", "link": "http://arxiv.org/abs/2211.12316v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Method for Determining the Similarity of Text Documents for the Kazakh\n  language, Taking Into Account Synonyms: Extension to TF-IDF", "abstract": "The task of determining the similarity of text documents has received\nconsiderable attention in many areas such as Information Retrieval, Text\nMining, Natural Language Processing (NLP) and Computational Linguistics.\nTransferring data to numeric vectors is a complex task where algorithms such as\ntokenization, stopword filtering, stemming, and weighting of terms are used.\nThe term frequency - inverse document frequency (TF-IDF) is the most widely\nused term weighting method to facilitate the search for relevant documents. To\nimprove the weighting of terms, a large number of TF-IDF extensions are made.\nIn this paper, another extension of the TF-IDF method is proposed where\nsynonyms are taken into account. The effectiveness of the method is confirmed\nby experiments on functions such as Cosine, Dice and Jaccard to measure the\nsimilarity of text documents for the Kazakh language.", "published": "2022-11-22 15:54:41", "link": "http://arxiv.org/abs/2211.12364v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "An Emotion-Aware Multi-Task Approach to Fake News and Rumour Detection\n  using Transfer Learning", "abstract": "Social networking sites, blogs, and online articles are instant sources of\nnews for internet users globally. However, in the absence of strict regulations\nmandating the genuineness of every text on social media, it is probable that\nsome of these texts are fake news or rumours. Their deceptive nature and\nability to propagate instantly can have an adverse effect on society. This\nnecessitates the need for more effective detection of fake news and rumours on\nthe web. In this work, we annotate four fake news detection and rumour\ndetection datasets with their emotion class labels using transfer learning. We\nshow the correlation between the legitimacy of a text with its intrinsic\nemotion for fake news and rumour detection, and prove that even within the same\nemotion class, fake and real news are often represented differently, which can\nbe used for improved feature extraction. Based on this, we propose a multi-task\nframework for fake news and rumour detection, predicting both the emotion and\nlegitimacy of the text. We train a variety of deep learning models in\nsingle-task and multi-task settings for a more comprehensive comparison. We\nfurther analyze the performance of our multi-task approach for fake news\ndetection in cross-domain settings to verify its efficacy for better\ngeneralization across datasets, and to verify that emotions act as a\ndomain-independent feature. Experimental results verify that our multi-task\nmodels consistently outperform their single-task counterparts in terms of\naccuracy, precision, recall, and F1 score, both for in-domain and cross-domain\nsettings. We also qualitatively analyze the difference in performance in\nsingle-task and multi-task learning models.", "published": "2022-11-22 16:15:25", "link": "http://arxiv.org/abs/2211.12374v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks", "abstract": "Vision language pre-training aims to learn alignments between vision and\nlanguage from a large amount of data. Most existing methods only learn\nimage-text alignments. Some others utilize pre-trained object detectors to\nleverage vision language alignments at the object level. In this paper, we\npropose to learn multi-grained vision language alignments by a unified\npre-training framework that learns multi-grained aligning and multi-grained\nlocalization simultaneously. Based on it, we present X$^2$-VLM, an all-in-one\nmodel with a flexible modular architecture, in which we further unify\nimage-text pre-training and video-text pre-training in one model. X$^2$-VLM is\nable to learn unlimited visual concepts associated with diverse text\ndescriptions. Experiment results show that X$^2$-VLM performs the best on base\nand large scale for both image-text and video-text tasks, making a good\ntrade-off between performance and model scale. Moreover, we show that the\nmodular design of X$^2$-VLM results in high transferability for it to be\nutilized in any language or domain. For example, by simply replacing the text\nencoder with XLM-R, X$^2$-VLM outperforms state-of-the-art multilingual\nmulti-modal pre-trained models without any multilingual pre-training. The code\nand pre-trained models are available at https://github.com/zengyan-97/X2-VLM.", "published": "2022-11-22 16:48:01", "link": "http://arxiv.org/abs/2211.12402v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Smart Agriculture : A Novel Multilevel Approach for Agricultural Risk\n  Assessment over Unstructured Data", "abstract": "Detecting opportunities and threats from massive text data is a challenging\ntask for most. Traditionally, companies would rely mainly on structured data to\ndetect and predict risks, losing a huge amount of information that could be\nextracted from unstructured text data. Fortunately, artificial intelligence\ncame to remedy this issue by innovating in data extraction and processing\ntechniques, allowing us to understand and make use of Natural Language data and\nturning it into structures that a machine can process and extract insight from.\nUncertainty refers to a state of not knowing what will happen in the future.\nThis paper aims to leverage natural language processing and machine learning\ntechniques to model uncertainties and evaluate the risk level in each\nuncertainty cluster using massive text data.", "published": "2022-11-22 16:47:47", "link": "http://arxiv.org/abs/2211.12515v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning\n  for Numerical Reasoning Tasks", "abstract": "Recently, there has been significant progress in teaching language models to\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\ntasks. CoT uses language models to perform both reasoning and computation in\nthe multi-step `thought' process. To disentangle computation from reasoning, we\npropose `Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto express the reasoning process as a program. The computation is relegated to\nan external computer, which executes the generated programs to derive the\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\nsettings, PoT can show an average performance gain over CoT by around 12\\%\nacross all the evaluated datasets. By combining PoT with self-consistency\ndecoding, we can achieve SoTA performance on all math problem datasets and\nnear-SoTA performance on financial datasets. All of our data and code are\nreleased in Github https://github.com/wenhuchen/Program-of-Thoughts", "published": "2022-11-22 21:06:00", "link": "http://arxiv.org/abs/2211.12588v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AutoReply: Detecting Nonsense in Dialogue Introspectively with\n  Discriminative Replies", "abstract": "Existing approaches built separate classifiers to detect nonsense in\ndialogues. In this paper, we show that without external classifiers, dialogue\nmodels can detect errors in their own messages introspectively, by calculating\nthe likelihood of replies that are indicative of poor messages. For example, if\nan agent believes its partner is likely to respond \"I don't understand\" to a\ncandidate message, that message may not make sense, so an alternative message\nshould be chosen. We evaluate our approach on a dataset from the game\nDiplomacy, which contains long dialogues richly grounded in the game state, on\nwhich existing models make many errors. We first show that hand-crafted replies\ncan be effective for the task of detecting nonsense in applications as complex\nas Diplomacy. We then design AutoReply, an algorithm to search for such\ndiscriminative replies automatically, given a small number of annotated\ndialogue examples. We find that AutoReply-generated replies outperform\nhandcrafted replies and perform on par with carefully fine-tuned large\nsupervised models. Results also show that one single reply without much\ncomputation overheads can also detect dialogue nonsense reasonably well.", "published": "2022-11-22 22:31:34", "link": "http://arxiv.org/abs/2211.12615v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Average Token Delay: A Latency Metric for Simultaneous Translation", "abstract": "Simultaneous translation is a task in which translation begins before the\nspeaker has finished speaking. In its evaluation, we have to consider the\nlatency of the translation in addition to the quality. The latency is\npreferably as small as possible for users to comprehend what the speaker says\nwith a small delay. Existing latency metrics focus on when the translation\nstarts but do not consider adequately when the translation ends. This means\nsuch metrics do not penalize the latency caused by a long translation output,\nwhich actually delays users' comprehension. In this work, we propose a novel\nlatency evaluation metric called Average Token Delay (ATD) that focuses on the\nend timings of partial translations in simultaneous translation. We discuss the\nadvantage of ATD using simulated examples and also investigate the differences\nbetween ATD and Average Lagging with simultaneous translation experiments.", "published": "2022-11-22 06:45:13", "link": "http://arxiv.org/abs/2211.13173v2", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "YZR-net : Self-supervised Hidden representations Invariant to\n  Transformations for profanity detection", "abstract": "On current {\\it e-}learning platforms, live classes are an important tool\nthat provides students with an opportunity to get more involved while learning\nnew concepts. In such classes, the element of interaction with teachers and\nfellow peers helps in removing learning silos and gives each student a chance\nto experience some aspects relevant to offline learning in this era of virtual\nclasses. One common way of interaction in a class is through the chats /\nmessaging framework, where the teacher can broadcast messages as well as get\ninstant feedback from the students in the live class. This freedom of\ninteraction is a crucial aspect for any student's learning growth but misuse of\nit can have serious repercussions. Some miscreants use this framework to send\nprofane messages which can have a negative impact on other students as well as\nthe teacher of the class. These rare but high impact situations obviate the\nneed for automatic detection mechanisms that prevent the posting of such chats\non any platform. In this work we develop YZR-Net which is a self-supervised\nframework that is able to robustly detect profane words used in a chat even if\nthe student tries to add clever modifications to fool the system. The matching\nmechanism on token / word level allows us to maintain a compact as well as\ndynamic profane vocabulary which can be updated without retraining the\nunderlying model. Our profanity detection framework is language independent and\ncan handle abuses in both English as well as its transliterated counterpart\nHinglish (Hindi language words written in English).", "published": "2022-11-22 05:35:18", "link": "http://arxiv.org/abs/2211.15532v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Visually Grounded Commonsense Knowledge Acquisition", "abstract": "Large-scale commonsense knowledge bases empower a broad range of AI\napplications, where the automatic extraction of commonsense knowledge (CKE) is\na fundamental and challenging problem. CKE from text is known for suffering\nfrom the inherent sparsity and reporting bias of commonsense in text. Visual\nperception, on the other hand, contains rich commonsense knowledge about\nreal-world entities, e.g., (person, can_hold, bottle), which can serve as\npromising sources for acquiring grounded commonsense knowledge. In this work,\nwe present CLEVER, which formulates CKE as a distantly supervised\nmulti-instance learning problem, where models learn to summarize commonsense\nrelations from a bag of images about an entity pair without any human\nannotation on image instances. To address the problem, CLEVER leverages\nvision-language pre-training models for deep understanding of each image in the\nbag, and selects informative instances from the bag to summarize commonsense\nentity relations via a novel contrastive attention mechanism. Comprehensive\nexperimental results in held-out and human evaluation show that CLEVER can\nextract commonsense knowledge in promising quality, outperforming pre-trained\nlanguage model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted\ncommonsense scores show strong correlation with human judgment with a 0.78\nSpearman coefficient. Moreover, the extracted commonsense can also be grounded\ninto images with reasonable interpretability. The data and codes can be\nobtained at https://github.com/thunlp/CLEVER.", "published": "2022-11-22 07:00:16", "link": "http://arxiv.org/abs/2211.12054v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "OLGA : An Ontology and LSTM-based approach for generating Arithmetic\n  Word Problems (AWPs) of transfer type", "abstract": "Machine generation of Arithmetic Word Problems (AWPs) is challenging as they\nexpress quantities and mathematical relationships and need to be consistent.\nML-solvers require a large annotated training set of consistent problems with\nlanguage variations. Exploiting domain-knowledge is needed for consistency\nchecking whereas LSTM-based approaches are good for producing text with\nlanguage variations. Combining these we propose a system, OLGA, to generate\nconsistent word problems of TC (Transfer-Case) type, involving object transfers\namong agents. Though we provide a dataset of consistent 2-agent TC-problems for\ntraining, only about 36% of the outputs of an LSTM-based generator are found\nconsistent. We use an extension of TC-Ontology, proposed by us previously, to\ndetermine the consistency of problems. Among the remaining 64%, about 40% have\nminor errors which we repair using the same ontology. To check consistency and\nfor the repair process, we construct an instance-specific representation (ABox)\nof an auto-generated problem. We use a sentence classifier and BERT models for\nthis task. The training set for these LMs is problem-texts where sentence-parts\nare annotated with ontology class-names. As three-agent problems are longer,\nthe percentage of consistent problems generated by an LSTM-based approach drops\nfurther. Hence, we propose an ontology-based method that extends consistent\n2-agent problems into consistent 3-agent problems. Overall, our approach\ngenerates a large number of consistent TC-type AWPs involving 2 or 3 agents. As\nABox has all the information of a problem, any annotations can also be\ngenerated. Adopting the proposed approach to generate other types of AWPs is\ninteresting future work.", "published": "2022-11-22 10:42:07", "link": "http://arxiv.org/abs/2211.12164v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LO"], "primary_category": "cs.CL"}
{"title": "PromptTTS: Controllable Text-to-Speech with Text Descriptions", "abstract": "Using a text description as prompt to guide the generation of text or images\n(e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and\nimage generation, in this work, we explore the possibility of utilizing text\ndescriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS)\nsystem (dubbed as PromptTTS) that takes a prompt with both style and content\ndescriptions as input to synthesize the corresponding speech. Specifically,\nPromptTTS consists of a style encoder and a content encoder to extract the\ncorresponding representations from the prompt, and a speech decoder to\nsynthesize speech according to the extracted style and content representations.\nCompared with previous works in controllable TTS that require users to have\nacoustic knowledge to understand style factors such as prosody and pitch,\nPromptTTS is more user-friendly since text descriptions are a more natural way\nto express speech style (e.g., ''A lady whispers to her friend slowly''). Given\nthat there is no TTS dataset with prompts, to benchmark the task of PromptTTS,\nwe construct and release a dataset containing prompts with style and content\ninformation and the corresponding speech. Experiments show that PromptTTS can\ngenerate speech with precise style control and high speech quality. Audio\nsamples and our dataset are publicly available.", "published": "2022-11-22 10:58:38", "link": "http://arxiv.org/abs/2211.12171v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken\n  Language Understanding", "abstract": "Multi-Intent Spoken Language Understanding (SLU), a novel and more complex\nscenario of SLU, is attracting increasing attention. Unlike traditional SLU,\neach intent in this scenario has its specific scope. Semantic information\noutside the scope even hinders the prediction, which tremendously increases the\ndifficulty of intent detection. More seriously, guiding slot filling with these\ninaccurate intent labels suffers error propagation problems, resulting in\nunsatisfied overall performance. To solve these challenges, in this paper, we\npropose a novel Scope-Sensitive Result Attention Network (SSRAN) based on\nTransformer, which contains a Scope Recognizer (SR) and a Result Attention\nNetwork (RAN). Scope Recognizer assignments scope information to each token,\nreducing the distraction of out-of-scope tokens. Result Attention Network\neffectively utilizes the bidirectional interaction between results of slot\nfilling and intent detection, mitigating the error propagation problem.\nExperiments on two public datasets indicate that our model significantly\nimproves SLU performance (5.4\\% and 2.1\\% on Overall accuracy) over the\nstate-of-the-art baseline.", "published": "2022-11-22 12:24:22", "link": "http://arxiv.org/abs/2211.12220v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Relation-dependent Contrastive Learning with Cluster Sampling for\n  Inductive Relation Prediction", "abstract": "Relation prediction is a task designed for knowledge graph completion which\naims to predict missing relationships between entities. Recent subgraph-based\nmodels for inductive relation prediction have received increasing attention,\nwhich can predict relation for unseen entities based on the extracted subgraph\nsurrounding the candidate triplet. However, they are not completely inductive\nbecause of their disability of predicting unseen relations. Moreover, they fail\nto pay sufficient attention to the role of relation as they only depend on the\nmodel to learn parameterized relation embedding, which leads to inaccurate\nprediction on long-tail relations. In this paper, we introduce\nRelation-dependent Contrastive Learning (ReCoLe) for inductive relation\nprediction, which adapts contrastive learning with a novel sampling method\nbased on clustering algorithm to enhance the role of relation and improve the\ngeneralization ability to unseen relations. Instead of directly learning\nembedding for relations, ReCoLe allocates a pre-trained GNN-based encoder to\neach relation to strengthen the influence of relation. The GNN-based encoder is\noptimized by contrastive learning, which ensures satisfactory performance on\nlong-tail relations. In addition, the cluster sampling method equips ReCoLe\nwith the ability to handle both unseen relations and entities. Experimental\nresults suggest that ReCoLe outperforms state-of-the-art methods on commonly\nused inductive datasets.", "published": "2022-11-22 13:30:49", "link": "http://arxiv.org/abs/2211.12266v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Time-Aware Datasets are Adaptive Knowledgebases for the New Normal", "abstract": "Recent advances in text classification and knowledge capture in language\nmodels have relied on availability of large-scale text datasets. However,\nlanguage models are trained on static snapshots of knowledge and are limited\nwhen that knowledge evolves. This is especially critical for misinformation\ndetection, where new types of misinformation continuously appear, replacing old\ncampaigns. We propose time-aware misinformation datasets to capture\ntime-critical phenomena. In this paper, we first present evidence of evolving\nmisinformation and show that incorporating even simple time-awareness\nsignificantly improves classifier accuracy. Second, we present COVID-TAD, a\nlarge-scale COVID-19 misinformation da-taset spanning 25 months. It is the\nfirst large-scale misinformation dataset that contains multiple snapshots of a\ndatastream and is orders of magnitude bigger than related misinformation\ndatasets. We describe the collection and labeling pro-cess, as well as\npreliminary experiments.", "published": "2022-11-22 05:38:29", "link": "http://arxiv.org/abs/2211.12508v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "NLP meets psychotherapy: Using predicted client emotions and\n  self-reported client emotions to measure emotional coherence", "abstract": "Emotions are experienced and expressed through various response systems.\nCoherence between emotional experience and emotional expression is considered\nimportant to clients' well being. To date, emotional coherence (EC) has been\nstudied at a single time point using lab-based tasks with relatively small\ndatasets. No study has examined EC between the subjective experience of\nemotions and emotion expression in therapy or whether this coherence is\nassociated with clients' well being. Natural language Processing (NLP)\napproaches have been applied to identify emotions from psychotherapy dialogue,\nwhich can be implemented to study emotional processes on a larger scale.\nHowever, these methods have yet to be used to study coherence between emotional\nexperience and emotional expression over the course of therapy and whether it\nrelates to clients' well-being. This work presents an end-to-end approach where\nwe use emotion predictions from our transformer based emotion recognition model\nto study emotional coherence and its diagnostic potential in psychotherapy\nresearch. We first employ our transformer based approach on a Hebrew\npsychotherapy dataset to automatically label clients' emotions at utterance\nlevel in psychotherapy dialogues. We subsequently investigate the emotional\ncoherence between clients' self-reported emotional states and our model-based\nemotion predictions. We also examine the association between emotional\ncoherence and clients' well being. Our findings indicate a significant\ncorrelation between clients' self-reported emotions and positive and negative\nemotions expressed verbally during psychotherapy sessions. Coherence in\npositive emotions was also highly correlated with clients well-being. These\nresults illustrate how NLP can be applied to identify important emotional\nprocesses in psychotherapy to improve diagnosis and treatment for clients\nsuffering from mental-health problems.", "published": "2022-11-22 14:28:41", "link": "http://arxiv.org/abs/2211.12512v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Multimodal Language Modeling", "abstract": "Recent multimodal models such as DALL-E and CM3 have achieved remarkable\nprogress in text-to-image and image-to-text generation. However, these models\nstore all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\nmodel parameters, requiring increasingly larger models and training data to\ncapture more knowledge. To integrate knowledge in a more scalable and modular\nway, we propose a retrieval-augmented multimodal model, which enables a base\nmultimodal model (generator) to refer to relevant text and images fetched by a\nretriever from external memory (e.g., documents on the web). Specifically, for\nthe retriever, we use a pretrained CLIP, and for the generator, we train a CM3\nTransformer on the LAION dataset. Our resulting model, named\nRetrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can\nretrieve and generate both text and images. We show that RA-CM3 significantly\noutperforms baseline multimodal models such as DALL-E and CM3 on both image and\ncaption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while\nrequiring much less compute for training (<30% of DALL-E). Moreover, we show\nthat RA-CM3 exhibits novel capabilities, such as faithful image generation and\nmultimodal in-context learning (e.g., image generation from demonstrations).", "published": "2022-11-22 20:26:44", "link": "http://arxiv.org/abs/2211.12561v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Benchmarking Evaluation Metrics for Code-Switching Automatic Speech\n  Recognition", "abstract": "Code-switching poses a number of challenges and opportunities for\nmultilingual automatic speech recognition. In this paper, we focus on the\nquestion of robust and fair evaluation metrics. To that end, we develop a\nreference benchmark data set of code-switching speech recognition hypotheses\nwith human judgments. We define clear guidelines for minimal editing of\nautomatic hypotheses. We validate the guidelines using 4-way inter-annotator\nagreement. We evaluate a large number of metrics in terms of correlation with\nhuman judgments. The metrics we consider vary in terms of representation\n(orthographic, phonological, semantic), directness (intrinsic vs extrinsic),\ngranularity (e.g. word, character), and similarity computation method. The\nhighest correlation to human judgment is achieved using transliteration\nfollowed by text normalization. We release the first corpus for human\nacceptance of code-switching speech recognition results in dialectal\nArabic/English conversation speech.", "published": "2022-11-22 08:14:07", "link": "http://arxiv.org/abs/2211.16319v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting the Type and Target of Offensive Social Media Posts in\n  Marathi", "abstract": "The presence of offensive language on social media is very common motivating\nplatforms to invest in strategies to make communities safer. This includes\ndeveloping robust machine learning systems capable of recognizing offensive\ncontent online. Apart from a few notable exceptions, most research on automatic\noffensive language identification has dealt with English and a few other high\nresource languages such as French, German, and Spanish. In this paper we\naddress this gap by tackling offensive language identification in Marathi, a\nlow-resource Indo-Aryan language spoken in India. We introduce the Marathi\nOffensive Language Dataset v.2.0 or MOLD 2.0 and present multiple experiments\non this dataset. MOLD 2.0 is a much larger version of MOLD with expanded\nannotation to the levels B (type) and C (target) of the popular OLID taxonomy.\nMOLD 2.0 is the first hierarchical offensive language dataset compiled for\nMarathi, thus opening new avenues for research in low-resource Indo-Aryan\nlanguages. Finally, we also introduce SeMOLD, a larger dataset annotated\nfollowing the semi-supervised methods presented in SOLID.", "published": "2022-11-22 20:36:44", "link": "http://arxiv.org/abs/2211.12570v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "On Narrative Information and the Distillation of Stories", "abstract": "The act of telling stories is a fundamental part of what it means to be\nhuman. This work introduces the concept of narrative information, which we\ndefine to be the overlap in information space between a story and the items\nthat compose the story. Using contrastive learning methods, we show how modern\nartificial neural networks can be leveraged to distill stories and extract a\nrepresentation of the narrative information. We then demonstrate how\nevolutionary algorithms can leverage this to extract a set of narrative\ntemplates and how these templates -- in tandem with a novel curve-fitting\nalgorithm we introduce -- can reorder music albums to automatically induce\nstories in them. In the process of doing so, we give strong statistical\nevidence that these narrative information templates are present in existing\nalbums. While we experiment only with music albums here, the premises of our\nwork extend to any form of (largely) independent media.", "published": "2022-11-22 17:30:36", "link": "http://arxiv.org/abs/2211.12423v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "cs.NE", "cs.SD", "eess.AS", "68T07 (Primary) 68P30, 68W50, 94A15 (Secondary)", "H.1.1; H.5.5; I.2.6; I.5.1; J.5"], "primary_category": "cs.CL"}
{"title": "Dynamic Acoustic Compensation and Adaptive Focal Training for\n  Personalized Speech Enhancement", "abstract": "Recently, more and more personalized speech enhancement systems (PSE) with\nexcellent performance have been proposed. However, two critical issues still\nlimit the performance and generalization ability of the model: 1) Acoustic\nenvironment mismatch between the test noisy speech and target speaker\nenrollment speech; 2) Hard sample mining and learning. In this paper, dynamic\nacoustic compensation (DAC) is proposed to alleviate the environment mismatch,\nby intercepting the noise or environmental acoustic segments from noisy speech\nand mixing it with the clean enrollment speech. To well exploit the hard\nsamples in training data, we propose an adaptive focal training (AFT) strategy\nby assigning adaptive loss weights to hard and non-hard samples during\ntraining. A time-frequency multi-loss training is further introduced to improve\nand generalize our previous work sDPCCN for PSE. The effectiveness of proposed\nmethods are examined on the DNS4 Challenge dataset. Results show that, the DAC\nbrings large improvements in terms of multiple evaluation metrics, and AFT\nreduces the hard sample rate significantly and produces obvious MOS score\nimprovement.", "published": "2022-11-22 08:58:23", "link": "http://arxiv.org/abs/2211.12097v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech\n  Enhancement from Beam-Space Dictionary Perspective", "abstract": "Despite the promising performance of existing frame-wise all-neural\nbeamformers in the speech enhancement field, it remains unclear what the\nunderlying mechanism exists. In this paper, we revisit the beamforming behavior\nfrom the beam-space dictionary perspective and formulate it into the learning\nand mixing of different beam-space components. Based on that, we propose an\nall-neural beamformer called TaylorBM to simulate Taylor's series expansion\noperation in which the 0th-order term serves as a spatial filter to conduct the\nbeam mixing, and several high-order terms are tasked with residual noise\ncancellation for post-processing. The whole system is devised to work in an\nend-to-end manner. Experiments are conducted on the spatialized LibriSpeech\ncorpus and results show that the proposed approach outperforms existing\nadvanced baselines in terms of evaluation metrics.", "published": "2022-11-22 05:39:02", "link": "http://arxiv.org/abs/2211.12024v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust Training for Speaker Verification against Noisy Labels", "abstract": "The deep learning models used for speaker verification rely heavily on large\namounts of data and correct labeling. However, noisy (incorrect) labels often\noccur, which degrades the performance of the system. In this paper, we propose\na novel two-stage learning method to filter out noisy labels from speaker\ndatasets. Since a DNN will first fit data with clean labels, we first train the\nmodel with all data for several epochs. Then, based on this model, the model\npredictions are compared with the labels using our proposed the OR-Gate with\ntop-k mechanism to select the data with clean labels and the selected data is\nused to train the model. This process is iterated until the training is\ncompleted. We have demonstrated the effectiveness of this method in filtering\nnoisy labels through extensive experiments and have achieved excellent\nperformance on the VoxCeleb (1 and 2) with different added noise rates.", "published": "2022-11-22 08:23:33", "link": "http://arxiv.org/abs/2211.12080v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech\n  Separation", "abstract": "We propose TF-GridNet for speech separation. The model is a novel deep neural\nnetwork (DNN) integrating full- and sub-band modeling in the time-frequency\n(T-F) domain. It stacks several blocks, each consisting of an intra-frame\nfull-band module, a sub-band temporal module, and a cross-frame self-attention\nmodule. It is trained to perform complex spectral mapping, where the real and\nimaginary (RI) components of input signals are stacked as features to predict\ntarget RI components. We first evaluate it on monaural anechoic speaker\nseparation. Without using data augmentation and dynamic mixing, it obtains a\nstate-of-the-art 23.5 dB improvement in scale-invariant signal-to-distortion\nratio (SI-SDR) on WSJ0-2mix, a standard dataset for two-speaker separation. To\nshow its robustness to noise and reverberation, we evaluate it on monaural\nreverberant speaker separation using the SMS-WSJ dataset and on\nnoisy-reverberant speaker separation using WHAMR!, and obtain state-of-the-art\nperformance on both datasets. We then extend TF-GridNet to multi-microphone\nconditions through multi-microphone complex spectral mapping, and integrate it\ninto a two-DNN system with a beamformer in between (named as MISO-BF-MISO in\nearlier studies), where the beamformer proposed in this paper is a novel\nmulti-frame Wiener filter computed based on the outputs of the first DNN.\nState-of-the-art performance is obtained on the multi-channel tasks of SMS-WSJ\nand WHAMR!. Besides speaker separation, we apply the proposed algorithms to\nspeech dereverberation and noisy-reverberant speech enhancement.\nState-of-the-art performance is obtained on a dereverberation dataset and on\nthe dataset of the recent L3DAS22 multi-channel speech enhancement challenge.", "published": "2022-11-22 17:43:12", "link": "http://arxiv.org/abs/2211.12433v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Latent Iterative Refinement for Modular Source Separation", "abstract": "Traditional source separation approaches train deep neural network models\nend-to-end with all the data available at once by minimizing the empirical risk\non the whole training set. On the inference side, after training the model, the\nuser fetches a static computation graph and runs the full model on some\nspecified observed mixture signal to get the estimated source signals.\nAdditionally, many of those models consist of several basic processing blocks\nwhich are applied sequentially. We argue that we can significantly increase\nresource efficiency during both training and inference stages by reformulating\na model's training and inference procedures as iterative mappings of latent\nsignal representations. First, we can apply the same processing block more than\nonce on its output to refine the input signal and consequently improve\nparameter efficiency. During training, we can follow a block-wise procedure\nwhich enables a reduction on memory requirements. Thus, one can train a very\ncomplicated network structure using significantly less computation compared to\nend-to-end training. During inference, we can dynamically adjust how many\nprocessing blocks and iterations of a specific block an input signal needs\nusing a gating module.", "published": "2022-11-22 00:02:57", "link": "http://arxiv.org/abs/2211.11917v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "COVID-Net Assistant: A Deep Learning-Driven Virtual Assistant for\n  COVID-19 Symptom Prediction and Recommendation", "abstract": "As the COVID-19 pandemic continues to put a significant burden on healthcare\nsystems worldwide, there has been growing interest in finding inexpensive\nsymptom pre-screening and recommendation methods to assist in efficiently using\navailable medical resources such as PCR tests. In this study, we introduce the\ndesign of COVID-Net Assistant, an efficient virtual assistant designed to\nprovide symptom prediction and recommendations for COVID-19 by analyzing users'\ncough recordings through deep convolutional neural networks. We explore a\nvariety of highly customized, lightweight convolutional neural network\narchitectures generated via machine-driven design exploration (which we refer\nto as COVID-Net Assistant neural networks) on the Covid19-Cough benchmark\ndataset. The Covid19-Cough dataset comprises 682 cough recordings from a\nCOVID-19 positive cohort and 642 from a COVID-19 negative cohort. Among the 682\ncough recordings labeled positive, 382 recordings were verified by PCR test.\nOur experimental results show promising, with the COVID-Net Assistant neural\nnetworks demonstrating robust predictive performance, achieving AUC scores of\nover 0.93, with the best score over 0.95 while being fast and efficient in\ninference. The COVID-Net Assistant models are made available in an open source\nmanner through the COVID-Net open initiative and, while not a production-ready\nsolution, we hope their availability acts as a good resource for clinical\nscientists, machine learning researchers, as well as citizen scientists to\ndevelop innovative solutions.", "published": "2022-11-22 01:41:48", "link": "http://arxiv.org/abs/2211.11944v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Disentangled Feature Learning for Real-Time Neural Speech Coding", "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential\nto outperform traditional signal analysis based audio codecs. This is mostly\nachieved by following the VQ-VAE paradigm where blind features are learned,\nvector-quantized and coded. In this paper, instead of blind end-to-end\nlearning, we propose to learn disentangled features for real-time neural speech\ncoding. Specifically, more global-like speaker identity and local content\nfeatures are learned with disentanglement to represent speech. Such a compact\nfeature decomposition not only achieves better coding efficiency by exploiting\nbit allocation among different features but also provides the flexibility to do\naudio editing in embedding space, such as voice conversion in real-time\ncommunications. Both subjective and objective results demonstrate its coding\nefficiency and we find that the learned disentangled features show comparable\nperformance on any-to-any voice conversion with modern self-supervised speech\nrepresentation learning models with far less parameters and low latency,\nshowing the potential of our neural coding framework.", "published": "2022-11-22 02:50:12", "link": "http://arxiv.org/abs/2211.11960v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AERO: Audio Super Resolution in the Spectral Domain", "abstract": "We present AERO, a audio super-resolution model that processes speech and\nmusic signals in the spectral domain. AERO is based on an encoder-decoder\narchitecture with U-Net like skip connections. We optimize the model using both\ntime and frequency domain loss functions. Specifically, we consider a set of\nreconstruction losses together with perceptual ones in the form of adversarial\nand feature discriminator loss functions. To better handle phase information\nthe proposed method operates over the complex-valued spectrogram using two\nseparate channels. Unlike prior work which mainly considers low and high\nfrequency concatenation for audio super-resolution, the proposed method\ndirectly predicts the full frequency range. We demonstrate high performance\nacross a wide range of sample rates considering both speech and music. AERO\noutperforms the evaluated baselines considering Log-Spectral Distance, ViSQOL,\nand the subjective MUSHRA test. Audio samples and code are available at\nhttps://pages.cs.huji.ac.il/adiyoss-lab/aero", "published": "2022-11-22 12:37:01", "link": "http://arxiv.org/abs/2211.12232v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Neural Mel-Subband Beamformer for In-car Speech Separation", "abstract": "While current deep learning (DL)-based beamforming techniques have been\nproved effective in speech separation, they are often designed to process\nnarrow-band (NB) frequencies independently which results in higher\ncomputational costs and inference times, making them unsuitable for real-world\nuse. In this paper, we propose DL-based mel-subband spatio-temporal beamformer\nto perform speech separation in a car environment with reduced computation cost\nand inference time. As opposed to conventional subband (SB) approaches, our\nframework uses a mel-scale based subband selection strategy which ensures a\nfine-grained processing for lower frequencies where most speech formant\nstructure is present, and coarse-grained processing for higher frequencies. In\na recursive way, robust frame-level beamforming weights are determined for each\nspeaker location/zone in a car from the estimated subband speech and noise\ncovariance matrices. Furthermore, proposed framework also estimates and\nsuppresses any echoes from the loudspeaker(s) by using the echo reference\nsignals. We compare the performance of our proposed framework to several NB,\nSB, and full-band (FB) processing techniques in terms of speech quality and\nrecognition metrics. Based on experimental evaluations on simulated and\nreal-world recordings, we find that our proposed framework achieves better\nseparation performance over all SB and FB approaches and achieves performance\ncloser to NB processing techniques while requiring lower computing cost.", "published": "2022-11-22 21:11:26", "link": "http://arxiv.org/abs/2211.12590v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SkipConvGAN: Monaural Speech Dereverberation using Generative\n  Adversarial Networks via Complex Time-Frequency Masking", "abstract": "With the advancements in deep learning approaches, the performance of speech\nenhancing systems in the presence of background noise have shown significant\nimprovements. However, improving the system's robustness against reverberation\nis still a work in progress, as reverberation tends to cause loss of formant\nstructure due to smearing effects in time and frequency. A wide range of deep\nlearning-based systems either enhance the magnitude response and reuse the\ndistorted phase or enhance complex spectrogram using a complex time-frequency\nmask. Though these approaches have demonstrated satisfactory performance, they\ndo not directly address the lost formant structure caused by reverberation. We\nbelieve that retrieving the formant structure can help improve the efficiency\nof existing systems. In this study, we propose SkipConvGAN - an extension of\nour prior work SkipConvNet. The proposed system's generator network tries to\nestimate an efficient complex time-frequency mask, while the discriminator\nnetwork aids in driving the generator to restore the lost formant structure. We\nevaluate the performance of our proposed system on simulated and real\nrecordings of reverberant speech from the single-channel task of the REVERB\nchallenge corpus. The proposed system shows a consistent improvement across\nmultiple room configurations over other deep learning-based generative\nadversarial frameworks.", "published": "2022-11-22 23:02:49", "link": "http://arxiv.org/abs/2211.12623v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Complex-Valued Time-Frequency Self-Attention for Speech Dereverberation", "abstract": "Several speech processing systems have demonstrated considerable performance\nimprovements when deep complex neural networks (DCNN) are coupled with\nself-attention (SA) networks. However, the majority of DCNN-based studies on\nspeech dereverberation that employ self-attention do not explicitly account for\nthe inter-dependencies between real and imaginary features when computing\nattention. In this study, we propose a complex-valued T-F attention (TFA)\nmodule that models spectral and temporal dependencies by computing\ntwo-dimensional attention maps across time and frequency dimensions. We\nvalidate the effectiveness of our proposed complex-valued TFA module with the\ndeep complex convolutional recurrent network (DCCRN) using the REVERB challenge\ncorpus. Experimental findings indicate that integrating our complex-TFA module\nwith DCCRN improves overall speech quality and performance of back-end speech\napplications, such as automatic speech recognition, compared to earlier\napproaches for self-attention.", "published": "2022-11-22 23:38:10", "link": "http://arxiv.org/abs/2211.12632v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Ontology-aware Learning and Evaluation for Audio Tagging", "abstract": "This study defines a new evaluation metric for audio tagging tasks to\novercome the limitation of the conventional mean average precision (mAP)\nmetric, which treats different kinds of sound as independent classes without\nconsidering their relations. Also, due to the ambiguities in sound labeling,\nthe labels in the training and evaluation set are not guaranteed to be accurate\nand exhaustive, which poses challenges for robust evaluation with mAP. The\nproposed metric, ontology-aware mean average precision (OmAP) addresses the\nweaknesses of mAP by utilizing the AudioSet ontology information during the\nevaluation. Specifically, we reweight the false positive events in the model\nprediction based on the ontology graph distance to the target classes. The OmAP\nmeasure also provides more insights into model performance by evaluations with\ndifferent coarse-grained levels in the ontology graph. We conduct human\nevaluations and demonstrate that OmAP is more consistent with human perception\nthan mAP. To further verify the importance of utilizing the ontology\ninformation, we also propose a novel loss function (OBCE) that reweights binary\ncross entropy (BCE) loss based on the ontology distance. Our experiment shows\nthat OBCE can improve both mAP and OmAP metrics on the AudioSet tagging task.", "published": "2022-11-22 11:35:14", "link": "http://arxiv.org/abs/2211.12195v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
