{"title": "Exploring Strategies for Generalizable Commonsense Reasoning with\n  Pre-trained Models", "abstract": "Commonsense reasoning benchmarks have been largely solved by fine-tuning\nlanguage models. The downside is that fine-tuning may cause models to overfit\nto task-specific data and thereby forget their knowledge gained during\npre-training. Recent works only propose lightweight model updates as models may\nalready possess useful knowledge from past experience, but a challenge remains\nin understanding what parts and to what extent models should be refined for a\ngiven task. In this paper, we investigate what models learn from commonsense\nreasoning datasets. We measure the impact of three different adaptation methods\non the generalization and accuracy of models. Our experiments with two models\nshow that fine-tuning performs best, by learning both the content and the\nstructure of the task, but suffers from overfitting and limited generalization\nto novel answers. We observe that alternative adaptation methods like\nprefix-tuning have comparable accuracy, but generalize better to unseen answers\nand are more robust to adversarial splits.", "published": "2021-09-07 03:13:06", "link": "http://arxiv.org/abs/2109.02837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Datasets: A Community Library for Natural Language Processing", "abstract": "The scale, variety, and quantity of publicly-available NLP datasets has grown\nrapidly as researchers propose new tasks, larger models, and novel benchmarks.\nDatasets is a community library for contemporary NLP designed to support this\necosystem. Datasets aims to standardize end-user interfaces, versioning, and\ndocumentation, while providing a lightweight front-end that behaves similarly\nfor small datasets as for internet-scale corpora. The design of the library\nincorporates a distributed, community-driven approach to adding datasets and\ndocumenting usage. After a year of development, the library now includes more\nthan 650 unique datasets, has more than 250 contributors, and has helped\nsupport a variety of novel cross-dataset research projects and shared tasks.\nThe library is available at https://github.com/huggingface/datasets.", "published": "2021-09-07 03:59:22", "link": "http://arxiv.org/abs/2109.02846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Regular Expressions with Neural Networks via DFA", "abstract": "Human-designed rules are widely used to build industry applications. However,\nit is infeasible to maintain thousands of such hand-crafted rules. So it is\nvery important to integrate the rule knowledge into neural networks to build a\nhybrid model that achieves better performance. Specifically, the human-designed\nrules are formulated as Regular Expressions (REs), from which the equivalent\nMinimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to\nuse the MDFA as an intermediate model to capture the matched RE patterns as\nrule-based features for each input sentence and introduce these additional\nfeatures into neural networks. We evaluate the proposed method on the ATIS\nintent classification task. The experiment results show that the proposed\nmethod achieves the best performance compared to neural networks and four other\nmethods that combine REs and neural networks when the training dataset is\nrelatively small.", "published": "2021-09-07 05:48:51", "link": "http://arxiv.org/abs/2109.02882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Reasoning Chains for Multi-hop Science Question Answering", "abstract": "We propose a novel Chain Guided Retriever-reader ({\\tt CGR}) framework to\nmodel the reasoning chain for multi-hop Science Question Answering. Our\nframework is capable of performing explainable reasoning without the need of\nany corpus-specific annotations, such as the ground-truth reasoning chain, or\nhuman-annotated entity mentions. Specifically, we first generate reasoning\nchains from a semantic graph constructed by Abstract Meaning Representation of\nretrieved evidence facts. A \\textit{Chain-aware loss}, concerning both local\nand global chain information, is also designed to enable the generated chains\nto serve as distant supervision signals for training the retriever, where\nreinforcement learning is also adopted to maximize the utility of the reasoning\nchains. Our framework allows the retriever to capture step-by-step clues of the\nentire reasoning process, which is not only shown to be effective on two\nchallenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge,\nbut also favors explainability.", "published": "2021-09-07 07:22:07", "link": "http://arxiv.org/abs/2109.02905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Driven Content Creation using Statistical and Natural Language\n  Processing Techniques for Financial Domain", "abstract": "Over the years customers' expectation of getting information instantaneously\nhas given rise to the increased usage of channels like virtual assistants.\nTypically, customers try to get their questions answered by low-touch channels\nlike search and virtual assistant first, before getting in touch with a live\nchat agent or the phone representative. Higher usage of these low-touch systems\nis a win-win for both customers and the organization since it enables\norganizations to attain a low cost of service while customers get served\nwithout delay. In this paper, we propose a two-part framework where the first\npart describes methods to combine the information from different interaction\nchannels like call, search, and chat. We do this by summarizing (using a\nstacked Bi-LSTM network) the high-touch interaction channel data such as call\nand chat into short searchquery like customer intents and then creating an\norganically grown intent taxonomy from interaction data (using Hierarchical\nAgglomerative Clustering). The second part of the framework focuses on\nextracting customer questions by analyzing interaction data sources. It\ncalculates similarity scores using TF-IDF and BERT(Devlin et al., 2019). It\nalso maps these identified questions to the output of the first part of the\nframework using syntactic and semantic similarity.", "published": "2021-09-07 08:37:28", "link": "http://arxiv.org/abs/2109.02935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrase Generation as Unsupervised Machine Translation", "abstract": "In this paper, we propose a new paradigm for paraphrase generation by\ntreating the task as unsupervised machine translation (UMT) based on the\nassumption that there must be pairs of sentences expressing the same meaning in\na large-scale unlabeled monolingual corpus. The proposed paradigm first splits\na large unlabeled corpus into multiple clusters, and trains multiple UMT models\nusing pairs of these clusters. Then based on the paraphrase pairs produced by\nthese UMT models, a unified surrogate model can be trained to serve as the\nfinal \\sts model to generate paraphrases, which can be directly used for test\nin the unsupervised setup, or be finetuned on labeled datasets in the\nsupervised setup. The proposed method offers merits over\nmachine-translation-based paraphrase generation methods, as it avoids reliance\non bilingual sentence pairs. It also allows human intervene with the model so\nthat more diverse paraphrases can be generated using different filtering\ncriteria. Extensive experiments on existing paraphrase dataset for both the\nsupervised and unsupervised setups demonstrate the effectiveness the proposed\nparadigm.", "published": "2021-09-07 09:08:58", "link": "http://arxiv.org/abs/2109.02950v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to\n  Identify Toxic, Engaging, & Fact-Claiming Comments", "abstract": "In this paper we describe the methods we used for our submissions to the\nGermEval 2021 shared task on the identification of toxic, engaging, and\nfact-claiming comments. For all three subtasks we fine-tuned freely available\ntransformer-based models from the Huggingface model hub. We evaluated the\nperformance of various pre-trained models after fine-tuning on 80% of the\ntraining data with different hyperparameters and submitted predictions of the\ntwo best performing resulting models. We found that this approach worked best\nfor subtask 3, for which we achieved an F1-score of 0.736.", "published": "2021-09-07 09:46:27", "link": "http://arxiv.org/abs/2109.02966v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Go Far Off: An Empirical Study on Neural Poetry Translation", "abstract": "Despite constant improvements in machine translation quality, automatic\npoetry translation remains a challenging problem due to the lack of\nopen-sourced parallel poetic corpora, and to the intrinsic complexities\ninvolved in preserving the semantics, style, and figurative nature of poetry.\nWe present an empirical investigation for poetry translation along several\ndimensions: 1) size and style of training data (poetic vs. non-poetic),\nincluding a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)\nlanguage-family-specific models vs. mixed-multilingual models. To accomplish\nthis, we contribute a parallel dataset of poetry translations for several\nlanguage pairs. Our results show that multilingual fine-tuning on poetic text\nsignificantly outperforms multilingual fine-tuning on non-poetic text that is\n35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and\nhuman evaluation metrics such as faithfulness (meaning and poetic style).\nMoreover, multilingual fine-tuning on poetic data outperforms \\emph{bilingual}\nfine-tuning on poetic data.", "published": "2021-09-07 10:00:44", "link": "http://arxiv.org/abs/2109.02972v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Context Choices for Context-aware Machine Translation", "abstract": "One of the most popular methods for context-aware machine translation (MT) is\nto use separate encoders for the source sentence and context as multiple\nsources for one target sentence. Recent work has cast doubt on whether these\nmodels actually learn useful signals from the context or are improvements in\nautomatic evaluation metrics just a side-effect. We show that multi-source\ntransformer models improve MT over standard transformer-base models even with\nempty lines provided as context, but the translation quality improves\nsignificantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is\nprovided. We also show that even though randomly shuffling in-domain context\ncan also improve over baselines, the correct context further improves\ntranslation quality and random out-of-domain context further degrades it.", "published": "2021-09-07 11:03:34", "link": "http://arxiv.org/abs/2109.02995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Patient Outcome and Zero-shot Diagnosis Prediction with\n  Hypernetwork-guided Multitask Learning", "abstract": "Multitask deep learning has been applied to patient outcome prediction from\ntext, taking clinical notes as input and training deep neural networks with a\njoint loss function of multiple tasks. However, the joint training scheme of\nmultitask learning suffers from inter-task interference, and diagnosis\nprediction among the multiple tasks has the generalizability issue due to rare\ndiseases or unseen diagnoses. To solve these challenges, we propose a\nhypernetwork-based approach that generates task-conditioned parameters and\ncoefficients of multitask prediction heads to learn task-specific prediction\nand balance the multitask learning. We also incorporate semantic task\ninformation to improves the generalizability of our task-conditioned multitask\nmodel. Experiments on early and discharge notes extracted from the real-world\nMIMIC database show our method can achieve better performance on multitask\npatient outcome prediction than strong baselines in most cases. Besides, our\nmethod can effectively handle the scenario with limited information and improve\nzero-shot prediction on unseen diagnosis categories.", "published": "2021-09-07 12:52:26", "link": "http://arxiv.org/abs/2109.03062v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GOLD: Improving Out-of-Scope Detection in Dialogues using Data\n  Augmentation", "abstract": "Practical dialogue systems require robust methods of detecting out-of-scope\n(OOS) utterances to avoid conversational breakdowns and related failure modes.\nDirectly training a model with labeled OOS examples yields reasonable\nperformance, but obtaining such data is a resource-intensive process. To tackle\nthis limited-data problem, previous methods focus on better modeling the\ndistribution of in-scope (INS) examples. We introduce GOLD as an orthogonal\ntechnique that augments existing data to train better OOS detectors operating\nin low-data regimes. GOLD generates pseudo-labeled candidates using samples\nfrom an auxiliary dataset and keeps only the most beneficial candidates for\ntraining through a novel filtering mechanism. In experiments across three\ntarget benchmarks, the top GOLD model outperforms all existing methods on all\nkey metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median\nbaseline performance. We also analyze the unique properties of OOS data to\nidentify key factors for optimally applying our proposed method.", "published": "2021-09-07 13:35:03", "link": "http://arxiv.org/abs/2109.03079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FHAC at GermEval 2021: Identifying German toxic, engaging, and\n  fact-claiming comments with ensemble learning", "abstract": "The availability of language representations learned by large pretrained\nneural network models (such as BERT and ELECTRA) has led to improvements in\nmany downstream Natural Language Processing tasks in recent years. Pretrained\nmodels usually differ in pretraining objectives, architectures, and datasets\nthey are trained on which can affect downstream performance. In this\ncontribution, we fine-tuned German BERT and German ELECTRA models to identify\ntoxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)\nin Facebook data provided by the GermEval 2021 competition. We created\nensembles of these models and investigated whether and how classification\nperformance depends on the number of ensemble members and their composition. On\nout-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for\nall subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,\nrespectively.", "published": "2021-09-07 13:52:39", "link": "http://arxiv.org/abs/2109.03094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Infusing Future Information into Monotonic Attention Through Language\n  Models", "abstract": "Simultaneous neural machine translation(SNMT) models start emitting the\ntarget sequence before they have processed the source sequence. The recent\nadaptive policies for SNMT use monotonic attention to perform read/write\ndecisions based on the partial source and target sequences. The lack of\nsufficient information might cause the monotonic attention to take poor\nread/write decisions, which in turn negatively affects the performance of the\nSNMT model. On the other hand, human translators make better read/write\ndecisions since they can anticipate the immediate future words using linguistic\ninformation and domain knowledge.Motivated by human translators, in this work,\nwe propose a framework to aid monotonic attention with an external language\nmodel to improve its decisions.We conduct experiments on the MuST-C\nEnglish-German and English-French speech-to-text translation tasks to show the\neffectiveness of the proposed framework.The proposed SNMT method improves the\nquality-latency trade-off over the state-of-the-art monotonic multihead\nattention.", "published": "2021-09-07 14:32:36", "link": "http://arxiv.org/abs/2109.03121v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via\n  Adaptive Gradient Gating for Rare Token Embeddings", "abstract": "Recent studies have determined that the learned token embeddings of\nlarge-scale neural language models are degenerated to be anisotropic with a\nnarrow-cone shape. This phenomenon, called the representation degeneration\nproblem, facilitates an increase in the overall similarity between token\nembeddings that negatively affect the performance of the models. Although the\nexisting methods that address the degeneration problem based on observations of\nthe phenomenon triggered by the problem improves the performance of the text\ngeneration, the training dynamics of token embeddings behind the degeneration\nproblem are still not explored. In this study, we analyze the training dynamics\nof the token embeddings focusing on rare token embedding. We demonstrate that\nthe specific part of the gradient for rare token embeddings is the key cause of\nthe degeneration problem for all tokens during training stage. Based on the\nanalysis, we propose a novel method called, adaptive gradient gating (AGG). AGG\naddresses the degeneration problem by gating the specific part of the gradient\nfor rare token embeddings. Experimental results from language modeling, word\nsimilarity, and machine translation tasks quantitatively and qualitatively\nverify the effectiveness of AGG.", "published": "2021-09-07 14:48:12", "link": "http://arxiv.org/abs/2109.03127v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers\n  Reveals Distinctive yet Consistent Individual Styles", "abstract": "An individual's variation in writing style is often a function of both social\nand personal attributes. While structured social variation has been extensively\nstudied, e.g., gender based variation, far less is known about how to\ncharacterize individual styles due to their idiosyncratic nature. We introduce\na new approach to studying idiolects through a massive cross-author comparison\nto identify and encode stylistic features. The neural model achieves strong\nperformance at authorship identification on short texts and through an\nanalogy-based probing task, showing that the learned representations exhibit\nsurprising regularities that encode qualitative and quantitative shifts of\nidiolectal styles. Through text perturbation, we quantify the relative\ncontributions of different linguistic elements to idiolectal variation.\nFurthermore, we provide a description of idiolects through measuring inter- and\nintra-author variation, showing that variation in idiolects is often\ndistinctive yet consistent.", "published": "2021-09-07 15:49:23", "link": "http://arxiv.org/abs/2109.03158v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How much pretraining data do language models need to learn syntax?", "abstract": "Transformers-based pretrained language models achieve outstanding results in\nmany well-known NLU benchmarks. However, while pretraining methods are very\nconvenient, they are expensive in terms of time and resources. This calls for a\nstudy of the impact of pretraining data size on the knowledge of the models. We\nexplore this impact on the syntactic capabilities of RoBERTa, using models\ntrained on incremental sizes of raw text data. First, we use syntactic\nstructural probes to determine whether models pretrained on more data encode a\nhigher amount of syntactic information. Second, we perform a targeted syntactic\nevaluation to analyze the impact of pretraining data size on the syntactic\ngeneralization performance of the models. Third, we compare the performance of\nthe different models on three downstream applications: part-of-speech tagging,\ndependency parsing and paraphrase identification. We complement our study with\nan analysis of the cost-benefit trade-off of training such models. Our\nexperiments show that while models pretrained on more data encode more\nsyntactic knowledge and perform better on downstream applications, they do not\nalways offer a better performance across the different syntactic phenomena and\ncome at a higher financial and environmental cost.", "published": "2021-09-07 15:51:39", "link": "http://arxiv.org/abs/2109.03160v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-Controllable Opinion Summarization", "abstract": "Recent work on opinion summarization produces general summaries based on a\nset of input reviews and the popularity of opinions expressed in them. In this\npaper, we propose an approach that allows the generation of customized\nsummaries based on aspect queries (e.g., describing the location and room of a\nhotel). Using a review corpus, we create a synthetic training dataset of\n(review, summary) pairs enriched with aspect controllers which are induced by a\nmulti-instance learning model that predicts the aspects of a document at\ndifferent levels of granularity. We fine-tune a pretrained model using our\nsynthetic dataset and generate aspect-specific summaries by modifying the\naspect controllers. Experiments on two benchmarks show that our model\noutperforms the previous state of the art and generates personalized summaries\nby controlling the number of aspects discussed in them.", "published": "2021-09-07 16:09:17", "link": "http://arxiv.org/abs/2109.03171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Conversation Disentanglement through Co-Training", "abstract": "Conversation disentanglement aims to separate intermingled messages into\ndetached sessions, which is a fundamental task in understanding multi-party\nconversations. Existing work on conversation disentanglement relies heavily\nupon human-annotated datasets, which are expensive to obtain in practice. In\nthis work, we explore to train a conversation disentanglement model without\nreferencing any human annotations. Our method is built upon a deep co-training\nalgorithm, which consists of two neural networks: a message-pair classifier and\na session classifier. The former is responsible for retrieving local relations\nbetween two messages while the latter categorizes a message to a session by\ncapturing context-aware information. Both networks are initialized respectively\nwith pseudo data built from an unannotated corpus. During the deep co-training\nprocess, we use the session classifier as a reinforcement learning component to\nlearn a session assigning policy by maximizing the local rewards given by the\nmessage-pair classifier. For the message-pair classifier, we enrich its\ntraining data by retrieving message pairs with high confidence from the\ndisentangled sessions predicted by the session classifier. Experimental results\non the large Movie Dialogue Dataset demonstrate that our proposed approach\nachieves competitive performance compared to the previous supervised methods.\nFurther experiments show that the predicted disentangled conversations can\npromote the performance on the downstream task of multi-party response\nselection.", "published": "2021-09-07 17:05:18", "link": "http://arxiv.org/abs/2109.03199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint model for intent and entity recognition", "abstract": "The semantic understanding of natural dialogues composes of several parts.\nSome of them, like intent classification and entity detection, have a crucial\nrole in deciding the next steps in handling user input. Handling each task as\nan individual problem can be wasting of training resources, and also each\nproblem can benefit from each other. This paper tackles these problems as one.\nOur new model, which combine intent and entity recognition into one system, is\nachieving better metrics in both tasks with lower training requirements than\nsolving each task separately. We also optimize the model based on the inputs.", "published": "2021-09-07 17:50:11", "link": "http://arxiv.org/abs/2109.03221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hi, my name is Martha: Using names to measure and mitigate bias in\n  generative dialogue models", "abstract": "All AI models are susceptible to learning biases in data that they are\ntrained on. For generative dialogue models, being trained on real human\nconversations containing unbalanced gender and race/ethnicity references can\nlead to models that display learned biases, which we define here broadly as any\nmeasurable differences in the distributions of words or semantic content of\nconversations based on demographic groups. We measure the strength of such\nbiases by producing artificial conversations between two copies of a dialogue\nmodel, conditioning one conversational partner to state a name commonly\nassociated with a certain gender and/or race/ethnicity. We find that larger\ncapacity models tend to exhibit more gender bias and greater stereotyping of\noccupations by gender. We show that several methods of tuning these dialogue\nmodels, specifically name scrambling, controlled generation, and unlikelihood\ntraining, are effective in reducing bias in conversation, including on a\ndownstream conversational task. Name scrambling is also effective in lowering\ndifferences in token usage across conversations where partners have names\nassociated with different genders or races/ethnicities.", "published": "2021-09-07 19:20:24", "link": "http://arxiv.org/abs/2109.03300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Corpus-based Open-Domain Event Type Induction", "abstract": "Traditional event extraction methods require predefined event types and their\ncorresponding annotations to learn event extractors. These prerequisites are\noften hard to be satisfied in real-world applications. This work presents a\ncorpus-based open-domain event type induction method that automatically\ndiscovers a set of event types from a given corpus. As events of the same type\ncould be expressed in multiple ways, we propose to represent each event type as\na cluster of <predicate sense, object head> pairs. Specifically, our method (1)\nselects salient predicates and object heads, (2) disambiguates predicate senses\nusing only a verb sense dictionary, and (3) obtains event types by jointly\nembedding and clustering <predicate sense, object head> pairs in a latent\nspherical space. Our experiments, on three datasets from different domains,\nshow our method can discover salient and high-quality event types, according to\nboth automatic and human evaluations.", "published": "2021-09-07 20:42:44", "link": "http://arxiv.org/abs/2109.03322v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory-Based Semantic Parsing", "abstract": "We present a memory-based model for context-dependent semantic parsing.\nPrevious approaches focus on enabling the decoder to copy or modify the parse\nfrom the previous utterance, assuming there is a dependency between the current\nand previous parses. In this work, we propose to represent contextual\ninformation using an external memory. We learn a context memory controller that\nmanages the memory by maintaining the cumulative meaning of sequential user\nutterances. We evaluate our approach on three semantic parsing benchmarks.\nExperimental results show that our model can better process context-dependent\ninformation and demonstrates improved performance without using task-specific\ndecoders.", "published": "2021-09-07 16:15:13", "link": "http://arxiv.org/abs/2110.07358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixed Attention Transformer for Leveraging Word-Level Knowledge to\n  Neural Cross-Lingual Information Retrieval", "abstract": "Pretrained contextualized representations offer great success for many\ndownstream tasks, including document ranking. The multilingual versions of such\npretrained representations provide a possibility of jointly learning many\nlanguages with the same model. Although it is expected to gain big with such\njoint training, in the case of cross lingual information retrieval (CLIR), the\nmodels under a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize that the\nperformance drop is due to the translation gap between query and documents. In\nthe monolingual retrieval task, because of the same lexical inputs, it is\neasier for model to identify the query terms that occurred in documents.\nHowever, in the multilingual pretrained models that the words in different\nlanguages are projected into the same hyperspace, the model tends to translate\nquery terms into related terms, i.e., terms that appear in a similar context,\nin addition to or sometimes rather than synonyms in the target language. This\nproperty is creating difficulties for the model to connect terms that cooccur\nin both query and document. To address this issue, we propose a novel Mixed\nAttention Transformer (MAT) that incorporates external word level knowledge,\nsuch as a dictionary or translation table. We design a sandwich like\narchitecture to embed MAT into the recent transformer based deep neural models.\nBy encoding the translation knowledge into an attention matrix, the model with\nMAT is able to focus on the mutually translated words in the input sequence.\nExperimental results demonstrate the effectiveness of the external knowledge\nand the significant improvement of MAT embedded neural reranking model on CLIR\ntask.", "published": "2021-09-07 00:33:14", "link": "http://arxiv.org/abs/2109.02789v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "IndicBART: A Pre-trained Model for Indic Natural Language Generation", "abstract": "In this paper, we study pre-trained sequence-to-sequence models for a group\nof related languages, with a focus on Indic languages. We present IndicBART, a\nmultilingual, sequence-to-sequence pre-trained model focusing on 11 Indic\nlanguages and English. IndicBART utilizes the orthographic similarity between\nIndic scripts to improve transfer learning between similar Indic languages. We\nevaluate IndicBART on two NLG tasks: Neural Machine Translation (NMT) and\nextreme summarization. Our experiments on NMT and extreme summarization show\nthat a model specific to related languages like IndicBART is competitive with\nlarge pre-trained models like mBART50 despite being significantly smaller. It\nalso performs well on very low-resource translation scenarios where languages\nare not included in pre-training or fine-tuning. Script sharing, multilingual\ntraining, and better utilization of limited model capacity contribute to the\ngood performance of the compact IndicBART model.", "published": "2021-09-07 07:08:33", "link": "http://arxiv.org/abs/2109.02903v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Naturalness Evaluation of Natural Language Generation in Task-oriented\n  Dialogues using BERT", "abstract": "This paper presents an automatic method to evaluate the naturalness of\nnatural language generation in dialogue systems. While this task was previously\nrendered through expensive and time-consuming human labor, we present this\nnovel task of automatic naturalness evaluation of generated language. By\nfine-tuning the BERT model, our proposed naturalness evaluation method shows\nrobust results and outperforms the baselines: support vector machines,\nbi-directional LSTMs, and BLEURT. In addition, the training speed and\nevaluation performance of naturalness model are improved by transfer learning\nfrom quality and informativeness linguistic knowledge.", "published": "2021-09-07 08:40:14", "link": "http://arxiv.org/abs/2109.02938v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Countering Online Hate Speech: An NLP Perspective", "abstract": "Online hate speech has caught everyone's attention from the news related to\nthe COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -\nan umbrella term for online hateful behavior, manifests itself in forms such as\nonline hate speech. Hate speech is a deliberate attack directed towards an\nindividual or a group motivated by the targeted entity's identity or opinions.\nThe rising mass communication through social media further exacerbates the\nharmful consequences of online hate speech. While there has been significant\nresearch on hate-speech identification using Natural Language Processing (NLP),\nthe work on utilizing NLP for prevention and intervention of online hate speech\nlacks relatively. This paper presents a holistic conceptual framework on\nhate-speech NLP countering methods along with a thorough survey on the current\nprogress of NLP for countering online hate speech. It classifies the countering\ntechniques based on their time of action, and identifies potential future\nresearch areas on this topic.", "published": "2021-09-07 08:48:13", "link": "http://arxiv.org/abs/2109.02941v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and\n  External Knowledge", "abstract": "One challenge for dialogue agents is to recognize feelings of the\nconversation partner and respond accordingly. In this work, RoBERTa-GPT2 is\nproposed for empathetic dialogue generation, where the pre-trained\nauto-encoding RoBERTa is utilised as encoder and the pre-trained\nauto-regressive GPT-2 as decoder. With the combination of the pre-trained\nRoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.\nTo enable the empathetic ability of RoBERTa-GPT2 model, we propose a\ncommonsense knowledge and emotional concepts extractor, in which the\ncommonsensible and emotional concepts of dialogue context are extracted for the\nGPT-2 decoder. The experiment results demonstrate that the empathetic dialogue\ngeneration benefits from both pre-trained encoder-decoder architecture and\nexternal knowledge.", "published": "2021-09-07 11:40:02", "link": "http://arxiv.org/abs/2109.03004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sequential Attention Module for Natural Language Processing", "abstract": "Recently, large pre-trained neural language models have attained remarkable\nperformance on many downstream natural language processing (NLP) applications\nvia fine-tuning. In this paper, we target at how to further improve the token\nrepresentations on the language models. We, therefore, propose a simple yet\neffective plug-and-play module, Sequential Attention Module (SAM), on the token\nembeddings learned from a pre-trained language model. Our proposed SAM consists\nof two main attention modules deployed sequentially: Feature-wise Attention\nModule (FAM) and Token-wise Attention Module (TAM). More specifically, FAM can\neffectively identify the importance of features at each dimension and promote\nthe effect via dot-product on the original token embeddings for downstream NLP\napplications. Meanwhile, TAM can further re-weight the features at the\ntoken-wise level. Moreover, we propose an adaptive filter on FAM to prevent\nnoise impact and increase information absorption. Finally, we conduct extensive\nexperiments to demonstrate the advantages and properties of our proposed SAM.\nWe first show how SAM plays a primary role in the champion solution of two\nsubtasks of SemEval'21 Task 7. After that, we apply SAM on sentiment analysis\nand three popular NLP tasks and demonstrate that SAM consistently outperforms\nthe state-of-the-art baselines.", "published": "2021-09-07 11:48:23", "link": "http://arxiv.org/abs/2109.03009v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Generate & Rank: A Multi-task Framework for Math Word Problems", "abstract": "Math word problem (MWP) is a challenging and critical task in natural\nlanguage processing. Many recent studies formalize MWP as a generation task and\nhave adopted sequence-to-sequence models to transform problem descriptions to\nmathematical expressions. However, mathematical expressions are prone to minor\nmistakes while the generation objective does not explicitly handle such\nmistakes. To address this limitation, we devise a new ranking task for MWP and\npropose Generate & Rank, a multi-task framework based on a generative\npre-trained language model. By joint training with generation and ranking, the\nmodel learns from its own mistakes and is able to distinguish between correct\nand incorrect expressions. Meanwhile, we perform tree-based disturbance\nspecially designed for MWP and an online update to boost the ranker. We\ndemonstrate the effectiveness of our proposed method on the benchmark and the\nresults show that our method consistently outperforms baselines in all\ndatasets. Particularly, in the classical Math23k, our method is 7% (78.4%\n$\\rightarrow$ 85.4%) higher than the state-of-the-art.", "published": "2021-09-07 12:21:49", "link": "http://arxiv.org/abs/2109.03034v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning grounded word meaning representations on similarity graphs", "abstract": "This paper introduces a novel approach to learn visually grounded meaning\nrepresentations of words as low-dimensional node embeddings on an underlying\ngraph hierarchy. The lower level of the hierarchy models modality-specific word\nrepresentations through dedicated but communicating graphs, while the higher\nlevel puts these representations together on a single graph to learn a\nrepresentation jointly from both modalities. The topology of each graph models\nsimilarity relations among words, and is estimated jointly with the graph\nembedding. The assumption underlying this model is that words sharing similar\nmeaning correspond to communities in an underlying similarity graph in a\nlow-dimensional space. We named this model Hierarchical Multi-Modal Similarity\nGraph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE\nto simulate human similarity judgements and concept categorization,\noutperforming the state of the art.", "published": "2021-09-07 13:40:32", "link": "http://arxiv.org/abs/2109.03084v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NumGPT: Improving Numeracy Ability of Generative Pre-trained Models", "abstract": "Existing generative pre-trained language models (e.g., GPT) focus on modeling\nthe language structure and semantics of general texts. However, those models do\nnot consider the numerical properties of numbers and cannot perform robustly on\nnumerical reasoning tasks (e.g., math word problems and measurement\nestimation). In this paper, we propose NumGPT, a generative pre-trained model\nthat explicitly models the numerical properties of numbers in texts.\nSpecifically, it leverages a prototype-based numeral embedding to encode the\nmantissa of the number and an individual embedding to encode the exponent of\nthe number. A numeral-aware loss function is designed to integrate numerals\ninto the pre-training objective of NumGPT. We conduct extensive experiments on\nfour different datasets to evaluate the numeracy ability of NumGPT. The\nexperiment results show that NumGPT outperforms baseline models (e.g., GPT and\nGPT with DICE) on a range of numerical reasoning tasks such as measurement\nestimation, number comparison, math word problems, and magnitude\nclassification. Ablation studies are also conducted to evaluate the impact of\npre-training and model hyperparameters on the performance.", "published": "2021-09-07 15:06:12", "link": "http://arxiv.org/abs/2109.03137v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When differential privacy meets NLP: The devil is in the detail", "abstract": "Differential privacy provides a formal approach to privacy of individuals.\nApplications of differential privacy in various scenarios, such as protecting\nusers' original utterances, must satisfy certain mathematical properties. Our\ncontribution is a formal analysis of ADePT, a differentially private\nauto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising\nresults on downstream tasks while providing tight privacy guarantees. Our proof\nreveals that ADePT is not differentially private, thus rendering the\nexperimental results unsubstantiated. We also quantify the impact of the error\nin its private mechanism, showing that the true sensitivity is higher by at\nleast factor 6 in an optimistic case of a very small encoder's dimension and\nthat the amount of utterances that are not privatized could easily reach 100%\nof the entire dataset. Our intention is neither to criticize the authors, nor\nthe peer-reviewing process, but rather point out that if differential privacy\napplications in NLP rely on formal guarantees, these should be outlined in full\nand put under detailed scrutiny.", "published": "2021-09-07 16:12:25", "link": "http://arxiv.org/abs/2109.03175v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "On the Challenges of Evaluating Compositional Explanations in Multi-Hop\n  Inference: Relevance, Completeness, and Expert Ratings", "abstract": "Building compositional explanations requires models to combine two or more\nfacts that, together, describe why the answer to a question is correct.\nTypically, these \"multi-hop\" explanations are evaluated relative to one (or a\nsmall number of) gold explanations. In this work, we show these evaluations\nsubstantially underestimate model performance, both in terms of the relevance\nof included facts, as well as the completeness of model-generated explanations,\nbecause models regularly discover and produce valid explanations that are\ndifferent than gold explanations. To address this, we construct a large corpus\nof 126k domain-expert (science teacher) relevance ratings that augment a corpus\nof explanations to standardized science exam questions, discovering 80k\nadditional relevant facts not rated as gold. We build three strong models based\non different methodologies (generation, ranking, and schemas), and empirically\nshow that while expert-augmented ratings provide better estimates of\nexplanation quality, both original (gold) and expert-augmented automatic\nevaluations still substantially underestimate performance by up to 36% when\ncompared with full manual expert judgements, with different models being\ndisproportionately affected. This poses a significant methodological challenge\nto accurately evaluating explanations produced by compositional reasoning\nmodels.", "published": "2021-09-07 21:00:05", "link": "http://arxiv.org/abs/2109.03334v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Powering Comparative Classification with Sentiment Analysis via Domain\n  Adaptive Knowledge Transfer", "abstract": "We study Comparative Preference Classification (CPC) which aims at predicting\nwhether a preference comparison exists between two entities in a given sentence\nand, if so, which entity is preferred over the other. High-quality CPC models\ncan significantly benefit applications such as comparative question answering\nand review-based recommendations. Among the existing approaches, non-deep\nlearning methods suffer from inferior performances. The state-of-the-art graph\nneural network-based ED-GAT (Ma et al., 2020) only considers syntactic\ninformation while ignoring the critical semantic relations and the sentiments\nto the compared entities. We proposed sentiment Analysis Enhanced COmparative\nNetwork (SAECON) which improves CPC ac-curacy with a sentiment analyzer that\nlearns sentiments to individual entities via domain adaptive knowledge\ntransfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset\npresent a significant improvement on the F1 scores over the best existing CPC\napproaches.", "published": "2021-09-07 19:17:12", "link": "http://arxiv.org/abs/2109.03819v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attention based Sequence to Sequence Learning for Machine Translation of\n  Low Resourced Indic Languages -- A case of Sanskrit to Hindi", "abstract": "Deep Learning techniques are powerful in mimicking humans in a particular set\nof problems. They have achieved a remarkable performance in complex learning\ntasks. Deep learning inspired Neural Machine Translation (NMT) is a proficient\ntechnique that outperforms traditional machine translation. Performing\nmachine-aided translation on Indic languages has always been a challenging task\nconsidering their rich and diverse grammar. The neural machine translation has\nshown quality results compared to the traditional machine translation\napproaches. The fully automatic machine translation becomes problematic when it\ncomes to low-resourced languages, especially with Sanskrit. This paper presents\nattention mechanism based neural machine translation by selectively focusing on\na particular part of language sentences during translation. The work shows the\nconstruction of Sanskrit to Hindi bilingual parallel corpus with nearly 10K\nsamples and having 178,000 tokens. The neural translation model equipped with\nan attention mechanism has been trained on Sanskrit to Hindi parallel corpus.\nThe approach has shown the significance of attention mechanisms to overcome\nlong-term dependencies, primarily associated with low resources Indic\nlanguages. The paper shows the attention plots on testing data to demonstrate\nthe alignment between source and translated words. For the evaluation of the\ntranslated sentences, manual score based human evaluation and automatic\nevaluation metric based techniques have been adopted. The attention mechanism\nbased neural translation has achieved 88% accuracy in human evaluation and a\nBLEU score of 0.92 on Sanskrit to Hindi translation.", "published": "2021-09-07 04:55:48", "link": "http://arxiv.org/abs/2110.00435v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An N-gram based approach to auto-extracting topics from research\n  articles", "abstract": "A lot of manual work goes into identifying a topic for an article. With a\nlarge volume of articles, the manual process can be exhausting. Our approach\naims to address this issue by automatically extracting topics from the text of\nlarge Numbers of articles. This approach takes into account the efficiency of\nthe process. Based on existing N-gram analysis, our research examines how often\ncertain words appear in documents in order to support automatic topic\nextraction. In order to improve efficiency, we apply custom filtering standards\nto our research. Additionally, delete as many noncritical or irrelevant phrases\nas possible. In this way, we can ensure we are selecting unique keyphrases for\neach article, which capture its core idea. For our research, we chose to center\non the autonomous vehicle domain, since the research is relevant to our daily\nlives. We have to convert the PDF versions of most of the research papers into\neditable types of files such as TXT. This is because most of the research\npapers are only in PDF format. To test our proposed idea of automating,\nnumerous articles on robotics have been selected. Next, we evaluate our\napproach by comparing the result with that obtained manually.", "published": "2021-09-07 04:49:01", "link": "http://arxiv.org/abs/2110.11879v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Puzzle Solving without Search or Human Knowledge: An Unnatural Language\n  Approach", "abstract": "The application of Generative Pre-trained Transformer (GPT-2) to learn\ntext-archived game notation provides a model environment for exploring sparse\nreward gameplay. The transformer architecture proves amenable to training on\nsolved text archives describing mazes, Rubik's Cube, and Sudoku solvers. The\nmethod benefits from fine-tuning the transformer architecture to visualize\nplausible strategies derived outside any guidance from human heuristics or\ndomain expertise. The large search space ($>10^{19}$) for the games provides a\npuzzle environment in which the solution has few intermediate rewards and a\nfinal move that solves the challenge.", "published": "2021-09-07 01:20:28", "link": "http://arxiv.org/abs/2109.02797v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "POSSCORE: A Simple Yet Effective Evaluation of Conversational Search\n  with Part of Speech Labelling", "abstract": "Conversational search systems, such as Google Assistant and Microsoft\nCortana, provide a new search paradigm where users are allowed, via natural\nlanguage dialogues, to communicate with search systems. Evaluating such systems\nis very challenging since search results are presented in the format of natural\nlanguage sentences. Given the unlimited number of possible responses,\ncollecting relevance assessments for all the possible responses is infeasible.\nIn this paper, we propose POSSCORE, a simple yet effective automatic evaluation\nmethod for conversational search. The proposed embedding-based metric takes the\ninfluence of part of speech (POS) of the terms in the response into account. To\nthe best knowledge, our work is the first to systematically demonstrate the\nimportance of incorporating syntactic information, such as POS labels, for\nconversational search evaluation. Experimental results demonstrate that our\nmetrics can correlate with human preference, achieving significant improvements\nover state-of-the-art baseline metrics.", "published": "2021-09-07 12:31:29", "link": "http://arxiv.org/abs/2109.03039v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PAUSE: Positive and Annealed Unlabeled Sentence Embedding", "abstract": "Sentence embedding refers to a set of effective and versatile techniques for\nconverting raw text into numerical vector representations that can be used in a\nwide range of natural language processing (NLP) applications. The majority of\nthese techniques are either supervised or unsupervised. Compared to the\nunsupervised methods, the supervised ones make less assumptions about\noptimization objectives and usually achieve better results. However, the\ntraining requires a large amount of labeled sentence pairs, which is not\navailable in many industrial scenarios. To that end, we propose a generic and\nend-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence\nEmbedding), capable of learning high-quality sentence embeddings from a\npartially labeled dataset. We experimentally show that PAUSE achieves, and\nsometimes surpasses, state-of-the-art results using only a small fraction of\nlabeled sentence pairs on various benchmark tasks. When applied to a real\nindustrial use case where labeled samples are scarce, PAUSE encourages us to\nextend our dataset without the liability of extensive manual annotation work.", "published": "2021-09-07 15:46:19", "link": "http://arxiv.org/abs/2109.03155v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on\n  Code-Mixed Data using BERT models", "abstract": "The increasing use of social media sites in countries like India has given\nrise to large volumes of code-mixed data. Sentiment analysis of this data can\nprovide integral insights into people's perspectives and opinions. Developing\nrobust explainability techniques which explain why models make their\npredictions becomes essential. In this paper, we propose an adequate\nmethodology to integrate explainable approaches into code-mixed sentiment\nanalysis.", "published": "2021-09-07 17:06:54", "link": "http://arxiv.org/abs/2109.03200v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT\n  Compression", "abstract": "Recent studies on compression of pretrained language models (e.g., BERT)\nusually use preserved accuracy as the metric for evaluation. In this paper, we\npropose two new metrics, label loyalty and probability loyalty that measure how\nclosely a compressed model (i.e., student) mimics the original model (i.e.,\nteacher). We also explore the effect of compression with regard to robustness\nunder adversarial attacks. We benchmark quantization, pruning, knowledge\ndistillation and progressive module replacing with loyalty and robustness. By\ncombining multiple compression techniques, we provide a practical strategy to\nachieve better accuracy, loyalty and robustness.", "published": "2021-09-07 17:55:47", "link": "http://arxiv.org/abs/2109.03228v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification\ntasks, while its capability of generating novel speech, similar to how GPT-2\ncan generate coherent paragraphs, has barely been explored. Generative Spoken\nLanguage Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing\nthe generative aspects of speech pre-training, which replaces text with\ndiscovered phone-like units for language modeling and shows the ability to\ngenerate meaningful novel sentences. Unfortunately, despite eliminating the\nneed of text, the units used in GSLM discard most of the prosodic information.\nHence, GSLM fails to leverage prosody for better comprehension, and does not\ngenerate expressive speech. In this work, we present a prosody-aware generative\nspoken language model (pGSLM). It is composed of a multi-stream transformer\nlanguage model (MS-TLM) of speech, represented as discovered unit and prosodic\nfeature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to\nwaveforms. We devise a series of metrics for prosody modeling and generation,\nand re-use metrics from GSLM for content modeling. Experimental results show\nthat the pGSLM can utilize prosody to improve both prosody and content\nmodeling, and also generate natural, meaningful, and coherent speech given a\nspoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.\nCodes and models are available at\nhttps://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "published": "2021-09-07 18:03:21", "link": "http://arxiv.org/abs/2109.03264v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Scalable AI Approach for Clinical Trial Cohort Optimization", "abstract": "FDA has been promoting enrollment practices that could enhance the diversity\nof clinical trial populations, through broadening eligibility criteria.\nHowever, how to broaden eligibility remains a significant challenge. We propose\nan AI approach to Cohort Optimization (AICO) through transformer-based natural\nlanguage processing of the eligibility criteria and evaluation of the criteria\nusing real-world data. The method can extract common eligibility criteria\nvariables from a large set of relevant trials and measure the generalizability\nof trial designs to real-world patients. It overcomes the scalability limits of\nexisting manual methods and enables rapid simulation of eligibility criteria\ndesign for a disease of interest. A case study on breast cancer trial design\ndemonstrates the utility of the method in improving trial generalizability.", "published": "2021-09-07 01:49:05", "link": "http://arxiv.org/abs/2109.02808v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "The DKU-DukeECE System for the Self-Supervision Speaker Verification\n  Task of the 2021 VoxCeleb Speaker Recognition Challenge", "abstract": "This report describes the submission of the DKU-DukeECE team to the\nself-supervision speaker verification task of the 2021 VoxCeleb Speaker\nRecognition Challenge (VoxSRC). Our method employs an iterative labeling\nframework to learn self-supervised speaker representation based on a deep\nneural network (DNN). The framework starts with training a self-supervision\nspeaker embedding network by maximizing agreement between different segments\nwithin an utterance via a contrastive loss. Taking advantage of DNN's ability\nto learn from data with label noise, we propose to cluster the speaker\nembedding obtained from the previous speaker network and use the subsequent\nclass assignments as pseudo labels to train a new DNN. Moreover, we iteratively\ntrain the speaker network with pseudo labels generated from the previous step\nto bootstrap the discriminative power of a DNN. Also, visual modal data is\nincorporated in this self-labeling framework. The visual pseudo label and the\naudio pseudo label are fused with a cluster ensemble algorithm to generate a\nrobust supervisory signal for representation learning. Our submission achieves\nan equal error rate (EER) of 5.58% and 5.59% on the challenge development and\ntest set, respectively.", "published": "2021-09-07 04:13:48", "link": "http://arxiv.org/abs/2109.02853v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal\n  and Multimodal Detectors", "abstract": "Significant advancements made in the generation of deepfakes have caused\nsecurity and privacy issues. Attackers can easily impersonate a person's\nidentity in an image by replacing his face with the target person's face.\nMoreover, a new domain of cloning human voices using deep-learning technologies\nis also emerging. Now, an attacker can generate realistic cloned voices of\nhumans using only a few seconds of audio of the target person. With the\nemerging threat of potential harm deepfakes can cause, researchers have\nproposed deepfake detection methods. However, they only focus on detecting a\nsingle modality, i.e., either video or audio. On the other hand, to develop a\ngood deepfake detector that can cope with the recent advancements in deepfake\ngeneration, we need to have a detector that can detect deepfakes of multiple\nmodalities, i.e., videos and audios. To build such a detector, we need a\ndataset that contains video and respective audio deepfakes. We were able to\nfind a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection\nDataset (FakeAVCeleb), that contains not only deepfake videos but synthesized\nfake audios as well. We used this multimodal deepfake dataset and performed\ndetailed baseline experiments using state-of-the-art unimodal, ensemble-based,\nand multimodal detection methods to evaluate it. We conclude through detailed\nexperimentation that unimodals, addressing only a single modality, video or\naudio, do not perform well compared to ensemble-based methods. Whereas purely\nmultimodal-based baselines provide the worst performance.", "published": "2021-09-07 11:00:20", "link": "http://arxiv.org/abs/2109.02993v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS", "eess.IV", "I.4.9; I.5.4"], "primary_category": "cs.CV"}
