{"title": "Using Neural Generative Models to Release Synthetic Twitter Corpora with\n  Reduced Stylometric Identifiability of Users", "abstract": "We present a method for generating synthetic versions of Twitter data using\nneural generative models. The goal is protecting individuals in the source data\nfrom stylometric re-identification attacks while still releasing data that\ncarries research value. Specifically, we generate tweet corpora that maintain\nuser-level word distributions by augmenting the neural language models with\nuser-specific components. We compare our approach to two standard text data\nprotection methods: redaction and iterative translation. We evaluate the three\nmethods on measures of risk and utility. We define risk following the\nstylometric models of re-identification, and we define utility based on two\ngeneral word distribution measures and two common text analysis research tasks.\nWe find that neural models are able to significantly lower risk over previous\nmethods with little cost to utility. We also demonstrate that the neural models\nallow data providers to actively control the risk-utility trade-off through\nmodel tuning parameters. This work presents promising results for a new tool\naddressing the problem of privacy for free text and sharing social media data\nin a way that respects privacy and is ethically responsible.", "published": "2016-06-03 15:43:15", "link": "http://arxiv.org/abs/1606.01151v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task\n  Learning", "abstract": "Various treebanks have been released for dependency parsing. Despite that\ntreebanks may belong to different languages or have different annotation\nschemes, they contain syntactic knowledge that is potential to benefit each\nother. This paper presents an universal framework for exploiting these\nmulti-typed treebanks to improve parsing with deep multi-task learning. We\nconsider two kinds of treebanks as source: the multilingual universal treebanks\nand the monolingual heterogeneous treebanks. Multiple treebanks are trained\njointly and interacted with multi-level parameter sharing. Experiments on\nseveral benchmark datasets in various languages demonstrate that our approach\ncan make effective use of arbitrary source treebanks to improve target parsing\nmodels.", "published": "2016-06-03 16:09:52", "link": "http://arxiv.org/abs/1606.01161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing the LexVec Distributed Word Representation Model Using\n  Positional Contexts and External Memory", "abstract": "In this paper we take a state-of-the-art model for distributed word\nrepresentation that explicitly factorizes the positive pointwise mutual\ninformation (PPMI) matrix using window sampling and negative sampling and\naddress two of its shortcomings. We improve syntactic performance by using\npositional contexts, and solve the need to store the PPMI matrix in memory by\nworking on aggregate data in external memory. The effectiveness of both\nmodifications is shown using word similarity and analogy tasks.", "published": "2016-06-03 21:39:42", "link": "http://arxiv.org/abs/1606.01283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency Parsing as Head Selection", "abstract": "Conventional graph-based dependency parsers guarantee a tree structure both\nduring training and inference. Instead, we formalize dependency parsing as the\nproblem of independently selecting the head of each word in a sentence. Our\nmodel which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bf\nN}eural {\\bf Se}lection) produces a distribution over possible heads for each\nword using features obtained from a bidirectional recurrent neural network.\nWithout enforcing structural constraints during training, \\textsc{DeNSe}\ngenerates (at inference time) trees for the overwhelming majority of sentences,\nwhile non-tree outputs can be adjusted with a maximum spanning tree algorithm.\nWe evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, and\nGerman) with varying degrees of non-projectivity. Despite the simplicity of the\napproach, our parsers are on par with the state of the art.", "published": "2016-06-03 21:27:03", "link": "http://arxiv.org/abs/1606.01280v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Attentional Neural Conversation Model with Improved Specificity", "abstract": "In this paper we propose a neural conversation model for conducting\ndialogues. We demonstrate the use of this model to generate help desk\nresponses, where users are asking questions about PC applications. Our model is\ndistinguished by two characteristics. First, it models intention across turns\nwith a recurrent network, and incorporates an attention model that is\nconditioned on the representation of intention. Secondly, it avoids generating\nnon-specific responses by incorporating an IDF term in the objective function.\nThe model is evaluated both as a pure generation model in which a help-desk\nresponse is generated from scratch, and as a retrieval model with performance\nmeasured using recall rates of the correct response. Experimental results\nindicate that the model outperforms previously proposed neural conversation\narchitectures, and that using specificity in the objective function\nsignificantly improves performances for both generation and retrieval.", "published": "2016-06-03 22:26:01", "link": "http://arxiv.org/abs/1606.01292v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Question Answering over Knowledge Base with Neural Attention Combining\n  Global Knowledge Information", "abstract": "With the rapid growth of knowledge bases (KBs) on the web, how to take full\nadvantage of them becomes increasingly important. Knowledge base-based question\nanswering (KB-QA) is one of the most promising approaches to access the\nsubstantial knowledge. Meantime, as the neural network-based (NN-based) methods\ndevelop, NN-based KB-QA has already achieved impressive results. However,\nprevious work did not put emphasis on question representation, and the question\nis converted into a fixed vector regardless of its candidate answers. This\nsimple representation strategy is unable to express the proper information of\nthe question. Hence, we present a neural attention-based model to represent the\nquestions dynamically according to the different focuses of various candidate\nanswer aspects. In addition, we leverage the global knowledge inside the\nunderlying KB, aiming at integrating the rich KB information into the\nrepresentation of the answers. And it also alleviates the out of vocabulary\n(OOV) problem, which helps the attention model to represent the question more\nprecisely. The experimental results on WEBQUESTIONS demonstrate the\neffectiveness of the proposed approach.", "published": "2016-06-03 06:40:14", "link": "http://arxiv.org/abs/1606.00979v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.IR"}
{"title": "Learning Stylometric Representations for Authorship Analysis", "abstract": "Authorship analysis (AA) is the study of unveiling the hidden properties of\nauthors from a body of exponentially exploding textual data. It extracts an\nauthor's identity and sociolinguistic characteristics based on the reflected\nwriting styles in the text. It is an essential process for various areas, such\nas cybercrime investigation, psycholinguistics, political socialization, etc.\nHowever, most of the previous techniques critically depend on the manual\nfeature engineering process. Consequently, the choice of feature set has been\nshown to be scenario- or dataset-dependent. In this paper, to mimic the human\nsentence composition process using a neural network approach, we propose to\nincorporate different categories of linguistic features into distributed\nrepresentation of words in order to learn simultaneously the writing style\nrepresentations based on unlabeled texts for authorship analysis. In\nparticular, the proposed models allow topical, lexical, syntactical, and\ncharacter-level feature vectors of each document to be extracted as\nstylometrics. We evaluate the performance of our approach on the problems of\nauthorship characterization and authorship verification with the Twitter,\nnovel, and essay datasets. The experiments suggest that our proposed text\nrepresentation outperforms the bag-of-lexical-n-grams, Latent Dirichlet\nAllocation, Latent Semantic Analysis, PVDM, PVDBOW, and word2vec\nrepresentations.", "published": "2016-06-03 18:42:14", "link": "http://arxiv.org/abs/1606.01219v1", "categories": ["cs.CL", "cs.CY", "cs.SI", "K.4.1; I.7.5; I.2.7"], "primary_category": "cs.CL"}
{"title": "End-to-end LSTM-based dialog control optimized with supervised and\n  reinforcement learning", "abstract": "This paper presents a model for end-to-end learning of task-oriented dialog\nsystems. The main component of the model is a recurrent neural network (an\nLSTM), which maps from raw dialog history directly to a distribution over\nsystem actions. The LSTM automatically infers a representation of dialog\nhistory, which relieves the system developer of much of the manual feature\nengineering of dialog state. In addition, the developer can provide software\nthat expresses business rules and provides access to programmatic APIs,\nenabling the LSTM to take actions in the real world on behalf of the user. The\nLSTM can be optimized using supervised learning (SL), where a domain expert\nprovides example dialogs which the LSTM should imitate; or using reinforcement\nlearning (RL), where the system improves by interacting directly with end\nusers. Experiments show that SL and RL are complementary: SL alone can derive a\nreasonable initial policy from a small number of training dialogs; and starting\nRL optimization with a policy trained with SL substantially accelerates the\nlearning rate of RL.", "published": "2016-06-03 20:32:52", "link": "http://arxiv.org/abs/1606.01269v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep,\nzoneout stochastically forces some hidden units to maintain their previous\nvalues. Like dropout, zoneout uses random noise to train a pseudo-ensemble,\nimproving generalization. But by preserving instead of dropping hidden units,\ngradient information and state information are more readily propagated through\ntime, as in feedforward stochastic depth networks. We perform an empirical\ninvestigation of various RNN regularizers, and find that zoneout gives\nsignificant performance improvements across tasks. We achieve competitive\nresults with relatively simple models in character- and word-level language\nmodelling on the Penn Treebank and Text8 datasets, and combining with recurrent\nbatch normalization yields state-of-the-art results on permuted sequential\nMNIST.", "published": "2016-06-03 23:31:47", "link": "http://arxiv.org/abs/1606.01305v4", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
