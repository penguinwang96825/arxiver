{"title": "Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via\n  Gradient-based Decoding", "abstract": "Written language carries explicit and implicit biases that can distract from\nmeaningful signals. For example, letters of reference may describe male and\nfemale candidates differently, or their writing style may indirectly reveal\ndemographic characteristics. At best, such biases distract from the meaningful\ncontent of the text; at worst they can lead to unfair outcomes. We investigate\nthe challenge of re-generating input sentences to 'neutralize' sensitive\nattributes while maintaining the semantic meaning of the original text (e.g. is\nthe candidate qualified?). We propose a gradient-based rewriting framework,\nDetect and Perturb to Neutralize (DEPEN), that first detects sensitive\ncomponents and masks them for regeneration, then perturbs the generation model\nat decoding time under a neutralizing constraint that pushes the (predicted)\ndistribution of sensitive attributes towards a uniform distribution. Our\nexperiments in two different scenarios show that DEPEN can regenerate fluent\nalternatives that are neutral in the sensitive attribute while maintaining the\nsemantics of other attributes.", "published": "2021-09-24 01:53:07", "link": "http://arxiv.org/abs/2109.11708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Diversity-Enhanced and Constraints-Relaxed Augmentation for\n  Low-Resource Classification", "abstract": "Data augmentation (DA) aims to generate constrained and diversified data to\nimprove classifiers in Low-Resource Classification (LRC). Previous studies\nmostly use a fine-tuned Language Model (LM) to strengthen the constraints but\nignore the fact that the potential of diversity could improve the effectiveness\nof generated data. In LRC, strong constraints but weak diversity in DA result\nin the poor generalization ability of classifiers. To address this dilemma, we\npropose a {D}iversity-{E}nhanced and {C}onstraints-\\{R}elaxed {A}ugmentation\n(DECRA). Our DECRA has two essential components on top of a transformer-based\nbackbone model. 1) A k-beta augmentation, an essential component of DECRA, is\nproposed to enhance the diversity in generating constrained data. It expands\nthe changing scope and improves the degree of complexity of the generated data.\n2) A masked language model loss, instead of fine-tuning, is used as a\nregularization. It relaxes constraints so that the classifier can be trained\nwith more scattered generated data. The combination of these two components\ngenerates data that can reach or approach category boundaries and hence help\nthe classifier generalize better. We evaluate our DECRA on three public\nbenchmark datasets under low-resource settings. Extensive experiments\ndemonstrate that our DECRA outperforms state-of-the-art approaches by 3.8% in\nthe overall score.", "published": "2021-09-24 09:26:29", "link": "http://arxiv.org/abs/2109.11834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robustness and Sensitivity of BERT Models Predicting Alzheimer's Disease\n  from Text", "abstract": "Understanding robustness and sensitivity of BERT models predicting\nAlzheimer's disease from text is important for both developing better\nclassification models and for understanding their capabilities and limitations.\nIn this paper, we analyze how a controlled amount of desired and undesired text\nalterations impacts performance of BERT. We show that BERT is robust to natural\nlinguistic variations in text. On the other hand, we show that BERT is not\nsensitive to removing clinically important information from text.", "published": "2021-09-24 11:15:06", "link": "http://arxiv.org/abs/2109.11888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Translation of German--Lower Sorbian: Exploring Training\n  and Novel Transfer Methods on a Low-Resource Language", "abstract": "This paper describes the methods behind the systems submitted by the\nUniversity of Groningen for the WMT 2021 Unsupervised Machine Translation task\nfor German--Lower Sorbian (DE--DSB): a high-resource language to a low-resource\none. Our system uses a transformer encoder-decoder architecture in which we\nmake three changes to the standard training procedure. First, our training\nfocuses on two languages at a time, contrasting with a wealth of research on\nmultilingual systems. Second, we introduce a novel method for initializing the\nvocabulary of an unseen language, achieving improvements of 3.2 BLEU for\nDE$\\rightarrow$DSB and 4.0 BLEU for DSB$\\rightarrow$DE. Lastly, we experiment\nwith the order in which offline and online back-translation are used to train\nan unsupervised system, finding that using online back-translation first works\nbetter for DE$\\rightarrow$DSB by 2.76 BLEU. Our submissions ranked first (tied\nwith another team) for DSB$\\rightarrow$DE and third for DE$\\rightarrow$DSB.", "published": "2021-09-24 15:11:22", "link": "http://arxiv.org/abs/2109.12012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Indirectly Supervised English Sentence Break Prediction Using Paragraph\n  Break Probability Estimates", "abstract": "This report explores the use of paragraph break probability estimates to help\npredict the location of sentence breaks in English natural language text. We\nshow that a sentence break predictor based almost solely on paragraph break\nprobability estimates can achieve high accuracy on this task. This sentence\nbreak predictor is trained almost entirely on a large amount of naturally\noccurring text without sentence break annotations, with only a small amount of\nannotated data needed to tune two hyperparameters. We also show that even\nbetter results can be achieved across in-domain and out-of-domain test data, if\nparagraph break probability signals are combined with a support vector machine\nclassifier trained on a somewhat larger amount of sentence-break-annotated\ndata. Numerous related issues are addressed along the way.", "published": "2021-09-24 15:30:06", "link": "http://arxiv.org/abs/2109.12023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Post-pretraining Representation Alignment for\n  Cross-Lingual Question Answering", "abstract": "Human knowledge is collectively encoded in the roughly 6500 languages spoken\naround the world, but it is not distributed equally across languages. Hence,\nfor information-seeking question answering (QA) systems to adequately serve\nspeakers of all languages, they need to operate cross-lingually. In this work\nwe investigate the capabilities of multilingually pre-trained language models\non cross-lingual QA. We find that explicitly aligning the representations\nacross languages with a post-hoc fine-tuning step generally leads to improved\nperformance. We additionally investigate the effect of data size as well as the\nlanguage choice in this fine-tuning step, also releasing a dataset for\nevaluating cross-lingual QA systems. Code and dataset are publicly available\nhere: https://github.com/ffaisal93/aligned_qa", "published": "2021-09-24 15:32:45", "link": "http://arxiv.org/abs/2109.12028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformers Generalize Linearly", "abstract": "Natural language exhibits patterns of hierarchically governed dependencies,\nin which relations between words are sensitive to syntactic structure rather\nthan linear ordering. While re-current network models often fail to generalize\nin a hierarchically sensitive way (McCoy et al.,2020) when trained on ambiguous\ndata, the improvement in performance of newer Trans-former language models\n(Vaswani et al., 2017)on a range of syntactic benchmarks trained on large data\nsets (Goldberg, 2019; Warstadtet al., 2019) opens the question of whether these\nmodels might exhibit hierarchical generalization in the face of impoverished\ndata.In this paper we examine patterns of structural generalization for\nTransformer sequence-to-sequence models and find that not only do Transformers\nfail to generalize hierarchically across a wide variety of grammatical mapping\ntasks, but they exhibit an even stronger preference for linear generalization\nthan comparable recurrent networks", "published": "2021-09-24 15:48:46", "link": "http://arxiv.org/abs/2109.12036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Monolingual and Cross-Lingual Acceptability Judgments with the Italian\n  CoLA corpus", "abstract": "The development of automated approaches to linguistic acceptability has been\ngreatly fostered by the availability of the English CoLA corpus, which has also\nbeen included in the widely used GLUE benchmark. However, this kind of research\nfor languages other than English, as well as the analysis of cross-lingual\napproaches, has been hindered by the lack of resources with a comparable size\nin other languages. We have therefore developed the ItaCoLA corpus, containing\nalmost 10,000 sentences with acceptability judgments, which has been created\nfollowing the same approach and the same steps as the English one. In this\npaper we describe the corpus creation, we detail its content, and we present\nthe first experiments on this new resource. We compare in-domain and\nout-of-domain classification, and perform a specific evaluation of nine\nlinguistic phenomena. We also present the first cross-lingual experiments,\naimed at assessing whether multilingual transformerbased approaches can benefit\nfrom using sentences in two languages during fine-tuning.", "published": "2021-09-24 16:18:53", "link": "http://arxiv.org/abs/2109.12053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SD-QA: Spoken Dialectal Question Answering for the Real World", "abstract": "Question answering (QA) systems are now available through numerous commercial\napplications for a wide variety of domains, serving millions of users that\ninteract with them via speech interfaces. However, current benchmarks in QA\nresearch do not account for the errors that speech recognition models might\nintroduce, nor do they consider the language variations (dialects) of the\nusers. To address this gap, we augment an existing QA dataset to construct a\nmulti-dialect, spoken QA benchmark on five languages (Arabic, Bengali, English,\nKiswahili, Korean) with more than 68k audio prompts in 24 dialects from 255\nspeakers. We provide baseline results showcasing the real-world performance of\nQA systems and analyze the effect of language variety and other sensitive\nspeaker attributes on downstream performance. Last, we study the fairness of\nthe ASR and QA models with respect to the underlying user populations. The\ndataset, model outputs, and code for reproducing all our experiments are\navailable: https://github.com/ffaisal93/SD-QA.", "published": "2021-09-24 16:54:27", "link": "http://arxiv.org/abs/2109.12072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Adversarial Learning for Bootstrapping: A Case Study on\n  Entity Set Expansion", "abstract": "Bootstrapping has become the mainstream method for entity set expansion.\nConventional bootstrapping methods mostly define the expansion boundary using\nseed-based distance metrics, which heavily depend on the quality of selected\nseeds and are hard to be adjusted due to the extremely sparse supervision. In\nthis paper, we propose BootstrapGAN, a new learning method for bootstrapping\nwhich jointly models the bootstrapping process and the boundary learning\nprocess in a GAN framework. Specifically, the expansion boundaries of different\nbootstrapping iterations are learned via different discriminator networks; the\nbootstrapping network is the generator to generate new positive entities, and\nthe discriminator networks identify the expansion boundaries by trying to\ndistinguish the generated entities from known positive entities. By iteratively\nperforming the above adversarial learning, the generator and the discriminators\ncan reinforce each other and be progressively refined along the whole\nbootstrapping process. Experiments show that BootstrapGAN achieves the new\nstate-of-the-art entity set expansion performance.", "published": "2021-09-24 17:19:34", "link": "http://arxiv.org/abs/2109.12082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-based NP Enrichment", "abstract": "Understanding the relations between entities denoted by NPs in a text is a\ncritical part of human-like natural language understanding. However, only a\nfraction of such relations is covered by standard NLP tasks and benchmarks\nnowadays. In this work, we propose a novel task termed text-based NP enrichment\n(TNE), in which we aim to enrich each NP in a text with all the\npreposition-mediated relations -- either explicit or implicit -- that hold\nbetween it and other NPs in the text. The relations are represented as\ntriplets, each denoted by two NPs related via a preposition. Humans recover\nsuch relations seamlessly, while current state-of-the-art models struggle with\nthem due to the implicit nature of the problem. We build the first large-scale\ndataset for the problem, provide the formal framing and scope of annotation,\nanalyze the data, and report the results of fine-tuned language models on the\ntask, demonstrating the challenge it poses to current technology. A webpage\nwith a data-exploration UI, a demo, and links to the code, models, and\nleaderboard, to foster further research into this challenging problem can be\nfound at: yanaiela.github.io/TNE/.", "published": "2021-09-24 17:23:25", "link": "http://arxiv.org/abs/2109.12085v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithful Target Attribute Prediction in Neural Machine Translation", "abstract": "The training data used in NMT is rarely controlled with respect to specific\nattributes, such as word casing or gender, which can cause errors in\ntranslations. We argue that predicting the target word and attributes\nsimultaneously is an effective way to ensure that translations are more\nfaithful to the training data distribution with respect to these attributes.\nExperimental results on two tasks, uppercased input translation and gender\nprediction, show that this strategy helps mirror the training data distribution\nin testing. It also facilitates data augmentation on the task of uppercased\ninput translation.", "published": "2021-09-24 17:55:07", "link": "http://arxiv.org/abs/2109.12105v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Attention Sparsity in Transformers", "abstract": "Transformers' quadratic complexity with respect to the input sequence length\nhas motivated a body of work on efficient sparse approximations to softmax. An\nalternative path, used by entmax transformers, consists of having built-in\nexact sparse attention; however this approach still requires quadratic\ncomputation. In this paper, we propose Sparsefinder, a simple model trained to\nidentify the sparsity pattern of entmax attention before computing it. We\nexperiment with three variants of our method, based on distances, quantization,\nand clustering, on two tasks: machine translation (attention in the decoder)\nand masked language modeling (encoder-only). Our work provides a new angle to\nstudy model efficiency by doing extensive analysis of the tradeoff between the\nsparsity and recall of the predicted attention graph. This allows for detailed\ncomparison between different models along their Pareto curves, important to\nguide future benchmarks for sparse attention models.", "published": "2021-09-24 20:51:21", "link": "http://arxiv.org/abs/2109.12188v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Control for Schema-Guided Natural Language Generation", "abstract": "Natural Language Generation (NLG) for task-oriented dialogue systems focuses\non communicating specific content accurately, fluently, and coherently. While\nthese attributes are crucial for a successful dialogue, it is also desirable to\nsimultaneously accomplish specific stylistic goals, such as response length,\npoint-of-view, descriptiveness, sentiment, formality, and empathy. In this\nwork, we focus on stylistic control and evaluation for schema-guided NLG, with\njoint goals of achieving both semantic and stylistic control. We experiment in\ndetail with various controlled generation methods for large pretrained language\nmodels: specifically, conditional training, guided fine-tuning, and guided\ndecoding. We discuss their advantages and limitations, and evaluate them with a\nbroad range of automatic and human evaluation metrics. Our results show that\nwhile high style accuracy and semantic correctness are easier to achieve for\nmore lexically-defined styles with conditional training, stylistic control is\nalso achievable for more semantically complex styles using discriminator-based\nguided decoding methods. The results also suggest that methods that are more\nscalable (with less hyper-parameters tuning) and that disentangle content\ngeneration and stylistic variations are more effective at achieving semantic\ncorrectness and style accuracy.", "published": "2021-09-24 21:47:58", "link": "http://arxiv.org/abs/2109.12211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Fine-to-Coarse Distillation for Coarse-grained Response\n  Selection in Open-Domain Conversations", "abstract": "We study the problem of coarse-grained response selection in retrieval-based\ndialogue systems. The problem is equally important with fine-grained response\nselection, but is less explored in existing literature. In this paper, we\npropose a Contextual Fine-to-Coarse (CFC) distilled model for coarse-grained\nresponse selection in open-domain conversations. In our CFC model, dense\nrepresentations of query, candidate response and corresponding context is\nlearned based on the multi-tower architecture, and more expressive knowledge\nlearned from the one-tower architecture (fine-grained) is distilled into the\nmulti-tower architecture (coarse-grained) to enhance the performance of the\nretriever. To evaluate the performance of our proposed model, we construct two\nnew datasets based on the Reddit comments dump and Twitter corpus. Extensive\nexperimental results on the two datasets show that the proposed methods achieve\na significant improvement over all evaluation metrics compared with traditional\nbaseline methods.", "published": "2021-09-24 08:22:35", "link": "http://arxiv.org/abs/2109.13087v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DACT-BERT: Differentiable Adaptive Computation Time for an Efficient\n  BERT Inference", "abstract": "Large-scale pre-trained language models have shown remarkable results in\ndiverse NLP applications. Unfortunately, these performance gains have been\naccompanied by a significant increase in computation time and model size,\nstressing the need to develop new or complementary strategies to increase the\nefficiency of these models. In this paper we propose DACT-BERT, a\ndifferentiable adaptive computation time strategy for BERT-like models.\nDACT-BERT adds an adaptive computational mechanism to BERT's regular processing\npipeline, which controls the number of Transformer blocks that need to be\nexecuted at inference time. By doing this, the model learns to combine the most\nappropriate intermediate representations for the task at hand. Our experiments\ndemonstrate that our approach, when compared to the baselines, excels on a\nreduced computational regime and is competitive in other less restrictive ones.", "published": "2021-09-24 04:45:55", "link": "http://arxiv.org/abs/2109.11745v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lacking the embedding of a word? Look it up into a traditional\n  dictionary", "abstract": "Word embeddings are powerful dictionaries, which may easily capture language\nvariations. However, these dictionaries fail to give sense to rare words, which\nare surprisingly often covered by traditional dictionaries. In this paper, we\npropose to use definitions retrieved in traditional dictionaries to produce\nword embeddings for rare words. For this purpose, we introduce two methods:\nDefinition Neural Network (DefiNNet) and Define BERT (DefBERT). In our\nexperiments, DefiNNet and DefBERT significantly outperform state-of-the-art as\nwell as baseline methods devised for producing embeddings of unknown words. In\nfact, DefiNNet significantly outperforms FastText, which implements a method\nfor the same task-based on n-grams, and DefBERT significantly outperforms the\nBERT method for OOV words. Then, definitions in traditional dictionaries are\nuseful to build word embeddings for rare words.", "published": "2021-09-24 06:27:58", "link": "http://arxiv.org/abs/2109.11763v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dense Contrastive Visual-Linguistic Pretraining", "abstract": "Inspired by the success of BERT, several multimodal representation learning\napproaches have been proposed that jointly represent image and text. These\napproaches achieve superior performance by capturing high-level semantic\ninformation from large-scale multimodal pretraining. In particular, LXMERT and\nUNITER adopt visual region feature regression and label classification as\npretext tasks. However, they tend to suffer from the problems of noisy labels\nand sparse semantic annotations, based on the visual features having been\npretrained on a crowdsourced dataset with limited and inconsistent semantic\nlabeling. To overcome these issues, we propose unbiased Dense Contrastive\nVisual-Linguistic Pretraining (DCVLP), which replaces the region regression and\nclassification with cross-modality region contrastive learning that requires no\nannotations. Two data augmentation strategies (Mask Perturbation and\nIntra-/Inter-Adversarial Perturbation) are developed to improve the quality of\nnegative samples used in contrastive learning. Overall, DCVLP allows\ncross-modality dense region contrastive learning in a self-supervised setting\nindependent of any object annotations. We compare our method against prior\nvisual-linguistic pretraining frameworks to validate the superiority of dense\ncontrastive learning on multimodal representation learning.", "published": "2021-09-24 07:20:13", "link": "http://arxiv.org/abs/2109.11778v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models", "abstract": "Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for large amounts of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, CPT\nenables strong few-shot and even zero-shot visual grounding capabilities of\nVL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs\noutperform their fine-tuned counterparts by a large margin (e.g., 17.3%\nabsolute accuracy improvement, and 73.8% relative standard deviation reduction\non average with one shot in RefCOCO evaluation). We make the data and code for\nthis paper publicly available at https://github.com/thunlp/CPT.", "published": "2021-09-24 08:07:29", "link": "http://arxiv.org/abs/2109.11797v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Separating Retention from Extraction in the Evaluation of End-to-end\n  Relation Extraction", "abstract": "State-of-the-art NLP models can adopt shallow heuristics that limit their\ngeneralization capability (McCoy et al., 2019). Such heuristics include lexical\noverlap with the training set in Named-Entity Recognition (Taill\\'e et al.,\n2020) and Event or Type heuristics in Relation Extraction (Rosenman et al.,\n2020). In the more realistic end-to-end RE setting, we can expect yet another\nheuristic: the mere retention of training relation triples. In this paper, we\npropose several experiments confirming that retention of known facts is a key\nfactor of performance on standard benchmarks. Furthermore, one experiment\nsuggests that a pipeline model able to use intermediate type representations is\nless prone to over-rely on retention.", "published": "2021-09-24 15:04:39", "link": "http://arxiv.org/abs/2109.12008v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SAIS: Supervising and Augmenting Intermediate Steps for Document-Level\n  Relation Extraction", "abstract": "Stepping from sentence-level to document-level, the research on relation\nextraction (RE) confronts increasing text length and more complicated entity\ninteractions. Consequently, it is more challenging to encode the key\ninformation sources--relevant contexts and entity types. However, existing\nmethods only implicitly learn to model these critical information sources while\nbeing trained for RE. As a result, they suffer the problems of ineffective\nsupervision and uninterpretable model predictions. In contrast, we propose to\nexplicitly teach the model to capture relevant contexts and entity types by\nsupervising and augmenting intermediate steps (SAIS) for RE. Based on a broad\nspectrum of carefully designed tasks, our proposed SAIS method not only\nextracts relations of better quality due to more effective supervision, but\nalso retrieves the corresponding supporting evidence more accurately so as to\nenhance interpretability. By assessing model uncertainty, SAIS further boosts\nthe performance via evidence-based data augmentation and ensemble inference\nwhile reducing the computational cost. Eventually, SAIS delivers\nstate-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and\noutperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval\non DocRED.", "published": "2021-09-24 17:37:35", "link": "http://arxiv.org/abs/2109.12093v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Pretrained Models for Automatic Summarization of\n  Doctor-Patient Conversations", "abstract": "Fine-tuning pretrained models for automatically summarizing doctor-patient\nconversation transcripts presents many challenges: limited training data,\nsignificant domain shift, long and noisy transcripts, and high target summary\nvariability. In this paper, we explore the feasibility of using pretrained\ntransformer models for automatically summarizing doctor-patient conversations\ndirectly from transcripts. We show that fluent and adequate summaries can be\ngenerated with limited training data by fine-tuning BART on a specially\nconstructed dataset. The resulting models greatly surpass the performance of an\naverage human annotator and the quality of previous published work for the\ntask. We evaluate multiple methods for handling long conversations, comparing\nthem to the obvious baseline of truncating the conversation to fit the\npretrained model length limit. We introduce a multistage approach that tackles\nthe task by learning two fine-tuned models: one for summarizing conversation\nchunks into partial summaries, followed by one for rewriting the collection of\npartial summaries into a complete summary. Using a carefully chosen fine-tuning\ndataset, this method is shown to be effective at handling longer conversations,\nimproving the quality of generated summaries. We conduct both an automatic\nevaluation (through ROUGE and two concept-based metrics focusing on medical\nfindings) and a human evaluation (through qualitative examples from literature,\nassessing hallucination, generalization, fluency, and general quality of the\ngenerated summaries).", "published": "2021-09-24 20:18:59", "link": "http://arxiv.org/abs/2109.12174v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient\n  Inference", "abstract": "Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling\nmultilingual translation models to billions of parameters without a\nproportional increase in training computation. However, MoE models are\nprohibitively large and practitioners often resort to methods such as\ndistillation for serving. In this work, we investigate routing strategies at\ndifferent granularity (token, sentence, task) in MoE models to bypass\ndistillation. Experiments on WMT and a web-scale dataset suggest that\ntask-level routing (task-MoE) enables us to extract smaller, ready-to-deploy\nsub-networks from large sparse models. On WMT, our task-MoE with 32 experts\n(533M parameters) outperforms the best performing token-level MoE model\n(token-MoE) by +1.0 BLEU on average across 30 language pairs. The peak\ninference throughput is also improved by a factor of 1.9x when we route by\ntasks instead of tokens. While distilling a token-MoE to a smaller dense model\npreserves only 32% of the BLEU gains, our sub-network task-MoE, by design,\npreserves all the gains with the same inference cost as the distilled student\nmodel. Finally, when scaling up to 200 language pairs, our 128-expert task-MoE\n(13B parameters) performs competitively with a token-level counterpart, while\nimproving the peak inference throughput by a factor of 2.6x.", "published": "2021-09-24 20:42:16", "link": "http://arxiv.org/abs/2110.03742v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integrating Deep Event-Level and Script-Level Information for Script\n  Event Prediction", "abstract": "Scripts are structured sequences of events together with the participants,\nwhich are extracted from the texts.Script event prediction aims to predict the\nsubsequent event given the historical events in the script. Two kinds of\ninformation facilitate this task, namely, the event-level information and the\nscript-level information. At the event level, existing studies view an event as\na verb with its participants, while neglecting other useful properties, such as\nthe state of the participants. At the script level, most existing studies only\nconsider a single event sequence corresponding to one common protagonist. In\nthis paper, we propose a Transformer-based model, called MCPredictor, which\nintegrates deep event-level and script-level information for script event\nprediction. At the event level, MCPredictor utilizes the rich information in\nthe text to obtain more comprehensive event semantic representations. At the\nscript-level, it considers multiple event sequences corresponding to different\nparticipants of the subsequent event. The experimental results on the\nwidely-used New York Times corpus demonstrate the effectiveness and superiority\nof the proposed model.", "published": "2021-09-24 07:37:32", "link": "http://arxiv.org/abs/2110.15706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Proposal of Automatic Error Correction in Text", "abstract": "The great amount of information that can be stored in electronic media is\ngrowing up daily. Many of them is got mainly by typing, such as the huge of\ninformation obtained from web 2.0 sites; or scaned and processing by an Optical\nCharacter Recognition software, like the texts of libraries and goverment\noffices. Both processes introduce error in texts, so it is difficult to use the\ndata for other purposes than just to read it, i.e. the processing of those\ntexts by other applications like e-learning, learning of languages, electronic\ntutorials, data minning, information retrieval and even more specialized\nsystems such as tiflologic software, specifically blinded people-oriented\napplications like automatic reading, where the text would be error free as\npossible in order to make easier the text to speech task, and so on. In this\npaper it is showed an application of automatic recognition and correction of\nortographic errors in electronic texts. This task is composed of three stages:\na) error detection; b) candidate corrections generation; and c) correction\n-selection of the best candidate. The proposal is based in part of speech text\ncategorization, word similarity, word diccionaries, statistical measures,\nmorphologic analisys and n-grams based language model of Spanish.", "published": "2021-09-24 17:17:56", "link": "http://arxiv.org/abs/2112.01846v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AES Systems Are Both Overstable And Oversensitive: Explaining Why And\n  Proposing Defenses", "abstract": "Deep-learning based Automatic Essay Scoring (AES) systems are being actively\nused by states and language testing agencies alike to evaluate millions of\ncandidates for life-changing decisions ranging from college applications to\nvisa approvals. However, little research has been put to understand and\ninterpret the black-box nature of deep-learning based scoring algorithms.\nPrevious studies indicate that scoring models can be easily fooled. In this\npaper, we explore the reason behind their surprising adversarial brittleness.\nWe utilize recent advances in interpretability to find the extent to which\nfeatures such as coherence, content, vocabulary, and relevance are important\nfor automated scoring mechanisms. We use this to investigate the\noversensitivity i.e., large change in output score with a little change in\ninput essay content) and overstability i.e., little change in output scores\nwith large changes in input essay content) of AES. Our results indicate that\nautoscoring models, despite getting trained as \"end-to-end\" models with rich\ncontextual embeddings such as BERT, behave like bag-of-words models. A few\nwords determine the essay score without the requirement of any context making\nthe model largely overstable. This is in stark contrast to recent probing\nstudies on pre-trained representation learning models, which show that rich\nlinguistic features such as parts-of-speech and morphology are encoded by them.\nFurther, we also find that the models have learnt dataset biases, making them\noversensitive. To deal with these issues, we propose detection-based protection\nmodels that can detect oversensitivity and overstability causing samples with\nhigh accuracies. We find that our proposed models are able to detect unusual\nattribution patterns and flag adversarial samples successfully.", "published": "2021-09-24 03:49:38", "link": "http://arxiv.org/abs/2109.11728v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: A\n  Semantic Evidence View", "abstract": "Knowledge Graph Embedding (KGE) aims to learn representations for entities\nand relations. Most KGE models have gained great success, especially on\nextrapolation scenarios. Specifically, given an unseen triple (h, r, t), a\ntrained model can still correctly predict t from (h, r, ?), or h from (?, r,\nt), such extrapolation ability is impressive. However, most existing KGE works\nfocus on the design of delicate triple modeling function, which mainly tells us\nhow to measure the plausibility of observed triples, but offers limited\nexplanation of why the methods can extrapolate to unseen data, and what are the\nimportant factors to help KGE extrapolate. Therefore in this work, we attempt\nto study the KGE extrapolation of two problems: 1. How does KGE extrapolate to\nunseen data? 2. How to design the KGE model with better extrapolation ability?\nFor the problem 1, we first discuss the impact factors for extrapolation and\nfrom relation, entity and triple level respectively, propose three Semantic\nEvidences (SEs), which can be observed from train set and provide important\nsemantic information for extrapolation. Then we verify the effectiveness of SEs\nthrough extensive experiments on several typical KGE methods. For the problem\n2, to make better use of the three levels of SE, we propose a novel GNN-based\nKGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In\nSE-GNN, each level of SE is modeled explicitly by the corresponding neighbor\npattern, and merged sufficiently by the multi-layer aggregation, which\ncontributes to obtaining more extrapolative knowledge representation. Finally,\nthrough extensive experiments on FB15k-237 and WN18RR datasets, we show that\nSE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task\nand performs a better extrapolation ability. Our code is available at\nhttps://github.com/renli1024/SE-GNN.", "published": "2021-09-24 08:17:02", "link": "http://arxiv.org/abs/2109.11800v3", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Rethinking Crowd Sourcing for Semantic Similarity", "abstract": "Estimation of semantic similarity is crucial for a variety of natural\nlanguage processing (NLP) tasks. In the absence of a general theory of semantic\ninformation, many papers rely on human annotators as the source of ground truth\nfor semantic similarity estimation. This paper investigates the ambiguities\ninherent in crowd-sourced semantic labeling. It shows that annotators that\ntreat semantic similarity as a binary category (two sentences are either\nsimilar or not similar and there is no middle ground) play the most important\nrole in the labeling. The paper offers heuristics to filter out unreliable\nannotators and stimulates further discussions on human perception of semantic\nsimilarity.", "published": "2021-09-24 13:57:30", "link": "http://arxiv.org/abs/2109.11969v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7; H.5.2; K.6.1"], "primary_category": "cs.CL"}
{"title": "CLIPort: What and Where Pathways for Robotic Manipulation", "abstract": "How can we imbue robots with the ability to manipulate objects precisely but\nalso to reason about them in terms of abstract concepts? Recent works in\nmanipulation have shown that end-to-end networks can learn dexterous skills\nthat require precise spatial reasoning, but these methods often fail to\ngeneralize to new goals or quickly learn transferable concepts across tasks. In\nparallel, there has been great progress in learning generalizable semantic\nrepresentations for vision and language by training on large-scale internet\ndata, however these representations lack the spatial understanding necessary\nfor fine-grained manipulation. To this end, we propose a framework that\ncombines the best of both worlds: a two-stream architecture with semantic and\nspatial pathways for vision-based manipulation. Specifically, we present\nCLIPort, a language-conditioned imitation-learning agent that combines the\nbroad semantic understanding (what) of CLIP [1] with the spatial precision\n(where) of Transporter [2]. Our end-to-end framework is capable of solving a\nvariety of language-specified tabletop tasks from packing unseen objects to\nfolding cloths, all without any explicit representations of object poses,\ninstance segmentations, memory, symbolic states, or syntactic structures.\nExperiments in simulated and real-world settings show that our approach is data\nefficient in few-shot settings and generalizes effectively to seen and unseen\nsemantic concepts. We even learn one multi-task policy for 10 simulated and 9\nreal-world tasks that is better or comparable to single-task policies.", "published": "2021-09-24 17:44:28", "link": "http://arxiv.org/abs/2109.12098v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "GERNERMED -- An Open German Medical NER Model", "abstract": "The current state of adoption of well-structured electronic health records\nand integration of digital methods for storing medical patient data in\nstructured formats can often considered as inferior compared to the use of\ntraditional, unstructured text based patient data documentation. Data mining in\nthe field of medical data analysis often needs to rely solely on processing of\nunstructured data to retrieve relevant data. In natural language processing\n(NLP), statistical models have been shown successful in various tasks like\npart-of-speech tagging, relation extraction (RE) and named entity recognition\n(NER). In this work, we present GERNERMED, the first open, neural NLP model for\nNER tasks dedicated to detect medical entity types in German text data. Here,\nwe avoid the conflicting goals of protection of sensitive patient data from\ntraining data extraction and the publication of the statistical model weights\nby training our model on a custom dataset that was translated from publicly\navailable datasets in foreign language by a pretrained neural machine\ntranslation model. The sample code and the statistical model is available at:\nhttps://github.com/frankkramer-lab/GERNERMED", "published": "2021-09-24 17:53:47", "link": "http://arxiv.org/abs/2109.12104v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An animated picture says at least a thousand words: Selecting Gif-based\n  Replies in Multimodal Dialog", "abstract": "Online conversations include more than just text. Increasingly, image-based\nresponses such as memes and animated gifs serve as culturally recognized and\noften humorous responses in conversation. However, while NLP has broadened to\nmultimodal models, conversational dialog systems have largely focused only on\ngenerating text replies. Here, we introduce a new dataset of 1.56M text-gif\nconversation turns and introduce a new multimodal conversational model Pepe the\nKing Prawn for selecting gif-based replies. We demonstrate that our model\nproduces relevant and high-quality gif responses and, in a large randomized\ncontrol trial of multiple models replying to real users, we show that our model\nreplies with gifs that are significantly better received by the community.", "published": "2021-09-24 21:48:27", "link": "http://arxiv.org/abs/2109.12212v2", "categories": ["cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Detecting Harmful Memes and Their Targets", "abstract": "Among the various modes of communication in social media, the use of Internet\nmemes has emerged as a powerful means to convey political, psychological, and\nsocio-cultural opinions. Although memes are typically humorous in nature,\nrecent days have witnessed a proliferation of harmful memes targeted to abuse\nvarious social entities. As most harmful memes are highly satirical and\nabstruse without appropriate contexts, off-the-shelf multimodal models may not\nbe adequate to understand their underlying semantics. In this work, we propose\ntwo novel problem formulations: detecting harmful memes and the social entities\nthat these harmful memes target. To this end, we present HarMeme, the first\nbenchmark dataset, containing 3,544 memes related to COVID-19. Each meme went\nthrough a rigorous two-stage annotation process. In the first stage, we labeled\na meme as very harmful, partially harmful, or harmless; in the second stage, we\nfurther annotated the type of target(s) that each harmful meme points to:\nindividual, organization, community, or society/general public/other. The\nevaluation results using ten unimodal and multimodal models highlight the\nimportance of using multimodal signals for both tasks. We further discuss the\nlimitations of these models and we argue that more research is needed to\naddress these problems.", "published": "2021-09-24 17:11:42", "link": "http://arxiv.org/abs/2110.00413v1", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "RuleBert: Teaching Soft Rules to Pre-trained Language Models", "abstract": "While pre-trained language models (PLMs) are the go-to solution to tackle\nmany natural language processing problems, they are still very limited in their\nability to capture and to use common-sense knowledge. In fact, even if\ninformation is available in the form of approximate (soft) logical rules, it is\nnot clear how to transfer it to a PLM in order to improve its performance for\ndeductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how\nto reason with soft Horn rules. We introduce a classification task where, given\nfacts and soft rules, the PLM should return a prediction with a probability for\na given hypothesis. We release the first dataset for this task, and we propose\na revised loss function that enables the PLM to learn how to predict precise\nprobabilities for the task. Our evaluation results show that the resulting\nfine-tuned models achieve very high performance, even on logical rules that\nwere unseen at training. Moreover, we demonstrate that logical notions\nexpressed by the rules are transferred to the fine-tuned model, yielding\nstate-of-the-art results on external datasets.", "published": "2021-09-24 16:19:25", "link": "http://arxiv.org/abs/2109.13006v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO", "cs.NE", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.AI"}
{"title": "Causal Analysis of Carnatic Music: A Preliminary Study", "abstract": "The musicological analysis of Carnatic music is challenging, owing to its\nrich structure and complexity. Automated \\textit{r\\=aga} classification, pitch\ndetection, tonal analysis, modelling and information retrieval of this form of\nsouthern Indian classical music have, however, made significant progress in\nrecent times. A causal analysis to investigate the musicological structure of\nCarnatic compositions and the identification of the relationships embedded in\nthem have never been previously attempted. In this study, we propose a novel\nframework for causal discovery, using a compression-complexity measure. Owing\nto the limited number of compositions available, however, we generated\nsurrogates to further facilitate the analysis of the prevailing causal\nrelationships. Our analysis indicates that the context-free grammar, inferred\nfrom more complex compositions, such as the \\textit{M\\=e\\d{l}akarta}\n\\textit{r\\=aga}, are a \\textit{structural cause} for the \\textit{Janya}\n\\textit{r\\=aga}. We also analyse certain special cases of the \\textit{Janya\nr\\=aga} in order to understand their origins and structure better.", "published": "2021-09-24 07:24:12", "link": "http://arxiv.org/abs/2109.11782v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating X-vector-based Speaker Anonymization under White-box\n  Assessment", "abstract": "In the scenario of the Voice Privacy challenge, anonymization is achieved by\nconverting all utterances from a source speaker to match the same target\nidentity; this identity being randomly selected. In this context, an attacker\nwith maximum knowledge about the anonymization system can not infer the target\nidentity. This article proposed to constrain the target selection to a specific\nidentity, i.e., removing the random selection of identity, to evaluate the\nextreme threat under a whitebox assessment (the attacker has complete knowledge\nabout the system). Targeting a unique identity also allows us to investigate\nwhether some target's identities are better than others to anonymize a given\nspeaker.", "published": "2021-09-24 13:08:07", "link": "http://arxiv.org/abs/2109.11946v2", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A data acquisition setup for data driven acoustic design", "abstract": "In this paper, we present a novel interdisciplinary approach to study the\nrelationship between diffusive surface structures and their acoustic\nperformance. Using computational design, surface structures are iteratively\ngenerated and 3D printed at 1:10 model scale. They originate from different\nfabrication typologies and are designed to have acoustic diffusion and\nabsorption effects. An automated robotic process measures the impulse responses\nof these surfaces by positioning a microphone and a speaker at multiple\nlocations. The collected data serves two purposes: first, as an exploratory\ncatalogue of different spatio-temporal-acoustic scenarios and second, as data\nset for predicting the acoustic response of digitally designed surface\ngeometries using machine learning. In this paper, we present the automated data\nacquisition setup, the data processing and the computational generation of\ndiffusive surface structures. We describe first results of comparative studies\nof measured surface panels and conclude with steps of future research.", "published": "2021-09-24 15:20:02", "link": "http://arxiv.org/abs/2109.12014v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parameterized Channel Normalization for Far-field Deep Speaker\n  Verification", "abstract": "We address far-field speaker verification with deep neural network (DNN)\nbased speaker embedding extractor, where mismatch between enrollment and test\ndata often comes from convolutive effects (e.g. room reverberation) and noise.\nTo mitigate these effects, we focus on two parametric normalization methods:\nper-channel energy normalization (PCEN) and parameterized cepstral mean\nnormalization (PCMN). Both methods contain differentiable parameters and thus\ncan be conveniently integrated to, and jointly optimized with the DNN using\nautomatic differentiation methods. We consider both fixed and trainable\n(data-driven) variants of each method. We evaluate the performance on Hi-MIA, a\nrecent large-scale far-field speech corpus, with varied microphone and\npositional settings. Our methods outperform conventional mel filterbank\nfeatures, with maximum of 33.5% and 39.5% relative improvement on equal error\nrate under matched microphone and mismatched microphone conditions,\nrespectively.", "published": "2021-09-24 16:22:31", "link": "http://arxiv.org/abs/2109.12056v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimized Power Normalized Cepstral Coefficients towards Robust Deep\n  Speaker Verification", "abstract": "After their introduction to robust speech recognition, power normalized\ncepstral coefficient (PNCC) features were successfully adopted to other tasks,\nincluding speaker verification. However, as a feature extractor with long-term\noperations on the power spectrogram, its temporal processing and amplitude\nscaling steps dedicated on environmental compensation may be redundant.\nFurther, they might suppress intrinsic speaker variations that are useful for\nspeaker verification based on deep neural networks (DNN). Therefore, in this\nstudy, we revisit and optimize PNCCs by ablating its medium-time processor and\nby introducing channel energy normalization. Experimental results with a\nDNN-based speaker verification system indicate substantial improvement over\nbaseline PNCCs on both in-domain and cross-domain scenarios, reflected by\nrelatively 5.8% and 61.2% maximum lower equal error rate on VoxCeleb1 and\nVoxMovies, respectively.", "published": "2021-09-24 16:26:12", "link": "http://arxiv.org/abs/2109.12058v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visual Scene Graphs for Audio Source Separation", "abstract": "State-of-the-art approaches for visually-guided audio source separation\ntypically assume sources that have characteristic sounds, such as musical\ninstruments. These approaches often ignore the visual context of these sound\nsources or avoid modeling object interactions that may be useful to better\ncharacterize the sources, especially when the same object class may produce\nvaried sounds from distinct interactions. To address this challenging problem,\nwe propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning\nmodel that embeds the visual structure of the scene as a graph and segments\nthis graph into subgraphs, each subgraph being associated with a unique sound\nobtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a\nrecursive neural network that emits mutually-orthogonal sub-graph embeddings of\nthe visual graph using multi-head attention. These embeddings are used for\nconditioning an audio encoder-decoder towards source separation. Our pipeline\nis trained end-to-end via a self-supervised task consisting of separating audio\nsources using the visual graph from artificially mixed sounds. In this paper,\nwe also introduce an \"in the wild'' video dataset for sound source separation\nthat contains multiple non-musical sources, which we call Audio Separation in\nthe Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and\nprovides a challenging, natural, and daily-life setting for source separation.\nThorough experiments on the proposed ASIW and the standard MUSIC datasets\ndemonstrate state-of-the-art sound separation performance of our method against\nrecent prior approaches.", "published": "2021-09-24 13:40:51", "link": "http://arxiv.org/abs/2109.11955v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
