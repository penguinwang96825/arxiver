{"title": "Enabling On-Device Large Language Model Personalization with\n  Self-Supervised Data Selection and Synthesis", "abstract": "After a large language model (LLM) is deployed on edge devices, it is\ndesirable for these devices to learn from user-generated conversation data to\ngenerate user-specific and personalized responses in real-time. However,\nuser-generated data usually contains sensitive and private information, and\nuploading such data to the cloud for annotation is not preferred if not\nprohibited. While it is possible to obtain annotation locally by directly\nasking users to provide preferred responses, such annotations have to be sparse\nto not affect user experience. In addition, the storage of edge devices is\nusually too limited to enable large-scale fine-tuning with full user-generated\ndata. It remains an open question how to enable on-device LLM personalization,\nconsidering sparse annotation and limited on-device storage. In this paper, we\npropose a novel framework to select and store the most representative data\nonline in a self-supervised way. Such data has a small memory footprint and\nallows infrequent requests of user annotations for further fine-tuning. To\nenhance fine-tuning quality, multiple semantically similar pairs of question\ntexts and expected responses are generated using the LLM. Our experiments show\nthat the proposed framework achieves the best user-specific content-generating\ncapability (accuracy) and fine-tuning speed (performance) compared with vanilla\nbaselines. To the best of our knowledge, this is the very first on-device LLM\npersonalization framework.", "published": "2023-11-21 01:34:02", "link": "http://arxiv.org/abs/2311.12275v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AcademicGPT: Empowering Academic Research", "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Yet, many of these advanced\nLLMs are tailored for broad, general-purpose applications. In this technical\nreport, we introduce AcademicGPT, designed specifically to empower academic\nresearch. AcademicGPT is a continual training model derived from LLaMA2-70B.\nOur training corpus mainly consists of academic papers, thesis, content from\nsome academic domain, high-quality Chinese data and others. While it may not be\nextensive in data scale, AcademicGPT marks our initial venture into a\ndomain-specific GPT tailored for research area. We evaluate AcademicGPT on\nseveral established public benchmarks such as MMLU and CEval, as well as on\nsome specialized academic benchmarks like PubMedQA, SCIEval, and our\nnewly-created ComputerScienceQA, to demonstrate its ability from general\nknowledge ability, to Chinese ability, and to academic ability. Building upon\nAcademicGPT's foundation model, we also developed several applications catered\nto the academic area, including General Academic Question Answering,\nAI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract\nGeneration.", "published": "2023-11-21 03:17:14", "link": "http://arxiv.org/abs/2311.12315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Turing: A Comparative Analysis of Approaches for Detecting\n  Machine-Generated Text", "abstract": "Significant progress has been made on text generation by pre-trained language\nmodels (PLMs), yet distinguishing between human and machine-generated text\nposes an escalating challenge. This paper offers an in-depth evaluation of\nthree distinct methods used to address this task: traditional shallow learning,\nLanguage Model (LM) fine-tuning, and Multilingual Model fine-tuning. These\napproaches are rigorously tested on a wide range of machine-generated texts,\nproviding a benchmark of their competence in distinguishing between\nhuman-authored and machine-authored linguistic constructs. The results reveal\nconsiderable differences in performance across methods, thus emphasizing the\ncontinued need for advancement in this crucial area of NLP. This study offers\nvaluable insights and paves the way for future research aimed at creating\nrobust and highly discriminative models.", "published": "2023-11-21 06:23:38", "link": "http://arxiv.org/abs/2311.12373v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Obscure Limitation of Modular Multilingual Language Models", "abstract": "We expose the limitation of modular multilingual language models (MLMs) in\nmultilingual inference scenarios with unknown languages. Existing evaluations\nof modular MLMs exclude the involvement of language identification (LID)\nmodules, which obscures the performance of real-case multilingual scenarios of\nmodular MLMs. In this work, we showcase the effect of adding LID on the\nmultilingual evaluation of modular MLMs and provide discussions for closing the\nperformance gap of caused by the pipelined approach of LID and modular MLMs.", "published": "2023-11-21 06:27:25", "link": "http://arxiv.org/abs/2311.12375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Problems of Non-equivalent Words in Technical Translation", "abstract": "Translating words which do not have equivalent in target language is not easy\nand finding proper equivalent of those words are very important to render\ncorrectly and understandably, the article defines some thoughts and ideas of\nscientists on the common problems of non-equivalent words from English to\nRussian language and includes English and Russian examples and ideas of certain\nscientist. The English language is worldwide spoken and there are 1.35 billion\nEnglish speakers and over 258 million Russian speakers according to the 2021s\nstatistics. Inevitably, these billions of speakers around the world have\nconnection and they may have deal in different criteria. In order to understand\none another they need to have a pure and fully-understood language. These pure\nlanguages understanding directly relates to translation knowledge where\nlinguists and translators need to work and research to eradicate\nmisunderstanding. Misunderstandings mostly appear in non-equivalent words\nbecause there are different local and internal words like food, garment,\ncultural and traditional words and others in every notion. Truly, most of these\nwords do not have equivalent in the target language and these words need to be\nworked and find their equivalent in the target language to fully understand the\nboth languages. However, some of these non-equivalent words are already\nprofessionally rendered to the target language but still there many other words\nto be rendered. Hence, this research paper includes different ways and rules of\nrendering non-equivalent words from source language to the target language.", "published": "2023-11-21 07:11:39", "link": "http://arxiv.org/abs/2311.12395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian\n  Local Languages", "abstract": "Significant progress has been made on Indonesian NLP. Nevertheless,\nexploration of the code-mixing phenomenon in Indonesian is limited, despite\nmany languages being frequently mixed with Indonesian in daily conversation. In\nthis work, we explore code-mixing in Indonesian with four embedded languages,\ni.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta, a\nframework to evaluate and improve the code-mixing robustness. Our analysis\nshows that the pre-training corpus bias affects the model's ability to better\nhandle Indonesian-English code-mixing when compared to other local languages,\ndespite having higher language diversity.", "published": "2023-11-21 07:50:53", "link": "http://arxiv.org/abs/2311.12405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Word Embeddings for Low-Resource Languages using Anchors\n  and a Chain of Related Languages", "abstract": "Very low-resource languages, having only a few million tokens worth of data,\nare not well-supported by multilingual NLP approaches due to poor quality\ncross-lingual word representations. Recent work showed that good cross-lingual\nperformance can be achieved if a source language is related to the low-resource\ntarget language. However, not all language pairs are related. In this paper, we\npropose to build multilingual word embeddings (MWEs) via a novel language\nchain-based approach, that incorporates intermediate related languages to\nbridge the gap between the distant source and target. We build MWEs one\nlanguage at a time by starting from the resource rich source and sequentially\nadding each language in the chain till we reach the target. We extend a\nsemi-joint bilingual approach to multiple languages in order to eliminate the\nmain weakness of previous works, i.e., independently trained monolingual\nembeddings, by anchoring the target language around the multilingual space. We\nevaluate our method on bilingual lexicon induction for 4 language families,\ninvolving 4 very low-resource (<5M tokens) and 4 moderately low-resource (<50M)\ntarget languages, showing improved performance in both categories.\nAdditionally, our analysis reveals the importance of good quality embeddings\nfor intermediate languages as well as the importance of leveraging anchor\npoints from all languages in the multilingual space.", "published": "2023-11-21 09:59:29", "link": "http://arxiv.org/abs/2311.12489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation Metrics of Language Generation Models for Synthetic Traffic\n  Generation Tasks", "abstract": "Many Natural Language Generation (NLG) tasks aim to generate a single output\ntext given an input prompt. Other settings require the generation of multiple\ntexts, e.g., for Synthetic Traffic Generation (STG). This generation task is\ncrucial for training and evaluating QA systems as well as conversational\nagents, where the goal is to generate multiple questions or utterances\nresembling the linguistic variability of real users. In this paper, we show\nthat common NLG metrics, like BLEU, are not suitable for evaluating STG. We\npropose and evaluate several metrics designed to compare the generated traffic\nto the distribution of real user texts. We validate our metrics with an\nautomatic procedure to verify whether they capture different types of quality\nissues of generated data; we also run human annotations to verify the\ncorrelation with human judgements. Experiments on three tasks, i.e., Shopping\nUtterance Generation, Product Question Generation and Query Auto Completion,\ndemonstrate that our metrics are effective for evaluating STG tasks, and\nimprove the agreement with human judgement up to 20% with respect to common NLG\nmetrics. We believe these findings can pave the way towards better solutions\nfor estimating the representativeness of synthetic text data.", "published": "2023-11-21 11:26:26", "link": "http://arxiv.org/abs/2311.12534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MathGloss: Building mathematical glossaries from text", "abstract": "MathGloss is a project to create a knowledge graph (KG) for undergraduate\nmathematics from text, automatically, using modern natural language processing\n(NLP) tools and resources already available on the web. MathGloss is a linked\ndatabase of undergraduate concepts in mathematics. So far, it combines five\nresources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph\nhosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses\nat the University of Chicago, (iii) the syllabus of the French undergraduate\nmathematics curriculum which includes hyperlinks to the automated theorem\nprover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by\nmathematicians, and (v) the nLab, a wiki for category theory also curated by\nmathematicians. MathGloss's goal is to bring together resources for learning\nmathematics and to allow every mathematician to tailor their learning to their\nown preferences. Moreover, by organizing different resources for learning\nundergraduate mathematics alongside those for learning formal mathematics, we\nhope to make it easier for mathematicians and formal tools (theorem provers,\ncomputer algebra systems, etc) experts to \"understand\" each other and break\ndown some of the barriers to formal math.", "published": "2023-11-21 14:49:00", "link": "http://arxiv.org/abs/2311.12649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource\n  Sentiment Analysis of Bangla Language", "abstract": "This paper describes the system of the LowResource Team for Task 2 of\nBLP-2023, which involves conducting sentiment analysis on a dataset composed of\npublic posts and comments from diverse social media platforms. Our primary aim\nis to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus,\nusing various strategies including fine-tuning, dropping random tokens, and\nusing several external datasets. Our final model is an ensemble of the three\nbest BanglaBert variations. Our system has achieved overall 3rd in the Test Set\namong 30 participating teams with a score of 0.718. Additionally, we discuss\nthe promising systems that didn't perform well namely task-adaptive pertaining\nand paraphrasing using BanglaT5. Training codes and external datasets which are\nused for our system are publicly available at\nhttps://github.com/Aunabil4602/bnlp-workshop-task2-2023", "published": "2023-11-21 17:21:15", "link": "http://arxiv.org/abs/2311.12735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Systematic word meta-sense extension", "abstract": "The meaning of polysemous words often varies in a highly productive yet\npredictable way. Generalizing the regularity between conventional senses to\nderive novel word meaning is crucial for automated processing of non-literal\nlanguage uses such as figurative expressions. We introduce a novel task called\nsystematic word meta-sense extension (SWORME) to test and improve language\nmodels' ability to extend word meaning to denote new semantic domains (also\ncalled meta-senses) that bear regular semantic relations with existing senses.\nWe found that language models prefer incremental lexical semantic change toward\nconceptually similar meta-senses such as logical metonymy, and are much worse\nat predicting highly non-literal meaning extensions such as metaphors. We\npropose a novel analogy-based method of word meaning extension, and show that\nit effectively improves language model systematicity in making both gradual and\nradical types of meta-sense extension. We further demonstrate that learning\nsystematic meta-sense extensions benefits language models on multiple\nbenchmarks of figurative language understanding.", "published": "2023-11-21 22:30:37", "link": "http://arxiv.org/abs/2311.13029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Text: Unveiling Multimodal Proficiency of Large Language Models\n  with MultiAPI Benchmark", "abstract": "The proliferation of Large Language Models like ChatGPT has significantly\nadvanced language understanding and generation, impacting a broad spectrum of\napplications. However, these models predominantly excel in text-based tasks,\noverlooking the complexity of real-world multimodal information. This study\nintroduces MultiAPI, a pioneering comprehensive large-scale API benchmark\ndataset aimed at expanding LLMs' proficiency in multimodal contexts. Developed\ncollaboratively through ChatGPT, MultiAPI consists of 235 diverse API calls and\n2,038 contextual prompts, offering a unique platform evaluation of\ntool-augmented LLMs handling multimodal tasks. Through comprehensive\nexperiments, our findings reveal that while LLMs demonstrate proficiency in API\ncall decision-making, they face challenges in domain identification, function\nselection, and argument generation. What's more, we surprisingly notice that\nauxiliary context can actually impair the performance. An in-depth error\nanalysis paves the way for a new paradigm to address these challenges,\nsuggesting a potential direction for future LLM research.", "published": "2023-11-21 23:26:05", "link": "http://arxiv.org/abs/2311.13053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attribution and Alignment: Effects of Local Context Repetition on\n  Utterance Production and Comprehension in Dialogue", "abstract": "Language models are often used as the backbone of modern dialogue systems.\nThese models are pre-trained on large amounts of written fluent language.\nRepetition is typically penalised when evaluating language model generations.\nHowever, it is a key component of dialogue. Humans use local and partner\nspecific repetitions; these are preferred by human users and lead to more\nsuccessful communication in dialogue. In this study, we evaluate (a) whether\nlanguage models produce human-like levels of repetition in dialogue, and (b)\nwhat are the processing mechanisms related to lexical re-use they use during\ncomprehension. We believe that such joint analysis of model production and\ncomprehension behaviour can inform the development of cognitively inspired\ndialogue generation systems.", "published": "2023-11-21 23:50:33", "link": "http://arxiv.org/abs/2311.13061v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for\n  Interdisciplinary Science", "abstract": "Large language models record impressive performance on many natural language\nprocessing tasks. However, their knowledge capacity is limited to the\npretraining corpus. Retrieval augmentation offers an effective solution by\nretrieving context from external knowledge sources to complement the language\nmodel. However, existing retrieval augmentation techniques ignore the\nstructural relationships between these documents. Furthermore, retrieval models\nare not explored much in scientific tasks, especially in regard to the\nfaithfulness of retrieved documents. In this paper, we propose a novel\nstructure-aware retrieval augmented language model that accommodates document\nstructure during retrieval augmentation. We create a heterogeneous document\ngraph capturing multiple types of relationships (e.g., citation, co-authorship,\netc.) that connect documents from more than 15 scientific disciplines (e.g.,\nPhysics, Medicine, Chemistry, etc.). We train a graph neural network on the\ncurated document graph to act as a structural encoder for the corresponding\npassages retrieved during the model pretraining. Particularly, along with text\nembeddings of the retrieved passages, we obtain structural embeddings of the\ndocuments (passages) and fuse them together before feeding them to the language\nmodel. We evaluate our model extensively on various scientific benchmarks that\ninclude science question-answering and scientific document classification\ntasks. Experimental results demonstrate that structure-aware retrieval improves\nretrieving more coherent, faithful and contextually relevant passages, while\nshowing a comparable performance in the overall accuracy.", "published": "2023-11-21 02:02:46", "link": "http://arxiv.org/abs/2311.12289v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Noise in Relation Classification Dataset TACRED: Characterization and\n  Reduction", "abstract": "The overarching objective of this paper is two-fold. First, to explore\nmodel-based approaches to characterize the primary cause of the noise. in the\nRE dataset TACRED Second, to identify the potentially noisy instances. Towards\nthe first objective, we analyze predictions and performance of state-of-the-art\n(SOTA) models to identify the root cause of noise in the dataset. Our analysis\nof TACRED shows that the majority of the noise in the dataset originates from\nthe instances labeled as no-relation which are negative examples. For the\nsecond objective, we explore two nearest-neighbor-based strategies to\nautomatically identify potentially noisy examples for elimination and\nreannotation. Our first strategy, referred to as Intrinsic Strategy (IS), is\nbased on the assumption that positive examples are clean. Thus, we have used\nfalse-negative predictions to identify noisy negative examples. Whereas, our\nsecond approach, referred to as Extrinsic Strategy, is based on using a clean\nsubset of the dataset to identify potentially noisy negative examples. Finally,\nwe retrained the SOTA models on the eliminated and reannotated dataset. Our\nempirical results based on two SOTA models trained on TACRED-E following the IS\nshow an average 4% F1-score improvement, whereas reannotation (TACRED-R) does\nnot improve the original results. However, following ES, SOTA models show the\naverage F1-score improvement of 3.8% and 4.4% when trained on respective\neliminated (TACRED-EN) and reannotated (TACRED-RN) datasets respectively. We\nfurther extended the ES for cleaning positive examples as well, which resulted\nin an average performance improvement of 5.8% and 5.6% for the eliminated\n(TACRED-ENP) and reannotated (TACRED-RNP) datasets respectively.", "published": "2023-11-21 02:35:09", "link": "http://arxiv.org/abs/2311.12298v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Smaller Language Models Answer Contextualised Questions Through\n  Memorisation Or Generalisation?", "abstract": "A distinction is often drawn between a model's ability to predict a label for\nan evaluation sample that is directly memorised from highly similar training\nsamples versus an ability to predict the label via some method of\ngeneralisation. In the context of using Language Models for question-answering,\ndiscussion continues to occur as to the extent to which questions are answered\nthrough memorisation. We consider this issue for questions that would ideally\nbe answered through reasoning over an associated context. We propose a method\nof identifying evaluation samples for which it is very unlikely our model would\nhave memorised the answers. Our method is based on semantic similarity of input\ntokens and label tokens between training and evaluation samples. We show that\nour method offers advantages upon some prior approaches in that it is able to\nsurface evaluation-train pairs that have overlap in either contiguous or\ndiscontiguous sequences of tokens. We use this method to identify unmemorisable\nsubsets of our evaluation datasets. We train two Language Models in a multitask\nfashion whereby the second model differs from the first only in that it has two\nadditional datasets added to the training regime that are designed to impart\nsimple numerical reasoning strategies of a sort known to improve performance on\nsome of our evaluation datasets but not on others. We then show that there is\nperformance improvement between the two models on the unmemorisable subsets of\nthe evaluation datasets that were expected to benefit from the additional\ntraining datasets. Specifically, performance on unmemorisable subsets of two of\nour evaluation datasets, DROP and ROPES significantly improves by 9.0%, and\n25.7% respectively while other evaluation datasets have no significant change\nin performance.", "published": "2023-11-21 04:06:08", "link": "http://arxiv.org/abs/2311.12337v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing Transformer Architecture in Long-Context Large Language\n  Models: A Comprehensive Survey", "abstract": "Transformer-based Large Language Models (LLMs) have been applied in diverse\nareas such as knowledge bases, human interfaces, and dynamic agents, and\nmarking a stride towards achieving Artificial General Intelligence (AGI).\nHowever, current LLMs are predominantly pretrained on short text snippets,\nwhich compromises their effectiveness in processing the long-context prompts\nthat are frequently encountered in practical scenarios. This article offers a\ncomprehensive survey of the recent advancement in Transformer-based LLM\narchitectures aimed at enhancing the long-context capabilities of LLMs\nthroughout the entire model lifecycle, from pre-training through to inference.\nWe first delineate and analyze the problems of handling long-context input and\noutput with the current Transformer-based models. We then provide a taxonomy\nand the landscape of upgrades on Transformer architecture to solve these\nproblems. Afterwards, we provide an investigation on wildly used evaluation\nnecessities tailored for long-context LLMs, including datasets, metrics, and\nbaseline models, as well as optimization toolkits such as libraries,\nframeworks, and compilers to boost the efficacy of LLMs across different stages\nin runtime. Finally, we discuss the challenges and potential avenues for future\nresearch. A curated repository of relevant literature, continuously updated, is\navailable at https://github.com/Strivin0311/long-llms-learning.", "published": "2023-11-21 04:59:17", "link": "http://arxiv.org/abs/2311.12351v2", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6; I.2.11"], "primary_category": "cs.CL"}
{"title": "InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk\n  Factors in Reddit Posts", "abstract": "Mental health professionals and clinicians have observed the upsurge of\nmental disorders due to Interpersonal Risk Factors (IRFs). To simulate the\nhuman-in-the-loop triaging scenario for early detection of mental health\ndisorders, we recognized textual indications to ascertain these IRFs : Thwarted\nBelongingness (TBe) and Perceived Burdensomeness (PBu) within personal\nnarratives. In light of this, we use N-shot learning with GPT-3 model on the\nIRF dataset, and underscored the importance of fine-tuning GPT-3 model to\nincorporate the context-specific sensitivity and the interconnectedness of\ntextual cues that represent both IRFs.\n  In this paper, we introduce an Interpretable Prompting (InterPrompt)} method\nto boost the attention mechanism by fine-tuning the GPT-3 model. This allows a\nmore sophisticated level of language modification by adjusting the pre-trained\nweights. Our model learns to detect usual patterns and underlying connections\nacross both the IRFs, which leads to better system-level explainability and\ntrustworthiness. The results of our research demonstrate that all four variants\nof GPT-3 model, when fine-tuned with InterPrompt, perform considerably better\nas compared to the baseline methods, both in terms of classification and\nexplanation generation.", "published": "2023-11-21 07:43:50", "link": "http://arxiv.org/abs/2311.12404v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Visual Analytics for Generative Transformer Models", "abstract": "While transformer-based models have achieved state-of-the-art results in a\nvariety of classification and generation tasks, their black-box nature makes\nthem challenging for interpretability. In this work, we present a novel visual\nanalytical framework to support the analysis of transformer-based generative\nnetworks. In contrast to previous work, which has mainly focused on\nencoder-based models, our framework is one of the first dedicated to supporting\nthe analysis of transformer-based encoder-decoder models and decoder-only\nmodels for generative and classification tasks. Hence, we offer an intuitive\noverview that allows the user to explore different facets of the model through\ninteractive visualization. To demonstrate the feasibility and usefulness of our\nframework, we present three detailed case studies based on real-world NLP\nresearch problems.", "published": "2023-11-21 08:15:01", "link": "http://arxiv.org/abs/2311.12418v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild", "abstract": "Speech is considered as a multi-modal process where hearing and vision are\ntwo fundamentals pillars. In fact, several studies have demonstrated that the\nrobustness of Automatic Speech Recognition systems can be improved when audio\nand visual cues are combined to represent the nature of speech. In addition,\nVisual Speech Recognition, an open research problem whose purpose is to\ninterpret speech by reading the lips of the speaker, has been a focus of\ninterest in the last decades. Nevertheless, in order to estimate these systems\nin the currently Deep Learning era, large-scale databases are required. On the\nother hand, while most of these databases are dedicated to English, other\nlanguages lack sufficient resources. Thus, this paper presents a\nsemi-automatically annotated audiovisual database to deal with unconstrained\nnatural Spanish, providing 13 hours of data extracted from Spanish television.\nFurthermore, baseline results for both speaker-dependent and\nspeaker-independent scenarios are reported using Hidden Markov Models, a\ntraditional paradigm that has been widely used in the field of Speech\nTechnologies.", "published": "2023-11-21 09:12:21", "link": "http://arxiv.org/abs/2311.12457v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Analysis of Visual Features for Continuous Lipreading in Spanish", "abstract": "During a conversation, our brain is responsible for combining information\nobtained from multiple senses in order to improve our ability to understand the\nmessage we are perceiving. Different studies have shown the importance of\npresenting visual information in these situations. Nevertheless, lipreading is\na complex task whose objective is to interpret speech when audio is not\navailable. By dispensing with a sense as crucial as hearing, it will be\nnecessary to be aware of the challenge that this lack presents. In this paper,\nwe propose an analysis of different speech visual features with the intention\nof identifying which of them is the best approach to capture the nature of lip\nmovements for natural Spanish and, in this way, dealing with the automatic\nvisual speech recognition task. In order to estimate our system, we present an\naudiovisual corpus compiled from a subset of the RTVE database, which has been\nused in the Albayz\\'in evaluations. We employ a traditional system based on\nHidden Markov Models with Gaussian Mixture Models. Results show that, although\nthe task is difficult, in restricted conditions we obtain recognition results\nwhich determine that using eigenlips in combination with deep features is the\nbest visual approach.", "published": "2023-11-21 09:28:00", "link": "http://arxiv.org/abs/2311.12468v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CSMeD: Bridging the Dataset Gap in Automated Citation Screening for\n  Systematic Literature Reviews", "abstract": "Systematic literature reviews (SLRs) play an essential role in summarising,\nsynthesising and validating scientific evidence. In recent years, there has\nbeen a growing interest in using machine learning techniques to automate the\nidentification of relevant studies for SLRs. However, the lack of standardised\nevaluation datasets makes comparing the performance of such automated\nliterature screening systems difficult. In this paper, we analyse the citation\nscreening evaluation datasets, revealing that many of the available datasets\nare either too small, suffer from data leakage or have limited applicability to\nsystems treating automated literature screening as a classification task, as\nopposed to, for example, a retrieval or question-answering task. To address\nthese challenges, we introduce CSMeD, a meta-dataset consolidating nine\npublicly released collections, providing unified access to 325 SLRs from the\nfields of medicine and computer science. CSMeD serves as a comprehensive\nresource for training and evaluating the performance of automated citation\nscreening models. Additionally, we introduce CSMeD-FT, a new dataset designed\nexplicitly for evaluating the full text publication screening task. To\ndemonstrate the utility of CSMeD, we conduct experiments and establish\nbaselines on new datasets.", "published": "2023-11-21 09:36:11", "link": "http://arxiv.org/abs/2311.12474v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with\n  Unassimilated Loanwords", "abstract": "While WangchanBERTa has become the de facto standard in transformer-based\nThai language modeling, it still has shortcomings in regard to the\nunderstanding of foreign words, most notably English words, which are often\nborrowed without orthographic assimilation into Thai in many contexts. We\nidentify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the\nmain source of these shortcomings. We then expand WangchanBERTa's vocabulary\nvia vocabulary transfer from XLM-R's pretrained tokenizer and pretrain a new\nmodel using the expanded tokenizer, starting from WangchanBERTa's checkpoint,\non a new dataset that is larger than the one used to train WangchanBERTa. Our\nresults show that our new pretrained model, PhayaThaiBERT, outperforms\nWangchanBERTa in many downstream tasks and datasets.", "published": "2023-11-21 09:37:42", "link": "http://arxiv.org/abs/2311.12475v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Oasis: Data Curation and Assessment System for Pretraining of Large\n  Language Models", "abstract": "Data is one of the most critical elements in building a large language model.\nHowever, existing systems either fail to customize a corpus curation pipeline\nor neglect to leverage comprehensive corpus assessment for iterative\noptimization of the curation. To this end, we present a pretraining corpus\ncuration and assessment platform called Oasis -- a one-stop system for data\nquality improvement and quantification with user-friendly interactive\ninterfaces. Specifically, the interactive modular rule filter module can devise\ncustomized rules according to explicit feedback. The debiased neural filter\nmodule builds the quality classification dataset in a negative-centric manner\nto remove the undesired bias. The adaptive document deduplication module could\nexecute large-scale deduplication with limited memory resources. These three\nparts constitute the customized data curation module. And in the holistic data\nassessment module, a corpus can be assessed in local and global views, with\nthree evaluation means including human, GPT-4, and heuristic metrics. We\nexhibit a complete process to use Oasis for the curation and assessment of\npretraining data. In addition, an 800GB bilingual corpus curated by Oasis is\npublicly released.", "published": "2023-11-21 11:32:23", "link": "http://arxiv.org/abs/2311.12537v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IMGTB: A Framework for Machine-Generated Text Detection Benchmarking", "abstract": "In the era of large language models generating high quality texts, it is a\nnecessity to develop methods for detection of machine-generated text to avoid\nharmful use or simply due to annotation purposes. It is, however, also\nimportant to properly evaluate and compare such developed methods. Recently, a\nfew benchmarks have been proposed for this purpose; however, integration of\nnewest detection methods is rather challenging, since new methods appear each\nmonth and provide slightly different evaluation pipelines. In this paper, we\npresent the IMGTB framework, which simplifies the benchmarking of\nmachine-generated text detection methods by easy integration of custom (new)\nmethods and evaluation datasets. Its configurability and flexibility makes\nresearch and development of new detection methods easier, especially their\ncomparison to the existing state-of-the-art detectors. The default set of\nanalyses, metrics and visualizations offered by the tool follows the\nestablished practices of machine-generated text detection benchmarking found in\nstate-of-the-art literature.", "published": "2023-11-21 12:40:01", "link": "http://arxiv.org/abs/2311.12574v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The DURel Annotation Tool: Human and Computational Measurement of\n  Semantic Proximity, Sense Clusters and Semantic Change", "abstract": "We present the DURel tool that implements the annotation of semantic\nproximity between uses of words into an online, open source interface. The tool\nsupports standardized human annotation as well as computational annotation,\nbuilding on recent advances with Word-in-Context models. Annotator judgments\nare clustered with automatic graph clustering techniques and visualized for\nanalysis. This allows to measure word senses with simple and intuitive\nmicro-task judgments between use pairs, requiring minimal preparation efforts.\nThe tool offers additional functionalities to compare the agreement between\nannotators to guarantee the inter-subjectivity of the obtained judgments and to\ncalculate summary statistics giving insights into sense frequency\ndistributions, semantic variation or changes of senses over time.", "published": "2023-11-21 15:14:54", "link": "http://arxiv.org/abs/2311.12664v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Soft Random Sampling: A Theoretical and Empirical Analysis", "abstract": "Soft random sampling (SRS) is a simple yet effective approach for efficient\ntraining of large-scale deep neural networks when dealing with massive data.\nSRS selects a subset uniformly at random with replacement from the full data\nset in each epoch. In this paper, we conduct a theoretical and empirical\nanalysis of SRS. First, we analyze its sampling dynamics including data\ncoverage and occupancy. Next, we investigate its convergence with non-convex\nobjective functions and give the convergence rate. Finally, we provide its\ngeneralization performance. We empirically evaluate SRS for image recognition\non CIFAR10 and automatic speech recognition on Librispeech and an in-house\npayload dataset to demonstrate its effectiveness. Compared to existing\ncoreset-based data selection methods, SRS offers a better accuracy-efficiency\ntrade-off. Especially on real-world industrial scale data sets, it is shown to\nbe a powerful training strategy with significant speedup and competitive\nperformance with almost no additional computing cost.", "published": "2023-11-21 17:03:21", "link": "http://arxiv.org/abs/2311.12727v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GAIA: a benchmark for General AI Assistants", "abstract": "We introduce GAIA, a benchmark for General AI Assistants that, if solved,\nwould represent a milestone in AI research. GAIA proposes real-world questions\nthat require a set of fundamental abilities such as reasoning, multi-modality\nhandling, web browsing, and generally tool-use proficiency. GAIA questions are\nconceptually simple for humans yet challenging for most advanced AIs: we show\nthat human respondents obtain 92\\% vs. 15\\% for GPT-4 equipped with plugins.\nThis notable performance disparity contrasts with the recent trend of LLMs\noutperforming humans on tasks requiring professional skills in e.g. law or\nchemistry. GAIA's philosophy departs from the current trend in AI benchmarks\nsuggesting to target tasks that are ever more difficult for humans. We posit\nthat the advent of Artificial General Intelligence (AGI) hinges on a system's\ncapability to exhibit similar robustness as the average human does on such\nquestions. Using GAIA's methodology, we devise 466 questions and their answer.\nWe release our questions while retaining answers to 300 of them to power a\nleader-board available at https://huggingface.co/gaia-benchmark.", "published": "2023-11-21 20:34:47", "link": "http://arxiv.org/abs/2311.12983v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Graph Attention Autoencoder for Attributed Networks using\n  K-means Loss", "abstract": "Several natural phenomena and complex systems are often represented as\nnetworks. Discovering their community structure is a fundamental task for\nunderstanding these networks. Many algorithms have been proposed, but recently,\nGraph Neural Networks (GNN) have emerged as a compelling approach for enhancing\nthis task.In this paper, we introduce a simple, efficient, and\nclustering-oriented model based on unsupervised \\textbf{G}raph Attention\n\\textbf{A}uto\\textbf{E}ncoder for community detection in attributed networks\n(GAECO). The proposed model adeptly learns representations from both the\nnetwork's topology and attribute information, simultaneously addressing dual\nobjectives: reconstruction and community discovery. It places a particular\nemphasis on discovering compact communities by robustly minimizing clustering\nerrors. The model employs k-means as an objective function and utilizes a\nmulti-head Graph Attention Auto-Encoder for decoding the representations.\nExperiments conducted on three datasets of attributed networks show that our\nmethod surpasses state-of-the-art algorithms in terms of NMI and ARI.\nAdditionally, our approach scales effectively with the size of the network,\nmaking it suitable for large-scale applications. The implications of our\nfindings extend beyond biological network interpretation and social network\nanalysis, where knowledge of the fundamental community structure is essential.", "published": "2023-11-21 20:45:55", "link": "http://arxiv.org/abs/2311.12986v2", "categories": ["cs.CL", "cs.AI", "68T07", "I.2.4"], "primary_category": "cs.CL"}
{"title": "Data Diversity Matters for Robust Instruction Tuning", "abstract": "Recent works have shown that by curating high quality and diverse instruction\ntuning datasets, we can significantly improve instruction-following\ncapabilities. However, creating such datasets is difficult and most works rely\non manual curation or proprietary language models. Automatic data curation is\ndifficult as it is still not clear how we can define diversity for instruction\ntuning, how diversity and quality depend on one other, and how we can optimize\ndataset quality and diversity. To resolve these issue, we propose a new\nalgorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple\nmethod to simultaneously control dataset diversity and quality, allowing us to\nconduct an in-depth study on the effect of diversity and quality on instruction\ntuning performance. From this study we draw two key insights (1) there is a\nnatural tradeoff between data diversity and quality and (2) increasing data\ndiversity significantly improves the worst case instruction following\nperformance, therefore improving robustness. We validate the performance of\nQDIT on several large scale instruction tuning datasets, where we find it can\nsubstantially improve worst and average case performance compared to\nquality-driven data selection.", "published": "2023-11-21 19:12:18", "link": "http://arxiv.org/abs/2311.14736v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Baseline Analysis of Reward Models' Ability To Accurately Analyze\n  Foundation Models Under Distribution Shift", "abstract": "Foundation models, specifically Large Language Models (LLMs), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align LLM's. These reward models are additionally used at\ninference-time to estimate LLM responses' adherence to those desired behaviors.\nHowever, there is little work measuring how robust these reward models are to\ndistribution shifts. In this work, we evaluate how reward model performance -\nmeasured via accuracy and calibration (i.e. alignment between accuracy and\nconfidence) - is affected by distribution shift. We show novel calibration\npatterns and accuracy drops due to OOD prompts and responses, and that the\nreward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting to detect these distribution shifts\nin prompts and responses.", "published": "2023-11-21 18:41:26", "link": "http://arxiv.org/abs/2311.14743v7", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Political Orientation of Social Media Posts: An Extended\n  Analysis", "abstract": "Developing machine learning models to characterize political polarization on\nonline social media presents significant challenges. These challenges mainly\nstem from various factors such as the lack of annotated data, presence of noise\nin social media datasets, and the sheer volume of data. The common research\npractice typically examines the biased structure of online user communities for\na given topic or qualitatively measuring the impacts of polarized topics on\nsocial media. However, there is limited work focusing on analyzing polarization\nat the ground-level, specifically in the social media posts themselves. Such\nexisting analysis heavily relies on annotated data, which often requires\nlaborious human labeling, offers labels only to specific problems, and lacks\nthe ability to determine the near-future bias state of a social media\nconversations. Understanding the degree of political orientation conveyed in\nsocial media posts is crucial for quantifying the bias of online user\ncommunities and investigating the spread of polarized content. In this work, we\nfirst introduce two heuristic methods that leverage on news media bias and post\ncontent to label social media posts. Next, we compare the efficacy and quality\nof heuristically labeled dataset with a randomly sampled human-annotated\ndataset. Additionally, we demonstrate that current machine learning models can\nexhibit improved performance in predicting political orientation of social\nmedia posts, employing both traditional supervised learning and few-shot\nlearning setups. We conduct experiments using the proposed heuristic methods\nand machine learning approaches to predict the political orientation of posts\ncollected from two social media forums with diverse political ideologies: Gab\nand Twitter.", "published": "2023-11-21 03:34:20", "link": "http://arxiv.org/abs/2311.12323v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Utilizing Language Models for Tour Itinerary Recommendation", "abstract": "Tour itinerary recommendation involves planning a sequence of relevant\nPoint-of-Interest (POIs), which combines challenges from the fields of both\nOperations Research (OR) and Recommendation Systems (RS). As an OR problem,\nthere is the need to maximize a certain utility (e.g., popularity of POIs in\nthe tour) while adhering to some constraints (e.g., maximum time for the tour).\nAs a RS problem, it is heavily related to problem or filtering or ranking a\nsubset of POIs that are relevant to a user and recommending it as part of an\nitinerary. In this paper, we explore the use of language models for the task of\ntour itinerary recommendation and planning. This task has the unique\nrequirement of recommending personalized POIs relevant to users and planning\nthese POIs as an itinerary that satisfies various constraints. We discuss some\napproaches in this area, such as using word embedding techniques like Word2Vec\nand GloVe for learning POI embeddings and transformer-based techniques like\nBERT for generating\n  itineraries.", "published": "2023-11-21 05:15:56", "link": "http://arxiv.org/abs/2311.12355v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A Survey of Graph Meets Large Language Model: Progress and Future\n  Directions", "abstract": "Graph plays a significant role in representing and analyzing complex\nrelationships in real-world applications such as citation networks, social\nnetworks, and biological data. Recently, Large Language Models (LLMs), which\nhave achieved tremendous success in various domains, have also been leveraged\nin graph-related tasks to surpass traditional Graph Neural Networks (GNNs)\nbased methods and yield state-of-the-art performance. In this survey, we first\npresent a comprehensive review and analysis of existing methods that integrate\nLLMs with graphs. First of all, we propose a new taxonomy, which organizes\nexisting methods into three categories based on the role (i.e., enhancer,\npredictor, and alignment component) played by LLMs in graph-related tasks. Then\nwe systematically survey the representative methods along the three categories\nof the taxonomy. Finally, we discuss the remaining limitations of existing\nstudies and highlight promising avenues for future research. The relevant\npapers are summarized and will be consistently updated at:\nhttps://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.", "published": "2023-11-21 07:22:48", "link": "http://arxiv.org/abs/2311.12399v4", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "nach0: Multimodal Natural and Chemical Languages Foundation Model", "abstract": "Large Language Models (LLMs) have substantially driven scientific progress in\nvarious domains, and many papers have demonstrated their ability to tackle\ncomplex problems with creative solutions. Our paper introduces a new foundation\nmodel, nach0, capable of solving various chemical and biological tasks:\nbiomedical question answering, named entity recognition, molecular generation,\nmolecular synthesis, attributes prediction, and others. nach0 is a multi-domain\nand multi-task encoder-decoder LLM pre-trained on unlabeled text from\nscientific literature, patents, and molecule strings to incorporate a range of\nchemical and linguistic knowledge. We employed instruction tuning, where\nspecific task-related instructions are utilized to fine-tune nach0 for the\nfinal set of tasks. To train nach0 effectively, we leverage the NeMo framework,\nenabling efficient parallel optimization of both base and large model versions.\nExtensive experiments demonstrate that our model outperforms state-of-the-art\nbaselines on single-domain and cross-domain tasks. Furthermore, it can generate\nhigh-quality outputs in molecular and textual formats, showcasing its\neffectiveness in multi-domain setups.", "published": "2023-11-21 07:56:30", "link": "http://arxiv.org/abs/2311.12410v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "How Far Have We Gone in Vulnerability Detection Using Large Language\n  Models", "abstract": "As software becomes increasingly complex and prone to vulnerabilities,\nautomated vulnerability detection is critically important, yet challenging.\nGiven the significant successes of large language models (LLMs) in various\ntasks, there is growing anticipation of their efficacy in vulnerability\ndetection. However, a quantitative understanding of their potential in\nvulnerability detection is still missing. To bridge this gap, we introduce a\ncomprehensive vulnerability benchmark VulBench. This benchmark aggregates\nhigh-quality data from a wide range of CTF (Capture-the-Flag) challenges and\nreal-world applications, with annotations for each vulnerable function\ndetailing the vulnerability type and its root cause. Through our experiments\nencompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models\nand static analyzers, we find that several LLMs outperform traditional deep\nlearning approaches in vulnerability detection, revealing an untapped potential\nin LLMs. This work contributes to the understanding and utilization of LLMs for\nenhanced software security.", "published": "2023-11-21 08:20:39", "link": "http://arxiv.org/abs/2311.12420v3", "categories": ["cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.AI"}
{"title": "Speaker-Adapted End-to-End Visual Speech Recognition for Continuous\n  Spanish", "abstract": "Different studies have shown the importance of visual cues throughout the\nspeech perception process. In fact, the development of audiovisual approaches\nhas led to advances in the field of speech technologies. However, although\nnoticeable results have recently been achieved, visual speech recognition\nremains an open research problem. It is a task in which, by dispensing with the\nauditory sense, challenges such as visual ambiguities and the complexity of\nmodeling silence must be faced. Nonetheless, some of these challenges can be\nalleviated when the problem is approached from a speaker-dependent perspective.\nThus, this paper studies, using the Spanish LIP-RTVE database, how the\nestimation of specialized end-to-end systems for a specific person could affect\nthe quality of speech recognition. First, different adaptation strategies based\non the fine-tuning technique were proposed. Then, a pre-trained CTC/Attention\narchitecture was used as a baseline throughout our experiments. Our findings\nshowed that a two-step fine-tuning process, where the VSR system is first\nadapted to the task domain, provided significant improvements when the speaker\nadaptation was addressed. Furthermore, results comparable to the current state\nof the art were reached even when only a limited amount of data was available.", "published": "2023-11-21 09:44:33", "link": "http://arxiv.org/abs/2311.12480v1", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "In-Context Learning Functions with Varying Number of Minima", "abstract": "Large Language Models (LLMs) have proven effective at In-Context Learning\n(ICL), an ability that allows them to create predictors from labeled examples.\nFew studies have explored the interplay between ICL and specific properties of\nfunctions it attempts to approximate. In our study, we use a formal framework\nto explore ICL and propose a new task of approximating functions with varying\nnumber of minima. We implement a method that allows for producing functions\nwith given inputs as minima. We find that increasing the number of minima\ndegrades ICL performance. At the same time, our evaluation shows that ICL\noutperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster\nthan 2NN in all settings. We validate the findings through a set of few-shot\nexperiments across various hyperparameter configurations.", "published": "2023-11-21 11:33:03", "link": "http://arxiv.org/abs/2311.12538v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fair Text Classification with Wasserstein Independence", "abstract": "Group fairness is a central research topic in text classification, where\nreaching fair treatment between sensitive groups (e.g. women vs. men) remains\nan open challenge. This paper presents a novel method for mitigating biases in\nneural text classification, agnostic to the model architecture. Considering the\ndifficulty to distinguish fair from unfair information in a text encoder, we\ntake inspiration from adversarial training to induce Wasserstein independence\nbetween representations learned to predict our target label and the ones\nlearned to predict some sensitive attribute. Our approach provides two\nsignificant advantages. Firstly, it does not require annotations of sensitive\nattributes in both testing and training data. This is more suitable for\nreal-life scenarios compared to existing methods that require annotations of\nsensitive attributes at train time. Second, our approach exhibits a comparable\nor better fairness-accuracy trade-off compared to existing methods.", "published": "2023-11-21 15:51:06", "link": "http://arxiv.org/abs/2311.12689v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explore the Potential of LLMs in Misinformation Detection: An Empirical\n  Study", "abstract": "Large Language Models (LLMs) have garnered significant attention for their\npowerful ability in natural language understanding and reasoning. In this\npaper, we present a comprehensive empirical study to explore the performance of\nLLMs on misinformation detection tasks. This study stands as the pioneering\ninvestigation into the understanding capabilities of multiple LLMs regarding\nboth content and propagation across social media platforms. Our empirical\nstudies on eight misinformation detection datasets show that LLM-based\ndetectors can achieve comparable performance in text-based misinformation\ndetection but exhibit notably constrained capabilities in comprehending\npropagation structure compared to existing models in propagation-based\nmisinformation detection. Our experiments further demonstrate that LLMs exhibit\ngreat potential to enhance existing misinformation detection models. These\nfindings highlight the potential ability of LLMs to detect misinformation.", "published": "2023-11-21 16:03:51", "link": "http://arxiv.org/abs/2311.12699v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Keeping Users Engaged During Repeated Administration of the Same\n  Questionnaire: Using Large Language Models to Reliably Diversify Questions", "abstract": "Standardized, validated questionnaires are vital tools in research and\nhealthcare, offering dependable self-report data. Prior work has revealed that\nvirtual agent-administered questionnaires are almost equivalent to\nself-administered ones in an electronic form. Despite being an engaging method,\nrepeated use of virtual agent-administered questionnaires in longitudinal or\npre-post studies can induce respondent fatigue, impacting data quality via\nresponse biases and decreased response rates. We propose using large language\nmodels (LLMs) to generate diverse questionnaire versions while retaining good\npsychometric properties. In a longitudinal study, participants interacted with\nour agent system and responded daily for two weeks to one of the following\nquestionnaires: a standardized depression questionnaire, question variants\ngenerated by LLMs, or question variants accompanied by LLM-generated small\ntalk. The responses were compared to a validated depression questionnaire.\nPsychometric testing revealed consistent covariation between the external\ncriterion and focal measure administered across the three conditions,\ndemonstrating the reliability and validity of the LLM-generated variants.\nParticipants found that the variants were significantly less repetitive than\nrepeated administrations of the same standardized questionnaire. Our findings\nhighlight the potential of LLM-generated variants to invigorate\nagent-administered questionnaires and foster engagement and interest, without\ncompromising their validity.", "published": "2023-11-21 16:20:49", "link": "http://arxiv.org/abs/2311.12707v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Sentiment Analysis of Twitter Posts on Global Conflicts", "abstract": "Sentiment analysis of social media data is an emerging field with vast\napplications in various domains. In this study, we developed a sentiment\nanalysis model to analyze social media sentiment, especially tweets, during\nglobal conflicting scenarios. To establish our research experiment, we\nidentified a recent global dispute incident on Twitter and collected around\n31,000 filtered Tweets for several months to analyze human sentiment worldwide.", "published": "2023-11-21 04:39:47", "link": "http://arxiv.org/abs/2312.03715v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Eliminating Quantization Errors in Classification-Based Sound Source\n  Localization", "abstract": "Sound Source Localization (SSL) involves estimating the Direction of Arrival\n(DOA) of sound sources. Since the DOA estimation output space is continuous,\nregression might be more suitable for DOA, offering higher precision. However,\nin practice, classification often outperforms regression, exhibiting greater\nrobustness to interference. Conversely, classification's drawback is inherent\nquantization error. Within the classification paradigm, the DOA output space is\ndiscretized into intervals, each treated as a class. These classes show strong\ninter-class correlations, being inherently ordered, with higher similarity as\nintervals grow closer. Nevertheless, this has not been fully exploited. To\naddress this, we propose an Unbiased Label Distribution (ULD) to eliminate\nquantization error in training targets. Furthermore, we tailor two loss\nfunctions for the soft label family: Negative Log Absolute Error (NLAE) and\nMean Squared Error without activation (MSE(wo)). Finally, we introduce Weighted\nAdjacent Decoding (WAD) to overcome quantization error during model prediction\ndecoding. Experimental results demonstrate our approach surpasses\nclassification quantization limits, achieving state-of-the-art performance. Our\ncode and supplementary materials are available at\nhttps://github.com/linfeng-feng/ULD.", "published": "2023-11-21 02:48:02", "link": "http://arxiv.org/abs/2311.12305v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AudioLog: LLMs-Powered Long Audio Logging with Hybrid Token-Semantic\n  Contrastive Learning", "abstract": "Previous studies in automated audio captioning have faced difficulties in\naccurately capturing the complete temporal details of acoustic scenes and\nevents within long audio sequences. This paper presents AudioLog, a large\nlanguage models (LLMs)-powered audio logging system with hybrid token-semantic\ncontrastive learning. Specifically, we propose to fine-tune the pre-trained\nhierarchical token-semantic audio Transformer by incorporating contrastive\nlearning between hybrid acoustic representations. We then leverage LLMs to\ngenerate audio logs that summarize textual descriptions of the acoustic\nenvironment. Finally, we evaluate the AudioLog system on two datasets with both\nscene and event annotations. Experiments show that the proposed system achieves\nexceptional performance in acoustic scene classification and sound event\ndetection, surpassing existing methods in the field. Further analysis of the\nprompts to LLMs demonstrates that AudioLog can effectively summarize long audio\nsequences. To the best of our knowledge, this approach is the first attempt to\nleverage LLMs for summarizing long audio sequences.", "published": "2023-11-21 06:20:27", "link": "http://arxiv.org/abs/2311.12371v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Distributed Algorithm for Personal Sound Zones Systems", "abstract": "A Personal Sound Zones (PSZ) system aims to generate two or more independent\nlistening zones that allow multiple users to listen to different music/audio\ncontent in a shared space without the need for wearing headphones. Most\nexisting studies assume that the acoustic paths between loudspeakers and\nmicrophones are measured beforehand in a stationary environment. Recently,\nadaptive PSZ systems have been explored to adapt the system in a time-varying\nacoustic environment. However, because a PSZ system usually requires multiple\nloudspeakers, the multichannel adaptive algorithms impose a high computational\nload on the processor. To overcome that problem, this paper proposes an\nefficient distributed algorithm for PSZ systems, which not only spreads the\ncomputational burden over multiple nodes but also reduces the overall\ncomputational complexity, at the expense of a slight decrease in performance.\nSimulation results with true room impulse responses measured in a Hemi-Anechoic\nchamber are performed to verify the proposed distributed PSZ system.", "published": "2023-11-21 08:39:42", "link": "http://arxiv.org/abs/2311.12427v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning-based Array Configuration-Independent Binaural Audio\n  Telepresence with Scalable Signal Enhancement and Ambience Preservation", "abstract": "Audio Telepresence (AT) aims to create an immersive experience of the audio\nscene at the far end for the user(s) at the near end. The application of AT\ncould encompass scenarios with varying degrees of emphasis on signal\nenhancement and ambience preservation. It is desirable for an AT system to be\nscalable between these two extremes. To this end, we propose an array-based\nBinaural AT (BAT) system using the DeepFilterNet as the backbone to convert the\narray microphone signals into the Head-Related Transfer Function\n(HRTF)-filtered signals, with a tunable weighting between signal enhancement\nand ambience preservation. An array configuration-independent Spatial COherence\nREpresentation (SCORE) feature is proposed for the model training so that the\nnetwork remains robust to different array geometries and sensor counts.\nmagnitude-weighted Interaural Phase Difference error (mw-IPDe),\nmagnitude-weighted Interaural Level Difference error (mw-ILDe), and modified\nScale-Invariant Signal-to-Distortion Ratio (mSI-SDR) are defined as performance\nmetrics for objective evaluation. Subjective listening tests were also\nperformed to validate the proposed BAT system. The results have shown that the\nproposed BAT system can achieve superior telepresence performance with the\ndesired balance between signal enhancement and ambience preservation, even when\nthe array configurations are unseen in the training phase.", "published": "2023-11-21 16:19:40", "link": "http://arxiv.org/abs/2311.12706v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FedCPC: An Effective Federated Contrastive Learning Method for Privacy\n  Preserving Early-Stage Alzheimer's Speech Detection", "abstract": "The early-stage Alzheimer's disease (AD) detection has been considered an\nimportant field of medical studies. Like traditional machine learning methods,\nspeech-based automatic detection also suffers from data privacy risks because\nthe data of specific patients are exclusive to each medical institution. A\ncommon practice is to use federated learning to protect the patients' data\nprivacy. However, its distributed learning process also causes performance\nreduction. To alleviate this problem while protecting user privacy, we propose\na federated contrastive pre-training (FedCPC) performed before federated\ntraining for AD speech detection, which can learn a better representation from\nraw data and enables different clients to share data in the pre-training and\ntraining stages. Experimental results demonstrate that the proposed methods can\nachieve satisfactory performance while preserving data privacy.", "published": "2023-11-21 23:08:06", "link": "http://arxiv.org/abs/2311.13043v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "HPCNeuroNet: Advancing Neuromorphic Audio Signal Processing with\n  Transformer-Enhanced Spiking Neural Networks", "abstract": "This paper presents a novel approach to neuromorphic audio processing by\nintegrating the strengths of Spiking Neural Networks (SNNs), Transformers, and\nhigh-performance computing (HPC) into the HPCNeuroNet architecture. Utilizing\nthe Intel N-DNS dataset, we demonstrate the system's capability to process\ndiverse human vocal recordings across multiple languages and noise backgrounds.\nThe core of our approach lies in the fusion of the temporal dynamics of SNNs\nwith the attention mechanisms of Transformers, enabling the model to capture\nintricate audio patterns and relationships. Our architecture, HPCNeuroNet,\nemploys the Short-Time Fourier Transform (STFT) for time-frequency\nrepresentation, Transformer embeddings for dense vector generation, and SNN\nencoding/decoding mechanisms for spike train conversions. The system's\nperformance is further enhanced by leveraging the computational capabilities of\nNVIDIA's GeForce RTX 3060 GPU and Intel's Core i9 12900H CPU. Additionally, we\nintroduce a hardware implementation on the Xilinx VU37P HBM FPGA platform,\noptimizing for energy efficiency and real-time processing. The proposed\naccelerator achieves a throughput of 71.11 Giga-Operations Per Second (GOP/s)\nwith a 3.55 W on-chip power consumption at 100 MHz. The comparison results with\noff-the-shelf devices and recent state-of-the-art implementations illustrate\nthat the proposed accelerator has obvious advantages in terms of energy\nefficiency and design flexibility. Through design-space exploration, we provide\ninsights into optimizing core capacities for audio tasks. Our findings\nunderscore the transformative potential of integrating SNNs, Transformers, and\nHPC for neuromorphic audio processing, setting a new benchmark for future\nresearch and applications.", "published": "2023-11-21 09:01:38", "link": "http://arxiv.org/abs/2311.12449v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adapting pretrained speech model for Mandarin lyrics transcription and\n  alignment", "abstract": "The tasks of automatic lyrics transcription and lyrics alignment have\nwitnessed significant performance improvements in the past few years. However,\nmost of the previous works only focus on English in which large-scale datasets\nare available. In this paper, we address lyrics transcription and alignment of\npolyphonic Mandarin pop music in a low-resource setting. To deal with the data\nscarcity issue, we adapt pretrained Whisper model and fine-tune it on a\nmonophonic Mandarin singing dataset. With the use of data augmentation and\nsource separation model, results show that the proposed method achieves a\ncharacter error rate of less than 18% on a Mandarin polyphonic dataset for\nlyrics transcription, and a mean absolute error of 0.071 seconds for lyrics\nalignment. Our results demonstrate the potential of adapting a pretrained\nspeech model for lyrics transcription and alignment in low-resource scenarios.", "published": "2023-11-21 09:59:18", "link": "http://arxiv.org/abs/2311.12488v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Music Source Separation Using Vector-Quantized Source\n  Category Estimates", "abstract": "Music source separation is focused on extracting distinct sonic elements from\ncomposite tracks. Historically, many methods have been grounded in supervised\nlearning, necessitating labeled data, which is occasionally constrained in its\ndiversity. More recent methods have delved into N-shot techniques that utilize\none or more audio samples to aid in the separation. However, a challenge with\nsome of these methods is the necessity for an audio query during inference,\nmaking them less suited for genres with varied timbres and effects. This paper\noffers a proof-of-concept for a self-supervised music source separation system\nthat eliminates the need for audio queries at inference time. In the training\nphase, while it adopts a query-based approach, we introduce a modification by\nsubstituting the continuous embedding of query audios with Vector Quantized\n(VQ) representations. Trained end-to-end with up to N classes as determined by\nthe VQ's codebook size, the model seeks to effectively categorise instrument\nclasses. During inference, the input is partitioned into N sources, with some\npotentially left unutilized based on the mix's instrument makeup. This\nmethodology suggests an alternative avenue for considering source separation\nacross diverse music genres. We provide examples and additional results online.", "published": "2023-11-21 23:45:36", "link": "http://arxiv.org/abs/2311.13058v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Equipping Pretrained Unconditional Music Transformers with Instrument\n  and Genre Controls", "abstract": "The ''pretraining-and-finetuning'' paradigm has become a norm for training\ndomain-specific models in natural language processing and computer vision. In\nthis work, we aim to examine this paradigm for symbolic music generation\nthrough leveraging the largest ever symbolic music dataset sourced from the\nMuseScore forum. We first pretrain a large unconditional transformer model\nusing 1.5 million songs. We then propose a simple technique to equip this\npretrained unconditional music transformer model with instrument and genre\ncontrols by finetuning the model with additional control tokens. Our proposed\nrepresentation offers improved high-level controllability and expressiveness\nagainst two existing representations. The experimental results show that the\nproposed model can successfully generate music with user-specified instruments\nand genre. In a subjective listening test, the proposed model outperforms the\npretrained baseline model in terms of coherence, harmony, arrangement and\noverall quality.", "published": "2023-11-21 00:37:47", "link": "http://arxiv.org/abs/2311.12257v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HierSpeech++: Bridging the Gap between Semantic and Acoustic\n  Representation of Speech by Hierarchical Variational Inference for Zero-shot\n  Speech Synthesis", "abstract": "Large language models (LLM)-based speech synthesis has been widely adopted in\nzero-shot speech synthesis. However, they require a large-scale data and\npossess the same limitations as previous autoregressive speech models,\nincluding slow inference speed and lack of robustness. This paper proposes\nHierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech\n(TTS) and voice conversion (VC). We verified that hierarchical speech synthesis\nframeworks could significantly improve the robustness and expressiveness of the\nsynthetic speech. Furthermore, we significantly improve the naturalness and\nspeaker similarity of synthetic speech even in zero-shot speech synthesis\nscenarios. For text-to-speech, we adopt the text-to-vec framework, which\ngenerates a self-supervised speech representation and an F0 representation\nbased on text representations and prosody prompts. Then, HierSpeech++ generates\nspeech from the generated vector, F0, and voice prompt. We further introduce a\nhigh-efficient speech super-resolution framework from 16 kHz to 48 kHz. The\nexperimental results demonstrated that the hierarchical variational autoencoder\ncould be a strong zero-shot speech synthesizer given that it outperforms\nLLM-based and diffusion-based models. Moreover, we achieved the first\nhuman-level quality zero-shot speech synthesis. Audio samples and source code\nare available at https://github.com/sh-lee-prml/HierSpeechpp.", "published": "2023-11-21 09:07:11", "link": "http://arxiv.org/abs/2311.12454v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Summary of the DISPLACE Challenge 2023 - DIarization of SPeaker and\n  LAnguage in Conversational Environments", "abstract": "In multi-lingual societies, where multiple languages are spoken in a small\ngeographic vicinity, informal conversations often involve mix of languages.\nExisting speech technologies may be inefficient in extracting information from\nsuch conversations, where the speech data is rich in diversity with multiple\nlanguages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in\nConversational Environments) challenge constitutes an open-call for evaluating\nand bench-marking the speaker and language diarization technologies on this\nchallenging condition. The challenge entailed two tracks: Track-1 focused on\nspeaker diarization (SD) in multilingual situations while, Track-2 addressed\nthe language diarization (LD) in a multi-speaker scenario. Both the tracks were\nevaluated using the same underlying audio data. To facilitate this evaluation,\na real-world dataset featuring multilingual, multi-speaker conversational\nfar-field speech was recorded and distributed. Furthermore, a baseline system\nwas made available for both SD and LD task which mimicked the state-of-art in\nthese tasks. The challenge garnered a total of $42$ world-wide registrations\nand received a total of $19$ combined submissions for Track-1 and Track-2. This\npaper describes the challenge, details of the datasets, tasks, and the baseline\nsystem. Additionally, the paper provides a concise overview of the submitted\nsystems in both tracks, with an emphasis given to the top performing systems.\nThe paper also presents insights and future perspectives for SD and LD tasks,\nfocusing on the key challenges that the systems need to overcome before\nwide-spread commercial deployment on such conversations.", "published": "2023-11-21 12:23:58", "link": "http://arxiv.org/abs/2311.12564v3", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
