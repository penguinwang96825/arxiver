{"title": "Incremental Transformer with Deliberation Decoder for Document Grounded\n  Conversations", "abstract": "Document Grounded Conversations is a task to generate dialogue responses when\nchatting about the content of a given document. Obviously, document knowledge\nplays a critical role in Document Grounded Conversations, while existing\ndialogue models do not exploit this kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-based architecture for multi-turn\ndocument grounded conversations. In particular, we devise an Incremental\nTransformer to encode multi-turn utterances along with knowledge in related\ndocuments. Motivated by the human cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve context coherence and knowledge\ncorrectness. Our empirical study on a real-world Document Grounded Dataset\nproves that responses generated by our model significantly outperform\ncompetitive baselines on both context coherence and knowledge relevance.", "published": "2019-07-20 18:49:36", "link": "http://arxiv.org/abs/1907.08854v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ER-AE: Differentially Private Text Generation for Authorship\n  Anonymization", "abstract": "Most of privacy protection studies for textual data focus on removing\nexplicit sensitive identifiers. However, personal writing style, as a strong\nindicator of the authorship, is often neglected. Recent studies, such as SynTF,\nhave shown promising results on privacy-preserving text mining. However, their\nanonymization algorithm can only output numeric term vectors which are\ndifficult for the recipients to interpret. We propose a novel text generation\nmodel with a two-set exponential mechanism for authorship anonymization. By\naugmenting the semantic information through a REINFORCE training reward\nfunction, the model can generate differentially private text that has a close\nsemantic and similar grammatical structure to the original text while removing\npersonal traits of the writing style. It does not assume any conditioned labels\nor paralleled text data for training. We evaluate the performance of the\nproposed model on the real-life peer reviews dataset and the Yelp review\ndataset. The result suggests that our model outperforms the state-of-the-art on\nsemantic preservation, authorship obfuscation, and stylometric transformation.", "published": "2019-07-20 02:07:02", "link": "http://arxiv.org/abs/1907.08736v4", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
