{"title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of\n  LLM Responses Based on Hofstede Cultural Dimensions", "abstract": "Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs.", "published": "2024-06-21 00:58:01", "link": "http://arxiv.org/abs/2406.14805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction\n  in RAG-based Crowdsourcing Systems", "abstract": "Temporal relation extraction (TRE) aims to grasp the evolution of events or\nactions, and thus shape the workflow of associated tasks, so it holds promise\nin helping understand task requests initiated by requesters in crowdsourcing\nsystems. However, existing methods still struggle with limited and unevenly\ndistributed annotated data. Therefore, inspired by the abundant global\nknowledge stored within pre-trained language models (PLMs), we propose a\nmulti-task prompt learning framework for TRE (TemPrompt), incorporating prompt\ntuning and contrastive learning to tackle these issues. To elicit more\neffective prompts for PLMs, we introduce a task-oriented prompt construction\napproach that thoroughly takes the myriad factors of TRE into consideration for\nautomatic prompt generation. In addition, we design temporal event reasoning in\nthe form of masked language modeling as auxiliary tasks to bolster the model's\nfocus on events and temporal cues. The experimental results demonstrate that\nTemPrompt outperforms all compared baselines across the majority of metrics\nunder both standard and few-shot settings. A case study on designing and\nmanufacturing printed circuit boards is provided to validate its effectiveness\nin crowdsourcing scenarios.", "published": "2024-06-21 01:52:37", "link": "http://arxiv.org/abs/2406.14825v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Matters: What Influences Domain Adaptation in Summarization?", "abstract": "Domain adaptation aims to enable Large Language Models (LLMs) to generalize\ndomain datasets unseen effectively during the training phase. However, factors\nsuch as the size of the model parameters and the scale of training data are\ngeneral influencers and do not reflect the nuances of domain adaptation\nperformance. This paper investigates the fine-grained factors affecting domain\nadaptation performance, analyzing the specific impact of `words' in training\ndata on summarization tasks. We propose quantifying dataset learning difficulty\nas the learning difficulty of generative summarization, which is determined by\ntwo indicators: word-based compression rate and abstraction level. Our\nexperiments conclude that, when considering dataset learning difficulty, the\ncross-domain overlap and the performance gain in summarization tasks exhibit an\napproximate linear relationship, which is not directly related to the number of\nwords. Based on this finding, predicting a model's performance on unknown\ndomain datasets is possible without undergoing training.", "published": "2024-06-21 02:15:49", "link": "http://arxiv.org/abs/2406.14828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is This a Bad Table? A Closer Look at the Evaluation of Table Generation\n  from Text", "abstract": "Understanding whether a generated table is of good quality is important to be\nable to use it in creating or editing documents using automatic methods. In\nthis work, we underline that existing measures for table quality evaluation\nfail to capture the overall semantics of the tables, and sometimes unfairly\npenalize good tables and reward bad ones. We propose TabEval, a novel table\nevaluation strategy that captures table semantics by first breaking down a\ntable into a list of natural language atomic statements and then compares them\nwith ground truth statements using entailment-based measures. To validate our\napproach, we curate a dataset comprising of text descriptions for 1,250 diverse\nWikipedia tables, covering a range of topics and structures, in contrast to the\nlimited scope of existing datasets. We compare TabEval with existing metrics\nusing unsupervised and supervised text-to-table generation methods,\ndemonstrating its stronger correlation with human judgments of table quality\nacross four datasets.", "published": "2024-06-21 02:18:03", "link": "http://arxiv.org/abs/2406.14829v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Continual Pre-training by Mitigating the Stability Gap", "abstract": "Continual pre-training has increasingly become the predominant approach for\nadapting Large Language Models (LLMs) to new domains. This process involves\nupdating the pre-trained LLM with a corpus from a new domain, resulting in a\nshift in the training distribution. To study the behavior of LLMs during this\nshift, we measured the model's performance throughout the continual\npre-training process. we observed a temporary performance drop at the\nbeginning, followed by a recovery phase, a phenomenon known as the \"stability\ngap,\" previously noted in vision models classifying new classes. To address\nthis issue and enhance LLM performance within a fixed compute budget, we\npropose three effective strategies: (1) Continually pre-training the LLM on a\nsubset with a proper size for multiple epochs, resulting in faster performance\nrecovery than pre-training the LLM on a large corpus in a single epoch; (2)\nPre-training the LLM only on high-quality sub-corpus, which rapidly boosts\ndomain performance; and (3) Using a data mixture similar to the pre-training\ndata to reduce distribution gap. We conduct various experiments on Llama-family\nmodels to validate the effectiveness of our strategies in both medical\ncontinual pre-training and instruction tuning. For example, our strategies\nimprove the average medical task performance of the OpenLlama-3B model from\n36.2% to 40.7% with only 40% of the original training budget and enhance the\naverage general task performance without causing forgetting. Furthermore, we\napply our strategies to the Llama-3-8B model. The resulting model,\nLlama-3-Physician, achieves the best medical performance among current\nopen-source models, and performs comparably to or even better than GPT-4 on\nseveral medical benchmarks. We release our models at\n\\url{https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct}.", "published": "2024-06-21 02:28:37", "link": "http://arxiv.org/abs/2406.14833v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sports Intelligence: Assessing the Sports Understanding Capabilities of\n  Language Models through Question Answering from Text to Video", "abstract": "Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.", "published": "2024-06-21 05:57:50", "link": "http://arxiv.org/abs/2406.14877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "70B-parameter large language models in Japanese medical\n  question-answering", "abstract": "Since the rise of large language models (LLMs), the domain adaptation has\nbeen one of the hot topics in various domains. Many medical LLMs trained with\nEnglish medical dataset have made public recently. However, Japanese LLMs in\nmedical domain still lack its research. Here we utilize multiple 70B-parameter\nLLMs for the first time and show that instruction tuning using Japanese medical\nquestion-answering dataset significantly improves the ability of Japanese LLMs\nto solve Japanese medical license exams, surpassing 50\\% in accuracy. In\nparticular, the Japanese-centric models exhibit a more significant leap in\nimprovement through instruction tuning compared to their English-centric\ncounterparts. This underscores the importance of continual pretraining and the\nadjustment of the tokenizer in our local language. We also examine two slightly\ndifferent prompt formats, resulting in non-negligible performance improvement.", "published": "2024-06-21 06:04:10", "link": "http://arxiv.org/abs/2406.14882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for\n  LLM-based Agents", "abstract": "LLM-based agents have emerged as promising tools, which are crafted to\nfulfill complex tasks by iterative planning and action. However, these agents\nare susceptible to undesired planning hallucinations when lacking specific\nknowledge for expertise-intensive tasks. To address this, preliminary attempts\nare made to enhance planning reliability by incorporating external\nworkflow-related knowledge. Despite the promise, such infused knowledge is\nmostly disorganized and diverse in formats, lacking rigorous formalization and\ncomprehensive comparisons. Motivated by this, we formalize different formats of\nworkflow knowledge and present FlowBench, the first benchmark for\nworkflow-guided planning. FlowBench covers 51 different scenarios from 6\ndomains, with knowledge presented in diverse formats. To assess different LLMs\non FlowBench, we design a multi-tiered evaluation framework. We evaluate the\nefficacy of workflow knowledge across multiple formats, and the results\nindicate that current LLM agents need considerable improvements for\nsatisfactory planning. We hope that our challenging benchmark can pave the way\nfor future agent planning research.", "published": "2024-06-21 06:13:00", "link": "http://arxiv.org/abs/2406.14884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InternLM-Law: An Open Source Chinese Legal Large Language Model", "abstract": "While large language models (LLMs) have showcased impressive capabilities,\nthey struggle with addressing legal queries due to the intricate complexities\nand specialized expertise required in the legal field. In this paper, we\nintroduce InternLM-Law, a specialized LLM tailored for addressing diverse legal\nqueries related to Chinese laws, spanning from responding to standard legal\nquestions (e.g., legal exercises in textbooks) to analyzing complex real-world\nlegal situations. We meticulously construct a dataset in the Chinese legal\ndomain, encompassing over 1 million queries, and implement a data filtering and\nprocessing pipeline to ensure its diversity and quality. Our training approach\ninvolves a novel two-stage process: initially fine-tuning LLMs on both\nlegal-specific and general-purpose content to equip the models with broad\nknowledge, followed by exclusive fine-tuning on high-quality legal data to\nenhance structured output generation. InternLM-Law achieves the highest average\nperformance on LawBench, outperforming state-of-the-art models, including\nGPT-4, on 13 out of 20 subtasks. We make InternLM-Law and our dataset publicly\navailable to facilitate future research in applying LLMs within the legal\ndomain.", "published": "2024-06-21 06:19:03", "link": "http://arxiv.org/abs/2406.14887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language\n  Models", "abstract": "Emotion Support Conversation (ESC) is a crucial application, which aims to\nreduce human stress, offer emotional guidance, and ultimately enhance human\nmental and physical well-being. With the advancement of Large Language Models\n(LLMs), many researchers have employed LLMs as the ESC models. However, the\nevaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome\ndevelopment of role-playing agents, we propose an ESC Evaluation framework\n(ESC-Eval), which uses a role-playing agent to interact with ESC models,\nfollowed by a manual evaluation of the interactive dialogues. In detail, we\nfirst re-organize 2,801 role-playing cards from seven existing datasets to\ndefine the roles of the role-playing agent. Second, we train a specific\nrole-playing model called ESC-Role which behaves more like a confused person\nthan GPT-4. Third, through ESC-Role and organized role cards, we systematically\nconduct experiments using 14 LLMs as the ESC models, including general\nAI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct\ncomprehensive human annotations on interactive multi-turn dialogues of\ndifferent ESC models. The results show that ESC-oriented LLMs exhibit superior\nESC abilities compared to general AI-assistant LLMs, but there is still a gap\nbehind human performance. Moreover, to automate the scoring process for future\nESC models, we developed ESC-RANK, which trained on the annotated data,\nachieving a scoring performance surpassing 35 points of GPT-4. Our data and\ncode are available at https://github.com/AIFlames/Esc-Eval.", "published": "2024-06-21 08:03:33", "link": "http://arxiv.org/abs/2406.14952v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ICLEval: Evaluating In-Context Learning Ability of Large Language Models", "abstract": "In-Context Learning (ICL) is a critical capability of Large Language Models\n(LLMs) as it empowers them to comprehend and reason across interconnected\ninputs. Evaluating the ICL ability of LLMs can enhance their utilization and\ndeepen our understanding of how this ability is acquired at the training stage.\nHowever, existing evaluation frameworks primarily focus on language abilities\nand knowledge, often overlooking the assessment of ICL ability. In this work,\nwe introduce the ICLEval benchmark to evaluate the ICL abilities of LLMs, which\nencompasses two key sub-abilities: exact copying and rule learning. Through the\nICLEval benchmark, we demonstrate that ICL ability is universally present in\ndifferent LLMs, and model size is not the sole determinant of ICL efficacy.\nSurprisingly, we observe that ICL abilities, particularly copying, develop\nearly in the pretraining process and stabilize afterward. Our source codes and\nbenchmark are released at https://github.com/yiye3/ICLEval.", "published": "2024-06-21 08:06:10", "link": "http://arxiv.org/abs/2406.14955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework\n  for Knowledge-Intensive LLM Generation", "abstract": "Despite the significant progress of large language models (LLMs) in various\ntasks, they often produce factual errors due to their limited internal\nknowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with\nexternal knowledge sources, offers a promising solution. However, these methods\ncan be misled by irrelevant paragraphs in retrieved documents. Due to the\ninherent uncertainty in LLM generation, inputting the entire document may\nintroduce off-topic information, causing the model to deviate from the central\ntopic and affecting the relevance of the generated content. To address these\nissues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates\nplan tokens to guide subsequent generation in the plan stage. In the answer\nstage, the model selects relevant fine-grained paragraphs based on the plan and\nuses them for further answer generation. This plan-answer process is repeated\niteratively until completion, enhancing generation relevance by focusing on\nspecific topics. To implement this framework efficiently, we utilize a simple\nbut effective multi-task prompt-tuning method, enabling the existing LLMs to\nhandle both planning and answering. We comprehensively compare RPG with\nbaselines across 5 knowledge-intensive generation tasks, demonstrating the\neffectiveness of our approach.", "published": "2024-06-21 08:45:52", "link": "http://arxiv.org/abs/2406.14979v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedOdyssey: A Medical Domain Benchmark for Long Context Evaluation Up to\n  200K Tokens", "abstract": "Numerous advanced Large Language Models (LLMs) now support context lengths up\nto 128K, and some extend to 200K. Some benchmarks in the generic domain have\nalso followed up on evaluating long-context capabilities. In the medical\ndomain, tasks are distinctive due to the unique contexts and need for domain\nexpertise, necessitating further evaluation. However, despite the frequent\npresence of long texts in medical scenarios, evaluation benchmarks of\nlong-context capabilities for LLMs in this field are still rare. In this paper,\nwe propose MedOdyssey, the first medical long-context benchmark with seven\nlength levels ranging from 4K to 200K tokens. MedOdyssey consists of two\nprimary components: the medical-context \"needles in a haystack\" task and a\nseries of tasks specific to medical applications, together comprising 10\ndatasets. The first component includes challenges such as counter-intuitive\nreasoning and novel (unknown) facts injection to mitigate knowledge leakage and\ndata contamination of LLMs. The second component confronts the challenge of\nrequiring professional medical expertise. Especially, we design the ``Maximum\nIdentical Context'' principle to improve fairness by guaranteeing that\ndifferent LLMs observe as many identical contexts as possible. Our experiment\nevaluates advanced proprietary and open-source LLMs tailored for processing\nlong contexts and presents detailed performance analyses. This highlights that\nLLMs still face challenges and need for further research in this area. Our code\nand data are released in the repository:\n\\url{https://github.com/JOHNNY-fans/MedOdyssey.}", "published": "2024-06-21 09:46:57", "link": "http://arxiv.org/abs/2406.15019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Knowledge Retrieval and Large Language Models for Clinical\n  Report Correction", "abstract": "This study proposes an approach for error correction in radiology reports,\nleveraging large language models (LLMs) and retrieval-augmented generation\n(RAG) techniques. The proposed framework employs a novel internal+external\nretrieval mechanism to extract relevant medical entities and relations from the\nreport of interest and an external knowledge source. A three-stage inference\nprocess is introduced, decomposing the task into error detection, localization,\nand correction subtasks, which enhances the explainability and performance of\nthe system. The effectiveness of the approach is evaluated using a benchmark\ndataset created by corrupting real-world radiology reports with realistic\nerrors, guided by domain experts. Experimental results demonstrate the benefits\nof the proposed methods, with the combination of internal and external\nretrieval significantly improving the accuracy of error detection,\nlocalization, and correction across various state-of-the-art LLMs. The findings\ncontribute to the development of more robust and reliable error correction\nsystems for clinical documentation.", "published": "2024-06-21 10:48:21", "link": "http://arxiv.org/abs/2406.15045v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement\n  on Multilingual and Multi-Cultural Data", "abstract": "Evaluation of multilingual Large Language Models (LLMs) is challenging due to\na variety of factors -- the lack of benchmarks with sufficient linguistic\ndiversity, contamination of popular benchmarks into LLM pre-training data and\nthe lack of local, cultural nuances in translated benchmarks. In this work, we\nstudy human and LLM-based evaluation in a multilingual, multi-cultural setting.\nWe evaluate 30 models across 10 Indic languages by conducting 90K human\nevaluations and 30K LLM-based evaluations and find that models such as GPT-4o\nand Llama-3 70B consistently perform best for most Indic languages. We build\nleaderboards for two evaluation settings - pairwise comparison and direct\nassessment and analyze the agreement between humans and LLMs. We find that\nhumans and LLMs agree fairly well in the pairwise setting but the agreement\ndrops for direct assessment evaluation especially for languages such as Bengali\nand Odia. We also check for various biases in human and LLM-based evaluation\nand find evidence of self-bias in the GPT-based evaluator. Our work presents a\nsignificant step towards scaling up multilingual evaluation of LLMs.", "published": "2024-06-21 11:00:38", "link": "http://arxiv.org/abs/2406.15053v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual paraphrase identification", "abstract": "The paraphrase identification task involves measuring semantic similarity\nbetween two short sentences. It is a tricky task, and multilingual paraphrase\nidentification is even more challenging. In this work, we train a bi-encoder\nmodel in a contrastive manner to detect hard paraphrases across multiple\nlanguages. This approach allows us to use model-produced embeddings for various\ntasks, such as semantic search. We evaluate our model on downstream tasks and\nalso assess embedding space quality. Our performance is comparable to\nstate-of-the-art cross-encoders, with only a minimal relative drop of 7-10% on\nthe chosen dataset, while keeping decent quality of embeddings.", "published": "2024-06-21 11:37:24", "link": "http://arxiv.org/abs/2406.15066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Input Feature Explanations through a Unified Diagnostic\n  Evaluation Framework", "abstract": "Explaining the decision-making process of machine learning models is crucial\nfor ensuring their reliability and transparency for end users. One popular\nexplanation form highlights key input features, such as i) tokens (e.g.,\nShapley Values and Integrated Gradients), ii) interactions between tokens\n(e.g., Bivariate Shapley and Attention-based methods), or iii) interactions\nbetween spans of the input (e.g., Louvain Span Interactions). However, these\nexplanation types have only been studied in isolation, making it difficult to\njudge their respective applicability. To bridge this gap, we develop a unified\nframework that facilitates an automated and direct comparison between highlight\nand interactive explanations comprised of four diagnostic properties. We\nconduct an extensive analysis across these three types of input feature\nexplanations -- each utilizing three different explanation techniques -- across\ntwo datasets and two models, and reveal that each explanation has distinct\nstrengths across the different diagnostic properties. Nevertheless, interactive\nspan explanations outperform other types of input feature explanations across\nmost diagnostic properties. Despite being relatively understudied, our analysis\nunderscores the need for further research to improve methods generating these\nexplanation types. Additionally, integrating them with other explanation types\nthat perform better in certain characteristics could further enhance their\noverall effectiveness.", "published": "2024-06-21 12:01:03", "link": "http://arxiv.org/abs/2406.15085v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment\n  Analysis", "abstract": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks.", "published": "2024-06-21 14:08:25", "link": "http://arxiv.org/abs/2406.15163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Alignment Training for Large Language Models", "abstract": "Alignment training is crucial for enabling large language models (LLMs) to\ncater to human intentions and preferences. It is typically performed based on\ntwo stages with different objectives: instruction-following alignment and\nhuman-preference alignment. However, aligning LLMs with these objectives in\nsequence suffers from an inherent problem: the objectives may conflict, and the\nLLMs cannot guarantee to simultaneously align with the instructions and human\npreferences well. To response to these, in this work, we propose a Hybrid\nAlignment Training (Hbat) approach, based on alternating alignment and modified\nelastic weight consolidation methods. The basic idea is to alternate between\ndifferent objectives during alignment training, so that better collaboration\ncan be achieved between the two alignment tasks.We experiment with Hbat on\nsummarization and dialogue tasks. Experimental results show that the proposed\n\\textsc{Hbat} can significantly outperform all baselines. Notably, Hbat yields\nconsistent performance gains over the traditional two-stage alignment training\nwhen using both proximal policy optimization and direct preference\noptimization.", "published": "2024-06-21 14:23:57", "link": "http://arxiv.org/abs/2406.15178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inference Time Alignment with Reward-Guided Tree Search", "abstract": "Inference-time computation methods enhance the performance of Large Language\nModels (LLMs) by leveraging additional computational resources to achieve\nsuperior results. Common techniques, such as Best-of-N sampling, Majority\nVoting, and variants of tree-search algorithms have proven to be effective in\nboosting the performance of LLMs. These approaches strategically trade\nincreased computational resources for improved model responses. In this work,\nwe proposed DARWIN, an inference-time alignment method that leverages the\nguidance of a reward model to achieve alignment through a reward-guided tree\nsearch. Empirical evidences indicates that our method outperforms other\ninference-time alignment methods such as Best-of-N and ARGS on two widely\naccepted alignment benchmarks AlpacaEval 2 and MT-Bench. Furthermore, we show\nthat our inference-time approach achieves performance comparable to\npreference-tuned models on both benchmarks, highlighting the effectiveness of\ntrading inference-time compute for enhanced performance during inference. We\nhave released our codes at https://github.com/declare-lab/darwin.", "published": "2024-06-21 14:35:16", "link": "http://arxiv.org/abs/2406.15193v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Extraction of Dialogue Policies from Conversations", "abstract": "Dialogue policies play a crucial role in developing task-oriented dialogue\nsystems, yet their development and maintenance are challenging and typically\nrequire substantial effort from experts in dialogue modeling. While in many\nsituations, large amounts of conversational data are available for the task at\nhand, people lack an effective solution able to extract dialogue policies from\nthis data. In this paper, we address this gap by first illustrating how Large\nLanguage Models (LLMs) can be instrumental in extracting dialogue policies from\ndatasets, through the conversion of conversations into a unified intermediate\nrepresentation consisting of canonical forms. We then propose a novel method\nfor generating dialogue policies utilizing a controllable and interpretable\ngraph-based methodology. By combining canonical forms across conversations into\na flow network, we find that running graph traversal algorithms helps in\nextracting dialogue flows. These flows are a better representation of the\nunderlying interactions than flows extracted by prompting LLMs. Our technique\nfocuses on giving conversation designers greater control, offering a\nproductivity tool to improve the process of developing dialogue policies.", "published": "2024-06-21 14:57:25", "link": "http://arxiv.org/abs/2406.15214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A LLM-Based Ranking Method for the Evaluation of Automatic\n  Counter-Narrative Generation", "abstract": "This paper proposes a novel approach to evaluate Counter Narrative (CN)\ngeneration using a Large Language Model (LLM) as an evaluator. We show that\ntraditional automatic metrics correlate poorly with human judgements and fail\nto capture the nuanced relationship between generated CNs and human perception.\nTo alleviate this, we introduce a model ranking pipeline based on pairwise\ncomparisons of generated CNs from different models, organized in a\ntournament-style format. The proposed evaluation method achieves a high\ncorrelation with human preference, with a $\\rho$ score of 0.88. As an\nadditional contribution, we leverage LLMs as zero-shot CN generators and\nprovide a comparative analysis of chat, instruct, and base models, exploring\ntheir respective strengths and limitations. Through meticulous evaluation,\nincluding fine-tuning experiments, we elucidate the differences in performance\nand responsiveness to domain-specific data. We conclude that chat-aligned\nmodels in zero-shot are the best option for carrying out the task, provided\nthey do not refuse to generate an answer due to security concerns.", "published": "2024-06-21 15:11:33", "link": "http://arxiv.org/abs/2406.15227v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perception of Phonological Assimilation by Neural Speech Recognition\n  Models", "abstract": "Human listeners effortlessly compensate for phonological changes during\nspeech perception, often unconsciously inferring the intended sounds. For\nexample, listeners infer the underlying /n/ when hearing an utterance such as\n\"clea[m] pan\", where [m] arises from place assimilation to the following labial\n[p]. This article explores how the neural speech recognition model Wav2Vec2\nperceives assimilated sounds, and identifies the linguistic knowledge that is\nimplemented by the model to compensate for assimilation during Automatic Speech\nRecognition (ASR). Using psycholinguistic stimuli, we systematically analyze\nhow various linguistic context cues influence compensation patterns in the\nmodel's output. Complementing these behavioral experiments, our probing\nexperiments indicate that the model shifts its interpretation of assimilated\nsounds from their acoustic form to their underlying form in its final layers.\nFinally, our causal intervention experiments suggest that the model relies on\nminimal phonological context cues to accomplish this shift. These findings\nrepresent a step towards better understanding the similarities and differences\nin phonological processing between neural ASR models and humans.", "published": "2024-06-21 15:58:22", "link": "http://arxiv.org/abs/2406.15265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Diversity in Automatic Poetry Generation", "abstract": "Natural Language Generation (NLG), and more generally generative AI, are\namong the currently most impactful research fields. Creative NLG, such as\nautomatic poetry generation, is a fascinating niche in this area. While most\nprevious research has focused on forms of the Turing test when evaluating\nautomatic poetry generation -- can humans distinguish between automatic and\nhuman generated poetry -- we evaluate the diversity of automatically generated\npoetry (with a focus on quatrains), by comparing distributions of generated\npoetry to distributions of human poetry along structural, lexical, semantic and\nstylistic dimensions, assessing different model types (word vs.\ncharacter-level, general purpose LLMs vs. poetry-specific models), including\nthe very recent LLaMA3-8B, and types of fine-tuning (conditioned vs.\nunconditioned). We find that current automatic poetry systems are considerably\nunderdiverse along multiple dimensions -- they often do not rhyme sufficiently,\nare semantically too uniform and even do not match the length distribution of\nhuman poetry. Our experiments reveal, however, that style-conditioning and\ncharacter-level modeling clearly increases diversity across virtually all\ndimensions we explore. Our identified limitations may serve as the basis for\nmore genuinely diverse future poetry generation models.", "published": "2024-06-21 16:03:21", "link": "http://arxiv.org/abs/2406.15267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How language models extrapolate outside the training data: A case study\n  in Textualized Gridworld", "abstract": "Language models' ability to extrapolate learned behaviors to novel, more\ncomplex environments beyond their training scope is highly unknown. This study\nintroduces a path planning task in a textualized Gridworld to probe language\nmodels' extrapolation capabilities. We show that conventional approaches,\nincluding next token prediction and Chain of Thought (CoT) finetuning, fail to\nextrapolate in larger, unseen environments. Inspired by human cognition and\ndual process theory, we propose cognitive maps for path planning, a novel CoT\nframework that simulates humanlike mental representations. Our experiments show\nthat cognitive maps not only enhance extrapolation to unseen environments but\nalso exhibit humanlike characteristics through structured mental simulation and\nrapid adaptation. Our finding that these cognitive maps require specialized\ntraining schemes and cannot be induced through simple prompting opens up\nimportant questions about developing general-purpose cognitive maps in language\nmodels. Our comparison with exploration-based methods further illuminates the\ncomplementary strengths of offline planning and online exploration.", "published": "2024-06-21 16:10:05", "link": "http://arxiv.org/abs/2406.15275v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP-KG: A System for Exploratory Search of Scientific Literature in\n  Natural Language Processing", "abstract": "Scientific literature searches are often exploratory, whereby users are not\nyet familiar with a particular field or concept but are interested in learning\nmore about it. However, existing systems for scientific literature search are\ntypically tailored to keyword-based lookup searches, limiting the possibilities\nfor exploration. We propose NLP-KG, a feature-rich system designed to support\nthe exploration of research literature in unfamiliar natural language\nprocessing (NLP) fields. In addition to a semantic search, NLP-KG allows users\nto easily find survey papers that provide a quick introduction to a field of\ninterest. Further, a Fields of Study hierarchy graph enables users to\nfamiliarize themselves with a field and its related areas. Finally, a chat\ninterface allows users to ask questions about unfamiliar concepts or specific\narticles in NLP and obtain answers grounded in knowledge retrieved from\nscientific publications. Our system provides users with comprehensive\nexploration possibilities, supporting them in investigating the relationships\nbetween different fields, understanding unfamiliar concepts in NLP, and finding\nrelevant research literature. Demo, video, and code are available at:\nhttps://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.", "published": "2024-06-21 16:38:22", "link": "http://arxiv.org/abs/2406.15294v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A SMART Mnemonic Sounds like \"Glue Tonic\": Mixing LLMs with Student\n  Feedback to Make Mnemonic Learning Stick", "abstract": "Keyword mnemonics are memorable explanations that link new terms to simpler\nkeywords. Prior work generates mnemonics for students, but they do not train\nmodels using mnemonics students prefer and aid learning. We build SMART, a\nmnemonic generator trained on feedback from real students learning new terms.\nTo train SMART, we first fine-tune LLaMA-2 on a curated set of user-written\nmnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics\ngenerated by SMART in a flashcard app to find preferences on mnemonics students\nfavor. We gather 2684 preferences from 45 students across two types: expressed\n(inferred from ratings) and observed (inferred from student learning), yielding\nthree key findings. First, expressed and observed preferences disagree; what\nstudents think is helpful does not always capture what is truly helpful.\nSecond, Bayesian models can synthesize complementary data from multiple\npreference types into a single effectiveness signal. SMART is tuned via Direct\nPreference Optimization on this signal, which resolves ties and missing labels\nin the typical method of pairwise comparisons, augmenting data for LLM output\nquality gains. Third, mnemonic experts assess SMART as matching GPT-4 at much\nlower deployment costs, showing the utility of capturing diverse student\nfeedback to align LLMs in education.", "published": "2024-06-21 17:59:51", "link": "http://arxiv.org/abs/2406.15352v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship\n  Embeddings", "abstract": "The goal of text style transfer is to transform the style of texts while\npreserving their original meaning, often with only a few examples of the target\nstyle. Existing style transfer methods generally rely on the few-shot\ncapabilities of large language models or on complex controllable text\ngeneration approaches that are inefficient and underperform on fluency metrics.\nWe introduce TinyStyler, a lightweight but effective approach, which leverages\na small language model (800M params) and pre-trained authorship embeddings to\nperform efficient, few-shot text style transfer. We evaluate on the challenging\ntask of authorship style transfer and find TinyStyler outperforms strong\napproaches such as GPT-4. We also evaluate TinyStyler's ability to perform text\nattribute style transfer (formal $\\leftrightarrow$ informal) with automatic and\nhuman evaluations and find that the approach outperforms recent controllable\ntext generation methods. Our model has been made publicly available at\nhttps://huggingface.co/tinystyler/tinystyler .", "published": "2024-06-21 18:41:22", "link": "http://arxiv.org/abs/2406.15586v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToVo: Toxicity Taxonomy via Voting", "abstract": "Existing toxic detection models face significant limitations, such as lack of\ntransparency, customization, and reproducibility. These challenges stem from\nthe closed-source nature of their training data and the paucity of explanations\nfor their evaluation mechanism. To address these issues, we propose a dataset\ncreation mechanism that integrates voting and chain-of-thought processes,\nproducing a high-quality open-source dataset for toxic content detection. Our\nmethodology ensures diverse classification metrics for each sample and includes\nboth classification scores and explanatory reasoning for the classifications.\n  We utilize the dataset created through our proposed mechanism to train our\nmodel, which is then compared against existing widely-used detectors. Our\napproach not only enhances transparency and customizability but also\nfacilitates better fine-tuning for specific use cases. This work contributes a\nrobust framework for developing toxic content detection models, emphasizing\nopenness and adaptability, thus paving the way for more effective and\nuser-specific content moderation solutions.", "published": "2024-06-21 02:35:30", "link": "http://arxiv.org/abs/2406.14835v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with\n  Large Language Models", "abstract": "Recent studies have demonstrated the effectiveness of using large language\nlanguage models (LLMs) in passage ranking. The listwise approaches, such as\nRankGPT, have become new state-of-the-art in this task. However, the efficiency\nof RankGPT models is limited by the maximum context length and relatively high\nlatency of LLM inference. To address these issues, in this paper, we propose\nPE-Rank, leveraging the single passage embedding as a good context compression\nfor efficient listwise passage reranking. By treating each passage as a special\ntoken, we can directly input passage embeddings into LLMs, thereby reducing\ninput length. Additionally, we introduce an inference method that dynamically\nconstrains the decoding space to these special tokens, accelerating the\ndecoding process. For adapting the model to reranking, we employ listwise\nlearning to rank loss for training. Evaluation results on multiple benchmarks\ndemonstrate that PE-Rank significantly improves efficiency in both prefilling\nand decoding, while maintaining competitive ranking effectiveness. The Code is\navailable at https://github.com/liuqi6777/pe_rank.", "published": "2024-06-21 03:33:51", "link": "http://arxiv.org/abs/2406.14848v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "abstract": "The rapid development of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) has exposed vulnerabilities to various adversarial\nattacks. This paper provides a comprehensive overview of jailbreaking research\ntargeting both LLMs and MLLMs, highlighting recent advancements in evaluation\nbenchmarks, attack techniques and defense strategies. Compared to the more\nadvanced state of unimodal jailbreaking, multimodal domain remains\nunderexplored. We summarize the limitations and potential research directions\nof multimodal jailbreaking, aiming to inspire future research and further\nenhance the robustness and security of MLLMs.", "published": "2024-06-21 04:33:48", "link": "http://arxiv.org/abs/2406.14859v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Direct Multi-Turn Preference Optimization for Language Agents", "abstract": "Adapting Large Language Models (LLMs) for agent tasks is critical in\ndeveloping language agents. Direct Preference Optimization (DPO) is a promising\ntechnique for this adaptation with the alleviation of compounding errors,\noffering a means to directly optimize Reinforcement Learning (RL) objectives.\nHowever, applying DPO to multi-turn tasks presents challenges due to the\ninability to cancel the partition function. Overcoming this obstacle involves\nmaking the partition function independent of the current state and addressing\nlength disparities between preferred and dis-preferred trajectories. In this\nlight, we replace the policy constraint with the state-action occupancy measure\nconstraint in the RL objective and add length normalization to the\nBradley-Terry model, yielding a novel loss function named DMPO for multi-turn\nagent tasks with theoretical explanations. Extensive experiments on three\nmulti-turn agent task datasets confirm the effectiveness and superiority of the\nDMPO loss. The code is available at https://github.com/swt-user/DMPO.", "published": "2024-06-21 05:13:20", "link": "http://arxiv.org/abs/2406.14868v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OATH-Frames: Characterizing Online Attitudes Towards Homelessness with\n  LLM Assistants", "abstract": "Warning: Contents of this paper may be upsetting. Public attitudes towards\nkey societal issues, expressed on online media, are of immense value in policy\nand reform efforts, yet challenging to understand at scale. We study one such\nsocial issue: homelessness in the U.S., by leveraging the remarkable\ncapabilities of large language models to assist social work experts in\nanalyzing millions of posts from Twitter. We introduce a framing typology:\nOnline Attitudes Towards Homelessness (OATH) Frames: nine hierarchical frames\ncapturing critiques, responses and perceptions. We release annotations with\nvarying degrees of assistance from language models, with immense benefits in\nscaling: 6.5x speedup in annotation time while only incurring a 3 point F1\nreduction in performance with respect to the domain experts. Our experiments\ndemonstrate the value of modeling OATH-Frames over existing sentiment and\ntoxicity classifiers. Our large-scale analysis with predicted OATH-Frames on\n2.4M posts on homelessness reveal key trends in attitudes across states, time\nperiods and vulnerable populations, enabling new insights on the issue. Our\nwork provides a general framework to understand nuanced public attitudes at\nscale, on issues beyond homelessness.", "published": "2024-06-21 06:09:47", "link": "http://arxiv.org/abs/2406.14883v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate\n  Predictions", "abstract": "Despite recent advances in end-to-end speech recognition methods, their\noutput is biased to the training data's vocabulary, resulting in inaccurate\nrecognition of unknown terms or proper nouns. To improve the recognition\naccuracy for a given set of such terms, we propose an adaptation parameter-free\napproach based on Self-conditioned CTC. Our method improves the recognition\naccuracy of misrecognized target keywords by substituting their intermediate\nCTC predictions with corrected labels, which are then passed on to the\nsubsequent layers. First, we create pairs of correct labels and recognition\nerror instances for a keyword list using Text-to-Speech and a recognition\nmodel. We use these pairs to replace intermediate prediction errors by the\nlabels. Conditioning the subsequent layers of the encoder on the labels, it is\npossible to acoustically evaluate the target keywords. Experiments conducted in\nJapanese demonstrated that our method successfully improved the F1 score for\nunknown words.", "published": "2024-06-21 06:25:10", "link": "http://arxiv.org/abs/2406.14890v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop\n  Question Answering", "abstract": "Multi-Hop Question Answering (MHQA) tasks present a significant challenge for\nlarge language models (LLMs) due to the intensive knowledge required. Current\nsolutions, like Retrieval-Augmented Generation, typically retrieve potential\ndocuments from an external corpus to read an answer. However, the performance\nof this retrieve-then-read paradigm is constrained by the retriever and the\ninevitable noise in the retrieved documents. To mitigate these challenges, we\nintroduce a novel generate-then-ground (GenGround) framework, synergizing the\nparametric knowledge of LLMs and external documents to solve a multi-hop\nquestion. GenGround empowers LLMs to alternate two phases until the final\nanswer is derived: (1) formulate a simpler, single-hop question and directly\ngenerate the answer; (2) ground the question-answer pair in retrieved\ndocuments, amending any wrong predictions in the answer. We also propose an\ninstructional grounding distillation method to generalize our method into\nsmaller models. Extensive experiments conducted on four datasets illustrate the\nsuperiority of our method.", "published": "2024-06-21 06:26:38", "link": "http://arxiv.org/abs/2406.14891v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Safely Learning with Private Data: A Federated Learning Framework for\n  Large Language Model", "abstract": "Private data, being larger and quality-higher than public data, can greatly\nimprove large language models (LLM). However, due to privacy concerns, this\ndata is often dispersed in multiple silos, making its secure utilization for\nLLM training a challenge. Federated learning (FL) is an ideal solution for\ntraining models with distributed private data, but traditional frameworks like\nFedAvg are unsuitable for LLM due to their high computational demands on\nclients. An alternative, split learning, offloads most training parameters to\nthe server while training embedding and output layers locally, making it more\nsuitable for LLM. Nonetheless, it faces significant challenges in security and\nefficiency. Firstly, the gradients of embeddings are prone to attacks, leading\nto potential reverse engineering of private data. Furthermore, the server's\nlimitation of handle only one client's training request at a time hinders\nparallel training, severely impacting training efficiency. In this paper, we\npropose a Federated Learning framework for LLM, named FL-GLM, which prevents\ndata leakage caused by both server-side and peer-client attacks while improving\ntraining efficiency. Specifically, we first place the input block and output\nblock on local client to prevent embedding gradient attacks from server.\nSecondly, we employ key-encryption during client-server communication to\nprevent reverse engineering attacks from peer-clients. Lastly, we employ\noptimization methods like client-batching or server-hierarchical, adopting\ndifferent acceleration methods based on the actual computational capabilities\nof the server. Experimental results on NLU and generation tasks demonstrate\nthat FL-GLM achieves comparable metrics to centralized chatGLM model,\nvalidating the effectiveness of our federated learning framework.", "published": "2024-06-21 06:43:15", "link": "http://arxiv.org/abs/2406.14898v4", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Towards Retrieval Augmented Generation over Large Video Libraries", "abstract": "Video content creators need efficient tools to repurpose content, a task that\noften requires complex manual or automated searches. Crafting a new video from\nlarge video libraries remains a challenge. In this paper we introduce the task\nof Video Library Question Answering (VLQA) through an interoperable\narchitecture that applies Retrieval Augmented Generation (RAG) to video\nlibraries. We propose a system that uses large language models (LLMs) to\ngenerate search queries, retrieving relevant video moments indexed by speech\nand visual metadata. An answer generation module then integrates user queries\nwith this metadata to produce responses with specific video timestamps. This\napproach shows promise in multimedia content retrieval, and AI-assisted video\ncontent creation.", "published": "2024-06-21 07:52:01", "link": "http://arxiv.org/abs/2406.14938v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unlocking the Global Synergies in Low-Rank Adapters", "abstract": "Low-rank Adaption (LoRA) has been the de-facto parameter-efficient\nfine-tuning technique for large language models. We present HeteroLoRA, a\nlight-weight search algorithm that leverages zero-cost proxies to allocate the\nlimited LoRA trainable parameters across the model for better fine-tuned\nperformance. In addition to the allocation for the standard LoRA-adapted\nmodels, we also demonstrate the efficacy of HeteroLoRA by performing the\nallocation in a more challenging search space that includes LoRA modules and\nLoRA-adapted shortcut connections. Experiments show that HeteroLoRA enables\nimprovements in model performance given the same parameter budge. For example,\non MRPC, we see an improvement of 1.6% in accuracy with similar training\nparameter budget. We will open-source our algorithm once the paper is accepted.", "published": "2024-06-21 08:10:03", "link": "http://arxiv.org/abs/2406.14956v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems", "abstract": "Retrieval Augmented Generation (RAG) represents a significant advancement in\nartificial intelligence combining a retrieval phase with a generative phase,\nwith the latter typically being powered by large language models (LLMs). The\ncurrent common practices in RAG involve using \"instructed\" LLMs, which are\nfine-tuned with supervised training to enhance their ability to follow\ninstructions and are aligned with human preferences using state-of-the-art\ntechniques. Contrary to popular belief, our study demonstrates that base models\noutperform their instructed counterparts in RAG tasks by 20% on average under\nour experimental settings. This finding challenges the prevailing assumptions\nabout the superiority of instructed LLMs in RAG applications. Further\ninvestigations reveal a more nuanced situation, questioning fundamental aspects\nof RAG and suggesting the need for broader discussions on the topic; or, as\nFromm would have it, \"Seldom is a glance at the statistics enough to understand\nthe meaning of the figures\".", "published": "2024-06-21 08:31:02", "link": "http://arxiv.org/abs/2406.14972v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Exhibit Cognitive Dissonance? Studying the\n  Difference Between Revealed Beliefs and Stated Answers", "abstract": "Prompting and Multiple Choices Questions (MCQ) have become the preferred\napproach to assess the capabilities of Large Language Models (LLMs), due to\ntheir ease of manipulation and evaluation. Such experimental appraisals have\npointed toward the LLMs' apparent ability to perform causal reasoning or to\ngrasp uncertainty. In this paper, we investigate whether these abilities are\nmeasurable outside of tailored prompting and MCQ by reformulating these issues\nas direct text completion - the foundation of LLMs. To achieve this goal, we\ndefine scenarios with multiple possible outcomes and we compare the prediction\nmade by the LLM through prompting (their Stated Answer) to the probability\ndistributions they compute over these outcomes during next token prediction\n(their Revealed Belief). Our findings suggest that the Revealed Belief of LLMs\nsignificantly differs from their Stated Answer and hint at multiple biases and\nmisrepresentations that their beliefs may yield in many scenarios and outcomes.\nAs text completion is at the core of LLMs, these results suggest that common\nevaluation methods may only provide a partial picture and that more research is\nneeded to assess the extent and nature of their capabilities.", "published": "2024-06-21 08:56:35", "link": "http://arxiv.org/abs/2406.14986v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SpreadsheetBench: Towards Challenging Real World Spreadsheet\n  Manipulation", "abstract": "We introduce SpreadsheetBench, a challenging spreadsheet manipulation\nbenchmark exclusively derived from real-world scenarios, designed to immerse\ncurrent large language models (LLMs) in the actual workflow of spreadsheet\nusers. Unlike existing benchmarks that rely on synthesized queries and\nsimplified spreadsheet files, SpreadsheetBench is built from 912 real questions\ngathered from online Excel forums, which reflect the intricate needs of users.\nThe associated spreadsheets from the forums contain a variety of tabular data\nsuch as multiple tables, non-standard relational tables, and abundant\nnon-textual elements. Furthermore, we propose a more reliable evaluation metric\nakin to online judge platforms, where multiple spreadsheet files are created as\ntest cases for each instruction, ensuring the evaluation of robust solutions\ncapable of handling spreadsheets with varying values. Our comprehensive\nevaluation of various LLMs under both single-round and multi-round inference\nsettings reveals a substantial gap between the state-of-the-art (SOTA) models\nand human performance, highlighting the benchmark's difficulty.", "published": "2024-06-21 09:06:45", "link": "http://arxiv.org/abs/2406.14991v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Disability Representations: Finding Biases in Automatic Image Generation", "abstract": "Recent advancements in image generation technology have enabled widespread\naccess to AI-generated imagery, prominently used in advertising, entertainment,\nand progressively in every form of visual content. However, these technologies\noften perpetuate societal biases. This study investigates the representation\nbiases in popular image generation models towards people with disabilities\n(PWD). Through a comprehensive experiment involving several popular\ntext-to-image models, we analyzed the depiction of disability. The results\nindicate a significant bias, with most generated images portraying disabled\nindividuals as old, sad, and predominantly using manual wheelchairs. These\nfindings highlight the urgent need for more inclusive AI development, ensuring\ndiverse and accurate representation of PWD in generated images. This research\nunderscores the importance of addressing and mitigating biases in AI models to\nfoster equitable and realistic representations.", "published": "2024-06-21 09:12:31", "link": "http://arxiv.org/abs/2406.14993v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unveiling the Impact of Multi-Modal Interactions on User Engagement: A\n  Comprehensive Evaluation in AI-driven Conversations", "abstract": "Large Language Models (LLMs) have significantly advanced user-bot\ninteractions, enabling more complex and coherent dialogues. However, the\nprevalent text-only modality might not fully exploit the potential for\neffective user engagement. This paper explores the impact of multi-modal\ninteractions, which incorporate images and audio alongside text, on user\nengagement in chatbot conversations. We conduct a comprehensive analysis using\na diverse set of chatbots and real-user interaction data, employing metrics\nsuch as retention rate and conversation length to evaluate user engagement. Our\nfindings reveal a significant enhancement in user engagement with multi-modal\ninteractions compared to text-only dialogues. Notably, the incorporation of a\nthird modality significantly amplifies engagement beyond the benefits observed\nwith just two modalities. These results suggest that multi-modal interactions\noptimize cognitive processing and facilitate richer information comprehension.\nThis study underscores the importance of multi-modality in chatbot design,\noffering valuable insights for creating more engaging and immersive AI\ncommunication experiences and informing the broader AI community about the\nbenefits of multi-modal interactions in enhancing user engagement.", "published": "2024-06-21 09:26:55", "link": "http://arxiv.org/abs/2406.15000v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GiusBERTo: A Legal Language Model for Personal Data De-identification in\n  Italian Court of Auditors Decisions", "abstract": "Recent advances in Natural Language Processing have demonstrated the\neffectiveness of pretrained language models like BERT for a variety of\ndownstream tasks. We present GiusBERTo, the first BERT-based model specialized\nfor anonymizing personal data in Italian legal documents. GiusBERTo is trained\non a large dataset of Court of Auditors decisions to recognize entities to\nanonymize, including names, dates, locations, while retaining contextual\nrelevance. We evaluate GiusBERTo on a held-out test set and achieve 97%\ntoken-level accuracy. GiusBERTo provides the Italian legal community with an\naccurate and tailored BERT model for de-identification, balancing privacy and\ndata protection.", "published": "2024-06-21 10:25:26", "link": "http://arxiv.org/abs/2406.15032v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Brain-Like Language Processing via a Shallow Untrained Multihead\n  Attention Network", "abstract": "Large Language Models (LLMs) have been shown to be effective models of the\nhuman language system, with some models predicting most explainable variance of\nbrain activity in current datasets. Even in untrained models, the\nrepresentations induced by architectural priors can exhibit reasonable\nalignment to brain data. In this work, we investigate the key architectural\ncomponents driving the surprising alignment of untrained models. To estimate\nLLM-to-brain similarity, we first select language-selective units within an\nLLM, similar to how neuroscientists identify the language network in the human\nbrain. We then benchmark the brain alignment of these LLM units across five\ndifferent brain recording datasets. By isolating critical components of the\nTransformer architecture, we identify tokenization strategy and multihead\nattention as the two major components driving brain alignment. A simple form of\nrecurrence further improves alignment. We further demonstrate this quantitative\nbrain alignment of our model by reproducing landmark studies in the language\nneuroscience field, showing that localized model units -- just like language\nvoxels measured empirically in the human brain -- discriminate more reliably\nbetween lexical than syntactic differences, and exhibit similar response\nprofiles under the same experimental conditions. Finally, we demonstrate the\nutility of our model's representations for language modeling, achieving\nimproved sample and parameter efficiency over comparable architectures. Our\nmodel's estimates of surprisal sets a new state-of-the-art in the behavioral\nalignment to human reading times. Taken together, we propose a highly brain-\nand behaviorally-aligned model that conceptualizes the human language system as\nan untrained shallow feature encoder, with structural priors, combined with a\ntrained decoder to achieve efficient and performant language processing.", "published": "2024-06-21 12:54:03", "link": "http://arxiv.org/abs/2406.15109v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing Good, Bad and Ugly Arguments Generated by ChatGPT: a New\n  Dataset, its Methodology and Associated Tasks", "abstract": "The recent success of Large Language Models (LLMs) has sparked concerns about\ntheir potential to spread misinformation. As a result, there is a pressing need\nfor tools to identify ``fake arguments'' generated by such models. To create\nthese tools, examples of texts generated by LLMs are needed. This paper\nintroduces a methodology to obtain good, bad and ugly arguments from\nargumentative essays produced by ChatGPT, OpenAI's LLM. We then describe a\nnovel dataset containing a set of diverse arguments, ArGPT. We assess the\neffectiveness of our dataset and establish baselines for several\nargumentation-related tasks. Finally, we show that the artificially generated\ndata relates well to human argumentation and thus is useful as a tool to train\nand test systems for the defined tasks.", "published": "2024-06-21 13:27:10", "link": "http://arxiv.org/abs/2406.15130v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Idiomatic Representation in Multiple Languages via an Adaptive\n  Contrastive Triplet Loss", "abstract": "Accurately modeling idiomatic or non-compositional language has been a\nlongstanding challenge in Natural Language Processing (NLP). This is partly\nbecause these expressions do not derive their meanings solely from their\nconstituent words, but also due to the scarcity of relevant data resources, and\ntheir impact on the performance of downstream tasks such as machine translation\nand simplification. In this paper we propose an approach to model idiomaticity\neffectively using a triplet loss that incorporates the asymmetric contribution\nof components words to an idiomatic meaning for training language models by\nusing adaptive contrastive learning and resampling miners to build an\nidiomatic-aware learning objective. Our proposed method is evaluated on a\nSemEval challenge and outperforms previous alternatives significantly in many\nmetrics.", "published": "2024-06-21 14:21:41", "link": "http://arxiv.org/abs/2406.15175v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Effective is GPT-4 Turbo in Generating School-Level Questions from\n  Textbooks Based on Bloom's Revised Taxonomy?", "abstract": "We evaluate the effectiveness of GPT-4 Turbo in generating educational\nquestions from NCERT textbooks in zero-shot mode. Our study highlights GPT-4\nTurbo's ability to generate questions that require higher-order thinking\nskills, especially at the \"understanding\" level according to Bloom's Revised\nTaxonomy. While we find a notable consistency between questions generated by\nGPT-4 Turbo and those assessed by humans in terms of complexity, there are\noccasional differences. Our evaluation also uncovers variations in how humans\nand machines evaluate question quality, with a trend inversely related to\nBloom's Revised Taxonomy levels. These findings suggest that while GPT-4 Turbo\nis a promising tool for educational question generation, its efficacy varies\nacross different cognitive levels, indicating a need for further refinement to\nfully meet educational standards.", "published": "2024-06-21 14:52:37", "link": "http://arxiv.org/abs/2406.15211v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Morphological Tree Tokenizer", "abstract": "As a cornerstone in language modeling, tokenization involves segmenting text\ninputs into pre-defined atomic units. Conventional statistical tokenizers often\ndisrupt constituent boundaries within words, thereby corrupting semantic\ninformation. To address this drawback, we introduce morphological structure\nguidance to tokenization and propose a deep model to induce character-level\nstructures of words. Specifically, the deep model jointly encodes internal\nstructures and representations of words with a mechanism named\n$\\textit{MorphOverriding}$ to ensure the indecomposability of morphemes. By\ntraining the model with self-supervised objectives, our method is capable of\ninducing character-level structures that align with morphological rules without\nannotated training data. Based on the induced structures, our algorithm\ntokenizes words through vocabulary matching in a top-down manner. Empirical\nresults indicate that the proposed method effectively retains complete\nmorphemes and outperforms widely adopted methods such as BPE and WordPiece on\nboth morphological segmentation tasks and language modeling tasks. The code\nwill be released later.", "published": "2024-06-21 15:35:49", "link": "http://arxiv.org/abs/2406.15245v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Fine-Grained Citation Evaluation in Generated Text: A\n  Comparative Analysis of Faithfulness Metrics", "abstract": "Large language models (LLMs) often produce unsupported or unverifiable\ncontent, known as \"hallucinations.\" To mitigate this, retrieval-augmented LLMs\nincorporate citations, grounding the content in verifiable sources. Despite\nsuch developments, manually assessing how well a citation supports the\nassociated statement remains a major challenge. Previous studies use\nfaithfulness metrics to estimate citation support automatically but are limited\nto binary classification, overlooking fine-grained citation support in\npractical scenarios. To investigate the effectiveness of faithfulness metrics\nin fine-grained scenarios, we propose a comparative evaluation framework that\nassesses the metric effectiveness in distinguishing citations between\nthree-category support levels: full, partial, and no support. Our framework\nemploys correlation analysis, classification evaluation, and retrieval\nevaluation to measure the alignment between metric scores and human judgments\ncomprehensively. Our results show no single metric consistently excels across\nall evaluations, revealing the complexity of assessing fine-grained support.\nBased on the findings, we provide practical recommendations for developing more\neffective metrics.", "published": "2024-06-21 15:57:24", "link": "http://arxiv.org/abs/2406.15264v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety\n  Alignment of Large Vision-Language Model", "abstract": "As Artificial General Intelligence (AGI) becomes increasingly integrated into\nvarious facets of human life, ensuring the safety and ethical alignment of such\nsystems is paramount. Previous studies primarily focus on single-modality\nthreats, which may not suffice given the integrated and complex nature of\ncross-modality interactions. We introduce a novel safety alignment challenge\ncalled Safe Inputs but Unsafe Output (SIUO) to evaluate cross-modality safety\nalignment. Specifically, it considers cases where single modalities are safe\nindependently but could potentially lead to unsafe or unethical outputs when\ncombined. To empirically investigate this problem, we developed the SIUO, a\ncross-modality benchmark encompassing 9 critical safety domains, such as\nself-harm, illegal activities, and privacy violations. Our findings reveal\nsubstantial safety vulnerabilities in both closed- and open-source LVLMs, such\nas GPT-4V and LLaVA, underscoring the inadequacy of current models to reliably\ninterpret and respond to complex, real-world scenarios.", "published": "2024-06-21 16:14:15", "link": "http://arxiv.org/abs/2406.15279v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "STARD: A Chinese Statute Retrieval Dataset with Real Queries Issued by\n  Non-professionals", "abstract": "Statute retrieval aims to find relevant statutory articles for specific\nqueries. This process is the basis of a wide range of legal applications such\nas legal advice, automated judicial decisions, legal document drafting, etc.\nExisting statute retrieval benchmarks focus on formal and professional queries\nfrom sources like bar exams and legal case documents, thereby neglecting\nnon-professional queries from the general public, which often lack precise\nlegal terminology and references. To address this gap, we introduce the STAtute\nRetrieval Dataset (STARD), a Chinese dataset comprising 1,543 query cases\ncollected from real-world legal consultations and 55,348 candidate statutory\narticles. Unlike existing statute retrieval datasets, which primarily focus on\nprofessional legal queries, STARD captures the complexity and diversity of real\nqueries from the general public. Through a comprehensive evaluation of various\nretrieval baselines, we reveal that existing retrieval approaches all fall\nshort of these real queries issued by non-professional users. The best method\nonly achieves a Recall@100 of 0.907, suggesting the necessity for further\nexploration and additional research in this area.\n  All the codes and datasets are available at:\nhttps://github.com/oneal2000/STARD/tree/main", "published": "2024-06-21 17:10:09", "link": "http://arxiv.org/abs/2406.15313v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs", "abstract": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to generate answers\nfrom the short retrieved units. The imbalanced `heavy' retriever and `light'\nreader design can lead to sub-optimal performance. The loss of contextual\ninformation in the short, chunked units may increase the likelihood of\nintroducing hard negatives during the retrieval stage. Additionally, the reader\nmight not fully leverage the capabilities of recent advancements in LLMs. In\norder to alleviate the imbalance, we propose a new framework LongRAG,\nconsisting of a `long retriever' and a `long reader'. In the two\nWikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire\nWikipedia corpus into 4K-token units by grouping related documents. By\nincreasing the unit size, we significantly reduce the total number of units.\nThis greatly reduces the burden on the retriever, resulting in strong retrieval\nperformance with only a few (less than 8) top units. Without requiring any\ntraining, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which\nare on par with the (fully-trained) SoTA model. Furthermore, we test on two\nnon-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes\neach individual document as a single (long) unit rather than chunking them into\nsmaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%\non MultiFieldQA-en. Our study offers insights into the future roadmap for\ncombining RAG with long-context LLMs.", "published": "2024-06-21 17:23:21", "link": "http://arxiv.org/abs/2406.15319v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Model Performance with Gradient-Based Parameter\n  Selection", "abstract": "Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.", "published": "2024-06-21 17:42:52", "link": "http://arxiv.org/abs/2406.15330v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Steering Without Side Effects: Improving Post-Deployment Control of\n  Language Models", "abstract": "Language models (LMs) have been shown to behave unexpectedly post-deployment.\nFor example, new jailbreaks continually arise, allowing model misuse, despite\nextensive red-teaming and adversarial training from developers. Given most\nmodel queries are unproblematic and frequent retraining results in unstable\nuser experience, methods for mitigation of worst-case behavior should be\ntargeted. One such method is classifying inputs as potentially problematic,\nthen selectively applying steering vectors on these problematic inputs, i.e.\nadding particular vectors to model hidden states. However, steering vectors can\nalso negatively affect model performance, which will be an issue on cases where\nthe classifier was incorrect. We present KL-then-steer (KTS), a technique that\ndecreases the side effects of steering while retaining its benefits, by first\ntraining a model to minimize Kullback-Leibler (KL) divergence between a steered\nand unsteered model on benign inputs, then steering the model that has\nundergone this training. Our best method prevents 44% of jailbreak attacks\ncompared to the original Llama-2-chat-7B model while maintaining helpfulness\n(as measured by MT-Bench) on benign requests almost on par with the original\nLM. To demonstrate the generality and transferability of our method beyond\njailbreaks, we show that our KTS model can be steered to reduce bias towards\nuser-suggested answers on TruthfulQA. Code is available:\nhttps://github.com/AsaCooperStickland/kl-then-steer.", "published": "2024-06-21 01:37:39", "link": "http://arxiv.org/abs/2406.15518v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking Pruning Large Language Models: Benefits and Pitfalls of\n  Reconstruction Error Minimization", "abstract": "This work suggests fundamentally rethinking the current practice of pruning\nlarge language models (LLMs). The way it is done is by divide and conquer:\nsplit the model into submodels, sequentially prune them, and reconstruct\npredictions of the dense counterparts on small calibration data one at a time;\nthe final model is obtained simply by putting the resulting sparse submodels\ntogether. While this approach enables pruning under memory constraints, it\ngenerates high reconstruction errors. In this work, we first present an array\nof reconstruction techniques that can significantly reduce this error by more\nthan $90\\%$. Unwittingly, however, we discover that minimizing reconstruction\nerror is not always ideal and can overfit the given calibration data, resulting\nin rather increased language perplexity and poor performance at downstream\ntasks. We find out that a strategy of self-generating calibration data can\nmitigate this trade-off between reconstruction and generalization, suggesting\nnew directions in the presence of both benefits and pitfalls of reconstruction\nfor pruning LLMs.", "published": "2024-06-21 05:13:34", "link": "http://arxiv.org/abs/2406.15524v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Efficient Evaluation of Large Language Models and Text-to-Image\n  Models via Adaptive Sampling", "abstract": "Evaluating LLMs and text-to-image models is a computationally intensive task\noften overlooked. Efficient evaluation is crucial for understanding the diverse\ncapabilities of these models and enabling comparisons across a growing number\nof new models and benchmarks. To address this, we introduce SubLIME, a\ndata-efficient evaluation framework that employs adaptive sampling techniques,\nsuch as clustering and quality-based methods, to create representative subsets\nof benchmarks. Our approach ensures statistically aligned model rankings\ncompared to full datasets, evidenced by high Pearson correlation coefficients.\nEmpirical analysis across six NLP benchmarks reveals that: (1) quality-based\nsampling consistently achieves strong correlations (0.85 to 0.95) with full\ndatasets at a 10\\% sampling rate such as Quality SE and Quality CPD (2)\nclustering methods excel in specific benchmarks such as MMLU (3) no single\nmethod universally outperforms others across all metrics. Extending this\nframework, we leverage the HEIM leaderboard to cover 25 text-to-image models on\n17 different benchmarks. SubLIME dynamically selects the optimal technique for\neach benchmark, significantly reducing evaluation costs while preserving\nranking integrity and score distribution. Notably, a minimal sampling rate of\n1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that\nemploying difficulty-based sampling to target more challenging benchmark\nsegments enhances model differentiation with broader score distributions. We\nalso combine semantic search, tool use, and GPT-4 review to identify redundancy\nacross benchmarks within specific LLM categories, such as coding benchmarks.\nThis allows us to further reduce the number of samples needed to maintain\ntargeted rank preservation. Overall, SubLIME offers a versatile and\ncost-effective solution for the robust evaluation of LLMs and text-to-image\nmodels.", "published": "2024-06-21 07:38:55", "link": "http://arxiv.org/abs/2406.15527v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DEM: Distribution Edited Model for Training with Mixed Data\n  Distributions", "abstract": "Training with mixed data distributions is a common and important part of\ncreating multi-task and instruction-following models. The diversity of the data\ndistributions and cost of joint training makes the optimization procedure\nextremely challenging. Data mixing methods partially address this problem,\nalbeit having a sub-optimal performance across data sources and require\nmultiple expensive training runs. In this paper, we propose a simple and\nefficient alternative for better optimization of the data sources by combining\nmodels individually trained on each data source with the base model using basic\nelement-wise vector operations. The resulting model, namely Distribution Edited\nModel (DEM), is 11x cheaper than standard data mixing and outperforms strong\nbaselines on a variety of benchmarks, yielding upto 6.2% improvement on MMLU,\n11.5% on BBH, 16.1% on DROP, 6% on MathQA, and 9.3% on HELM with models of size\n3B to 13B. Notably, DEM does not require full re-training when modifying a\nsingle data-source, thus making it very flexible and scalable for training with\ndiverse data sources.", "published": "2024-06-21 18:07:46", "link": "http://arxiv.org/abs/2406.15570v2", "categories": ["cs.CL", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Detecting AI-Generated Text: Factors Influencing Detectability with\n  Current Methods", "abstract": "Large language models (LLMs) have advanced to a point that even humans have\ndifficulty discerning whether a text was generated by another human, or by a\ncomputer. However, knowing whether a text was produced by human or artificial\nintelligence (AI) is important to determining its trustworthiness, and has\napplications in many domains including detecting fraud and academic dishonesty,\nas well as combating the spread of misinformation and political propaganda. The\ntask of AI-generated text (AIGT) detection is therefore both very challenging,\nand highly critical. In this survey, we summarize state-of-the art approaches\nto AIGT detection, including watermarking, statistical and stylistic analysis,\nand machine learning classification. We also provide information about existing\ndatasets for this task. Synthesizing the research findings, we aim to provide\ninsight into the salient factors that combine to determine how \"detectable\"\nAIGT text is under different scenarios, and to make practical recommendations\nfor future work towards this significant technical and societal challenge.", "published": "2024-06-21 18:31:49", "link": "http://arxiv.org/abs/2406.15583v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Benchmarking Uncertainty Quantification Methods for Large Language\n  Models with LM-Polygraph", "abstract": "The rapid proliferation of large language models (LLMs) has stimulated\nresearchers to seek effective and efficient approaches to deal with LLM\nhallucinations and low-quality outputs. Uncertainty quantification (UQ) is a\nkey element of machine learning applications in dealing with such challenges.\nHowever, research to date on UQ for LLMs has been fragmented in terms of\ntechniques and evaluation methodologies. In this work, we address this issue by\nintroducing a novel benchmark that implements a collection of state-of-the-art\nUQ baselines and offers an environment for controllable and consistent\nevaluation of novel UQ techniques over various text generation tasks. Our\nbenchmark also supports the assessment of confidence normalization methods in\nterms of their ability to provide interpretable scores. Using our benchmark, we\nconduct a large-scale empirical investigation of UQ and normalization\ntechniques across eleven tasks, identifying the most effective approaches.\nCode: https://github.com/IINemo/lm-polygraph Benchmark:\nhttps://huggingface.co/LM-Polygraph", "published": "2024-06-21 20:06:31", "link": "http://arxiv.org/abs/2406.15627v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models have Intrinsic Self-Correction Ability", "abstract": "Large language models (LLMs) have attracted significant attention for their\nexceptional abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.", "published": "2024-06-21 22:29:40", "link": "http://arxiv.org/abs/2406.15673v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AgriLLM: Harnessing Transformers for Farmer Queries", "abstract": "Agriculture, vital for global sustenance, necessitates innovative solutions\ndue to a lack of organized domain experts, particularly in developing countries\nwhere many farmers are impoverished and cannot afford expert consulting.\nInitiatives like Farmers Helpline play a crucial role in such countries, yet\nchallenges such as high operational costs persist. Automating query resolution\ncan alleviate the burden on traditional call centers, providing farmers with\nimmediate and contextually relevant information. The integration of Agriculture\nand Artificial Intelligence (AI) offers a transformative opportunity to empower\nfarmers and bridge information gaps. Language models like transformers, the\nrising stars of AI, possess remarkable language understanding capabilities,\nmaking them ideal for addressing information gaps in agriculture. This work\nexplores and demonstrates the transformative potential of Large Language Models\n(LLMs) in automating query resolution for agricultural farmers, leveraging\ntheir expertise in deciphering natural language and understanding context.\nUsing a subset of a vast dataset of real-world farmer queries collected in\nIndia, our study focuses on approximately 4 million queries from the state of\nTamil Nadu, spanning various sectors, seasonal crops, and query types.", "published": "2024-06-21 07:37:41", "link": "http://arxiv.org/abs/2407.04721v2", "categories": ["cs.CL", "cs.ET"], "primary_category": "cs.CL"}
{"title": "LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multimodal Large Language Models", "abstract": "Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\n\\textit{LatentExplainer}, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\n\\textit{LatentExplainer} tackles three main challenges: inferring the meaning\nof latent variables, aligning explanations with inductive biases, and handling\nvarying degrees of explainability. Our approach perturbs latent variables,\ninterpreting changes in generated data, and uses multi-modal large language\nmodels (MLLMs) to produce human-understandable explanations. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations for\nlatent variables. The results highlight the effectiveness of incorporating\ninductive biases and uncertainty quantification, significantly enhancing model\ninterpretability.", "published": "2024-06-21 04:39:03", "link": "http://arxiv.org/abs/2406.14862v5", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Investigating the Transferability of Code Repair for Low-Resource\n  Programming Languages", "abstract": "Large language models (LLMs) have shown remarkable performance on code\ngeneration tasks. A recent use case is iterative code repair, where an LLM\nfixes an incorrect program by rationalizing about errors and generating new\ncode. Recent works augment the code repair process by integrating modern\ntechniques such as chain-of-thought reasoning or distillation, but only study\ntheir benefits on high-resource languages like Python, and ignore low-resource\nlanguages like Perl. To address this gap of knowledge, we investigate the\nbenefits of distilling code repair for both high and low resource languages to\ndetermine if the techniques that are effective in a high resource setting are\nalso applicable in a low resource setting. Our evaluation shows that distilling\nthe ability to repair code has language dependent benefits. To explain this\nbehavior, we perform a further analysis and find that contrary to preexisting\nbeliefs, the correlation between reasoning ability and code correction ability\nis weak. We hypothesize this weak correlation is magnified in low-resource\nsettings where base models lack deep knowledge of a programming language,\nleading to wavering benefits of code repair.", "published": "2024-06-21 05:05:39", "link": "http://arxiv.org/abs/2406.14867v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model\n  Compression", "abstract": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction, boosting decode throughput by\n$6.6-8.2\\times$ and $1.7-1.9\\times$ compared to FlashAttention2 and vLLM, with\nminimal impact on performance. Our code is available at\n\\url{https://github.com/thu-nics/MoA}.", "published": "2024-06-21 06:58:37", "link": "http://arxiv.org/abs/2406.14909v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training\n  and Model Merging: A Comprehensive Evaluation", "abstract": "We conducted extensive experiments on domain adaptation of the\nMeta-Llama-3-70B-Instruct model on SEC data, exploring its performance on both\ngeneral and domain-specific benchmarks. Our focus included continual\npre-training (CPT) and model merging, aiming to enhance the model's\ndomain-specific capabilities while mitigating catastrophic forgetting. Through\nthis study, we evaluated the impact of integrating financial regulatory data\ninto a robust language model and examined the effectiveness of our model\nmerging techniques in preserving and improving the model's instructive\nabilities. The model is accessible at hugging face:\nhttps://huggingface.co/arcee-ai/Llama-3-SEC-Base, arcee-ai/Llama-3-SEC-Base.\nThis is an intermediate checkpoint of our final model, which has seen 20B\ntokens so far. The full model is still in the process of training. This is a\npreprint technical report with thorough evaluations to understand the entire\nprocess.", "published": "2024-06-21 08:29:31", "link": "http://arxiv.org/abs/2406.14971v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GraLMatch: Matching Groups of Entities with Graphs and Language Models", "abstract": "In this paper, we present an end-to-end multi-source Entity Matching problem,\nwhich we call entity group matching, where the goal is to assign to the same\ngroup, records originating from multiple data sources but representing the same\nreal-world entity. We focus on the effects of transitively matched records,\ni.e. the records connected by paths in the graph G = (V,E) whose nodes and\nedges represent the records and whether they are a match or not. We present a\nreal-world instance of this problem, where the challenge is to match records of\ncompanies and financial securities originating from different data providers.\nWe also introduce two new multi-source benchmark datasets that present similar\nmatching challenges as real-world records. A distinctive characteristic of\nthese records is that they are regularly updated following real-world events,\nbut updates are not applied uniformly across data sources. This phenomenon\nmakes the matching of certain groups of records only possible through the use\nof transitive information.\n  In our experiments, we illustrate how considering transitively matched\nrecords is challenging since a limited amount of false positive pairwise match\npredictions can throw off the group assignment of large quantities of records.\nThus, we propose GraLMatch, a method that can partially detect and remove false\npositive pairwise predictions through graph-based properties. Finally, we\nshowcase how fine-tuning a Transformer-based model (DistilBERT) on a reduced\nnumber of labeled samples yields a better final entity group matching than\ntraining on more samples and/or incorporating fine-tuning optimizations,\nillustrating how precision becomes the deciding factor in the entity group\nmatching of large volumes of records.", "published": "2024-06-21 09:44:16", "link": "http://arxiv.org/abs/2406.15015v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Online detection and infographic explanation of spam reviews with data\n  drift adaptation", "abstract": "Spam reviews are a pervasive problem on online platforms due to its\nsignificant impact on reputation. However, research into spam detection in data\nstreams is scarce. Another concern lies in their need for transparency.\nConsequently, this paper addresses those problems by proposing an online\nsolution for identifying and explaining spam reviews, incorporating data drift\nadaptation. It integrates (i) incremental profiling, (ii) data drift detection\n& adaptation, and (iii) identification of spam reviews employing Machine\nLearning. The explainable mechanism displays a visual and textual prediction\nexplanation in a dashboard. The best results obtained reached up to 87 % spam\nF-measure.", "published": "2024-06-21 10:35:46", "link": "http://arxiv.org/abs/2406.15038v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Tri-VQA: Triangular Reasoning Medical Visual Question Answering for\n  Multi-Attribute Analysis", "abstract": "The intersection of medical Visual Question Answering (Med-VQA) is a\nchallenging research topic with advantages including patient engagement and\nclinical expert involvement for second opinions. However, existing Med-VQA\nmethods based on joint embedding fail to explain whether their provided results\nare based on correct reasoning or coincidental answers, which undermines the\ncredibility of VQA answers. In this paper, we investigate the construction of a\nmore cohesive and stable Med-VQA structure. Motivated by causal effect, we\npropose a novel Triangular Reasoning VQA (Tri-VQA) framework, which constructs\nreverse causal questions from the perspective of \"Why this answer?\" to\nelucidate the source of the answer and stimulate more reasonable forward\nreasoning processes. We evaluate our method on the Endoscopic Ultrasound (EUS)\nmulti-attribute annotated dataset from five centers, and test it on medical VQA\ndatasets. Experimental results demonstrate the superiority of our approach over\nexisting methods. Our codes and pre-trained models are available at\nhttps://anonymous.4open.science/r/Tri_VQA.", "published": "2024-06-21 10:50:55", "link": "http://arxiv.org/abs/2406.15050v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "I.2.7; I.2.10; J.3"], "primary_category": "cs.LG"}
{"title": "Investigating the impact of 2D gesture representation on co-speech\n  gesture generation", "abstract": "Co-speech gestures play a crucial role in the interactions between humans and\nembodied conversational agents (ECA). Recent deep learning methods enable the\ngeneration of realistic, natural co-speech gestures synchronized with speech,\nbut such approaches require large amounts of training data. \"In-the-wild\"\ndatasets, which compile videos from sources such as YouTube through human pose\ndetection models, offer a solution by providing 2D skeleton sequences that are\npaired with speech. Concurrently, innovative lifting models have emerged,\ncapable of transforming these 2D pose sequences into their 3D counterparts,\nleading to large and diverse datasets of 3D gestures. However, the derived 3D\npose estimation is essentially a pseudo-ground truth, with the actual ground\ntruth being the 2D motion data. This distinction raises questions about the\nimpact of gesture representation dimensionality on the quality of generated\nmotions, a topic that, to our knowledge, remains largely unexplored. In this\nwork, we evaluate the impact of the dimensionality of the training data, 2D or\n3D joint coordinates, on the performance of a multimodal speech-to-gesture deep\ngenerative model. We use a lifting model to convert 2D-generated sequences of\nbody pose to 3D. Then, we compare the sequence of gestures generated directly\nin 3D to the gestures generated in 2D and lifted to 3D as post-processing.", "published": "2024-06-21 12:59:20", "link": "http://arxiv.org/abs/2406.15111v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Synthetic Lyrics Detection Across Languages and Genres", "abstract": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the modality of lyrics in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We conducted a comprehensive\nevaluation of existing synthetic text detection features on this novel data\ntype. Additionally, we explored strategies to adjust the best feature for\nlyrics using unsupervised adaptation. Adhering to constraints of our\napplication domain, we investigated cross-lingual generalization, data\nscalability, robustness to language combinations, and the impact of genre\nnovelty in a few-shot detection scenario. Our findings show promising results\nwithin language families and similar genres, yet challenges persist with lyrics\nin languages that exhibit distinct semantic structures.", "published": "2024-06-21 15:19:21", "link": "http://arxiv.org/abs/2406.15231v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Greek podcast corpus: Competitive speech models for low-resourced\n  languages with weakly supervised data", "abstract": "The development of speech technologies for languages with limited digital\nrepresentation poses significant challenges, primarily due to the scarcity of\navailable data. This issue is exacerbated in the era of large, data-intensive\nmodels. Recent research has underscored the potential of leveraging weak\nsupervision to augment the pool of available data. In this study, we compile an\n800-hour corpus of Modern Greek from podcasts and employ Whisper large-v3 to\ngenerate silver transcriptions. This corpus is utilized to fine-tune our\nmodels, aiming to assess the efficacy of this approach in enhancing ASR\nperformance. Our analysis spans 16 distinct podcast domains, alongside\nevaluations on established datasets for Modern Greek. The findings indicate\nconsistent WER improvements, correlating with increases in both data volume and\nmodel size. Our study confirms that assembling large, weakly supervised corpora\nserves as a cost-effective strategy for advancing speech technologies in\nunder-resourced languages.", "published": "2024-06-21 16:28:47", "link": "http://arxiv.org/abs/2406.15284v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning", "abstract": "The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV) -- compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference. Code: https://github.com/Brandon3964/MultiModal-Task-Vector", "published": "2024-06-21 17:50:02", "link": "http://arxiv.org/abs/2406.15334v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Geneverse: A collection of Open-source Multimodal Large Language Models\n  for Genomic and Proteomic Research", "abstract": "The applications of large language models (LLMs) are promising for biomedical\nand healthcare research. Despite the availability of open-source LLMs trained\nusing a wide range of biomedical data, current research on the applications of\nLLMs to genomics and proteomics is still limited. To fill this gap, we propose\na collection of finetuned LLMs and multimodal LLMs (MLLMs), known as Geneverse,\nfor three novel tasks in genomic and proteomic research. The models in\nGeneverse are trained and evaluated based on domain-specific datasets, and we\nuse advanced parameter-efficient finetuning techniques to achieve the model\nadaptation for tasks including the generation of descriptions for gene\nfunctions, protein function inference from its structure, and marker gene\nselection from spatial transcriptomic data. We demonstrate that adapted LLMs\nand MLLMs perform well for these tasks and may outperform closed-source\nlarge-scale models based on our evaluations focusing on both truthfulness and\nstructural correctness. All of the training strategies and base models we used\nare freely accessible.", "published": "2024-06-21 14:19:10", "link": "http://arxiv.org/abs/2406.15534v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "SAIL: Self-Improving Efficient Online Alignment of Large Language Models", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a key method for\naligning large language models (LLMs) with human preferences. However, current\noffline alignment approaches like DPO, IPO, and SLiC rely heavily on fixed\npreference datasets, which can lead to sub-optimal performance. On the other\nhand, recent literature has focused on designing online RLHF methods but still\nlacks a unified conceptual formulation and suffers from distribution shift\nissues. To address this, we establish that online LLM alignment is underpinned\nby bilevel optimization. By reducing this formulation to an efficient\nsingle-level first-order method (using the reward-policy equivalence), our\napproach generates new samples and iteratively refines model alignment by\nexploring responses and regulating preference labels. In doing so, we permit\nalignment methods to operate in an online and self-improving manner, as well as\ngeneralize prior online RLHF methods as special cases. Compared to\nstate-of-the-art iterative RLHF methods, our approach significantly improves\nalignment performance on open-sourced datasets with minimal computational\noverhead.", "published": "2024-06-21 18:05:35", "link": "http://arxiv.org/abs/2406.15567v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Contrastive Entity Coreference and Disambiguation for Historical Texts", "abstract": "Massive-scale historical document collections are crucial for social science\nresearch. Despite increasing digitization, these documents typically lack\nunique cross-document identifiers for individuals mentioned within the texts,\nas well as individual identifiers from external knowledgebases like\nWikipedia/Wikidata. Existing entity disambiguation methods often fall short in\naccuracy for historical documents, which are replete with individuals not\nremembered in contemporary knowledgebases. This study makes three key\ncontributions to improve cross-document coreference resolution and\ndisambiguation in historical texts: a massive-scale training dataset replete\nwith hard negatives - that sources over 190 million entity pairs from Wikipedia\ncontexts and disambiguation pages - high-quality evaluation data from\nhand-labeled historical newswire articles, and trained models evaluated on this\nhistorical benchmark. We contrastively train bi-encoder models for\ncoreferencing and disambiguating individuals in historical texts, achieving\naccurate, scalable performance that identifies out-of-knowledgebase\nindividuals. Our approach significantly surpasses other entity disambiguation\nmodels on our historical newswire benchmark. Our models also demonstrate\ncompetitive performance on modern entity disambiguation benchmarks,\nparticularly certain news disambiguation datasets.", "published": "2024-06-21 18:22:14", "link": "http://arxiv.org/abs/2406.15576v1", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "News Deja Vu: Connecting Past and Present with Semantic Search", "abstract": "Social scientists and the general public often analyze contemporary events by\ndrawing parallels with the past, a process complicated by the vast, noisy, and\nunstructured nature of historical texts. For example, hundreds of millions of\npage scans from historical newspapers have been noisily transcribed.\nTraditional sparse methods for searching for relevant material in these vast\ncorpora, e.g., with keywords, can be brittle given complex vocabularies and OCR\nnoise. This study introduces News Deja Vu, a novel semantic search tool that\nleverages transformer large language models and a bi-encoder approach to\nidentify historical news articles that are most similar to modern news queries.\nNews Deja Vu first recognizes and masks entities, in order to focus on broader\nparallels rather than the specific named entities being discussed. Then, a\ncontrastively trained, lightweight bi-encoder retrieves historical articles\nthat are most similar semantically to a modern query, illustrating how\nphenomena that might seem unique to the present have varied historical\nprecedents. Aimed at social scientists, the user-friendly News Deja Vu package\nis designed to be accessible for those who lack extensive familiarity with deep\nlearning. It works with large text datasets, and we show how it can be deployed\nto a massive scale corpus of historical, open-source news articles. While human\nexpertise remains important for drawing deeper insights, News Deja Vu provides\na powerful tool for exploring parallels in how people have perceived past and\npresent.", "published": "2024-06-21 18:50:57", "link": "http://arxiv.org/abs/2406.15593v2", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Shortcomings of LLMs for Low-Resource Translation: Retrieval and\n  Understanding are Both the Problem", "abstract": "This work investigates the in-context learning abilities of pretrained large\nlanguage models (LLMs) when instructed to translate text from a low-resource\nlanguage into a high-resource language as part of an automated machine\ntranslation pipeline. We conduct a set of experiments translating Southern\nQuechua to Spanish and examine the informativity of various types of context\nretrieved from a constrained database of digitized pedagogical materials\n(dictionaries and grammar lessons) and parallel corpora. Using both automatic\nand human evaluation of model output, we conduct ablation studies that\nmanipulate (1) context type (morpheme translations, grammar descriptions, and\ncorpus examples), (2) retrieval methods (automated vs. manual), and (3) model\ntype. Our results suggest that even relatively small LLMs are capable of\nutilizing prompt context for zero-shot low-resource translation when provided a\nminimally sufficient amount of relevant linguistic information. However, the\nvariable effects of context type, retrieval method, model type, and\nlanguage-specific factors highlight the limitations of using even the best LLMs\nas translation systems for the majority of the world's 7,000+ languages and\ntheir speakers.", "published": "2024-06-21 20:02:22", "link": "http://arxiv.org/abs/2406.15625v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PI-Whisper: Designing an Adaptive and Incremental Automatic Speech\n  Recognition System for Edge Devices", "abstract": "Edge-based automatic speech recognition (ASR) technologies are increasingly\nprevalent in the development of intelligent and personalized assistants.\nHowever, resource-constrained ASR models face significant challenges in\nadaptivity, incrementality, and inclusivity when faced with a diverse\npopulation. To tackle those challenges, we propose PI-Whisper, a novel ASR\nsystem that adaptively enhances recognition capabilities by identifying\nspeakers' characteristics in real-time. In this work, we show how the design of\nPI-Whisper allows for incremental adaptation of new characteristics without the\nneed for repetitive retraining, enhances recognition capabilities, and improves\nequity and fairness across diverse speaker groups. PI-Whisper demonstrates\nthese advantages by achieving state-of-the-art accuracy, reducing the word\nerror rate (WER) by up to 13.7% relative to baselines while scaling linearly to\ncomputing resources.", "published": "2024-06-21 21:58:37", "link": "http://arxiv.org/abs/2406.15668v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback", "abstract": "RAG systems face limitations when semantic relevance alone does not guarantee\nimproved generation quality. This issue becomes particularly evident due to the\nsensitivity of large language models (LLMs) to the ordering of few-shot\nprompts, which can affect model performance. To address this challenge,\naligning LLM outputs with human preferences using structured feedback, such as\noptions to copy, regenerate, or dislike, offers a promising method for\nimprovement. This feedback is applied to the entire list of inputs rather than\ngiving specific ratings for individual documents, making it a Listwide Labels\nLearning-to-Rank task.\n  To address this task, we propose Pistis-RAG, a new RAG framework designed\nwith a content-centric approach to better align LLMs with human preferences.\nPistis-RAG effectively utilizes human feedback, enhancing content ranking and\ngeneration quality. To validate our framework, we use public datasets to\nsimulate human feedback, allowing us to evaluate and refine our method\neffectively. Experimental results indicate that Pistis-RAG improves alignment\nwith human preferences relative to the baseline RAG system, showing a 6.06%\nincrease in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy\nmetrics. These results highlight Pistis-RAG's effectiveness in overcoming the\nlimitations associated with traditional RAG approaches.", "published": "2024-06-21 08:52:11", "link": "http://arxiv.org/abs/2407.00072v5", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference", "abstract": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.", "published": "2024-06-21 19:18:16", "link": "http://arxiv.org/abs/2407.00075v5", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of\n  Large Language Models in Lexical Entailment Recognition", "abstract": "Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic.", "published": "2024-06-21 06:30:16", "link": "http://arxiv.org/abs/2406.14894v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "LLM2FEA: Discover Novel Designs with Generative Evolutionary\n  Multitasking", "abstract": "The rapid research and development of generative artificial intelligence has\nenabled the generation of high-quality images, text, and 3D models from text\nprompts. This advancement impels an inquiry into whether these models can be\nleveraged to create digital artifacts for both creative and engineering\napplications. Drawing on innovative designs from other domains may be one\nanswer to this question, much like the historical practice of ``bionics\", where\nhumans have sought inspiration from nature's exemplary designs. This raises the\nintriguing possibility of using generative models to simultaneously tackle\ndesign tasks across multiple domains, facilitating cross-domain learning and\nresulting in a series of innovative design solutions. In this paper, we propose\nLLM2FEA as the first attempt to discover novel designs in generative models by\ntransferring knowledge across multiple domains. By utilizing a multi-factorial\nevolutionary algorithm (MFEA) to drive a large language model, LLM2FEA\nintegrates knowledge from various fields to generate prompts that guide the\ngenerative model in discovering novel and practical objects. Experimental\nresults in the context of 3D aerodynamic design verify the discovery\ncapabilities of the proposed LLM2FEA. The designs generated by LLM2FEA not only\nsatisfy practicality requirements to a certain degree but also feature novel\nand aesthetically pleasing shapes, demonstrating the potential applications of\nLLM2FEA in discovery tasks.", "published": "2024-06-21 07:20:51", "link": "http://arxiv.org/abs/2406.14917v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Autonomous Agents for Collaborative Task under Information Asymmetry", "abstract": "Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great\nprogress in solving complex tasks. It performs communication among agents\nwithin the system to collaboratively solve tasks, under the premise of shared\ninformation. However, when agents' collaborations are leveraged to perform\nmulti-person tasks, a new challenge arises due to information asymmetry, since\neach agent can only access the information of its human user. Previous MAS\nstruggle to complete tasks under this condition. To address this, we propose a\nnew MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems.\nIn iAgents, the human social network is mirrored in the agent network, where\nagents proactively exchange human information necessary for task resolution,\nthereby overcoming information asymmetry. iAgents employs a novel agent\nreasoning mechanism, InfoNav, to navigate agents' communication toward\neffective information exchange. Together with InfoNav, iAgents organizes human\ninformation in a mixed memory to provide agents with accurate and comprehensive\ninformation for exchange. Additionally, we introduce InformativeBench, the\nfirst benchmark tailored for evaluating LLM agents' task-solving ability under\ninformation asymmetry. Experimental results show that iAgents can collaborate\nwithin a social network of 140 individuals and 588 relationships, autonomously\ncommunicate over 30 turns, and retrieve information from nearly 70,000 messages\nto complete tasks within 3 minutes.", "published": "2024-06-21 07:37:19", "link": "http://arxiv.org/abs/2406.14928v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA", "cs.SI"], "primary_category": "cs.AI"}
{"title": "DExter: Learning and Controlling Performance Expression with Diffusion\n  Models", "abstract": "In the pursuit of developing expressive music performance models using\nartificial intelligence, this paper introduces DExter, a new approach\nleveraging diffusion probabilistic models to render Western classical piano\nperformances. In this approach, performance parameters are represented in a\ncontinuous expression space and a diffusion model is trained to predict these\ncontinuous parameters while being conditioned on the musical score.\nFurthermore, DExter also enables the generation of interpretations (expressive\nvariations of a performance) guided by perceptually meaningful features by\nconditioning jointly on score and perceptual feature representations.\nConsequently, we find that our model is useful for learning expressive\nperformance, generating perceptually steered performances, and transferring\nperformance styles. We assess the model through quantitative and qualitative\nanalyses, focusing on specific performance metrics regarding dimensions like\nasynchrony and articulation, as well as through listening tests comparing\ngenerated performances with different human interpretations. Results show that\nDExter is able to capture the time-varying correlation of the expressive\nparameters, and compares well to existing rendering models in subjectively\nevaluated ratings. The perceptual-feature-conditioned generation and\ntransferring capabilities of DExter are verified by a proxy model predicting\nperceptual characteristics of differently steered performances.", "published": "2024-06-21 03:42:23", "link": "http://arxiv.org/abs/2406.14850v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Prompting Whisper for QA-driven Zero-shot End-to-end Spoken Language\n  Understanding", "abstract": "Zero-shot spoken language understanding (SLU) enables systems to comprehend\nuser utterances in new domains without prior exposure to training data. Recent\nstudies often rely on large language models (LLMs), leading to excessive\nfootprints and complexity. This paper proposes the use of Whisper, a standalone\nspeech processing model, for zero-shot end-to-end (E2E) SLU. To handle unseen\nsemantic labels, SLU tasks are integrated into a question-answering (QA)\nframework, which prompts the Whisper decoder for semantics deduction. The\nsystem is efficiently trained with prefix-tuning, optimising a minimal set of\nparameters rather than the entire Whisper model. We show that the proposed\nsystem achieves a 40.7% absolute gain for slot filling (SLU-F1) on SLURP\ncompared to a recently introduced zero-shot benchmark. Furthermore, it performs\ncomparably to a Whisper-GPT-2 modular system under both in-corpus and\ncross-corpus evaluation settings, but with a relative 34.8% reduction in model\nparameters.", "published": "2024-06-21 14:51:32", "link": "http://arxiv.org/abs/2406.15209v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "GLOBE: A High-quality English Corpus with Global Accents for Zero-shot\n  Speaker Adaptive Text-to-Speech", "abstract": "This paper introduces GLOBE, a high-quality English corpus with worldwide\naccents, specifically designed to address the limitations of current zero-shot\nspeaker adaptive Text-to-Speech (TTS) systems that exhibit poor\ngeneralizability in adapting to speakers with accents. Compared to commonly\nused English corpora, such as LibriTTS and VCTK, GLOBE is unique in its\ninclusion of utterances from 23,519 speakers and covers 164 accents worldwide,\nalong with detailed metadata for these speakers. Compared to its original\ncorpus, i.e., Common Voice, GLOBE significantly improves the quality of the\nspeech data through rigorous filtering and enhancement processes, while also\npopulating all missing speaker metadata. The final curated GLOBE corpus\nincludes 535 hours of speech data at a 24 kHz sampling rate. Our benchmark\nresults indicate that the speaker adaptive TTS model trained on the GLOBE\ncorpus can synthesize speech with better speaker similarity and comparable\nnaturalness than that trained on other popular corpora. We will release GLOBE\npublicly after acceptance. The GLOBE dataset is available at\nhttps://globecorpus.github.io/.", "published": "2024-06-21 05:55:45", "link": "http://arxiv.org/abs/2406.14875v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Audio-Visual Information Fusion for Sound Event Localization\n  and Detection In Low-Resource Realistic Scenarios", "abstract": "This study presents an audio-visual information fusion approach to sound\nevent localization and detection (SELD) in low-resource scenarios. We aim at\nutilizing audio and video modality information through cross-modal learning and\nmulti-modal fusion. First, we propose a cross-modal teacher-student learning\n(TSL) framework to transfer information from an audio-only teacher model,\ntrained on a rich collection of audio data with multiple data augmentation\ntechniques, to an audio-visual student model trained with only a limited set of\nmulti-modal data. Next, we propose a two-stage audio-visual fusion strategy,\nconsisting of an early feature fusion and a late video-guided decision fusion\nto exploit synergies between audio and video modalities. Finally, we introduce\nan innovative video pixel swapping (VPS) technique to extend an audio channel\nswapping (ACS) method to an audio-visual joint augmentation. Evaluation results\non the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023\nChallenge data set demonstrate significant improvements in SELD performances.\nFurthermore, our submission to the SELD task of the DCASE 2023 Challenge ranks\nfirst place by effectively integrating the proposed techniques into a model\nensemble.", "published": "2024-06-21 14:06:48", "link": "http://arxiv.org/abs/2406.15160v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Speech Emotion Recognition under Resource Constraints with Data\n  Distillation", "abstract": "Speech emotion recognition (SER) plays a crucial role in human-computer\ninteraction. The emergence of edge devices in the Internet of Things (IoT)\npresents challenges in constructing intricate deep learning models due to\nconstraints in memory and computational resources. Moreover, emotional speech\ndata often contains private information, raising concerns about privacy leakage\nduring the deployment of SER models. To address these challenges, we propose a\ndata distillation framework to facilitate efficient development of SER models\nin IoT applications using a synthesised, smaller, and distilled dataset. Our\nexperiments demonstrate that the distilled dataset can be effectively utilised\nto train SER models with fixed initialisation, achieving performances\ncomparable to those developed using the original full emotional speech dataset.", "published": "2024-06-21 13:10:46", "link": "http://arxiv.org/abs/2406.15119v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "R&B -- Rhythm and Brain: Cross-subject Decoding of Music from Human\n  Brain Activity", "abstract": "Music is a universal phenomenon that profoundly influences human experiences\nacross cultures. This study investigates whether music can be decoded from\nhuman brain activity measured with functional MRI (fMRI) during its perception.\nLeveraging recent advancements in extensive datasets and pre-trained\ncomputational models, we construct mappings between neural data and latent\nrepresentations of musical stimuli. Our approach integrates functional and\nanatomical alignment techniques to facilitate cross-subject decoding,\naddressing the challenges posed by the low temporal resolution and\nsignal-to-noise ratio (SNR) in fMRI data. Starting from the GTZan fMRI dataset,\nwhere five participants listened to 540 musical stimuli from 10 different\ngenres while their brain activity was recorded, we used the CLAP (Contrastive\nLanguage-Audio Pretraining) model to extract latent representations of the\nmusical stimuli and developed voxel-wise encoding models to identify brain\nregions responsive to these stimuli. By applying a threshold to the association\nbetween predicted and actual brain activity, we identified specific regions of\ninterest (ROIs) which can be interpreted as key players in music processing.\nOur decoding pipeline, primarily retrieval-based, employs a linear map to\nproject brain activity to the corresponding CLAP features. This enables us to\npredict and retrieve the musical stimuli most similar to those that originated\nthe fMRI data. Our results demonstrate state-of-the-art identification\naccuracy, with our methods significantly outperforming existing approaches. Our\nfindings suggest that neural-based music retrieval systems could enable\npersonalized recommendations and therapeutic applications. Future work could\nuse higher temporal resolution neuroimaging and generative models to improve\ndecoding accuracy and explore the neural underpinnings of music perception and\nemotion.", "published": "2024-06-21 17:11:45", "link": "http://arxiv.org/abs/2406.15537v1", "categories": ["q-bio.NC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Generating Music with Structure Using Self-Similarity as Attention", "abstract": "Despite the innovations in deep learning and generative AI, creating long\nterm structure as well as the layers of repeated structure common in musical\nworks remains an open challenge in music generation. We propose an attention\nlayer that uses a novel approach applying user-supplied self-similarity\nmatrices to previous time steps, and demonstrate it in our Similarity\nIncentivized Neural Generator (SING) system, a deep learning autonomous music\ngeneration system with two layers. The first is a vanilla Long Short Term\nMemory layer, and the second is the proposed attention layer. During\ngeneration, this attention mechanism imposes a suggested structure from a\ntemplate piece on the generated music. We train SING on the MAESTRO dataset\nusing a novel variable batching method, and compare its performance to the same\nmodel without the attention mechanism. The addition of our proposed attention\nmechanism significantly improves the network's ability to replicate specific\nstructures, and it performs better on an unseen test set than a model without\nthe attention mechanism.", "published": "2024-06-21 20:56:12", "link": "http://arxiv.org/abs/2406.15647v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
