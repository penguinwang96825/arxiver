{"title": "Efficient Online Random Sampling via Randomness Recycling", "abstract": "``Randomness recycling'' is a powerful algorithmic technique for reusing a\nfraction of the random information consumed by a randomized algorithm to reduce\nits entropy requirements. This article presents a family of efficient\nrandomness recycling algorithms for sampling a sequence $X_1, X_2, X_3, \\dots$\nof discrete random variables whose joint distribution follows an arbitrary\nstochastic process. We develop randomness recycling strategies to reduce the\nentropy cost of a variety of prominent sampling algorithms, which include\nuniform sampling, inverse transform sampling, lookup table sampling, alias\nsampling, and discrete distribution generating (DDG) tree sampling. Our method\nachieves an expected amortized entropy cost of $H(X_1,\\dots,X_k)/k +\n\\varepsilon$ input random bits per output sample using $O(\\log(1/\\varepsilon))$\nspace, which is arbitrarily close to the optimal Shannon entropy rate. The\ncombination of space, time, and entropy properties of our method improve upon\nthe Han and Hoshi interval algorithm and Knuth and Yao entropy-optimal\nalgorithm for sampling a discrete random sequence. An empirical evaluation of\nthe algorithm shows that it achieves state-of-the-art runtime performance on\nthe Fisher-Yates shuffle when using a cryptographically secure pseudorandom\nnumber generator. Accompanying the manuscript is a performant random sampling\nlibrary in the C programming language.", "published": "2025-05-24 21:34:08", "link": "http://arxiv.org/abs/2505.18879v1", "categories": ["cs.DS", "cs.DM", "cs.IT", "math.IT", "math.PR", "stat.CO"], "primary_category": "cs.DS"}
{"title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study", "abstract": "Federated Retrieval-Augmented Generation (Federated RAG) combines Federated\nLearning (FL), which enables distributed model training without exposing raw\ndata, with Retrieval-Augmented Generation (RAG), which improves the factual\naccuracy of language models by grounding outputs in external knowledge. As\nlarge language models are increasingly deployed in privacy-sensitive domains\nsuch as healthcare, finance, and personalized assistance, Federated RAG offers\na promising framework for secure, knowledge-intensive natural language\nprocessing (NLP). To the best of our knowledge, this paper presents the first\nsystematic mapping study of Federated RAG, covering literature published\nbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-based\nsoftware engineering, we develop a structured classification of research\nfocuses, contribution types, and application domains. We analyze architectural\npatterns, temporal trends, and key challenges, including privacy-preserving\nretrieval, cross-client heterogeneity, and evaluation limitations. Our findings\nsynthesize a rapidly evolving body of research, identify recurring design\npatterns, and surface open questions, providing a foundation for future work at\nthe intersection of RAG and federated systems.", "published": "2025-05-24 23:45:12", "link": "http://arxiv.org/abs/2505.18906v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning", "abstract": "In search advertising, keyword matching connects user queries with relevant\nads. While token-based matching increases ad coverage, it can reduce relevance\ndue to overly permissive semantic expansion. This work extends keyword reach\nthrough document-side semantic keyword expansion, using a language model to\nbroaden token-level matching without altering queries. We propose a solution\nusing a pre-trained siamese model to generate dense vector representations of\nad keywords and identify semantically related variants through nearest neighbor\nsearch. To maintain precision, we introduce a cluster-based thresholding\nmechanism that adjusts similarity cutoffs based on local semantic density. Each\nexpanded keyword maps to a group of seller-listed items, which may only\npartially align with the original intent. To ensure relevance, we enhance the\ndownstream relevance model by adapting it to the expanded keyword space using\nan incremental learning strategy with a lightweight decision tree ensemble.\nThis system improves both relevance and click-through rate (CTR), offering a\nscalable, low-latency solution adaptable to evolving query behavior and\nadvertising inventory.", "published": "2025-05-24 23:02:19", "link": "http://arxiv.org/abs/2505.18897v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Enhancing LLMs' Reasoning-Intensive Multimedia Search Capabilities through Fine-Tuning and Reinforcement Learning", "abstract": "Existing large language models (LLMs) driven search agents typically rely on\nprompt engineering to decouple the user queries into search plans, limiting\ntheir effectiveness in complex scenarios requiring reasoning. Furthermore, they\nsuffer from excessive token consumption due to Python-based search plan\nrepresentations and inadequate integration of multimedia elements for both\ninput processing and response generation. To address these challenges, we\nintroduce SearchExpert, a training method for LLMs to improve their multimedia\nsearch capabilities in response to complex search queries. Firstly, we\nreformulate the search plan in an efficient natural language representation to\nreduce token consumption. Then, we propose the supervised fine-tuning for\nsearching (SFTS) to fine-tune LLM to adapt to these representations, together\nwith an automated dataset construction pipeline. Secondly, to improve\nreasoning-intensive search capabilities, we propose the reinforcement learning\nfrom search feedback (RLSF) that takes the search results planned by LLM as the\nreward signals. Thirdly, we propose a multimedia understanding and generation\nagent that enables the fine-tuned LLM to process visual input and produce\nvisual output during inference. Finally, we establish an automated benchmark\nconstruction pipeline and a human evaluation framework. Our resulting\nbenchmark, SearchExpertBench-25, comprises 200 multiple-choice questions\nspanning financial and international news scenarios that require reasoning in\nsearching. Experiments demonstrate that SearchExpert outperforms the commercial\nLLM search method (Perplexity Pro) by 36.60% on the existing FinSearchBench-24\nbenchmark and 54.54% on our proposed SearchExpertBench-25. Human evaluations\nfurther confirm the superior readability.", "published": "2025-05-24 19:00:36", "link": "http://arxiv.org/abs/2505.18831v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages", "abstract": "Background knowledge is typically needed for successful comprehension of\ntopical and domain specific reading passages, such as in the STEM domain.\nHowever, there are few automated measures of student knowledge that can be\nreadily deployed and scored in time to make predictions on whether a given\nstudent will likely be able to understand a specific content area text. In this\npaper, we present our effort in developing K-tool, an automated system for\ngenerating topical vocabulary tests that measure students' background knowledge\nrelated to a specific text. The system automatically detects the topic of a\ngiven text and produces topical vocabulary items based on their relationship\nwith the topic. This information is used to automatically generate background\nknowledge forms that contain words that are highly related to the topic and\nwords that share similar features but do not share high associations to the\ntopic. Prior research indicates that performance on such tasks can help\ndetermine whether a student is likely to understand a particular text based on\ntheir knowledge state. The described system is intended for use with middle and\nhigh school student population of native speakers of English. It is designed to\nhandle single reading passages and is not dependent on any corpus or text\ncollection. In this paper, we describe the system architecture and present an\ninitial evaluation of the system outputs.", "published": "2025-05-24 15:57:02", "link": "http://arxiv.org/abs/2505.18762v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis", "abstract": "The Retrieval-Augmented Generation (RAG) framework introduces a retrieval\nmodule to dynamically inject retrieved information into the input context of\nlarge language models (LLMs), and has demonstrated significant success in\nvarious NLP tasks. However, the current study points out that there is a\npreference gap between retrievers and LLMs in the RAG framework, which limit\nthe further improvement of system performance. Some highly relevant passages\nmay interfere with LLM reasoning because they contain complex or contradictory\ninformation; while some indirectly related or even inaccurate content may help\nLLM generate more accurate answers by providing suggestive information or\nlogical clues. To solve this, we propose GainRAG, a novel approach that aligns\nthe retriever's and LLM's preferences by defining a new metric, \"gain\", which\nmeasure how well an input passage contributes to correct outputs. Specifically,\nwe propose a method to estimate these gain signals and train a middleware that\naligns the preferences of the retriever and the LLM using only limited data. In\naddition, we introduce a pseudo-passage strategy to mitigate degradation. The\nexperimental results on 6 datasets verify the effectiveness of GainRAG.", "published": "2025-05-24 14:14:57", "link": "http://arxiv.org/abs/2505.18710v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "MTGR: Industrial-Scale Generative Recommendation Framework in Meituan", "abstract": "Scaling law has been extensively validated in many domains such as natural\nlanguage processing and computer vision. In the recommendation system, recent\nwork has adopted generative recommendations to achieve scalability, but their\ngenerative approaches require abandoning the carefully constructed cross\nfeatures of traditional recommendation models. We found that this approach\nsignificantly degrades model performance, and scaling up cannot compensate for\nit at all. In this paper, we propose MTGR (Meituan Generative Recommendation)\nto address this issue. MTGR is modeling based on the HSTU architecture and can\nretain the original deep learning recommendation model (DLRM) features,\nincluding cross features. Additionally, MTGR achieves training and inference\nacceleration through user-level compression to ensure efficient scaling. We\nalso propose Group-Layer Normalization (GLN) to enhance the performance of\nencoding within different semantic spaces and the dynamic masking strategy to\navoid information leakage. We further optimize the training frameworks,\nenabling support for our models with 10 to 100 times computational complexity\ncompared to the DLRM, without significant cost increases. MTGR achieved 65x\nFLOPs for single-sample forward inference compared to the DLRM model, resulting\nin the largest gain in nearly two years both offline and online. This\nbreakthrough was successfully deployed on Meituan, the world's largest food\ndelivery platform, where it has been handling the main traffic.", "published": "2025-05-24 11:47:28", "link": "http://arxiv.org/abs/2505.18654v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "EvdCLIP: Improving Vision-Language Retrieval with Entity Visual Descriptions from Large Language Models", "abstract": "Vision-language retrieval (VLR) has attracted significant attention in both\nacademia and industry, which involves using text (or images) as queries to\nretrieve corresponding images (or text). However, existing methods often\nneglect the rich visual semantics knowledge of entities, thus leading to\nincorrect retrieval results. To address this problem, we propose the Entity\nVisual Description enhanced CLIP (EvdCLIP), designed to leverage the visual\nknowledge of entities to enrich queries. Specifically, since humans recognize\nentities through visual cues, we employ a large language model (LLM) to\ngenerate Entity Visual Descriptions (EVDs) as alignment cues to complement\ntextual data. These EVDs are then integrated into raw queries to create\nvisually-rich, EVD-enhanced queries. Furthermore, recognizing that EVD-enhanced\nqueries may introduce noise or low-quality expansions, we develop a novel,\ntrainable EVD-aware Rewriter (EaRW) for vision-language retrieval tasks. EaRW\nutilizes EVD knowledge and the generative capabilities of the language model to\neffectively rewrite queries. With our specialized training strategy, EaRW can\ngenerate high-quality and low-noise EVD-enhanced queries. Extensive\nquantitative and qualitative experiments on image-text retrieval benchmarks\nvalidate the superiority of EvdCLIP on vision-language retrieval tasks.", "published": "2025-05-24 08:41:51", "link": "http://arxiv.org/abs/2505.18594v1", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV"}
{"title": "The Silent Saboteur: Imperceptible Adversarial Attacks against Black-Box Retrieval-Augmented Generation Systems", "abstract": "We explore adversarial attacks against retrieval-augmented generation (RAG)\nsystems to identify their vulnerabilities. We focus on generating\nhuman-imperceptible adversarial examples and introduce a novel imperceptible\nretrieve-to-generate attack against RAG. This task aims to find imperceptible\nperturbations that retrieve a target document, originally excluded from the\ninitial top-$k$ candidate set, in order to influence the final answer\ngeneration. To address this task, we propose ReGENT, a reinforcement\nlearning-based framework that tracks interactions between the attacker and the\ntarget RAG and continuously refines attack strategies based on\nrelevance-generation-naturalness rewards. Experiments on newly constructed\nfactual and non-factual question-answering benchmarks demonstrate that ReGENT\nsignificantly outperforms existing attack methods in misleading RAG systems\nwith small imperceptible text perturbations.", "published": "2025-05-24 08:19:25", "link": "http://arxiv.org/abs/2505.18583v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Benchmarking Poisoning Attacks against Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) has proven effective in mitigating\nhallucinations in large language models by incorporating external knowledge\nduring inference. However, this integration introduces new security\nvulnerabilities, particularly to poisoning attacks. Although prior work has\nexplored various poisoning strategies, a thorough assessment of their practical\nthreat to RAG systems remains missing. To address this gap, we propose the\nfirst comprehensive benchmark framework for evaluating poisoning attacks on\nRAG. Our benchmark covers 5 standard question answering (QA) datasets and 10\nexpanded variants, along with 13 poisoning attack methods and 7 defense\nmechanisms, representing a broad spectrum of existing techniques. Using this\nbenchmark, we conduct a comprehensive evaluation of all included attacks and\ndefenses across the full dataset spectrum. Our findings show that while\nexisting attacks perform well on standard QA datasets, their effectiveness\ndrops significantly on the expanded versions. Moreover, our results demonstrate\nthat various advanced RAG architectures, such as sequential, branching,\nconditional, and loop RAG, as well as multi-turn conversational RAG, multimodal\nRAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning\nattacks. Notably, current defense techniques fail to provide robust protection,\nunderscoring the pressing need for more resilient and generalizable defense\nstrategies.", "published": "2025-05-24 06:17:59", "link": "http://arxiv.org/abs/2505.18543v1", "categories": ["cs.CR", "cs.IR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking", "abstract": "Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models.", "published": "2025-05-24 05:15:49", "link": "http://arxiv.org/abs/2505.18512v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A Survey of LLM $\\times$ DATA", "abstract": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.", "published": "2025-05-24 01:57:12", "link": "http://arxiv.org/abs/2505.18458v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Joint Max-Min Power Control and Clustering in Cell-Free Wireless Networks: Design and Analysis", "abstract": "Cell-free wireless networks have attracted significant interest for their\nability to eliminate cell-edge effects and deliver uniformly high service\nquality through macro-diversity. In this paper, we develop an algorithm to\njointly optimize uplink transmit powers and dynamic user-centric access point\n(AP) clusters in a centralized cell-free network. This approach aims to\nefficiently mitigate inter-user interference and achieve higher max-min\nsignal-to-interference-plus-noise ratio (SINR) targets for users. To this end,\nwe re-purpose an iterative power control algorithm based on non-linear\nPerron-Frobenius theory and prove its convergence for the maximum ratio\ncombiner (MRC) receiver under various AP subset selection schemes. We further\nprovide analytical results by framing the joint optimization as a conditional\neigenvalue problem with power and AP association constraints, and leveraging\nPerron-Frobenius theory on a centrally constructed matrix. The numerical\nresults highlight that optimizing each user's serving AP cluster is essential\nto achieving higher max-min SINR targets with the simple MRC receiver.", "published": "2025-05-24 12:43:21", "link": "http://arxiv.org/abs/2505.18676v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Neural Coding Is Not Always Semantic: Towards The Standardized Coding Workflow in Semantic Communications", "abstract": "Semantic communication, leveraging advanced deep learning techniques, emerges\nas a new paradigm that meets the requirements of next-generation wireless\nnetworks. However, current semantic communication systems, which employ neural\ncoding for feature extraction from raw data, have not adequately addressed the\nfundamental question: Is general feature extraction through deep neural\nnetworks sufficient for understanding semantic meaning within raw data in\nsemantic communication? This article is thus motivated to clarify two critical\naspects: semantic understanding and general semantic representation. This\narticle presents a standardized definition on semantic coding, an extensive\nneural coding scheme for general semantic representation that clearly\nrepresents underlying data semantics based on contextual modeling. With these\ngeneral semantic representations obtained, both human- and machine-centric\nend-to-end data transmission can be achieved through only minimal specialized\nmodifications, such as fine-tuning and regularization. This article contributes\nto establishing a commonsense that semantic communication extends far beyond\nmere feature transmission, focusing instead on conveying compact semantic\nrepresentations through context-aware coding schemes.", "published": "2025-05-24 10:52:01", "link": "http://arxiv.org/abs/2505.18637v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Multi-Subarray FD-RIS Enhanced Multi-user Wireless Networks: With Joint Distance-Angle Beamforming", "abstract": "The concept of the frequency diverse reconfigurable intelligent surface\n(FD-RIS) technology has been introduced, which can enable simultaneous\nimplementation of distance-angle beamforming in far-field communication\nscenarios. In order to improve the managing ability on undesired harmonic\nsignals and the diversity of frequency offsets, this paper presents a novel\nmulti-subarray FD-RIS framework. In this framework, the RIS is evenly divided\ninto multiple subarrays, each employing a distinct time-modulation frequency to\nenable the diversity of frequency offsets. Additionally, to suppress the\nundesired harmonic signals, a new time-modulation technique is employed to\nperiodically adjust the phase-shift of each element. Based on the proposed\nmulti-subarray FD-RIS, the signal processing model is first analytically\nderived. To evaluate the effectiveness of the proposed multi-subarray FD-RIS,\nwe integrate it into a multi-user communication scenario and formulate an\noptimization problem that aims to maximize the weighted sum rate of all users.\nThis is achieved by jointly optimizing the active beamforming, time delays, and\nmodulation frequencies. Subsequently, a novel iterative algorithm is proposed\nto effectively solve this problem with low computing complexity. Simulation\nresults demonstrate that the proposed multi-subarray FD-RIS can significantly\nenhance the performance of far-field communication networks by leveraging\nunique distance-angle beamforming. Furthermore, to achieve same performance\ngains, the FD-RIS-assisted system can substantially reduce the required number\nof RIS elements, number of antennas, and power budget, than the conventional\nRIS-assisted schemes. The proposed algorithm also demonstrates a notably\nsuperiority in performance and computational complexity compared with the\nbaseline algorithms such as semi-definite relaxation (SDR) and zero-forcing\n(ZF).", "published": "2025-05-24 10:26:17", "link": "http://arxiv.org/abs/2505.18628v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Coordinated guidance and control for multiple parafoil system landing", "abstract": "Multiple parafoil landing is an enabling technology for massive supply\ndelivery missions. However, it is still an open question to design a\ncollision-free, computation-efficient guidance and control method for unpowered\nparafoils. To address this issue, this paper proposes a coordinated guidance\nand control method for multiple parafoil landing. First, the multiple parafoil\nlanding process is formulated as a trajectory optimization problem. Then, the\nlanding point allocation algorithm is designed to assign the landing point to\neach parafoil. In order to guarantee flight safety, the collision-free\ntrajectory replanning algorithm is designed. On this basis, the nonlinear model\npredictive control algorithm is adapted to leverage the nonlinear dynamics\nmodel for trajectory tracking. Finally, the parafoil kinematic model is\nutilized to reduce the computational burden of trajectory calculation, and\nkinematic model is updated by the moving horizon correction algorithm to\nimprove the trajectory accuracy. Simulation results demonstrate the\neffectiveness and computational efficiency of the proposed coordinated guidance\nand control method for the multiple parafoil landing.", "published": "2025-05-24 13:33:59", "link": "http://arxiv.org/abs/2505.18691v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation", "abstract": "Large Language Models (LLMs) demonstrate strong generalization and reasoning\nabilities, making them well-suited for complex decision-making tasks such as\nmedical consultation (MC). However, existing LLM-based methods often fail to\ncapture the dual nature of MC, which entails two distinct sub-tasks: symptom\ninquiry, a sequential decision-making process, and disease diagnosis, a\nclassification problem. This mismatch often results in ineffective symptom\ninquiry and unreliable disease diagnosis. To address this, we propose\n\\textbf{DDO}, a novel LLM-based framework that performs\n\\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and\nindependently optimizing the the two sub-tasks through a collaborative\nmulti-agent workflow. Experiments on three real-world MC datasets show that DDO\nconsistently outperforms existing LLM-based approaches and achieves competitive\nperformance with state-of-the-art generation-based methods, demonstrating its\neffectiveness in the MC task.", "published": "2025-05-24 10:26:57", "link": "http://arxiv.org/abs/2505.18630v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations", "abstract": "We study offline imitation learning (IL) in cooperative multi-agent settings,\nwhere demonstrations have unlabeled mixed quality - containing both expert and\nsuboptimal trajectories. Our proposed solution is structured in two stages:\ntrajectory labeling and multi-agent imitation learning, designed jointly to\nenable effective learning from heterogeneous, unlabeled data. In the first\nstage, we combine advances in large language models and preference-based\nreinforcement learning to construct a progressive labeling pipeline that\ndistinguishes expert-quality trajectories. In the second stage, we introduce\nMisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn\nrobust policies while addressing the computational complexity of large joint\nstate-action spaces. By extending the popular single-agent DICE framework to\nmulti-agent settings with a new value decomposition and mixing architecture,\nour method yields a convex policy optimization objective and ensures\nconsistency between global and local policies. We evaluate MisoDICE on multiple\nstandard multi-agent RL benchmarks and demonstrate superior performance,\nespecially when expert data is scarce.", "published": "2025-05-24 08:43:42", "link": "http://arxiv.org/abs/2505.18595v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework", "abstract": "Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit\nremarkable problem-solving and task planning capabilities across diverse\ndomains due to their specialized agentic roles and collaborative interactions.\nHowever, this also amplifies the severity of security risks under MAS attacks.\nTo address this, we introduce MASTER, a novel security research framework for\nMAS, focusing on diverse Role configurations and Topological structures across\nvarious scenarios. MASTER offers an automated construction process for\ndifferent MAS setups and an information-flow-based interaction paradigm. To\ntackle MAS security challenges in varied scenarios, we design a\nscenario-adaptive, extensible attack strategy utilizing role and topological\ninformation, which dynamically allocates targeted, domain-specific attack tasks\nfor collaborative agent execution. Our experiments demonstrate that such an\nattack, leveraging role and topological information, exhibits significant\ndestructive potential across most models. Additionally, we propose\ncorresponding defense strategies, substantially enhancing MAS resilience across\ndiverse scenarios. We anticipate that our framework and findings will provide\nvaluable insights for future research into MAS security challenges.", "published": "2025-05-24 07:24:29", "link": "http://arxiv.org/abs/2505.18572v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have been widely adopted for\nmedical report generation. Despite Med-LVLMs producing state-of-the-art\nperformance, they exhibit a bias toward predicting all findings as normal,\nleading to reports that overlook critical abnormalities. Furthermore, these\nmodels often fail to provide comprehensive descriptions of radiologically\nrelevant regions necessary for accurate diagnosis. To address these challenges,\nwe proposeMedical Report Generation Agents (MRGAgents), a novel multi-agent\nframework that fine-tunes specialized agents for different disease categories.\nBy curating subsets of the IU X-ray and MIMIC-CXR datasets to train\ndisease-specific agents, MRGAgents generates reports that more effectively\nbalance normal and abnormal findings while ensuring a comprehensive description\nof clinically relevant regions. Our experiments demonstrate that MRGAgents\noutperformed the state-of-the-art, improving both report comprehensiveness and\ndiagnostic utility.", "published": "2025-05-24 05:49:42", "link": "http://arxiv.org/abs/2505.18530v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks", "abstract": "This paper introduces EdgeAgentX, a novel framework integrating federated\nlearning (FL), multi-agent reinforcement learning (MARL), and adversarial\ndefense mechanisms, tailored for military communication networks. EdgeAgentX\nsignificantly improves autonomous decision-making, reduces latency, enhances\nthroughput, and robustly withstands adversarial disruptions, as evidenced by\ncomprehensive simulations.", "published": "2025-05-24 01:56:32", "link": "http://arxiv.org/abs/2505.18457v1", "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning", "abstract": "Actor-critic methods for decentralized multi-agent reinforcement learning\n(MARL) facilitate collaborative optimal decision making without centralized\ncoordination, thus enabling a wide range of applications in practice. To date,\nhowever, most theoretical convergence studies for existing actor-critic\ndecentralized MARL methods are limited to the guarantee of a stationary\nsolution under the linear function approximation. This leaves a significant gap\nbetween the highly successful use of deep neural actor-critic for decentralized\nMARL in practice and the current theoretical understanding. To bridge this gap,\nin this paper, we make the first attempt to develop a deep neural actor-critic\nmethod for decentralized MARL, where both the actor and critic components are\ninherently non-linear. We show that our proposed method enjoys a global\noptimality guarantee with a finite-time convergence rate of O(1/T), where T is\nthe total iteration times. This marks the first global convergence result for\ndeep neural actor-critic methods in the MARL literature. We also conduct\nextensive numerical experiments, which verify our theoretical results.", "published": "2025-05-24 00:00:43", "link": "http://arxiv.org/abs/2505.18433v1", "categories": ["cs.LG", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Unconditionally Stable Mixed Finite Element Methods for Darcy Flow", "abstract": "Unconditionally stable finite element methods for Darcy flow are derived by\nadding least-squares residual forms of the governing equations to the classical\nmixed formulations. The proposed methods are free of mesh dependent\nstabilization parameters and allow the use of the classical continuous\nLagrangian finite element spaces of any order for the velocity and the\npotential. Stability, convergence and error estimates are derived and numerical\nexperiments are presented to demonstrate the flexibility of the proposed finite\nelement formulations and to confirm the predicted rates of convergence.", "published": "2025-05-24 19:28:13", "link": "http://arxiv.org/abs/2505.18838v1", "categories": ["math.NA", "cs.NA", "65N12, 65N22, 65N30, 35A35"], "primary_category": "math.NA"}
{"title": "Discrete gradient methods for port-Hamiltonian differential-algebraic equations", "abstract": "Discrete gradient methods are a powerful tool for the time discretization of\ndynamical systems, since they are structure-preserving regardless of the form\nof the total energy. In this work, we discuss the application of discrete\ngradient methods to the system class of nonlinear port-Hamiltonian\ndifferential-algebraic equations - as they emerge from the port- and\nenergy-based modeling of physical systems in various domains. We introduce a\nnovel numerical scheme tailored for semi-explicit differential-algebraic\nequations and further address more general settings using the concepts of\ndiscrete gradient pairs and Dirac-dissipative structures. Additionally, the\nbehavior under system transformations is investigated and we demonstrate that\nunder suitable assumptions port-Hamiltonian differential-algebraic equations\nadmit a representation which consists of a parametrized port-Hamiltonian\nsemi-explicit system and an unstructured equation. Finally, we present the\napplication to multibody system dynamics and discuss numerical results to\ndemonstrate the capabilities of our approach.", "published": "2025-05-24 17:50:48", "link": "http://arxiv.org/abs/2505.18810v1", "categories": ["math.NA", "cs.CE", "cs.NA", "cs.RO", "cs.SY", "eess.SY", "math.DS", "34A09, 65L80, 65P10, 70E55, 93C10"], "primary_category": "math.NA"}
{"title": "Automatic Verification of Floating-Point Accumulation Networks", "abstract": "Floating-point accumulation networks (FPANs) are key building blocks used in\nmany floating-point algorithms, including compensated summation and\ndouble-double arithmetic. FPANs are notoriously difficult to analyze, and\nalgorithms using FPANs are often published without rigorous correctness proofs.\nIn fact, on at least one occasion, a published error bound for a widely used\nFPAN was later found to be incorrect. In this paper, we present an automatic\nprocedure that produces computer-verified proofs of several FPAN correctness\nproperties, including error bounds that are tight to the nearest bit. Our\napproach is underpinned by a novel floating-point abstraction that models the\nsign, exponent, and number of leading and trailing zeros and ones in the\nmantissa of each number flowing through an FPAN. We also present a new FPAN for\ndouble-double addition that is faster and more accurate than the previous best\nknown algorithm.", "published": "2025-05-24 17:07:45", "link": "http://arxiv.org/abs/2505.18791v1", "categories": ["math.NA", "cs.LO", "cs.NA"], "primary_category": "math.NA"}
{"title": "A generalized Riemann problem solver for a hyperbolic model of two-layer thin film flow", "abstract": "In this paper, a second-order generalized Riemann problem (GRP) solver is\ndeveloped for a two-layer thin film model. Extending the first-order Godunov\napproach, the solver is used to construct a temporal-spatial coupled\nsecond-order GRP-based finite-volume method. Numerical experiments including\ncomparisons to MUSCL finite-volume schemes with Runge-Kutta time stepping\nconfirm the accuracy, efficiency and robustness of the higher-order ansatz.\n  The construction of GRP methods requires to compute temporal derivatives of\nintermediate states in the entropy solution of the generalized Riemann problem.\nThese derivatives are obtained from the Rankine-Hugoniot conditions as well as\na characteristic decomposition using Riemann invariants. Notably, the latter\ncan be computed explicitly for the two-layer thin film model, which renders\nthis system to be very suitable for the GRP approach. Moreover, it becomes\npossible to determine the derivatives in an explicit, computationally cheap\nway.", "published": "2025-05-24 15:15:07", "link": "http://arxiv.org/abs/2505.18737v1", "categories": ["math.NA", "cs.NA", "math.AP", "65M06, 35L60, 35L65, 76M12,"], "primary_category": "math.NA"}
{"title": "AMG with Filtering: An Efficient Preconditioner for Interior Point Methods in Large-Scale Contact Mechanics Optimization", "abstract": "Large-scale contact mechanics simulations are crucial in many engineering\nfields such as structural design and manufacturing. In the frictionless case,\ncontact can be modeled by minimizing an energy functional; however, these\nproblems are often nonlinear, non-convex, and increasingly difficult to solve\nas mesh resolution increases. In this work, we employ a Newton-based\ninterior-point (IP) filter line-search method; an effective approach for\nlarge-scale constrained optimization. While this method converges rapidly, each\niteration requires solving a large saddle-point linear system that becomes\nill-conditioned as the optimization process converges, largely due to IP\ntreatment of the contact constraints. Such ill-conditioning can hinder solver\nscalability and increase iteration counts with mesh refinement. To address\nthis, we introduce a novel preconditioner, AMG with Filtering (AMGF), tailored\nto the Schur complement of the saddle-point system. Building on the classical\nalgebraic multigrid (AMG) solver, commonly used for elasticity, we augment it\nwith a specialized subspace correction that filters near null space components\nintroduced by contact interface constraints. Through theoretical analysis and\nnumerical experiments on a range of linear and nonlinear contact problems, we\ndemonstrate that the proposed solver achieves mesh independent convergence and\nmaintains robustness against the ill-conditioning that notoriously plagues IP\nmethods. These results indicate that AMGF makes contact mechanics simulations\nmore tractable and broadens the applicability of Newton-based IP methods in\nchallenging engineering scenarios. More broadly, AMGF is well suited for\nproblems, optimization or otherwise, where solver performance is limited by a\nproblematic low-dimensional subspace. This makes the method widely applicable\nbeyond contact mechanics and constrained optimization.", "published": "2025-05-24 07:42:59", "link": "http://arxiv.org/abs/2505.18576v1", "categories": ["math.NA", "cs.NA", "math.OC", "65F08, 65N55, 65N30, 74S05, 74M15, 90C51"], "primary_category": "math.NA"}
{"title": "Local Taylor-based polynomial quasi-Trefftz spaces for scalar linear equations", "abstract": "Trefftz-type of Galerkin methods for numerical PDEs use discrete spaces of\nproblem-dependent functions. While Trefftz methods leverage discrete spaces of\nlocal exact solutions to the governing PDE, Taylor-based quasi-Trefftz methods\nleverage discrete spaces of local approximate solutions to the governing PDE.\nThis notion of approximate solution, understood in the sense of a small Taylor\nremainder, is defined for differential operators with smooth variable\ncoefficients. In both cases, it is possible to use discrete spaces much smaller\nthan standard polynomial space to achieve the same orders of approximation\nproperties.\n  The present work is the first systematic study of local Taylor-based\npolynomial quasi-Trefftz spaces characterized as the kernel of the\nquasi-Trefftz operator, defined as the composition of Taylor truncation with\nthe differential operator. The proposed linear algebra framework reveals the\ngeneral structure of this linear operator and applies to any non-trivial linear\nscalar differential operator with smooth coefficients. It results in a fully\nexplicit procedure to construct a local quasi-Trefftz basis valid in all\ndimension and for operators of any order, guaranteeing a minimal computational\ncost for the construction of these equation-dependent bases.\n  The local quasi-Trefftz space is formally defined as the kernel of a linear\noperator between spaces of polynomials. The systematic approach relies on a\ndetailed study of the structure of this operator, strongly leveraging the\ngraded structure of polynomial spaces.", "published": "2025-05-24 02:55:39", "link": "http://arxiv.org/abs/2505.18480v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Bulls vs Bears: a Trinomial Model of a Financial Asset", "abstract": "We present a variation of the well-known binomial model of asset prices. This\nvariation incorporates a bound to short-selling, inspired by a model from\nGunduz Caginalp[2]. We formalize this model and prove a formula for all the\nmoments of the logarithmic returns. We also derive a formula for the case with\ninfinitely many investors. As an application of the model, we show how to\ncompute parameters in order to approximate given moments, enabling the modeling\nof skewness and excess kurtosis. Finally, we generalize the model and give the\ncorresponding formula for the moments of the logarithmic returns, and the\nalgorithm for fitting given moments.", "published": "2025-05-24 14:47:16", "link": "http://arxiv.org/abs/2505.18723v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Particle Systems with Local Interactions via Hitting Times and Cascades on Graphs", "abstract": "We study particle systems interacting via hitting times on sparsely connected\ngraphs, following the framework of Lacker, Ramanan and Wu (2023). We provide\ngeneral robustness conditions that guarantee the well-posedness of physical\nsolutions to the dynamics, and demonstrate their connections to the dynamic\npercolation theory. We then study the limiting behavior of the particle\nsystems, establishing the continuous dependence of the joint law of the\nphysical solution on the underlying graph structure with respect to local\nconvergence and showing the convergence of the global empirical measure, which\nextends the general results by Lacker et al. to systems with singular\ninteraction. The model proposed provides a general framework for analyzing\nsystemic risks in large sparsely connected financial networks with a focus on\nlocal interactions, featuring instantaneous default cascades.", "published": "2025-05-24 01:11:09", "link": "http://arxiv.org/abs/2505.18448v1", "categories": ["math.PR", "q-fin.MF", "q-fin.RM"], "primary_category": "math.PR"}
{"title": "Marginal Fairness: Fair Decision-Making under Risk Measures", "abstract": "This paper introduces marginal fairness, a new individual fairness notion for\nequitable decision-making in the presence of protected attributes such as\ngender, race, and religion. This criterion ensures that decisions based on\ngeneralized distortion risk measures are insensitive to distributional\nperturbations in protected attributes, regardless of whether these attributes\nare continuous, discrete, categorical, univariate, or multivariate. To\noperationalize this notion and reflect real-world regulatory environments (such\nas the EU gender-neutral pricing regulation), we model business decision-making\nin highly regulated industries (such as insurance and finance) as a two-step\nprocess: (i) a predictive modeling stage, in which a prediction function for\nthe target variable (e.g., insurance losses) is estimated based on both\nprotected and non-protected covariates; and (ii) a decision-making stage, in\nwhich a generalized distortion risk measure is applied to the target variable,\nconditional only on non-protected covariates, to determine the decision. In\nthis second step, we modify the risk measure such that the decision becomes\ninsensitive to the protected attribute, thus enforcing fairness to ensure\nequitable outcomes under risk-sensitive, regulatory constraints. Furthermore,\nby utilizing the concept of cascade sensitivity, we extend the marginal\nfairness framework to capture how dependencies between covariates propagate the\ninfluence of protected attributes through the modeling pipeline. A numerical\nstudy and an empirical implementation using an auto insurance dataset\ndemonstrate how the framework can be applied in practice.", "published": "2025-05-24 22:44:35", "link": "http://arxiv.org/abs/2505.18895v1", "categories": ["stat.ML", "cs.CC", "cs.CY", "cs.LG", "q-fin.RM"], "primary_category": "stat.ML"}
{"title": "Non-Stationary Lipschitz Bandits", "abstract": "We study the problem of non-stationary Lipschitz bandits, where the number of\nactions is infinite and the reward function, satisfying a Lipschitz assumption,\ncan change arbitrarily over time. We design an algorithm that adaptively tracks\nthe recently introduced notion of significant shifts, defined by large\ndeviations of the cumulative reward function. To detect such reward changes,\nour algorithm leverages a hierarchical discretization of the action space.\nWithout requiring any prior knowledge of the non-stationarity, our algorithm\nachieves a minimax-optimal dynamic regret bound of\n$\\mathcal{\\widetilde{O}}(\\tilde{L}^{1/3}T^{2/3})$, where $\\tilde{L}$ is the\nnumber of significant shifts and $T$ the horizon. This result provides the\nfirst optimal guarantee in this setting.", "published": "2025-05-24 21:23:19", "link": "http://arxiv.org/abs/2505.18871v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Governing Equation Discovery from Data Based on Differential Invariants", "abstract": "The explicit governing equation is one of the simplest and most intuitive\nforms for characterizing physical laws. However, directly discovering partial\ndifferential equations (PDEs) from data poses significant challenges, primarily\nin determining relevant terms from a vast search space. Symmetry, as a crucial\nprior knowledge in scientific fields, has been widely applied in tasks such as\ndesigning equivariant networks and guiding neural PDE solvers. In this paper,\nwe propose a pipeline for governing equation discovery based on differential\ninvariants, which can losslessly reduce the search space of existing equation\ndiscovery methods while strictly adhering to symmetry. Specifically, we compute\nthe set of differential invariants corresponding to the infinitesimal\ngenerators of the symmetry group and select them as the relevant terms for\nequation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as\nan example, we demonstrate that its success rate and accuracy in PDE discovery\nsurpass those of other symmetry-informed governing equation discovery methods\nacross a series of PDEs.", "published": "2025-05-24 17:19:02", "link": "http://arxiv.org/abs/2505.18798v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Supervised and Unsupervised protocols for hetero-associative neural networks", "abstract": "This paper introduces a learning framework for Three-Directional Associative\nMemory (TAM) models, extending the classical Hebbian paradigm to both\nsupervised and unsupervised protocols within an hetero-associative setting.\nThese neural networks consist of three interconnected layers of binary neurons\ninteracting via generalized Hebbian synaptic couplings that allow learning,\nstorage and retrieval of structured triplets of patterns. By relying upon\nglassy statistical mechanical techniques (mainly replica theory and Guerra\ninterpolation), we analyze the emergent computational properties of these\nnetworks, at work with random (Rademacher) datasets and at the\nreplica-symmetric level of description: we obtain a set of self-consistency\nequations for the order parameters that quantify the critical dataset sizes\n(i.e. their thresholds for learning) and describe the retrieval performance of\nthese networks, highlighting the differences between supervised and\nunsupervised protocols. Numerical simulations validate our theoretical findings\nand demonstrate the robustness of the captured picture about TAMs also at work\nwith structured datasets. In particular, this study provides insights into the\ncooperative interplay of layers, beyond that of the neurons within the layers,\nwith potential implications for optimal design of artificial neural network\narchitectures.", "published": "2025-05-24 17:15:55", "link": "http://arxiv.org/abs/2505.18796v1", "categories": ["cond-mat.dis-nn", "stat.ML"], "primary_category": "cond-mat.dis-nn"}
{"title": "Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization", "abstract": "We address the optimization problem of simultaneously minimizing multiple\nobjective functionals over a family of probability distributions. This type of\nMulti-Objective Distributional Optimization commonly arises in machine learning\nand statistics, with applications in areas such as multiple target sampling,\nmulti-task learning, and multi-objective generative modeling. To solve this\nproblem, we propose an iterative particle-based algorithm, which we call\nMuliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of\nintermediate empirical distributions, each being represented by a set of\nparticles, which gradually minimize the multiple objective functionals\nsimultaneously. Specifically, MWGraD consists of two key steps at each\niteration. First, it estimates the Wasserstein gradient for each objective\nfunctional based on the current particles. Then, it aggregates these gradients\ninto a single Wasserstein gradient using dynamically adjusted weights and\nupdates the particles accordingly. In addition, we provide theoretical analysis\nand present experimental results on both synthetic and real-world datasets,\ndemonstrating the effectiveness of MWGraD.", "published": "2025-05-24 16:08:13", "link": "http://arxiv.org/abs/2505.18765v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor", "abstract": "Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a\ncost-effective and stable alternative to conventional architectures, utilizing\nonly an absorber layer and an electron transport layer (ETL). This study\npresents a machine learning (ML)-driven framework to optimize the efficiency\nand stability of HTL-free PSCs by integrating experimental validation with\nnumerical simulations. Excellent agreement is achieved between a fabricated\ndevice and its simulated counterpart at a molar fraction \\( x = 68.7\\% \\) in\n\\(\\mathrm{MAPb}_{1-x}\\mathrm{Sb}_{2x/3}\\mathrm{I}_3\\), where MA is\nmethylammonium. A dataset of 1650 samples is generated by varying molar\nfraction, absorber defect density, thickness, and ETL doping, with\ncorresponding efficiency and 50-hour degradation as targets. A fourth-degree\npolynomial regressor (PR-4) shows the best performance, achieving RMSEs of\n0.0179 and 0.0117, and \\( R^2 \\) scores of 1 and 0.999 for efficiency and\ndegradation, respectively. The derived model generalizes beyond the training\nrange and is used in an L-BFGS-B optimization algorithm with a weighted\nobjective function to maximize efficiency and minimize degradation. This\nimproves device efficiency from 13.7\\% to 16.84\\% and reduces degradation from\n6.61\\% to 2.39\\% over 1000 hours. Finally, the dataset is labeled into superior\nand inferior classes, and a multilayer perceptron (MLP) classifier achieves\n100\\% accuracy, successfully identifying optimal configurations.", "published": "2025-05-24 13:37:48", "link": "http://arxiv.org/abs/2505.18693v1", "categories": ["cs.LG", "eess.SP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?", "abstract": "Representation intervention aims to locate and modify the representations\nthat encode the underlying concepts in Large Language Models (LLMs) to elicit\nthe aligned and expected behaviors. Despite the empirical success, it has never\nbeen examined whether one could locate the faithful concepts for intervention.\nIn this work, we explore the question in safety alignment. If the interventions\nare faithful, the intervened LLMs should erase the harmful concepts and be\nrobust to both in-distribution adversarial prompts and the out-of-distribution\n(OOD) jailbreaks. While it is feasible to erase harmful concepts without\ndegrading the benign functionalities of LLMs in linear settings, we show that\nit is infeasible in the general non-linear setting. To tackle the issue, we\npropose Concept Concentration (COCA). Instead of identifying the faithful\nlocations to intervene, COCA refractors the training data with an explicit\nreasoning process, which firstly identifies the potential unsafe concepts and\nthen decides the responses. Essentially, COCA simplifies the decision boundary\nbetween harmful and benign representations, enabling more effective linear\nerasure. Extensive experiments with multiple representation intervention\nmethods and model architectures demonstrate that COCA significantly reduces\nboth in-distribution and OOD jailbreak success rates, and meanwhile maintaining\nstrong performance on regular tasks such as math and code generation.", "published": "2025-05-24 12:23:52", "link": "http://arxiv.org/abs/2505.18672v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees", "abstract": "Selecting artificial intelligence (AI) models, such as large language models\n(LLMs), from multiple candidates requires accurate performance estimation. This\nis ideally achieved through empirical evaluations involving abundant real-world\ndata. However, such evaluations are costly and impractical at scale. To address\nthis challenge, autoevaluation methods leverage synthetic data produced by\nautomated evaluators, such as LLMs-as-judges, reducing variance but potentially\nintroducing bias. Recent approaches have employed semi-supervised\nprediction-powered inference (\\texttt{PPI}) to correct for the bias of\nautoevaluators. However, the use of autoevaluators may lead in practice to a\ndegradation in sample efficiency compared to conventional methods using only\nreal-world data. In this paper, we propose \\texttt{R-AutoEval+}, a novel\nframework that provides finite-sample reliability guarantees on the model\nevaluation, while also ensuring an enhanced (or at least no worse) sample\nefficiency compared to conventional methods. The key innovation of\n\\texttt{R-AutoEval+} is an adaptive construction of the model evaluation\nvariable, which dynamically tunes its reliance on synthetic data, reverting to\nconventional methods when the autoevaluator is insufficiently accurate.\nExperiments on the use of LLMs-as-judges for the optimization of quantization\nsettings for the weights of an LLM, and for prompt design in LLMs confirm the\nreliability and efficiency of \\texttt{R-AutoEval+}.", "published": "2025-05-24 11:53:29", "link": "http://arxiv.org/abs/2505.18659v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation", "abstract": "In recent machine learning systems, confidence scores are being utilized more\nand more to manage selective prediction, whereby a model can abstain from\nmaking a prediction when it is unconfident. Yet, conventional metrics like\naccuracy, expected calibration error (ECE), and area under the risk-coverage\ncurve (AURC) do not capture the actual reliability of predictions. These\nmetrics either disregard confidence entirely, dilute valuable localized\ninformation through averaging, or neglect to suitably penalize overconfident\nmisclassifications, which can be particularly detrimental in real-world\nsystems. We introduce two new metrics Confidence-Weighted Selective Accuracy\n(CWSA) and its normalized variant CWSA+ that offer a principled and\ninterpretable way to evaluate predictive models under confidence thresholds.\nUnlike existing methods, our metrics explicitly reward confident accuracy and\npenalize overconfident mistakes. They are threshold-local, decomposable, and\nusable in both evaluation and deployment settings where trust and risk must be\nquantified. Through exhaustive experiments on both real-world data sets (MNIST,\nCIFAR-10) and artificial model variants (calibrated, overconfident,\nunderconfident, random, perfect), we show that CWSA and CWSA+ both effectively\ndetect nuanced failure modes and outperform classical metrics in\ntrust-sensitive tests. Our results confirm that CWSA is a sound basis for\ndeveloping and assessing selective prediction systems for safety-critical\ndomains.", "published": "2025-05-24 10:07:48", "link": "http://arxiv.org/abs/2505.18622v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks", "abstract": "Meta-reinforcement learning trains a single reinforcement learning agent on a\ndistribution of tasks to quickly generalize to new tasks outside of the\ntraining set at test time. From a Bayesian perspective, one can interpret this\nas performing amortized variational inference on the posterior distribution\nover training tasks. Among the various meta-reinforcement learning approaches,\na common method is to represent this distribution with a point-estimate using a\nrecurrent neural network. We show how one can augment this point estimate to\ngive full distributions through the Laplace approximation, either at the start\nof, during, or after learning, without modifying the base model architecture.\nWith our approximation, we are able to estimate distribution statistics (e.g.,\nthe entropy) of non-Bayesian agents and observe that point-estimate based\nmethods produce overconfident estimators while not satisfying consistency.\nFurthermore, when comparing our approach to full-distribution based learning of\nthe task posterior, our method performs on par with variational baselines while\nhaving much fewer parameters.", "published": "2025-05-24 08:38:10", "link": "http://arxiv.org/abs/2505.18591v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning", "abstract": "Our examination of existing deep generative models (DGMs), including VAEs and\nGANs, reveals two problems. First, their capability in handling discrete\nobservations and latent codes is unsatisfactory, though there are interesting\nefforts. Second, both VAEs and GANs optimize some criteria that are indirectly\nrelated to the data likelihood. To address these problems, we formally present\nJoint-stochastic-approximation (JSA) autoencoders - a new family of algorithms\nfor building deep directed generative models, with application to\nsemi-supervised learning. The JSA learning algorithm directly maximizes the\ndata log-likelihood and simultaneously minimizes the inclusive KL divergence\nthe between the posteriori and the inference model. We provide theoretical\nresults and conduct a series of experiments to show its superiority such as\nbeing robust to structure mismatch between encoder and decoder, consistent\nhandling of both discrete and continuous variables. Particularly we empirically\nshow that JSA autoencoders with discrete latent space achieve comparable\nperformance to other state-of-the-art DGMs with continuous latent space in\nsemi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the\nbest of our knowledge, this is the first demonstration that discrete latent\nvariable models are successfully applied in the challenging semi-supervised\ntasks.", "published": "2025-05-24 06:52:23", "link": "http://arxiv.org/abs/2505.18558v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD", "abstract": "We study the convergence properties and escape dynamics of Stochastic\nGradient Descent (SGD) in one-dimensional landscapes, separately considering\ninfinite- and finite-variance noise. Our main focus is to identify the time\nscales on which SGD reliably moves from an initial point to the local minimum\nin the same ''basin''. Under suitable conditions on the noise distribution, we\nprove that SGD converges to the basin's minimum unless the initial point lies\ntoo close to a local maximum. In that near-maximum scenario, we show that SGD\ncan linger for a long time in its neighborhood. For initial points near a\n''sharp'' maximum, we show that SGD does not remain stuck there, and we provide\nresults to estimate the probability that it will reach each of the two\nneighboring minima. Overall, our findings present a nuanced view of SGD's\ntransitions between local maxima and minima, influenced by both noise\ncharacteristics and the underlying function geometry.", "published": "2025-05-24 06:00:45", "link": "http://arxiv.org/abs/2505.18535v1", "categories": ["cs.LG", "math.PR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition", "abstract": "Kernels are key to encoding prior beliefs and data structures in Gaussian\nprocess (GP) models. The design of expressive and scalable kernels has garnered\nsignificant research attention. Deep kernel learning enhances kernel\nflexibility by feeding inputs through a neural network before applying a\nstandard parametric form. However, this approach remains limited by the choice\nof base kernels, inherits high inference costs, and often demands sparse\napproximations. Drawing on Mercer's theorem, we introduce a fully data-driven,\nscalable deep kernel representation where a neural network directly represents\na low-rank kernel through a small set of basis functions. This construction\nenables highly efficient exact GP inference in linear time and memory without\ninvoking inducing points. It also supports scalable mini-batch training based\non a principled variational inference framework. We further propose a simple\nvariance correction procedure to guard against overconfidence in uncertainty\nestimates. Experiments on synthetic and real-world data demonstrate the\nadvantages of our deep kernel GP in terms of predictive accuracy, uncertainty\nquantification, and computational efficiency.", "published": "2025-05-24 05:42:11", "link": "http://arxiv.org/abs/2505.18526v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning", "abstract": "Although Large Language Models (LLMs) have demonstrated remarkable progress,\ntheir proficiency in graph-related tasks remains notably limited, hindering the\ndevelopment of truly general-purpose models. Previous attempts, including\npretraining graph foundation models or employing supervised fine-tuning, often\nface challenges such as the scarcity of large-scale, universally represented\ngraph data. We introduce G1, a simple yet effective approach demonstrating that\nReinforcement Learning (RL) on synthetic graph-theoretic tasks can\nsignificantly scale LLMs' graph reasoning abilities. To enable RL training, we\ncurate Erd\\~os, the largest graph reasoning dataset to date comprising 50\ndiverse graph-theoretic tasks of varying difficulty levels, 100k training data\nand 5k test data, all drived from real-world graphs. With RL on Erd\\~os, G1\nobtains substantial improvements in graph reasoning, where our finetuned 3B\nmodel even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also\nshow strong zero-shot generalization to unseen tasks, domains, and graph\nencoding schemes, including other graph-theoretic benchmarks as well as\nreal-world node classification and link prediction tasks, without compromising\ngeneral reasoning abilities. Our findings offer an efficient, scalable path for\nbuilding strong graph reasoners by finetuning LLMs with RL on graph-theoretic\ntasks, which combines the strengths of pretrained LLM capabilities with\nabundant, automatically generated synthetic data, suggesting that LLMs possess\ngraph understanding abilities that RL can elicit successfully.", "published": "2025-05-24 04:33:41", "link": "http://arxiv.org/abs/2505.18499v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Statistical Inference under Performativity", "abstract": "Performativity of predictions refers to the phenomena that\nprediction-informed decisions may influence the target they aim to predict,\nwhich is widely observed in policy-making in social sciences and economics. In\nthis paper, we initiate the study of statistical inference under\nperformativity. Our contribution is two-fold. First, we build a central limit\ntheorem for estimation and inference under performativity, which enables\ninferential purposes in policy-making such as constructing confidence intervals\nor testing hypotheses. Second, we further leverage the derived central limit\ntheorem to investigate prediction-powered inference (PPI) under performativity,\nwhich is based on a small labeled dataset and a much larger dataset of\nmachine-learning predictions. This enables us to obtain more precise estimation\nand improved confidence regions for the model parameter (i.e., policy) of\ninterest in performative prediction. We demonstrate the power of our framework\nby numerical experiments. To the best of our knowledge, this paper is the first\none to establish statistical inference under performativity, which brings up\nnew challenges and inference settings that we believe will add significant\nvalues to policy-making, statistics, and machine learning.", "published": "2025-05-24 03:59:49", "link": "http://arxiv.org/abs/2505.18493v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts", "abstract": "The softmax-contaminated mixture of experts (MoE) model is deployed when a\nlarge-scale pre-trained model, which plays the role of a fixed expert, is\nfine-tuned for learning downstream tasks by including a new contamination part,\nor prompt, functioning as a new, trainable expert. Despite its popularity and\nrelevance, the theoretical properties of the softmax-contaminated MoE have\nremained unexplored in the literature. In the paper, we study the convergence\nrates of the maximum likelihood estimator of gating and prompt parameters in\norder to gain insights into the statistical properties and potential challenges\nof fine-tuning with a new prompt. We find that the estimability of these\nparameters is compromised when the prompt acquires overlapping knowledge with\nthe pre-trained model, in the sense that we make precise by formulating a novel\nanalytic notion of distinguishability. Under distinguishability of the\npre-trained and prompt models, we derive minimax optimal estimation rates for\nall the gating and prompt parameters. By contrast, when the distinguishability\ncondition is violated, these estimation rates become significantly slower due\nto their dependence on the prompt convergence rate to the pre-trained model.\nFinally, we empirically corroborate our theoretical findings through several\nnumerical experiments.", "published": "2025-05-24 01:30:46", "link": "http://arxiv.org/abs/2505.18455v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Context-Driven Dynamic Pruning for Large Speech Foundation Models", "abstract": "Speech foundation models achieve strong generalization across languages and\nacoustic conditions, but require significant computational resources for\ninference. In the context of speech foundation models, pruning techniques have\nbeen studied that dynamically optimize model structures based on the target\naudio leveraging external context. In this work, we extend this line of\nresearch and propose context-driven dynamic pruning, a technique that optimizes\nthe model computation depending on the context between different input frames\nand additional context during inference. We employ the Open Whisper-style\nSpeech Model (OWSM) and incorporate speaker embeddings, acoustic event\nembeddings, and language information as additional context. By incorporating\nthe speaker embedding, our method achieves a reduction of 56.7 GFLOPs while\nimproving BLEU scores by a relative 25.7% compared to the fully fine-tuned OWSM\nmodel.", "published": "2025-05-24 20:40:51", "link": "http://arxiv.org/abs/2505.18860v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Audio Geolocation: A Natural Sounds Benchmark", "abstract": "Can we determine someone's geographic location purely from the sounds they\nhear? Are acoustic signals enough to localize within a country, state, or even\ncity? We tackle the challenge of global-scale audio geolocation, formalize the\nproblem, and conduct an in-depth analysis with wildlife audio from the\niNatSounds dataset. Adopting a vision-inspired approach, we convert audio\nrecordings to spectrograms and benchmark existing image geolocation techniques.\nWe hypothesize that species vocalizations offer strong geolocation cues due to\ntheir defined geographic ranges and propose an approach that integrates species\nrange prediction with retrieval-based geolocation. We further evaluate whether\ngeolocation improves when analyzing species-rich recordings or when aggregating\nacross spatiotemporal neighborhoods. Finally, we introduce case studies from\nmovies to explore multimodal geolocation using both audio and visual content.\nOur work highlights the advantages of integrating audio and visual cues, and\nsets the stage for future research in audio geolocation.", "published": "2025-05-24 14:49:49", "link": "http://arxiv.org/abs/2505.18726v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers", "abstract": "Speech-based Parkinson's disease (PD) detection has gained attention for its\nautomated, cost-effective, and non-intrusive nature. As research studies\nusually rely on data from diagnostic-oriented speech tasks, this work explores\nthe feasibility of diagnosing PD on the basis of speech data not originally\nintended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our\nfindings indicate that TT can be as useful as diagnostic-oriented PD datasets\nlike PC-GITA. We also investigate which specific dataset characteristics impact\nPD classification performance. The results show that concatenating audio\nrecordings and balancing participants' gender and status distributions can be\nbeneficial. Cross-dataset evaluation reveals that models trained on PC-GITA\ngeneralize poorly to TT, whereas models trained on TT perform better on\nPC-GITA. Furthermore, we provide insights into the high variability across\nfolds, which is mainly due to large differences in individual speaker\nperformance.", "published": "2025-05-24 14:45:55", "link": "http://arxiv.org/abs/2505.18722v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving", "abstract": "Large language models (LLMs) have shown remarkable generalization across\ntasks, leading to increased interest in integrating speech with LLMs. These\nspeech LLMs (SLLMs) typically use supervised fine-tuning to align speech with\ntext-based LLMs. However, the lack of annotated speech data across a wide range\nof tasks hinders alignment efficiency, resulting in poor generalization. To\naddress these issues, we propose a novel multi-task 'behavior imitation' method\nwith speech-text interleaving, called MTBI, which relies solely on paired\nspeech and transcripts. By ensuring the LLM decoder generates equivalent\nresponses to paired speech and text, we achieve a more generalized SLLM.\nInterleaving is used to further enhance alignment efficiency. We introduce a\nsimple benchmark to evaluate prompt and task generalization across different\nmodels. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs\non both prompt and task generalization, while requiring less supervised speech\ndata.", "published": "2025-05-24 11:09:13", "link": "http://arxiv.org/abs/2505.18644v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation", "abstract": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.", "published": "2025-05-24 09:28:09", "link": "http://arxiv.org/abs/2505.18614v1", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network", "abstract": "Universal speech enhancement aims to handle input speech with different\ndistortions and input formats. To tackle this challenge, we present TS-URGENet,\na Three-Stage Universal, Robust, and Generalizable speech Enhancement Network.\nTo address various distortions, the proposed system employs a novel three-stage\narchitecture consisting of a filling stage, a separation stage, and a\nrestoration stage. The filling stage mitigates packet loss by preliminarily\nfilling lost regions under noise interference, ensuring signal continuity. The\nseparation stage suppresses noise, reverberation, and clipping distortion to\nimprove speech clarity. Finally, the restoration stage compensates for\nbandwidth limitation, codec artifacts, and residual packet loss distortion,\nrefining the overall speech quality. Our proposed TS-URGENet achieved\noutstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd\nin Track 1.", "published": "2025-05-24 05:53:05", "link": "http://arxiv.org/abs/2505.18533v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs", "abstract": "Foundation models based on large language models (LLMs) have shown great\nsuccess in handling various tasks and modalities. However, adapting these\nmodels for general-purpose audio-language tasks is challenging due to\ndifferences in acoustic environments and task variations. In this work, we\nintroduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a\nframework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic\nprompt selection strategy with learnable key-value pairs, allowing the model to\nbalance general and task-specific knowledge while avoiding overfitting in a\nmultitask setting. Our approach reduces dependence on large-scale ASR or\ncaptioning datasets, achieves competitive performance with fewer trainable\nparameters, and simplifies training by using a single-stage process.\nAdditionally, LiSTEN enhances interpretability by analyzing the diversity and\noverlap of selected prompts across different tasks.", "published": "2025-05-24 05:28:22", "link": "http://arxiv.org/abs/2505.18517v1", "categories": ["cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Distinctive Feature Codec: Adaptive Segmentation for Efficient Speech Representation", "abstract": "The tokenization of speech with neural speech codec models is a crucial\naspect of AI systems designed for speech understanding and generation. While\ntext-based systems naturally benefit from token boundaries between discrete\nsymbols, tokenizing continuous speech signals is more complex due to the\nunpredictable timing of important acoustic variations. Most current neural\nspeech codecs typically address this by using uniform processing at fixed time\nintervals, which overlooks the varying information density inherent in speech.\nIn this paper, we introduce a distinctive feature-based approach that\ndynamically allocates tokens based on the perceptual significance of speech\ncontent. By learning to identify and prioritize distinctive regions in speech\nsignals, our approach achieves a significantly more efficient speech\nrepresentation compared with conventional frame-based methods. This work marks\nthe first successful extension of traditional signal processing-based\ndistinctive features into deep learning frameworks. Through rigorous\nexperimentation, we demonstrate the effectiveness of our approach and provide\ntheoretical insights into how aligning segment boundaries with natural acoustic\ntransitions improves codebook utilization. Additionally, we enhance\ntokenization stability by developing a Group-wise Scalar Quantization approach\nfor variable-length segments. Our distinctive feature-based approach offers a\npromising alternative to conventional frame-based processing and advances\ninterpretable representation learning in the modern deep learning speech\nprocessing framework.", "published": "2025-05-24 05:28:16", "link": "http://arxiv.org/abs/2505.18516v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning Emotion-Invariant Speaker Representations for Speaker Verification", "abstract": "In recent years, the rapid progress in speaker verification (SV) technology\nhas been driven by the extraction of speaker representations based on deep\nlearning. However, such representations are still vulnerable to emotion\nvariability. To address this issue, we propose multiple improvements to train\nspeaker encoders to increase emotion robustness. Firstly, we utilize\nCopyPaste-based data augmentation to gather additional parallel data, which\nincludes different emotional expressions from the same speaker. Secondly, we\napply cosine similarity loss to restrict parallel sample pairs and minimize\nintra-class variation of speaker representations to reduce their correlation\nwith emotional information. Finally, we use emotion-aware masking (EM) based on\nthe speech signal energy on the input parallel samples to further strengthen\nthe speaker representation and make it emotion-invariant. We conduct a\ncomprehensive ablation study to demonstrate the effectiveness of these various\ncomponents. Experimental results show that our proposed method achieves a\nrelative 19.29\\% drop in EER compared to the baseline system.", "published": "2025-05-24 04:31:12", "link": "http://arxiv.org/abs/2505.18498v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Token-Level Logits Matter: A Closer Look at Speech Foundation Models for Ambiguous Emotion Recognition", "abstract": "Emotional intelligence in conversational AI is crucial across domains like\nhuman-computer interaction. While numerous models have been developed, they\noften overlook the complexity and ambiguity inherent in human emotions. In the\nera of large speech foundation models (SFMs), understanding their capability in\nrecognizing ambiguous emotions is essential for the development of\nnext-generation emotion-aware models. This study examines the effectiveness of\nSFMs in ambiguous emotion recognition. We designed prompts for ambiguous\nemotion prediction and introduced two novel approaches to infer ambiguous\nemotion distributions: one analysing generated text responses and the other\nexamining the internal processing of SFMs through token-level logits. Our\nfindings suggest that while SFMs may not consistently generate accurate text\nresponses for ambiguous emotions, they can interpret such emotions at the token\nlevel based on prior knowledge, demonstrating robustness across different\nprompts.", "published": "2025-05-24 03:16:21", "link": "http://arxiv.org/abs/2505.18484v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CHSER: A Dataset and Case Study on Generative Speech Error Correction for Child ASR", "abstract": "Automatic Speech Recognition (ASR) systems struggle with child speech due to\nits distinct acoustic and linguistic variability and limited availability of\nchild speech datasets, leading to high transcription error rates. While ASR\nerror correction (AEC) methods have improved adult speech transcription, their\neffectiveness on child speech remains largely unexplored. To address this, we\nintroduce CHSER, a Generative Speech Error Correction (GenSEC) dataset for\nchild speech, comprising 200K hypothesis-transcription pairs spanning diverse\nage groups and speaking styles. Results demonstrate that fine-tuning on the\nCHSER dataset achieves up to a 28.5% relative WER reduction in a zero-shot\nsetting and a 13.3% reduction when applied to fine-tuned ASR systems.\nAdditionally, our error analysis reveals that while GenSEC improves\nsubstitution and deletion errors, it struggles with insertions and\nchild-specific disfluencies. These findings highlight the potential of GenSEC\nfor improving child ASR.", "published": "2025-05-24 02:06:03", "link": "http://arxiv.org/abs/2505.18463v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt", "abstract": "Most existing Zero-Shot Text-To-Speech(ZS-TTS) systems generate the unseen\nspeech based on single prompt, such as reference speech or text descriptions,\nwhich limits their flexibility. We propose a customized emotion ZS-TTS system\nbased on multi-modal prompt. The system disentangles speech into the content,\ntimbre, emotion and prosody, allowing emotion prompts to be provided as text,\nimage or speech. To extract emotion information from different prompts, we\npropose a multi-modal prompt emotion encoder. Additionally, we introduce an\nprosody predictor to fit the distribution of prosody and propose an emotion\nconsistency loss to preserve emotion information in the predicted prosody. A\ndiffusion-based acoustic model is employed to generate the target\nmel-spectrogram. Both objective and subjective experiments demonstrate that our\nsystem outperforms existing systems in terms of naturalness and similarity. The\nsamples are available at https://mpetts-demo.github.io/mpetts_demo/.", "published": "2025-05-24 01:26:02", "link": "http://arxiv.org/abs/2505.18453v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distributed Expectation Propagation for Multi-Object Tracking over Sensor Networks", "abstract": "In this paper, we present a novel distributed expectation propagation\nalgorithm for multiple sensors, multiple objects tracking in cluttered\nenvironments. The proposed framework enables each sensor to operate locally\nwhile collaboratively exchanging moment estimates with other sensors, thus\neliminating the need to transmit all data to a central processing node.\nSpecifically, we introduce a fast and parallelisable Rao-Blackwellised Gibbs\nsampling scheme to approximate the tilted distributions, which enhances the\naccuracy and efficiency of expectation propagation updates. Results demonstrate\nthat the proposed algorithm improves both communication and inference\nefficiency for multi-object tracking tasks with dynamic sensor connectivity and\nvarying clutter levels.", "published": "2025-05-24 17:15:50", "link": "http://arxiv.org/abs/2505.18795v1", "categories": ["eess.SP", "cs.RO"], "primary_category": "eess.SP"}
{"title": "Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation", "abstract": "With the proliferation of smart grids, smart cities face growing challenges\ndue to cyber-attacks and sophisticated electricity theft behaviors,\nparticularly in residential photovoltaic (PV) generation systems. Traditional\nElectricity Theft Detection (ETD) methods often struggle to capture complex\ntemporal dependencies and integrating multi-source data, limiting their\neffectiveness. In this work, we propose an efficient ETD method that accurately\nidentifies fraudulent behaviors in residential PV generation, thus ensuring the\nsupply-demand balance in smart cities. Our hybrid deep learning model,\ncombining multi-scale Convolutional Neural Network (CNN), Long Short-Term\nMemory (LSTM), and Transformer, excels in capturing both short-term and\nlong-term temporal dependencies. Additionally, we introduce a data embedding\ntechnique that seamlessly integrates time-series data with discrete temperature\nvariables, enhancing detection robustness. Extensive simulation experiments\nusing real-world data validate the effectiveness of our approach, demonstrating\nsignificant improvements in the accuracy of detecting sophisticated energy\ntheft activities, thereby contributing to the stability and fairness of energy\nsystems in smart cities.", "published": "2025-05-24 15:47:00", "link": "http://arxiv.org/abs/2505.18755v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion", "abstract": "With the advancement of energy Internet and energy system integration, the\nincreasing adoption of distributed photovoltaic (PV) systems presents new\nchallenges on smart monitoring and measurement for utility companies,\nparticularly in separating PV generation from net electricity load. Existing\nmethods struggle with feature extraction from net load and capturing the\nrelevance between weather factors. This paper proposes a PV disaggregation\nmethod that integrates Hierarchical Interpolation (HI) and multi-head\nself-attention mechanisms. By using HI to extract net load features and\nmulti-head self-attention to capture the complex dependencies between weather\nfactors, the method achieves precise PV generation predictions. Simulation\nexperiments demonstrate the effectiveness of the proposed method in real-world\ndata, supporting improved monitoring and management of distributed energy\nsystems.", "published": "2025-05-24 15:25:46", "link": "http://arxiv.org/abs/2505.18747v1", "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Waveform Coexistence-Driven RSMA: A Pioneering Strategy for Future 6G Networks", "abstract": "Two critical approaches have emerged in the literature for the successful\nrealization of 6G wireless networks: the coexistence of multiple waveforms and\nthe adoption of non-orthogonal multiple access. These strategies hold\ntransformative potential for addressing the limitations of current systems and\nenabling the robust and scalable design of next-generation wireless networks.\nThis paper presents a novel rate splitting multiple access (RSMA) framework\nthat leverages the coexistence of affine frequency division multiplexing (AFDM)\nand orthogonal frequency division multiplexing (OFDM). By transmitting common\ndata via AFDM at higher power in the affine domain and private data via OFDM at\nlower power in the frequency domain, the proposed framework eliminates the\nreliance on successive interference cancellation (SIC), significantly\nsimplifying receiver design. Furthermore, two data mapping approaches are\nproposed: a clean pilot method, where pilots are allocated without any data\noverlapping, ensuring clear separation, and an embedded pilot method, where\npilots overlap with data for more efficient resource utilization. Channel\nestimation is then performed for different channel types. Simulation results\ndemonstrate the robustness and efficiency of the proposed approach, achieving\nsuperior performance in efficiency, reliability, and adaptability under diverse\nchannel conditions. This framework transforms non-orthogonal multi-access\ndesign, paving the way for scalable and efficient solutions in 6G networks.", "published": "2025-05-24 15:20:19", "link": "http://arxiv.org/abs/2505.18739v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "EOTNet: Deep Memory Aided Bayesian Filter for Extended Object Tracking", "abstract": "Extended object tracking methods based on random matrices, founded on\nBayesian filters, have been able to achieve efficient recursive processes while\njointly estimating the kinematic states and extension of the targets. Existing\nrandom matrix approaches typically assume that the evolution of state and\nextension follows a first-order Markov process, where the current estimate of\nthe target depends solely on the previous moment. However, in real-world\nscenarios, this assumption fails because the evolution of states and extension\nis usually non-Markovian. In this paper, we introduce a novel extended object\ntracking method: a Bayesian recursive neural network assisted by deep memory.\nInitially, we propose an equivalent model under a non-Markovian assumption and\nderive the implementation of its Bayesian filtering framework. Thereafter,\nGaussian approximation and moment matching are employed to derive the\nanalytical solution for the proposed Bayesian filtering framework. Finally,\nbased on the closed-form solution, we design an end-to-end trainable Bayesian\nrecursive neural network for extended object tracking. Experiment results on\nsimulated and real-world datasets show that the proposed methods outperforms\ntraditional extended object tracking methods and state-of-the-art deep learning\napproaches.", "published": "2025-05-24 12:59:01", "link": "http://arxiv.org/abs/2505.18684v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "FDMA-Based Passive Multiple Users SWIPT Utilizing Resonant Beams", "abstract": "The rapid development of IoT technology has led to a shortage of spectrum\nresources and energy, giving rise to simultaneous wireless information and\npower transfer (SWIPT) technology. However, traditional multiple input multiple\noutput (MIMO)-based SWIPT faces challenges in target detection. We have\ndesigned a passive multi-user resonant beam system (MU-RBS) that can achieve\nefficient power transfer and communication through adaptive beam alignment. The\nfrequency division multiple access (FDMA) is employed in the downlink (DL)\nchannel, while frequency conversion is utilized in the uplink (UL) channel to\navoid echo interference and co-channel interference, and the system\narchitecture design and corresponding mathematical model are presented. The\nsimulation results show that MU-RBS can achieve adaptive beam-forming without\nthe target transmitting pilot signals, has high directivity, and as the number\nof iterations increases, the power transmission efficiency, signal-to-noise\nratio and spectral efficiency of the UL and DL are continuously optimized until\nthe system reaches the optimal state.", "published": "2025-05-24 11:02:23", "link": "http://arxiv.org/abs/2505.18641v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power", "abstract": "Structural health monitoring (SHM) involves sensor deployment, data\nacquisition, and data interpretation, commonly implemented via a tedious wired\nsystem. The information processing in current practice majorly depends on\nelectronic computers, albeit with universal applications, delivering challenges\nsuch as high energy consumption and low throughput due to the nature of digital\nunits. In recent years, there has been a renaissance interest in shifting\ncomputations from electronic computing units to the use of real physical\nsystems, a concept known as physical computation. This approach provides the\npossibility of thinking out of the box for SHM, seamlessly integrating sensing\nand computing into a pure-physical entity, without relying on external\nelectronic power supplies, thereby properly coping with resource-restricted\nscenarios. The latest advances of metamaterials (MM) hold great promise for\nthis proactive idea. In this paper, we introduce a programmable\nmetamaterial-based sensor (termed as MM-sensor) for physically processing\nstructural vibration information to perform specific SHM tasks, such as\nstructural damage warning (binary classification) in this initiation, without\nthe need for further information processing or resource-consuming, that is, the\ndata collection and analysis are completed in-situ at the sensor level. We\nadopt the configuration of a locally resonant metamaterial plate (LRMP) to\nachieve the first fabrication of the MM-sensor. We take advantage of the\nbandgap properties of LRMP to physically differentiate the dynamic behavior of\nstructures before and after damage. By inversely designing the geometric\nparameters, our current approach allows for adjustments to the bandgap\nfeatures. This is effective for engineering systems with a first natural\nfrequency ranging from 9.54 Hz to 81.86 Hz.", "published": "2025-05-24 08:08:02", "link": "http://arxiv.org/abs/2505.18579v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "A DSP-Free Carrier Phase Recovery System using 16-Offset-QAM Laser Forwarded Links for 400Gb/s and Beyond", "abstract": "Optical interconnects are becoming a major bottleneck in scaling up future\nGPU racks and network switches within data centers. Although 200 Gb/s optical\ntransceivers using PAM-4 modulation have been demonstrated, achieving higher\ndata rates and energy efficiencies requires high-order coherent modulations\nlike 16-QAM. Current coherent links rely on energy-intensive digital signal\nprocessing (DSP) for channel impairment compensation and carrier phase recovery\n(CPR), which consumes approximately 50pJ/b - 10x higher than future intra-data\ncenter requirements. For shorter links, simpler or DSP-free CPR methods can\nsignificantly reduce power and complexity. While Costas loops enable CPR for\nQPSK, they face challenges in scaling to higher-order modulations (e.g.,\n16/64-QAM) due to varying symbol amplitudes. In this work, we propose an\noptical coherent link architecture using laser forwarding and a novel DSP-free\nCPR system using offset-QAM modulation. The proposed analog CPR feedback loop\nis highly scalable, capable of supporting arbitrary offset-QAM modulations\nwithout requiring architectural modifications. This scalability is achieved\nthrough its phase error detection mechanism, which operates independently of\nthe data rate and modulation type. We validated this method using\nGlobalFoundry's monolithic 45nm silicon photonics PDK models, with circuit- and\nsystem-level implementation at 100GBaud in the O-band. We will investigate the\nfeedback loop dynamics, circuit-level implementations, and phase-noise\nperformance of the proposed CPR loop. Our method can be adopted to realize\nlow-power QAM optical interconnects for future coherent-lite pluggable\ntransceivers as well as co-packaged optics (CPO) applications.", "published": "2025-05-24 05:53:36", "link": "http://arxiv.org/abs/2505.18534v1", "categories": ["eess.SP", "cs.NI", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "MTGR: Industrial-Scale Generative Recommendation Framework in Meituan", "abstract": "Scaling law has been extensively validated in many domains such as natural\nlanguage processing and computer vision. In the recommendation system, recent\nwork has adopted generative recommendations to achieve scalability, but their\ngenerative approaches require abandoning the carefully constructed cross\nfeatures of traditional recommendation models. We found that this approach\nsignificantly degrades model performance, and scaling up cannot compensate for\nit at all. In this paper, we propose MTGR (Meituan Generative Recommendation)\nto address this issue. MTGR is modeling based on the HSTU architecture and can\nretain the original deep learning recommendation model (DLRM) features,\nincluding cross features. Additionally, MTGR achieves training and inference\nacceleration through user-level compression to ensure efficient scaling. We\nalso propose Group-Layer Normalization (GLN) to enhance the performance of\nencoding within different semantic spaces and the dynamic masking strategy to\navoid information leakage. We further optimize the training frameworks,\nenabling support for our models with 10 to 100 times computational complexity\ncompared to the DLRM, without significant cost increases. MTGR achieved 65x\nFLOPs for single-sample forward inference compared to the DLRM model, resulting\nin the largest gain in nearly two years both offline and online. This\nbreakthrough was successfully deployed on Meituan, the world's largest food\ndelivery platform, where it has been handling the main traffic.", "published": "2025-05-24 11:47:28", "link": "http://arxiv.org/abs/2505.18654v2", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "A Survey of LLM $\\times$ DATA", "abstract": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.", "published": "2025-05-24 01:57:12", "link": "http://arxiv.org/abs/2505.18458v2", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning", "abstract": "Our examination of deep generative models (DGMs) developed for\nsemi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems.\nFirst, mode missing and mode covering phenomenons are observed in genertion\nwith GANs and VAEs. Second, there exists an awkward conflict between good\nclassification and good generation in SSL by employing directed generative\nmodels. To address these problems, we formally present\njoint-stochastic-approximation random fields (JRFs) -- a new family of\nalgorithms for building deep undirected generative models, with application to\nSSL. It is found through synthetic experiments that JRFs work well in balancing\nmode covering and mode missing, and match the empirical data distribution well.\nEmpirically, JRFs achieve good classification results comparable to the\nstate-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in\nSSL, and simultaneously perform good generation.", "published": "2025-05-24 07:04:32", "link": "http://arxiv.org/abs/2505.20330v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset", "abstract": "Text-based speech editing (TSE) modifies speech using only text, eliminating\nre-recording. However, existing TSE methods, mainly focus on the content\naccuracy and acoustic consistency of synthetic speech segments, and often\noverlook the emotional shifts or inconsistency issues introduced by text\nchanges. To address this issue, we propose EmoCorrector, a novel\npost-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented\nGeneration (RAG) by extracting the edited text's emotional features, retrieving\nspeech samples with matching emotions, and synthesizing speech that aligns with\nthe desired emotion while preserving the speaker's identity and quality. To\nsupport the training and evaluation of emotional consistency modeling in TSE,\nwe pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The\nprominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data\nfeaturing diverse text variations and a range of emotional expressions.\nSubjective and objective experiments and comprehensive analysis on ECD-TSE\nconfirm that EmoCorrector significantly enhances the expression of intended\nemotion while addressing emotion inconsistency limitations in current TSE\nmethods. Code and audio examples are available at\nhttps://github.com/AI-S2-Lab/EmoCorrector.", "published": "2025-05-24 16:10:56", "link": "http://arxiv.org/abs/2505.20341v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
