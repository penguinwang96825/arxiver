{"title": "High-Frequency Options Trading | With Portfolio Optimization", "abstract": "This paper explores the effectiveness of high-frequency options trading\nstrategies enhanced by advanced portfolio optimization techniques,\ninvestigating their ability to consistently generate positive returns compared\nto traditional long or short positions on options. Utilizing SPY options data\nrecorded in five-minute intervals over a one-month period, we calculate key\nmetrics such as Option Greeks and implied volatility, applying the Binomial\nTree model for American options pricing and the Newton-Raphson algorithm for\nimplied volatility calculation. Investment universes are constructed based on\ncriteria like implied volatility and Greeks, followed by the application of\nvarious portfolio optimization models, including Standard Mean-Variance and\nRobust Methods. Our research finds that while basic long-short strategies\ncentered on implied volatility and Greeks generally underperform, more\nsophisticated strategies incorporating advanced Greeks, such as Vega and Rho,\nalong with dynamic portfolio optimization, show potential in effectively\nnavigating the complexities of the options market. The study highlights the\nimportance of adaptability and responsiveness in dynamic portfolio strategies\nwithin the high-frequency trading environment, particularly under volatile\nmarket conditions. Future research could refine strategy parameters and explore\nless frequently traded options, offering new insights into high-frequency\noptions trading and portfolio management.", "published": "2024-08-16 17:49:21", "link": "http://arxiv.org/abs/2408.08866v1", "categories": ["q-fin.TR", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for\n  Advanced Retrieval-Augmented Generation in Fact-Checking", "abstract": "Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG) systems, their effectiveness is often hindered by a lack of\nintegration with entity relationships and community structures, limiting their\nability to provide contextually rich and accurate information retrieval for\nfact-checking. We introduce CommunityKG-RAG (Community Knowledge\nGraph-Retrieval Augmented Generation), a novel zero-shot framework that\nintegrates community structures within Knowledge Graphs (KGs) with RAG systems\nto enhance the fact-checking process. Capable of adapting to new domains and\nqueries without additional training, CommunityKG-RAG utilizes the multi-hop\nnature of community structures within KGs to significantly improve the accuracy\nand relevance of information retrieval. Our experimental results demonstrate\nthat CommunityKG-RAG outperforms traditional methods, representing a\nsignificant advancement in fact-checking by offering a robust, scalable, and\nefficient solution.", "published": "2024-08-16 05:15:12", "link": "http://arxiv.org/abs/2408.08535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models", "abstract": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.", "published": "2024-08-16 06:11:21", "link": "http://arxiv.org/abs/2408.08545v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Multi-view Analysis: Multi-view Mixture-of-Expert for\n  Textual Personality Detection", "abstract": "Textual personality detection aims to identify personality traits by\nanalyzing user-generated content. To achieve this effectively, it is essential\nto thoroughly examine user-generated content from various perspectives.\nHowever, previous studies have struggled with automatically extracting and\neffectively integrating information from multiple perspectives, thereby\nlimiting their performance on personality detection. To address these\nchallenges, we propose the Multi-view Mixture-of-Experts Model for Textual\nPersonality Detection (MvP). MvP introduces a Multi-view Mixture-of-Experts\n(MoE) network to automatically analyze user posts from various perspectives.\nAdditionally, it employs User Consistency Regularization to mitigate conflicts\namong different perspectives and learn a multi-view generic user\nrepresentation. The model's training is optimized via a multi-task joint\nlearning strategy that balances supervised personality detection with\nself-supervised user consistency constraints. Experimental results on two\nwidely-used personality detection datasets demonstrate the effectiveness of the\nMvP model and the benefits of automatically analyzing user posts from diverse\nperspectives for textual personality detection.", "published": "2024-08-16 06:35:31", "link": "http://arxiv.org/abs/2408.08551v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of\n  Biomedical Research Articles", "abstract": "This paper presents the setup and results of the second edition of the\nBioLaySumm shared task on the Lay Summarisation of Biomedical Research\nArticles, hosted at the BioNLP Workshop at ACL 2024. In this task edition, we\naim to build on the first edition's success by further increasing research\ninterest in this important task and encouraging participants to explore novel\napproaches that will help advance the state-of-the-art. Encouragingly, we found\nresearch interest in the task to be high, with this edition of the task\nattracting a total of 53 participating teams, a significant increase in\nengagement from the previous edition. Overall, our results show that a broad\nrange of innovative approaches were adopted by task participants, with a\npredictable shift towards the use of Large Language Models (LLMs).", "published": "2024-08-16 07:00:08", "link": "http://arxiv.org/abs/2408.08566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persona is a Double-edged Sword: Mitigating the Negative Impact of\n  Role-playing Prompts in Zero-shot Reasoning Tasks", "abstract": "Recent studies demonstrate that prompting a role-playing persona to an LLM\nimproves reasoning capability. However, assigning an adequate persona is\ndifficult since LLMs are extremely sensitive to assigned prompts; thus,\ninaccurately defined personas sometimes hinder LLMs and degrade their reasoning\ncapabilities. In this paper, we first investigate the potential negative impact\nof injecting persona into language models. Furthermore, we propose a novel\nframework, Jekyll \\& Hyde, which ensembles the outcomes of both role-playing\nand neutral prompts to enhance the robustness of reasoning ability.\nSpecifically, Jekyll \\& Hyde predicts an appropriate persona using an LLM when\ndefining the role-playing prompt. Then, Jekyll \\& Hyde collects two potential\nsolutions from role-playing and neutral prompts and selects a better solution\nusing the LLM evaluator. The experimental analysis demonstrates that\nrole-playing prompts sometimes distract LLMs, degrading their reasoning\nabilities in 7 out of 12 datasets in llama3. Meanwhile, Jekyll \\& Hyde improve\nreasoning capabilities by selecting better choices among the potential\nsolutions on twelve widely-used natural language reasoning datasets. In\naddition, we reveal that assigning LLM-generated personas obtains more stable\nresults than handcrafted personas.", "published": "2024-08-16 09:49:51", "link": "http://arxiv.org/abs/2408.08631v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Math-PUMA: Progressive Upward Multimodal Alignment to Enhance\n  Mathematical Reasoning", "abstract": "Multimodal Large Language Models (MLLMs) excel in solving text-based\nmathematical problems, but they struggle with mathematical diagrams since they\nare primarily trained on natural scene images. For humans, visual aids\ngenerally enhance problem-solving, but MLLMs perform worse as information\nshifts from textual to visual modality. This decline is mainly due to their\nshortcomings in aligning images and text. To tackle aforementioned challenges,\nwe propose Math-PUMA, a methodology focused on Progressive Upward Multimodal\nAlignment. This approach is designed to improve the mathematical reasoning\nskills of MLLMs through a three-stage training process, with the second stage\nbeing the critical alignment stage. We first enhance the language model's\nmathematical reasoning capabilities with extensive set of textual mathematical\nproblems. We then construct a multimodal dataset with varying degrees of\ntextual and visual information, creating data pairs by presenting each problem\nin at least two forms. By leveraging the Kullback-Leibler (KL) divergence of\nnext-token prediction distributions to align visual and textual modalities,\nconsistent problem-solving abilities are ensured. Finally, we utilize\nmultimodal instruction tuning for MLLMs with high-quality multimodal data.\nExperimental results on multiple mathematical reasoning benchmarks demonstrate\nthat the MLLMs trained with Math-PUMA surpass most open-source MLLMs. Our\napproach effectively narrows the performance gap for problems presented in\ndifferent modalities. The code and data are available at:\n\\url{https://github.com/wwzhuang01/Math-PUMA}.", "published": "2024-08-16 10:11:05", "link": "http://arxiv.org/abs/2408.08640v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation", "abstract": "Photo-Sharing Multi-modal dialogue generation requires a dialogue agent not\nonly to generate text responses but also to share photos at the proper moment.\nUsing image text caption as the bridge, a pipeline model integrates an image\ncaption model, a text generation model, and an image generation model to handle\nthis complex multi-modal task. However, representing the images with text\ncaptions may loss important visual details and information and cause error\npropagation in the complex dialogue system. Besides, the pipeline model\nisolates the three models separately because discrete image text captions\nhinder end-to-end gradient propagation. We propose the first end-to-end model\nfor photo-sharing multi-modal dialogue generation, which integrates an image\nperceptron and an image generator with a large language model. The large\nlanguage model employs the Q-Former to perceive visual images in the input end.\nFor image generation in the output end, we propose a dynamic vocabulary\ntransformation matrix and use straight-through and gumbel-softmax techniques to\nalign the large language model and stable diffusion model and achieve\nend-to-end gradient propagation. We perform experiments on PhotoChat and\nDialogCC datasets to evaluate our end-to-end model. Compared with pipeline\nmodels, the end-to-end model gains state-of-the-art performances on various\nmetrics of text and image generation. More analysis experiments also verify the\neffectiveness of the end-to-end model for photo-sharing multi-modal dialogue\ngeneration.", "published": "2024-08-16 10:33:19", "link": "http://arxiv.org/abs/2408.08650v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Are Biased Towards Output Formats! Systematically Evaluating and\n  Mitigating Output Format Bias of LLMs", "abstract": "We present the first systematic evaluation examining format bias in\nperformance of large language models (LLMs). Our approach distinguishes between\ntwo categories of an evaluation metric under format constraints to reliably and\naccurately assess performance: one measures performance when format constraints\nare adhered to, while the other evaluates performance regardless of constraint\nadherence. We then define a metric for measuring the format bias of LLMs and\nestablish effective strategies to reduce it. Subsequently, we present our\nempirical format bias evaluation spanning four commonly used categories --\nmultiple-choice question-answer, wrapping, list, and mapping -- covering 15\nwidely-used formats. Our evaluation on eight generation tasks uncovers\nsignificant format bias across state-of-the-art LLMs. We further discover that\nimproving the format-instruction following capabilities of LLMs across formats\npotentially reduces format bias. Based on our evaluation findings, we study\nprompting and fine-tuning with synthesized format data techniques to mitigate\nformat bias. Our methods successfully reduce the variance in ChatGPT's\nperformance among wrapping formats from 235.33 to 0.71 (%$^2$).", "published": "2024-08-16 10:45:45", "link": "http://arxiv.org/abs/2408.08656v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Med-PMC: Medical Personalized Multi-modal Consultation with a Proactive\n  Ask-First-Observe-Next Paradigm", "abstract": "The application of the Multi-modal Large Language Models (MLLMs) in medical\nclinical scenarios remains underexplored. Previous benchmarks only focus on the\ncapacity of the MLLMs in medical visual question-answering (VQA) or report\ngeneration and fail to assess the performance of the MLLMs on complex clinical\nmulti-modal tasks. In this paper, we propose a novel Medical Personalized\nMulti-modal Consultation (Med-PMC) paradigm to evaluate the clinical capacity\nof the MLLMs. Med-PMC builds a simulated clinical environment where the MLLMs\nare required to interact with a patient simulator to complete the multi-modal\ninformation-gathering and decision-making task. Specifically, the patient\nsimulator is decorated with personalized actors to simulate diverse patients in\nreal scenarios. We conduct extensive experiments to access 12 types of MLLMs,\nproviding a comprehensive view of the MLLMs' clinical performance. We found\nthat current MLLMs fail to gather multimodal information and show potential\nbias in the decision-making task when consulted with the personalized patient\nsimulators. Further analysis demonstrates the effectiveness of Med-PMC, showing\nthe potential to guide the development of robust and reliable clinical MLLMs.\nCode and data are available at https://github.com/LiuHC0428/Med-PMC.", "published": "2024-08-16 12:14:55", "link": "http://arxiv.org/abs/2408.08693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target\n  Language", "abstract": "Although large language models(LLMs) show amazing capabilities, among various\nexciting applications discovered for LLMs fall short in other low-resource\nlanguages. Besides, most existing methods depend on large-scale dialogue\ncorpora and thus building systems for dialogue generation in a zero-shot\nscenario remains a considerable challenge. To address this challenge, we\npropose a novel end-to-end zero-shot dialogue generation model ChatZero based\non cross-lingual code-switching method. First, we construct code-switching\nlanguage and pseudo-target language with placeholders. Then for cross-lingual\nsemantic transfer, we employ unsupervised contrastive learning to minimize the\nsemantics gap of the source language, code-switching language, and\npseudo-target language that are mutually positive examples in the high\ndimensional semantic space. Experiments on the multilingual DailyDialog and\nDSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\\% of the\noriginal performance under the zero-shot case compared to supervised learning,\nand achieve state-of-the-art performance compared with other baselines.", "published": "2024-08-16 13:11:53", "link": "http://arxiv.org/abs/2408.08724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion\n  Contrastive Decoding with Truthfulness Refocused", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks, yet they occasionally tend to yield\ncontent that factually inaccurate or discordant with the expected output, a\nphenomenon empirically referred to as \"hallucination\". To tackle this issue,\nrecent works have investigated contrastive decoding between the original model\nand an amateur model with induced hallucination, which has shown promising\nresults. Nonetheless, this method may undermine the output distribution of the\noriginal LLM caused by its coarse contrast and simplistic subtraction\noperation, potentially leading to errors in certain cases. In this paper, we\nintroduce a novel contrastive decoding framework termed LOL (LOwer Layer\nMatters). Our approach involves concatenating the contrastive decoding of both\nthe final and lower layers between the original model and the amateur model,\nthereby achieving multi-layer fusion to aid in the mitigation of hallucination.\nAdditionally, we incorporate a truthfulness refocused module that leverages\ncontextual guidance to enhance factual encoding, further capturing truthfulness\nduring contrastive decoding. Extensive experiments conducted on two publicly\navailable datasets illustrate that our proposed LOL framework can substantially\nalleviate hallucination while surpassing existing baselines in most cases.\nCompared with the best baseline, we improve by average 4.5 points on all\nmetrics of TruthfulQA. The source code is coming soon.", "published": "2024-08-16 14:23:59", "link": "http://arxiv.org/abs/2408.08769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAC: Decomposed Automation Correction for Text-to-SQL", "abstract": "Text-to-SQL is an important task that helps people obtain information from\ndatabases by automatically generating SQL queries. Considering the brilliant\nperformance, approaches based on Large Language Models (LLMs) become the\nmainstream for text-to-SQL. Among these approaches, automated correction is an\neffective approach that further enhances performance by correcting the mistakes\nin the generated results. The existing correction methods require LLMs to\ndirectly correct with generated SQL, while previous research shows that LLMs do\nnot know how to detect mistakes, leading to poor performance. Therefore, in\nthis paper, we propose to employ the decomposed correction to enhance\ntext-to-SQL performance. We first demonstrate that decomposed correction\noutperforms direct correction since detecting and fixing mistakes with the\nresults of the decomposed sub-tasks is easier than with SQL. Based on this\nanalysis, we introduce Decomposed Automation Correction (DAC), which corrects\nSQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC\nfirst generates the entity and skeleton corresponding to the question and then\ncompares the differences between the initial SQL and the generated entities and\nskeleton as feedback for correction. Experimental results show that our method\nimproves performance by $3.7\\%$ on average of Spider, Bird, and KaggleDBQA\ncompared with the baseline method, demonstrating the effectiveness of DAC.", "published": "2024-08-16 14:43:15", "link": "http://arxiv.org/abs/2408.08779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Might Not Care What You Are Saying: Prompt Format\n  Beats Descriptions", "abstract": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL performance. But to our surprise, LLMs might not care what the\ndescriptions actually say, and the performance gain is primarily caused by the\nensemble format, since it could lead to improvement even with random\ndescriptive nouns. We further apply this new ensemble framework on a range of\ncommonsense, math, logical reasoning and hallucination tasks with three LLMs\nand achieve promising results, suggesting again that designing a proper prompt\nformat would be much more effective and efficient than paying effort into\nspecific descriptions. Our code will be publicly available once this paper is\npublished.", "published": "2024-08-16 14:49:04", "link": "http://arxiv.org/abs/2408.08780v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics", "abstract": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.", "published": "2024-08-16 14:54:41", "link": "http://arxiv.org/abs/2408.08782v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FourierKAN outperforms MLP on Text Classification Head Fine-tuning", "abstract": "In resource constraint settings, adaptation to downstream classification\ntasks involves fine-tuning the final layer of a classifier (i.e. classification\nhead) while keeping rest of the model weights frozen. Multi-Layer Perceptron\n(MLP) heads fine-tuned with pre-trained transformer backbones have long been\nthe de facto standard for text classification head fine-tuning. However, the\nfixed non-linearity of MLPs often struggles to fully capture the nuances of\ncontextual embeddings produced by pre-trained models, while also being\ncomputationally expensive. In our work, we investigate the efficacy of KAN and\nits variant, Fourier KAN (FR-KAN), as alternative text classification heads.\nOur experiments reveal that FR-KAN significantly outperforms MLPs with an\naverage improvement of 10% in accuracy and 11% in F1-score across seven\npre-trained transformer models and four text classification tasks. Beyond\nperformance gains, FR-KAN is more computationally efficient and trains faster\nwith fewer parameters. These results underscore the potential of FR-KAN to\nserve as a lightweight classification head, with broader implications for\nadvancing other Natural Language Processing (NLP) tasks.", "published": "2024-08-16 15:28:02", "link": "http://arxiv.org/abs/2408.08803v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats", "abstract": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods.", "published": "2024-08-16 17:00:11", "link": "http://arxiv.org/abs/2408.08841v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models", "abstract": "This paper explores the intersection of psychology and artificial\nintelligence through the development and evaluation of specialized Large\nLanguage Models (LLMs). We introduce PsychoLex, a suite of resources designed\nto enhance LLMs' proficiency in psychological tasks in both Persian and\nEnglish. Key contributions include the PsychoLexQA dataset for instructional\ncontent and the PsychoLexEval dataset for rigorous evaluation of LLMs in\ncomplex psychological scenarios. Additionally, we present the PsychoLexLLaMA\nmodel, optimized specifically for psychological applications, demonstrating\nsuperior performance compared to general-purpose models. The findings\nunderscore the potential of tailored LLMs for advancing psychological research\nand applications, while also highlighting areas for further refinement. This\nresearch offers a foundational step towards integrating LLMs into specialized\npsychological domains, with implications for future advancements in AI-driven\npsychological practice.", "published": "2024-08-16 17:19:23", "link": "http://arxiv.org/abs/2408.08848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment\n  Analysis", "abstract": "The widespread availability of code-mixed data can provide valuable insights\ninto low-resource languages like Bengali, which have limited datasets.\nSentiment analysis has been a fundamental text classification task across\nseveral languages for code-mixed data. However, there has yet to be a\nlarge-scale and diverse sentiment analysis dataset on code-mixed Bengali. We\naddress this limitation by introducing BnSentMix, a sentiment analysis dataset\non code-mixed Bengali consisting of 20,000 samples with 4 sentiment labels from\nFacebook, YouTube, and e-commerce sites. We ensure diversity in data sources to\nreplicate realistic code-mixed scenarios. Additionally, we propose 14 baseline\nmethods including novel transformer encoders further pre-trained on code-mixed\nBengali-English, achieving an overall accuracy of 69.8% and an F1 score of\n69.1% on sentiment classification tasks. Detailed analyses reveal variations in\nperformance across different sentiment labels and text types, highlighting\nareas for future improvement.", "published": "2024-08-16 18:30:22", "link": "http://arxiv.org/abs/2408.08964v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Task and Multi-Label Classification Model for Implicit Discourse\n  Relation Recognition", "abstract": "We address the inherent ambiguity in Implicit Discourse Relation Recognition\n(IDRR) by introducing a novel multi-task classification model capable of\nlearning both multi-label and single-label representations of discourse\nrelations. Our model is trained exclusively on the DiscoGeM corpus and\nevaluated both on the DiscoGeM and the PDTB 3.0 corpus. We establish the first\nbenchmark on multi-label IDRR classification and achieve SOTA results on\nsingle-label IDRR classification using the DiscoGeM corpus. Finally, we present\nthe first evaluation on the potential of transfer learning between the DiscoGeM\nand the PDTB 3.0 corpus on single-label IDRR classification.", "published": "2024-08-16 18:47:08", "link": "http://arxiv.org/abs/2408.08971v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering\n  LLM Weaknesses", "abstract": "The impressive performance of Large Language Models (LLMs) has consistently\nsurpassed numerous human-designed benchmarks, presenting new challenges in\nassessing the shortcomings of LLMs. Designing tasks and finding LLMs'\nlimitations are becoming increasingly important. In this paper, we investigate\nthe question of whether an LLM can discover its own limitations from the errors\nit makes. To this end, we propose a Self-Challenge evaluation framework with\nhuman-in-the-loop. Starting from seed instances that GPT-4 fails to answer, we\nprompt GPT-4 to summarize error patterns that can be used to generate new\ninstances and incorporate human feedback on them to refine these patterns for\ngenerating more challenging data, iteratively. We end up with 8 diverse\npatterns, such as text manipulation and questions with assumptions. We then\nbuild a benchmark, SC-G4, consisting of 1,835 instances generated by GPT-4\nusing these patterns, with human-annotated gold responses. The SC-G4 serves as\na challenging benchmark that allows for a detailed assessment of LLMs'\nabilities. Our results show that only 44.96\\% of instances in SC-G4 can be\nanswered correctly by GPT-4. Interestingly, our pilot study indicates that\nthese error patterns also challenge other LLMs, such as Claude-3 and Llama-3,\nand cannot be fully resolved through fine-tuning. Our work takes the first step\nto demonstrate that LLMs can autonomously identify their inherent flaws and\nprovide insights for future dynamic and automatic evaluation.", "published": "2024-08-16 19:01:52", "link": "http://arxiv.org/abs/2408.08978v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using large language models to estimate features of multi-word\n  expressions: Concreteness, valence, arousal", "abstract": "This study investigates the potential of large language models (LLMs) to\nprovide accurate estimates of concreteness, valence and arousal for multi-word\nexpressions. Unlike previous artificial intelligence (AI) methods, LLMs can\ncapture the nuanced meanings of multi-word expressions. We systematically\nevaluated ChatGPT-4o's ability to predict concreteness, valence and arousal. In\nStudy 1, ChatGPT-4o showed strong correlations with human concreteness ratings\n(r = .8) for multi-word expressions. In Study 2, these findings were repeated\nfor valence and arousal ratings of individual words, matching or outperforming\nprevious AI models. Study 3 extended the prevalence and arousal analysis to\nmulti-word expressions and showed promising results despite the lack of\nlarge-scale human benchmarks. These findings highlight the potential of LLMs\nfor generating valuable psycholinguistic data related to multiword expressions.\nTo help researchers with stimulus selection, we provide datasets with AI norms\nof concreteness, valence and arousal for 126,397 English single words and\n63,680 multi-word expressions", "published": "2024-08-16 07:02:34", "link": "http://arxiv.org/abs/2408.16012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding", "abstract": "Generating long-term texts such as novels using artificial intelligence has\nalways been a challenge. A common approach is to use large language models\n(LLMs) to construct a hierarchical framework that first plans and then writes.\nDespite the fact that the generated novels reach a sufficient length, they\nexhibit poor logical coherence and appeal in their plots and deficiencies in\ncharacter and event depiction, ultimately compromising the overall narrative\nquality. In this paper, we propose a method named Extracting Excelsior and\nExpanding. Ex3 initially extracts structure information from raw novel data. By\ncombining this structure information with the novel data, an\ninstruction-following dataset is meticulously crafted. This dataset is then\nutilized to fine-tune the LLM, aiming for excelsior generation performance. In\nthe final stage, a tree-like expansion method is deployed to facilitate the\ngeneration of arbitrarily long novels. Evaluation against previous methods\nshowcases Ex3's ability to produce higher-quality long-form novels.", "published": "2024-08-16 03:06:57", "link": "http://arxiv.org/abs/2408.08506v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement\n  Framework for Multimodal Question Answering", "abstract": "Recent advancements in retrieval-augmented generation (RAG) have demonstrated\nimpressive performance in the question-answering (QA) task. However, most\nprevious works predominantly focus on text-based answers. While some studies\naddress multimodal data, they still fall short in generating comprehensive\nmultimodal answers, particularly for explaining concepts or providing\nstep-by-step tutorials on how to accomplish specific goals. This capability is\nespecially valuable for applications such as enterprise chatbots and settings\nsuch as customer service and educational systems, where the answers are sourced\nfrom multimodal data. In this paper, we introduce a simple and effective\nframework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR\nenhances text-based answers by retrieving relevant multimodal data and refining\nthe responses to create coherent multimodal answers. This framework can be\neasily extended to support multimodal answers in enterprise chatbots with\nminimal modifications. Human evaluation results indicate that multimodal\nanswers generated by MuRAR are more useful and readable compared to plain text\nanswers.", "published": "2024-08-16 04:32:10", "link": "http://arxiv.org/abs/2408.08521v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Where is the signal in tokenization space?", "abstract": "Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models.", "published": "2024-08-16 05:56:10", "link": "http://arxiv.org/abs/2408.08541v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Collaborative Cross-modal Fusion with Large Language Model for\n  Recommendation", "abstract": "Despite the success of conventional collaborative filtering (CF) approaches\nfor recommendation systems, they exhibit limitations in leveraging semantic\nknowledge within the textual attributes of users and items. Recent focus on the\napplication of large language models for recommendation (LLM4Rec) has\nhighlighted their capability for effective semantic knowledge capture. However,\nthese methods often overlook the collaborative signals in user behaviors. Some\nsimply instruct-tune a language model, while others directly inject the\nembeddings of a CF-based model, lacking a synergistic fusion of different\nmodalities. To address these issues, we propose a framework of Collaborative\nCross-modal Fusion with Large Language Models, termed CCF-LLM, for\nrecommendation. In this framework, we translate the user-item interactions into\na hybrid prompt to encode both semantic knowledge and collaborative signals,\nand then employ an attentive cross-modal fusion strategy to effectively fuse\nlatent embeddings of both modalities. Extensive experiments demonstrate that\nCCF-LLM outperforms existing methods by effectively utilizing semantic and\ncollaborative signals in the LLM4Rec context.", "published": "2024-08-16 06:54:10", "link": "http://arxiv.org/abs/2408.08564v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "RealMedQA: A pilot biomedical question answering dataset containing\n  realistic clinical questions", "abstract": "Clinical question answering systems have the potential to provide clinicians\nwith relevant and timely answers to their questions. Nonetheless, despite the\nadvances that have been made, adoption of these systems in clinical settings\nhas been slow. One issue is a lack of question-answering datasets which reflect\nthe real-world needs of health professionals. In this work, we present\nRealMedQA, a dataset of realistic clinical questions generated by humans and an\nLLM. We describe the process for generating and verifying the QA pairs and\nassess several QA models on BioASQ and RealMedQA to assess the relative\ndifficulty of matching answers to questions. We show that the LLM is more\ncost-efficient for generating \"ideal\" QA pairs. Additionally, we achieve a\nlower lexical similarity between questions and answers than BioASQ which\nprovides an additional challenge to the top two QA models, as per the results.\nWe release our code and our dataset publicly to encourage further research.", "published": "2024-08-16 09:32:43", "link": "http://arxiv.org/abs/2408.08624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Enthymemes in Argument Maps: Bridging Argument Mining and\n  Logic-based Argumentation", "abstract": "Argument mining is natural language processing technology aimed at\nidentifying arguments in text. Furthermore, the approach is being developed to\nidentify the premises and claims of those arguments, and to identify the\nrelationships between arguments including support and attack relationships. In\nthis paper, we assume that an argument map contains the premises and claims of\narguments, and support and attack relationships between them, that have been\nidentified by argument mining. So from a piece of text, we assume an argument\nmap is obtained automatically by natural language processing. However, to\nunderstand and to automatically analyse that argument map, it would be\ndesirable to instantiate that argument map with logical arguments. Once we have\nthe logical representation of the arguments in an argument map, we can use\nautomated reasoning to analyze the argumentation (e.g. check consistency of\npremises, check validity of claims, and check the labelling on each arc\ncorresponds with thw logical arguments). We address this need by using\nclassical logic for representing the explicit information in the text, and\nusing default logic for representing the implicit information in the text. In\norder to investigate our proposal, we consider some specific options for\ninstantiation.", "published": "2024-08-16 10:30:30", "link": "http://arxiv.org/abs/2408.08648v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of\n  Thought Reasoning", "abstract": "Language models are known to absorb biases from their training data, leading\nto predictions driven by statistical regularities rather than semantic\nrelevance. We investigate the impact of these biases on answer choice\npreferences in the Massive Multi-Task Language Understanding (MMLU) task. Our\nfindings reveal that differences in learned regularities across answer options\nare predictive of model preferences and mirror human test-taking strategies. To\naddress this issue, we introduce two novel methods: Counterfactual Prompting\nwith Chain of Thought (CoT) and Counterfactual Prompting with Agnostically\nPrimed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with\nCoT alone is insufficient to mitigate bias, our novel Primed Counterfactual\nPrompting with CoT approach effectively reduces the influence of base-rate\nprobabilities while improving overall accuracy. Our results suggest that\nmitigating bias requires a \"System-2\" like process and that CoT reasoning is\nsusceptible to confirmation bias under some prompting methodologies. Our\ncontributions offer practical solutions for developing more robust and fair\nlanguage models.", "published": "2024-08-16 10:34:50", "link": "http://arxiv.org/abs/2408.08651v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic\n  Preference Optimization Dataset Generation", "abstract": "This paper presents a novel methodology for generating synthetic Preference\nOptimization (PO) datasets using multi-agent workflows. We evaluate the\neffectiveness and potential of these workflows in automating and enhancing the\ndataset generation process. PO dataset generation requires two modules: (1)\nresponse evaluation, and (2) response generation. In the response evaluation\nmodule, the responses from Large Language Models (LLMs) are evaluated and\nranked - a task typically carried out by human annotators that we automate\nusing LLMs. We assess the response evaluation module in a 2 step process. In\nstep 1, we assess LLMs as evaluators using three distinct prompting strategies.\nIn step 2, we apply the winning prompting strategy to compare the performance\nof LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that\nGPT-4o-as-a-Judge is more consistent across all datasets. For the response\ngeneration module, we use the identified LLM evaluator configuration and\ncompare different configurations of the LLM Feedback Loop. We use the win rate\nto determine the best multi-agent configuration for generation. Experimenting\nwith various configurations, we find that the LLM Feedback Loop, with Llama as\nthe generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win\nrate over single-agent Llama and Gemma, respectively. After identifying the\nbest configurations for both modules, we generate our PO datasets using the\nabove pipeline.", "published": "2024-08-16 12:01:55", "link": "http://arxiv.org/abs/2408.08688v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Turning Trash into Treasure: Accelerating Inference of Large Language\n  Models with Token Recycling", "abstract": "The rapid growth in the parameters of large language models (LLMs) has made\ninference latency a fundamental bottleneck, limiting broader application of\nLLMs. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm, leveraging the parallel\ncapabilities of modern hardware. Some speculative decoding methods rely on\nadditional structures to guess draft tokens, such as small models or\nparameter-efficient architectures, which need extra training before use.\nAlternatively, retrieval-based train-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nThis approach stores candidate tokens in an adjacency matrix and employs a\nbreadth-first search (BFS)-like algorithm on the matrix to construct a draft\ntree. The tree is then validated through tree attention. New candidate tokens\nfrom the decoding process are then used to update the matrix. Token Recycling\nrequires \\textless2MB of additional storage and achieves approximately 2x\nspeedup across all sizes of LLMs. It significantly outperforms existing\ntrain-free methods by 30\\% and even a training method by 25\\%. It can be\ndirectly applied to any existing LLMs and tasks without the need for\nadaptation.", "published": "2024-08-16 12:20:56", "link": "http://arxiv.org/abs/2408.08696v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation\n  Instructions", "abstract": "LLMs-as-a-judge is a recently popularized method which replaces human\njudgements in task evaluation (Zheng et al. 2024) with automatic evaluation\nusing LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human\nFeedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have\nstrong alignment with human preferences when prompted for a quality judgement,\nsuch as the coherence of a text. While this seems beneficial, it is not clear\nwhether the assessments by an LLM-as-a-judge constitute only an evaluation\nbased on the instructions in the prompts, or reflect its preference for\nhigh-quality data similar to its fine-tune data. To investigate how much\ninfluence prompting the LLMs-as-a-judge has on the alignment of AI judgements\nto human judgements, we analyze prompts with increasing levels of instructions\nabout the target quality of an evaluation, for several LLMs-as-a-judge.\nFurther, we compare to a prompt-free method using model perplexity as a quality\nmeasure instead. We aggregate a taxonomy of quality criteria commonly used\nacross state-of-the-art evaluations with LLMs and provide this as a rigorous\nbenchmark of models as judges. Overall, we show that the LLMs-as-a-judge\nbenefit only little from highly detailed instructions in prompts and that\nperplexity can sometimes align better with human judgements than prompting,\nespecially on textual quality.", "published": "2024-08-16 14:49:35", "link": "http://arxiv.org/abs/2408.08781v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational\n  Dialogue Systems", "abstract": "In this study, we introduce CIKMar, an efficient approach to educational\ndialogue systems powered by the Gemma Language model. By leveraging a\nDual-Encoder ranking system that incorporates both BERT and SBERT model, we\nhave designed CIKMar to deliver highly relevant and accurate responses, even\nwith the constraints of a smaller language model size. Our evaluation reveals\nthat CIKMar achieves a robust recall and F1-score of 0.70 using BERTScore\nmetrics. However, we have identified a significant challenge: the Dual-Encoder\ntends to prioritize theoretical responses over practical ones. These findings\nunderscore the potential of compact and efficient models like Gemma in\ndemocratizing access to advanced educational AI systems, ensuring effective and\ncontextually appropriate responses.", "published": "2024-08-16 15:29:54", "link": "http://arxiv.org/abs/2408.08805v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using\n  Diverse Exemplars", "abstract": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable performance gains in text\ngeneration with Large Language Models (LLMs). However, such techniques depend\non the availability of an accurate answer extraction process to aggregate\nacross multiple outputs. Moreover, they acquire higher inference cost, in\ncomparison to Greedy Decoding, due to generation of relatively higher number of\noutput tokens. Research has shown that the free form text outputs from\nSelf-Consistency can be aggregated reliably using LLMs to produce the final\noutput. Additionally, recent advancements in LLM inference have demonstrated\nthat usage of diverse exemplars in prompts have the ability to induce diversity\nin the LLM outputs. Such proven techniques can be easily extended to\nself-ensembling based approaches to achieve enhanced results in text\ngeneration. In this paper, we introduce PEDAL (Prompts based on Exemplar\nDiversity Aggregated using LLMs), a hybrid self-ensembling approach, that\ncombines the strengths of diverse exemplar based prompts and LLM based\naggregation to achieve improvement in overall performance. On the publicly\navailable SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve\nbetter accuracy than Greedy Decoding based strategies with lower inference cost\ncompared to Self Consistency based approaches.", "published": "2024-08-16 17:54:09", "link": "http://arxiv.org/abs/2408.08869v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trust-Oriented Adaptive Guardrails for Large Language Models", "abstract": "Guardrail, an emerging mechanism designed to ensure that large language\nmodels (LLMs) align with human values by moderating harmful or toxic responses,\nrequires a sociotechnical approach in their design. This paper addresses a\ncritical issue: existing guardrails lack a well-founded methodology to\naccommodate the diverse needs of different user groups, particularly concerning\naccess rights. Supported by trust modeling (primarily on `social' aspect) and\nenhanced with online in-context learning via retrieval-augmented generation (on\n`technical' aspect), we introduce an adaptive guardrail mechanism, to\ndynamically moderate access to sensitive content based on user trust metrics.\nUser trust metrics, defined as a novel combination of direct interaction trust\nand authority-verified trust, enable the system to precisely tailor the\nstrictness of content moderation by aligning with the user's credibility and\nthe specific context of their inquiries. Our empirical evaluation demonstrates\nthe effectiveness of the adaptive guardrail in meeting diverse user needs,\noutperforming existing guardrails while securing sensitive information and\nprecisely managing potentially hazardous content through a context-aware\nknowledge base. To the best of our knowledge, this work is the first to\nintroduce trust-oriented concept into a guardrail system, offering a scalable\nsolution that enriches the discourse on ethical deployment for next-generation\nLLM service.", "published": "2024-08-16 18:07:48", "link": "http://arxiv.org/abs/2408.08959v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "From Lazy to Prolific: Tackling Missing Labels in Open Vocabulary\n  Extreme Classification by Positive-Unlabeled Sequence Learning", "abstract": "Open-vocabulary Extreme Multi-label Classification (OXMC) extends traditional\nXMC by allowing prediction beyond an extremely large, predefined label set\n(typically $10^3$ to $10^{12}$ labels), addressing the dynamic nature of\nreal-world labeling tasks. However, self-selection bias in data annotation\nleads to significant missing labels in both training and test data,\nparticularly for less popular inputs. This creates two critical challenges:\ngeneration models learn to be \"lazy'\" by under-generating labels, and\nevaluation becomes unreliable due to insufficient annotation in the test set.\nIn this work, we introduce Positive-Unlabeled Sequence Learning (PUSL), which\nreframes OXMC as an infinite keyphrase generation task, addressing the\ngeneration model's laziness. Additionally, we propose to adopt a suite of\nevaluation metrics, F1@$\\mathcal{O}$ and newly proposed B@$k$, to reliably\nassess OXMC models with incomplete ground truths. In a highly imbalanced\ne-commerce dataset with substantial missing labels, PUSL generates 30% more\nunique labels, and 72% of its predictions align with actual user queries. On\nthe less skewed EURLex-4.3k dataset, PUSL demonstrates superior F1 scores,\nespecially as label counts increase from 15 to 30. Our approach effectively\ntackles both the modeling and evaluation challenges in OXMC with missing\nlabels.", "published": "2024-08-16 19:10:48", "link": "http://arxiv.org/abs/2408.08981v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Studying the Effects of Collaboration in Interactive Theme Discovery\n  Systems", "abstract": "NLP-assisted solutions have gained considerable traction to support\nqualitative data analysis. However, there does not exist a unified evaluation\nframework that can account for the many different settings in which qualitative\nresearchers may employ them. In this paper, we take a first step in this\ndirection by proposing an evaluation framework to study the way in which\ndifferent tools may result in different outcomes depending on the collaboration\nstrategy employed. Specifically, we study the impact of synchronous vs.\nasynchronous collaboration using two different NLP-assisted qualitative\nresearch tools and present a comprehensive analysis of significant differences\nin the consistency, cohesiveness, and correctness of their outputs.", "published": "2024-08-16 21:57:23", "link": "http://arxiv.org/abs/2408.09030v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Risks and NLP Design: A Case Study on Procedural Document QA", "abstract": "As NLP systems are increasingly deployed at scale, concerns about their\npotential negative impacts have attracted the attention of the research\ncommunity, yet discussions of risk have mostly been at an abstract level and\nfocused on generic AI or NLP applications. We argue that clearer assessments of\nrisks and harms to users--and concrete strategies to mitigate them--will be\npossible when we specialize the analysis to more concrete applications and\ntheir plausible users. As an illustration, this paper is grounded in cooking\nrecipe procedural document question answering (ProcDocQA), where there are\nwell-defined risks to users such as injuries or allergic reactions. Our case\nstudy shows that an existing language model, applied in \"zero-shot\" mode,\nquantitatively answers real-world questions about recipes as well or better\nthan the humans who have answered the questions on the web. Using a novel\nquestionnaire informed by theoretical work on AI risk, we conduct a\nrisk-oriented error analysis that could then inform the design of a future\nsystem to be deployed with lower risk of harm and better performance.", "published": "2024-08-16 17:23:43", "link": "http://arxiv.org/abs/2408.11860v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive\n  Language Models", "abstract": "Recent studies on logical reasoning in Language Models (LMs) have sparked a\ndebate on whether they can learn systematic reasoning principles during\npre-training or merely exploit superficial patterns in the training data. This\npaper presents a mechanistic interpretation of syllogistic reasoning in LMs to\nadvance the understanding of internal dynamics. Specifically, we present a\nmethodology for circuit discovery aimed at interpreting content-independent\nreasoning mechanisms. Through two distinct intervention methods, we uncover a\nsufficient and necessary circuit involving middle-term suppression that\nelucidates how LMs transfer information to derive valid conclusions from\npremises. Furthermore, we investigate how belief biases manifest in syllogistic\nreasoning, finding evidence of partial contamination from additional attention\nheads responsible for encoding commonsense and contextualized knowledge.\nFinally, we explore the generalization of the discovered mechanisms across\nvarious syllogistic schemes, model sizes and architectures, finding that the\nidentified circuit is sufficient and necessary for the schemes on which the\nmodels achieve high downstream accuracy (> 60%), and that the activation\npatterns apply to models of different families. Overall, our findings suggest\nthat LMs indeed learn transferable content-independent reasoning mechanisms,\nbut that, at the same time, such mechanisms do not involve generalizable and\nabstract logical primitives, being susceptible to contamination by the same\nworld knowledge acquired during pre-training.", "published": "2024-08-16 07:47:39", "link": "http://arxiv.org/abs/2408.08590v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Benchmarks of Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in\nboth academia and industry due to their remarkable performance in various\napplications such as visual question answering, visual perception,\nunderstanding, and reasoning. Over the past few years, significant efforts have\nbeen made to examine MLLMs from multiple perspectives. This paper presents a\ncomprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on\n(1)perception and understanding, (2)cognition and reasoning, (3)specific\ndomains, (4)key capabilities, and (5)other modalities. Finally, we discuss the\nlimitations of the current evaluation methods for MLLMs and explore promising\nfuture directions. Our key argument is that evaluation should be regarded as a\ncrucial discipline to support the development of MLLMs better. For more\ndetails, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.", "published": "2024-08-16 09:52:02", "link": "http://arxiv.org/abs/2408.08632v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector", "abstract": "The increasing parameters and expansive dataset of large language models\n(LLMs) highlight the urgent demand for a technical solution to audit the\nunderlying privacy risks and copyright issues associated with LLMs. Existing\nstudies have partially addressed this need through an exploration of the\npre-training data detection problem, which is an instance of a membership\ninference attack (MIA). This problem involves determining whether a given piece\nof text has been used during the pre-training phase of the target LLM. Although\nexisting methods have designed various sophisticated MIA score functions to\nachieve considerable detection performance in pre-trained LLMs, how to achieve\nhigh-confidence detection and how to perform MIA on aligned LLMs remain\nchallenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA\nmethod, which instructs LLMs themselves to serve as a more precise pre-training\ndata detector internally, rather than design an external MIA score function.\nFurthermore, we design two instruction-based safeguards to respectively\nmitigate the privacy risks brought by the existing methods and MIA-Tuner. To\ncomprehensively evaluate the most recent state-of-the-art LLMs, we collect a\nmore up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely\nadopted benchmark WIKIMIA. We conduct extensive experiments across various\naligned and unaligned LLMs over the two benchmark datasets. The results\ndemonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9.", "published": "2024-08-16 11:09:56", "link": "http://arxiv.org/abs/2408.08661v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression", "abstract": "The key to effective point cloud compression is to obtain a robust context\nmodel consistent with complex 3D data structures. Recently, the advancement of\nlarge language models (LLMs) has highlighted their capabilities not only as\npowerful generators for in-context learning and generation but also as\neffective compressors. These dual attributes of LLMs make them particularly\nwell-suited to meet the demands of data compression. Therefore, this paper\nexplores the potential of using LLM for compression tasks, focusing on lossless\npoint cloud geometry compression (PCGC) experiments. However, applying LLM\ndirectly to PCGC tasks presents some significant challenges, i.e., LLM does not\nunderstand the structure of the point cloud well, and it is a difficult task to\nfill the gap between text and point cloud through text description, especially\nfor large complicated and small shapeless point clouds. To address these\nproblems, we introduce a novel architecture, namely the Large Language\nModel-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to\ncompress point cloud geometry information without any text description or\naligning operation. By utilizing different adaptation techniques for\ncross-modality representation alignment and semantic consistency, including\nclustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA),\nthe proposed method can translate LLM to a compressor/generator for point\ncloud. To the best of our knowledge, this is the first structure to employ LLM\nas a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC\noutperforms the other existing methods significantly, by achieving -40.213% bit\nrate reduction compared to the reference software of MPEG Geometry-based Point\nCloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction\ncompared to the state-of-the-art learning-based method.", "published": "2024-08-16 11:55:44", "link": "http://arxiv.org/abs/2408.08682v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Quantifying the Effectiveness of Student Organization Activities using\n  Natural Language Processing", "abstract": "Student extracurricular activities play an important role in enriching the\nstudents' educational experiences. With the increasing popularity of Machine\nLearning and Natural Language Processing, it becomes a logical step that\nincorporating ML-NLP in improving extracurricular activities is a potential\nfocus of study in Artificial Intelligence (AI). This research study aims to\ndevelop a machine learning workflow that will quantify the effectiveness of\nstudent-organized activities based on student emotional responses using\nsentiment analysis. The study uses the Bidirectional Encoder Representations\nfrom Transformers (BERT) Large Language Model (LLM) called via the\npysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data\nset from Organization C, a Recognized Student Organization (RSO) of a higher\neducational institute in the Philippines, College X, was used to develop the\nworkflow. The workflow consisted of data preprocessing, key feature selection,\nLLM feature processing, and score aggregation, resulting in an Event Score for\neach data set. The results show that the BERT LLM can also be used effectively\nin analyzing sentiment beyond product reviews and post comments. For the\nstudent affairs offices of educational institutions, this study can provide a\npractical example of how NLP can be applied to real-world scenarios, showcasing\nthe potential impact of data-driven decision making.", "published": "2024-08-16 12:16:59", "link": "http://arxiv.org/abs/2408.08694v1", "categories": ["cs.CL", "cs.AI", "cs.ET"], "primary_category": "cs.CL"}
{"title": "ConcateNet: Dialogue Separation Using Local And Global Feature\n  Concatenation", "abstract": "Dialogue separation involves isolating a dialogue signal from a mixture, such\nas a movie or a TV program. This can be a necessary step to enable dialogue\nenhancement for broadcast-related applications. In this paper, ConcateNet for\ndialogue separation is proposed, which is based on a novel approach for\nprocessing local and global features aimed at better generalization for\nout-of-domain signals. ConcateNet is trained using a noise reduction-focused,\npublicly available dataset and evaluated using three datasets: two noise\nreduction-focused datasets (in-domain), which show competitive performance for\nConcateNet, and a broadcast-focused dataset (out-of-domain), which verifies the\nbetter generalization performance for the proposed architecture compared to\nconsidered state-of-the-art noise-reduction methods.", "published": "2024-08-16 13:22:55", "link": "http://arxiv.org/abs/2408.08729v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models", "abstract": "This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.", "published": "2024-08-16 17:57:01", "link": "http://arxiv.org/abs/2408.08872v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DePrompt: Desensitization and Evaluation of Personal Identifiable\n  Information in Large Language Model Prompts", "abstract": "Prompt serves as a crucial link in interacting with large language models\n(LLMs), widely impacting the accuracy and interpretability of model outputs.\nHowever, acquiring accurate and high-quality responses necessitates precise\nprompts, which inevitably pose significant risks of personal identifiable\ninformation (PII) leakage. Therefore, this paper proposes DePrompt, a\ndesensitization protection and effectiveness evaluation framework for prompt,\nenabling users to safely and transparently utilize LLMs. Specifically, by\nleveraging large model fine-tuning techniques as the underlying privacy\nprotection method, we integrate contextual attributes to define privacy types,\nachieving high-precision PII entity identification. Additionally, through the\nanalysis of key features in prompt desensitization scenarios, we devise\nadversarial generative desensitization methods that retain important semantic\ncontent while disrupting the link between identifiers and privacy attributes.\nFurthermore, we present utility evaluation metrics for prompt to better gauge\nand balance privacy and usability. Our framework is adaptable to prompts and\ncan be extended to text usability-dependent scenarios. Through comparison with\nbenchmarks and other model methods, experimental evaluations demonstrate that\nour desensitized prompt exhibit superior privacy protection utility and model\ninference results.", "published": "2024-08-16 02:38:25", "link": "http://arxiv.org/abs/2408.08930v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Improving VTE Identification through Language Models from Radiology\n  Reports: A Comparative Study of Mamba, Phi-3 Mini, and BERT", "abstract": "Venous thromboembolism (VTE) is a critical cardiovascular condition,\nencompassing deep vein thrombosis (DVT) and pulmonary embolism (PE). Accurate\nand timely identification of VTE is essential for effective medical care. This\nstudy builds upon our previous work, which addressed VTE detection using deep\nlearning methods for DVT and a hybrid approach combining deep learning and\nrule-based classification for PE. Our earlier approaches, while effective, had\ntwo major limitations: they were complex and required expert involvement for\nfeature engineering of the rule set. To overcome these challenges, we utilize\nthe Mamba architecture-based classifier. This model achieves remarkable\nresults, with a 97\\% accuracy and F1 score on the DVT dataset and a 98\\%\naccuracy and F1 score on the PE dataset. In contrast to the previous hybrid\nmethod on PE identification, the Mamba classifier eliminates the need for\nhand-engineered rules, significantly reducing model complexity while\nmaintaining comparable performance. Additionally, we evaluated a lightweight\nLarge Language Model (LLM), Phi-3 Mini, in detecting VTE. While this model\ndelivers competitive results, outperforming the baseline BERT models, it proves\nto be computationally intensive due to its larger parameter set. Our evaluation\nshows that the Mamba-based model demonstrates superior performance and\nefficiency in VTE identification, offering an effective solution to the\nlimitations of previous approaches.", "published": "2024-08-16 22:51:56", "link": "http://arxiv.org/abs/2408.09043v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "When Prompting Fails to Sway: Inertia in Moral and Value Judgments of\n  Large Language Models", "abstract": "Large Language Models (LLMs) exhibit non-deterministic behavior, and\nprompting has emerged as a primary method for steering their outputs toward\ndesired directions. One popular strategy involves assigning a specific\n\"persona\" to the model to induce more varied and context-sensitive responses,\nakin to the diversity found in human perspectives. However, contrary to the\nexpectation that persona-based prompting would yield a wide range of opinions,\nour experiments demonstrate that LLMs maintain consistent value orientations.\nIn particular, we observe a persistent inertia in their responses, where\ncertain moral and value dimensions, especially harm avoidance and fairness,\nremain distinctly skewed in one direction despite varied persona settings. To\ninvestigate this phenomenon systematically, use role-play at scale, which\ncombines randomized, diverse persona prompts with a macroscopic trend analysis\nof model outputs. Our findings highlight the strong internal biases and value\npreferences in LLMs, underscoring the need for careful scrutiny and potential\nadjustment of these models to ensure balanced and equitable applications.", "published": "2024-08-16 23:24:10", "link": "http://arxiv.org/abs/2408.09049v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Learning to Route for Dynamic Adapter Composition in Continual Learning\n  with Language Models", "abstract": "Parameter-efficient fine-tuning (PEFT) methods are increasingly used with\npre-trained language models (PLMs) for continual learning (CL). These methods\ntypically involve training a PEFT module for each new task and employing\nsimilarity-based selection to route modules during inference. However, they\nface two major limitations: 1) interference during module training with already\nlearned modules and 2) suboptimal routing when composing modules. In this\npaper, we present L2R, a method that isolates the training of new PEFT modules\nto ensure their task specialization. L2R then learns to compose the learned\nmodules by training a network of routers that leverages a small memory\ncontaining examples of previously seen tasks. We evaluate our method in two CL\nsetups using various benchmarks. Our results demonstrate that L2R provides an\neffective composition of PEFT modules, leading to improved generalization and\nperformance compared to other methods.", "published": "2024-08-16 23:57:29", "link": "http://arxiv.org/abs/2408.09053v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SEAL: Systematic Error Analysis for Value ALignment", "abstract": "Reinforcement Learning from Human Feedback (RLHF) aims to align language\nmodels (LMs) with human values by training reward models (RMs) on binary\npreferences and using these RMs to fine-tune the base LMs. Despite its\nimportance, the internal mechanisms of RLHF remain poorly understood. This\npaper introduces new metrics to evaluate the effectiveness of modeling and\naligning human values, namely feature imprint, alignment resistance and\nalignment robustness. We categorize alignment datasets into target features\n(desired values) and spoiler features (undesired concepts). By regressing RM\nscores against these features, we quantify the extent to which RMs reward them\n- a metric we term feature imprint. We define alignment resistance as the\nproportion of the preference dataset where RMs fail to match human preferences,\nand we assess alignment robustness by analyzing RM responses to perturbed\ninputs. Our experiments, utilizing open-source components like the\nAnthropic/hh-rlhf preference dataset and OpenAssistant RMs, reveal significant\nimprints of target features and a notable sensitivity to spoiler features. We\nobserved a 26% incidence of alignment resistance in portions of the dataset\nwhere LM-labelers disagreed with human preferences. Furthermore, we find that\nmisalignment often arises from ambiguous entries within the alignment dataset.\nThese findings underscore the importance of scrutinizing both RMs and alignment\ndatasets for a deeper understanding of value alignment.", "published": "2024-08-16 18:48:30", "link": "http://arxiv.org/abs/2408.10270v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Convexity-based Pruning of Speech Representation Models", "abstract": "Speech representation models based on the transformer architecture and\ntrained by self-supervised learning have shown great promise for solving tasks\nsuch as speech and speaker recognition, keyword spotting, emotion detection,\nand more. Typically, it is found that larger models lead to better performance.\nHowever, the significant computational effort involved in such large\ntransformer systems is a challenge for embedded and real-world applications.\nRecent work has shown that there is significant redundancy in the transformer\nmodels for NLP and massive layer pruning is feasible (Sajjad et al., 2023).\nHere, we investigate layer pruning in audio models. We base the pruning\ndecision on a convexity criterion. Convexity of classification regions has\nrecently been proposed as an indicator of subsequent fine-tuning performance in\na range of application domains, including NLP and audio. In empirical\ninvestigations, we find a massive reduction in the computational effort with no\nloss of performance or even improvements in certain cases.", "published": "2024-08-16 09:04:54", "link": "http://arxiv.org/abs/2408.11858v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speaking the Same Language: Leveraging LLMs in Standardizing Clinical\n  Data for AI", "abstract": "The implementation of Artificial Intelligence (AI) in the healthcare industry\nhas garnered considerable attention, attributable to its prospective\nenhancement of clinical outcomes, expansion of access to superior healthcare,\ncost reduction, and elevation of patient satisfaction. Nevertheless, the\nprimary hurdle that persists is related to the quality of accessible\nmulti-modal healthcare data in conjunction with the evolution of AI\nmethodologies. This study delves into the adoption of large language models to\naddress specific challenges, specifically, the standardization of healthcare\ndata. We advocate the use of these models to identify and map clinical data\nschemas to established data standard attributes, such as the Fast Healthcare\nInteroperability Resources. Our results illustrate that employing large\nlanguage models significantly diminishes the necessity for manual data curation\nand elevates the efficacy of the data standardization process. Consequently,\nthe proposed methodology has the propensity to expedite the integration of AI\nin healthcare, ameliorate the quality of patient care, whilst minimizing the\ntime and financial resources necessary for the preparation of data for AI.", "published": "2024-08-16 20:51:21", "link": "http://arxiv.org/abs/2408.11861v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Uncertainty Quantification for Generative AI", "abstract": "This work is concerned with conformal prediction in contemporary applications\n(including generative AI) where a black-box model has been trained on data that\nare not accessible to the user. Mirroring split-conformal inference, we design\na wrapper around a black-box algorithm which calibrates conformity scores. This\ncalibration is local and proceeds in two stages by first adaptively\npartitioning the predictor space into groups and then calibrating sectionally\ngroup by group. Adaptive partitioning (self-grouping) is achieved by fitting a\nrobust regression tree to the conformity scores on the calibration set. This\nnew tree variant is designed in such a way that adding a single new observation\ndoes not change the tree fit with overwhelmingly large probability. This\nadd-one-in robustness property allows us to conclude a finite sample\ngroup-conditional coverage guarantee, a refinement of the marginal guarantee.\nIn addition, unlike traditional split-conformal inference, adaptive splitting\nand within-group calibration yields adaptive bands which can stretch and shrink\nlocally. We demonstrate benefits of local tightening on several simulated as\nwell as real examples using non-parametric regression. Finally, we consider two\ncontemporary classification applications for obtaining uncertainty\nquantification around GPT-4o predictions. We conformalize skin disease\ndiagnoses based on self-reported symptoms as well as predicted states of U.S.\nlegislators based on summaries of their ideology. We demonstrate substantial\nlocal tightening of the uncertainty sets while attaining similar marginal\ncoverage.", "published": "2024-08-16 19:37:33", "link": "http://arxiv.org/abs/2408.08990v1", "categories": ["stat.ME", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "GAPS: A Large and Diverse Classical Guitar Dataset and Benchmark\n  Transcription Model", "abstract": "We introduce GAPS (Guitar-Aligned Performance Scores), a new dataset of\nclassical guitar performances, and a benchmark guitar transcription model that\nachieves state-of-the-art performance on GuitarSet in both supervised and\nzero-shot settings. GAPS is the largest dataset of real guitar audio,\ncontaining 14 hours of freely available audio-score aligned pairs, recorded in\ndiverse conditions by over 200 performers, together with high-resolution\nnote-level MIDI alignments and performance videos. These enable us to train a\nstate-of-the-art model for automatic transcription of solo guitar recordings\nwhich can generalise well to real world audio that is unseen during training.", "published": "2024-08-16 10:40:49", "link": "http://arxiv.org/abs/2408.08653v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HSDreport: Heart Sound Diagnosis with Echocardiography Reports", "abstract": "Heart sound auscultation holds significant importance in the diagnosis of\ncongenital heart disease. However, existing methods for Heart Sound Diagnosis\n(HSD) tasks are predominantly limited to a few fixed categories, framing the\nHSD task as a rigid classification problem that does not fully align with\nmedical practice and offers only limited information to physicians. Besides,\nsuch methods do not utilize echocardiography reports, the gold standard in the\ndiagnosis of related diseases. To tackle this challenge, we introduce\nHSDreport, a new benchmark for HSD, which mandates the direct utilization of\nheart sounds obtained from auscultation to predict echocardiography reports.\nThis benchmark aims to merge the convenience of auscultation with the\ncomprehensive nature of echocardiography reports. First, we collect a new\ndataset for this benchmark, comprising 2,275 heart sound samples along with\ntheir corresponding reports. Subsequently, we develop a knowledge-aware\nquery-based transformer to handle this task. The intent is to leverage the\ncapabilities of medically pre-trained models and the internal knowledge of\nlarge language models (LLMs) to address the task's inherent complexity and\nvariability, thereby enhancing the robustness and scientific validity of the\nmethod. Furthermore, our experimental results indicate that our method\nsignificantly outperforms traditional HSD approaches and existing multimodal\nLLMs in detecting key abnormalities in heart sounds.", "published": "2024-08-16 11:26:39", "link": "http://arxiv.org/abs/2408.08669v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MAT-SED: A Masked Audio Transformer with Masked-Reconstruction Based\n  Pre-training for Sound Event Detection", "abstract": "Sound event detection (SED) methods that leverage a large pre-trained\nTransformer encoder network have shown promising performance in recent DCASE\nchallenges. However, they still rely on an RNN-based context network to model\ntemporal dependencies, largely due to the scarcity of labeled data. In this\nwork, we propose a pure Transformer-based SED model with masked-reconstruction\nbased pre-training, termed MAT-SED. Specifically, a Transformer with relative\npositional encoding is first designed as the context network, pre-trained by\nthe masked-reconstruction task on all available target data in a\nself-supervised way. Both the encoder and the context network are jointly\nfine-tuned in a semi-supervised manner. Furthermore, a global-local feature\nfusion strategy is proposed to enhance the localization capability. Evaluation\nof MAT-SED on DCASE2023 task4 surpasses state-of-the-art performance, achieving\n0.587/0.896 PSDS1/PSDS2 respectively.", "published": "2024-08-16 11:33:16", "link": "http://arxiv.org/abs/2408.08673v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASVspoof 5: Crowdsourced Speech Data, Deepfakes, and Adversarial Attacks\n  at Scale", "abstract": "ASVspoof 5 is the fifth edition in a series of challenges that promote the\nstudy of speech spoofing and deepfake attacks, and the design of detection\nsolutions. Compared to previous challenges, the ASVspoof 5 database is built\nfrom crowdsourced data collected from a vastly greater number of speakers in\ndiverse acoustic conditions. Attacks, also crowdsourced, are generated and\ntested using surrogate detection models, while adversarial attacks are\nincorporated for the first time. New metrics support the evaluation of\nspoofing-robust automatic speaker verification (SASV) as well as stand-alone\ndetection solutions, i.e., countermeasures without ASV. We describe the two\nchallenge tracks, the new database, the evaluation metrics, baselines, and the\nevaluation platform, and present a summary of the results. Attacks\nsignificantly compromise the baseline systems, while submissions bring\nsubstantial improvements.", "published": "2024-08-16 13:37:20", "link": "http://arxiv.org/abs/2408.08739v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Autoregressive Audio Modeling via Next-Scale Prediction", "abstract": "Audio generation has achieved remarkable progress with the advance of\nsophisticated generative models, such as diffusion models (DMs) and\nautoregressive (AR) models. However, due to the naturally significant sequence\nlength of audio, the efficiency of audio generation remains an essential issue\nto be addressed, especially for AR models that are incorporated in large\nlanguage models (LLMs). In this paper, we analyze the token length of audio\ntokenization and propose a novel \\textbf{S}cale-level \\textbf{A}udio\n\\textbf{T}okenizer (SAT), with improved residual quantization. Based on SAT, a\nscale-level \\textbf{A}coustic \\textbf{A}uto\\textbf{R}egressive (AAR) modeling\nframework is further proposed, which shifts the next-token AR prediction to\nnext-scale AR prediction, significantly reducing the training cost and\ninference time. To validate the effectiveness of the proposed approach, we\ncomprehensively analyze design choices and demonstrate the proposed AAR\nframework achieves a remarkable \\textbf{35}$\\times$ faster inference speed and\n+\\textbf{1.33} Fr\\'echet Audio Distance (FAD) against baselines on the AudioSet\nbenchmark. Code: \\url{https://github.com/qiuk2/AAR}.", "published": "2024-08-16 21:48:53", "link": "http://arxiv.org/abs/2408.09027v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
