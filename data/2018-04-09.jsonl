{"title": "A GPU-based WFST Decoder with Exact Lattice Generation", "abstract": "We describe initial work on an extension of the Kaldi toolkit that supports\nweighted finite-state transducer (WFST) decoding on Graphics Processing Units\n(GPUs). We implement token recombination as an atomic GPU operation in order to\nfully parallelize the Viterbi beam search, and propose a dynamic load balancing\nstrategy for more efficient token passing scheduling among GPU threads. We also\nredesign the exact lattice generation and lattice pruning algorithms for better\nutilization of the GPUs. Experiments on the Switchboard corpus show that the\nproposed method achieves identical 1-best results and lattice quality in\nrecognition and confidence measure tasks, while running 3 to 15 times faster\nthan the single process Kaldi decoder. The above results are reported on\ndifferent GPU architectures. Additionally we obtain a 46-fold speedup with\nsequence parallelism and multi-process service (MPS) in GPU.", "published": "2018-04-09 21:23:26", "link": "http://arxiv.org/abs/1804.03243v3", "categories": ["cs.CL", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Efficient Graph-based Word Sense Induction by Distributional Inclusion\n  Vector Embeddings", "abstract": "Word sense induction (WSI), which addresses polysemy by unsupervised\ndiscovery of multiple word senses, resolves ambiguities for downstream NLP\ntasks and also makes word representations more interpretable. This paper\nproposes an accurate and efficient graph-based method for WSI that builds a\nglobal non-negative vector embedding basis (which are interpretable like\ntopics) and clusters the basis indexes in the ego network of each polysemous\nword. By adopting distributional inclusion vector embeddings as our basis\nformation model, we avoid the expensive step of nearest neighbor search that\nplagues other graph-based methods without sacrificing the quality of sense\nclusters. Experiments on three datasets show that our proposed method produces\nsimilar or better sense clusters and embeddings compared with previous\nstate-of-the-art methods while being significantly more efficient.", "published": "2018-04-09 22:10:57", "link": "http://arxiv.org/abs/1804.03257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Intra-User and Inter-User Representation Learning for\n  Automated Hate Speech Detection", "abstract": "Hate speech detection is a critical, yet challenging problem in Natural\nLanguage Processing (NLP). Despite the existence of numerous studies dedicated\nto the development of NLP hate speech detection approaches, the accuracy is\nstill poor. The central problem is that social media posts are short and noisy,\nand most existing hate speech detection solutions take each post as an isolated\ninput instance, which is likely to yield high false positive and negative\nrates. In this paper, we radically improve automated hate speech detection by\npresenting a novel model that leverages intra-user and inter-user\nrepresentation learning for robust hate speech detection on Twitter. In\naddition to the target Tweet, we collect and analyze the user's historical\nposts to model intra-user Tweet representations. To suppress the noise in a\nsingle Tweet, we also model the similar Tweets posted by all other users with\nreinforced inter-user representation learning techniques. Experimentally, we\nshow that leveraging these two representations can significantly improve the\nf-score of a strong bidirectional LSTM baseline model by 10.1%.", "published": "2018-04-09 17:46:33", "link": "http://arxiv.org/abs/1804.03124v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "abstract": "Describes an audio dataset of spoken words designed to help train and\nevaluate keyword spotting systems. Discusses why this task is an interesting\nchallenge, and why it requires a specialized dataset that is different from\nconventional datasets used for automatic speech recognition of full sentences.\nSuggests a methodology for reproducible and comparable accuracy metrics for\nthis task. Describes how the data was collected and verified, what it contains,\nprevious versions and properties. Concludes by reporting baseline results of\nmodels trained on this dataset.", "published": "2018-04-09 19:58:17", "link": "http://arxiv.org/abs/1804.03209v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Multi-target Voice Conversion without Parallel Data by Adversarially\n  Learning Disentangled Audio Representations", "abstract": "Recently, cycle-consistent adversarial network (Cycle-GAN) has been\nsuccessfully applied to voice conversion to a different speaker without\nparallel data, although in those approaches an individual model is needed for\neach target speaker. In this paper, we propose an adversarial learning\nframework for voice conversion, with which a single model can be trained to\nconvert the voice to many different speakers, all without parallel data, by\nseparating the speaker characteristics from the linguistic content in speech\nsignals. An autoencoder is first trained to extract speaker-independent latent\nrepresentations and speaker embedding separately using another auxiliary\nspeaker classifier to regularize the latent representation. The decoder then\ntakes the speaker-independent latent representation and the target speaker\nembedding as the input to generate the voice of the target speaker with the\nlinguistic content of the source utterance. The quality of decoder output is\nfurther improved by patching with the residual signal produced by another pair\nof generator and discriminator. A target speaker set size of 20 was tested in\nthe preliminary experiments, and very good voice quality was obtained.\nConventional voice conversion metrics are reported. We also show that the\nspeaker information has been properly reduced from the latent representations.", "published": "2018-04-09 04:31:43", "link": "http://arxiv.org/abs/1804.02812v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Vision as an Interlingua: Learning Multilingual Semantic Embeddings of\n  Untranscribed Speech", "abstract": "In this paper, we explore the learning of neural network embeddings for\nnatural images and speech waveforms describing the content of those images.\nThese embeddings are learned directly from the waveforms without the use of\nlinguistic transcriptions or conventional speech recognition technology. While\nprior work has investigated this setting in the monolingual case using English\nspeech data, this work represents the first effort to apply these techniques to\nlanguages beyond English. Using spoken captions collected in English and Hindi,\nwe show that the same model architecture can be successfully applied to both\nlanguages. Further, we demonstrate that training a multilingual model\nsimultaneously on both languages offers improved performance over the\nmonolingual models. Finally, we show that these models are capable of\nperforming semantic cross-lingual speech-to-speech retrieval.", "published": "2018-04-09 15:15:37", "link": "http://arxiv.org/abs/1804.03052v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Scalable Factorized Hierarchical Variational Autoencoder Training", "abstract": "Deep generative models have achieved great success in unsupervised learning\nwith the ability to capture complex nonlinear relationships between latent\ngenerating factors and observations. Among them, a factorized hierarchical\nvariational autoencoder (FHVAE) is a variational inference-based model that\nformulates a hierarchical generative process for sequential data. Specifically,\nan FHVAE model can learn disentangled and interpretable representations, which\nhave been proven useful for numerous speech applications, such as speaker\nverification, robust speech recognition, and voice conversion. However, as we\nwill elaborate in this paper, the training algorithm proposed in the original\npaper is not scalable to datasets of thousands of hours, which makes this model\nless applicable on a larger scale. After identifying limitations in terms of\nruntime, memory, and hyperparameter optimization, we propose a hierarchical\nsampling training algorithm to address all three issues. Our proposed method is\nevaluated comprehensively on a wide variety of datasets, ranging from 3 to\n1,000 hours and involving different types of generating factors, such as\nrecording conditions and noise types. In addition, we also present a new\nvisualization method for qualitatively evaluating the performance with respect\nto the interpretability and disentanglement. Models trained with our proposed\nalgorithm demonstrate the desired characteristics on all the datasets.", "published": "2018-04-09 19:44:29", "link": "http://arxiv.org/abs/1804.03201v2", "categories": ["stat.ML", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Polyphonic Pitch Tracking with Deep Layered Learning", "abstract": "This paper presents a polyphonic pitch tracking system able to extract both\nframewise and note-based estimates from audio. The system uses several\nartificial neural networks in a deep layered learning setup. First, cascading\nnetworks are applied to a spectrogram for framewise fundamental frequency (f0)\nestimation. A sparse receptive field is learned by the first network and then\nused as a filter kernel for parameter sharing throughout the system. The f0\nactivations are connected across time to extract pitch contours. These contours\ndefine a framework within which subsequent networks perform onset and offset\ndetection, operating across both time and smaller pitch fluctuations at the\nsame time. As input, the networks use, e.g., variations of latent\nrepresentations from the f0 estimation network. Finally, incorrect tentative\nnotes are removed one by one in an iterative procedure that allows a network to\nclassify notes within an accurate context. The system was evaluated on four\npublic test sets: MAPS, Bach10, TRIOS, and the MIREX Woodwind quintet, and\nperformed state-of-the-art results for all four datasets. It performs well\nacross all subtasks: f0, pitched onset, and pitched offset tracking.", "published": "2018-04-09 11:27:09", "link": "http://arxiv.org/abs/1804.02918v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Sound of Pixels", "abstract": "We introduce PixelPlayer, a system that, by leveraging large amounts of\nunlabeled videos, learns to locate image regions which produce sounds and\nseparate the input sounds into a set of components that represents the sound\nfrom each pixel. Our approach capitalizes on the natural synchronization of the\nvisual and audio modalities to learn models that jointly parse sounds and\nimages, without requiring additional manual supervision. Experimental results\non a newly collected MUSIC dataset show that our proposed Mix-and-Separate\nframework outperforms several baselines on source separation. Qualitative\nresults suggest our model learns to ground sounds in vision, enabling\napplications such as independently adjusting the volume of sound sources.", "published": "2018-04-09 18:00:36", "link": "http://arxiv.org/abs/1804.03160v4", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
