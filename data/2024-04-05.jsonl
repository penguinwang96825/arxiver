{"title": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data", "abstract": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in their\npre-training data. We propose Quote-Tuning, which demonstrates the feasibility\nof aligning models to quote. The core of Quote-Tuning is a fast membership\ninference function that efficiently verifies text against trusted corpora. We\nleverage this tool to design a reward function to quantify quotes in model\nresponses, and curate datasets for preference learning. Experiments show that\nQuote-Tuning significantly increases verbatim quotes from high-quality\ndocuments by up to 130% relative to base models while maintaining response\nquality. Quote-Tuning is applicable in different tasks, generalizes to\nout-of-domain data and diverse model families, and provides additional benefits\nto truthfulness. Our method not only serves as a hassle-free method to increase\nquoting but also opens up avenues for improving LLM trustworthiness through\nbetter verifiability.", "published": "2024-04-05 02:27:09", "link": "http://arxiv.org/abs/2404.03862v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bi-consolidating Model for Joint Relational Triple Extraction", "abstract": "Current methods to extract relational triples directly make a prediction\nbased on a possible entity pair in a raw sentence without depending on entity\nrecognition. The task suffers from a serious semantic overlapping problem, in\nwhich several relation triples may share one or two entities in a sentence. In\nthis paper, based on a two-dimensional sentence representation, a\nbi-consolidating model is proposed to address this problem by simultaneously\nreinforcing the local and global semantic features relevant to a relation\ntriple. This model consists of a local consolidation component and a global\nconsolidation component. The first component uses a pixel difference\nconvolution to enhance semantic information of a possible triple representation\nfrom adjacent regions and mitigate noise in neighbouring neighbours. The second\ncomponent strengthens the triple representation based a channel attention and a\nspatial attention, which has the advantage to learn remote semantic\ndependencies in a sentence. They are helpful to improve the performance of both\nentity identification and relation type classification in relation triple\nextraction. After evaluated on several publish datasets, the bi-consolidating\nmodel achieves competitive performance. Analytical experiments demonstrate the\neffectiveness of our model for relational triple extraction and give motivation\nfor other natural language processing tasks.", "published": "2024-04-05 04:04:23", "link": "http://arxiv.org/abs/2404.03881v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Techniques for Enhancing Sentence Embeddings in Generative\n  Language Models", "abstract": "Sentence Embedding stands as a fundamental task within the realm of Natural\nLanguage Processing, finding extensive application in search engines, expert\nsystems, and question-and-answer platforms. With the continuous evolution of\nlarge language models such as LLaMA and Mistral, research on sentence embedding\nhas recently achieved notable breakthroughs. However, these advancements mainly\npertain to fine-tuning scenarios, leaving explorations into computationally\nefficient direct inference methods for sentence representation in a nascent\nstage. This paper endeavors to bridge this research gap. Through comprehensive\nexperimentation, we challenge the widely held belief in the necessity of an\nExplicit One-word Limitation for deriving sentence embeddings from Pre-trained\nLanguage Models (PLMs). We demonstrate that this approach, while beneficial for\ngenerative models under direct inference scenario, is not imperative for\ndiscriminative models or the fine-tuning of generative PLMs. This discovery\nsheds new light on the design of manual templates in future studies. Building\nupon this insight, we propose two innovative prompt engineering techniques\ncapable of further enhancing the expressive power of PLMs' raw embeddings:\nPretended Chain of Thought and Knowledge Enhancement. We confirm their\neffectiveness across various PLM types and provide a detailed exploration of\nthe underlying factors contributing to their success.", "published": "2024-04-05 07:07:15", "link": "http://arxiv.org/abs/2404.03921v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation with In-Context Learning and Comparative Evaluation in\n  Math Word Problem Solving", "abstract": "Math Word Problem (MWP) solving presents a challenging task in Natural\nLanguage Processing (NLP). This study aims to provide MWP solvers with a more\ndiverse training set, ultimately improving their ability to solve various math\nproblems. We propose several methods for data augmentation by modifying the\nproblem texts and equations, such as synonym replacement, rule-based: question\nreplacement, and rule based: reversing question methodologies over two English\nMWP datasets. This study extends by introducing a new in-context learning\naugmentation method, employing the Llama-7b language model. This approach\ninvolves instruction-based prompting for rephrasing the math problem texts.\nPerformance evaluations are conducted on 9 baseline models, revealing that\naugmentation methods outperform baseline models. Moreover, concatenating\nexamples generated by various augmentation methods further improves\nperformance.", "published": "2024-04-05 07:57:03", "link": "http://arxiv.org/abs/2404.03938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language\n  Models on Natural Language Inference for Clinical Trials", "abstract": "This paper describes our submission to Task 2 of SemEval-2024: Safe\nBiomedical Natural Language Inference for Clinical Trials. The Multi-evidence\nNatural Language Inference for Clinical Trial Data (NLI4CT) consists of a\nTextual Entailment (TE) task focused on the evaluation of the consistency and\nfaithfulness of Natural Language Inference (NLI) models applied to Clinical\nTrial Reports (CTR). We test 2 distinct approaches, one based on finetuning and\nensembling Masked Language Models and the other based on prompting Large\nLanguage Models using templates, in particular, using Chain-Of-Thought and\nContrastive Chain-Of-Thought. Prompting Flan-T5-large in a 2-shot setting leads\nto our best system that achieves 0.57 F1 score, 0.64 Faithfulness, and 0.56\nConsistency.", "published": "2024-04-05 09:18:50", "link": "http://arxiv.org/abs/2404.03977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Robustness of Modelling Decisions for Few-Shot\n  Cross-Topic Stance Detection: A Preregistered Study", "abstract": "For a viewpoint-diverse news recommender, identifying whether two news\narticles express the same viewpoint is essential. One way to determine \"same or\ndifferent\" viewpoint is stance detection. In this paper, we investigate the\nrobustness of operationalization choices for few-shot stance detection, with\nspecial attention to modelling stance across different topics. Our experiments\ntest pre-registered hypotheses on stance detection. Specifically, we compare\ntwo stance task definitions (Pro/Con versus Same Side Stance), two LLM\narchitectures (bi-encoding versus cross-encoding), and adding Natural Language\nInference knowledge, with pre-trained RoBERTa models trained with shots of 100\nexamples from 7 different stance detection datasets. Some of our hypotheses and\nclaims from earlier work can be confirmed, while others give more inconsistent\nresults. The effect of the Same Side Stance definition on performance differs\nper dataset and is influenced by other modelling choices. We found no\nrelationship between the number of training topics in the training shots and\nperformance. In general, cross-encoding out-performs bi-encoding, and adding\nNLI training to our models gives considerable improvement, but these results\nare not consistent across all datasets. Our results indicate that it is\nessential to include multiple datasets and systematic modelling experiments\nwhen aiming to find robust modelling choices for the concept `stance'.", "published": "2024-04-05 09:48:00", "link": "http://arxiv.org/abs/2404.03987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BuDDIE: A Business Document Dataset for Multi-task Information\n  Extraction", "abstract": "The field of visually rich document understanding (VRDU) aims to solve a\nmultitude of well-researched NLP tasks in a multi-modal domain. Several\ndatasets exist for research on specific tasks of VRDU such as document\nclassification (DC), key entity extraction (KEE), entity linking, visual\nquestion answering (VQA), inter alia. These datasets cover documents like\ninvoices and receipts with sparse annotations such that they support one or two\nco-related tasks (e.g., entity extraction and entity linking). Unfortunately,\nonly focusing on a single specific of documents or task is not representative\nof how documents often need to be processed in the wild - where variety in\nstyle and requirements is expected. In this paper, we introduce BuDDIE\n(Business Document Dataset for Information Extraction), the first multi-task\ndataset of 1,665 real-world business documents that contains rich and dense\nannotations for DC, KEE, and VQA. Our dataset consists of publicly available\nbusiness entity documents from US state government websites. The documents are\nstructured and vary in their style and layout across states and types (e.g.,\nforms, certificates, reports, etc.). We provide data variety and quality\nmetrics for BuDDIE as well as a series of baselines for each task. Our\nbaselines cover traditional textual, multi-modal, and large language model\napproaches to VRDU.", "published": "2024-04-05 10:26:42", "link": "http://arxiv.org/abs/2404.04003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Good Books are Complex Matters: Gauging Complexity Profiles Across\n  Diverse Categories of Perceived Literary Quality", "abstract": "In this study, we employ a classification approach to show that different\ncategories of literary \"quality\" display unique linguistic profiles, leveraging\na corpus that encompasses titles from the Norton Anthology, Penguin Classics\nseries, and the Open Syllabus project, contrasted against contemporary\nbestsellers, Nobel prize winners and recipients of prestigious literary awards.\nOur analysis reveals that canonical and so called high-brow texts exhibit\ndistinct textual features when compared to other quality categories such as\nbestsellers and popular titles as well as to control groups, likely responding\nto distinct (but not mutually exclusive) models of quality. We apply a classic\nmachine learning approach, namely Random Forest, to distinguish quality novels\nfrom \"control groups\", achieving up to 77\\% F1 scores in differentiating\nbetween the categories. We find that quality category tend to be easier to\ndistinguish from control groups than from other quality categories, suggesting\nthan literary quality features might be distinguishable but shared through\nquality proxies.", "published": "2024-04-05 11:06:07", "link": "http://arxiv.org/abs/2404.04022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset for Physical and Abstract Plausibility and Sources of Human\n  Disagreement", "abstract": "We present a novel dataset for physical and abstract plausibility of events\nin English. Based on naturally occurring sentences extracted from Wikipedia, we\ninfiltrate degrees of abstractness, and automatically generate perturbed\npseudo-implausible events. We annotate a filtered and balanced subset for\nplausibility using crowd-sourcing, and perform extensive cleansing to ensure\nannotation quality. In-depth quantitative analyses indicate that annotators\nfavor plausibility over implausibility and disagree more on implausible events.\nFurthermore, our plausibility dataset is the first to capture abstractness in\nevents to the same extent as concreteness, and we find that event abstractness\nhas an impact on plausibility ratings: more concrete event participants trigger\na perception of implausibility.", "published": "2024-04-05 11:37:40", "link": "http://arxiv.org/abs/2404.04035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer", "abstract": "This paper explores cost-efficient methods to adapt pretrained Large Language\nModels (LLMs) to new lower-resource languages, with a specific focus on\nEstonian. Leveraging the Llama 2 model, we investigate the impact of combining\ncross-lingual instruction-tuning with additional monolingual pretraining. Our\nresults demonstrate that even a relatively small amount of additional\nmonolingual pretraining followed by cross-lingual instruction-tuning\nsignificantly enhances results on Estonian. Furthermore, we showcase\ncross-lingual knowledge transfer from high-quality English instructions to\nEstonian, resulting in improvements in commonsense reasoning and multi-turn\nconversation capabilities. Our best model, named \\textsc{Llammas}, represents\nthe first open-source instruction-following LLM for Estonian. Additionally, we\npublish Alpaca-est, the first general task instruction dataset for Estonia.\nThese contributions mark the initial progress in the direction of developing\nopen-source LLMs for Estonian.", "published": "2024-04-05 11:52:02", "link": "http://arxiv.org/abs/2404.04042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the quality of information extraction", "abstract": "Advances in large language models have notably enhanced the efficiency of\ninformation extraction from unstructured and semi-structured data sources. As\nthese technologies become integral to various applications, establishing an\nobjective measure for the quality of information extraction becomes imperative.\nHowever, the scarcity of labeled data presents significant challenges to this\nendeavor. In this paper, we introduce an automatic framework to assess the\nquality of the information extraction/retrieval and its completeness. The\nframework focuses on information extraction in the form of entity and its\nproperties. We discuss how to handle the input/output size limitations of the\nlarge language models and analyze their performance when extracting the\ninformation. In particular, we introduce scores to evaluate the quality of the\nextraction and provide an extensive discussion on how to interpret them.", "published": "2024-04-05 12:51:48", "link": "http://arxiv.org/abs/2404.04068v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Factual Accuracy of Neural Table-to-Text Output by Addressing\n  Input Problems in ToTTo", "abstract": "Neural Table-to-Text models tend to hallucinate, producing texts that contain\nfactual errors. We investigate whether such errors in the output can be traced\nback to problems with the input. We manually annotated 1,837 texts generated by\nmultiple models in the politics domain of the ToTTo dataset. We identify the\ninput problems that are responsible for many output errors and show that fixing\nthese inputs reduces factual errors by between 52% and 76% (depending on the\nmodel). In addition, we observe that models struggle in processing tabular\ninputs that are structured in a non-standard way, particularly when the input\nlacks distinct row and column values or when the column headers are not\ncorrectly mapped to corresponding values.", "published": "2024-04-05 13:59:12", "link": "http://arxiv.org/abs/2404.04103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal\n  and Masked Language Models", "abstract": "Knowledge probing assesses to which degree a language model (LM) has\nsuccessfully learned relational knowledge during pre-training. Probing is an\ninexpensive way to compare LMs of different sizes and training configurations.\nHowever, previous approaches rely on the objective function used in\npre-training LMs and are thus applicable only to masked or causal LMs. As a\nresult, comparing different types of LMs becomes impossible. To address this,\nwe propose an approach that uses an LM's inherent ability to estimate the\nlog-likelihood of any given textual statement. We carefully design an\nevaluation dataset of 7,731 instances (40,916 in a larger variant) from which\nwe produce alternative statements for each relational fact, one of which is\ncorrect. We then evaluate whether an LM correctly assigns the highest\nlog-likelihood to the correct statement. Our experimental evaluation of 22\ncommon LMs shows that our proposed framework, BEAR, can effectively probe for\nknowledge across different LM types. We release the BEAR datasets and an\nopen-source framework that implements the probing approach to the research\ncommunity to facilitate the evaluation and development of LMs.", "published": "2024-04-05 14:13:55", "link": "http://arxiv.org/abs/2404.04113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language\n  Translation", "abstract": "Parameter-efficient fine-tuning (PEFT) methods are increasingly vital in\nadapting large-scale pre-trained language models for diverse tasks, offering a\nbalance between adaptability and computational efficiency. They are important\nin Low-Resource Language (LRL) Neural Machine Translation (NMT) to enhance\ntranslation accuracy with minimal resources. However, their practical\neffectiveness varies significantly across different languages. We conducted\ncomprehensive empirical experiments with varying LRL domains and sizes to\nevaluate the performance of 8 PEFT methods with in total of 15 architectures\nusing the SacreBLEU score. We showed that 6 PEFT architectures outperform the\nbaseline for both in-domain and out-domain tests and the Houlsby+Inversion\nadapter has the best performance overall, proving the effectiveness of PEFT\nmethods.", "published": "2024-04-05 16:42:28", "link": "http://arxiv.org/abs/2404.04212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Lexical is Bilingual Lexicon Induction?", "abstract": "In contemporary machine learning approaches to bilingual lexicon induction\n(BLI), a model learns a mapping between the embedding spaces of a language\npair. Recently, retrieve-and-rank approach to BLI has achieved state of the art\nresults on the task. However, the problem remains challenging in low-resource\nsettings, due to the paucity of data. The task is complicated by factors such\nas lexical variation across languages. We argue that the incorporation of\nadditional lexical information into the recent retrieve-and-rank approach\nshould improve lexicon induction. We demonstrate the efficacy of our proposed\napproach on XLING, improving over the previous state of the art by an average\nof 2\\% across all language pairs.", "published": "2024-04-05 17:10:33", "link": "http://arxiv.org/abs/2404.04221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking and Improving Compositional Generalization of Multi-aspect\n  Controllable Text Generation", "abstract": "Compositional generalization, representing the model's ability to generate\ntext with new attribute combinations obtained by recombining single attributes\nfrom the training data, is a crucial property for multi-aspect controllable\ntext generation (MCTG) methods. Nonetheless, a comprehensive compositional\ngeneralization evaluation benchmark of MCTG is still lacking. We propose\nCompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets and a\ncrafted three-dimensional evaluation protocol, to holistically evaluate the\ncompositional generalization of MCTG approaches. We observe that existing MCTG\nworks generally confront a noticeable performance drop in compositional\ntesting. To mitigate this issue, we introduce Meta-MCTG, a training framework\nincorporating meta-learning, where we enable models to learn how to generalize\nby simulating compositional generalization scenarios in the training phase. We\ndemonstrate the effectiveness of Meta-MCTG through achieving obvious\nimprovement (by at most 3.64%) for compositional testing performance in 94.4%\ncases.", "published": "2024-04-05 17:26:22", "link": "http://arxiv.org/abs/2404.04232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GroundCocoa: A Benchmark for Evaluating Compositional & Conditional\n  Reasoning in Language Models", "abstract": "The rapid progress of large language models (LLMs) has seen them excel and\nfrequently surpass human performance on standard benchmarks. This has enabled\nmany downstream applications, such as LLM agents, to rely on their reasoning to\naddress complex task requirements. However, LLMs are known to unexpectedly\nfalter in simple tasks and under seemingly straightforward circumstances -\nunderscoring the need for better and more diverse evaluation setups to measure\ntheir true capabilities. To this end, we choose to study compositional and\nconditional reasoning, two aspects that are central to human cognition, and\nintroduce GroundCocoa - a lexically diverse benchmark connecting these\nreasoning skills to the real-world problem of flight booking. Our task involves\naligning detailed user preferences with available flight options presented in a\nmultiple-choice format. Results indicate a significant disparity in performance\namong current state-of-the-art LLMs with even the best performing model, GPT-4\nTurbo, not exceeding 67% accuracy despite advanced prompting techniques.", "published": "2024-04-05 17:36:26", "link": "http://arxiv.org/abs/2404.04237v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deciphering Political Entity Sentiment in News with Large Language\n  Models: Zero-Shot and Few-Shot Strategies", "abstract": "Sentiment analysis plays a pivotal role in understanding public opinion,\nparticularly in the political domain where the portrayal of entities in news\narticles influences public perception. In this paper, we investigate the\neffectiveness of Large Language Models (LLMs) in predicting entity-specific\nsentiment from political news articles. Leveraging zero-shot and few-shot\nstrategies, we explore the capability of LLMs to discern sentiment towards\npolitical entities in news content. Employing a chain-of-thought (COT) approach\naugmented with rationale in few-shot in-context learning, we assess whether\nthis method enhances sentiment prediction accuracy. Our evaluation on\nsentiment-labeled datasets demonstrates that LLMs, outperform fine-tuned BERT\nmodels in capturing entity-specific sentiment. We find that learning in-context\nsignificantly improves model performance, while the self-consistency mechanism\nenhances consistency in sentiment prediction. Despite the promising results, we\nobserve inconsistencies in the effectiveness of the COT prompting method.\nOverall, our findings underscore the potential of LLMs in entity-centric\nsentiment analysis within the political news domain and highlight the\nimportance of suitable prompting strategies and model architectures.", "published": "2024-04-05 19:14:38", "link": "http://arxiv.org/abs/2404.04361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed\n  Forward Skipping", "abstract": "Autoregressive Large Language Models (e.g., LLaMa, GPTs) are omnipresent\nachieving remarkable success in language understanding and generation. However,\nsuch impressive capability typically comes with a substantial model size, which\npresents significant challenges for autoregressive token-by-token generation.\nTo mitigate computation overload incurred during generation, several early-exit\nand layer-dropping strategies have been proposed. Despite some promising\nsuccess due to the redundancy across LLMs layers on metrics like Rough-L/BLUE,\nour careful knowledge-intensive evaluation unveils issues such as generation\ncollapse, hallucination of wrong facts, and noticeable performance drop even at\nthe trivial exit ratio of 10-15% of layers. We attribute these errors primarily\nto ineffective handling of the KV cache through state copying during\nearly-exit. In this work, we observed the saturation of computationally\nexpensive feed-forward blocks of LLM layers and proposed FFN-SkipLLM, which is\na novel fine-grained skip strategy of autoregressive LLMs. More specifically,\nFFN-SkipLLM is an input-adaptive feed-forward skipping strategy that can skip\n25-30% of FFN blocks of LLMs with marginal change in performance on\nknowledge-intensive generation tasks without any requirement to handle KV\ncache. Our extensive experiments and ablation across benchmarks like MT-Bench,\nFactoid-QA, and variable-length text summarization illustrate how our simple\nand ease-at-use method can facilitate faster autoregressive decoding.", "published": "2024-04-05 02:35:43", "link": "http://arxiv.org/abs/2404.03865v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical\n  Reasoning in Large Language Models", "abstract": "This study presents a novel learning approach designed to enhance both\nmathematical reasoning and problem-solving abilities of Large Language Models\n(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the\nProgram-of-Thought (PoT) learning, hypothesizing that prioritizing the learning\nof mathematical reasoning ability is helpful for the amplification of\nproblem-solving ability. Thus, the initial learning with CoT is essential for\nsolving challenging mathematical problems. To this end, we propose a sequential\nlearning approach, named SAAS (Solving Ability Amplification Strategy), which\nstrategically transitions from CoT learning to PoT learning. Our empirical\nstudy, involving an extensive performance comparison using several benchmarks,\ndemonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The\nresults underscore the effectiveness of our sequential learning approach,\nmarking a significant advancement in the field of mathematical reasoning in\nLLMs.", "published": "2024-04-05 04:25:47", "link": "http://arxiv.org/abs/2404.03887v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for\n  Low-Resource Languages with Application to Luxembourgish", "abstract": "In NLP, zero-shot classification (ZSC) is the task of assigning labels to\ntextual data without any labeled examples for the target classes. A common\nmethod for ZSC is to fine-tune a language model on a Natural Language Inference\n(NLI) dataset and then use it to infer the entailment between the input\ndocument and the target labels. However, this approach faces certain\nchallenges, particularly for languages with limited resources. In this paper,\nwe propose an alternative solution that leverages dictionaries as a source of\ndata for ZSC. We focus on Luxembourgish, a low-resource language spoken in\nLuxembourg, and construct two new topic relevance classification datasets based\non a dictionary that provides various synonyms, word translations and example\nsentences. We evaluate the usability of our dataset and compare it with the\nNLI-based approach on two topic classification tasks in a zero-shot manner. Our\nresults show that by using the dictionary-based dataset, the trained models\noutperform the ones following the NLI-based approach for ZSC. While we focus on\na single low-resource language in this study, we believe that the efficacy of\nour approach can also transfer to other languages where such a dictionary is\navailable.", "published": "2024-04-05 06:35:31", "link": "http://arxiv.org/abs/2404.03912v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the\n  Evaluative Meaning of German Personal Name Compounds", "abstract": "We present a comprehensive computational study of the under-investigated\nphenomenon of personal name compounds (PNCs) in German such as\nWillkommens-Merkel ('Welcome-Merkel'). Prevalent in news, social media, and\npolitical discourse, PNCs are hypothesized to exhibit an evaluative function\nthat is reflected in a more positive or negative perception as compared to the\nrespective personal full name (such as Angela Merkel). We model 321 PNCs and\ntheir corresponding full names at discourse level, and show that PNCs bear an\nevaluative nature that can be captured through a variety of computational\nmethods. Specifically, we assess through valence information whether a PNC is\nmore positively or negatively evaluative than the person's name, by applying\nand comparing two approaches using (i) valence norms and (ii) pretrained\nlanguage models (PLMs). We further enrich our data with personal,\ndomain-specific, and extra-linguistic information and perform a range of\nregression analyses revealing that factors including compound and modifier\nvalence, domain, and political party membership influence how a PNC is\nevaluated.", "published": "2024-04-05 11:24:41", "link": "http://arxiv.org/abs/2404.04031v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Dwell in the Beginning: How Language Models Embed Long Documents for\n  Dense Retrieval", "abstract": "This study investigates the existence of positional biases in\nTransformer-based models for text representation learning, particularly in the\ncontext of web document retrieval. We build on previous research that\ndemonstrated loss of information in the middle of input sequences for causal\nlanguage models, extending it to the domain of representation learning. We\nexamine positional biases at various stages of training for an encoder-decoder\nmodel, including language model pre-training, contrastive pre-training, and\ncontrastive fine-tuning. Experiments with the MS-MARCO document collection\nreveal that after contrastive pre-training the model already generates\nembeddings that better capture early contents of the input, with fine-tuning\nfurther aggravating this effect.", "published": "2024-04-05 15:16:16", "link": "http://arxiv.org/abs/2404.04163v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model", "abstract": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.", "published": "2024-04-05 15:20:02", "link": "http://arxiv.org/abs/2404.04167v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Sentence Transformers Learn Quasi-Geospatial Concepts from General\n  Text?", "abstract": "Sentence transformers are language models designed to perform semantic\nsearch. This study investigates the capacity of sentence transformers,\nfine-tuned on general question-answering datasets for asymmetric semantic\nsearch, to associate descriptions of human-generated routes across Great\nBritain with queries often used to describe hiking experiences. We find that\nsentence transformers have some zero-shot capabilities to understand\nquasi-geospatial concepts, such as route types and difficulty, suggesting their\npotential utility for routing recommendation systems.", "published": "2024-04-05 15:22:02", "link": "http://arxiv.org/abs/2404.04169v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Social Skill Training with Large Language Models", "abstract": "People rely on social skills like conflict resolution to communicate\neffectively and to thrive in both work and personal life. However, practice\nenvironments for social skills are typically out of reach for most people. How\ncan we make social skill training more available, accessible, and inviting?\nDrawing upon interdisciplinary research from communication and psychology, this\nperspective paper identifies social skill barriers to enter specialized fields.\nThen we present a solution that leverages large language models for social\nskill training via a generic framework. Our AI Partner, AI Mentor framework\nmerges experiential learning with realistic practice and tailored feedback.\nThis work ultimately calls for cross-disciplinary innovation to address the\nbroader implications for workforce development and social equality.", "published": "2024-04-05 16:29:58", "link": "http://arxiv.org/abs/2404.04204v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Scope Ambiguities in Large Language Models", "abstract": "Sentences containing multiple semantic operators with overlapping scope often\ncreate ambiguities in interpretation, known as scope ambiguities. These\nambiguities offer rich insights into the interaction between semantic structure\nand world knowledge in language processing. Despite this, there has been little\nresearch into how modern large language models treat them. In this paper, we\ninvestigate how different versions of certain autoregressive language models --\nGPT-2, GPT-3/3.5, Llama 2 and GPT-4 -- treat scope ambiguous sentences, and\ncompare this with human judgments. We introduce novel datasets that contain a\njoint total of almost 1,000 unique scope-ambiguous sentences, containing\ninteractions between a range of semantic operators, and annotated for human\njudgments. Using these datasets, we find evidence that several models (i) are\nsensitive to the meaning ambiguity in these sentences, in a way that patterns\nwell with human judgments, and (ii) can successfully identify human-preferred\nreadings at a high level of accuracy (over 90% in some cases).", "published": "2024-04-05 18:01:02", "link": "http://arxiv.org/abs/2404.04332v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and\n  Evaluation", "abstract": "We introduce a meta dataset for few-shot relation extraction, which includes\ntwo datasets derived from existing supervised relation extraction datasets\nNYT29 (Takanobu et al., 2019; Nayak and Ng, 2020) and WIKIDATA (Sorokin and\nGurevych, 2017) as well as a few-shot form of the TACRED dataset (Sabo et al.,\n2021). Importantly, all these few-shot datasets were generated under realistic\nassumptions such as: the test relations are different from any relations a\nmodel might have seen before, limited training data, and a preponderance of\ncandidate relation mentions that do not correspond to any of the relations of\ninterest. Using this large resource, we conduct a comprehensive evaluation of\nsix recent few-shot relation extraction methods, and observe that no method\ncomes out as a clear winner. Further, the overall performance on this task is\nlow, indicating substantial need for future research. We release all versions\nof the data, i.e., both supervised and few-shot, for future research.", "published": "2024-04-05 23:12:46", "link": "http://arxiv.org/abs/2404.04445v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Distributionally Robust Alignment for Medical Federated Vision-Language\n  Pre-training Under Data Heterogeneity", "abstract": "Vision-language pre-training (VLP) has emerged as an effective scheme for\nmultimodal representation learning, but its reliance on large-scale multimodal\ndata poses significant challenges for medical applications. Federated learning\n(FL) offers a promising solution to scale up the dataset for medical VLP while\npreserving data privacy. However, we observe that client data heterogeneity in\nreal-world scenarios could cause models to learn biased cross-modal alignment\nduring local pre-training. This would limit the transferability of the\nfederally learned representation model on downstream tasks. To address this\nchallenge, we propose Federated Distributionally Robust Alignment (FedDRA), a\nframework for federated VLP that achieves robust vision-language alignment\nunder heterogeneous conditions. Based on client datasets, we construct a\ndistribution family that encompasses potential test-time domains, and apply a\ndistributionally robust framework to optimize the pre-trained model's\nperformance across this distribution space. This approach bridges the gap\nbetween pre-training samples and downstream applications. To avoid over-fitting\non client-specific information, we use anchor representation from the global\nmodel to guide the local training, and adopt a two-stage approach to first tune\ndeeper layers before updating the entire network. Extensive experiments on\nreal-world datasets demonstrate FedDRA's effectiveness in enhancing medical\nfederated VLP under data heterogeneity. Our method also adapts well to various\nmedical pre-training methods.", "published": "2024-04-05 01:17:25", "link": "http://arxiv.org/abs/2404.03854v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge\n  Graph Construction", "abstract": "In this work, we are interested in automated methods for knowledge graph\ncreation (KGC) from input text. Progress on large language models (LLMs) has\nprompted a series of recent works applying them to KGC, e.g., via zero/few-shot\nprompting. Despite successes on small domain-specific datasets, these models\nface difficulties scaling up to text common in many real-world applications. A\nprincipal issue is that, in prior methods, the KG schema has to be included in\nthe LLM prompt to generate valid triplets; larger and more complex schemas\neasily exceed the LLMs' context window length. Furthermore, there are scenarios\nwhere a fixed pre-defined schema is not available and we would like the method\nto construct a high-quality KG with a succinct self-generated schema. To\naddress these problems, we propose a three-phase framework named\nExtract-Define-Canonicalize (EDC): open information extraction followed by\nschema definition and post-hoc canonicalization. EDC is flexible in that it can\nbe applied to settings where a pre-defined target schema is available and when\nit is not; in the latter case, it constructs a schema automatically and applies\nself-canonicalization. To further improve performance, we introduce a trained\ncomponent that retrieves schema elements relevant to the input text; this\nimproves the LLMs' extraction performance in a retrieval-augmented\ngeneration-like manner. We demonstrate on three KGC benchmarks that EDC is able\nto extract high-quality triplets without any parameter tuning and with\nsignificantly larger schemas compared to prior works. Code for EDC is available\nat https://github.com/clear-nus/edc.", "published": "2024-04-05 02:53:51", "link": "http://arxiv.org/abs/2404.03868v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers for molecular property prediction: Lessons learned from the\n  past five years", "abstract": "Molecular Property Prediction (MPP) is vital for drug discovery, crop\nprotection, and environmental science. Over the last decades, diverse\ncomputational techniques have been developed, from using simple physical and\nchemical properties and molecular fingerprints in statistical models and\nclassical machine learning to advanced deep learning approaches. In this\nreview, we aim to distill insights from current research on employing\ntransformer models for MPP. We analyze the currently available models and\nexplore key questions that arise when training and fine-tuning a transformer\nmodel for MPP. These questions encompass the choice and scale of the\npre-training data, optimal architecture selections, and promising pre-training\nobjectives. Our analysis highlights areas not yet covered in current research,\ninviting further exploration to enhance the field's understanding.\nAdditionally, we address the challenges in comparing different models,\nemphasizing the need for standardized data splitting and robust statistical\nanalysis.", "published": "2024-04-05 09:05:37", "link": "http://arxiv.org/abs/2404.03969v1", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "VoicePilot: Harnessing LLMs as Speech Interfaces for Physically\n  Assistive Robots", "abstract": "Physically assistive robots present an opportunity to significantly increase\nthe well-being and independence of individuals with motor impairments or other\nforms of disability who are unable to complete activities of daily living.\nSpeech interfaces, especially ones that utilize Large Language Models (LLMs),\ncan enable individuals to effectively and naturally communicate high-level\ncommands and nuanced preferences to robots. Frameworks for integrating LLMs as\ninterfaces to robots for high level task planning and code generation have been\nproposed, but fail to incorporate human-centric considerations which are\nessential while developing assistive interfaces. In this work, we present a\nframework for incorporating LLMs as speech interfaces for physically assistive\nrobots, constructed iteratively with 3 stages of testing involving a feeding\nrobot, culminating in an evaluation with 11 older adults at an independent\nliving facility. We use both quantitative and qualitative data from the final\nstudy to validate our framework and additionally provide design guidelines for\nusing LLMs as speech interfaces for assistive robots. Videos and supporting\nfiles are located on our project website:\nhttps://sites.google.com/andrew.cmu.edu/voicepilot/", "published": "2024-04-05 12:45:10", "link": "http://arxiv.org/abs/2404.04066v2", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Does Biomedical Training Lead to Better Medical Performance?", "abstract": "Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, biomedical\ntraining has not been systematically evaluated on medical tasks. This study\ninvestigates the effect of biomedical training in the context of six practical\nmedical tasks evaluating $25$ models. In contrast to previous evaluations, our\nresults reveal a performance decline in nine out of twelve biomedical models\nafter fine-tuning, particularly on tasks involving hallucinations, ICD10\ncoding, and instruction adherence. General-domain models like\nMeta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts,\nindicating a trade-off between domain-specific fine-tuning and general medical\ntask performance. We open-source all evaluation scripts and datasets at\nhttps://github.com/TIO-IKIM/CLUE to support further research in this critical\narea.", "published": "2024-04-05 12:51:37", "link": "http://arxiv.org/abs/2404.04067v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ROPO: Robust Preference Optimization for Large Language Models", "abstract": "Preference alignment is pivotal for empowering large language models (LLMs)\nto generate helpful and harmless responses. However, the performance of\npreference alignment is highly sensitive to the prevalent noise in the\npreference data. Recent efforts for this problem either marginally alleviate\nthe impact of noise without the ability to actually reduce its presence, or\nrely on costly teacher LLMs prone to reward misgeneralization. To address these\nchallenges, we propose the RObust Preference Optimization (ROPO) framework, an\niterative alignment approach that integrates noise-tolerance and filtering of\nnoisy samples without the aid of external models. Specifically, ROPO\niteratively solves a constrained optimization problem, where we dynamically\nassign a quality-aware weight for each sample and constrain the sum of the\nweights to the number of samples we intend to retain. For noise-tolerant\ntraining and effective noise identification, we derive a robust loss by\nsuppressing the gradients of samples with high uncertainty. We demonstrate both\nempirically and theoretically that the derived loss is critical for\ndistinguishing noisy samples from clean ones. Furthermore, inspired by our\nderived loss, we propose a robustness-guided rejection sampling technique to\ncompensate for the potential important information in discarded queries.\nExperiments on three widely-used datasets with Mistral-7B and Llama-2-7B\ndemonstrate that ROPO significantly outperforms existing preference alignment\nmethods, with its superiority growing as the noise rate increases.", "published": "2024-04-05 13:58:51", "link": "http://arxiv.org/abs/2404.04102v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "player2vec: A Language Modeling Approach to Understand Player Behavior\n  in Games", "abstract": "Methods for learning latent user representations from historical behavior\nlogs have gained traction for recommendation tasks in e-commerce, content\nstreaming, and other settings. However, this area still remains relatively\nunderexplored in video and mobile gaming contexts. In this work, we present a\nnovel method for overcoming this limitation by extending a long-range\nTransformer model from the natural language processing domain to player\nbehavior data. We discuss specifics of behavior tracking in games and propose\npreprocessing and tokenization approaches by viewing in-game events in an\nanalogous way to words in sentences, thus enabling learning player\nrepresentations in a self-supervised manner in the absence of ground-truth\nannotations. We experimentally demonstrate the efficacy of the proposed\napproach in fitting the distribution of behavior events by evaluating intrinsic\nlanguage modeling metrics. Furthermore, we qualitatively analyze the emerging\nstructure of the learned embedding space and show its value for generating\ninsights into behavior patterns to inform downstream applications.", "published": "2024-04-05 17:29:47", "link": "http://arxiv.org/abs/2404.04234v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Physical Property Understanding from Language-Embedded Feature Fields", "abstract": "Can computers perceive the physical properties of objects solely through\nvision? Research in cognitive science and vision science has shown that humans\nexcel at identifying materials and estimating their physical properties based\npurely on visual appearance. In this paper, we present a novel approach for\ndense prediction of the physical properties of objects using a collection of\nimages. Inspired by how humans reason about physics through vision, we leverage\nlarge language models to propose candidate materials for each object. We then\nconstruct a language-embedded point cloud and estimate the physical properties\nof each 3D point using a zero-shot kernel regression approach. Our method is\naccurate, annotation-free, and applicable to any object in the open world.\nExperiments demonstrate the effectiveness of the proposed approach in various\nphysical property reasoning tasks, such as estimating the mass of common\nobjects, as well as other properties like friction and hardness.", "published": "2024-04-05 17:45:07", "link": "http://arxiv.org/abs/2404.04242v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt\n  Coherence Metrics with T2IScoreScore (TS2)", "abstract": "With advances in the quality of text-to-image (T2I) models has come interest\nin benchmarking their prompt faithfulness -- the semantic coherence of\ngenerated images to the prompts they were conditioned on. A variety of T2I\nfaithfulness metrics have been proposed, leveraging advances in cross-modal\nembeddings and vision-language models (VLMs). However, these metrics are not\nrigorously compared and benchmarked, instead presented with correlation to\nhuman Likert scores over a set of easy-to-discriminate images against seemingly\nweak baselines.\n  We introduce T2IScoreScore, a curated set of semantic error graphs containing\na prompt and a set of increasingly erroneous images. These allow us to\nrigorously judge whether a given prompt faithfulness metric can correctly order\nimages with respect to their objective error count and significantly\ndiscriminate between different error nodes, using meta-metric scores derived\nfrom established statistical tests. Surprisingly, we find that the\nstate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we\ntested fail to significantly outperform simple (and supposedly worse)\nfeature-based metrics like CLIPScore, particularly on a hard subset of\nnaturally-occurring T2I model errors. TS2 will enable the development of better\nT2I prompt faithfulness metrics through more rigorous comparison of their\nconformity to expected orderings and separations under objective criteria.", "published": "2024-04-05 17:57:16", "link": "http://arxiv.org/abs/2404.04251v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AuditGPT: Auditing Smart Contracts with ChatGPT", "abstract": "To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each containing a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to either manually audit each single contract or use expert-developed,\nlimited-scope program-analysis tools, both of which are far from being\neffective in identifying ERC rule violations. This paper presents a tool named\nAuditGPT that leverages large language models (LLMs) to automatically and\ncomprehensively verify ERC rules against smart contracts. To build AuditGPT, we\nfirst conduct an empirical study on 222 ERC rules specified in four popular\nERCs to understand their content, their security impacts, their specification\nin natural language, and their implementation in Solidity. Guided by the study,\nwe construct AuditGPT by separating the large, complex auditing process into\nsmall, manageable tasks and design prompts specialized for each ERC rule type\nto enhance LLMs' auditing performance. In the evaluation, AuditGPT successfully\npinpoints 418 ERC rule violations and only reports 18 false positives,\nshowcasing its effectiveness and accuracy. Moreover, AuditGPT beats an auditing\nservice provided by security experts in effectiveness, accuracy, and cost,\ndemonstrating its advancement over state-of-the-art smart-contract auditing\npractices.", "published": "2024-04-05 07:19:13", "link": "http://arxiv.org/abs/2404.04306v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CR"}
{"title": "Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation", "abstract": "With the increasingly powerful performances and enormous scales of pretrained\nmodels, promoting parameter efficiency in fine-tuning has become a crucial need\nfor effective and efficient adaptation to various downstream tasks. One\nrepresentative line of fine-tuning methods is Orthogonal Fine-tuning (OFT),\nwhich rigorously preserves the angular distances within the parameter space to\npreserve the pretrained knowledge. Despite the empirical effectiveness, OFT\nstill suffers low parameter efficiency at $\\mathcal{O}(d^2)$ and limited\ncapability of downstream adaptation. Inspired by Givens rotation, in this\npaper, we proposed quasi-Givens Orthogonal Fine-Tuning (qGOFT) to address the\nproblems. We first use $\\mathcal{O}(d)$ Givens rotations to accomplish\narbitrary orthogonal transformation in $SO(d)$ with provable equivalence,\nreducing parameter complexity from $\\mathcal{O}(d^2)$ to $\\mathcal{O}(d)$. Then\nwe introduce flexible norm and relative angular adjustments under soft\northogonality regularization to enhance the adaptation capability of downstream\nsemantic deviations. Extensive experiments on various tasks and pretrained\nmodels validate the effectiveness of our methods.", "published": "2024-04-05 15:28:44", "link": "http://arxiv.org/abs/2404.04316v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Hypothesis Generation with Large Language Models", "abstract": "Effective generation of novel hypotheses is instrumental to scientific\nprogress. So far, researchers have been the main powerhouse behind hypothesis\ngeneration by painstaking data analysis and thinking (also known as the Eureka\nmoment). In this paper, we examine the potential of large language models\n(LLMs) to generate hypotheses. We focus on hypothesis generation based on data\n(i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts,\nwe generate initial hypotheses from a small number of examples and then update\nthem iteratively to improve the quality of hypotheses. Inspired by multi-armed\nbandits, we design a reward function to inform the exploitation-exploration\ntradeoff in the update process. Our algorithm is able to generate hypotheses\nthat enable much better predictive performance than few-shot prompting in\nclassification tasks, improving accuracy by 31.7% on a synthetic dataset and by\n13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform\nsupervised learning by 12.8% and 11.2% on two challenging real-world datasets.\nFurthermore, we find that the generated hypotheses not only corroborate\nhuman-verified theories but also uncover new insights for the tasks.", "published": "2024-04-05 18:00:07", "link": "http://arxiv.org/abs/2404.04326v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Assisting humans in complex comparisons: automated information\n  comparison at scale", "abstract": "Generative Large Language Models enable efficient analytics across knowledge\ndomains, rivalling human experts in information comparisons. However, the\napplications of LLMs for information comparisons face scalability challenges\ndue to the difficulties in maintaining information across large contexts and\novercoming model token limitations. To address these challenges, we developed\nthe novel Abstractive Summarization & Criteria-driven Comparison Endpoint\n(ASC$^2$End) system to automate information comparison at scale. Our system\nemploys Semantic Text Similarity comparisons for generating evidence-supported\nanalyses. We utilize proven data-handling strategies such as abstractive\nsummarization and retrieval augmented generation to overcome token limitations\nand retain relevant information during model inference. Prompts were designed\nusing zero-shot strategies to contextualize information for improved model\nreasoning. We evaluated abstractive summarization using ROUGE scoring and\nassessed the generated comparison quality using survey responses. Models\nevaluated on the ASC$^2$End system show desirable results providing insights on\nthe expected performance of the system. ASC$^2$End is a novel system and tool\nthat enables accurate, automated information comparison at scale across\nknowledge domains, overcoming limitations in context length and retrieval.", "published": "2024-04-05 18:44:54", "link": "http://arxiv.org/abs/2404.04351v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.8"], "primary_category": "cs.CL"}
{"title": "Prompt Public Large Language Models to Synthesize Data for Private\n  On-device Applications", "abstract": "Pre-training on public data is an effective method to improve the performance\nfor federated learning (FL) with differential privacy (DP). This paper\ninvestigates how large language models (LLMs) trained on public data can\nimprove the quality of pre-training data for the on-device language models\ntrained with DP and FL. We carefully design LLM prompts to filter and transform\nexisting public data, and generate new data to resemble the real user data\ndistribution. The model pre-trained on our synthetic dataset achieves relative\nimprovement of 19.0% and 22.8% in next word prediction accuracy compared to the\nbaseline model pre-trained on a standard public dataset, when evaluated over\nthe real user data in Gboard (Google Keyboard, a production mobile keyboard\napplication). Furthermore, our method achieves evaluation accuracy better than\nor comparable to the baseline during the DP FL fine-tuning over millions of\nmobile devices, and our final model outperforms the baseline in production A/B\ntesting. Our experiments demonstrate the strengths of LLMs in synthesizing data\nclose to the private distribution even without accessing the private data, and\nalso suggest future research directions to further reduce the distribution gap.", "published": "2024-04-05 19:14:14", "link": "http://arxiv.org/abs/2404.04360v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Counting Like Transformers: Compiling Temporal Counting Logic Into\n  Softmax Transformers", "abstract": "Deriving formal bounds on the expressivity of transformers, as well as\nstudying transformers that are constructed to implement known algorithms, are\nboth effective methods for better understanding the computational power of\ntransformers. Towards both ends, we introduce the temporal counting logic\n$\\textsf{K}_\\text{t}$[#] alongside the RASP variant $\\textsf{C-RASP}$. We show\nthey are equivalent to each other, and that together they are the best-known\nlower bound on the formal expressivity of future-masked soft attention\ntransformers with unbounded input size. We prove this by showing all\n$\\textsf{K}_\\text{t}$[#] formulas can be compiled into these transformers.", "published": "2024-04-05 20:36:30", "link": "http://arxiv.org/abs/2404.04393v2", "categories": ["cs.LO", "cs.CL", "cs.FL", "cs.LG"], "primary_category": "cs.LO"}
{"title": "Sentiment analysis and random forest to classify LLM versus human source\n  applied to Scientific Texts", "abstract": "After the launch of ChatGPT v.4 there has been a global vivid discussion on\nthe ability of this artificial intelligence powered platform and some other\nsimilar ones for the automatic production of all kinds of texts, including\nscientific and technical texts. This has triggered a reflection in many\ninstitutions on whether education and academic procedures should be adapted to\nthe fact that in future many texts we read will not be written by humans\n(students, scholars, etc.), at least, not entirely. In this work it is proposed\na new methodology to classify texts coming from an automatic text production\nengine or a human, based on Sentiment Analysis as a source for feature\nengineering independent variables and then train with them a Random Forest\nclassification algorithm. Using four different sentiment lexicons, a number of\nnew features where produced, and then fed to a machine learning random forest\nmethodology, to train such a model. Results seem very convincing that this may\nbe a promising research line to detect fraud, in such environments where human\nare supposed to be the source of texts.", "published": "2024-04-05 16:14:36", "link": "http://arxiv.org/abs/2404.08673v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "68"], "primary_category": "cs.CL"}
{"title": "Effects of Different Prompts on the Quality of GPT-4 Responses to\n  Dementia Care Questions", "abstract": "Evidence suggests that different prompts lead large language models (LLMs) to\ngenerate responses with varying quality. Yet, little is known about prompts'\neffects on response quality in healthcare domains. In this exploratory study,\nwe address this gap, focusing on a specific healthcare domain: dementia\ncaregiving. We first developed an innovative prompt template with three\ncomponents: (1) system prompts (SPs) featuring 4 different roles; (2) an\ninitialization prompt; and (3) task prompts (TPs) specifying different levels\nof details, totaling 12 prompt combinations. Next, we selected 3 social media\nposts containing complicated, real-world questions about dementia caregivers'\nchallenges in 3 areas: memory loss and confusion, aggression, and driving. We\nthen entered these posts into GPT-4, with our 12 prompts, to generate 12\nresponses per post, totaling 36 responses. We compared the word count of the 36\nresponses to explore potential differences in response length. Two experienced\ndementia care clinicians on our team assessed the response quality using a\nrating scale with 5 quality indicators: factual, interpretation, application,\nsynthesis, and comprehensiveness (scoring range: 0-5; higher scores indicate\nhigher quality).", "published": "2024-04-05 19:24:57", "link": "http://arxiv.org/abs/2404.08674v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Large language models as oracles for instantiating ontologies with\n  domain-specific knowledge", "abstract": "Background. Endowing intelligent systems with semantic data commonly requires\ndesigning and instantiating ontologies with domain-specific knowledge.\nEspecially in the early phases, those activities are typically performed\nmanually by human experts possibly leveraging on their own experience. The\nresulting process is therefore time-consuming, error-prone, and often biased by\nthe personal background of the ontology designer. Objective. To mitigate that\nissue, we propose a novel domain-independent approach to automatically\ninstantiate ontologies with domain-specific knowledge, by leveraging on large\nlanguage models (LLMs) as oracles. Method. Starting from (i) an initial schema\ncomposed by inter-related classes and properties and (ii) a set of query\ntemplates, our method queries the LLM multiple times, and generates instances\nfor both classes and properties from its replies. Thus, the ontology is\nautomatically filled with domain-specific knowledge, compliant to the initial\nschema. As a result, the ontology is quickly and automatically enriched with\nmanifold instances, which experts may consider to keep, adjust, discard, or\ncomplement according to their own needs and expertise. Contribution. We\nformalise our method in general way and instantiate it over various LLMs, as\nwell as on a concrete case study. We report experiments rooted in the\nnutritional domain where an ontology of food meals and their ingredients is\nautomatically instantiated from scratch, starting from a categorisation of\nmeals and their relationships. There, we analyse the quality of the generated\nontologies and compare ontologies attained by exploiting different LLMs.\nExperimentally, our approach achieves a quality metric that is up to five times\nhigher than the state-of-the-art, while reducing erroneous entities and\nrelations by up to ten times. Finally, we provide a SWOT analysis of the\nproposed method.", "published": "2024-04-05 14:04:07", "link": "http://arxiv.org/abs/2404.04108v2", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Watermark-based Attribution of AI-Generated Content", "abstract": "Several companies have deployed watermark-based detection to identify\nAI-generated content. However, attribution--the ability to trace back to the\nuser of a generative AI (GenAI) service who created a given piece of\nAI-generated content--remains largely unexplored despite its growing\nimportance. In this work, we aim to bridge this gap by conducting the first\nsystematic study on watermark-based, user-level attribution of AI-generated\ncontent. Our key idea is to assign a unique watermark to each user of the GenAI\nservice and embed this watermark into the AI-generated content created by that\nuser. Attribution is then performed by identifying the user whose watermark\nbest matches the one extracted from the given content. This approach, however,\nfaces a key challenge: How should watermarks be selected for users to maximize\nattribution performance? To address the challenge, we first theoretically\nderive lower bounds on detection and attribution performance through rigorous\nprobabilistic analysis for any given set of user watermarks. Then, we select\nwatermarks for users to maximize these lower bounds, thereby optimizing\ndetection and attribution performance. Our theoretical and empirical results\nshow that watermark-based attribution inherits both the accuracy and\n(non-)robustness properties of the underlying watermark. Specifically,\nattribution remains highly accurate when the watermarked AI-generated content\nis either not post-processed or subjected to common post-processing such as\nJPEG compression, as well as black-box adversarial post-processing with limited\nquery budgets.", "published": "2024-04-05 17:58:52", "link": "http://arxiv.org/abs/2404.04254v3", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Taxonomy and Analysis of Sensitive User Queries in Generative AI Search", "abstract": "Although there has been a growing interest among industries in integrating\ngenerative LLMs into their services, limited experience and scarcity of\nresources act as a barrier in launching and servicing large-scale LLM-based\nservices. In this paper, we share our experiences in developing and operating\ngenerative AI models within a national-scale search engine, with a specific\nfocus on the sensitiveness of user queries. We propose a taxonomy for sensitive\nsearch queries, outline our approaches, and present a comprehensive analysis\nreport on sensitive queries from actual users. We believe that our experiences\nin launching generative AI search systems can contribute to reducing the\nbarrier in building generative LLM-based services.", "published": "2024-04-05 05:14:46", "link": "http://arxiv.org/abs/2404.08672v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Uformer: A UNet-Transformer fused robust end-to-end deep learning\n  framework for real-time denoising of lung sounds", "abstract": "Objective: Lung auscultation is a valuable tool in diagnosing and monitoring\nvarious respiratory diseases. However, lung sounds (LS) are significantly\naffected by numerous sources of contamination, especially when recorded in\nreal-world clinical settings. Conventional denoising models prove impractical\nfor LS denoising, primarily owing to spectral overlap complexities arising from\ndiverse noise sources. To address this issue, we propose a specialized\ndeep-learning model (Uformer) for lung sound denoising. Methods: The proposed\nUformer model is constituted of three modules: a Convolutional Neural Network\n(CNN) encoder module, dedicated to extracting latent features; a Transformer\nencoder module, employed to further enhance the encoding of unique LS features\nand effectively capture intricate long-range dependencies; and a CNN decoder\nmodule, employed to generate the denoised signals. An ablation study was\nperformed in order to find the most optimal architecture. Results: The\nperformance of the proposed Uformer model was evaluated on lung sounds induced\nwith different types of synthetic and real-world noises. Lung sound signals of\n-12 dB to 15 dB signal-to-noise ratio (SNR) were considered in testing\nexperiments. The proposed model showed an average SNR improvement of 16.51 dB\nwhen evaluated with -12 dB LS signals. Our end-to-end model, with an average\nSNR improvement of 19.31 dB, outperforms the existing model when evaluated with\nambient noise and fewer parameters. Conclusion: Based on the qualitative and\nquantitative findings in this study, it can be stated that Uformer is robust\nand generalized to be used in assisting the monitoring of respiratory\nconditions.", "published": "2024-04-05 19:24:39", "link": "http://arxiv.org/abs/2404.04365v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "\"It is okay to be uncommon\": Quantizing Sound Event Detection Networks\n  on Hardware Accelerators with Uncommon Sub-Byte Support", "abstract": "If our noise-canceling headphones can understand our audio environments, they\ncan then inform us of important sound events, tune equalization based on the\ntypes of content we listen to, and dynamically adjust noise cancellation\nparameters based on audio scenes to further reduce distraction. However,\nrunning multiple audio understanding models on headphones with a limited energy\nbudget and on-chip memory remains a challenging task. In this work, we identify\na new class of neural network accelerators (e.g., NE16 on GAP9) that allows\nnetwork weights to be quantized to different common (e.g., 8 bits) and uncommon\nbit-widths (e.g., 3 bits). We then applied a differentiable neural architecture\nsearch to search over the optimal bit-widths of a network on two different\nsound event detection tasks with potentially different requirements on\nquantization and prediction granularity (i.e., classification vs. embeddings\nfor few-shot learning). We further evaluated our quantized models on actual\nhardware, showing that we reduce memory usage, inference latency, and energy\nconsumption by an average of 62%, 46%, and 61% respectively compared to 8-bit\nmodels while maintaining floating point performance. Our work sheds light on\nthe benefits of such accelerators on sound event detection tasks when combined\nwith an appropriate search method.", "published": "2024-04-05 20:08:43", "link": "http://arxiv.org/abs/2404.04386v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Holon: a cybernetic interface for bio-semiotics", "abstract": "This paper presents an interactive artwork, \"Holon\", a collection of 130\nautonomous, cybernetic organisms that listen and make sound in collaboration\nwith the natural environment. The work was developed for installation on water\nat a heritage-listed dock in Melbourne, Australia. Conceptual issues informing\nthe work are presented, along with a detailed technical overview of the\nimplementation. Individual holons are of three types, inspired by biological\nmodels of animal communication: composer/generators, collector/critics and\ndisruptors. Collectively, Holon integrates and occupies elements of the\nacoustic spectrum in collaboration with human and non-human agents.", "published": "2024-04-05 05:03:39", "link": "http://arxiv.org/abs/2404.03894v1", "categories": ["cs.SD", "cs.AI", "cs.MA", "eess.AS", "I.2.11; J.5"], "primary_category": "cs.SD"}
{"title": "Open vocabulary keyword spotting through transfer learning from speech\n  synthesis", "abstract": "Identifying keywords in an open-vocabulary context is crucial for\npersonalizing interactions with smart devices. Previous approaches to open\nvocabulary keyword spotting dependon a shared embedding space created by audio\nand text encoders. However, these approaches suffer from heterogeneous modality\nrepresentations (i.e., audio-text mismatch). To address this issue, our\nproposed framework leverages knowledge acquired from a pre-trained\ntext-to-speech (TTS) system. This knowledge transfer allows for the\nincorporation of awareness of audio projections into the text representations\nderived from the text encoder. The performance of the proposed approach is\ncompared with various baseline methods across four different datasets. The\nrobustness of our proposed model is evaluated by assessing its performance\nacross different word lengths and in an Out-of-Vocabulary (OOV) scenario.\nAdditionally, the effectiveness of transfer learning from the TTS system is\ninvestigated by analyzing its different intermediate representations. The\nexperimental results indicate that, in the challenging LibriPhrase Hard\ndataset, the proposed approach outperformed the cross-modality correspondence\ndetector (CMCD) method by a significant improvement of 8.22% in area under the\ncurve (AUC) and 12.56% in equal error rate (EER).", "published": "2024-04-05 06:42:20", "link": "http://arxiv.org/abs/2404.03914v2", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "The NES Video-Music Database: A Dataset of Symbolic Video Game Music\n  Paired with Gameplay Videos", "abstract": "Neural models are one of the most popular approaches for music generation,\nyet there aren't standard large datasets tailored for learning music directly\nfrom game data. To address this research gap, we introduce a novel dataset\nnamed NES-VMDB, containing 98,940 gameplay videos from 389 NES games, each\npaired with its original soundtrack in symbolic format (MIDI). NES-VMDB is\nbuilt upon the Nintendo Entertainment System Music Database (NES-MDB),\nencompassing 5,278 music pieces from 397 NES games. Our approach involves\ncollecting long-play videos for 389 games of the original dataset, slicing them\ninto 15-second-long clips, and extracting the audio from each clip.\nSubsequently, we apply an audio fingerprinting algorithm (similar to Shazam) to\nautomatically identify the corresponding piece in the NES-MDB dataset.\nAdditionally, we introduce a baseline method based on the Controllable Music\nTransformer to generate NES music conditioned on gameplay clips. We evaluated\nthis approach with objective metrics, and the results showed that the\nconditional CMT improves musical structural quality when compared to its\nunconditional counterpart. Moreover, we used a neural classifier to predict the\ngame genre of the generated pieces. Results showed that the CMT generator can\nlearn correlations between gameplay videos and game genres, but further\nresearch has to be conducted to achieve human-level performance.", "published": "2024-04-05 21:41:20", "link": "http://arxiv.org/abs/2404.04420v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rethinking Non-Negative Matrix Factorization with Implicit Neural\n  Representations", "abstract": "Non-negative Matrix Factorization (NMF) is a powerful technique for analyzing\nregularly-sampled data, i.e., data that can be stored in a matrix. For audio,\nthis has led to numerous applications using time-frequency (TF) representations\nlike the Short-Time Fourier Transform. However extending these applications to\nirregularly-spaced TF representations, like the Constant-Q transform, wavelets,\nor sinusoidal analysis models, has not been possible since these\nrepresentations cannot be directly stored in matrix form. In this paper, we\nformulate NMF in terms of continuous functions (instead of fixed vectors) and\nshow that NMF can be extended to a wider variety of signal classes that need\nnot be regularly sampled.", "published": "2024-04-05 22:48:57", "link": "http://arxiv.org/abs/2404.04439v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Characteristics-Based Design of Generalized-Exponent Bandpass Filters", "abstract": "We develop characteristics-based filter design methods for a class of IIR\nbandpass filters, which we refer to as Generalized-Exponent Filters (GEFs) and\nthat are represented as second-order filters raised to non-unitary exponents.\nGEFs have a peak, are effectively linear phase, and are useful for seismic\nsignal phase-picking, cochlear implants, and equalizers. The native\nfrequency-domain specifications for GEFs are not on given frequency responses\nbut rather on filter characteristics such as peak frequency, bandwidth, and\ngroup delay. Our characteristics-based method for filter design accommodates\ndirect specification of a trio of frequency-domain characteristics from amongst\nthe peak frequency, convexity, ndB quality factors, equivalent rectangular\nbandwidth, maximum group delay, and phase accumulation. We achieve this by\nderiving filter parameterizations with sets of filter characteristics which\ninvolves deriving closed-form analytic expressions mapping sets of filter\ncharacteristics to the original filter constants by making sharp-filter\napproximations. This results in parameterizations for GEFs including ones with\nsimultaneous specification of magnitude-based and phase-based characteristics\n(e.g. bandwidths and group delays). This in turn enables designing sharply\ntuned filters without significant group delay, and simultaneous control over\nfrequency selectivity and synchronization which is important in designing\nfilterbanks. Our filter design methods with direct control over characteristics\nmay also be utilized beyond static filter design for higher-order variable\nbandpass filter design and may be useful for characteristics-based adaptive\nfiltering. Our methods are inherently stable, highly accurate in meeting strict\nspecifications on desired characteristics, simple, and computationally\nefficient. The methods extend to the design of related bandpass and multiband\nfilters.", "published": "2024-04-05 09:08:24", "link": "http://arxiv.org/abs/2404.15321v3", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Rational-Exponent Filters with Applications to Generalized Exponent\n  Filters", "abstract": "We present filters with rational exponents in order to provide a continuum of\nfilter behavior not classically achievable. We discuss their stability, the\nflexibility they afford, and various representations useful for analysis,\ndesign and implementations. We do this for a generalization of second-order\nfilters which we refer to as rational-exponent Generalized Exponent Filters\n(GEFs) that are useful for a diverse array of applications. We present\nequivalent representations for rational-exponent GEFs in the time and frequency\ndomains: transfer functions, impulse responses, and integral expressions - the\nlast of which allows for efficient real-time processing without preprocessing\nrequirements. Rational-exponent filters enable filter characteristics to be on\na continuum rather than limiting them to discrete values thereby resulting in\ngreater flexibility in the behavior of these filters without additional\ncomplexity in causality and stability analyses compared with classical filters.\nIn the case of GEFs, this allows for having arbitrary continuous rather than\ndiscrete values for filter characteristics such as (1) the ratio of 3dB quality\nfactor to maximum group delay - particularly important for filterbanks which\nhave simultaneous requirements on frequency selectivity and synchronization;\nand (2) the ratio of 3dB to 15dB quality factors that dictates the shape of the\nfrequency response magnitude.", "published": "2024-04-05 10:08:44", "link": "http://arxiv.org/abs/2406.16877v3", "categories": ["eess.SP", "cs.SD", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "eess.SP"}
