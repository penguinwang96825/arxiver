{"title": "LEPOR: An Augmented Machine Translation Evaluation Metric", "abstract": "Machine translation (MT) was developed as one of the hottest research topics\nin the natural language processing (NLP) literature. One important issue in MT\nis that how to evaluate the MT system reasonably and tell us whether the\ntranslation system makes an improvement or not. The traditional manual judgment\nmethods are expensive, time-consuming, unrepeatable, and sometimes with low\nagreement. On the other hand, the popular automatic MT evaluation methods have\nsome weaknesses. Firstly, they tend to perform well on the language pairs with\nEnglish as the target language, but weak when English is used as source.\nSecondly, some methods rely on many additional linguistic features to achieve\ngood performance, which makes the metric unable to replicate and apply to other\nlanguage pairs easily. Thirdly, some popular metrics utilize incomprehensive\nfactors, which result in low performance on some practical tasks. In this\nthesis, to address the existing problems, we design novel MT evaluation methods\nand investigate their performances on different languages. Firstly, we design\naugmented factors to yield highly accurate evaluation. Secondly, we design a\ntunable evaluation model where weighting of factors can be optimized according\nto the characteristics of languages. Thirdly, in the enhanced version of our\nmethods, we design concise linguistic feature using part-of-speech (POS) to\nshow that our methods can yield even higher performance when using some\nexternal linguistic resources. Finally, we introduce the practical performance\nof our metrics in the ACL-WMT workshop shared tasks, which show that the\nproposed methods are robust across different languages. In addition, we also\npresent some novel work on quality estimation of MT without using reference\ntranslations including the usage of probability models of Na\\\"ive Bayes (NB),\nsupport vector machine (SVM) classification algorithms, and CRFs.", "published": "2017-03-26 00:30:38", "link": "http://arxiv.org/abs/1703.08748v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Simpler Language Models with the Differential State Framework", "abstract": "Learning useful information across long time lags is a critical and difficult\nproblem for temporal neural models in tasks such as language modeling. Existing\narchitectures that address the issue are often complex and costly to train. The\nDifferential State Framework (DSF) is a simple and high-performing design that\nunifies previously introduced gated neural models. DSF models maintain\nlonger-term memory by learning to interpolate between a fast-changing\ndata-driven representation and a slowly changing, implicitly stable state. This\nrequires hardly any more parameters than a classical, simple recurrent network.\nWithin the DSF framework, a new architecture is presented, the Delta-RNN. In\nlanguage modeling at the word and character levels, the Delta-RNN outperforms\npopular complex architectures, such as the Long Short Term Memory (LSTM) and\nthe Gated Recurrent Unit (GRU), and, when regularized, performs comparably to\nseveral state-of-the-art baselines. At the subword level, the Delta-RNN's\nperformance is comparable to that of complex gated architectures.", "published": "2017-03-26 20:02:44", "link": "http://arxiv.org/abs/1703.08864v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering from Unstructured Text by Retrieval and Comprehension", "abstract": "Open domain Question Answering (QA) systems must interact with external\nknowledge sources, such as web pages, to find relevant information. Information\nsources like Wikipedia, however, are not well structured and difficult to\nutilize in comparison with Knowledge Bases (KBs). In this work we present a\ntwo-step approach to question answering from unstructured text, consisting of a\nretrieval step and a comprehension step. For comprehension, we present an RNN\nbased attention model with a novel mixture mechanism for selecting answers from\neither retrieved articles or a fixed vocabulary. For retrieval we introduce a\nhand-crafted model and a neural model for ranking relevant articles. We achieve\nstate-of-the-art performance on W IKI M OVIES dataset, reducing the error by\n40%. Our experimental results further demonstrate the importance of each of the\nintroduced components.", "published": "2017-03-26 23:48:06", "link": "http://arxiv.org/abs/1703.08885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
