{"title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language\n  Model", "abstract": "Recent breakthroughs of pretrained language models have shown the\neffectiveness of self-supervised learning for a wide range of natural language\nprocessing (NLP) tasks. In addition to standard syntactic and semantic NLP\ntasks, pretrained models achieve strong improvements on tasks that involve\nreal-world knowledge, suggesting that large-scale language modeling could be an\nimplicit method to capture knowledge. In this work, we further investigate the\nextent to which pretrained models such as BERT capture knowledge using a\nzero-shot fact completion task. Moreover, we propose a simple yet effective\nweakly supervised pretraining objective, which explicitly forces the model to\nincorporate knowledge about real-world entities. Models trained with our new\nobjective yield significant improvements on the fact completion task. When\napplied to downstream tasks, our model consistently outperforms BERT on four\nentity-related question answering datasets (i.e., WebQuestions, TriviaQA,\nSearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard\nfine-grained entity typing dataset (i.e., FIGER) with 5.7 accuracy gains.", "published": "2019-12-20 04:25:48", "link": "http://arxiv.org/abs/1912.09637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SberQuAD -- Russian Reading Comprehension Dataset: Description and\n  Analysis", "abstract": "SberQuAD -- a large scale analog of Stanford SQuAD in the Russian language -\nis a valuable resource that has not been properly presented to the scientific\ncommunity. We fill this gap by providing a description, a thorough analysis,\nand baseline experimental results.", "published": "2019-12-20 09:44:42", "link": "http://arxiv.org/abs/1912.09723v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When to Talk: Chatbot Controls the Timing of Talking during Multi-turn\n  Open-domain Dialogue Generation", "abstract": "Despite the multi-turn open-domain dialogue systems have attracted more and\nmore attention and made great progress, the existing dialogue systems are still\nvery boring. Nearly all the existing dialogue models only provide a response\nwhen the user's utterance is accepted. But during daily conversations, humans\nalways decide whether to continue to utter an utterance based on the context.\nIntuitively, a dialogue model that can control the timing of talking\nautonomously based on the conversation context can chat with humans more\nnaturally. In this paper, we explore the dialogue system that automatically\ncontrols the timing of talking during the conversation. Specifically, we adopt\nthe decision module for the existing dialogue models. Furthermore, modeling\nconversation context effectively is very important for controlling the timing\nof talking. So we also adopt the graph neural networks to process the context\nwith the natural graph structure. Extensive experiments on two benchmarks show\nthat controlling the timing of talking can effectively improve the quality of\ndialogue generation, and the proposed methods significantly improve the\naccuracy of the timing of talking. In addition, we have publicly released the\ncodes of our proposed model.", "published": "2019-12-20 15:25:57", "link": "http://arxiv.org/abs/1912.09879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Character Embeddings: Learning Phonological and Semantic\n  Representations in Languages of Logographic Origin using Recursive Neural\n  Networks", "abstract": "Logographs (Chinese characters) have recursive structures (i.e. hierarchies\nof sub-units in logographs) that contain phonological and semantic information,\nas developmental psychology literature suggests that native speakers leverage\non the structures to learn how to read. Exploiting these structures could\npotentially lead to better embeddings that can benefit many downstream tasks.\nWe propose building hierarchical logograph (character) embeddings from\nlogograph recursive structures using treeLSTM, a recursive neural network.\nUsing recursive neural network imposes a prior on the mapping from logographs\nto embeddings since the network must read in the sub-units in logographs\naccording to the order specified by the recursive structures. Based on human\nbehavior in language learning and reading, we hypothesize that modeling\nlogographs' structures using recursive neural network should be beneficial. To\nverify this claim, we consider two tasks (1) predicting logographs' Cantonese\npronunciation from logographic structures and (2) language modeling. Empirical\nresults show that the proposed hierarchical embeddings outperform baseline\napproaches. Diagnostic analysis suggests that hierarchical embeddings\nconstructed using treeLSTM is less sensitive to distractors, thus is more\nrobust, especially on complex logographs.", "published": "2019-12-20 16:24:23", "link": "http://arxiv.org/abs/1912.09913v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Intent, Dialog Policies and Response Adaptation for\n  Goal-Oriented Interactions", "abstract": "Building a machine learning driven spoken dialog system for goal-oriented\ninteractions involves careful design of intents and data collection along with\ndevelopment of intent recognition models and dialog policy learning algorithms.\nThe models should be robust enough to handle various user distractions during\nthe interaction flow and should steer the user back into an engaging\ninteraction for successful completion of the interaction. In this work, we have\ndesigned a goal-oriented interaction system where children can engage with\nagents for a series of interactions involving `Meet \\& Greet' and `Simon Says'\ngame play. We have explored various feature extractors and models for improved\nintent recognition and looked at leveraging previous user and system\ninteractions in novel ways with attention models. We have also looked at dialog\nadaptation methods for entrained response selection. Our bootstrapped models\nfrom limited training data perform better than many baseline approaches we have\nlooked at for intent recognition and dialog action prediction.", "published": "2019-12-20 22:53:18", "link": "http://arxiv.org/abs/1912.10130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Context, Attention and Audio Features for Audio Visual\n  Scene-Aware Dialog", "abstract": "We are witnessing a confluence of vision, speech and dialog system\ntechnologies that are enabling the IVAs to learn audio-visual groundings of\nutterances and have conversations with users about the objects, activities and\nevents surrounding them. Recent progress in visual grounding techniques and\nAudio Understanding are enabling machines to understand shared semantic\nconcepts and listen to the various sensory events in the environment. With\naudio and visual grounding methods, end-to-end multimodal SDS are trained to\nmeaningfully communicate with us in natural language about the real dynamic\naudio-visual sensory world around us. In this work, we explore the role of\n`topics' as the context of the conversation along with multimodal attention\ninto such an end-to-end audio-visual scene-aware dialog system architecture. We\nalso incorporate an end-to-end audio classification ConvNet, AclNet, into our\nmodels. We develop and test our approaches on the Audio Visual Scene-Aware\nDialog (AVSD) dataset released as a part of the DSTC7. We present the analysis\nof our experiments and show that some of our model variations outperform the\nbaseline system released for AVSD.", "published": "2019-12-20 22:56:54", "link": "http://arxiv.org/abs/1912.10132v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using\n  Financial News", "abstract": "Stock price prediction is important for value investments in the stock\nmarket. In particular, short-term prediction that exploits financial news\narticles is promising in recent years. In this paper, we propose a novel deep\nneural network DP-LSTM for stock price prediction, which incorporates the news\narticles as hidden information and integrates difference news sources through\nthe differential privacy mechanism. First, based on the autoregressive moving\naverage model (ARMA), a sentiment-ARMA is formulated by taking into\nconsideration the information of financial news articles in the model. Then, an\nLSTM-based deep neural network is designed, which consists of three components:\nLSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM\nscheme can reduce prediction errors and increase the robustness. Extensive\nexperiments on S&P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32%\nimprovement in mean MPA of prediction result, and (ii) for the prediction of\nthe market index S&P 500, we achieve up to 65.79% improvement in MSE.", "published": "2019-12-20 02:52:27", "link": "http://arxiv.org/abs/1912.10806v1", "categories": ["q-fin.ST", "cs.CL"], "primary_category": "q-fin.ST"}
{"title": "End-to-end Named Entity Recognition and Relation Extraction using\n  Pre-trained Language Models", "abstract": "Named entity recognition (NER) and relation extraction (RE) are two important\ntasks in information extraction and retrieval (IE \\& IR). Recent work has\ndemonstrated that it is beneficial to learn these tasks jointly, which avoids\nthe propagation of error inherent in pipeline-based systems and improves\nperformance. However, state-of-the-art joint models typically rely on external\nnatural language processing (NLP) tools, such as dependency parsers, limiting\ntheir usefulness to domains (e.g. news) where those tools perform well. The few\nneural, end-to-end models that have been proposed are trained almost completely\nfrom scratch. In this paper, we propose a neural, end-to-end model for jointly\nextracting entities and their relations which does not rely on external NLP\ntools and which integrates a large, pre-trained language model. Because the\nbulk of our model's parameters are pre-trained and we eschew recurrence for\nself-attention, our model is fast to train. On 5 datasets across 3 domains, our\nmodel matches or exceeds state-of-the-art performance, sometimes by a large\nmargin.", "published": "2019-12-20 19:47:56", "link": "http://arxiv.org/abs/1912.13415v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Voice Interactive Multilingual Student Support System using IBM Watson", "abstract": "Systems powered by artificial intelligence are being developed to be more\nuser-friendly by communicating with users in a progressively human-like\nconversational way. Chatbots, also known as dialogue systems, interactive\nconversational agents, or virtual agents are an example of such systems used in\na wide variety of applications ranging from customer support in the business\ndomain to companionship in the healthcare sector. It is becoming increasingly\nimportant to develop chatbots that can best respond to the personalized needs\nof their users so that they can be as helpful to the user as possible in a real\nhuman way. This paper investigates and compares three popular existing chatbots\nAPI offerings and then propose and develop a voice interactive and multilingual\nchatbot that can effectively respond to users mood, tone, and language using\nIBM Watson Assistant, Tone Analyzer, and Language Translator. The chatbot was\nevaluated using a use case that was targeted at responding to users needs\nregarding exam stress based on university students survey data generated using\nGoogle Forms. The results of measuring the chatbot effectiveness at analyzing\nresponses regarding exam stress indicate that the chatbot responding\nappropriately to the user queries regarding how they are feeling about exams\n76.5%. The chatbot could also be adapted for use in other application areas\nsuch as student info-centers, government kiosks, and mental health support\nsystems.", "published": "2019-12-20 18:58:25", "link": "http://arxiv.org/abs/2001.00471v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.HC"}
{"title": "Shareable Representations for Search Query Understanding", "abstract": "Understanding search queries is critical for shopping search engines to\ndeliver a satisfying customer experience. Popular shopping search engines\nreceive billions of unique queries yearly, each of which can depict any of\nhundreds of user preferences or intents. In order to get the right results to\ncustomers it must be known queries like \"inexpensive prom dresses\" are intended\nto not only surface results of a certain product type but also products with a\nlow price. Referred to as query intents, examples also include preferences for\nauthor, brand, age group, or simply a need for customer service. Recent works\nsuch as BERT have demonstrated the success of a large transformer encoder\narchitecture with language model pre-training on a variety of NLP tasks. We\nadapt such an architecture to learn intents for search queries and describe\nmethods to account for the noisiness and sparseness of search query data. We\nalso describe cost effective ways of hosting transformer encoder models in\ncontext with low latency requirements. With the right domain-specific training\nwe can build a shareable deep learning model whose internal representation can\nbe reused for a variety of query understanding tasks including query intent\nidentification. Model sharing allows for fewer large models needed to be served\nat inference time and provides a platform to quickly build and roll out new\nsearch query classifiers.", "published": "2019-12-20 22:12:47", "link": "http://arxiv.org/abs/2001.04345v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Measuring Compositional Generalization: A Comprehensive Method on\n  Realistic Data", "abstract": "State-of-the-art machine learning methods exhibit limited compositional\ngeneralization. At the same time, there is a lack of realistic benchmarks that\ncomprehensively measure this ability, which makes it challenging to find and\nevaluate improvements. We introduce a novel method to systematically construct\nsuch benchmarks by maximizing compound divergence while guaranteeing a small\natom divergence between train and test sets, and we quantitatively compare this\nmethod to other approaches for creating compositional generalization\nbenchmarks. We present a large and realistic natural language question\nanswering dataset that is constructed according to this method, and we use it\nto analyze the compositional generalization ability of three machine learning\narchitectures. We find that they fail to generalize compositionally and that\nthere is a surprisingly strong negative correlation between compound divergence\nand accuracy. We also demonstrate how our method can be used to create new\ncompositionality benchmarks on top of the existing SCAN dataset, which confirms\nthese findings.", "published": "2019-12-20 09:32:41", "link": "http://arxiv.org/abs/1912.09713v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Probability Calibration for Knowledge Graph Embedding Models", "abstract": "Knowledge graph embedding research has overlooked the problem of probability\ncalibration. We show popular embedding models are indeed uncalibrated. That\nmeans probability estimates associated to predicted triples are unreliable. We\npresent a novel method to calibrate a model when ground truth negatives are not\navailable, which is the usual case in knowledge graphs. We propose to use Platt\nscaling and isotonic regression alongside our method. Experiments on three\ndatasets with ground truth negatives show our contribution leads to\nwell-calibrated models when compared to the gold standard of using negatives.\nWe get significantly better results than the uncalibrated models from all\ncalibration methods. We show isotonic regression offers the best the\nperformance overall, not without trade-offs. We also show that calibrated\nmodels reach state-of-the-art accuracy without the need to define\nrelation-specific decision thresholds.", "published": "2019-12-20 18:31:33", "link": "http://arxiv.org/abs/1912.10000v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Hierarchical Model for Data-to-Text Generation", "abstract": "Transcribing structured data into natural language descriptions has emerged\nas a challenging task, referred to as \"data-to-text\". These structures\ngenerally regroup multiple elements, as well as their attributes. Most attempts\nrely on translation encoder-decoder methods which linearize elements into a\nsequence. This however loses most of the structure contained in the data. In\nthis work, we propose to overpass this limitation with a hierarchical model\nthat encodes the data-structure at the element-level and the structure level.\nEvaluations on RotoWire show the effectiveness of our model w.r.t. qualitative\nand quantitative metrics.", "published": "2019-12-20 18:41:32", "link": "http://arxiv.org/abs/1912.10011v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Singing From Speech", "abstract": "We propose an algorithm that is capable of synthesizing high quality target\nspeaker's singing voice given only their normal speech samples. The proposed\nalgorithm first integrate speech and singing synthesis into a unified\nframework, and learns universal speaker embeddings that are shareable between\nspeech and singing synthesis tasks. Specifically, the speaker embeddings\nlearned from normal speech via the speech synthesis objective are shared with\nthose learned from singing samples via the singing synthesis objective in the\nunified training framework. This makes the learned speaker embedding a\ntransferable representation for both speaking and singing. We evaluate the\nproposed algorithm on singing voice conversion task where the content of\noriginal singing is covered with the timbre of another speaker's voice learned\npurely from their normal speech samples. Our experiments indicate that the\nproposed algorithm generates high-quality singing voices that sound highly\nsimilar to target speaker's voice given only his or her normal speech samples.\nWe believe that proposed algorithm will open up new opportunities for singing\nsynthesis and conversion for broader users and applications.", "published": "2019-12-20 22:45:23", "link": "http://arxiv.org/abs/1912.10128v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Topics and Audio Features with Multimodal Attention for Audio\n  Visual Scene-Aware Dialog", "abstract": "With the recent advancements in Artificial Intelligence (AI), Intelligent\nVirtual Assistants (IVA) such as Alexa, Google Home, etc., have become a\nubiquitous part of many homes. Currently, such IVAs are mostly audio-based, but\ngoing forward, we are witnessing a confluence of vision, speech and dialog\nsystem technologies that are enabling the IVAs to learn audio-visual groundings\nof utterances. This will enable agents to have conversations with users about\nthe objects, activities and events surrounding them. In this work, we present\nthree main architectural explorations for the Audio Visual Scene-Aware Dialog\n(AVSD): 1) investigating `topics' of the dialog as an important contextual\nfeature for the conversation, 2) exploring several multimodal attention\nmechanisms during response generation, 3) incorporating an end-to-end audio\nclassification ConvNet, AclNet, into our architecture. We discuss detailed\nanalysis of the experimental results and show that our model variations\noutperform the baseline system presented for the AVSD task.", "published": "2019-12-20 22:55:40", "link": "http://arxiv.org/abs/1912.10131v1", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "What do Asian Religions Have in Common? An Unsupervised Text Analytics\n  Exploration", "abstract": "The main source of various religious teachings is their sacred texts which\nvary from religion to religion based on different factors like the geographical\nlocation or time of the birth of a particular religion. Despite these\ndifferences, there could be similarities between the sacred texts based on what\nlessons it teaches to its followers. This paper attempts to find the similarity\nusing text mining techniques. The corpus consisting of Asian (Tao Te Ching,\nBuddhism, Yogasutra, Upanishad) and non-Asian (four Bible texts) is used to\nexplore findings of similarity measures like Euclidean, Manhattan, Jaccard and\nCosine on raw Document Term Frequency [DTM], normalized DTM which reveals\nsimilarity based on word usage. The performance of Supervised learning\nalgorithms like K-Nearest Neighbor [KNN], Support Vector Machine [SVM] and\nRandom Forest is measured based on its accuracy to predict correct scared text\nfor any given chapter in the corpus. The K-means clustering visualizations on\nEuclidean distances of raw DTM reveals that there exists a pattern of\nsimilarity among these sacred texts with Upanishads and Tao Te Ching is the\nmost similar text in the corpus.", "published": "2019-12-20 18:28:29", "link": "http://arxiv.org/abs/1912.10847v1", "categories": ["cs.CL", "cs.LG", "stat.AP"], "primary_category": "cs.CL"}
{"title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities\n  for Automated Financial Information Filtering", "abstract": "Multimodal analysis that uses numerical time series and textual corpora as\ninput data sources is becoming a promising approach, especially in the\nfinancial industry. However, the main focus of such analysis has been on\nachieving high prediction accuracy while little effort has been spent on the\nimportant task of understanding the association between the two data\nmodalities. Performance on the time series hence receives little explanation\nthough human-understandable textual information is available. In this work, we\naddress the problem of given a numerical time series, and a general corpus of\ntextual stories collected in the same period of the time series, the task is to\ntimely discover a succinct set of textual stories associated with that time\nseries. Towards this goal, we propose a novel multi-modal neural model called\nMSIN that jointly learns both numerical time series and categorical text\narticles in order to unearth the association between them. Through multiple\nsteps of data interrelation between the two data modalities, MSIN learns to\nfocus on a small subset of text articles that best align with the performance\nin the time series. This succinct set is timely discovered and presented as\nrecommended documents, acting as automated information filtering, for the given\ntime series. We empirically evaluate the performance of our model on\ndiscovering relevant news articles for two stock time series from Apple and\nGoogle companies, along with the daily news articles collected from the Thomson\nReuters over a period of seven consecutive years. The experimental results\ndemonstrate that MSIN achieves up to 84.9% and 87.2% in recalling the ground\ntruth articles respectively to the two examined time series, far more superior\nto state-of-the-art algorithms that rely on conventional attention mechanism in\ndeep learning.", "published": "2019-12-20 14:37:27", "link": "http://arxiv.org/abs/1912.10858v1", "categories": ["cs.CL", "cs.LG", "q-fin.ST", "stat.ML"], "primary_category": "cs.CL"}
