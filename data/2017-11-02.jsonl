{"title": "SRL4ORL: Improving Opinion Role Labeling using Multi-task Learning with\n  Semantic Role Labeling", "abstract": "For over a decade, machine learning has been used to extract\nopinion-holder-target structures from text to answer the question \"Who\nexpressed what kind of sentiment towards what?\". Recent neural approaches do\nnot outperform the state-of-the-art feature-based models for Opinion Role\nLabeling (ORL). We suspect this is due to the scarcity of labeled training data\nand address this issue using different multi-task learning (MTL) techniques\nwith a related task which has substantially more data, i.e. Semantic Role\nLabeling (SRL). We show that two MTL models improve significantly over the\nsingle-task model for labeling of both holders and targets, on the development\nand the test sets. We found that the vanilla MTL model which makes predictions\nusing only shared ORL and SRL features, performs the best. With deeper analysis\nwe determine what works and what might be done to make further improvements for\nORL.", "published": "2017-11-02 14:47:00", "link": "http://arxiv.org/abs/1711.00768v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Mention Learning for Reading Comprehension with Neural Cascades", "abstract": "Reading comprehension is a challenging task, especially when executed across\nlonger or across multiple evidence documents, where the answer is likely to\nreoccur. Existing neural architectures typically do not scale to the entire\nevidence, and hence, resort to selecting a single passage in the document\n(either via truncation or other means), and carefully searching for the answer\nwithin that passage. However, in some cases, this strategy can be suboptimal,\nsince by focusing on a specific passage, it becomes difficult to leverage\nmultiple mentions of the same answer throughout the document. In this work, we\ntake a different approach by constructing lightweight models that are combined\nin a cascade to find the answer. Each submodel consists only of feed-forward\nnetworks equipped with an attention mechanism, making it trivially\nparallelizable. We show that our approach can scale to approximately an order\nof magnitude larger evidence documents and can aggregate information at the\nrepresentation level from multiple mentions of each answer candidate across the\ndocument. Empirically, our approach achieves state-of-the-art performance on\nboth the Wikipedia and web domains of the TriviaQA dataset, outperforming more\ncomplex, recurrent architectures.", "published": "2017-11-02 19:13:55", "link": "http://arxiv.org/abs/1711.00894v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparison of Feature-Based and Neural Scansion of Poetry", "abstract": "Automatic analysis of poetic rhythm is a challenging task that involves\nlinguistics, literature, and computer science. When the language to be analyzed\nis known, rule-based systems or data-driven methods can be used. In this paper,\nwe analyze poetic rhythm in English and Spanish. We show that the\nrepresentations of data learned from character-based neural models are more\ninformative than the ones from hand-crafted features, and that a\nBi-LSTM+CRF-model produces state-of-the art accuracy on scansion of poetry in\ntwo languages. Results also show that the information about whole word\nstructure, and not just independent syllables, is highly informative for\nperforming scansion.", "published": "2017-11-02 21:15:46", "link": "http://arxiv.org/abs/1711.00938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting an English-Persian Parallel Corpus from Comparable Corpora", "abstract": "Parallel data are an important part of a reliable Statistical Machine\nTranslation (SMT) system. The more of these data are available, the better the\nquality of the SMT system. However, for some language pairs such as\nPersian-English, parallel sources of this kind are scarce. In this paper, a\nbidirectional method is proposed to extract parallel sentences from English and\nPersian document aligned Wikipedia. Two machine translation systems are\nemployed to translate from Persian to English and the reverse after which an IR\nsystem is used to measure the similarity of the translated sentences. Adding\nthe extracted sentences to the training data of the existing SMT systems is\nshown to improve the quality of the translation. Furthermore, the proposed\nmethod slightly outperforms the one-directional approach. The extracted corpus\nconsists of about 200,000 sentences which have been sorted by their degree of\nsimilarity calculated by the IR system and is freely available for public\naccess on the Web.", "published": "2017-11-02 11:00:09", "link": "http://arxiv.org/abs/1711.00681v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Hi, how can I help you?: Automating enterprise IT support help desks", "abstract": "Question answering is one of the primary challenges of natural language\nunderstanding. In realizing such a system, providing complex long answers to\nquestions is a challenging task as opposed to factoid answering as the former\nneeds context disambiguation. The different methods explored in the literature\ncan be broadly classified into three categories namely: 1) classification\nbased, 2) knowledge graph based and 3) retrieval based. Individually, none of\nthem address the need of an enterprise wide assistance system for an IT support\nand maintenance domain. In this domain the variance of answers is large ranging\nfrom factoid to structured operating procedures; the knowledge is present\nacross heterogeneous data sources like application specific documentation,\nticket management systems and any single technique for a general purpose\nassistance is unable to scale for such a landscape. To address this, we have\nbuilt a cognitive platform with capabilities adopted for this domain. Further,\nwe have built a general purpose question answering system leveraging the\nplatform that can be instantiated for multiple products, technologies in the\nsupport domain. The system uses a novel hybrid answering model that\norchestrates across a deep learning classifier, a knowledge graph based context\ndisambiguation module and a sophisticated bag-of-words search system. This\norchestration performs context switching for a provided question and also does\na smooth hand-off of the question to a human expert if none of the automated\ntechniques can provide a confident answer. This system has been deployed across\n675 internal enterprise IT support and maintenance projects.", "published": "2017-11-02 20:04:06", "link": "http://arxiv.org/abs/1711.02012v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon", "abstract": "We propose a neural language model capable of unsupervised syntactic\nstructure induction. The model leverages the structure information to form\nbetter semantic representations and better language modeling. Standard\nrecurrent neural networks are limited by their structure and fail to\nefficiently use syntactic information. On the other hand, tree-structured\nrecursive networks usually require additional structural supervision at the\ncost of human expert annotation. In this paper, We propose a novel neural\nlanguage model, called the Parsing-Reading-Predict Networks (PRPN), that can\nsimultaneously induce the syntactic structure from unannotated sentences and\nleverage the inferred structure to learn a better language model. In our model,\nthe gradient can be directly back-propagated from the language model loss into\nthe neural parsing network. Experiments show that the proposed model can\ndiscover the underlying syntactic structure and achieve state-of-the-art\nperformance on word/character-level language model tasks.", "published": "2017-11-02 23:02:52", "link": "http://arxiv.org/abs/1711.02013v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Audio Set classification with attention model: A probabilistic\n  perspective", "abstract": "This paper investigates the classification of the Audio Set dataset. Audio\nSet is a large scale weakly labelled dataset of sound clips. Previous work used\nmultiple instance learning (MIL) to classify weakly labelled data. In MIL, a\nbag consists of several instances, and a bag is labelled positive if at least\none instances in the audio clip is positive. A bag is labelled negative if all\nthe instances in the bag are negative. We propose an attention model to tackle\nthe MIL problem and explain this attention model from a novel probabilistic\nperspective. We define a probability space on each bag, where each instance in\nthe bag has a trainable probability measure for each class. Then the\nclassification of a bag is the expectation of the classification output of the\ninstances in the bag with respect to the learned probability measure.\nExperimental results show that our proposed attention model modeled by fully\nconnected deep neural network obtains mAP of 0.327 on Audio Set dataset,\noutperforming the Google's baseline of 0.314 and recurrent neural network of\n0.325.", "published": "2017-11-02 20:40:29", "link": "http://arxiv.org/abs/1711.00927v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Framework for evaluation of sound event detection in web videos", "abstract": "The largest source of sound events is web videos. Most videos lack sound\nevent labels at segment level, however, a significant number of them do respond\nto text queries, from a match found using metadata by search engines. In this\npaper we explore the extent to which a search query can be used as the true\nlabel for detection of sound events in videos. We present a framework for\nlarge-scale sound event recognition on web videos. The framework crawls videos\nusing search queries corresponding to 78 sound event labels drawn from three\ndatasets. The datasets are used to train three classifiers, and we obtain a\nprediction on 3.7 million web video segments. We evaluated performance using\nthe search query as true label and compare it with human labeling. Both types\nof ground truth exhibited close performance, to within 10%, and similar\nperformance trend with increasing number of evaluated segments. Hence, our\nexperiments show potential for using search query as a preliminary true label\nfor sound event recognition in web videos.", "published": "2017-11-02 16:32:23", "link": "http://arxiv.org/abs/1711.00804v2", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Does Phase Matter For Monaural Source Separation?", "abstract": "The \"cocktail party\" problem of fully separating multiple sources from a\nsingle channel audio waveform remains unsolved. Current biological\nunderstanding of neural encoding suggests that phase information is preserved\nand utilized at every stage of the auditory pathway. However, current\ncomputational approaches primarily discard phase information in order to mask\namplitude spectrograms of sound. In this paper, we seek to address whether\npreserving phase information in spectral representations of sound provides\nbetter results in monaural separation of vocals from a musical track by using a\nneurally plausible sparse generative model. Our results demonstrate that\npreserving phase information reduces artifacts in the separated tracks, as\nquantified by the signal to artifact ratio (GSAR). Furthermore, our proposed\nmethod achieves state-of-the-art performance for source separation, as\nquantified by a mean signal to interference ratio (GSIR) of 19.46.", "published": "2017-11-02 20:10:00", "link": "http://arxiv.org/abs/1711.00913v1", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Identification of potential Music Information Retrieval technologies for\n  computer-aided jingju singing training", "abstract": "Music Information Retrieval (MIR) technologies have been proven useful in\nassisting western classical singing training. Jingju (also known as Beijing or\nPeking opera) singing is different from western singing in terms of most of the\nperceptual dimensions, and the trainees are taught by using mouth/heart method.\nIn this paper, we first present the training method used in the professional\njingju training classroom scenario and show the potential benefits of\nintroducing the MIR technologies into the training process. The main part of\nthis paper dedicates to identify the potential MIR technologies for jingju\nsinging training. To this intent, we answer the question: how the jingju\nsinging tutors and trainees value the importance of each jingju musical\ndimension-intonation, rhythm, loudness, tone quality and pronunciation? This is\ndone by (i) classifying the classroom singing practices, tutor's verbal\nfeedbacks into these 5 dimensions, (ii) surveying the trainees. Then, with the\nhelp of the music signal analysis, a finer inspection on the classroom practice\nrecording examples reveals the detailed elements in the training process.\nFinally, based on the above analysis, several potential MIR technologies are\nidentified and would be useful for the jingju singing training.", "published": "2017-11-02 04:37:58", "link": "http://arxiv.org/abs/1711.07551v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
