{"title": "GNN-SL: Sequence Labeling Based on Nearest Examples via GNN", "abstract": "To better handle long-tail cases in the sequence labeling (SL) task, in this\nwork, we introduce graph neural networks sequence labeling (GNN-SL), which\naugments the vanilla SL model output with similar tagging examples retrieved\nfrom the whole training set. Since not all the retrieved tagging examples\nbenefit the model prediction, we construct a heterogeneous graph, and leverage\ngraph neural networks (GNNs) to transfer information between the retrieved\ntagging examples and the input word sequence. The augmented node which\naggregates information from neighbors is used to do prediction. This strategy\nenables the model to directly acquire similar tagging examples and improves the\ngeneral quality of predictions. We conduct a variety of experiments on three\ntypical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech\nTagging (POS), and Chinese Word Segmentation (CWS) to show the significant\nperformance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2)\non PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the\nCWS task, and results comparable to SOTA performances on NER datasets, and POS\ndatasets.", "published": "2022-12-05 04:22:00", "link": "http://arxiv.org/abs/2212.02017v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query Your Model with Definitions in FrameNet: An Effective Method for\n  Frame Semantic Role Labeling", "abstract": "Frame Semantic Role Labeling (FSRL) identifies arguments and labels them with\nframe semantic roles defined in FrameNet. Previous researches tend to divide\nFSRL into argument identification and role classification. Such methods usually\nmodel role classification as naive multi-class classification and treat\narguments individually, which neglects label semantics and interactions between\narguments and thus hindering performance and generalization of models. In this\npaper, we propose a query-based framework named ArGument Extractor with\nDefinitions in FrameNet (AGED) to mitigate these problems. Definitions of\nframes and frame elements (FEs) in FrameNet can be used to query arguments in\ntext. Encoding text-definition pairs can guide models in learning label\nsemantics and strengthening argument interactions. Experiments show that AGED\noutperforms previous state-of-the-art by up to 1.3 F1-score in two FrameNet\ndatasets and the generalization power of AGED in zero-shot and fewshot\nscenarios. Our code and technical appendix is available at\nhttps://github.com/PKUnlp-icler/AGED.", "published": "2022-12-05 05:09:12", "link": "http://arxiv.org/abs/2212.02036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of Domain-Adapted Multilingual Neural Machine Translation in the\n  Medical Domain", "abstract": "Multilingual Neural Machine Translation (MNMT) models leverage many language\npairs during training to improve translation quality for low-resource languages\nby transferring knowledge from high-resource languages. We study the quality of\na domain-adapted MNMT model in the medical domain for English-Romanian with\nautomatic metrics and a human error typology annotation which includes\nterminology-specific error categories. We compare the out-of-domain MNMT with\nthe in-domain adapted MNMT. The in-domain MNMT model outperforms the\nout-of-domain MNMT in all measured automatic metrics and produces fewer\nterminology errors.", "published": "2022-12-05 10:33:59", "link": "http://arxiv.org/abs/2212.02143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Video Games as a Corpus: Sentiment Analysis using Fallout New Vegas\n  Dialog", "abstract": "We present a method for extracting a multilingual sentiment annotated dialog\ndata set from Fallout New Vegas. The game developers have preannotated every\nline of dialog in the game in one of the 8 different sentiments: \\textit{anger,\ndisgust, fear, happy, neutral, pained, sad } and \\textit{surprised}. The game\nhas been translated into English, Spanish, German, French and Italian. We\nconduct experiments on multilingual, multilabel sentiment analysis on the\nextracted data set using multilingual BERT, XLMRoBERTa and language specific\nBERT models. In our experiments, multilingual BERT outperformed XLMRoBERTa for\nmost of the languages, also language specific models were slightly better than\nmultilingual BERT for most of the languages. The best overall accuracy was 54\\%\nand it was achieved by using multilingual BERT on Spanish data. The extracted\ndata set presents a challenging task for sentiment analysis. We have released\nthe data, including the testing and training splits, openly on Zenodo. The data\nset has been shuffled for copyright reasons.", "published": "2022-12-05 11:09:05", "link": "http://arxiv.org/abs/2212.02168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Factual News Headlines in Finnish", "abstract": "We present a novel approach to generating news headlines in Finnish for a\ngiven news story. We model this as a summarization task where a model is given\na news article, and its task is to produce a concise headline describing the\nmain topic of the article. Because there are no openly available GPT-2 models\nfor Finnish, we will first build such a model using several corpora. The model\nis then fine-tuned for the headline generation task using a massive news\ncorpus. The system is evaluated by 3 expert journalists working in a Finnish\nmedia house. The results showcase the usability of the presented approach as a\nheadline suggestion tool to facilitate the news production process.", "published": "2022-12-05 11:12:14", "link": "http://arxiv.org/abs/2212.02170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Momentum Decoding: Open-ended Text Generation As Graph Exploration", "abstract": "Open-ended text generation with autoregressive language models (LMs) is one\nof the core tasks in natural language processing. However, maximization-based\ndecoding methods (e.g., greedy/beam search) often lead to the degeneration\nproblem, i.e., the generated text is unnatural and contains undesirable\nrepetitions. Existing solutions to this problem either introduce randomness\nprone to incoherence or require a look-ahead mechanism that demands extra\ncomputational overhead. In this study, we formulate open-ended text generation\nfrom a new perspective, i.e., we view it as an exploration process within a\ndirected graph. Thereby, we understand the phenomenon of degeneration as\ncircular loops within the directed graph. Based on our formulation, we propose\na novel decoding method -- \\textit{momentum decoding} -- which encourages the\nLM to \\textit{greedily} explore new nodes outside the current graph. Meanwhile,\nit also allows the LM to return to the existing nodes with a momentum\ndowngraded by a pre-defined resistance function. We extensively test our\napproach on three benchmarks from different domains through automatic and human\nevaluations. The results show that momentum decoding performs comparably with\nthe current state of the art while enjoying notably improved inference speed\nand computation FLOPs. Furthermore, we conduct a detailed analysis to reveal\nthe merits and inner workings of our approach. Our codes and other related\nresources are publicly available at\nhttps://github.com/gmftbyGMFTBY/MomentumDecoding.", "published": "2022-12-05 11:16:47", "link": "http://arxiv.org/abs/2212.02175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Few-Shot Performance of Language Models via Nearest Neighbor\n  Calibration", "abstract": "Pre-trained language models (PLMs) have exhibited remarkable few-shot\nlearning capabilities when provided a few examples in a natural language prompt\nas demonstrations of test instances, i.e., in-context learning. However, the\nperformance of in-context learning is susceptible to the choice of prompt\nformat, training examples and the ordering of the training examples. In this\npaper, we propose a novel nearest-neighbor calibration framework for in-context\nlearning to ease this issue. It is inspired by a phenomenon that the in-context\nlearning paradigm produces incorrect labels when inferring training instances,\nwhich provides a useful supervised signal to calibrate predictions. Thus, our\nmethod directly augments the predictions with a $k$-nearest-neighbor ($k$NN)\nclassifier over a datastore of cached few-shot instance representations\nobtained by PLMs and their corresponding labels. Then adaptive neighbor\nselection and feature regularization modules are introduced to make full use of\na few support instances to reduce the $k$NN retrieval noise. Experiments on\nvarious few-shot text classification tasks demonstrate that our method\nsignificantly improves in-context learning, while even achieving comparable\nperformance with state-of-the-art tuning-based approaches in some sentiment\nanalysis tasks.", "published": "2022-12-05 12:49:41", "link": "http://arxiv.org/abs/2212.02216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake News and Hate Speech: Language in Common", "abstract": "In this paper we raise the research question of whether fake news and hate\nspeech spreaders share common patterns in language. We compute a novel index,\nthe ingroup vs outgroup index, in three different datasets and we show that\nboth phenomena share an \"us vs them\" narrative.", "published": "2022-12-05 15:35:10", "link": "http://arxiv.org/abs/2212.02352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Distribution Shift at Test Time in Pre-trained Language\n  Models", "abstract": "State-of-the-art pre-trained language models (PLMs) outperform other models\nwhen applied to the majority of language processing tasks. However, PLMs have\nbeen found to degrade in performance under distribution shift, a phenomenon\nthat occurs when data at test-time does not come from the same distribution as\nthe source training set. Equally as challenging is the task of obtaining labels\nin real-time due to issues like long-labeling feedback loops. The lack of\nadequate methods that address the aforementioned challenges constitutes the\nneed for approaches that continuously adapt the PLM to a distinct distribution.\nUnsupervised domain adaptation adapts a source model to an unseen as well as\nunlabeled target domain. While some techniques such as data augmentation can\nadapt models in several scenarios, they have only been sparsely studied for\naddressing the distribution shift problem. In this work, we present an approach\n(MEMO-CL) that improves the performance of PLMs at test-time under distribution\nshift. Our approach takes advantage of the latest unsupervised techniques in\ndata augmentation and adaptation to minimize the entropy of the PLM's output\ndistribution. MEMO-CL operates on a batch of augmented samples from a single\nobservation in the test set. The technique introduced is unsupervised,\ndomain-agnostic, easy to implement, and requires no additional data. Our\nexperiments result in a 3% improvement over current test-time adaptation\nbaselines.", "published": "2022-12-05 16:04:54", "link": "http://arxiv.org/abs/2212.02384v1", "categories": ["cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "In-context Examples Selection for Machine Translation", "abstract": "Large-scale generative models show an impressive ability to perform a wide\nrange of Natural Language Processing (NLP) tasks using in-context learning,\nwhere a few examples are used to describe a task to the model. For Machine\nTranslation (MT), these examples are typically randomly sampled from the\ndevelopment dataset with a similar distribution as the evaluation set. However,\nit is unclear how the choice of these in-context examples and their ordering\nimpacts the output translation quality. In this work, we aim to understand the\nproperties of good in-context examples for MT in both in-domain and\nout-of-domain settings. We show that the translation quality and the domain of\nthe in-context examples matter and that 1-shot noisy unrelated example can have\na catastrophic impact on output quality. While concatenating multiple random\nexamples reduces the effect of noise, a single good prompt optimized to\nmaximize translation quality on the development dataset can elicit learned\ninformation from the pre-trained language model. Adding similar examples based\non an n-gram overlap with the test source significantly and consistently\nimproves the translation quality of the outputs, outperforming a strong kNN-MT\nbaseline in 2 out of 4 out-of-domain datasets.", "published": "2022-12-05 17:25:15", "link": "http://arxiv.org/abs/2212.02437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantized Wasserstein Procrustes Alignment of Word Embedding Spaces", "abstract": "Optimal Transport (OT) provides a useful geometric framework to estimate the\npermutation matrix under unsupervised cross-lingual word embedding (CLWE)\nmodels that pose the alignment task as a Wasserstein-Procrustes problem.\nHowever, linear programming algorithms and approximate OT solvers via Sinkhorn\nfor computing the permutation matrix come with a significant computational\nburden since they scale cubically and quadratically, respectively, in the input\nsize. This makes it slow and infeasible to compute OT distances exactly for a\nlarger input size, resulting in a poor approximation quality of the permutation\nmatrix and subsequently a less robust learned transfer function or mapper. This\npaper proposes an unsupervised projection-based CLWE model called quantized\nWasserstein Procrustes (qWP). qWP relies on a quantization step of both the\nsource and target monolingual embedding space to estimate the permutation\nmatrix given a cheap sampling procedure. This approach substantially improves\nthe approximation quality of empirical OT solvers given fixed computational\ncost. We demonstrate that qWP achieves state-of-the-art results on the\nBilingual lexicon Induction (BLI) task.", "published": "2022-12-05 18:23:59", "link": "http://arxiv.org/abs/2212.02468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta-Learning Fast Weight Language Models", "abstract": "Dynamic evaluation of language models (LMs) adapts model parameters at test\ntime using gradient information from previous tokens and substantially improves\nLM performance. However, it requires over 3x more compute than standard\ninference. We present Fast Weight Layers (FWLs), a neural component that\nprovides the benefits of dynamic evaluation much more efficiently by expressing\ngradient updates as linear attention. A key improvement over dynamic evaluation\nis that FWLs can also be applied at training time so the model learns to make\ngood use of gradient updates. FWLs can easily be added on top of existing\ntransformer models, require relatively little extra compute or memory to run,\nand significantly improve language modeling perplexity.", "published": "2022-12-05 18:37:09", "link": "http://arxiv.org/abs/2212.02475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INCLUSIFY: A benchmark and a model for gender-inclusive German", "abstract": "Gender-inclusive language is important for achieving gender equality in\nlanguages with gender inflections, such as German. While stirring some\ncontroversy, it is increasingly adopted by companies and political\ninstitutions. A handful of tools have been developed to help people use\ngender-inclusive language by identifying instances of the generic masculine and\nproviding suggestions for more inclusive reformulations. In this report, we\ndefine the underlying tasks in terms of natural language processing, and\npresent a dataset and measures for benchmarking them. We also present a model\nthat implements these tasks, by combining an inclusive language database with\nan elaborate sequence of processing steps via standard pre-trained models. Our\nmodel achieves a recall of 0.89 and a precision of 0.82 in our benchmark for\nidentifying exclusive language; and one of its top five suggestions is chosen\nin real-world texts in 44% of cases. We sketch how the area could be further\nadvanced by training end-to-end models and using large language models; and we\nurge the community to include more gender-inclusive texts in their training\ndata in order to not present an obstacle to the adoption of gender-inclusive\nlanguage. Through these efforts, we hope to contribute to restoring justice in\nlanguage and, to a small extent, in reality.", "published": "2022-12-05 19:37:48", "link": "http://arxiv.org/abs/2212.02564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "POQue: Asking Participant-specific Outcome Questions for a Deeper\n  Understanding of Complex Events", "abstract": "Knowledge about outcomes is critical for complex event understanding but is\nhard to acquire. We show that by pre-identifying a participant in a complex\nevent, crowd workers are able to (1) infer the collective impact of salient\nevents that make up the situation, (2) annotate the volitional engagement of\nparticipants in causing the situation, and (3) ground the outcome of the\nsituation in state changes of the participants. By creating a multi-step\ninterface and a careful quality control strategy, we collect a high quality\nannotated dataset of 8K short newswire narratives and ROCStories with high\ninter-annotator agreement (0.74-0.96 weighted Fleiss Kappa). Our dataset, POQue\n(Participant Outcome Questions), enables the exploration and development of\nmodels that address multiple aspects of semantic understanding. Experimentally,\nwe show that current language models lag behind human performance in subtle\nways through our task formulations that target abstract and specific\ncomprehension of a complex event, its outcome, and a participant's influence\nover the event culmination.", "published": "2022-12-05 22:23:27", "link": "http://arxiv.org/abs/2212.02629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-Based Named Entity Recognition for French Using Adversarial\n  Adaptation to Similar Domain Corpora", "abstract": "Named Entity Recognition (NER) involves the identification and classification\nof named entities in unstructured text into predefined classes. NER in\nlanguages with limited resources, like French, is still an open problem due to\nthe lack of large, robust, labelled datasets. In this paper, we propose a\ntransformer-based NER approach for French using adversarial adaptation to\nsimilar domain or general corpora for improved feature extraction and better\ngeneralization. We evaluate our approach on three labelled datasets and show\nthat our adaptation framework outperforms the corresponding non-adaptive models\nfor various combinations of transformer models, source datasets and target\ncorpora.", "published": "2022-12-05 23:33:36", "link": "http://arxiv.org/abs/2212.03692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Metadata Inference Using a Transducer Based Language Model", "abstract": "Solving the challenges of automatic machine translation of Building\nAutomation System text metadata is a crucial first step in efficiently\ndeploying smart building applications. The vocabulary used to describe building\nmetadata appears small compared to general natural languages, but each term has\nmultiple commonly used abbreviations. Conventional machine learning techniques\nare inefficient since they need to learn many different forms for the same\nword, and large amounts of data must be used to train these models. It is also\ndifficult to apply standard techniques such as tokenisation since this commonly\nresults in multiple output tags being associated with a single input token,\nsomething traditional sequence labelling models do not allow. Finite State\nTransducers can model sequence-to-sequence tasks where the input and output\nsequences are different lengths, and they can be combined with language models\nto ensure a valid output sequence is generated. We perform a preliminary\nanalysis into the use of transducer-based language models to parse and\nnormalise building point metadata.", "published": "2022-12-05 00:37:59", "link": "http://arxiv.org/abs/2212.01964v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Wish I Can Feel What You Feel: A Neural Approach for Empathetic Response\n  Generation", "abstract": "Expressing empathy is important in everyday conversations, and exploring how\nempathy arises is crucial in automatic response generation. Most previous\napproaches consider only a single factor that affects empathy. However, in\npractice, empathy generation and expression is a very complex and dynamic\npsychological process. A listener needs to find out events which cause a\nspeaker's emotions (emotion cause extraction), project the events into some\nexperience (knowledge extension), and express empathy in the most appropriate\nway (communication mechanism). To this end, we propose a novel approach, which\nintegrates the three components - emotion cause, knowledge graph, and\ncommunication mechanism for empathetic response generation. Experimental\nresults on the benchmark dataset demonstrate the effectiveness of our method\nand show that incorporating the key components generates more informative and\nempathetic responses.", "published": "2022-12-05 03:20:37", "link": "http://arxiv.org/abs/2212.02000v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Analysis of Utterance Embeddings and Clustering Methods Related to\n  Intent Induction for Task-Oriented Dialogue", "abstract": "The focus of this work is to investigate unsupervised approaches to overcome\nquintessential challenges in designing task-oriented dialog schema: assigning\nintent labels to each dialog turn (intent clustering) and generating a set of\nintents based on the intent clustering methods (intent induction). We postulate\nthere are two salient factors for automatic induction of intents: (1)\nclustering algorithm for intent labeling and (2) user utterance embedding\nspace. We compare existing off-the-shelf clustering models and embeddings based\non DSTC11 evaluation. Our extensive experiments demonstrate that the combined\nselection of utterance embedding and clustering method in the intent induction\ntask should be carefully considered. We also present that pretrained MiniLM\nwith Agglomerative clustering shows significant improvement in NMI, ARI, F1,\naccuracy and example coverage in intent induction tasks. The source codes are\navailable at https://github.com/Jeiyoon/dstc11-track2.", "published": "2022-12-05 04:37:22", "link": "http://arxiv.org/abs/2212.02021v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval as Attention: End-to-end Learning of Retrieval and Reading\n  within a Single Transformer", "abstract": "Systems for knowledge-intensive tasks such as open-domain question answering\n(QA) usually consist of two stages: efficient retrieval of relevant documents\nfrom a large corpus and detailed reading of the selected documents to generate\nanswers. Retrievers and readers are usually modeled separately, which\nnecessitates a cumbersome implementation and is hard to train and adapt in an\nend-to-end fashion. In this paper, we revisit this design and eschew the\nseparate architecture and training in favor of a single Transformer that\nperforms Retrieval as Attention (ReAtt), and end-to-end training solely based\non supervision from the end QA task. We demonstrate for the first time that a\nsingle model trained end-to-end can achieve both competitive retrieval and QA\nperformance, matching or slightly outperforming state-of-the-art separately\ntrained retrievers and readers. Moreover, end-to-end adaptation significantly\nboosts its performance on out-of-domain datasets in both supervised and\nunsupervised settings, making our model a simple and adaptable solution for\nknowledge-intensive tasks. Code and models are available at\nhttps://github.com/jzbjyb/ReAtt.", "published": "2022-12-05 04:51:21", "link": "http://arxiv.org/abs/2212.02027v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Syntactic Multi-view Learning for Open Information Extraction", "abstract": "Open Information Extraction (OpenIE) aims to extract relational tuples from\nopen-domain sentences. Traditional rule-based or statistical models have been\ndeveloped based on syntactic structures of sentences, identified by syntactic\nparsers. However, previous neural OpenIE models under-explore the useful\nsyntactic information. In this paper, we model both constituency and dependency\ntrees into word-level graphs, and enable neural OpenIE to learn from the\nsyntactic structures. To better fuse heterogeneous information from both\ngraphs, we adopt multi-view learning to capture multiple relationships from\nthem. Finally, the finetuned constituency and dependency representations are\naggregated with sentential semantic representations for tuple generation.\nExperiments show that both constituency and dependency information, and the\nmulti-view learning are effective.", "published": "2022-12-05 07:15:41", "link": "http://arxiv.org/abs/2212.02068v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Human-in-the-Loop Hate Speech Classification in a Multilingual Context", "abstract": "The shift of public debate to the digital sphere has been accompanied by a\nrise in online hate speech. While many promising approaches for hate speech\nclassification have been proposed, studies often focus only on a single\nlanguage, usually English, and do not address three key concerns:\npost-deployment performance, classifier maintenance and infrastructural\nlimitations. In this paper, we introduce a new human-in-the-loop BERT-based\nhate speech classification pipeline and trace its development from initial data\ncollection and annotation all the way to post-deployment. Our classifier,\ntrained using data from our original corpus of over 422k examples, is\nspecifically developed for the inherently multilingual setting of Switzerland\nand outperforms with its F1 score of 80.5 the currently best-performing\nBERT-based multilingual classifier by 5.8 F1 points in German and 3.6 F1 points\nin French. Our systematic evaluations over a 12-month period further highlight\nthe vital importance of continuous, human-in-the-loop classifier maintenance to\nensure robust hate speech classification post-deployment.", "published": "2022-12-05 09:05:40", "link": "http://arxiv.org/abs/2212.02108v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Legal Prompt Engineering for Multilingual Legal Judgement Prediction", "abstract": "Legal Prompt Engineering (LPE) or Legal Prompting is a process to guide and\nassist a large language model (LLM) with performing a natural legal language\nprocessing (NLLP) skill. Our goal is to use LPE with LLMs over long legal\ndocuments for the Legal Judgement Prediction (LJP) task. We investigate the\nperformance of zero-shot LPE for given facts in case-texts from the European\nCourt of Human Rights (in English) and the Federal Supreme Court of Switzerland\n(in German, French and Italian). Our results show that zero-shot LPE is better\ncompared to the baselines, but it still falls short compared to current state\nof the art supervised approaches. Nevertheless, the results are important,\nsince there was 1) no explicit domain-specific data used - so we show that the\ntransfer to the legal domain is possible for general-purpose LLMs, and 2) the\nLLMs where directly applied without any further training or fine-tuning - which\nin turn saves immensely in terms of additional computational costs.", "published": "2022-12-05 12:17:02", "link": "http://arxiv.org/abs/2212.02199v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Federated Neural Topic Models", "abstract": "Over the last years, topic modeling has emerged as a powerful technique for\norganizing and summarizing big collections of documents or searching for\nparticular patterns in them. However, privacy concerns may arise when\ncross-analyzing data from different sources. Federated topic modeling solves\nthis issue by allowing multiple parties to jointly train a topic model without\nsharing their data. While several federated approximations of classical topic\nmodels do exist, no research has been conducted on their application for neural\ntopic models. To fill this gap, we propose and analyze a federated\nimplementation based on state-of-the-art neural topic modeling implementations,\nshowing its benefits when there is a diversity of topics across the nodes'\ndocuments and the need to build a joint model. In practice, our approach is\nequivalent to a centralized model training, but preserves the privacy of the\nnodes. Advantages of this federated scenario are illustrated by means of\nexperiments using both synthetic and real data scenarios.", "published": "2022-12-05 13:49:26", "link": "http://arxiv.org/abs/2212.02269v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Entity Set Co-Expansion in StackOverflow", "abstract": "Given a few seed entities of a certain type (e.g., Software or Programming\nLanguage), entity set expansion aims to discover an extensive set of entities\nthat share the same type as the seeds. Entity set expansion in software-related\ndomains such as StackOverflow can benefit many downstream tasks (e.g., software\nknowledge graph construction) and facilitate better IT operations and service\nmanagement. Meanwhile, existing approaches are less concerned with two\nproblems: (1) How to deal with multiple types of seed entities simultaneously?\n(2) How to leverage the power of pre-trained language models (PLMs)? Being\naware of these two problems, in this paper, we study the entity set\nco-expansion task in StackOverflow, which extracts Library, OS, Application,\nand Language entities from StackOverflow question-answer threads. During the\nco-expansion process, we use PLMs to derive embeddings of candidate entities\nfor calculating similarities between entities. Experimental results show that\nour proposed SECoExpan framework outperforms previous approaches significantly.", "published": "2022-12-05 13:50:35", "link": "http://arxiv.org/abs/2212.02271v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Few-Shot Relation Extraction via Representation Learning\n  and Domain Adaptation", "abstract": "Few-shot relation extraction aims to recognize novel relations with few\nlabeled sentences in each relation. Previous metric-based few-shot relation\nextraction algorithms identify relationships by comparing the prototypes\ngenerated by the few labeled sentences embedding with the embeddings of the\nquery sentences using a trained metric function. However, as these domains\nalways have considerable differences from those in the training dataset, the\ngeneralization ability of these approaches on unseen relations in many domains\nis limited. Since the prototype is necessary for obtaining relationships\nbetween entities in the latent space, we suggest learning more interpretable\nand efficient prototypes from prior knowledge and the intrinsic semantics of\nrelations to extract new relations in various domains more effectively. By\nexploring the relationships between relations using prior information, we\neffectively improve the prototype representation of relations. By using\ncontrastive learning to make the classification margins between sentence\nembedding more distinct, the prototype's geometric interpretability is\nenhanced. Additionally, utilizing a transfer learning approach for the\ncross-domain problem allows the generation process of the prototype to account\nfor the gap between other domains, making the prototype more robust and\nenabling the better extraction of associations across multiple domains. The\nexperiment results on the benchmark FewRel dataset demonstrate the advantages\nof the suggested method over some state-of-the-art approaches.", "published": "2022-12-05 19:34:52", "link": "http://arxiv.org/abs/2212.02560v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QBERT: Generalist Model for Processing Questions", "abstract": "Using a single model across various tasks is beneficial for training and\napplying deep neural sequence models. We address the problem of developing\ngeneralist representations of text that can be used to perform a range of\ndifferent tasks rather than being specialised to a single application. We focus\non processing short questions and developing an embedding for these questions\nthat is useful on a diverse set of problems, such as question topic\nclassification, equivalent question recognition, and question answering. This\npaper introduces QBERT, a generalist model for processing questions. With\nQBERT, we demonstrate how we can train a multi-task network that performs all\nquestion-related tasks and has achieved similar performance compared to its\ncorresponding single-task models.", "published": "2022-12-05 00:56:28", "link": "http://arxiv.org/abs/2212.01967v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast and accurate factorized neural transducer for text adaption of\n  end-to-end speech recognition models", "abstract": "Neural transducer is now the most popular end-to-end model for speech\nrecognition, due to its naturally streaming ability. However, it is challenging\nto adapt it with text-only data. Factorized neural transducer (FNT) model was\nproposed to mitigate this problem. The improved adaptation ability of FNT on\ntext-only adaptation data came at the cost of lowered accuracy compared to the\nstandard neural transducer model. We propose several methods to improve the\nperformance of the FNT model. They are: adding CTC criterion during training,\nadding KL divergence loss during adaptation, using a pre-trained language model\nto seed the vocabulary predictor, and an efficient adaptation approach by\ninterpolating the vocabulary predictor with the n-gram language model. A\ncombination of these approaches results in a relative word-error-rate reduction\nof 9.48\\% from the standard FNT model. Furthermore, n-gram interpolation with\nthe vocabulary predictor improves the adaptation speed hugely with satisfactory\nadaptation performance.", "published": "2022-12-05 02:52:21", "link": "http://arxiv.org/abs/2212.01992v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fine-tuning a Subtle Parsing Distinction Using a Probabilistic Decision\n  Tree: the Case of Postnominal \"that\" in Noun Complement Clauses vs. Relative\n  Clauses", "abstract": "In this paper we investigated two different methods to parse relative and\nnoun complement clauses in English and resorted to distinct tags for their\ncorresponding that as a relative pronoun and as a complementizer. We used an\nalgorithm to relabel a corpus parsed with the GUM Treebank using Universal\nDependency. Our second experiment consisted in using TreeTagger, a\nProbabilistic Decision Tree, to learn the distinction between the two\ncomplement and relative uses of postnominal \"that\". We investigated the effect\nof the training set size on TreeTagger accuracy and how representative the GUM\nTreebank files are for the two structures under scrutiny. We discussed some of\nthe linguistic and structural tenets of the learnability of this distinction.", "published": "2022-12-05 20:52:41", "link": "http://arxiv.org/abs/2212.02591v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unifying Vision, Text, and Layout for Universal Document Processing", "abstract": "We propose Universal Document Processing (UDOP), a foundation Document AI\nmodel which unifies text, image, and layout modalities together with varied\ntask formats, including document understanding and generation. UDOP leverages\nthe spatial correlation between textual content and document image to model\nimage, text, and layout modalities with one uniform representation. With a\nnovel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain\ndownstream tasks into a prompt-based sequence generation scheme. UDOP is\npretrained on both large-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP also learns to\ngenerate document images from text and layout modalities via masked image\nreconstruction. To the best of our knowledge, this is the first time in the\nfield of document AI that one model simultaneously achieves high-quality neural\ndocument editing and content customization. Our method sets the\nstate-of-the-art on 8 Document AI tasks, e.g., document understanding and QA,\nacross diverse data domains like finance reports, academic papers, and\nwebsites. UDOP ranks first on the leaderboard of the Document Understanding\nBenchmark.", "published": "2022-12-05 22:14:49", "link": "http://arxiv.org/abs/2212.02623v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Transformer-Based User Satisfaction Prediction for Proactive\n  Interaction Mechanism in DuerOS", "abstract": "Recently, spoken dialogue systems have been widely deployed in a variety of\napplications, serving a huge number of end-users. A common issue is that the\nerrors resulting from noisy utterances, semantic misunderstandings, or lack of\nknowledge make it hard for a real system to respond properly, possibly leading\nto an unsatisfactory user experience. To avoid such a case, we consider a\nproactive interaction mechanism where the system predicts the user satisfaction\nwith the candidate response before giving it to the user. If the user is not\nlikely to be satisfied according to the prediction, the system will ask the\nuser a suitable question to determine the real intent of the user instead of\nproviding the response directly. With such an interaction with the user, the\nsystem can give a better response to the user. Previous models that predict the\nuser satisfaction are not applicable to DuerOS which is a large-scale\ncommercial dialogue system. They are based on hand-crafted features and thus\ncan hardly learn the complex patterns lying behind millions of conversations\nand temporal dependency in multiple turns of the conversation. Moreover, they\nare trained and evaluated on the benchmark datasets with adequate labels, which\nare expensive to obtain in a commercial dialogue system. To face these\nchallenges, we propose a pipeline to predict the user satisfaction to help\nDuerOS decide whether to ask for clarification in each turn. Specifically, we\npropose to first generate a large number of weak labels and then train a\ntransformer-based model to predict the user satisfaction with these weak\nlabels. Empirically, we deploy and evaluate our model on DuerOS, and observe a\n19% relative improvement on the accuracy of user satisfaction prediction and\n2.3% relative improvement on user experience.", "published": "2022-12-05 09:17:49", "link": "http://arxiv.org/abs/2212.03817v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evince the artifacts of Spoof Speech by blending Vocal Tract and Voice\n  Source Features", "abstract": "With the rapid advancement in synthetic speech generation technologies, great\ninterest in differentiating spoof speech from the natural speech is emerging in\nthe research community. The identification of these synthetic signals is a\ndifficult task not only for the cutting-edge classification models but also for\nhumans themselves. To prevent potential adverse effects, it becomes crucial to\ndetect spoof signals. From a forensics perspective, it is also important to\npredict the algorithm which generated them to identify the forger. This needs\nan understanding of the underlying attributes of spoof signals which serve as a\nsignature for the synthesizer. This study emphasizes the segments of speech\nsignals critical in identifying their authenticity by utilizing the Vocal Tract\nSystem(\\textit{VTS}) and Voice Source(\\textit{VS}) features.\n  In this paper, we propose a system that detects spoof signals as well as\nidentifies the corresponding speech-generating algorithm. We achieve 99.58\\% in\nalgorithm classification accuracy. From experiments, we found that a VS\nfeature-based system gives more attention to the transition of phonemes, while,\na VTS feature-based system gives more attention to stationary segments of\nspeech signals. We perform model fusion techniques on the VS-based and\nVTS-based systems to exploit the complementary information to develop a robust\nclassifier. Upon analyzing the confusion plots we found that WaveRNN is poorly\nclassified depicting more naturalness. On the other hand, we identified that\nsynthesizer like Waveform Concatenation, and Neural Source Filter is classified\nwith the highest accuracy. Practical implications of this work can aid\nresearchers from both forensics (leverage artifacts) and the speech communities\n(mitigate artifacts).", "published": "2022-12-05 04:02:50", "link": "http://arxiv.org/abs/2212.02013v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NBC2: Multichannel Speech Separation with Revised Narrow-band Conformer", "abstract": "This work proposes a multichannel narrow-band speech separation network. In\nthe short-time Fourier transform (STFT) domain, the proposed network processes\neach frequency independently, and all frequencies use a shared network. For\neach frequency, the network performs end-to-end speech separation, namely\ntaking as input the STFT coefficients of microphone signals, and predicting the\nseparated STFT coefficients of multiple speakers. The proposed network learns\nto cluster the frame-wise spatial/steering vectors that belong to different\nspeakers. It is mainly composed of three components. First, a self-attention\nnetwork. Clustering of spatial vectors shares a similar principle with the\nself-attention mechanism in the sense of computing the similarity of vectors\nand then aggregating similar vectors. Second, a convolutional feed-forward\nnetwork. The convolutional layers are employed for signal smoothing and\nreverberation processing. Third, a novel hidden-layer normalization method,\ni.e. group batch normalization (GBN), is especially designed for the proposed\nnarrow-band network to maintain the distribution of hidden units over\nfrequencies. Overall, the proposed network is named NBC2, as it is a revised\nversion of our previous NBC (narrow-band conformer) network. Experiments show\nthat 1) the proposed network outperforms other state-of-the-art methods by a\nlarge margin, 2) the proposed GBN improves the signal-to-distortion ratio by 3\ndB, relative to other normalization methods, such as batch/layer/group\nnormalization, 3) the proposed narrow-band network is spectrum-agnostic, as it\ndoes not learn spectral patterns, and 4) the proposed network is indeed\nperforming frame clustering (demonstrated by the attention maps).", "published": "2022-12-05 07:44:32", "link": "http://arxiv.org/abs/2212.02076v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-end Recording Device Identification Based on Deep Representation\n  Learning", "abstract": "Deep learning techniques have achieved specific results in recording device\nsource identification. The recording device source features include spatial\ninformation and certain temporal information. However, most recording device\nsource identification methods based on deep learning only use spatial\nrepresentation learning from recording device source features, which cannot\nmake full use of recording device source information. Therefore, in this paper,\nto fully explore the spatial information and temporal information of recording\ndevice source, we propose a new method for recording device source\nidentification based on the fusion of spatial feature information and temporal\nfeature information by using an end-to-end framework. From a feature\nperspective, we designed two kinds of networks to extract recording device\nsource spatial and temporal information. Afterward, we use the attention\nmechanism to adaptively assign the weight of spatial information and temporal\ninformation to obtain fusion features. From a model perspective, our model uses\nan end-to-end framework to learn the deep representation from spatial feature\nand temporal feature and train using deep and shallow loss to joint optimize\nour network. This method is compared with our previous work and baseline\nsystem. The results show that the proposed method is better than our previous\nwork and baseline system under general conditions.", "published": "2022-12-05 07:56:04", "link": "http://arxiv.org/abs/2212.02084v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LMEC: Learnable Multiplicative Absolute Position Embedding Based\n  Conformer for Speech Recognition", "abstract": "This paper proposes a Learnable Multiplicative absolute position Embedding\nbased Conformer (LMEC). It contains a kernelized linear attention (LA) module\ncalled LMLA to solve the time-consuming problem for long sequence speech\nrecognition as well as an alternative to the FFN structure. First, the ELU\nfunction is adopted as the kernel function of our proposed LA module. Second,\nwe propose a novel Learnable Multiplicative Absolute Position Embedding\n(LM-APE) based re-weighting mechanism that can reduce the well-known quadratic\ntemporal-space complexity of softmax self-attention. Third, we use Gated Linear\nUnits (GLU) to substitute the Feed Forward Network (FFN) for better\nperformance. Extensive experiments have been conducted on the public\nLibriSpeech datasets. Compared to the Conformer model with cosFormer style\nlinear attention, our proposed method can achieve up to 0.63% word-error-rate\nimprovement on test-other and improve the inference speed by up to 13% (left\nproduct) and 33% (right product) on the LA module.", "published": "2022-12-05 08:36:17", "link": "http://arxiv.org/abs/2212.02099v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Generating Diverse Audio Captions via Adversarial Training", "abstract": "Automated audio captioning is a cross-modal translation task for describing\nthe content of audio clips with natural language sentences. This task has\nattracted increasing attention and substantial progress has been made in recent\nyears. Captions generated by existing models are generally faithful to the\ncontent of audio clips, however, these machine-generated captions are often\ndeterministic (e.g., generating a fixed caption for a given audio clip), simple\n(e.g., using common words and simple grammar), and generic (e.g., generating\nthe same caption for similar audio clips). When people are asked to describe\nthe content of an audio clip, different people tend to focus on different sound\nevents and describe an audio clip diversely from various aspects using distinct\nwords and grammar. We believe that an audio captioning system should have the\nability to generate diverse captions, either for a fixed audio clip, or across\nsimilar audio clips. To this end, we propose an adversarial training framework\nbased on a conditional generative adversarial network (C-GAN) to improve\ndiversity of audio captioning systems. A caption generator and two hybrid\ndiscriminators compete and are learned jointly, where the caption generator can\nbe any standard encoder-decoder captioning model used to generate captions, and\nthe hybrid discriminators assess the generated captions from different\ncriteria, such as their naturalness and semantics. We conduct experiments on\nthe Clotho dataset. The results show that our proposed model can generate\ncaptions with better diversity as compared to state-of-the-art methods.", "published": "2022-12-05 05:06:19", "link": "http://arxiv.org/abs/2212.02033v2", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DeAR: A Deep-learning-based Audio Re-recording Resilient Watermarking", "abstract": "Audio watermarking is widely used for leaking source tracing. The robustness\nof the watermark determines the traceability of the algorithm. With the\ndevelopment of digital technology, audio re-recording (AR) has become an\nefficient and covert means to steal secrets. AR process could drastically\ndestroy the watermark signal while preserving the original information. This\nputs forward a new requirement for audio watermarking at this stage, that is,\nto be robust to AR distortions. Unfortunately, none of the existing algorithms\ncan effectively resist AR attacks due to the complexity of the AR process. To\naddress this limitation, this paper proposes DeAR, a deep-learning-based audio\nre-recording resistant watermarking. Inspired by DNN-based image watermarking,\nwe pioneer a deep learning framework for audio carriers, based on which the\nwatermark signal can be effectively embedded and extracted. Meanwhile, in order\nto resist the AR attack, we delicately analyze the distortions that occurred in\nthe AR process and design the corresponding distortion layer to cooperate with\nthe proposed watermarking framework. Extensive experiments show that the\nproposed algorithm can resist not only common electronic channel distortions\nbut also AR distortions. Under the premise of high-quality embedding\n(SNR=25.86dB), in the case of a common re-recording distance (20cm), the\nalgorithm can effectively achieve an average bit recovery accuracy of 98.55%.", "published": "2022-12-05 15:15:10", "link": "http://arxiv.org/abs/2212.02339v4", "categories": ["cs.SD", "cs.CR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Latent Space Cartography", "abstract": "We explore the generation of visualisations of audio latent spaces using an\naudio-to-image generation pipeline. We believe this can help with the\ninterpretability of audio latent spaces. We demonstrate a variety of results on\nthe NSynth dataset. A web demo is available.", "published": "2022-12-05 21:51:33", "link": "http://arxiv.org/abs/2212.02610v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "J.5"], "primary_category": "cs.SD"}
{"title": "Sound emergence as a predictor of short-term annoyance from wind turbine\n  noise", "abstract": "While sound emergence is used in several countries to regulate wind energy\ndevelopment, there is no published evidence that it is a relevant noise\ndescriptor for this purpose. In the present work, we carried out two listening\ntests to evaluate the merits of sound emergence. Three definitions of sound\nemergence were considered: the one in ISO 1996-1, sound emergence under\naudibility condition $e_{UAC}$, and spectral emergence $e_{SP}$. We also\nconsidered the specific to residual ratio and loudness metrics. In each\nlistening test, the sound stimuli consisted of 48 sound stimuli at 3 A-weighted\nsound pressure levels $\\{30, 40, 50\\}$~dB and 4 specific-to-residual ratios\n$\\{-10, -5, 0, +5 \\}$~dB. The results lead to the conclusion that short term\nannoyance is better predicted by the total sound pressure level than by sound\nemergence, whatever the definition considered for the latter, or than by the\nspecific to residual ratio. Short-term annoyance is slightly better predicted\nby $e_{UAC}$ than by $e$, while $e$ is a better predictor than $e_{SP}$. The\ntotal sound pressure level and the loudness metrics performed similarly.\nFurthermore, the results provide evidence that sound emergence is a poor\npredictor of the audibility of wind turbine sounds.", "published": "2022-12-05 21:59:28", "link": "http://arxiv.org/abs/2212.02616v2", "categories": ["eess.AS", "cs.SD", "eess.SP", "physics.class-ph"], "primary_category": "eess.AS"}
{"title": "MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music\n  Audio Representation Learning", "abstract": "The deep learning community has witnessed an exponentially growing interest\nin self-supervised learning (SSL). However, it still remains unexplored how to\nbuild a framework for learning useful representations of raw music waveforms in\na self-supervised manner. In this work, we design Music2Vec, a framework\nexploring different SSL algorithmic components and tricks for music audio\nrecordings. Our model achieves comparable results to the state-of-the-art\n(SOTA) music SSL model Jukebox, despite being significantly smaller with less\nthan 2% of parameters of the latter. The model will be released on\nHuggingface(Please refer to: https://huggingface.co/m-a-p/music2vec-v1)", "published": "2022-12-05 16:04:26", "link": "http://arxiv.org/abs/2212.02508v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
