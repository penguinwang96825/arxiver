{"title": "Optimal Execution under Incomplete Information", "abstract": "We study optimal liquidation strategies under partial information for a\nsingle asset within a finite time horizon. We propose a model tailored for\nhigh-frequency trading, capturing price formation driven solely by order flow\nthrough mutually stimulating marked Hawkes processes. The model assumes a limit\norder book framework, accounting for both permanent price impact and transient\nmarket impact. Importantly, we incorporate liquidity as a hidden Markov\nprocess, influencing the intensities of the point processes governing bid and\nask prices. Within this setting, we formulate the optimal liquidation problem\nas an impulse control problem. We elucidate the dynamics of the hidden Markov\nchain's filter and determine the related normalized filtering equations. We\nthen express the value function as the limit of a sequence of auxiliary\ncontinuous functions, defined recursively. This characterization enables the\nuse of a dynamic programming principle for optimal stopping problems and the\ndetermination of an optimal strategy. It also facilitates the development of an\nimplementable algorithm to approximate the original liquidation problem. We\nenrich our analysis with numerical results and visualizations of candidate\noptimal strategies.", "published": "2024-11-07 10:57:32", "link": "http://arxiv.org/abs/2411.04616v1", "categories": ["q-fin.MF", "q-fin.TR"], "primary_category": "q-fin.MF"}
{"title": "The Role of AI in Financial Forecasting: ChatGPT's Potential and Challenges", "abstract": "The outlook for the future of artificial intelligence (AI) in the financial\nsector, especially in financial forecasting, the challenges and implications.\nThe dynamics of AI technology, including deep learning, reinforcement learning,\nand integration with blockchAIn and the Internet of Things, also highlight the\ncontinued improvement in data processing capabilities. Explore how AI is\nreshaping financial services with precisely tAIlored services that can more\nprecisely meet the diverse needs of individual investors. The integration of AI\nchallenges regulatory and ethical issues in the financial sector, as well as\nthe implications for data privacy protection. Analyze the limitations of\ncurrent AI technology in financial forecasting and its potential impact on the\nfuture financial industry landscape, including changes in the job market, the\nemergence of new financial institutions, and user interface innovations.\nEmphasizing the importance of increasing investor understanding and awareness\nof AI and looking ahead to future trends in AI tools for user experience to\ndrive wider adoption of AI in financial decision making. The huge potential,\nchallenges, and future directions of AI in the financial sector highlight the\ncritical role of AI technology in driving transformation and innovation in the\nfinancial sector", "published": "2024-11-07 15:35:16", "link": "http://arxiv.org/abs/2411.13562v1", "categories": ["q-fin.ST", "cs.AI", "cs.CY"], "primary_category": "q-fin.ST"}
{"title": "Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research", "abstract": "In recent years, the application of generative artificial intelligence\n(GenAI) in financial analysis and investment decision-making has gained\nsignificant attention. However, most existing approaches rely on single-agent\nsystems, which fail to fully utilize the collaborative potential of multiple AI\nagents. In this paper, we propose a novel multi-agent collaboration system\ndesigned to enhance decision-making in financial investment research. The\nsystem incorporates agent groups with both configurable group sizes and\ncollaboration structures to leverage the strengths of each agent group type. By\nutilizing a sub-optimal combination strategy, the system dynamically adapts to\nvarying market conditions and investment scenarios, optimizing performance\nacross different tasks. We focus on three sub-tasks: fundamentals, market\nsentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30\ncompanies listed on the Dow Jones Index. Our findings reveal significant\nperformance variations based on the configurations of AI agents for different\ntasks. The results demonstrate that our multi-agent collaboration system\noutperforms traditional single-agent models, offering improved accuracy,\nefficiency, and adaptability in complex financial environments. This study\nhighlights the potential of multi-agent systems in transforming financial\nanalysis and investment decision-making by integrating diverse analytical\nperspectives.", "published": "2024-11-07 15:28:20", "link": "http://arxiv.org/abs/2411.04788v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-fin.ST", "q-fin.TR"], "primary_category": "cs.AI"}
{"title": "Balancing Transparency and Accuracy: A Comparative Analysis of\n  Rule-Based and Deep Learning Models in Political Bias Classification", "abstract": "The unchecked spread of digital information, combined with increasing\npolitical polarization and the tendency of individuals to isolate themselves\nfrom opposing political viewpoints, has driven researchers to develop systems\nfor automatically detecting political bias in media. This trend has been\nfurther fueled by discussions on social media. We explore methods for\ncategorizing bias in US news articles, comparing rule-based and deep learning\napproaches. The study highlights the sensitivity of modern self-learning\nsystems to unconstrained data ingestion, while reconsidering the strengths of\ntraditional rule-based systems. Applying both models to left-leaning (CNN) and\nright-leaning (FOX) news articles, we assess their effectiveness on data beyond\nthe original training and test sets.This analysis highlights each model's\naccuracy, offers a framework for exploring deep-learning explainability, and\nsheds light on political bias in US news media. We contrast the opaque\narchitecture of a deep learning model with the transparency of a linguistically\ninformed rule-based model, showing that the rule-based model performs\nconsistently across different data conditions and offers greater transparency,\nwhereas the deep learning model is dependent on the training set and struggles\nwith unseen data.", "published": "2024-11-07 00:09:18", "link": "http://arxiv.org/abs/2411.04328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeTree: Agent-guided Tree Search for Code Generation with Large\n  Language Models", "abstract": "Pre-trained on massive amounts of code and text data, large language models\n(LLMs) have demonstrated remarkable achievements in performing code generation\ntasks. With additional execution-based feedback, these models can act as agents\nwith capabilities to self-refine and improve generated code autonomously.\nHowever, on challenging coding tasks with extremely large search space, current\nagentic approaches still struggle with multi-stage planning, generating, and\ndebugging. To address this problem, we propose CodeTree, a framework for LLM\nagents to efficiently explore the search space in different stages of the code\ngeneration process. Specifically, we adopted a unified tree structure to\nexplicitly explore different coding strategies, generate corresponding coding\nsolutions, and subsequently refine the solutions. In each stage, critical\ndecision-making (ranking, termination, expanding) of the exploration process is\nguided by both the environmental execution-based feedback and\nLLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code\ngeneration benchmarks and demonstrated the significant performance gains of\nCodeTree against strong baselines. Using GPT-4o as the base model, we\nconsistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0\non CodeContests. On the challenging SWEBench benchmark, our approach led to\nsignificant performance gains.", "published": "2024-11-07 00:09:54", "link": "http://arxiv.org/abs/2411.04329v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring short-form factuality in large language models", "abstract": "We present SimpleQA, a benchmark that evaluates the ability of language\nmodels to answer short, fact-seeking questions. We prioritized two properties\nin designing this eval. First, SimpleQA is challenging, as it is adversarially\ncollected against GPT-4 responses. Second, responses are easy to grade, because\nquestions are created such that there exists only a single, indisputable\nanswer. Each answer in SimpleQA is graded as either correct, incorrect, or not\nattempted. A model with ideal behavior would get as many questions correct as\npossible while not attempting the questions for which it is not confident it\nknows the correct answer. SimpleQA is a simple, targeted evaluation for whether\nmodels \"know what they know,\" and our hope is that this benchmark will remain\nrelevant for the next few generations of frontier models. SimpleQA can be found\nat https://github.com/openai/simple-evals.", "published": "2024-11-07 01:58:42", "link": "http://arxiv.org/abs/2411.04368v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DELIFT: Data Efficient Language model Instruction Fine Tuning", "abstract": "Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy.", "published": "2024-11-07 04:38:29", "link": "http://arxiv.org/abs/2411.04425v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One fish, two fish, but not the whole sea: Alignment reduces language\n  models' conceptual diversity", "abstract": "Researchers in social science and psychology have recently proposed using\nlarge language models (LLMs) as replacements for humans in behavioral research.\nIn addition to arguments about whether LLMs accurately capture population-level\npatterns, this has raised questions about whether LLMs capture human-like\nconceptual diversity. Separately, it is debated whether post-training alignment\n(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,\nwe use a new way of measuring the conceptual diversity of\nsynthetically-generated LLM \"populations\" by relating the internal variability\nof simulated individuals to the population-level variability. We use this\napproach to evaluate non-aligned and aligned LLMs on two domains with rich\nhuman behavioral data. While no model reaches human-like diversity, aligned\nmodels generally display less diversity than their instruction fine-tuned\ncounterparts. Our findings highlight potential trade-offs between increasing\nmodels' value alignment and decreasing the diversity of their conceptual\nrepresentations.", "published": "2024-11-07 04:38:58", "link": "http://arxiv.org/abs/2411.04427v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradient Localization Improves Lifelong Pretraining of Language Models", "abstract": "Large Language Models (LLMs) trained on web-scale text corpora have been\nshown to capture world knowledge in their parameters. However, the mechanism by\nwhich language models store different types of knowledge is poorly understood.\nIn this work, we examine two types of knowledge relating to temporally\nsensitive entities and demonstrate that each type is localized to different\nsets of parameters within the LLMs. We hypothesize that the lack of\nconsideration of the locality of knowledge in existing continual learning\nmethods contributes to both: the failed uptake of new information, and\ncatastrophic forgetting of previously learned information. We observe that\nsequences containing references to updated and newly mentioned entities exhibit\nlarger gradient norms in a subset of layers. We demonstrate that targeting\nparameter updates to these relevant layers can improve the performance of\ncontinually pretraining on language containing temporal drift.", "published": "2024-11-07 05:43:50", "link": "http://arxiv.org/abs/2411.04448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ML-Promise: A Multilingual Dataset for Corporate Promise Verification", "abstract": "Promises made by politicians, corporate leaders, and public figures have a\nsignificant impact on public perception, trust, and institutional reputation.\nHowever, the complexity and volume of such commitments, coupled with\ndifficulties in verifying their fulfillment, necessitate innovative methods for\nassessing their credibility. This paper introduces the concept of Promise\nVerification, a systematic approach involving steps such as promise\nidentification, evidence assessment, and the evaluation of timing for\nverification. We propose the first multilingual dataset, ML-Promise, which\nincludes English, French, Chinese, Japanese, and Korean, aimed at facilitating\nin-depth verification of promises, particularly in the context of\nEnvironmental, Social, and Governance (ESG) reports. Given the growing emphasis\non corporate environmental contributions, this dataset addresses the challenge\nof evaluating corporate promises, especially in light of practices like\ngreenwashing. Our findings also explore textual and image-based baselines, with\npromising results from retrieval-augmented generation (RAG) approaches. This\nwork aims to foster further discourse on the accountability of public\ncommitments across multiple languages and domains.", "published": "2024-11-07 06:51:24", "link": "http://arxiv.org/abs/2411.04473v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large\n  Language Model", "abstract": "To increase social bonding with interlocutors, humans naturally acquire the\nability to respond appropriately in a given situation by considering which\nconversational skill is most suitable for the response - a process we call\nskill-of-mind. For large language model (LLM)-based conversational agents,\nplanning appropriate conversational skills, as humans do, is challenging due to\nthe complexity of social dialogue, especially in interactive scenarios. To\naddress this, we propose a skill-of-mind-annotated conversation dataset, named\nMultifaceted Skill-of-Mind, which includes multi-turn and multifaceted\nconversational skills across various interactive scenarios (e.g., long-term,\ncounseling, task-oriented), grounded in diverse social contexts (e.g.,\ndemographics, persona, rules of thumb). This dataset consists of roughly 100K\nconversations. Using this dataset, we introduce a new family of\nskill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B\nparameters. With extensive experiments, these models successfully demonstrate\nthe skill-of-mind process and exhibit strong generalizability in inferring\nmultifaceted skills across a variety of domains. Moreover, we show that Thanos\nsignificantly enhances the quality of responses generated by LLM-based\nconversational agents and promotes prosocial behavior in human evaluations.", "published": "2024-11-07 07:46:06", "link": "http://arxiv.org/abs/2411.04496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tomato, Tomahto, Tomate: Measuring the Role of Shared Semantics among\n  Subwords in Multilingual Language Models", "abstract": "Human understanding of language is robust to different word choices as far as\nthey represent similar semantic concepts. To what extent does our human\nintuition transfer to language models, which represent all subwords as distinct\nembeddings? In this work, we take an initial step on measuring the role of\nshared semantics among subwords in the encoder-only multilingual language\nmodels (mLMs). To this end, we form \"semantic tokens\" by merging the\nsemantically similar subwords and their embeddings, and evaluate the updated\nmLMs on 5 heterogeneous multilingual downstream tasks. Results show that the\ngeneral shared semantics could get the models a long way in making the\npredictions on mLMs with different tokenizers and model sizes. Inspections on\nthe grouped subwords show that they exhibit a wide range of semantic\nsimilarities, including synonyms and translations across many languages and\nscripts. Lastly, we found the zero-shot results with semantic tokens are on par\nor even better than the original models on certain classification tasks,\nsuggesting that the shared subword-level semantics may serve as the anchors for\ncross-lingual transferring.", "published": "2024-11-07 08:38:32", "link": "http://arxiv.org/abs/2411.04530v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The State and Fate of Summarization Datasets: A Survey", "abstract": "Automatic summarization has consistently attracted attention due to its\nversatility and wide application in various downstream tasks. Despite its\npopularity, we find that annotation efforts have largely been disjointed, and\nhave lacked common terminology. Consequently, it is challenging to discover\nexisting resources or identify coherent research directions. To address this,\nwe survey a large body of work spanning 133 datasets in over 100 languages,\ncreating a novel ontology covering sample properties, collection methods and\ndistribution. With this ontology we make key observations, including the lack\nin accessible high-quality datasets for low-resource languages, and the field's\nover-reliance on the news domain and on automatically collected distant\nsupervision. Finally, we make available a web interface that allows users to\ninteract and explore our ontology and dataset collection, as well as a template\nfor a summarization data card, which can be used to streamline future research\ninto a more coherent body of work.", "published": "2024-11-07 10:11:38", "link": "http://arxiv.org/abs/2411.04585v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FASSILA: A Corpus for Algerian Dialect Fake News Detection and Sentiment\n  Analysis", "abstract": "In the context of low-resource languages, the Algerian dialect (AD) faces\nchallenges due to the absence of annotated corpora, hindering its effective\nprocessing, notably in Machine Learning (ML) applications reliant on corpora\nfor training and assessment. This study outlines the development process of a\nspecialized corpus for Fake News (FN) detection and sentiment analysis (SA) in\nAD called FASSILA. This corpus comprises 10,087 sentences, encompassing over\n19,497 unique words in AD, and addresses the significant lack of linguistic\nresources in the language and covers seven distinct domains. We propose an\nannotation scheme for FN detection and SA, detailing the data collection,\ncleaning, and labelling process. Remarkable Inter-Annotator Agreement indicates\nthat the annotation scheme produces consistent annotations of high quality.\nSubsequent classification experiments using BERT-based models and ML models are\npresented, demonstrate promising results and highlight avenues for further\nresearch. The dataset is made freely available on GitHub\n(https://github.com/amincoding/FASSILA) to facilitate future advancements in\nthe field.", "published": "2024-11-07 10:39:10", "link": "http://arxiv.org/abs/2411.04604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop", "abstract": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects.", "published": "2024-11-07 11:51:14", "link": "http://arxiv.org/abs/2411.04637v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BhasaAnuvaad: A Speech Translation Dataset for 13 Indian Languages", "abstract": "Automatic Speech Translation (AST) datasets for Indian languages remain\ncritically scarce, with public resources covering fewer than 10 of the 22\nofficial languages. This scarcity has resulted in AST systems for Indian\nlanguages lagging far behind those available for high-resource languages like\nEnglish. In this paper, we first evaluate the performance of widely-used AST\nsystems on Indian languages, identifying notable performance gaps and\nchallenges. Our findings show that while these systems perform adequately on\nread speech, they struggle significantly with spontaneous speech, including\ndisfluencies like pauses and hesitations. Additionally, there is a striking\nabsence of systems capable of accurately translating colloquial and informal\nlanguage, a key aspect of everyday communication. To this end, we introduce\nBhasaAnuvaad, the largest publicly available dataset for AST involving 13 out\nof 22 scheduled Indian languages and English spanning over 44,400 hours and 17M\ntext segments. BhasaAnuvaad contains data for English speech to Indic text, as\nwell as Indic speech to English text. This dataset comprises three key\ncategories: (1) Curated datasets from existing resources, (2) Large-scale web\nmining, and (3) Synthetic data generation. By offering this diverse and\nexpansive dataset, we aim to bridge the resource gap and promote advancements\nin AST for Indian languages.", "published": "2024-11-07 13:33:34", "link": "http://arxiv.org/abs/2411.04699v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced\n  Code-Mixed Information Retrieval", "abstract": "Code-mixing, the integration of lexical and grammatical elements from\nmultiple languages within a single sentence, is a widespread linguistic\nphenomenon, particularly prevalent in multilingual societies. In India, social\nmedia users frequently engage in code-mixed conversations using the Roman\nscript, especially among migrant communities who form online groups to share\nrelevant local information. This paper focuses on the challenges of extracting\nrelevant information from code-mixed conversations, specifically within Roman\ntransliterated Bengali mixed with English. This study presents a novel approach\nto address these challenges by developing a mechanism to automatically identify\nthe most relevant answers from code-mixed conversations. We have experimented\nwith a dataset comprising of queries and documents from Facebook, and Query\nRelevance files (QRels) to aid in this task. Our results demonstrate the\neffectiveness of our approach in extracting pertinent information from complex,\ncode-mixed digital conversations, contributing to the broader field of natural\nlanguage processing in multilingual and informal text environments. We use\nGPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant\ndocuments to frame a mathematical model which helps to detect relevant\ndocuments corresponding to a query.", "published": "2024-11-07 14:41:01", "link": "http://arxiv.org/abs/2411.04752v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A study of Vietnamese readability assessing through semantic and\n  statistical features", "abstract": "Determining the difficulty of a text involves assessing various textual\nfeatures that may impact the reader's text comprehension, yet current research\nin Vietnamese has only focused on statistical features. This paper introduces a\nnew approach that integrates statistical and semantic approaches to assessing\ntext readability. Our research utilized three distinct datasets: the Vietnamese\nText Readability Dataset (ViRead), OneStopEnglish, and RACE, with the latter\ntwo translated into Vietnamese. Advanced semantic analysis methods were\nemployed for the semantic aspect using state-of-the-art language models such as\nPhoBERT, ViDeBERTa, and ViBERT. In addition, statistical methods were\nincorporated to extract syntactic and lexical features of the text. We\nconducted experiments using various machine learning models, including Support\nVector Machine (SVM), Random Forest, and Extra Trees and evaluated their\nperformance using accuracy and F1 score metrics. Our results indicate that a\njoint approach that combines semantic and statistical features significantly\nenhances the accuracy of readability classification compared to using each\nmethod in isolation. The current study emphasizes the importance of considering\nboth statistical and semantic aspects for a more accurate assessment of text\ndifficulty in Vietnamese. This contribution to the field provides insights into\nthe adaptability of advanced language models in the context of Vietnamese text\nreadability. It lays the groundwork for future research in this area.", "published": "2024-11-07 14:54:42", "link": "http://arxiv.org/abs/2411.04756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LuxBank: The First Universal Dependency Treebank for Luxembourgish", "abstract": "The Universal Dependencies (UD) project has significantly expanded linguistic\ncoverage across 161 languages, yet Luxembourgish, a West Germanic language\nspoken by approximately 400,000 people, has remained absent until now. In this\npaper, we introduce LuxBank, the first UD Treebank for Luxembourgish,\naddressing the gap in syntactic annotation and analysis for this `low-research'\nlanguage. We establish formal guidelines for Luxembourgish language annotation,\nproviding the foundation for the first large-scale quantitative analysis of its\nsyntax. LuxBank serves not only as a resource for linguists and language\nlearners but also as a tool for developing spell checkers and grammar checkers,\norganising existing text archives and even training large language models. By\nincorporating Luxembourgish into the UD framework, we aim to enhance the\nunderstanding of syntactic variation within West Germanic languages and offer a\nmodel for documenting smaller, semi-standardised languages. This work positions\nLuxembourgish as a valuable resource in the broader linguistic and NLP\ncommunities, contributing to the study of languages with limited research and\nresources.", "published": "2024-11-07 15:50:40", "link": "http://arxiv.org/abs/2411.04813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Does Classical Chinese Help? Quantifying Cross-Lingual Transfer in\n  Hanja and Kanbun", "abstract": "Historical and linguistic connections within the Sinosphere have led\nresearchers to use Classical Chinese resources for cross-lingual transfer when\nprocessing historical documents from Korea and Japan. In this paper, we\nquestion the assumption of cross-lingual transferability from Classical Chinese\nto Hanja and Kanbun, the ancient written languages of Korea and Japan,\nrespectively. Our experiments across machine translation, named entity\nrecognition, and punctuation restoration tasks show minimal impact of Classical\nChinese datasets on language model performance for ancient Korean documents\nwritten in Hanja, with performance differences within $\\pm{}0.0068$ F1-score\nfor sequence labeling tasks and up to $+0.84$ BLEU score for translation. These\nlimitations persist consistently across various model sizes, architectures, and\ndomain-specific datasets. Our analysis reveals that the benefits of Classical\nChinese resources diminish rapidly as local language data increases for Hanja,\nwhile showing substantial improvements only in extremely low-resource scenarios\nfor both Korean and Japanese historical documents. These mixed results\nemphasize the need for careful empirical validation rather than assuming\nbenefits from indiscriminate cross-lingual transfer.", "published": "2024-11-07 15:59:54", "link": "http://arxiv.org/abs/2411.04822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-Guided Internal States for Hallucination Detection of Large\n  Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes to the structure\nrelated to text truthfulness in LLMs' internal states, we make this structure\nmore salient and consistent across texts from different domains. We integrated\nour framework with existing hallucination detection methods and conducted\nexperiments on datasets from different domains. The experimental results\nindicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods.", "published": "2024-11-07 16:33:48", "link": "http://arxiv.org/abs/2411.04847v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GASE: Generatively Augmented Sentence Encoding", "abstract": "We propose an approach to enhance sentence embeddings by applying generative\ntext models for data augmentation at inference time. Unlike conventional data\naugmentation that utilises synthetic training data, our approach does not\nrequire access to model parameters or the computational resources typically\nrequired for fine-tuning state-of-the-art models. Generatively Augmented\nSentence Encoding uses diverse linguistic synthetic variants of input texts\ngenerated by paraphrasing, summarising, or extracting keywords, followed by\npooling the original and synthetic embeddings. Experimental results on the\nMassive Text Embedding Benchmark for Semantic Textual Similarity (STS)\ndemonstrate performance improvements across a range of embedding models using\ndifferent generative models for augmentation. We find that generative\naugmentation leads to larger performance improvements for embedding models with\nlower baseline performance. These findings suggest that integrating generative\naugmentation at inference time adds semantic diversity and can enhance the\nrobustness and generalizability of sentence embeddings for embedding models.\nOur results show that the degree to which generative augmentation can improve\nSTS performance depends not only on the embedding model but also on the\ndataset. From a broader perspective, the approach allows trading training for\ninference compute.", "published": "2024-11-07 17:53:47", "link": "http://arxiv.org/abs/2411.04914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating the Influence of Sequentially Correlated Literary Properties\n  in Textual Classification: A Data-Centric Hypothesis-Testing Approach", "abstract": "Stylometry aims to distinguish authors by analyzing literary traits assumed\nto reflect semi-conscious choices distinct from elements like genre or theme.\nHowever, these components often overlap, complicating text classification based\nsolely on feature distributions. While some literary properties, such as\nthematic content, are likely to manifest as correlations between adjacent text\nunits, others, like authorial style, may be independent thereof. We introduce a\nhypothesis-testing approach to evaluate the influence of sequentially\ncorrelated literary properties on text classification, aiming to determine when\nthese correlations drive classification. Using a multivariate binary\ndistribution, our method models sequential correlations between text units as a\nstochastic process, assessing the likelihood of clustering across varying\nadjacency scales. This enables us to examine whether classification is\ndominated by sequentially correlated properties or remains independent. In\nexperiments on a diverse English prose corpus, our analysis integrates\ntraditional and neural embeddings within supervised and unsupervised\nframeworks. Results demonstrate that our approach effectively identifies when\ntextual classification is not primarily influenced by sequentially correlated\nliterary properties, particularly in cases where texts differ in authorial\nstyle or genre rather than by a single author within a similar genre.", "published": "2024-11-07 18:28:40", "link": "http://arxiv.org/abs/2411.04950v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Semantic Hub Hypothesis: Language Models Share Semantic\n  Representations Across Languages and Modalities", "abstract": "Modern language models can process inputs across diverse languages and\nmodalities. We hypothesize that models acquire this capability through learning\na shared representation space across heterogeneous data types (e.g., different\nlanguages and modalities), which places semantically similar inputs near one\nanother, even if they are from different modalities/languages. We term this the\nsemantic hub hypothesis, following the hub-and-spoke model from neuroscience\n(Patterson et al., 2007) which posits that semantic knowledge in the human\nbrain is organized through a transmodal semantic \"hub\" which integrates\ninformation from various modality-specific \"spokes\" regions. We first show that\nmodel representations for semantically equivalent inputs in different languages\nare similar in the intermediate layers, and that this space can be interpreted\nusing the model's dominant pretraining language via the logit lens. This\ntendency extends to other data types, including arithmetic expressions, code,\nand visual/audio inputs. Interventions in the shared representation space in\none data type also predictably affect model outputs in other data types,\nsuggesting that this shared representations space is not simply a vestigial\nbyproduct of large-scale training on broad data, but something that is actively\nutilized by the model during input processing.", "published": "2024-11-07 18:55:09", "link": "http://arxiv.org/abs/2411.04986v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for\n  Multi-Modal Foundation Models", "abstract": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs).", "published": "2024-11-07 18:59:06", "link": "http://arxiv.org/abs/2411.04996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale\n  Haystacks?", "abstract": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data.", "published": "2024-11-07 18:59:27", "link": "http://arxiv.org/abs/2411.05000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance-Guided LLM Knowledge Distillation for Efficient Text\n  Classification at Scale", "abstract": "Large Language Models (LLMs) face significant challenges at inference time\ndue to their high computational demands. To address this, we present\nPerformance-Guided Knowledge Distillation (PGKD), a cost-effective and\nhigh-throughput solution for production text classification applications. PGKD\nutilizes teacher-student Knowledge Distillation to distill the knowledge of\nLLMs into smaller, task-specific models. PGKD establishes an active learning\nroutine between the student model and the LLM; the LLM continuously generates\nnew training data leveraging hard-negative mining, student model validation\nperformance, and early-stopping protocols to inform the data generation. By\nemploying a cyclical, performance-aware approach tailored for highly\nmulti-class, sparsely annotated datasets prevalent in industrial text\nclassification, PGKD effectively addresses training challenges and outperforms\ntraditional BERT-base models and other knowledge distillation methods on\nseveral multi-class classification datasets. Additionally, cost and latency\nbenchmarking reveals that models fine-tuned with PGKD are up to 130X faster and\n25X less expensive than LLMs for inference on the same classification task.\nWhile PGKD is showcased for text classification tasks, its versatile framework\ncan be extended to any LLM distillation task, including language generation,\nmaking it a powerful tool for optimizing performance across a wide range of AI\napplications.", "published": "2024-11-07 01:45:29", "link": "http://arxiv.org/abs/2411.05045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProverbEval: Exploring LLM Evaluation Challenges for Low-resource\n  Language Understanding", "abstract": "With the rapid development of evaluation datasets to assess LLMs\nunderstanding across a wide range of subjects and domains, identifying a\nsuitable language understanding benchmark has become increasingly challenging.\nIn this work, we explore LLM evaluation challenges for low-resource language\nunderstanding and introduce \\proverbeval, LLM evaluation benchmark for\nlow-resource languages, focusing on low-resource language understanding in\nculture-specific scenarios. We benchmark various LLMs and explore factors that\ncreate variability in the benchmarking process. We observed performance\nvariances of up to 50\\%, depending on the order in which answer choices were\npresented in multiple-choice tasks. Native language proverb descriptions\nsignificantly improve tasks such as proverb generation, contributing to\nimproved outcomes. Additionally, monolingual evaluations consistently\noutperformed their cross-lingual counterparts in generation tasks. We argue\nthat special attention must be given to the order of choices, the choice of\nprompt language, task variability, and generation tasks when creating LLM\nevaluation benchmarks. Evaluation data available at\nhttps://huggingface.co/datasets/israel/ProverbEval, evaluation code\nhttps://github.com/EthioNLP/EthioProverbEval.", "published": "2024-11-07 06:34:48", "link": "http://arxiv.org/abs/2411.05049v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Brief History of Named Entity Recognition", "abstract": "A large amount of information in today's world is now stored in knowledge\nbases. Named Entity Recognition (NER) is a process of extracting,\ndisambiguation, and linking an entity from raw text to insightful and\nstructured knowledge bases. More concretely, it is identifying and classifying\nentities in the text that are crucial for Information Extraction, Semantic\nAnnotation, Question Answering, Ontology Population, and so on. The process of\nNER has evolved in the last three decades since it first appeared in 1996. In\nthis survey, we study the evolution of techniques employed for NER and compare\nthe results, starting from supervised to the developing unsupervised learning\nmethods.", "published": "2024-11-07 17:49:03", "link": "http://arxiv.org/abs/2411.05057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the IWSLT 2024 Evaluation Campaign", "abstract": "This paper reports on the shared tasks organized by the 21st IWSLT\nConference. The shared tasks address 7 scientific challenges in spoken language\ntranslation: simultaneous and offline translation, automatic subtitling and\ndubbing, speech-to-speech translation, dialect and low-resource speech\ntranslation, and Indic languages. The shared tasks attracted 18 teams whose\nsubmissions are documented in 26 system papers. The growing interest towards\nspoken language translation is also witnessed by the constantly increasing\nnumber of shared task organizers and contributors to the overview paper, almost\nevenly distributed across industry and academia.", "published": "2024-11-07 19:11:55", "link": "http://arxiv.org/abs/2411.05088v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ImpScore: A Learnable Metric For Quantifying The Implicitness Level of\n  Sentence", "abstract": "Handling implicit language is essential for natural language processing\nsystems to achieve precise text understanding and facilitate natural\ninteractions with users. Despite its importance, the absence of a metric for\naccurately measuring the implicitness of language significantly constrains the\ndepth of analysis possible in evaluating models' comprehension capabilities.\nThis paper addresses this gap by developing a scalar metric that quantifies the\nimplicitness level of language without relying on external references. Drawing\non principles from traditional linguistics, we define \"implicitness\" as the\ndivergence between semantic meaning and pragmatic interpretation. To\noperationalize this definition, we introduce ImpScore, a reference-free metric\nformulated through an interpretable regression model. This model is trained\nusing pairwise contrastive learning on a specially curated dataset consisting\nof (implicit sentence, explicit sentence) pairs. We validate ImpScore through a\nuser study that compares its assessments with human evaluations on\nout-of-distribution data, demonstrating its accuracy and strong correlation\nwith human judgments. Additionally, we apply ImpScore to hate speech detection\ndatasets, illustrating its utility and highlighting significant limitations in\ncurrent large language models' ability to understand highly implicit content.\nOur metric is publicly available at https://github.com/audreycs/ImpScore.", "published": "2024-11-07 20:23:29", "link": "http://arxiv.org/abs/2411.05172v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement", "abstract": "Large Language Models (LLMs) have revolutionized code generation but require\nsignificant resources and often over-generalize, limiting their task-specific\nefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective\nalternative. However, standard supervised approaches rely only on correct\nexamples, missing valuable insights from failures. We introduce CodeLutra, a\nframework that leverages both correct and incorrect code attempts. Instead of\nusing only correct solutions, CodeLutra applies iterative preference-based\nrefinement, comparing successful and failed outputs to better approximate\ndesired results. This approach narrows the performance gap with\nstate-of-the-art larger models without requiring massive datasets or auxiliary\nmodels. For instance, on a challenging data science coding task, using only 500\nsamples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's\nlevel. By learning from both successes and mistakes, CodeLutra provides a\nscalable and efficient path to high-quality code generation, making smaller\nopen-source models more competitive with leading closed-source alternatives.", "published": "2024-11-07 21:51:07", "link": "http://arxiv.org/abs/2411.05199v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STAND-Guard: A Small Task-Adaptive Content Moderation Model", "abstract": "Content moderation, the process of reviewing and monitoring the safety of\ngenerated content, is important for development of welcoming online platforms\nand responsible large language models. Content moderation contains various\ntasks, each with its unique requirements tailored to specific scenarios.\nTherefore, it is crucial to develop a model that can be easily adapted to novel\nor customized content moderation tasks accurately without extensive model\ntuning. This paper presents STAND-GUARD, a Small Task-Adaptive coNtent\nmoDeration model. The basic motivation is: by performing instruct tuning on\nvarious content moderation tasks, we can unleash the power of small language\nmodels (SLMs) on unseen (out-of-distribution) content moderation tasks. We also\ncarefully study the effects of training tasks and model size on the efficacy of\ncross-task fine-tuning mechanism. Experiments demonstrate STAND-Guard is\ncomparable to GPT-3.5-Turbo across over 40 public datasets, as well as\nproprietary datasets derived from real-world business scenarios. Remarkably,\nSTAND-Guard achieved nearly equivalent results to GPT-4-Turbo on unseen English\nbinary classification tasks", "published": "2024-11-07 22:19:24", "link": "http://arxiv.org/abs/2411.05214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond the Numbers: Transparency in Relation Extraction Benchmark\n  Creation and Leaderboards", "abstract": "This paper investigates the transparency in the creation of benchmarks and\nthe use of leaderboards for measuring progress in NLP, with a focus on the\nrelation extraction (RE) task. Existing RE benchmarks often suffer from\ninsufficient documentation, lacking crucial details such as data sources,\ninter-annotator agreement, the algorithms used for the selection of instances\nfor datasets, and information on potential biases like dataset imbalance.\nProgress in RE is frequently measured by leaderboards that rank systems based\non evaluation methods, typically limited to aggregate metrics like F1-score.\nHowever, the absence of detailed performance analysis beyond these metrics can\nobscure the true generalisation capabilities of models. Our analysis reveals\nthat widely used RE benchmarks, such as TACRED and NYT, tend to be highly\nimbalanced and contain noisy labels. Moreover, the lack of class-based\nperformance metrics fails to accurately reflect model performance across\ndatasets with a large number of relation types. These limitations should be\ncarefully considered when reporting progress in RE. While our discussion\ncenters on the transparency of RE benchmarks and leaderboards, the observations\nwe discuss are broadly applicable to other NLP tasks as well. Rather than\nundermining the significance and value of existing RE benchmarks and the\ndevelopment of new models, this paper advocates for improved documentation and\nmore rigorous evaluation to advance the field.", "published": "2024-11-07 22:36:19", "link": "http://arxiv.org/abs/2411.05224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHATTER: A Character Attribution Dataset for Narrative Understanding", "abstract": "Computational narrative understanding studies the identification,\ndescription, and interaction of the elements of a narrative: characters,\nattributes, events, and relations. Narrative research has given considerable\nattention to defining and classifying character types. However, these\ncharacter-type taxonomies do not generalize well because they are small, too\nsimple, or specific to a domain. We require robust and reliable benchmarks to\ntest whether narrative models truly understand the nuances of the character's\ndevelopment in the story. Our work addresses this by curating the Chatter\ndataset that labels whether a character portrays some attribute for 88148\ncharacter-attribute pairs, encompassing 2998 characters, 13324 attributes and\n660 movies. We validate a subset of Chatter, called ChatterEval, using human\nannotations to serve as an evaluation benchmark for the character attribution\ntask in movie scripts. ChatterEval assesses narrative understanding and the\nlong-context modeling capacity of language models.", "published": "2024-11-07 22:37:30", "link": "http://arxiv.org/abs/2411.05227v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Precision", "abstract": "Low precision training and inference affect both the quality and cost of\nlanguage models, but current scaling laws do not account for this. In this\nwork, we devise \"precision-aware\" scaling laws for both training and inference.\nWe propose that training in lower precision reduces the model's \"effective\nparameter count,\" allowing us to predict the additional loss incurred from\ntraining in low precision and post-train quantization. For inference, we find\nthat the degradation introduced by post-training quantization increases as\nmodels are trained on more data, eventually making additional pretraining data\nactively harmful. For training, our scaling laws allow us to predict the loss\nof a model with different parts in different precisions, and suggest that\ntraining larger models in lower precision may be compute optimal. We unify the\nscaling laws for post and pretraining quantization to arrive at a single\nfunctional form that predicts degradation from training and inference in varied\nprecisions. We fit on over 465 pretraining runs and validate our predictions on\nmodel sizes up to 1.7B parameters trained on up to 26B tokens.", "published": "2024-11-07 00:10:10", "link": "http://arxiv.org/abs/2411.04330v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Robust and Efficient Fine-tuning of LLMs with Bayesian\n  Reparameterization of Low-Rank Adaptation", "abstract": "Large Language Models (LLMs) are highly resource-intensive to fine-tune due\nto their enormous size. While low-rank adaptation is a prominent\nparameter-efficient fine-tuning approach, it suffers from sensitivity to\nhyperparameter choices, leading to instability in model performance on\nfine-tuning downstream tasks. This paper highlights the importance of effective\nparameterization in low-rank fine-tuning to reduce estimator variance and\nenhance the stability of final model outputs. We propose MonteCLoRA, an\nefficient fine-tuning technique, employing Monte Carlo estimation to learn an\nunbiased posterior estimation of low-rank parameters with low expected\nvariance, which stabilizes fine-tuned LLMs with only O(1) additional\nparameters. MonteCLoRA shows significant improvements in accuracy and\nrobustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness\nthan existing efficient fine-tuning methods on natural language understanding\ntasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with\npre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance\nwith 50% lower variance than the contemporary efficient fine-tuning methods.\nThe theoretical and empirical results presented in the paper underscore how\nparameterization and hyperpriors balance exploration-exploitation in the\nlow-rank parametric space, therefore leading to more optimal and robust\nparameter estimation during efficient fine-tuning.", "published": "2024-11-07 01:31:48", "link": "http://arxiv.org/abs/2411.04358v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "abstract": "Recent advances in large language models (LLMs) show the potential of using\nLLMs as evaluators for assessing the quality of text generations from LLMs.\nHowever, applying LLM evaluators naively to compare or judge between different\nsystems can lead to unreliable results due to the intrinsic win rate estimation\nbias of LLM evaluators. In order to mitigate this problem, we propose two\ncalibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian\nDawid-Skene, both of which leverage Bayesian inference to more accurately infer\nthe true win rate of generative language models. We empirically validate our\nmethods on six datasets covering story generation, summarization, and\ninstruction following tasks. We show that both our methods are effective in\nimproving the accuracy of win rate estimation using LLMs as evaluators,\noffering a promising direction for reliable automatic text quality evaluation.", "published": "2024-11-07 04:32:40", "link": "http://arxiv.org/abs/2411.04424v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ACCIO: Table Understanding Enhanced via Contrastive Learning with\n  Aggregations", "abstract": "The attention to table understanding using recent natural language models has\nbeen growing. However, most related works tend to focus on learning the\nstructure of the table directly. Just as humans improve their understanding of\nsentences by comparing them, they can also enhance their understanding by\ncomparing tables. With this idea, in this paper, we introduce ACCIO, tAble\nunderstanding enhanCed via Contrastive learnIng with aggregatiOns, a novel\napproach to enhancing table understanding by contrasting original tables with\ntheir pivot summaries through contrastive learning. ACCIO trains an encoder to\nbring these table pairs closer together. Through validation via column type\nannotation, ACCIO achieves competitive performance with a macro F1 score of\n91.1 compared to state-of-the-art methods. This work represents the first\nattempt to utilize pairs of tables for table embedding, promising significant\nadvancements in table comprehension. Our code is available at\nhttps://github.com/whnhch/ACCIO/.", "published": "2024-11-07 05:35:39", "link": "http://arxiv.org/abs/2411.04443v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Meta-Reasoning Improves Tool Use in Large Language Models", "abstract": "External tools help large language models succeed at tasks where they would\notherwise typically fail. In existing frameworks, choosing tools at test time\nrelies on naive greedy decoding, regardless of whether the model has been\nfine-tuned on tool-annotated data or prompted with in-context examples. In\ncontrast, we find that gathering and choosing among a suitable set of candidate\ntools has greater potential to lead to an optimal selection. We present Tool\nselECTion via meta-reasONing (TECTON), a two-phase system that first reasons\nover a task and outputs candidate tools using a custom fine-tuned language\nmodelling head. Then, with the custom head disabled, it meta-reasons (i.e., it\nreasons over the previous reasoning process) to make a final choice. We show\nthat TECTON results in substantial gains--both in-distribution and\nout-of-distribution--on a range of math reasoning datasets.", "published": "2024-11-07 08:48:33", "link": "http://arxiv.org/abs/2411.04535v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Best Practices for Distilling Large Language Models into BERT for Web\n  Search Ranking", "abstract": "Recent studies have highlighted the significant potential of Large Language\nModels (LLMs) as zero-shot relevance rankers. These methods predominantly\nutilize prompt learning to assess the relevance between queries and documents\nby generating a ranked list of potential documents. Despite their promise, the\nsubstantial costs associated with LLMs pose a significant challenge for their\ndirect implementation in commercial search systems. To overcome this barrier\nand fully exploit the capabilities of LLMs for text ranking, we explore\ntechniques to transfer the ranking expertise of LLMs to a more compact model\nsimilar to BERT, using a ranking loss to enable the deployment of less\nresource-intensive models. Specifically, we enhance the training of LLMs\nthrough Continued Pre-Training, taking the query as input and the clicked title\nand summary as output. We then proceed with supervised fine-tuning of the LLM\nusing a rank loss, assigning the final token as a representative of the entire\nsentence. Given the inherent characteristics of autoregressive language models,\nonly the final token </s> can encapsulate all preceding tokens. Additionally,\nwe introduce a hybrid point-wise and margin MSE loss to transfer the ranking\nknowledge from LLMs to smaller models like BERT. This method creates a viable\nsolution for environments with strict resource constraints. Both offline and\nonline evaluations have confirmed the efficacy of our approach, and our model\nhas been successfully integrated into a commercial web search engine as of\nFebruary 2024.", "published": "2024-11-07 08:54:46", "link": "http://arxiv.org/abs/2411.04539v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Pruning Literals for Highly Efficient Explainability at Word Level", "abstract": "Designing an explainable model becomes crucial now for Natural Language\nProcessing(NLP) since most of the state-of-the-art machine learning models\nprovide a limited explanation for the prediction. In the spectrum of an\nexplainable model, Tsetlin Machine(TM) is promising because of its capability\nof providing word-level explanation using proposition logic. However, concern\nrises over the elaborated combination of literals (propositional logic) in the\nclause that makes the model difficult for humans to comprehend, despite having\na transparent learning process. In this paper, we design a post-hoc pruning of\nclauses that eliminate the randomly placed literals in the clause thereby\nmaking the model more efficiently interpretable than the vanilla TM.\nExperiments on the publicly available YELP-HAT Dataset demonstrate that the\nproposed pruned TM's attention map aligns more with the human attention map\nthan the vanilla TM's attention map. In addition, the pairwise similarity\nmeasure also surpasses the attention map-based neural network models. In terms\nof accuracy, the proposed pruning method does not degrade the accuracy\nsignificantly but rather enhances the performance up to 4% to 9% in some test\ndata.", "published": "2024-11-07 09:28:38", "link": "http://arxiv.org/abs/2411.04557v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tibyan Corpus: Balanced and Comprehensive Error Coverage Corpus Using\n  ChatGPT for Arabic Grammatical Error Correction", "abstract": "Natural language processing (NLP) utilizes text data augmentation to overcome\nsample size constraints. Increasing the sample size is a natural and widely\nused strategy for alleviating these challenges. In this study, we chose Arabic\nto increase the sample size and correct grammatical errors. Arabic is\nconsidered one of the languages with limited resources for grammatical error\ncorrection (GEC). Furthermore, QALB-14 and QALB-15 are the only datasets used\nin most Arabic grammatical error correction research, with approximately 20,500\nparallel examples, which is considered low compared with other languages.\nTherefore, this study aims to develop an Arabic corpus called \"Tibyan\" for\ngrammatical error correction using ChatGPT. ChatGPT is used as a data augmenter\ntool based on a pair of Arabic sentences containing grammatical errors matched\nwith a sentence free of errors extracted from Arabic books, called guide\nsentences. Multiple steps were involved in establishing our corpus, including\nthe collection and pre-processing of a pair of Arabic texts from various\nsources, such as books and open-access corpora. We then used ChatGPT to\ngenerate a parallel corpus based on the text collected previously, as a guide\nfor generating sentences with multiple types of errors. By engaging linguistic\nexperts to review and validate the automatically generated sentences, we\nensured that they were correct and error-free. The corpus was validated and\nrefined iteratively based on feedback provided by linguistic experts to improve\nits accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to\nanalyze the types of errors in the Tibyan corpus. Our corpus contained 49 of\nerrors, including seven types: orthography, morphology, syntax, semantics,\npunctuation, merge, and split. The Tibyan corpus contains approximately 600 K\ntokens.", "published": "2024-11-07 10:17:40", "link": "http://arxiv.org/abs/2411.04588v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Calibrated Listwise Reranking with Large Language Models", "abstract": "Large language models (LLMs), with advanced linguistic capabilities, have\nbeen employed in reranking tasks through a sequence-to-sequence approach. In\nthis paradigm, multiple passages are reranked in a listwise manner and a\ntextual reranked permutation is generated. However, due to the limited context\nwindow of LLMs, this reranking paradigm requires a sliding window strategy to\niteratively handle larger candidate sets. This not only increases computational\ncosts but also restricts the LLM from fully capturing all the comparison\ninformation for all candidates. To address these challenges, we propose a novel\nself-calibrated listwise reranking method, which aims to leverage LLMs to\nproduce global relevance scores for ranking. To achieve it, we first propose\nthe relevance-aware listwise reranking framework, which incorporates explicit\nlist-view relevance scores to improve reranking efficiency and enable global\ncomparison across the entire candidate set. Second, to ensure the comparability\nof the computed scores, we propose self-calibrated training that uses\npoint-view relevance assessments generated internally by the LLM itself to\ncalibrate the list-view relevance assessments. Extensive experiments and\ncomprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks\ndemonstrate the effectiveness and efficiency of our proposed method.", "published": "2024-11-07 10:31:31", "link": "http://arxiv.org/abs/2411.04602v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Kwai-STaR: Transform LLMs into State-Transition Reasoners", "abstract": "Mathematical reasoning presents a significant challenge to the cognitive\ncapabilities of LLMs. Various methods have been proposed to enhance the\nmathematical ability of LLMs. However, few recognize the value of state\ntransition for LLM reasoning. In this work, we define mathematical\nproblem-solving as a process of transiting from an initial unsolved state to\nthe final resolved state, and propose Kwai-STaR framework, which transforms\nLLMs into State-Transition Reasoners to improve their intuitive reasoning\ncapabilities. Our approach comprises three main steps: (1) Define the state\nspace tailored to the mathematical reasoning. (2) Generate state-transition\ndata based on the state space. (3) Convert original LLMs into State-Transition\nReasoners via a curricular training strategy. Our experiments validate the\neffectiveness of Kwai-STaR in enhancing mathematical reasoning: After training\non the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and\nLLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard\ndataset. Additionally, the state transition-based design endows Kwai-STaR with\nremarkable training and inference efficiency. Further experiments are underway\nto establish the generality of Kwai-STaR.", "published": "2024-11-07 15:38:25", "link": "http://arxiv.org/abs/2411.04799v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Spanish Political Party Tweets Using Pre-trained\n  Language Models", "abstract": "Title: Sentiment Analysis of Spanish Political Party Communications on\nTwitter Using Pre-trained Language Models\n  Authors: Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen\n  Comments: 21 pages, 6 figures\n  Abstract: This study investigates sentiment patterns within Spanish political\nparty communications on Twitter by leveraging BETO and RoBERTuito, two\npre-trained language models optimized for Spanish text. Using a dataset of\ntweets from major Spanish political parties: PSOE, PP, Vox, Podemos, and\nCiudadanos, spanning 2019 to 2024, this research analyzes sentiment\ndistributions and explores the relationship between sentiment expression and\nparty ideology. The findings indicate that both models consistently identify a\npredominant Neutral sentiment across all parties, with significant variations\nin Negative and Positive sentiments that align with ideological distinctions.\nSpecifically, Vox exhibits higher levels of Negative sentiment, while PSOE\ndemonstrates relatively high Positive sentiment, supporting the hypothesis that\nemotional appeals in political messaging reflect ideological stances. This\nstudy underscores the potential of pre-trained language models for non-English\nsentiment analysis on social media, providing insights into sentiment dynamics\nthat shape public discourse within Spain's multi-party political system.\n  Keywords: Spanish politics, sentiment analysis, pre-trained language models,\nTwitter, BETO, RoBERTuito, political ideology, multi-party system", "published": "2024-11-07 16:53:09", "link": "http://arxiv.org/abs/2411.04862v1", "categories": ["cs.CL", "cs.CY", "68T50 (Natural Language Processing), 68T10 (Pattern Recognition,\n  Speech Recognition), 91F10 (Political Science)"], "primary_category": "cs.CL"}
{"title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models", "abstract": "Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems. While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an \"open cookbook\" for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI.", "published": "2024-11-07 17:47:25", "link": "http://arxiv.org/abs/2411.04905v3", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Position Paper On Diagnostic Uncertainty Estimation from Large Language\n  Models: Next-Word Probability Is Not Pre-test Probability", "abstract": "Large language models (LLMs) are being explored for diagnostic decision\nsupport, yet their ability to estimate pre-test probabilities, vital for\nclinical decision-making, remains limited. This study evaluates two LLMs,\nMistral-7B and Llama3-70B, using structured electronic health record data on\nthree diagnosis tasks. We examined three current methods of extracting LLM\nprobability estimations and revealed their limitations. We aim to highlight the\nneed for improved techniques in LLM confidence estimation.", "published": "2024-11-07 18:39:04", "link": "http://arxiv.org/abs/2411.04962v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs", "abstract": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.", "published": "2024-11-07 18:41:50", "link": "http://arxiv.org/abs/2411.04965v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation", "abstract": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared space using contrastive learning on large-scale image-text pairs.\nIts strength lies in leveraging natural language as a rich supervisory signal.\nWith the rapid progress of large language models (LLMs), we explore their\npotential to further enhance CLIP's multimodal representation learning. This\nwork introduces a fine-tuning approach that integrates LLMs with the pretrained\nCLIP visual encoder, leveraging LLMs' advanced text understanding and\nopen-world knowledge to improve CLIP's ability to process long and complex\ncaptions. To address the challenge of LLMs' autoregressive nature, we propose a\ncaption-to-caption contrastive learning framework to enhance the discriminative\npower of their outputs. Our method achieves substantial performance gains on\nvarious downstream tasks, demonstrating the effectiveness of combining LLMs\nwith CLIP for enhanced multimodal learning.", "published": "2024-11-07 18:59:16", "link": "http://arxiv.org/abs/2411.04997v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Selecting Between BERT and GPT for Text Classification in Political\n  Science Research", "abstract": "Political scientists often grapple with data scarcity in text classification.\nRecently, fine-tuned BERT models and their variants have gained traction as\neffective solutions to address this issue. In this study, we investigate the\npotential of GPT-based models combined with prompt engineering as a viable\nalternative. We conduct a series of experiments across various classification\ntasks, differing in the number of classes and complexity, to evaluate the\neffectiveness of BERT-based versus GPT-based models in low-data scenarios. Our\nfindings indicate that while zero-shot and few-shot learning with GPT models\nprovide reasonable performance and are well-suited for early-stage research\nexploration, they generally fall short - or, at best, match - the performance\nof BERT fine-tuning, particularly as the training set reaches a substantial\nsize (e.g., 1,000 samples). We conclude by comparing these approaches in terms\nof performance, ease of use, and cost, providing practical guidance for\nresearchers facing data limitations. Our results are particularly relevant for\nthose engaged in quantitative text analysis in low-resource settings or with\nlimited labeled data.", "published": "2024-11-07 07:29:39", "link": "http://arxiv.org/abs/2411.05050v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FMEA Builder: Expert Guided Text Generation for Equipment Maintenance", "abstract": "Foundation models show great promise for generative tasks in many domains.\nHere we discuss the use of foundation models to generate structured documents\nrelated to critical assets. A Failure Mode and Effects Analysis (FMEA) captures\nthe composition of an asset or piece of equipment, the ways it may fail and the\nconsequences thereof. Our system uses large language models to enable fast and\nexpert supervised generation of new FMEA documents. Empirical analysis shows\nthat foundation models can correctly generate over half of an FMEA's key\ncontent. Results from polling audiences of reliability professionals show a\npositive outlook on using generative AI to create these documents for critical\nassets.", "published": "2024-11-07 13:11:03", "link": "http://arxiv.org/abs/2411.05054v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Precision or Recall? An Analysis of Image Captions for Training\n  Text-to-Image Generation Model", "abstract": "Despite advancements in text-to-image models, generating images that\nprecisely align with textual descriptions remains challenging due to\nmisalignment in training data. In this paper, we analyze the critical role of\ncaption precision and recall in text-to-image model training. Our analysis of\nhuman-annotated captions shows that both precision and recall are important for\ntext-image alignment, but precision has a more significant impact. Leveraging\nthese insights, we utilize Large Vision Language Models to generate synthetic\ncaptions for training. Models trained with these synthetic captions show\nsimilar behavior to those trained on human-annotated captions, underscores the\npotential for synthetic data in text-to-image training.", "published": "2024-11-07 19:00:37", "link": "http://arxiv.org/abs/2411.05079v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Explaining Mixtures of Sources in News Articles", "abstract": "Human writers plan, then write. For large language models (LLMs) to play a\nrole in longer-form article generation, we must understand the planning steps\nhumans make before writing. We explore one kind of planning, source-selection\nin news, as a case-study for evaluating plans in long-form generation. We ask:\nwhy do specific stories call for specific kinds of sources? We imagine a\ngenerative process for story writing where a source-selection schema is first\nselected by a journalist, and then sources are chosen based on categories in\nthat schema. Learning the article's plan means predicting the schema initially\nchosen by the journalist. Working with professional journalists, we adapt five\nexisting schemata and introduce three new ones to describe journalistic plans\nfor the inclusion of sources in documents. Then, inspired by Bayesian\nlatent-variable modeling, we develop metrics to select the most likely plan, or\nschema, underlying a story, which we use to compare schemata. We find that two\nschemata: stance and social affiliation best explain source plans in most\ndocuments. However, other schemata like textual entailment explain source plans\nin factually rich topics like \"Science\". Finally, we find we can predict the\nmost suitable schema given just the article's headline with reasonable\naccuracy. We see this as an important case-study for human planning, and\nprovides a framework and approach for evaluating other kinds of plans. We\nrelease a corpora, NewsSources, with annotations for 4M articles.", "published": "2024-11-07 21:34:05", "link": "http://arxiv.org/abs/2411.05192v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Alopex: A Computational Framework for Enabling On-Device Function Calls\n  with LLMs", "abstract": "The rapid advancement of Large Language Models (LLMs) has led to their\nincreased integration into mobile devices for personalized assistance, which\nenables LLMs to call external API functions to enhance their performance.\nHowever, challenges such as data scarcity, ineffective question formatting, and\ncatastrophic forgetting hinder the development of on-device LLM agents. To\ntackle these issues, we propose Alopex, a framework that enables precise\non-device function calls using the Fox LLM. Alopex introduces a logic-based\nmethod for generating high-quality training data and a novel\n``description-question-output'' format for fine-tuning, reducing risks of\nfunction information leakage. Additionally, a data mixing strategy is used to\nmitigate catastrophic forgetting, combining function call data with textbook\ndatasets to enhance performance in various tasks. Experimental results show\nthat Alopex improves function call accuracy and significantly reduces\ncatastrophic forgetting, providing a robust solution for integrating function\ncall capabilities into LLMs without manual intervention.", "published": "2024-11-07 22:15:17", "link": "http://arxiv.org/abs/2411.05209v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Abstract2Appendix: Academic Reviews Enhance LLM Long-Context\n  Capabilities", "abstract": "Large language models (LLMs) have shown remarkable performance across various\ntasks, yet their ability to handle long-context reading remains challenging.\nThis study explores the effectiveness of leveraging high-quality academic peer\nreview data for fine-tuning LLMs to enhance their long-context capabilities. We\ncompare the Direct Preference Optimization (DPO) method with the Supervised\nFine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency.\nOur experiments show that the fine-tuned model achieves a 4.04-point\nimprovement over phi-3 and a 2.6\\% increase on the Qasper benchmark using only\n2000 samples. Despite facing limitations in data scale and processing costs,\nthis study underscores the potential of DPO and high-quality data in advancing\nLLM performance.\n  Additionally, the zero-shot benchmark results indicate that aggregated\nhigh-quality human reviews are overwhelmingly preferred over LLM-generated\nresponses, even for the most capable models like GPT-4o. This suggests that\nhigh-quality human reviews are extremely rich in information, reasoning, and\nlong-context retrieval, capabilities that even the most advanced models have\nnot fully captured. These findings highlight the high utility of leveraging\nhuman reviews to further advance the field.", "published": "2024-11-07 22:57:02", "link": "http://arxiv.org/abs/2411.05232v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deploying Large Language Models With Retrieval Augmented Generation", "abstract": "Knowing that the generative capabilities of large language models (LLM) are\nsometimes hampered by tendencies to hallucinate or create non-factual\nresponses, researchers have increasingly focused on methods to ground generated\noutputs in factual data. Retrieval Augmented Generation (RAG) has emerged as a\nkey approach for integrating knowledge from data sources outside of the LLM's\ntraining set, including proprietary and up-to-date information. While many\nresearch papers explore various RAG strategies, their true efficacy is tested\nin real-world applications with actual data. The journey from conceiving an\nidea to actualizing it in the real world is a lengthy process. We present\ninsights from the development and field-testing of a pilot project that\nintegrates LLMs with RAG for information retrieval. Additionally, we examine\nthe impacts on the information value chain, encompassing people, processes, and\ntechnology. Our aim is to identify the opportunities and challenges of\nimplementing this emerging technology, particularly within the context of\nbehavioral research in the information systems (IS) field. The contributions of\nthis work include the development of best practices and recommendations for\nadopting this promising technology while ensuring compliance with industry\nregulations through a proposed AI governance model.", "published": "2024-11-07 22:11:51", "link": "http://arxiv.org/abs/2411.11895v1", "categories": ["cs.IR", "cs.CL", "68T01", "I.2.0"], "primary_category": "cs.IR"}
{"title": "Variational Low-Rank Adaptation Using IVON", "abstract": "We show that variational learning can significantly improve the accuracy and\ncalibration of Low-Rank Adaptation (LoRA) without a substantial increase in the\ncost. We replace AdamW by the Improved Variational Online Newton (IVON)\nalgorithm to finetune large language models. For Llama-2 with 7 billion\nparameters, IVON improves the accuracy over AdamW by 2.8% and expected\ncalibration error by 4.6%. The accuracy is also better than the other Bayesian\nalternatives, yet the cost is lower and the implementation is easier. Our work\nprovides additional evidence for the effectiveness of IVON for large language\nmodels. The code is available at\nhttps://github.com/team-approx-bayes/ivon-lora.", "published": "2024-11-07 04:17:30", "link": "http://arxiv.org/abs/2411.04421v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multistage Fine-tuning Strategies for Automatic Speech Recognition in\n  Low-resource Languages", "abstract": "This paper presents a novel multistage fine-tuning strategy designed to\nenhance automatic speech recognition (ASR) performance in low-resource\nlanguages using OpenAI's Whisper model. In this approach we aim to build ASR\nmodel for languages with limited digital resources by sequentially adapting the\nmodel across linguistically similar languages. We experimented this on the\nMalasar language, a Dravidian language spoken by approximately ten thousand\npeople in the Western Ghats of South India. Malasar language faces critical\nchallenges for technological intervention due to its lack of a native script\nand absence of digital or spoken data resources. Working in collaboration with\nWycliffe India and Malasar community members, we created a spoken Malasar\ncorpus paired with transcription in Tamil script, a closely related major\nlanguage. In our approach to build ASR model for Malasar, we first build an\nintermediate Tamil ASR, leveraging higher data availability for Tamil annotated\nspeech. This intermediate model is subsequently fine-tuned on Malasar data,\nallowing for more effective ASR adaptation despite limited resources. The\nmultistage fine-tuning strategy demonstrated significant improvements over\ndirect fine-tuning on Malasar data alone, achieving a word error rate (WER) of\n51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning\nmethod. Further a WER reduction to 47.3% was achieved through punctuation\nremoval in post-processing, which addresses formatting inconsistencies that\nimpact evaluation. Our results underscore the effectiveness of sequential\nmultistage fine-tuning combined with targeted post-processing as a scalable\nstrategy for ASR system development in low-resource languages, especially where\nlinguistic similarities can be leveraged to bridge gaps in training data.", "published": "2024-11-07 09:57:57", "link": "http://arxiv.org/abs/2411.04573v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DISCO: DISCovering Overfittings as Causal Rules for Text Classification\n  Models", "abstract": "With the rapid advancement of neural language models, the deployment of\nover-parameterized models has surged, increasing the need for interpretable\nexplanations comprehensible to human inspectors. Existing post-hoc\ninterpretability methods, which often focus on unigram features of single input\ntextual instances, fail to capture the models' decision-making process fully.\nAdditionally, many methods do not differentiate between decisions based on\nspurious correlations and those based on a holistic understanding of the input.\nOur paper introduces DISCO, a novel method for discovering global, rule-based\nexplanations by identifying causal n-gram associations with model predictions.\nThis method employs a scalable sequence mining technique to extract relevant\ntext spans from training data, associate them with model predictions, and\nconduct causality checks to distill robust rules that elucidate model behavior.\nThese rules expose potential overfitting and provide insights into misleading\nfeature combinations. We validate DISCO through extensive testing,\ndemonstrating its superiority over existing methods in offering comprehensive\ninsights into complex model behaviors. Our approach successfully identifies all\nshortcuts manually introduced into the training data (100% detection rate on\nthe MultiRC dataset), resulting in an 18.8% regression in model performance --\na capability unmatched by any other method. Furthermore, DISCO supports\ninteractive explanations, enabling human inspectors to distinguish spurious\ncauses in the rule-based output. This alleviates the burden of abundant\ninstance-wise explanations and helps assess the model's risk when encountering\nout-of-distribution (OOD) data.", "published": "2024-11-07 12:12:44", "link": "http://arxiv.org/abs/2411.04649v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.3; I.2.7"], "primary_category": "cs.AI"}
{"title": "KnowCoder-X: Boosting Multilingual Information Extraction via Code", "abstract": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual\nalignment. However, although LLMs show promising cross-lingual alignment in IE,\na significant imbalance across languages persists, highlighting an underlying\ndeficiency. To address this, we propose KnowCoder-X, a powerful code LLM with\nadvanced cross-lingual and multilingual capabilities for universal information\nextraction. Firstly, it standardizes the representation of multilingual schemas\nusing Python classes, ensuring a consistent ontology across different\nlanguages. Then, IE across languages is formulated as a unified code generation\ntask. Secondly, we enhance the model's cross-lingual transferability through IE\ncross-lingual alignment instruction tuning on a translated instance prediction\ntask we proposed. During this phase, we also construct a high-quality and\ndiverse bilingual IE parallel dataset with 257k samples, called ParallelNER,\nsynthesized by our proposed robust three-stage pipeline, with manual annotation\nto ensure quality. Although without training in 29 unseen languages,\nKnowCoder-X surpasses ChatGPT by $30.17\\%$ and SoTA by $20.03\\%$, thereby\ndemonstrating superior cross-lingual IE capabilities. Comprehensive evaluations\non 64 IE benchmarks in Chinese and English under various settings demonstrate\nthat KnowCoder-X significantly enhances cross-lingual IE transfer through\nboosting the IE alignment. Our code and dataset are available at:\nhttps://github.com/ICT-GoKnow/KnowCoder", "published": "2024-11-07 15:36:05", "link": "http://arxiv.org/abs/2411.04794v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and\n  Benchmark Models", "abstract": "Existing text simplification or paraphrase datasets mainly focus on\nsentence-level text generation in a general domain. These datasets are\ntypically developed without using domain knowledge. In this paper, we release a\nnovel dataset, VTechAGP, which is the first academic-to-general-audience text\nparaphrase dataset consisting of document-level these and dissertation academic\nand general-audience abstract pairs from 8 colleges authored over 25 years. We\nalso propose a novel dynamic soft prompt generative language model, DSPT5. For\ntraining, we leverage a contrastive-generative loss function to learn the\nkeyword vectors in the dynamic prompt. For inference, we adopt a crowd-sampling\ndecoding strategy at both semantic and structural levels to further select the\nbest output candidate. We evaluate DSPT5 and various state-of-the-art large\nlanguage models (LLMs) from multiple perspectives. Results demonstrate that the\nSOTA LLMs do not provide satisfactory outcomes, while the lightweight DSPT5 can\nachieve competitive results. To the best of our knowledge, we are the first to\nbuild a benchmark dataset and solutions for academic-to-general-audience text\nparaphrase dataset. Models will be public after acceptance.", "published": "2024-11-07 16:06:00", "link": "http://arxiv.org/abs/2411.04825v2", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPTKB: Comprehensively Materializing Factual LLM Knowledge", "abstract": "LLMs have majorly advanced NLP and AI, and next to their ability to perform a\nwide range of procedural tasks, a major success factor is their internalized\nfactual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has\ngained attention. However, most approaches investigate one question at a time\nvia modest-sized pre-defined samples, introducing an availability bias (Tversky\nand Kahnemann, 1973) that prevents the discovery of knowledge (or beliefs) of\nLLMs beyond the experimenter's predisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterializing an LLM's factual knowledge through recursive querying and result\nconsolidation.\n  As a prototype, we employ GPT-4o-mini to construct GPTKB, a large-scale\nknowledge base (KB) comprising 105 million triples for over 2.9 million\nentities - achieved at 1% of the cost of previous KB projects. This work marks\na milestone in two areas: For LLM research, for the first time, it provides\nconstructive insights into the scope and structure of LLMs' knowledge (or\nbeliefs). For KB construction, it pioneers new pathways for the long-standing\nchallenge of general-domain KB construction. GPTKB is accessible at\nhttps://gptkb.org.", "published": "2024-11-07 17:57:03", "link": "http://arxiv.org/abs/2411.04920v3", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page\n  Multi-document Understanding", "abstract": "Document visual question answering (DocVQA) pipelines that answer questions\nfrom documents have broad applications. Existing methods focus on handling\nsingle-page documents with multi-modal language models (MLMs), or rely on\ntext-based retrieval-augmented generation (RAG) that uses text extraction tools\nsuch as optical character recognition (OCR). However, there are difficulties in\napplying these methods in real-world scenarios: (a) questions often require\ninformation across different pages or documents, where MLMs cannot handle many\nlong documents; (b) documents often have important information in visual\nelements such as figures, but text extraction tools ignore them. We introduce\nM3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various\ndocument contexts (closed-domain and open-domain), question hops (single-hop\nand multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG\nfinds relevant documents and answers questions using a multi-modal retriever\nand an MLM, so that it can efficiently handle single or many documents while\npreserving visual information. Since previous DocVQA datasets ask questions in\nthe context of a specific document, we also present M3DocVQA, a new benchmark\nfor evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.\nIn three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results\nshow that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance\nthan many strong baselines, including state-of-the-art performance in\nMP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and\nretrieval models. Lastly, we qualitatively show that M3DocRAG can successfully\nhandle various scenarios, such as when relevant information exists across\nmultiple pages and when answer evidence only exists in images.", "published": "2024-11-07 18:29:38", "link": "http://arxiv.org/abs/2411.04952v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SuffixDecoding: A Model-Free Approach to Speeding Up Large Language\n  Model Inference", "abstract": "We present SuffixDecoding, a novel model-free approach to accelerating large\nlanguage model (LLM) inference through speculative decoding. Unlike existing\nmethods that rely on draft models or specialized decoding heads, SuffixDecoding\nleverages suffix trees built from previously generated outputs to efficiently\npredict candidate token sequences. Our approach enables flexible\ntree-structured speculation without the overhead of maintaining and\norchestrating additional models. SuffixDecoding builds and dynamically updates\nsuffix trees to capture patterns in the generated text, using them to construct\nspeculation trees through a principled scoring mechanism based on empirical\ntoken frequencies. SuffixDecoding requires only CPU memory which is plentiful\nand underutilized on typical LLM serving nodes. We demonstrate that\nSuffixDecoding achieves competitive speedups compared to model-based approaches\nacross diverse workloads including open-domain chat, code generation, and\ntext-to-SQL tasks. For open-ended chat and code generation tasks,\nSuffixDecoding achieves up to $1.4\\times$ higher output throughput than\nSpecInfer and up to $1.1\\times$ lower time-per-token (TPOT) latency. For a\nproprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to\n$2.9\\times$ higher output throughput and $3\\times$ lower latency than\nspeculative decoding. Our evaluation shows that SuffixDecoding maintains high\nacceptance rates even with small reference corpora of 256 examples, while\ncontinuing to improve performance as more historical outputs are incorporated.", "published": "2024-11-07 18:49:33", "link": "http://arxiv.org/abs/2411.04975v1", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing The Language of Visual Tokens", "abstract": "With the introduction of transformer-based models for vision and language\ntasks, such as LLaVA and Chameleon, there has been renewed interest in the\ndiscrete tokenized representation of images. These models often treat image\npatches as discrete tokens, analogous to words in natural language, learning\njoint alignments between visual and human languages. However, little is known\nabout the statistical behavior of these visual languages - whether they follow\nsimilar frequency distributions, grammatical structures, or topologies as\nnatural languages. In this paper, we take a natural-language-centric approach\nto analyzing discrete visual languages and uncover striking similarities and\nfundamental differences. We demonstrate that, although visual languages adhere\nto Zipfian distributions, higher token innovation drives greater entropy and\nlower compression, with tokens predominantly representing object parts,\nindicating intermediate granularity. We also show that visual languages lack\ncohesive grammatical structures, leading to higher perplexity and weaker\nhierarchical organization compared to natural languages. Finally, we\ndemonstrate that, while vision models align more closely with natural languages\nthan other models, this alignment remains significantly weaker than the\ncohesion found within natural languages. Through these experiments, we\ndemonstrate how understanding the statistical properties of discrete visual\nlanguages can inform the design of more effective computer vision models.", "published": "2024-11-07 18:59:28", "link": "http://arxiv.org/abs/2411.05001v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PhoneLM:an Efficient and Capable Small Language Model Family through\n  Principled Pre-training", "abstract": "The interest in developing small language models (SLM) for on-device\ndeployment is fast growing. However, the existing SLM design hardly considers\nthe device hardware characteristics. Instead, this work presents a simple yet\neffective principle for SLM design: architecture searching for (near-)optimal\nruntime efficiency before pre-training. Guided by this principle, we develop\nPhoneLM SLM family (currently with 0.5B and 1.5B versions), that acheive the\nstate-of-the-art capability-efficiency tradeoff among those with similar\nparameter size. We fully open-source the code, weights, and training datasets\nof PhoneLM for reproducibility and transparency, including both base and\ninstructed versions. We also release a finetuned version of PhoneLM capable of\naccurate Android Intent invocation, and an end-to-end Android demo. All\nmaterials are available at https://github.com/UbiquitousLearning/PhoneLM.", "published": "2024-11-07 02:19:00", "link": "http://arxiv.org/abs/2411.05046v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge\n  into LLMs?", "abstract": "There is great interest in fine-tuning frontier large language models (LLMs)\nto inject new information and update existing knowledge. While commercial LLM\nfine-tuning APIs from providers such as OpenAI and Google promise flexible\nadaptation for various applications, the efficacy of fine-tuning remains\nunclear. In this study, we introduce FineTuneBench, an evaluation framework and\ndataset for understanding how well commercial fine-tuning APIs can successfully\nlearn new and updated knowledge. We analyze five frontier LLMs with\ncommercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro,\non their effectiveness in two settings: (1) ingesting novel information, such\nas recent news events and new people profiles, and (2) updating existing\nknowledge, such as updated medical guidelines and code frameworks. Our results\nreveal substantial shortcomings in all the models' abilities to effectively\nlearn new information through fine-tuning, with an average generalization\naccuracy of 37% across all models. When updating existing knowledge, such as\nincorporating medical guideline updates, commercial fine-tuning APIs show even\nmore limited capability (average generalization accuracy of 19%). Overall,\nfine-tuning GPT-4o mini is the most effective for infusing new knowledge and\nupdating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs\nfor Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or\nupdate existing knowledge. These findings underscore a major shortcoming in\nusing current commercial fine-tuning services to achieve reliable knowledge\ninfusion in common scenarios. We open source the FineTuneBench dataset at\nhttps://github.com/kevinwu23/StanfordFineTuneBench.", "published": "2024-11-07 18:22:14", "link": "http://arxiv.org/abs/2411.05059v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Guide to Misinformation Detection Data and Evaluation", "abstract": "Misinformation is a complex societal issue, and mitigating solutions are\ndifficult to create due to data deficiencies. To address this, we have curated\nthe largest collection of (mis)information datasets in the literature, totaling\n75. From these, we evaluated the quality of 36 datasets that consist of\nstatements or claims, as well as the 9 datasets that consist of data in purely\nparagraph form. We assess these datasets to identify those with solid\nfoundations for empirical work and those with flaws that could result in\nmisleading and non-generalizable results, such as spurious correlations, or\nexamples that are ambiguous or otherwise impossible to assess for veracity. We\nfind the latter issue is particularly severe and affects most datasets in the\nliterature. We further provide state-of-the-art baselines on all these\ndatasets, but show that regardless of label quality, categorical labels may no\nlonger give an accurate evaluation of detection model performance. Finally, we\nwe propose and highlight Evaluation Quality Assessment (EQA) as a tool to guide\nthe field toward systemic solutions rather than inadvertently propagating\nissues in evaluation. Overall, this guide aims to provide a roadmap for higher\nquality data and better grounded evaluations, ultimately improving research in\nmisinformation detection. All datasets and other artifacts are available at\nmisinfo-datasets.complexdatalab.com.", "published": "2024-11-07 18:47:39", "link": "http://arxiv.org/abs/2411.05060v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology\n  Report Generation", "abstract": "Radiology report generation (RRG) aims to create free-text radiology reports\nfrom clinical imaging. Grounded radiology report generation (GRRG) extends RRG\nby including the localisation of individual findings on the image. Currently,\nthere are no manually annotated chest X-ray (CXR) datasets to train GRRG\nmodels. In this work, we present a dataset called PadChest-GR\n(Grounded-Reporting) derived from PadChest aimed at training GRRG models for\nCXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with\ngrounded reports (3,099 abnormal and 1,456 normal), each containing complete\nlists of sentences describing individual present (positive) and absent\n(negative) findings in English and Spanish. In total, PadChest-GR contains\n7,037 positive and 3,422 negative finding sentences. Every positive finding\nsentence is associated with up to two independent sets of bounding boxes\nlabelled by different readers and has categorical labels for finding type,\nlocations, and progression. To the best of our knowledge, PadChest-GR is the\nfirst manually curated dataset designed to train GRRG models for understanding\nand interpreting radiological images and generated text. By including detailed\nlocalization and comprehensive annotations of all clinically relevant findings,\nit provides a valuable resource for developing and evaluating GRRG models from\nCXR images. PadChest-GR can be downloaded under request from\nhttps://bimcv.cipf.es/bimcv-projects/padchest-gr/", "published": "2024-11-07 19:06:17", "link": "http://arxiv.org/abs/2411.05085v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Watermarking Language Models through Language Models", "abstract": "This paper presents a novel framework for watermarking language models\nthrough prompts generated by language models. The proposed approach utilizes a\nmulti-model setup, incorporating a Prompting language model to generate\nwatermarking instructions, a Marking language model to embed watermarks within\ngenerated content, and a Detecting language model to verify the presence of\nthese watermarks. Experiments are conducted using ChatGPT and Mistral as the\nPrompting and Marking language models, with detection accuracy evaluated using\na pretrained classifier model. Results demonstrate that the proposed framework\nachieves high classification accuracy across various configurations, with 95%\naccuracy for ChatGPT, 88.79% for Mistral. These findings validate the and\nadaptability of the proposed watermarking strategy across different language\nmodel architectures. Hence the proposed framework holds promise for\napplications in content attribution, copyright protection, and model\nauthentication.", "published": "2024-11-07 19:16:49", "link": "http://arxiv.org/abs/2411.05091v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "abstract": "Value-based reinforcement learning (RL) can in principle learn effective\npolicies for a wide range of multi-turn problems, from games to dialogue to\nrobotic control, including via offline RL from static previously collected\ndatasets. However, despite the widespread use of policy gradient methods to\ntrain large language models for single turn tasks (e.g., question answering),\nvalue-based methods for multi-turn RL in an off-policy or offline setting have\nproven particularly challenging to scale to the setting of large language\nmodels. This setting requires effectively leveraging pretraining, scaling to\nlarge architectures with billions of parameters, and training on large\ndatasets, all of which represent major challenges for current value-based RL\nmethods. In this work, we propose a novel offline RL algorithm that addresses\nthese drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)\nproblem where the probabilities of tokens directly translate to Q-values. In\nthis way we obtain an algorithm that smoothly transitions from maximizing the\nlikelihood of the data during pretraining to learning a near-optimal Q-function\nduring finetuning. Our algorithm has strong theoretical foundations, enjoying\nperformance bounds similar to state-of-the-art Q-learning methods, while in\npractice utilizing an objective that closely resembles SFT. Because of this,\nour approach can enjoy the full benefits of the pretraining of language models,\nwithout the need to reinitialize any weights before RL finetuning, and without\nthe need to initialize new heads for predicting values or advantages.\nEmpirically, we evaluate our method on both pretrained LLMs and VLMs, on a\nvariety of tasks including both natural language dialogue and robotic\nmanipulation and navigation from images.", "published": "2024-11-07 21:36:52", "link": "http://arxiv.org/abs/2411.05193v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Interactive Dialogue Agents via Reinforcement Learning on Hindsight\n  Regenerations", "abstract": "Recent progress on large language models (LLMs) has enabled dialogue agents\nto generate highly naturalistic and plausible text. However, current LLM\nlanguage generation focuses on responding accurately to questions and requests\nwith a single effective response. In reality, many real dialogues are\ninteractive, meaning an agent's utterances will influence their conversational\npartner, elicit information, or change their opinion. Accounting for how an\nagent can effectively steer a conversation is a crucial ability in many\ndialogue tasks, from healthcare to preference elicitation. Existing methods for\nfine-tuning dialogue agents to accomplish such tasks would rely on curating\nsome amount of expert data. However, doing so often requires understanding the\nunderlying cognitive processes of the conversational partner, which is a skill\nneither humans nor LLMs trained on human data can reliably do. Our key insight\nis that while LLMs may not be adept at identifying effective strategies for\nsteering conversations a priori, or in the middle of an ongoing conversation,\nthey can do so post-hoc, or in hindsight, after seeing how their conversational\npartner responds. We use this fact to rewrite and augment existing suboptimal\ndata, and train via offline reinforcement learning (RL) an agent that\noutperforms both prompting and learning from unaltered human demonstrations. We\napply our approach to two domains that require understanding human mental\nstate, intelligent interaction, and persuasion: mental health support, and\nsoliciting charitable donations. Our results in a user study with real humans\nshow that our approach greatly outperforms existing state-of-the-art dialogue\nagents.", "published": "2024-11-07 21:37:51", "link": "http://arxiv.org/abs/2411.05194v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring How Generative MLLMs Perceive More Than CLIP with the Same\n  Vision Encoder", "abstract": "Recent research has shown that CLIP models struggle with visual reasoning\ntasks that require grounding compositionality, understanding spatial\nrelationships, or capturing fine-grained details. One natural hypothesis is\nthat the CLIP vision encoder does not embed essential information for these\ntasks. However, we find that this is not always the case: The encoder gathers\nquery-relevant visual information, while CLIP fails to extract it. In\nparticular, we show that another branch of Vision-Language Models (VLMs),\nGenerative Multimodal Large Language Models (MLLMs), achieve significantly\nhigher accuracy than CLIP in many of these tasks using the same vision encoder\nand weights, indicating that these Generative MLLMs perceive more -- as they\nextract and utilize visual information more effectively. We conduct a series of\ncontrolled experiments and reveal that their success is attributed to multiple\nkey design choices, including patch tokens, position embeddings, and\nprompt-based weighting. On the other hand, enhancing the training data alone or\napplying a stronger text encoder does not suffice to solve the task, and\nadditional text tokens offer little benefit. Interestingly, we find that\nfine-grained visual reasoning is not exclusive to generative models trained by\nan autoregressive loss: When converted into CLIP-like encoders by contrastive\nfinetuning, these MLLMs still outperform CLIP under the same cosine\nsimilarity-based evaluation protocol. Our study highlights the importance of\nVLM architectural choices and suggests directions for improving the performance\nof CLIP-like contrastive VLMs.", "published": "2024-11-07 21:39:51", "link": "http://arxiv.org/abs/2411.05195v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Toward Cultural Interpretability: A Linguistic Anthropological Framework\n  for Describing and Evaluating Large Language Models (LLMs)", "abstract": "This article proposes a new integration of linguistic anthropology and\nmachine learning (ML) around convergent interests in both the underpinnings of\nlanguage and making language technologies more socially responsible. While\nlinguistic anthropology focuses on interpreting the cultural basis for human\nlanguage use, the ML field of interpretability is concerned with uncovering the\npatterns that Large Language Models (LLMs) learn from human verbal behavior.\nThrough the analysis of a conversation between a human user and an LLM-powered\nchatbot, we demonstrate the theoretical feasibility of a new, conjoint field of\ninquiry, cultural interpretability (CI). By focusing attention on the\ncommunicative competence involved in the way human users and AI chatbots\nco-produce meaning in the articulatory interface of human-computer interaction,\nCI emphasizes how the dynamic relationship between language and culture makes\ncontextually sensitive, open-ended conversation possible. We suggest that, by\nexamining how LLMs internally \"represent\" relationships between language and\nculture, CI can: (1) provide insight into long-standing linguistic\nanthropological questions about the patterning of those relationships; and (2)\naid model developers and interface designers in improving value alignment\nbetween language models and stylistically diverse speakers and culturally\ndiverse speech communities. Our discussion proposes three critical research\naxes: relativity, variation, and indexicality.", "published": "2024-11-07 22:01:50", "link": "http://arxiv.org/abs/2411.05200v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Evaluating GPT-4 at Grading Handwritten Solutions in Math Exams", "abstract": "Recent advances in generative artificial intelligence (AI) have shown promise\nin accurately grading open-ended student responses. However, few prior works\nhave explored grading handwritten responses due to a lack of data and the\nchallenge of combining visual and textual information. In this work, we\nleverage state-of-the-art multi-modal AI models, in particular GPT-4o, to\nautomatically grade handwritten responses to college-level math exams. Using\nreal student responses to questions in a probability theory exam, we evaluate\nGPT-4o's alignment with ground-truth scores from human graders using various\nprompting techniques. We find that while providing rubrics improves alignment,\nthe model's overall accuracy is still too low for real-world settings, showing\nthere is significant room for growth in this task.", "published": "2024-11-07 22:51:47", "link": "http://arxiv.org/abs/2411.05231v2", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Dialectal Coverage And Generalization in Arabic Speech Recognition", "abstract": "Developing robust automatic speech recognition (ASR) systems for Arabic, a\nlanguage characterized by its rich dialectal diversity and often considered a\nlow-resource language in speech technology, demands effective strategies to\nmanage its complexity. This study explores three critical factors influencing\nASR performance: the role of dialectal coverage in pre-training, the\neffectiveness of dialect-specific fine-tuning compared to a multi-dialectal\napproach, and the ability to generalize to unseen dialects. Through extensive\nexperiments across different dialect combinations, our findings offer key\ninsights towards advancing the development of ASR systems for pluricentric\nlanguages like Arabic.", "published": "2024-11-07 22:23:30", "link": "http://arxiv.org/abs/2411.05872v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Improved Preference Optimization Pipeline: from Data Generation\n  to Budget-Controlled Regularization", "abstract": "Direct Preference Optimization (DPO) and its variants have become the de\nfacto standards for aligning large language models (LLMs) with human\npreferences or specific goals. However, DPO requires high-quality preference\ndata and suffers from unstable preference optimization. In this work, we aim to\nimprove the preference optimization pipeline by taking a closer look at\npreference data generation and training regularization techniques. For\npreference data generation, we demonstrate that existing scoring-based reward\nmodels produce unsatisfactory preference data and perform poorly on\nout-of-distribution tasks. This significantly impacts the LLM alignment\nperformance when using these data for preference tuning. To ensure high-quality\npreference data generation, we propose an iterative pairwise ranking mechanism\nthat derives preference ranking of completions using pairwise comparison\nsignals. For training regularization, we observe that preference optimization\ntends to achieve better convergence when the LLM predicted likelihood of\npreferred samples gets slightly reduced. However, the widely used supervised\nnext-word prediction regularization strictly prevents any likelihood reduction\nof preferred samples. This observation motivates our design of a\nbudget-controlled regularization formulation. Empirically we show that\ncombining the two designs leads to aligned models that surpass existing SOTA\nacross two popular benchmarks.", "published": "2024-11-07 23:03:11", "link": "http://arxiv.org/abs/2411.05875v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Leveraging LLMs to Enable Natural Language Search on Go-to-market\n  Platforms", "abstract": "Enterprise searches require users to have complex knowledge of queries,\nconfigurations, and metadata, rendering it difficult for them to access\ninformation as needed. Most go-to-market (GTM) platforms utilize advanced\nsearch, an interface that enables users to filter queries by various fields\nusing categories or keywords, which, historically, however, has proven to be\nexceedingly cumbersome, as users are faced with seemingly hundreds of options,\nfields, and buttons. Consequently, querying with natural language has long been\nideal, a notion further empowered by Large Language Models (LLMs).\n  In this paper, we implement and evaluate a solution for the Zoominfo product\nfor sellers, which prompts the LLM with natural language, producing search\nfields through entity extraction that are then converted into a search query.\nThe intermediary search fields offer numerous advantages for each query,\nincluding the elimination of syntax errors, simpler ground truths, and an\nintuitive format for the LLM to interpret.\n  We paired this pipeline with many advanced prompt engineering strategies,\nfeaturing an intricate system message, few-shot prompting, chain-of-thought\n(CoT) reasoning, and execution refinement. Furthermore, we manually created the\nground truth for 500+ natural language queries, enabling the supervised\nfine-tuning of Llama-3-8B-Instruct and the introduction of sophisticated\nnumerical metrics.\n  Comprehensive experiments with closed, open source, and fine-tuned LLM models\nwere conducted through exact, Jaccard, cosine, and semantic similarity on\nindividual search entities to demonstrate the efficacy of our approach.\nOverall, the most accurate closed model had an average accuracy of 97% per\nquery, with only one field performing under 90%, with comparable results\nobserved from the fine-tuned models.", "published": "2024-11-07 03:58:38", "link": "http://arxiv.org/abs/2411.05048v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG", "I.2.7; H.3.3; H.3.4"], "primary_category": "cs.CL"}
{"title": "A Pre-training Framework that Encodes Noise Information for Speech\n  Quality Assessment", "abstract": "Self-supervised learning (SSL) has grown in interest within the speech\nprocessing community, since it produces representations that are useful for\nmany downstream tasks. SSL uses global and contextual methods to produce robust\nrepresentations, where SSL even outperforms supervised models. Most\nself-supervised approaches, however, are limited to embedding information\nabout, i.e., the phonemes, speaker identity, and emotion, into the extracted\nrepresentations, where they become invariant to background sounds due to\ncontrastive and auto-regressive learning. This is limiting because many\ndownstream tasks leverage noise information to function accurately. Therefore,\nwe propose a pre-training framework that learns information pertaining to\nbackground noise in a supervised manner, while jointly embedding speech\ninformation using a self-supervised strategy. We experiment with multiple\nencoders and show that our framework is useful for perceptual speech quality\nestimation, which relies on background cues. Our results show that the proposed\napproach improves performance with fewer parameters, in comparison to multiple\nbaselines.", "published": "2024-11-07 02:34:13", "link": "http://arxiv.org/abs/2411.04379v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with\n  Retrieval-Augmented Generation", "abstract": "Current leading Text-To-Audio (TTA) generation models suffer from degraded\nperformance on zero-shot and few-shot settings. It is often challenging to\ngenerate high-quality audio for audio events that are unseen or uncommon in the\ntraining set. Inspired by the success of Retrieval-Augmented Generation (RAG)\nin Large Language Model (LLM)-based knowledge-intensive tasks, we extend the\nTTA process with additional conditioning contexts. We propose Audiobox TTA-RAG,\na novel retrieval-augmented TTA approach based on Audiobox, a conditional\nflow-matching audio generation model. Unlike the vanilla Audiobox TTA solution\nwhich generates audio conditioned on text, we augmented the conditioning input\nwith retrieved audio samples that provide additional acoustic information to\ngenerate the target audio. Our retrieval method does not require the external\ndatabase to have labeled audio, offering more practical use cases. To evaluate\nour proposed method, we curated test sets in zero-shot and few-shot settings.\nOur empirical results show that the proposed model can effectively leverage the\nretrieved audio samples and significantly improve zero-shot and few-shot TTA\nperformance, with large margins on multiple evaluation metrics, while\nmaintaining the ability to generate semantically aligned audio for the\nin-domain setting. In addition, we investigate the effect of different\nretrieval methods and data sources.", "published": "2024-11-07 19:50:28", "link": "http://arxiv.org/abs/2411.05141v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Model and Deep learning based Dynamic Range Compression Inversion", "abstract": "Dynamic Range Compression (DRC) is a popular audio effect used to control the\ndynamic range of a signal. Inverting DRC can also help to restore the original\ndynamics to produce new mixes and/or to improve the overall quality of the\naudio signal. Since, state-of-the-art DRC inversion techniques either ignore\nparameters or require precise parameters that are difficult to estimate, we\nfill the gap by combining a model-based approach with neural networks for DRC\ninversion. To this end, depending on the scenario, we use different neural\nnetworks to estimate DRC parameters. Then, a model-based inversion is completed\nto restore the original audio signal. Our experimental results show the\neffectiveness and robustness of the proposed method in comparison to several\nstate-of-the-art methods, when applied on two music datasets.", "published": "2024-11-07 00:33:07", "link": "http://arxiv.org/abs/2411.04337v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Concatenator: A Bayesian Approach To Real Time Concatenative\n  Musaicing", "abstract": "We present ``The Concatenator,'' a real time system for audio-guided\nconcatenative synthesis. Similarly to Driedger et al.'s ``musaicing'' (or\n``audio mosaicing'') technique, we concatenate a set number of windows within a\ncorpus of audio to re-create the harmonic and percussive aspects of a target\naudio stream. Unlike Driedger's NMF-based technique, however, we instead use an\nexplicitly Bayesian point of view, where corpus window indices are hidden\nstates and the target audio stream is an observation. We use a particle filter\nto infer the best hidden corpus states in real-time. Our transition model\nincludes a tunable parameter to control the time-continuity of corpus grains,\nand our observation model allows users to prioritize how quickly windows change\nto match the target. Because the computational complexity of the system is\nindependent of the corpus size, our system scales to corpora that are hours\nlong, which is an important feature in the age of vast audio data collections.\nWithin The Concatenator module itself, composers can vary grain length, fit to\ntarget, and pitch shift in real time while reacting to the sounds they hear,\nenabling them to rapidly iterate ideas. To conclude our work, we evaluate our\nsystem with extensive quantitative tests of the effects of parameters, as well\nas a qualitative evaluation with artistic insights. Based on the quality of the\nresults, we believe the real-time capability unlocks new avenues for musical\nexpression and control, suitable for live performance and modular synthesis\nintegration, which furthermore represents an essential breakthrough in\nconcatenative synthesis technology.", "published": "2024-11-07 01:52:46", "link": "http://arxiv.org/abs/2411.04366v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS", "J.5; I.5.4; H.3.3"], "primary_category": "cs.SD"}
