{"title": "Predicting Causes of Reformulation in Intelligent Assistants", "abstract": "Intelligent assistants (IAs) such as Siri and Cortana conversationally\ninteract with users and execute a wide range of actions (e.g., searching the\nWeb, setting alarms, and chatting). IAs can support these actions through the\ncombination of various components such as automatic speech recognition, natural\nlanguage understanding, and language generation. However, the complexity of\nthese components hinders developers from determining which component causes an\nerror. To remove this hindrance, we focus on reformulation, which is a useful\nsignal of user dissatisfaction, and propose a method to predict the\nreformulation causes. We evaluate the method using the user logs of a\ncommercial IA. The experimental results have demonstrated that features\ndesigned to detect the error of a specific component improve the performance of\nreformulation cause detection.", "published": "2017-07-13 03:47:32", "link": "http://arxiv.org/abs/1707.03968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is writing style predictive of scientific fraud?", "abstract": "The problem of detecting scientific fraud using machine learning was recently\nintroduced, with initial, positive results from a model taking into account\nvarious general indicators. The results seem to suggest that writing style is\npredictive of scientific fraud. We revisit these initial experiments, and show\nthat the leave-one-out testing procedure they used likely leads to a slight\nover-estimate of the predictability, but also that simple models can outperform\ntheir proposed model by some margin. We go on to explore more abstract\nlinguistic features, such as linguistic complexity and discourse structure,\nonly to obtain negative results. Upon analyzing our models, we do see some\ninteresting patterns, though: Scientific fraud, for examples, contains less\ncomparison, as well as different types of hedging and ways of presenting\nlogical reasoning.", "published": "2017-07-13 12:51:39", "link": "http://arxiv.org/abs/1707.04095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Convolutional Networks need to be Deep for Text Classification ?", "abstract": "We study in this work the importance of depth in convolutional models for\ntext classification, either when character or word inputs are considered. We\nshow on 5 standard text classification and sentiment analysis tasks that deep\nmodels indeed give better performances than shallow networks when the text\ninput is represented as a sequence of characters. However, a simple\nshallow-and-wide network outperforms deep models such as DenseNet with word\ninputs. Our shallow word model further establishes new state-of-the-art\nperformances on two datasets: Yelp Binary (95.9\\%) and Yelp Full (64.9\\%).", "published": "2017-07-13 13:18:52", "link": "http://arxiv.org/abs/1707.04108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Web-Based Tool for Analysing Normative Documents in English", "abstract": "Our goal is to use formal methods to analyse normative documents written in\nEnglish, such as privacy policies and service-level agreements. This requires\nthe combination of a number of different elements, including information\nextraction from natural language, formal languages for model representation,\nand an interface for property specification and verification. We have worked on\na collection of components for this task: a natural language extraction tool, a\nsuitable formalism for representing such documents, an interface for building\nmodels in this formalism, and methods for answering queries asked of a given\nmodel. In this work, each of these concerns is brought together in a web-based\ntool, providing a single interface for analysing normative texts in English.\nThrough the use of a running example, we describe each component and\ndemonstrate the workflow established by our tool.", "published": "2017-07-13 07:22:18", "link": "http://arxiv.org/abs/1707.03997v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Parsing with Traces: An $O(n^4)$ Algorithm and a Structural\n  Representation", "abstract": "General treebank analyses are graph structured, but parsers are typically\nrestricted to tree structures for efficiency and modeling reasons. We propose a\nnew representation and algorithm for a class of graph structures that is\nflexible enough to cover almost all treebank structures, while still admitting\nefficient learning and inference. In particular, we consider directed, acyclic,\none-endpoint-crossing graph structures, which cover most long-distance\ndislocation, shared argumentation, and similar tree-violating linguistic\nphenomena. We describe how to convert phrase structure parses, including\ntraces, to our new representation in a reversible manner. Our dynamic program\nuniquely decomposes structures, is sound and complete, and covers 97.3% of the\nPenn English Treebank. We also implement a proof-of-concept parser that\nrecovers a range of null elements and trace types.", "published": "2017-07-13 16:52:08", "link": "http://arxiv.org/abs/1707.04221v1", "categories": ["cs.CL", "cs.DM", "I.2.7; F.2.2; G.2.2"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition with Very Large Conversational Finnish and\n  Estonian Vocabularies", "abstract": "Today, the vocabulary size for language models in large vocabulary speech\nrecognition is typically several hundreds of thousands of words. While this is\nalready sufficient in some applications, the out-of-vocabulary words are still\nlimiting the usability in others. In agglutinative languages the vocabulary for\nconversational speech should include millions of word forms to cover the\nspelling variations due to colloquial pronunciations, in addition to the word\ncompounding and inflections. Very large vocabularies are also needed, for\nexample, when the recognition of rare proper names is important.", "published": "2017-07-13 17:16:16", "link": "http://arxiv.org/abs/1707.04227v5", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Representation Learning for Grounded Spatial Reasoning", "abstract": "The interpretation of spatial references is highly contextual, requiring\njoint inference over both language and the environment. We consider the task of\nspatial reasoning in a simulated environment, where an agent can act and\nreceive rewards. The proposed model learns a representation of the world\nsteered by instruction text. This design allows for precise alignment of local\nneighborhoods with corresponding verbalizations, while also handling global\nreferences in the instructions. We train our model with reinforcement learning\nusing a variant of generalized value iteration. The model outperforms\nstate-of-the-art approaches on several metrics, yielding a 45% reduction in\ngoal localization error.", "published": "2017-07-13 00:17:45", "link": "http://arxiv.org/abs/1707.03938v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Networks for Information Retrieval", "abstract": "Machine learning plays a role in many aspects of modern IR systems, and deep\nlearning is applied in all of them. The fast pace of modern-day research has\ngiven rise to many different approaches for many different IR problems. The\namount of information available can be overwhelming both for junior students\nand for experienced researchers looking for new research topics and directions.\nAdditionally, it is interesting to see what key insights into IR problems the\nnew technologies are able to give us. The aim of this full-day tutorial is to\ngive a clear overview of current tried-and-trusted neural methods in IR and how\nthey benefit IR research. It covers key architectures, as well as the most\npromising future directions.", "published": "2017-07-13 17:46:59", "link": "http://arxiv.org/abs/1707.04242v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Lithium NLP: A System for Rich Information Extraction from Noisy User\n  Generated Text on Social Media", "abstract": "In this paper, we describe the Lithium Natural Language Processing (NLP)\nsystem - a resource-constrained, high- throughput and language-agnostic system\nfor information extraction from noisy user generated text on social media.\nLithium NLP extracts a rich set of information including entities, topics,\nhashtags and sentiment from text. We discuss several real world applications of\nthe system currently incorporated in Lithium products. We also compare our\nsystem with existing commercial and academic NLP systems in terms of\nperformance, information extracted and languages supported. We show that\nLithium NLP is at par with and in some cases, outperforms state- of-the-art\ncommercial NLP systems.", "published": "2017-07-13 17:52:51", "link": "http://arxiv.org/abs/1707.04244v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Learning Features from Co-occurrences: A Theoretical Analysis", "abstract": "Representing a word by its co-occurrences with other words in context is an\neffective way to capture the meaning of the word. However, the theory behind\nremains a challenge. In this work, taking the example of a word classification\ntask, we give a theoretical analysis of the approaches that represent a word X\nby a function f(P(C|X)), where C is a context feature, P(C|X) is the\nconditional probability estimated from a text corpus, and the function f maps\nthe co-occurrence measure to a prediction score. We investigate the impact of\ncontext feature C and the function f. We also explain the reasons why using the\nco-occurrences with multiple context features may be better than just using a\nsingle one. In addition, some of the results shed light on the theory of\nfeature learning and machine learning in general.", "published": "2017-07-13 16:46:50", "link": "http://arxiv.org/abs/1707.04218v1", "categories": ["cs.CL", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.CL"}
