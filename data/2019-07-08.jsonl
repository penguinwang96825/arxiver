{"title": "Correct-and-Memorize: Learning to Translate from Interactive Revisions", "abstract": "State-of-the-art machine translation models are still not on par with human\ntranslators. Previous work takes human interactions into the neural machine\ntranslation process to obtain improved results in target languages. However,\nnot all model-translation errors are equal -- some are critical while others\nare minor. In the meanwhile, the same translation mistakes occur repeatedly in\na similar context. To solve both issues, we propose CAMIT, a novel method for\ntranslating in an interactive environment. Our proposed method works with\ncritical revision instructions, therefore allows human to correct arbitrary\nwords in model-translated sentences. In addition, CAMIT learns from and softly\nmemorizes revision actions based on the context, alleviating the issue of\nrepeating mistakes. Experiments in both ideal and real interactive translation\nsettings demonstrate that our proposed \\method enhances machine translation\nresults significantly while requires fewer revision instructions from human\ncompared to previous methods.", "published": "2019-07-08 09:09:45", "link": "http://arxiv.org/abs/1907.03468v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Effective Neural Extractive Summarization: What Works and\n  What's Next", "abstract": "The recent years have seen remarkable success in the use of deep neural\nnetworks on text summarization.\n  However, there is no clear understanding of \\textit{why} they perform so\nwell, or \\textit{how} they might be improved.\n  In this paper, we seek to better understand how neural extractive\nsummarization systems could benefit from different types of model\narchitectures, transferable knowledge and\n  learning schemas. Additionally, we find an effective way to improve current\nframeworks and achieve the state-of-the-art result on CNN/DailyMail by a large\nmargin based on our\n  observations and analyses. Hopefully, our work could provide more clues for\nfuture research on extractive summarization.", "published": "2019-07-08 10:17:28", "link": "http://arxiv.org/abs/1907.03491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Early Discovery of Emerging Entities in Microblogs", "abstract": "Keeping up to date on emerging entities that appear every day is\nindispensable for various applications, such as social-trend analysis and\nmarketing research. Previous studies have attempted to detect unseen entities\nthat are not registered in a particular knowledge base as emerging entities and\nconsequently find non-emerging entities since the absence of entities in\nknowledge bases does not guarantee their emergence. We therefore introduce a\nnovel task of discovering truly emerging entities when they have just been\nintroduced to the public through microblogs and propose an effective method\nbased on time-sensitive distant supervision, which exploits distinctive\nearly-stage contexts of emerging entities. Experimental results with a\nlarge-scale Twitter archive show that the proposed method achieves 83.2%\nprecision of the top 500 discovered emerging entities, which outperforms\nbaselines based on unseen entity recognition with burst detection. Besides\nnotable emerging entities, our method can discover massive long-tail and\nhomographic emerging entities. An evaluation of relative recall shows that the\nmethod detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of\nthem are discovered earlier than their registration in Wikipedia, and the\naverage lead-time is more than one year (571 days).", "published": "2019-07-08 11:13:42", "link": "http://arxiv.org/abs/1907.03513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-aware Pronoun Coreference Resolution", "abstract": "Resolving pronoun coreference requires knowledge support, especially for\nparticular domains (e.g., medicine). In this paper, we explore how to leverage\ndifferent types of knowledge to better resolve pronoun coreference with a\nneural model. To ensure the generalization ability of our model, we directly\nincorporate knowledge in the format of triplets, which is the most common\nformat of modern knowledge graphs, instead of encoding it with features or\nrules as that in conventional approaches. Moreover, since not all knowledge is\nhelpful in certain contexts, to selectively use them, we propose a knowledge\nattention module, which learns to select and use informative knowledge based on\ncontexts, to enhance our model. Experimental results on two datasets from\ndifferent domains prove the validity and effectiveness of our model, where it\noutperforms state-of-the-art baselines by a large margin. Moreover, since our\nmodel learns to use external knowledge rather than only fitting the training\ndata, it also demonstrates superior performance to baselines in the\ncross-domain setting.", "published": "2019-07-08 15:01:17", "link": "http://arxiv.org/abs/1907.03663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Natural Language Corpus of Common Grounding under Continuous and\n  Partially-Observable Context", "abstract": "Common grounding is the process of creating, repairing and updating mutual\nunderstandings, which is a critical aspect of sophisticated human\ncommunication. However, traditional dialogue systems have limited capability of\nestablishing common ground, and we also lack task formulations which introduce\nnatural difficulty in terms of common grounding while enabling easy evaluation\nand analysis of complex models. In this paper, we propose a minimal dialogue\ntask which requires advanced skills of common grounding under continuous and\npartially-observable context. Based on this task formulation, we collected a\nlargescale dataset of 6,760 dialogues which fulfills essential requirements of\nnatural language corpora. Our analysis of the dataset revealed important\nphenomena related to common grounding that need to be considered. Finally, we\nevaluate and analyze baseline neural models on a simple subtask that requires\nrecognition of the created common ground. We show that simple baseline models\nperform decently but leave room for further improvement. Overall, we show that\nour proposed task will be a fundamental testbed where we can train, evaluate,\nand analyze dialogue system's ability for sophisticated common grounding.", "published": "2019-07-08 04:19:17", "link": "http://arxiv.org/abs/1907.03399v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multiple Generative Models Ensemble for Knowledge-Driven Proactive\n  Human-Computer Dialogue Agent", "abstract": "Multiple sequence to sequence models were used to establish an end-to-end\nmulti-turns proactive dialogue generation agent, with the aid of data\naugmentation techniques and variant encoder-decoder structure designs. A\nrank-based ensemble approach was developed for boosting performance. Results\nindicate that our single model, in average, makes an obvious improvement in the\nterms of F1-score and BLEU over the baseline by 18.67% on the DuConv dataset.\nIn particular, the ensemble methods further significantly outperform the\nbaseline by 35.85%.", "published": "2019-07-08 13:16:07", "link": "http://arxiv.org/abs/1907.03590v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Neural Relation Extraction with Implicit Mutual Relations", "abstract": "Relation extraction (RE) aims at extracting the relation between two entities\nfrom the text corpora. It is a crucial task for Knowledge Graph (KG)\nconstruction. Most existing methods predict the relation between an entity pair\nby learning the relation from the training sentences, which contain the\ntargeted entity pair. In contrast to existing distant supervision approaches\nthat suffer from insufficient training corpora to extract relations, our\nproposal of mining implicit mutual relation from the massive unlabeled corpora\ntransfers the semantic information of entity pairs into the RE model, which is\nmore expressive and semantically plausible. After constructing an entity\nproximity graph based on the implicit mutual relations, we preserve the\nsemantic relations of entity pairs via embedding each vertex of the graph into\na low-dimensional space. As a result, we can easily and flexibly integrate the\nimplicit mutual relations and other entity information, such as entity types,\ninto the existing RE methods.\n  Our experimental results on a New York Times and another Google Distant\nSupervision datasets suggest that our proposed neural RE framework provides a\npromising improvement for the RE task, and significantly outperforms the\nstate-of-the-art methods. Moreover, the component for mining implicit mutual\nrelations is so flexible that can help to improve the performance of both\nCNN-based and RNN-based RE models significant.", "published": "2019-07-08 02:16:11", "link": "http://arxiv.org/abs/1907.05333v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Parallelism Theorem and Derived Rules for Parallel Coherent\n  Transformations", "abstract": "An Independent Parallelism Theorem is proven in the theory of adhesive HLR\ncategories. It shows the bijective correspondence between sequential\nindependent and parallel independent direct derivations in the Weak\nDouble-Pushout framework, see [2]. The parallel derivations are expressed by\nmeans of Parallel Coherent Transformations (PCTs), hence without assuming the\nexistence of coproducts compatible with M as in the standard Parallelism\nTheorem. It is aslo shown that a derived rule can be extracted from any PCT, in\nthe sense that to any direct derivation of this rule corresponds a valid PCT.", "published": "2019-07-08 13:08:12", "link": "http://arxiv.org/abs/1907.06585v1", "categories": ["math.CT", "cs.CL", "F.1.1; F.4.2"], "primary_category": "math.CT"}
{"title": "Predicting Customer Call Intent by Analyzing Phone Call Transcripts\n  based on CNN for Multi-Class Classification", "abstract": "Auto dealerships receive thousands of calls daily from customers who are\ninterested in sales, service, vendors and jobseekers. With so many calls, it is\nvery important for auto dealers to understand the intent of these calls to\nprovide positive customer experiences that ensure customer satisfaction, deep\ncustomer engagement to boost sales and revenue, and optimum allocation of\nagents or customer service representatives across the business. In this paper,\nwe define the problem of customer phone call intent as a multi-class\nclassification problem stemming from the large database of recorded phone call\ntranscripts. To solve this problem, we develop a convolutional neural network\n(CNN)-based supervised learning model to classify the customer calls into four\nintent categories: sales, service, vendor and jobseeker. Experimental results\nshow that with the thrust of our scalable data labeling method to provide\nsufficient training data, the CNN-based predictive model performs very well on\nlong text classification according to the quantitative metrics of F1-Score,\nprecision, recall, and accuracy.", "published": "2019-07-08 16:39:23", "link": "http://arxiv.org/abs/1907.03715v1", "categories": ["cs.LG", "cs.CL", "stat.ML", "97R40"], "primary_category": "cs.LG"}
{"title": "An Intrinsic Nearest Neighbor Analysis of Neural Machine Translation\n  Architectures", "abstract": "Earlier approaches indirectly studied the information captured by the hidden\nstates of recurrent and non-recurrent neural machine translation models by\nfeeding them into different classifiers. In this paper, we look at the encoder\nhidden states of both transformer and recurrent machine translation models from\nthe nearest neighbors perspective. We investigate to what extent the nearest\nneighbors share information with the underlying word embeddings as well as\nrelated WordNet entries. Additionally, we study the underlying syntactic\nstructure of the nearest neighbors to shed light on the role of syntactic\nsimilarities in bringing the neighbors together. We compare transformer and\nrecurrent models in a more intrinsic way in terms of capturing lexical\nsemantics and syntactic structures, in contrast to extrinsic approaches used by\nprevious works. In agreement with the extrinsic evaluations in the earlier\nworks, our experimental results show that transformers are superior in\ncapturing lexical semantics, but not necessarily better in capturing the\nunderlying syntax. Additionally, we show that the backward recurrent layer in a\nrecurrent model learns more about the semantics of words, whereas the forward\nrecurrent layer encodes more context.", "published": "2019-07-08 21:39:29", "link": "http://arxiv.org/abs/1907.03885v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Attending to Emotional Narratives", "abstract": "Attention mechanisms in deep neural networks have achieved excellent\nperformance on sequence-prediction tasks. Here, we show that these\nrecently-proposed attention-based mechanisms---in particular, the Transformer\nwith its parallelizable self-attention layers, and the Memory Fusion Network\nwith attention across modalities and time---also generalize well to multimodal\ntime-series emotion recognition. Using a recently-introduced dataset of\nemotional autobiographical narratives, we adapt and apply these two attention\nmechanisms to predict emotional valence over time. Our models perform extremely\nwell, in some cases reaching a performance comparable with human raters. We end\nwith a discussion of the implications of attention mechanisms to affective\ncomputing.", "published": "2019-07-08 03:50:43", "link": "http://arxiv.org/abs/1907.04197v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Development of email classifier in Brazilian Portuguese using feature\n  selection for automatic response", "abstract": "Automatic email categorization is an important application of text\nclassification. We study the automatic reply of email business messages in\nBrazilian Portuguese. We present a novel corpus containing messages from a real\napplication, and baseline categorization experiments using Naive Bayes and\nsupport Vector Machines. We then discuss the effect of lemmatization and the\nrole of part-of-speech tagging filtering on precision and recall. Support\nVector Machines classification coupled with nonlemmatized selection of verbs,\nnouns and adjectives was the best approach, with 87.3% maximum accuracy.\nStraightforward lemmatization in Portuguese led to the lowest classification\nresults in the group, with 85.3% and 81.7% precision in SVM and Naive Bayes\nrespectively. Thus, while lemmatization reduced precision and recall,\npart-of-speech filtering improved overall results.", "published": "2019-07-08 03:24:53", "link": "http://arxiv.org/abs/1907.04905v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Topic Modeling in Embedding Spaces", "abstract": "Topic modeling analyzes documents to learn meaningful patterns of words.\nHowever, existing topic models fail to learn interpretable topics when working\nwith large and heavy-tailed vocabularies. To this end, we develop the Embedded\nTopic Model (ETM), a generative model of documents that marries traditional\ntopic models with word embeddings. In particular, it models each word with a\ncategorical distribution whose natural parameter is the inner product between a\nword embedding and an embedding of its assigned topic. To fit the ETM, we\ndevelop an efficient amortized variational inference algorithm. The ETM\ndiscovers interpretable topics even with large vocabularies that include rare\nwords and stop words. It outperforms existing document models, such as latent\nDirichlet allocation (LDA), in terms of both topic quality and predictive\nperformance.", "published": "2019-07-08 03:50:57", "link": "http://arxiv.org/abs/1907.04907v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "XFake: Explainable Fake News Detector with Visualizations", "abstract": "In this demo paper, we present the XFake system, an explainable fake news\ndetector that assists end-users to identify news credibility. To effectively\ndetect and interpret the fakeness of news items, we jointly consider both\nattributes (e.g., speaker) and statements. Specifically, MIMIC, ATTN and PERT\nframeworks are designed, where MIMIC is built for attribute analysis, ATTN is\nfor statement semantic analysis and PERT is for statement linguistic analysis.\nBeyond the explanations extracted from the designed frameworks, relevant\nsupporting examples as well as visualization are further provided to facilitate\nthe interpretation. Our implemented system is demonstrated on a real-world\ndataset crawled from PolitiFact, where thousands of verified political news\nhave been collected.", "published": "2019-07-08 18:29:58", "link": "http://arxiv.org/abs/1907.07757v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence\n  ASR", "abstract": "Sequence-to-sequence (seq2seq) based ASR systems have shown state-of-the-art\nperformances while having clear advantages in terms of simplicity. However,\ncomparisons are mostly done on speaker independent (SI) ASR systems, though\nspeaker adapted conventional systems are commonly used in practice for\nimproving robustness to speaker and environment variations. In this paper, we\napply speaker adaptation to seq2seq models with the goal of matching the\nperformance of conventional ASR adaptation. Specifically, we investigate\nKullback-Leibler divergence (KLD) as well as Linear Hidden Network (LHN) based\nadaptation for seq2seq ASR, using different amounts (up to 20 hours) of\nadaptation data per speaker. Our SI models are trained on large amounts of\ndictation data and achieve state-of-the-art results. We obtained 25% relative\nword error rate (WER) improvement with KLD adaptation of the seq2seq model vs.\n18.7% gain from acoustic model adaptation in the conventional system. We also\nshow that the WER of the seq2seq model decreases log-linearly with the amount\nof adaptation data. Finally, we analyze adaptation based on the minimum WER\ncriterion and adapting the language model (LM) for score fusion with the\nspeaker adapted seq2seq model, which result in further improvements of the\nseq2seq system performance.", "published": "2019-07-08 15:09:40", "link": "http://arxiv.org/abs/1907.04916v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Privacy-Preserving Speaker Recognition with Cohort Score Normalisation", "abstract": "In many voice biometrics applications there is a requirement to preserve\nprivacy, not least because of the recently enforced General Data Protection\nRegulation (GDPR). Though progress in bringing privacy preservation to voice\nbiometrics is lagging behind developments in other biometrics communities,\nrecent years have seen rapid progress, with secure computation mechanisms such\nas homomorphic encryption being applied successfully to speaker recognition.\nEven so, the computational overhead incurred by processing speech data in the\nencrypted domain is substantial. While still tolerable for single biometric\ncomparisons, most state-of-the-art systems perform some form of cohort-based\nscore normalisation, requiring many thousands of biometric comparisons. The\ncomputational overhead is then prohibitive, meaning that one must accept either\ndegraded performance (no score normalisation) or potential for privacy\nviolations. This paper proposes the first computationally feasible approach to\nprivacy-preserving cohort score normalisation. Our solution is a cohort pruning\nscheme based on secure multi-party computation which enables privacy-preserving\nscore normalisation using probabilistic linear discriminant analysis (PLDA)\ncomparisons. The solution operates upon binary voice representations. While the\nbinarisation is lossy in biometric rank-1 performance, it supports\ncomputationally-feasible biometric rank-n comparisons in the encrypted domain.", "published": "2019-07-08 08:33:44", "link": "http://arxiv.org/abs/1907.03454v1", "categories": ["eess.AS", "cs.CR"], "primary_category": "eess.AS"}
{"title": "The GDPR & Speech Data: Reflections of Legal and Technology Communities,\n  First Steps towards a Common Understanding", "abstract": "Privacy preservation and the protection of speech data is in high demand, not\nleast as a result of recent regulation, e.g. the General Data Protection\nRegulation (GDPR) in the EU. While there has been a period with which to\nprepare for its implementation, its implications for speech data is poorly\nunderstood. This assertion applies to both the legal and technology\ncommunities, and is hardly surprising since there is no universal definition of\n'privacy', let alone a clear understanding of when or how the GDPR applies to\nthe capture, storage and processing of speech data. In aiming to initiate the\ndiscussion that is needed to establish a level of harmonisation that is thus\nfar lacking, this contribution presents some reflections of both legal and\ntechnology communities on the implications of the GDPR as regards speech data.\nThe article outlines the need for taxonomies at the intersection of speech\ntechnology and data privacy - a discussion that is still very much in its\ninfancy - and describes the ways to safeguards and priorities for future\nresearch. In being agnostic to any specific application, the treatment should\nbe of interest to the speech communication community at large.", "published": "2019-07-08 08:42:42", "link": "http://arxiv.org/abs/1907.03458v1", "categories": ["eess.AS", "cs.CY"], "primary_category": "eess.AS"}
{"title": "ShrinkML: End-to-End ASR Model Compression Using Reinforcement Learning", "abstract": "End-to-end automatic speech recognition (ASR) models are increasingly large\nand complex to achieve the best possible accuracy. In this paper, we build an\nAutoML system that uses reinforcement learning (RL) to optimize the per-layer\ncompression ratios when applied to a state-of-the-art attention based\nend-to-end ASR model composed of several LSTM layers. We use singular value\ndecomposition (SVD) low-rank matrix factorization as the compression method.\nFor our RL-based AutoML system, we focus on practical considerations such as\nthe choice of the reward/punishment functions, the formation of an effective\nsearch space, and the creation of a representative but small data set for quick\nevaluation between search steps. Finally, we present accuracy results on\nLibriSpeech of the model compressed by our AutoML system, and we compare it to\nmanually-compressed models. Our results show that in the absence of retraining\nour RL-based search is an effective and practical method to compress a\nproduction-grade ASR system. When retraining is possible, we show that our\nAutoML system can select better highly-compressed seed models compared to\nmanually hand-crafted rank selection, thus allowing for more compression than\npreviously possible.", "published": "2019-07-08 12:10:18", "link": "http://arxiv.org/abs/1907.03540v2", "categories": ["cs.LG", "cs.AI", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
