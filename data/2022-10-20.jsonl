{"title": "Automatic Document Selection for Efficient Encoder Pretraining", "abstract": "Building pretrained language models is considered expensive and\ndata-intensive, but must we increase dataset size to achieve better\nperformance? We propose an alternative to larger training sets by automatically\nidentifying smaller yet domain-representative subsets. We extend Cynical Data\nSelection, a statistical sentence scoring method that conditions on a\nrepresentative target domain corpus. As an example, we treat the OntoNotes\ncorpus as a target domain and pretrain a RoBERTa-like encoder from a cynically\nselected subset of the Pile. On both perplexity and across several downstream\ntasks in the target domain, it consistently outperforms random selection with\n20x less data, 3x fewer training iterations, and 2x less estimated cloud\ncompute cost, validating the recipe of automatic document selection for LM\npretraining.", "published": "2022-10-20 01:45:02", "link": "http://arxiv.org/abs/2210.10951v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Chinese Spelling Check by Character Pronunciation Prediction:\n  The Effects of Adaptivity and Granularity", "abstract": "Chinese spelling check (CSC) is a fundamental NLP task that detects and\ncorrects spelling errors in Chinese texts. As most of these spelling errors are\ncaused by phonetic similarity, effectively modeling the pronunciation of\nChinese characters is a key factor for CSC. In this paper, we consider\nintroducing an auxiliary task of Chinese pronunciation prediction (CPP) to\nimprove CSC, and, for the first time, systematically discuss the adaptivity and\ngranularity of this auxiliary task. We propose SCOPE which builds on top of a\nshared encoder two parallel decoders, one for the primary CSC task and the\nother for a fine-grained auxiliary CPP task, with a novel adaptive weighting\nscheme to balance the two tasks. In addition, we design a delicate iterative\ncorrection strategy for further improvements during inference. Empirical\nevaluation shows that SCOPE achieves new state-of-the-art on three CSC\nbenchmarks, demonstrating the effectiveness and superiority of the auxiliary\nCPP task. Comprehensive ablation studies further verify the positive effects of\nadaptivity and granularity of the task. Code and data used in this paper are\npublicly available at https://github.com/jiahaozhenbang/SCOPE.", "published": "2022-10-20 03:42:35", "link": "http://arxiv.org/abs/2210.10996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Granularity Optimization for Non-Autoregressive Translation", "abstract": "Despite low latency, non-autoregressive machine translation (NAT) suffers\nsevere performance deterioration due to the naive independence assumption. This\nassumption is further strengthened by cross-entropy loss, which encourages a\nstrict match between the hypothesis and the reference token by token. To\nalleviate this issue, we propose multi-granularity optimization for NAT, which\ncollects model behaviors on translation segments of various granularities and\nintegrates feedback for backpropagation. Experiments on four WMT benchmarks\nshow that the proposed method significantly outperforms the baseline models\ntrained with cross-entropy loss, and achieves the best performance on WMT'16\nEn-Ro and highly competitive results on WMT'14 En-De for fully\nnon-autoregressive translation.", "published": "2022-10-20 04:54:29", "link": "http://arxiv.org/abs/2210.11017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots", "abstract": "This paper introduces Doc2Bot, a novel dataset for building machines that\nhelp users seek information via conversations. This is of particular interest\nfor companies and organizations that own a large number of manuals or\ninstruction books. Despite its potential, the nature of our task poses several\nchallenges: (1) documents contain various structures that hinder the ability of\nmachines to comprehend, and (2) user information needs are often\nunderspecified. Compared to prior datasets that either focus on a single\nstructural type or overlook the role of questioning to uncover user needs, the\nDoc2Bot dataset is developed to target such challenges systematically. Our\ndataset contains over 100,000 turns based on Chinese documents from five\ndomains, larger than any prior document-grounded dialog dataset for information\nseeking. We propose three tasks in Doc2Bot: (1) dialog state tracking to track\nuser intentions, (2) dialog policy learning to plan system actions and\ncontents, and (3) response generation which generates responses based on the\noutputs of the dialog policy. Baseline methods based on the latest deep\nlearning models are presented, indicating that our proposed tasks are\nchallenging and worthy of further research.", "published": "2022-10-20 07:33:05", "link": "http://arxiv.org/abs/2210.11060v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training Language Models with Deterministic Factual Knowledge", "abstract": "Previous works show that Pre-trained Language Models (PLMs) can capture\nfactual knowledge. However, some analyses reveal that PLMs fail to perform it\nrobustly, e.g., being sensitive to the changes of prompts when extracting\nfactual knowledge. To mitigate this issue, we propose to let PLMs learn the\ndeterministic relationship between the remaining context and the masked\ncontent. The deterministic relationship ensures that the masked factual content\ncan be deterministically inferable based on the existing clues in the context.\nThat would provide more stable patterns for PLMs to capture factual knowledge\nthan randomly masking. Two pre-training tasks are further introduced to\nmotivate PLMs to rely on the deterministic relationship when filling masks.\nSpecifically, we use an external Knowledge Base (KB) to identify deterministic\nrelationships and continuously pre-train PLMs with the proposed methods. The\nfactual knowledge probing experiments indicate that the continuously\npre-trained PLMs achieve better robustness in factual knowledge capturing.\nFurther experiments on question-answering datasets show that trying to learn a\ndeterministic relationship with the proposed methods can also help other\nknowledge-intensive tasks.", "published": "2022-10-20 11:04:09", "link": "http://arxiv.org/abs/2210.11165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evidence > Intuition: Transferability Estimation for Encoder Selection", "abstract": "With the increase in availability of large pre-trained language models (LMs)\nin Natural Language Processing (NLP), it becomes critical to assess their fit\nfor a specific target task a priori - as fine-tuning the entire space of\navailable LMs is computationally prohibitive and unsustainable. However,\nencoder transferability estimation has received little to no attention in NLP.\nIn this paper, we propose to generate quantitative evidence to predict which\nLM, out of a pool of models, will perform best on a target task without having\nto fine-tune all candidates. We provide a comprehensive study on LM ranking for\n10 NLP tasks spanning the two fundamental problem types of classification and\nstructured prediction. We adopt the state-of-the-art Logarithm of Maximum\nEvidence (LogME) measure from Computer Vision (CV) and find that it positively\ncorrelates with final LM performance in 94% of the setups. In the first study\nof its kind, we further compare transferability measures with the de facto\nstandard of human practitioner ranking, finding that evidence from quantitative\nmetrics is more robust than pure intuition and can help identify unexpected LM\ncandidates.", "published": "2022-10-20 13:25:21", "link": "http://arxiv.org/abs/2210.11255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Reasoning Capabilities from Language Models with\n  Compositional Reasoning Transformers", "abstract": "This paper presents ReasonFormer, a unified reasoning framework for mirroring\nthe modular and compositional reasoning process of humans in complex\ndecision-making. Inspired by dual-process theory in cognitive science, the\nrepresentation module (automatic thinking) and reasoning modules (controlled\nthinking) are decoupled to capture different levels of cognition. Upon the top\nof the representation module, the pre-trained reasoning modules are modular and\nprofessional in specific and fundamental reasoning skills (e.g., logic, simple\nQA, etc). To mimic the controlled compositional thinking process, different\nreasoning modules are dynamically activated and composed in both parallel and\ncascaded manners to control what reasoning skills are activated and how deep\nthe reasoning process will be reached to solve the current problems. The\nunified reasoning framework solves multiple tasks with a single model, and is\ntrained and inferred in an end-to-end manner. Evaluated on 11 datasets\nrequiring different reasoning skills and complexity, ReasonFormer demonstrates\nsubstantial performance boosts, revealing the compositional reasoning ability.\nFew-shot experiments exhibit better generalization ability by learning to\ncompose pre-trained skills for new tasks with limited data, and decoupling the\nrepresentation module and the reasoning modules. Further analysis shows the\nmodularity of reasoning modules as different tasks activate distinct reasoning\nskills at different reasoning depths.", "published": "2022-10-20 13:39:55", "link": "http://arxiv.org/abs/2210.11265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts", "abstract": "Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing\npre-trained models (PTMs) that simply prepends a soft prompt to the input and\nonly optimizes the prompt to adapt PTMs to downstream tasks. Although it is\nparameter- and deployment-efficient, its performance still lags behind other\nstate-of-the-art PETuning methods. Besides, the training cost of prompt tuning\nis not significantly reduced due to the back-propagation through the entire\nmodel. Through empirical analyses, we shed some light on the lagging\nperformance of prompt tuning and recognize a trade-off between the propagation\ndistance from label signals to the inserted prompt and the influence of the\nprompt on model outputs. Further, we present Late Prompt Tuning (LPT) that\ninserts a late prompt into an intermediate layer of the PTM instead of the\ninput layer or all layers. The late prompt is obtained by a neural prompt\ngenerator conditioned on the hidden states before the prompt insertion layer\nand therefore is instance-dependent. Through extensive experimental results\nacross various tasks and PTMs, we show that LPT can achieve competitive\nperformance to full model tuning and other PETuning methods under both\nfull-data and few-shot scenarios while possessing faster training speed and\nlower memory cost.", "published": "2022-10-20 14:23:52", "link": "http://arxiv.org/abs/2210.11292v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The University of Edinburgh's Submission to the WMT22 Code-Mixing Shared\n  Task (MixMT)", "abstract": "The University of Edinburgh participated in the WMT22 shared task on\ncode-mixed translation. This consists of two subtasks: i) generating code-mixed\nHindi/English (Hinglish) text generation from parallel Hindi and English\nsentences and ii) machine translation from Hinglish to English. As both\nsubtasks are considered low-resource, we focused our efforts on careful data\ngeneration and curation, especially the use of backtranslation from monolingual\nresources. For subtask 1 we explored the effects of constrained decoding on\nEnglish and transliterated subwords in order to produce Hinglish. For subtask\n2, we investigated different pretraining techniques, namely comparing simple\ninitialisation from existing machine translation models and aligned\naugmentation. For both subtasks, we found that our baseline systems worked\nbest. Our systems for both subtasks were one of the overall top-performing\nsubmissions.", "published": "2022-10-20 14:40:10", "link": "http://arxiv.org/abs/2210.11309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-Efficient Strategies for Expanding Hate Speech Detection into\n  Under-Resourced Languages", "abstract": "Hate speech is a global phenomenon, but most hate speech datasets so far\nfocus on English-language content. This hinders the development of more\neffective hate speech detection models in hundreds of languages spoken by\nbillions across the world. More data is needed, but annotating hateful content\nis expensive, time-consuming and potentially harmful to annotators. To mitigate\nthese issues, we explore data-efficient strategies for expanding hate speech\ndetection into under-resourced languages. In a series of experiments with mono-\nand multilingual models across five non-English languages, we find that 1) a\nsmall amount of target-language fine-tuning data is needed to achieve strong\nperformance, 2) the benefits of using more such data decrease exponentially,\nand 3) initial fine-tuning on readily-available English data can partially\nsubstitute target-language data and improve model generalisability. Based on\nthese findings, we formulate actionable recommendations for hate speech\ndetection in low-resource language settings.", "published": "2022-10-20 15:49:00", "link": "http://arxiv.org/abs/2210.11359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meeting Decision Tracker: Making Meeting Minutes with De-Contextualized\n  Utterances", "abstract": "Meetings are a universal process to make decisions in business and project\ncollaboration. The capability to automatically itemize the decisions in daily\nmeetings allows for extensive tracking of past discussions. To that end, we\ndeveloped Meeting Decision Tracker, a prototype system to construct decision\nitems comprising decision utterance detector (DUD) and decision utterance\nrewriter (DUR). We show that DUR makes a sizable contribution to improving the\nuser experience by dealing with utterance collapse in natural conversation. An\nintroduction video of our system is also available at\nhttps://youtu.be/TG1pJJo0Iqo.", "published": "2022-10-20 16:14:03", "link": "http://arxiv.org/abs/2210.11374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual Recipe Generation: Exploring Compositional Generalization\n  in a Realistic Scenario", "abstract": "People can acquire knowledge in an unsupervised manner by reading, and\ncompose the knowledge to make novel combinations. In this paper, we investigate\nwhether pretrained language models can perform compositional generalization in\na realistic setting: recipe generation. We design the counterfactual recipe\ngeneration task, which asks models to modify a base recipe according to the\nchange of an ingredient. This task requires compositional generalization at two\nlevels: the surface level of incorporating the new ingredient into the base\nrecipe, and the deeper level of adjusting actions related to the changing\ningredient. We collect a large-scale recipe dataset in Chinese for models to\nlearn culinary knowledge, and a subset of action-level fine-grained annotations\nfor evaluation. We finetune pretrained language models on the recipe corpus,\nand use unsupervised counterfactual generation methods to generate modified\nrecipes. Results show that existing models have difficulties in modifying the\ningredients while preserving the original text style, and often miss actions\nthat need to be adjusted. Although pretrained language models can generate\nfluent recipe texts, they fail to truly learn and use the culinary knowledge in\na compositional way. Code and data are available at\nhttps://github.com/xxxiaol/counterfactual-recipe-generation.", "published": "2022-10-20 17:21:46", "link": "http://arxiv.org/abs/2210.11431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Choose Your Lenses: Flaws in Gender Bias Evaluation", "abstract": "Considerable efforts to measure and mitigate gender bias in recent years have\nled to the introduction of an abundance of tasks, datasets, and metrics used in\nthis vein. In this position paper, we assess the current paradigm of gender\nbias evaluation and identify several flaws in it. First, we highlight the\nimportance of extrinsic bias metrics that measure how a model's performance on\nsome task is affected by gender, as opposed to intrinsic evaluations of model\nrepresentations, which are less strongly connected to specific harms to people\ninteracting with systems. We find that only a few extrinsic metrics are\nmeasured in most studies, although more can be measured. Second, we find that\ndatasets and metrics are often coupled, and discuss how their coupling hinders\nthe ability to obtain reliable conclusions, and how one may decouple them. We\nthen investigate how the choice of the dataset and its composition, as well as\nthe choice of the metric, affect bias measurement, finding significant\nvariations across each of them. Finally, we propose several guidelines for more\nreliable gender bias evaluation.", "published": "2022-10-20 17:59:55", "link": "http://arxiv.org/abs/2210.11471v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Balanced Adversarial Training: Balancing Tradeoffs between Fickleness\n  and Obstinacy in NLP Models", "abstract": "Traditional (fickle) adversarial examples involve finding a small\nperturbation that does not change an input's true label but confuses the\nclassifier into outputting a different prediction. Conversely, obstinate\nadversarial examples occur when an adversary finds a small perturbation that\npreserves the classifier's prediction but changes the true label of an input.\nAdversarial training and certified robust training have shown some\neffectiveness in improving the robustness of machine learnt models to fickle\nadversarial examples. We show that standard adversarial training methods\nfocused on reducing vulnerability to fickle adversarial examples may make a\nmodel more vulnerable to obstinate adversarial examples, with experiments for\nboth natural language inference and paraphrase identification tasks. To counter\nthis phenomenon, we introduce Balanced Adversarial Training, which incorporates\ncontrastive learning to increase robustness against both fickle and obstinate\nadversarial examples.", "published": "2022-10-20 18:02:07", "link": "http://arxiv.org/abs/2210.11498v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Communication breakdown: On the low mutual intelligibility between human\n  and neural captioning", "abstract": "We compare the 0-shot performance of a neural caption-based image retriever\nwhen given as input either human-produced captions or captions generated by a\nneural captioner. We conduct this comparison on the recently introduced\nImageCoDe data-set (Krojer et al., 2022) which contains hard distractors nearly\nidentical to the images to be retrieved. We find that the neural retriever has\nmuch higher performance when fed neural rather than human captions, despite the\nfact that the former, unlike the latter, were generated without awareness of\nthe distractors that make the task hard. Even more remarkably, when the same\nneural captions are given to human subjects, their retrieval performance is\nalmost at chance level. Our results thus add to the growing body of evidence\nthat, even when the ``language'' of neural models resembles English, this\nsuperficial resemblance might be deeply misleading.", "published": "2022-10-20 18:24:11", "link": "http://arxiv.org/abs/2210.11512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Text Deidentification", "abstract": "Deidentification seeks to anonymize textual data prior to distribution.\nAutomatic deidentification primarily uses supervised named entity recognition\nfrom human-labeled data points. We propose an unsupervised deidentification\nmethod that masks words that leak personally-identifying information. The\napproach utilizes a specially trained reidentification model to identify\nindividuals from redacted personal documents. Motivated by K-anonymity based\nprivacy, we generate redactions that ensure a minimum reidentification rank for\nthe correct profile of the document. To evaluate this approach, we consider the\ntask of deidentifying Wikipedia Biographies, and evaluate using an adversarial\nreidentification metric. Compared to a set of unsupervised baselines, our\napproach deidentifies documents more completely while removing fewer words.\nQualitatively, we see that the approach eliminates many identifying aspects\nthat would fall outside of the common named entity based approach.", "published": "2022-10-20 18:54:39", "link": "http://arxiv.org/abs/2210.11528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CONSISTENT: Open-Ended Question Generation From News Articles", "abstract": "Recent work on question generation has largely focused on factoid questions\nsuch as who, what, where, when about basic facts. Generating open-ended why,\nhow, what, etc. questions that require long-form answers have proven more\ndifficult. To facilitate the generation of open-ended questions, we propose\nCONSISTENT, a new end-to-end system for generating open-ended questions that\nare answerable from and faithful to the input text. Using news articles as a\ntrustworthy foundation for experimentation, we demonstrate our model's strength\nover several baselines using both automatic and human=based evaluations. We\ncontribute an evaluation dataset of expert-generated open-ended questions.We\ndiscuss potential downstream applications for news media organizations.", "published": "2022-10-20 19:10:07", "link": "http://arxiv.org/abs/2210.11536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Dataset Shortcuts with Grammar Induction", "abstract": "Many NLP datasets have been found to contain shortcuts: simple decision rules\nthat achieve surprisingly high accuracy. However, it is difficult to discover\nshortcuts automatically. Prior work on automatic shortcut detection has focused\non enumerating features like unigrams or bigrams, which can find only low-level\nshortcuts, or relied on post-hoc model interpretability methods like saliency\nmaps, which reveal qualitative patterns without a clear statistical\ninterpretation. In this work, we propose to use probabilistic grammars to\ncharacterize and discover shortcuts in NLP datasets. Specifically, we use a\ncontext-free grammar to model patterns in sentence classification datasets and\nuse a synchronous context-free grammar to model datasets involving sentence\npairs. The resulting grammars reveal interesting shortcut features in a number\nof datasets, including both simple and high-level features, and automatically\nidentify groups of test examples on which conventional classifiers fail.\nFinally, we show that the features we discover can be used to generate\ndiagnostic contrast examples and incorporated into standard robust optimization\nmethods to improve worst-group accuracy.", "published": "2022-10-20 19:54:11", "link": "http://arxiv.org/abs/2210.11560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Human Strategies for Generating Word-Level Adversarial\n  Examples", "abstract": "Adversarial examples in NLP are receiving increasing research attention. One\nline of investigation is the generation of word-level adversarial examples\nagainst fine-tuned Transformer models that preserve naturalness and\ngrammaticality. Previous work found that human- and machine-generated\nadversarial examples are comparable in their naturalness and grammatical\ncorrectness. Most notably, humans were able to generate adversarial examples\nmuch more effortlessly than automated attacks. In this paper, we provide a\ndetailed analysis of exactly how humans create these adversarial examples. By\nexploring the behavioural patterns of human workers during the generation\nprocess, we identify statistically significant tendencies based on which words\nhumans prefer to select for adversarial replacement (e.g., word frequencies,\nword saliencies, sentiment) as well as where and when words are replaced in an\ninput sequence. With our findings, we seek to inspire efforts that harness\nhuman strategies for more robust NLP models.", "published": "2022-10-20 21:16:44", "link": "http://arxiv.org/abs/2210.11598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The VolcTrans System for WMT22 Multilingual Machine Translation Task", "abstract": "This report describes our VolcTrans system for the WMT22 shared task on\nlarge-scale multilingual machine translation. We participated in the\nunconstrained track which allows the use of external resources. Our system is a\ntransformerbased multilingual model trained on data from multiple sources\nincluding the public training set from the data track, NLLB data provided by\nMeta AI, self-collected parallel corpora, and pseudo bitext from\nback-translation. A series of heuristic rules clean both bilingual and\nmonolingual texts. On the official test set, our system achieves 17.3 BLEU,\n21.9 spBLEU, and 41.9 chrF2++ on average over all language pairs. The average\ninference speed is 11.5 sentences per second using a single Nvidia Tesla V100\nGPU. Our code and trained models are available at\nhttps://github.com/xian8/wmt22", "published": "2022-10-20 21:18:03", "link": "http://arxiv.org/abs/2210.11599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tag-Set-Sequence Learning for Generating Question-Answer Pairs", "abstract": "Transformer-based QG models can generate question-answer pairs (QAPs) with\nhigh qualities, but may also generate silly questions for certain texts. We\npresent a new method called tag-set sequence learning to tackle this problem,\nwhere a tag-set sequence is a sequence of tag sets to capture the syntactic and\nsemantic information of the underlying sentence, and a tag set consists of one\nor more language feature tags, including, for example, semantic-role-labeling,\npart-of-speech, named-entity-recognition, and sentiment-indication tags. We\nconstruct a system called TSS-Learner to learn tag-set sequences from given\ndeclarative sentences and the corresponding interrogative sentences, and derive\nanswers to the latter. We train a TSS-Learner model for the English language\nusing a small training dataset and show that it can indeed generate adequate\nQAPs for certain texts that transformer-based models do poorly. Human\nevaluation on the QAPs generated by TSS-Learner over SAT practice reading tests\nis encouraging.", "published": "2022-10-20 21:51:00", "link": "http://arxiv.org/abs/2210.11608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Self-Improve", "abstract": "Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and\n63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.", "published": "2022-10-20 21:53:54", "link": "http://arxiv.org/abs/2210.11610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Domains Be Transferred Across Languages in Multi-Domain Multilingual\n  Neural Machine Translation?", "abstract": "Previous works mostly focus on either multilingual or multi-domain aspects of\nneural machine translation (NMT). This paper investigates whether the domain\ninformation can be transferred across languages on the composition of\nmulti-domain and multilingual NMT, particularly for the incomplete data\ncondition where in-domain bitext is missing for some language pairs. Our\nresults in the curated leave-one-domain-out experiments show that multi-domain\nmultilingual (MDML) NMT can boost zero-shot translation performance up to +10\ngains on BLEU, as well as aid the generalisation of multi-domain NMT to the\nmissing domain. We also explore strategies for effective integration of\nmultilingual and multi-domain NMT, including language and domain tag\ncombination and auxiliary task training. We find that learning domain-aware\nrepresentations and adding target-language tags to the encoder leads to\neffective MDML-NMT.", "published": "2022-10-20 23:13:54", "link": "http://arxiv.org/abs/2210.11628v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AugCSE: Contrastive Sentence Embedding with Diverse Augmentations", "abstract": "Data augmentation techniques have been proven useful in many applications in\nNLP fields. Most augmentations are task-specific, and cannot be used as a\ngeneral-purpose tool. In our work, we present AugCSE, a unified framework to\nutilize diverse sets of data augmentations to achieve a better, general\npurpose, sentence embedding model. Building upon the latest sentence embedding\nmodels, our approach uses a simple antagonistic discriminator that\ndifferentiates the augmentation types. With the finetuning objective borrowed\nfrom domain adaptation, we show that diverse augmentations, which often lead to\nconflicting contrastive signals, can be tamed to produce a better and more\nrobust sentence representation. Our methods achieve state-of-the-art results on\ndownstream transfer tasks and perform competitively on semantic textual\nsimilarity tasks, using only unsupervised data.", "published": "2022-10-20 03:52:07", "link": "http://arxiv.org/abs/2210.13749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-trained Sentence Embeddings for Implicit Discourse Relation\n  Classification", "abstract": "Implicit discourse relations bind smaller linguistic units into coherent\ntexts. Automatic sense prediction for implicit relations is hard, because it\nrequires understanding the semantics of the linked arguments. Furthermore,\nannotated datasets contain relatively few labeled examples, due to the scale of\nthe phenomenon: on average each discourse relation encompasses several dozen\nwords. In this paper, we explore the utility of pre-trained sentence embeddings\nas base representations in a neural network for implicit discourse relation\nsense classification. We present a series of experiments using both supervised\nend-to-end trained models and pre-trained sentence encoding techniques -\nSkipThought, Sent2vec and Infersent. The pre-trained embeddings are competitive\nwith the end-to-end model, and the approaches are complementary, with combined\nmodels yielding significant performance improvements on two of the three\nevaluations.", "published": "2022-10-20 04:17:03", "link": "http://arxiv.org/abs/2210.11005v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Out-of-Distribution Detection in Natural Language\n  Understanding via Implicit Layer Ensemble", "abstract": "Out-of-distribution (OOD) detection aims to discern outliers from the\nintended data distribution, which is crucial to maintaining high reliability\nand a good user experience. Most recent studies in OOD detection utilize the\ninformation from a single representation that resides in the penultimate layer\nto determine whether the input is anomalous or not. Although such a method is\nstraightforward, the potential of diverse information in the intermediate\nlayers is overlooked. In this paper, we propose a novel framework based on\ncontrastive learning that encourages intermediate features to learn\nlayer-specialized representations and assembles them implicitly into a single\nrepresentation to absorb rich information in the pre-trained language model.\nExtensive experiments in various intent classification and OOD datasets\ndemonstrate that our approach is significantly more effective than other works.", "published": "2022-10-20 06:05:58", "link": "http://arxiv.org/abs/2210.11034v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via\n  Contrastive Learning", "abstract": "This paper finds that contrastive learning can produce superior sentence\nembeddings for pre-trained models but is also vulnerable to backdoor attacks.\nWe present the first backdoor attack framework, BadCSE, for state-of-the-art\nsentence embeddings under supervised and unsupervised learning settings. The\nattack manipulates the construction of positive and negative pairs so that the\nbackdoored samples have a similar embedding with the target sample (targeted\nattack) or the negative embedding of its clean version (non-targeted attack).\nBy injecting the backdoor in sentence embeddings, BadCSE is resistant against\ndownstream fine-tuning. We evaluate BadCSE on both STS tasks and other\ndownstream tasks. The supervised non-targeted attack obtains a performance\ndegradation of 194.86%, and the targeted attack maps the backdoored samples to\nthe target embedding with a 97.70% success rate while maintaining the model\nutility.", "published": "2022-10-20 08:19:18", "link": "http://arxiv.org/abs/2210.11082v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text\n  Generation", "abstract": "Image-to-text tasks, such as open-ended image captioning and controllable\nimage description, have received extensive attention for decades. Here, we\nfurther advance this line of work by presenting Visual Spatial Description\n(VSD), a new perspective for image-to-text toward spatial semantics. Given an\nimage and two objects inside it, VSD aims to produce one description focusing\non the spatial perspective between the two objects. Accordingly, we manually\nannotate a dataset to facilitate the investigation of the newly-introduced task\nand build several benchmark encoder-decoder models by using VL-BART and VL-T5\nas backbones. In addition, we investigate pipeline and joint end-to-end\narchitectures for incorporating visual spatial relationship classification\n(VSRC) information into our model. Finally, we conduct experiments on our\nbenchmark dataset to evaluate all our models. Results show that our models are\nimpressive, providing accurate and human-like spatial-oriented text\ndescriptions. Meanwhile, VSRC has great potential for VSD, and the joint\nend-to-end architecture is the better choice for their integration. We make the\ndataset and codes public for research purposes.", "published": "2022-10-20 09:10:17", "link": "http://arxiv.org/abs/2210.11109v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Wait-info Policy: Balancing Source and Target at Information Level for\n  Simultaneous Machine Translation", "abstract": "Simultaneous machine translation (SiMT) outputs the translation while\nreceiving the source inputs, and hence needs to balance the received source\ninformation and translated target information to make a reasonable decision\nbetween waiting for inputs or outputting translation. Previous methods always\nbalance source and target information at the token level, either directly\nwaiting for a fixed number of tokens or adjusting the waiting based on the\ncurrent token. In this paper, we propose a Wait-info Policy to balance source\nand target at the information level. We first quantify the amount of\ninformation contained in each token, named info. Then during simultaneous\ntranslation, the decision of waiting or outputting is made based on the\ncomparison results between the total info of previous target outputs and\nreceived source inputs. Experiments show that our method outperforms strong\nbaselines under and achieves better balance via the proposed info.", "published": "2022-10-20 12:53:25", "link": "http://arxiv.org/abs/2210.11220v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DialogUSR: Complex Dialogue Utterance Splitting and Reformulation for\n  Multiple Intent Detection", "abstract": "While interacting with chatbots, users may elicit multiple intents in a\nsingle dialogue utterance. Instead of training a dedicated multi-intent\ndetection model, we propose DialogUSR, a dialogue utterance splitting and\nreformulation task that first splits multi-intent user query into several\nsingle-intent sub-queries and then recovers all the coreferred and omitted\ninformation in the sub-queries. DialogUSR can serve as a plug-in and\ndomain-agnostic module that empowers the multi-intent detection for the\ndeployed chatbots with minimal efforts. We collect a high-quality naturally\noccurring dataset that covers 23 domains with a multi-step crowd-souring\nprocedure. To benchmark the proposed dataset, we propose multiple action-based\ngenerative models that involve end-to-end and two-stage training, and conduct\nin-depth analyses on the pros and cons of the proposed baselines.", "published": "2022-10-20 13:56:35", "link": "http://arxiv.org/abs/2210.11279v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Instruction-Finetuned Language Models", "abstract": "Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.", "published": "2022-10-20 16:58:32", "link": "http://arxiv.org/abs/2210.11416v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dense Paraphrasing for Textual Enrichment", "abstract": "Understanding inferences and answering questions from text requires more than\nmerely recovering surface arguments, adjuncts, or strings associated with the\nquery terms. As humans, we interpret sentences as contextualized components of\na narrative or discourse, by both filling in missing information, and reasoning\nabout event consequences. In this paper, we define the process of rewriting a\ntextual expression (lexeme or phrase) such that it reduces ambiguity while also\nmaking explicit the underlying semantics that is not (necessarily) expressed in\nthe economy of sentence structure as Dense Paraphrasing (DP). We build the\nfirst complete DP dataset, provide the scope and design of the annotation task,\nand present results demonstrating how this DP process can enrich a source text\nto improve inferencing and QA task performance. The data and the source code\nwill be publicly available.", "published": "2022-10-20 19:58:31", "link": "http://arxiv.org/abs/2210.11563v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Searching for a higher power in the human evaluation of MT", "abstract": "In MT evaluation, pairwise comparisons are conducted to identify the better\nsystem. In conducting the comparison, the experimenter must allocate a budget\nto collect Direct Assessment (DA) judgments. We provide a cost effective way to\nspend the budget, but show that typical budget sizes often do not allow for\nsolid comparison. Taking the perspective that the basis of solid comparison is\nin achieving statistical significance, we study the power (rate of achieving\nsignificance) on a large collection of pairwise DA comparisons. Due to the\nnature of statistical estimation, power is low for differentiating less than\n1-2 DA points, and to achieve a notable increase in power requires at least\n2-3x more samples. Applying variance reduction alone will not yield these\ngains, so we must face the reality of undetectable differences and spending\nincreases. In this context, we propose interim testing, an \"early stopping\"\ncollection procedure that yields more power per judgment collected, which\nadaptively focuses the budget on pairs that are borderline significant. Interim\ntesting can achieve up to a 27% efficiency gain when spending 3x the current\nbudget, or 18% savings at the current evaluation power.", "published": "2022-10-20 22:07:12", "link": "http://arxiv.org/abs/2210.11612v2", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "Boosting Natural Language Generation from Instructions with\n  Meta-Learning", "abstract": "Recent work has shown that language models (LMs) trained with multi-task\n\\textit{instructional learning} (MTIL) can solve diverse NLP tasks in zero- and\nfew-shot settings with improved performance compared to prompt tuning. MTIL\nillustrates that LMs can extract and use information about the task from\ninstructions beyond the surface patterns of the inputs and outputs. This\nsuggests that meta-learning may further enhance the utilization of instructions\nfor effective task transfer. In this paper we investigate whether meta-learning\napplied to MTIL can further improve generalization to unseen tasks in a\nzero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in\nthree directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network\n(HNet) based adaptation to generate task specific parameters conditioned on\ninstructions, and 3) an approach combining HNet and MAML. Through extensive\nexperiments on the large scale Natural Instructions V2 dataset, we show that\nour proposed approaches significantly improve over strong baselines in\nzero-shot settings. In particular, meta-learning improves the effectiveness of\ninstructions and is most impactful when the test tasks are strictly zero-shot\n(i.e. no similar tasks in the training set) and are \"hard\" for LMs,\nillustrating the potential of meta-learning for MTIL for out-of-distribution\ntasks.", "published": "2022-10-20 22:23:23", "link": "http://arxiv.org/abs/2210.11617v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MBTI Personality Prediction for Fictional Characters Using Movie Scripts", "abstract": "An NLP model that understands stories should be able to understand the\ncharacters in them. To support the development of neural models for this\npurpose, we construct a benchmark, Story2Personality. The task is to predict a\nmovie character's MBTI or Big 5 personality types based on the narratives of\nthe character. Experiments show that our task is challenging for the existing\ntext classification models, as none is able to largely outperform random\nguesses. We further proposed a multi-view model for personality prediction\nusing both verbal and non-verbal descriptions, which gives improvement compared\nto using only verbal descriptions. The uniqueness and challenges in our dataset\ncall for the development of narrative comprehension techniques from the\nperspective of understanding characters.", "published": "2022-10-20 03:41:07", "link": "http://arxiv.org/abs/2210.10994v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MovieCLIP: Visual Scene Recognition in Movies", "abstract": "Longform media such as movies have complex narrative structures, with events\nspanning a rich variety of ambient visual scenes. Domain specific challenges\nassociated with visual scenes in movies include transitions, person coverage,\nand a wide array of real-life and fictional scenarios. Existing visual scene\ndatasets in movies have limited taxonomies and don't consider the visual scene\ntransition within movie clips. In this work, we address the problem of visual\nscene recognition in movies by first automatically curating a new and extensive\nmovie-centric taxonomy of 179 scene labels derived from movie scripts and\nauxiliary web-based video datasets. Instead of manual annotations which can be\nexpensive, we use CLIP to weakly label 1.12 million shots from 32K movie clips\nbased on our proposed taxonomy. We provide baseline visual models trained on\nthe weakly labeled dataset called MovieCLIP and evaluate them on an independent\ndataset verified by human raters. We show that leveraging features from models\npretrained on MovieCLIP benefits downstream tasks such as multi-label scene and\ngenre classification of web videos and movie trailers.", "published": "2022-10-20 07:38:56", "link": "http://arxiv.org/abs/2210.11065v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Transcending Scaling Laws with 0.1% Extra Compute", "abstract": "Scaling language models improves performance but comes with significant\ncomputational costs. This paper proposes UL2R, a method that substantially\nimproves existing language models and their scaling curves with a relatively\ntiny amount of extra compute. The key idea is to continue training a\nstate-of-the-art large language model (e.g., PaLM) on a few more steps with\nUL2's mixture-of-denoiser objective. We show that, with almost negligible extra\ncomputational costs and no new sources of data, we are able to substantially\nimprove the scaling properties of large language models on downstream metrics.\nIn this paper, we continue training PaLM with UL2R, introducing a new set of\nmodels at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B\nscale, we show an approximately 2x computational savings rate where U-PaLM\nachieves the same performance as the final PaLM 540B model at around half its\ncomputational budget (i.e., saving $\\sim$4.4 million TPUv4 hours). We further\nshow that this improved scaling curve leads to 'emergent abilities' on\nchallenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM\non some tasks or demonstrates better quality at much smaller scale (62B as\nopposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many\nfew-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question\nanswering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual\ntasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide\nqualitative examples showing the new capabilities of U-PaLM for single and\nmulti-span infilling.", "published": "2022-10-20 16:46:41", "link": "http://arxiv.org/abs/2210.11399v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multitasking Models are Robust to Structural Failure: A Neural Model for\n  Bilingual Cognitive Reserve", "abstract": "We find a surprising connection between multitask learning and robustness to\nneuron failures. Our experiments show that bilingual language models retain\nhigher performance under various neuron perturbations, such as random\ndeletions, magnitude pruning and weight noise compared to equivalent\nmonolingual ones. We provide a theoretical justification for this robustness by\nmathematically analyzing linear representation learning and showing that\nmultitasking creates more robust representations. Our analysis connects\nrobustness to spectral properties of the learned representation and proves that\nmultitasking leads to higher robustness for diverse task vectors. We\nopen-source our code and models:\nhttps://github.com/giannisdaras/multilingual_robustness", "published": "2022-10-20 22:23:27", "link": "http://arxiv.org/abs/2210.11618v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SMaLL-100: Introducing Shallow Multilingual Machine Translation Model\n  for Low-Resource Languages", "abstract": "In recent years, multilingual machine translation models have achieved\npromising performance on low-resource language pairs by sharing information\nbetween similar languages, thus enabling zero-shot translation. To overcome the\n\"curse of multilinguality\", these models often opt for scaling up the number of\nparameters, which makes their use in resource-constrained environments\nchallenging. We introduce SMaLL-100, a distilled version of the M2M-100 (12B)\nmodel, a massively multilingual machine translation model covering 100\nlanguages. We train SMaLL-100 with uniform sampling across all language pairs\nand therefore focus on preserving the performance of low-resource languages. We\nevaluate SMaLL-100 on different low-resource benchmarks: FLORES-101, Tatoeba,\nand TICO-19 and demonstrate that it outperforms previous massively multilingual\nmodels of comparable sizes (200-600M) while improving inference latency and\nmemory usage. Additionally, our model achieves comparable results to M2M-100\n(1.2B), while being 3.6x smaller and 4.3x faster at inference. Code and\npre-trained models: https://github.com/alirezamshi/small100", "published": "2022-10-20 22:32:29", "link": "http://arxiv.org/abs/2210.11621v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models to Enhance Programming Error Messages", "abstract": "A key part of learning to program is learning to understand programming error\nmessages. They can be hard to interpret and identifying the cause of errors can\nbe time-consuming. One factor in this challenge is that the messages are\ntypically intended for an audience that already knows how to program, or even\nfor programming environments that then use the information to highlight areas\nin code. Researchers have been working on making these errors more novice\nfriendly since the 1960s, however progress has been slow. The present work\ncontributes to this stream of research by using large language models to\nenhance programming error messages with explanations of the errors and\nsuggestions on how to fix the error. Large language models can be used to\ncreate useful and novice-friendly enhancements to programming error messages\nthat sometimes surpass the original programming error messages in\ninterpretability and actionability. These results provide further evidence of\nthe benefits of large language models for computing educators, highlighting\ntheir use in areas known to be challenging for students. We further discuss the\nbenefits and downsides of large language models and highlight future streams of\nresearch for enhancing programming error messages.", "published": "2022-10-20 23:17:26", "link": "http://arxiv.org/abs/2210.11630v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.HC"}
{"title": "Improving Semi-supervised End-to-end Automatic Speech Recognition using\n  CycleGAN and Inter-domain Losses", "abstract": "We propose a novel method that combines CycleGAN and inter-domain losses for\nsemi-supervised end-to-end automatic speech recognition. Inter-domain loss\ntargets the extraction of an intermediate shared representation of speech and\ntext inputs using a shared network. CycleGAN uses cycle-consistent loss and the\nidentity mapping loss to preserve relevant characteristics of the input feature\nafter converting from one domain to another. As such, both approaches are\nsuitable to train end-to-end models on unpaired speech-text inputs. In this\npaper, we exploit the advantages from both inter-domain loss and CycleGAN to\nachieve better shared representation of unpaired speech and text inputs and\nthus improve the speech-to-text mapping. Our experimental results on the WSJ\neval92 and Voxforge (non English) show 8~8.5% character error rate reduction\nover the baseline, and the results on LibriSpeech test_clean also show\nnoticeable improvement.", "published": "2022-10-20 23:58:50", "link": "http://arxiv.org/abs/2210.11642v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Predictive Uncertainty and Calibration in NLP: A Study on the\n  Impact of Method & Data Scarcity", "abstract": "We investigate the problem of determining the predictive confidence (or,\nconversely, uncertainty) of a neural classifier through the lens of\nlow-resource languages. By training models on sub-sampled datasets in three\ndifferent languages, we assess the quality of estimates from a wide array of\napproaches and their dependence on the amount of available data. We find that\nwhile approaches based on pre-trained models and ensembles achieve the best\nresults overall, the quality of uncertainty estimates can surprisingly suffer\nwith more data. We also perform a qualitative analysis of uncertainties on\nsequences, discovering that a model's total uncertainty seems to be influenced\nto a large degree by its data uncertainty, not model uncertainty. All model\nimplementations are open-sourced in a software package.", "published": "2022-10-20 15:42:02", "link": "http://arxiv.org/abs/2210.15452v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context-driven Visual Object Recognition based on Knowledge Graphs", "abstract": "Current deep learning methods for object recognition are purely data-driven\nand require a large number of training samples to achieve good results. Due to\ntheir sole dependence on image data, these methods tend to fail when confronted\nwith new environments where even small deviations occur. Human perception,\nhowever, has proven to be significantly more robust to such distribution\nshifts. It is assumed that their ability to deal with unknown scenarios is\nbased on extensive incorporation of contextual knowledge. Context can be based\neither on object co-occurrences in a scene or on memory of experience. In\naccordance with the human visual cortex which uses context to form different\nobject representations for a seen image, we propose an approach that enhances\ndeep learning methods by using external contextual knowledge encoded in a\nknowledge graph. Therefore, we extract different contextual views from a\ngeneric knowledge graph, transform the views into vector space and infuse it\ninto a DNN. We conduct a series of experiments to investigate the impact of\ndifferent contextual views on the learned object representations for the same\nimage dataset. The experimental results provide evidence that the contextual\nviews influence the image representations in the DNN differently and therefore\nlead to different predictions for the same images. We also show that context\nhelps to strengthen the robustness of object recognition models for\nout-of-distribution images, usually occurring in transfer learning tasks or\nreal-world scenarios.", "published": "2022-10-20 13:09:00", "link": "http://arxiv.org/abs/2210.11233v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SC"], "primary_category": "cs.AI"}
{"title": "Speech Dereverberation with a Reverberation Time Shortening Target", "abstract": "This work proposes a new learning target based on reverberation time\nshortening (RTS) for speech dereverberation. The learning target for\ndereverberation is usually set as the direct-path speech or optionally with\nsome early reflections. This type of target suddenly truncates the\nreverberation, and thus it may not be suitable for network training. The\nproposed RTS target suppresses reverberation and meanwhile maintains the\nexponential decaying property of reverberation, which will ease the network\ntraining, and thus reduce signal distortion caused by the prediction error.\nMoreover, this work experimentally study to adapt our previously proposed\nFullSubNet speech denoising network to speech dereverberation. Experiments show\nthat RTS is a more suitable learning target than direct-path speech and early\nreflections, in terms of better suppressing reverberation and signal\ndistortion. FullSubNet is able to achieve outstanding dereverberation\nperformance.", "published": "2022-10-20 08:33:23", "link": "http://arxiv.org/abs/2210.11089v6", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Model-matching Principle Applied to the Design of an Array-based\n  All-neural Binaural Rendering System for Audio Telepresence", "abstract": "Telepresence aims to create an immersive but virtual experience of the audio\nand visual scene at the far end for users at the near end. In this\ncontribution, we propose an array-based binaural rendering system that converts\nthe array microphone signals into the head-related transfer function (HRTF)\nfiltered output signals for headphone-rendering. The proposed approach is\nformulated in light of a model-matching principle (MMP) and is capable of\ndelivering more immersive experience than the conventional\nlocalization-beamforming-HRTF filtering (LBH) approach. The MMP-based rendering\nsystem can be realized via multichannel inverse filtering (MIF) and\nmultichannel deep filtering (MDF). In this study, we adopted the MDF approach\nand used the LBH as well as MIF as the baselines. The all-neural system jointly\ncaptures the spatial information (spatial rendering), preserves ambient sound\n(enhancement), and reduces noise (enhancement) before generating binaural\noutputs. Objective and subjective tests are employed to compare the proposed\ntelepresence system with two baselines.", "published": "2022-10-20 09:33:20", "link": "http://arxiv.org/abs/2210.11123v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text Enhancement for Paragraph Processing in End-to-End Code-switching\n  TTS", "abstract": "Current end-to-end code-switching Text-to-Speech (TTS) can already generate\nhigh quality two languages speech in the same utterance with single speaker\nbilingual corpora. When the speakers of the bilingual corpora are different,\nthe naturalness and consistency of the code-switching TTS will be poor. The\ncross-lingual embedding layers structure we proposed makes similar syllables in\ndifferent languages relevant, thus improving the naturalness and consistency of\ngenerated speech. In the end-to-end code-switching TTS, there exists problem of\nprosody instability when synthesizing paragraph text. The text enhancement\nmethod we proposed makes the input contain prosodic information and\nsentence-level context information, thus improving the prosody stability of\nparagraph text. Experimental results demonstrate the effectiveness of the\nproposed methods in the naturalness, consistency, and prosody stability. In\naddition to Mandarin and English, we also apply these methods to Shanghaiese\nand Cantonese corpora, proving that the methods we proposed can be extended to\nother languages to build end-to-end code-switching TTS system.", "published": "2022-10-20 17:19:51", "link": "http://arxiv.org/abs/2210.11429v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discriminatory and orthogonal feature learning for noise robust keyword\n  spotting", "abstract": "Keyword Spotting (KWS) is an essential component in a smart device for\nalerting the system when a user prompts it with a command. As these devices are\ntypically constrained by computational and energy resources, the KWS model\nshould be designed with a small footprint. In our previous work, we developed\nlightweight dynamic filters which extract a robust feature map within a noisy\nenvironment. The learning variables of the dynamic filter are jointly optimized\nwith KWS weights by using Cross-Entropy (CE) loss. CE loss alone, however, is\nnot sufficient for high performance when the SNR is low. In order to train the\nnetwork for more robust performance in noisy environments, we introduce the LOw\nVariant Orthogonal (LOVO) loss. The LOVO loss is composed of a triplet loss\napplied on the output of the dynamic filter, a spectral norm-based orthogonal\nloss, and an inner class distance loss applied in the KWS model. These losses\nare particularly useful in encouraging the network to extract discriminatory\nfeatures in unseen noise environments.", "published": "2022-10-20 18:44:16", "link": "http://arxiv.org/abs/2210.11519v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Anchored Speech Recognition with Neural Transducers", "abstract": "Neural transducers have achieved human level performance on standard speech\nrecognition benchmarks. However, their performance significantly degrades in\nthe presence of cross-talk, especially when the primary speaker has a low\nsignal-to-noise ratio. Anchored speech recognition refers to a class of methods\nthat use information from an anchor segment (e.g., wake-words) to recognize\ndevice-directed speech while ignoring interfering background speech. In this\npaper, we investigate anchored speech recognition to make neural transducers\nrobust to background speech. We extract context information from the anchor\nsegment with a tiny auxiliary network, and use encoder biasing and joiner\ngating to guide the transducer towards the target speech. Moreover, to improve\nthe robustness of context embedding extraction, we propose auxiliary training\nobjectives to disentangle lexical content from speaking style. We evaluate our\nmethods on synthetic LibriSpeech-based mixtures comprising several SNR and\noverlap conditions; they improve relative word error rates by 19.6% over a\nstrong baseline, when averaged over all conditions.", "published": "2022-10-20 21:00:42", "link": "http://arxiv.org/abs/2210.11588v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large-scale learning of generalised representations for speaker\n  recognition", "abstract": "The objective of this work is to develop a speaker recognition model to be\nused in diverse scenarios. We hypothesise that two components should be\nadequately configured to build such a model. First, adequate architecture would\nbe required. We explore several recent state-of-the-art models, including\nECAPA-TDNN and MFA-Conformer, as well as other baselines. Second, a massive\namount of data would be required. We investigate several new training data\nconfigurations combining a few existing datasets. The most extensive\nconfiguration includes over 87k speakers' 10.22k hours of speech. Four\nevaluation protocols are adopted to measure how the trained model performs in\ndiverse scenarios. Through experiments, we find that MFA-Conformer with the\nleast inductive bias generalises the best. We also show that training with\nproposed large data configurations gives better performance. A boost in\ngeneralisation is observed, where the average performance on four evaluation\nprotocols improves by more than 20%. In addition, we also demonstrate that\nthese models' performances can improve even further when increasing capacity.", "published": "2022-10-20 03:08:18", "link": "http://arxiv.org/abs/2210.10985v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DisC-VC: Disentangled and F0-Controllable Neural Voice Conversion", "abstract": "Voice conversion is a task to convert a non-linguistic feature of a given\nutterance. Since naturalness of speech strongly depends on its pitch pattern,\nin some applications, it would be desirable to keep the original rise/fall\npitch pattern while changing the speaker identity. Some of the existing methods\naddress this problem by either using a source-filter model or developing a\nneural network that takes an F0 pattern as input to the model. Although the\nlatter approach can achieve relatively high sound quality compared to the\nformer one, there is no consideration for discrepancy between the target and\ngenerated F0 patterns in its training process. In this paper, we propose a new\nvariational-autoencoder-based voice conversion model accompanied by an\nauxiliary network, which ensures that the conversion result correctly reflects\nthe specified F0/timbre information. We show the effectiveness of the proposed\nmethod by objective and subjective evaluations.", "published": "2022-10-20 07:30:07", "link": "http://arxiv.org/abs/2210.11059v1", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Frequency of Interest-based Noise Attenuation Method to Improve Anomaly\n  Detection Performance", "abstract": "Accurately extracting driving events is the way to maximize computational\nefficiency and anomaly detection performance in the tire frictional nose-based\nanomaly detection task. This study proposes a concise and highly useful method\nfor improving the precision of the event extraction that is hindered by extra\nnoise such as wind noise, which is difficult to characterize clearly due to its\nrandomness. The core of the proposed method is based on the identification of\nthe road friction sound corresponding to the frequency of interest and removing\nthe opposite characteristics with several frequency filters. Our method enables\nprecision maximization of driving event extraction while improving anomaly\ndetection performance by an average of 8.506%. Therefore, we conclude our\nmethod is a practical solution suitable for road surface anomaly detection\npurposes in outdoor edge computing environments.", "published": "2022-10-20 07:42:33", "link": "http://arxiv.org/abs/2210.11068v3", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Robust One-Shot Singing Voice Conversion", "abstract": "Recent progress in deep generative models has improved the quality of voice\nconversion in the speech domain. However, high-quality singing voice conversion\n(SVC) of unseen singers remains challenging due to the wider variety of musical\nexpressions in pitch, loudness, and pronunciation. Moreover, singing voices are\noften recorded with reverb and accompaniment music, which make SVC even more\nchallenging. In this work, we present a robust one-shot SVC (ROSVC) that\nperforms any-to-any SVC robustly even on such distorted singing voices. To this\nend, we first propose a one-shot SVC model based on generative adversarial\nnetworks that generalizes to unseen singers via partial domain conditioning and\nlearns to accurately recover the target pitch via pitch distribution matching\nand AdaIN-skip conditioning. We then propose a two-stage training method called\nRobustify that train the one-shot SVC model in the first stage on clean data to\nensure high-quality conversion, and introduces enhancement modules to the\nencoders of the model in the second stage to enhance the feature extraction\nfrom distorted singing voices. To further improve the voice quality and pitch\nreconstruction accuracy, we finally propose a hierarchical diffusion model for\nsinging voice neural vocoders. Experimental results show that the proposed\nmethod outperforms state-of-the-art one-shot SVC baselines for both seen and\nunseen singers and significantly improves the robustness against distortions.", "published": "2022-10-20 08:47:35", "link": "http://arxiv.org/abs/2210.11096v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Play It Back: Iterative Attention for Audio Recognition", "abstract": "A key function of auditory cognition is the association of characteristic\nsounds with their corresponding semantics over time. Humans attempting to\ndiscriminate between fine-grained audio categories, often replay the same\ndiscriminative sounds to increase their prediction confidence. We propose an\nend-to-end attention-based architecture that through selective repetition\nattends over the most discriminative sounds across the audio sequence. Our\nmodel initially uses the full audio sequence and iteratively refines the\ntemporal segments replayed based on slot attention. At each playback, the\nselected segments are replayed using a smaller hop length which represents\nhigher resolution features within these segments. We show that our method can\nconsistently achieve state-of-the-art performance across three\naudio-classification benchmarks: AudioSet, VGG-Sound, and EPIC-KITCHENS-100.", "published": "2022-10-20 15:03:22", "link": "http://arxiv.org/abs/2210.11328v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
