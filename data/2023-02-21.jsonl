{"title": "Tell Model Where to Attend: Improving Interpretability of Aspect-Based\n  Sentiment Classification via Small Explanation Annotations", "abstract": "Gradient-based explanation methods play an important role in the field of\ninterpreting complex deep neural networks for NLP models. However, the existing\nwork has shown that the gradients of a model are unstable and easily\nmanipulable, which impacts the model's reliability largely. According to our\npreliminary analyses, we also find the interpretability of gradient-based\nmethods is limited for complex tasks, such as aspect-based sentiment\nclassification (ABSC). In this paper, we propose an\n\\textbf{I}nterpretation-\\textbf{E}nhanced \\textbf{G}radient-based framework for\n\\textbf{A}BSC via a small number of explanation annotations, namely\n\\texttt{{IEGA}}. Particularly, we first calculate the word-level saliency map\nbased on gradients to measure the importance of the words in the sentence\ntowards the given aspect. Then, we design a gradient correction module to\nenhance the model's attention on the correct parts (e.g., opinion words). Our\nmodel is model agnostic and task agnostic so that it can be integrated into the\nexisting ABSC methods or other tasks. Comprehensive experimental results on\nfour benchmark datasets show that our \\texttt{IEGA} can improve not only the\ninterpretability of the model but also the performance and robustness.", "published": "2023-02-21 06:55:08", "link": "http://arxiv.org/abs/2302.10479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connecting Humanities and Social Sciences: Applying Language and Speech\n  Technology to Online Panel Surveys", "abstract": "In this paper, we explore the application of language and speech technology\nto open-ended questions in a Dutch panel survey. In an experimental wave\nrespondents could choose to answer open questions via speech or keyboard.\nAutomatic speech recognition (ASR) was used to process spoken responses. We\nevaluated answers from these input modalities to investigate differences\nbetween spoken and typed answers.We report the errors the ASR system produces\nand investigate the impact of these errors on downstream analyses. Open-ended\nquestions give more freedom to answer for respondents, but entail a non-trivial\namount of work to analyse. We evaluated the feasibility of using\ntransformer-based models (e.g. BERT) to apply sentiment analysis and topic\nmodelling on the answers of open questions. A big advantage of\ntransformer-based models is that they are trained on a large amount of language\nmaterials and do not necessarily need training on the target materials. This is\nespecially advantageous for survey data, which does not contain a lot of text\nmaterials. We tested the quality of automatic sentiment analysis by comparing\nautomatic labeling with three human raters and tested the robustness of topic\nmodelling by comparing the generated models based on automatic and manually\ntranscribed spoken answers.", "published": "2023-02-21 10:52:15", "link": "http://arxiv.org/abs/2302.10593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generic Dependency Modeling for Multi-Party Conversation", "abstract": "To model the dependencies between utterances in multi-party conversations, we\npropose a simple and generic framework based on the dependency parsing results\nof utterances. Particularly, we present an approach to encoding the\ndependencies in the form of relative dependency encoding (ReDE) and illustrate\nhow to implement it in Transformers by modifying the computation of\nself-attention. Experimental results on four multi-party conversation\nbenchmarks show that this framework successfully boosts the general performance\nof two Transformer-based language models and leads to comparable or even\nsuperior performance compared to the state-of-the-art methods. The codes are\navailable at https://github.com/shenwzh3/ReDE.", "published": "2023-02-21 13:58:19", "link": "http://arxiv.org/abs/2302.10680v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Sentence-Level Explanation Generation for Real-World\n  Low-Resource Scenarios", "abstract": "In order to reveal the rationale behind model predictions, many works have\nexploited providing explanations in various forms. Recently, to further\nguarantee readability, more and more works turn to generate sentence-level\nhuman language explanations. However, current works pursuing sentence-level\nexplanations rely heavily on annotated training data, which limits the\ndevelopment of interpretability to only a few tasks. As far as we know, this\npaper is the first to explore this problem smoothly from weak-supervised\nlearning to unsupervised learning. Besides, we also notice the high latency of\nautoregressive sentence-level explanation generation, which leads to\nasynchronous interpretability after prediction. Therefore, we propose a\nnon-autoregressive interpretable model to facilitate parallel explanation\ngeneration and simultaneous prediction. Through extensive experiments on\nNatural Language Inference task and Spouse Prediction task, we find that users\nare able to train classifiers with comparable performance $10-15\\times$ faster\nwith parallel explanation generation using only a few or no annotated training\ndata.", "published": "2023-02-21 14:52:21", "link": "http://arxiv.org/abs/2302.10707v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models", "abstract": "Fine-tuning a language model on a new domain is standard practice for domain\nadaptation. However, it can be infeasible when it comes to modern large-scale\nlanguage models such as GPT-3, which can only be accessed through APIs, making\nit difficult to access the internal parameters of the model. In this paper, we\npropose $k$NN-Adapter, a method to effectively adapt these black-box large\nlanguage models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the\nretrieval-augmented language model, and adaptively learns to interpolate the\noutput of the language model with retrieval results from a datastore consisting\nof the target domain data. Our experiments on four different domains\ndemonstrate that $k$NN-Adapter significantly improves perplexity, and works\nparticularly well in settings with limited access to LLMs. Additionally, we\nshow that $k$NN-Adapter is more effective than fine-tuning when the amount of\ntraining data is limited. We also release a dataset to encourage further study.", "published": "2023-02-21 18:54:21", "link": "http://arxiv.org/abs/2302.10879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Text-to-SQL: An Odyssey into State-of-the-Art and\n  Challenges Ahead", "abstract": "Conversational, multi-turn, text-to-SQL (CoSQL) tasks map natural language\nutterances in a dialogue to SQL queries. State-of-the-art (SOTA) systems use\nlarge, pre-trained and finetuned language models, such as the T5-family, in\nconjunction with constrained decoding. With multi-tasking (MT) over coherent\ntasks with discrete prompts during training, we improve over specialized\ntext-to-SQL T5-family models. Based on Oracle analyses over n-best hypotheses,\nwe apply a query plan model and a schema linking algorithm as rerankers.\nCombining MT and reranking, our results using T5-3B show absolute accuracy\nimprovements of 1.0% in exact match and 3.4% in execution match over a SOTA\nbaseline on CoSQL. While these gains consistently manifest at turn level,\ncontext dependent turns are considerably harder. We conduct studies to tease\napart errors attributable to domain and compositional generalization, with the\nlatter remaining a challenge for multi-turn conversations, especially in\ngenerating SQL with unseen parse trees.", "published": "2023-02-21 23:15:33", "link": "http://arxiv.org/abs/2302.11054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mask-guided BERT for Few Shot Text Classification", "abstract": "Transformer-based language models have achieved significant success in\nvarious domains. However, the data-intensive nature of the transformer\narchitecture requires much labeled data, which is challenging in low-resource\nscenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the\ndifficulty of training robust models on small amounts of samples, which\nfrequently leads to overfitting. Here we present Mask-BERT, a simple and\nmodular framework to help BERT-based architectures tackle FSL. The proposed\napproach fundamentally differs from existing FSL strategies such as prompt\ntuning and meta-learning. The core idea is to selectively apply masks on text\ninputs and filter out irrelevant information, which guides the model to focus\non discriminative tokens that influence prediction results. In addition, to\nmake the text representations from different categories more separable and the\ntext representations from the same category more compact, we introduce a\ncontrastive learning loss function. Experimental results on public-domain\nbenchmark datasets demonstrate the effectiveness of Mask-BERT.", "published": "2023-02-21 05:24:00", "link": "http://arxiv.org/abs/2302.10447v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KG-ECO: Knowledge Graph Enhanced Entity Correction for Query Rewriting", "abstract": "Query Rewriting (QR) plays a critical role in large-scale dialogue systems\nfor reducing frictions. When there is an entity error, it imposes extra\nchallenges for a dialogue system to produce satisfactory responses. In this\nwork, we propose KG-ECO: Knowledge Graph enhanced Entity COrrection for query\nrewriting, an entity correction system with corrupt entity span detection and\nentity retrieval/re-ranking functionalities. To boost the model performance, we\nincorporate Knowledge Graph (KG) to provide entity structural information\n(neighboring entities encoded by graph neural networks) and textual information\n(KG entity descriptions encoded by RoBERTa). Experimental results show that our\napproach yields a clear performance gain over two baselines: utterance level QR\nand entity correction without utilizing KG information. The proposed system is\nparticularly effective for few-shot learning cases where target entities are\nrarely seen in training or there is a KG relation between the target entity and\nother contextual entities in the query.", "published": "2023-02-21 05:42:06", "link": "http://arxiv.org/abs/2302.10454v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Playing the Werewolf game with artificial intelligence for language\n  understanding", "abstract": "The Werewolf game is a social deduction game based on free natural language\ncommunication, in which players try to deceive others in order to survive. An\nimportant feature of this game is that a large portion of the conversations are\nfalse information, and the behavior of artificial intelligence (AI) in such a\nsituation has not been widely investigated. The purpose of this study is to\ndevelop an AI agent that can play Werewolf through natural language\nconversations. First, we collected game logs from 15 human players. Next, we\nfine-tuned a Transformer-based pretrained language model to construct a value\nnetwork that can predict a posterior probability of winning a game at any given\nphase of the game and given a candidate for the next action. We then developed\nan AI agent that can interact with humans and choose the best voting target on\nthe basis of its probability from the value network. Lastly, we evaluated the\nperformance of the agent by having it actually play the game with human\nplayers. We found that our AI agent, Deep Wolf, could play Werewolf as\ncompetitively as average human players in a villager or a betrayer role,\nwhereas Deep Wolf was inferior to human players in a werewolf or a seer role.\nThese results suggest that current language models have the capability to\nsuspect what others are saying, tell a lie, or detect lies in conversations.", "published": "2023-02-21 13:03:20", "link": "http://arxiv.org/abs/2302.10646v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Hyena Hierarchy: Towards Larger Convolutional Language Models", "abstract": "Recent advances in deep learning have relied heavily on the use of large\nTransformers due to their ability to learn at scale. However, the core building\nblock of Transformers, the attention operator, exhibits quadratic cost in\nsequence length, limiting the amount of context accessible. Existing\nsubquadratic methods based on low-rank and sparse approximations need to be\ncombined with dense attention layers to match Transformers, indicating a gap in\ncapability. In this work, we propose Hyena, a subquadratic drop-in replacement\nfor attention constructed by interleaving implicitly parametrized long\nconvolutions and data-controlled gating. In recall and reasoning tasks on\nsequences of thousands to hundreds of thousands of tokens, Hyena improves\naccuracy by more than 50 points over operators relying on state-spaces and\nother implicit and explicit methods, matching attention-based models. We set a\nnew state-of-the-art for dense-attention-free architectures on language\nmodeling in standard datasets (WikiText103 and The Pile), reaching Transformer\nquality with a 20% reduction in training compute required at sequence length\n2K. Hyena operators are twice as fast as highly optimized attention at sequence\nlength 8K, and 100x faster at sequence length 64K.", "published": "2023-02-21 18:29:25", "link": "http://arxiv.org/abs/2302.10866v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "In-context Example Selection with Influences", "abstract": "In-context learning (ICL) is a powerful paradigm emerged from large language\nmodels (LLMs). Despite its promises, ICL performance is known to be highly\nsensitive to input examples. In this work, we use $\\textit{in-context\ninfluences}$ to analyze few-shot ICL performance directly from the in-context\nexamples. Our proposed influence-based example selection method can identify\nboth positive and negative examples, outperforming several baselines when\nevaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\\%$\nperformance gap between using the most negative in-context examples compared to\nthe most positive. In a case study, we apply our influence-based framework to\nquantify the phenomena of recency bias in example ordering for few-shot ICL.", "published": "2023-02-21 22:47:45", "link": "http://arxiv.org/abs/2302.11042v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Co-Driven Recognition of Semantic Consistency via the Fusion of\n  Transformer and HowNet Sememes Knowledge", "abstract": "Semantic consistency recognition aims to detect and judge whether the\nsemantics of two text sentences are consistent with each other. However, the\nexisting methods usually encounter the challenges of synonyms, polysemy and\ndifficulty to understand long text. To solve the above problems, this paper\nproposes a co-driven semantic consistency recognition method based on the\nfusion of Transformer and HowNet sememes knowledge. Multi-level encoding of\ninternal sentence structures via data-driven is carried out firstly by\nTransformer, sememes knowledge base HowNet is introduced for knowledge-driven\nto model the semantic knowledge association among sentence pairs. Then,\ninteractive attention calculation is carried out utilizing soft-attention and\nfusion the knowledge with sememes matrix. Finally, bidirectional long\nshort-term memory network (BiLSTM) is exploited to encode the conceptual\nsemantic information and infer the semantic consistency. Experiments are\nconducted on two financial text matching datasets (BQ, AFQMC) and a\ncross-lingual adversarial dataset (PAWSX) for paraphrase identification.\nCompared with lightweight models including DSSM, MwAN, DRCN, and pre-training\nmodels such as ERNIE etc., the proposed model can not only improve the accuracy\nof semantic consistency recognition effectively (by 2.19%, 5.57% and 6.51%\ncompared with the DSSM, MWAN and DRCN models on the BQ dataset), but also\nreduce the number of model parameters (to about 16M). In addition, driven by\nthe HowNet sememes knowledge, the proposed method is promising to adapt to\nscenarios with long text.", "published": "2023-02-21 09:53:19", "link": "http://arxiv.org/abs/2302.10570v1", "categories": ["cs.CL", "cs.AI", "cs.SC"], "primary_category": "cs.CL"}
{"title": "ChatGPT: Jack of all trades, master of none", "abstract": "OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and\nrevolutionized the approach in artificial intelligence to human-model\ninteraction. Several publications on ChatGPT evaluation test its effectiveness\non well-known natural language processing (NLP) tasks. However, the existing\nstudies are mostly non-automated and tested on a very limited scale. In this\nwork, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,\nmost of them subjective even to humans, such as sentiment analysis, emotion\nrecognition, offensiveness, and stance detection. In contrast, the other tasks\nrequire more objective reasoning like word sense disambiguation, linguistic\nacceptability, and question answering. We also evaluated GPT-4 model on five\nselected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process\nand analyzed more than 49k responses. Our comparison of its results with\navailable State-of-the-Art (SOTA) solutions showed that the average loss in\nquality of the ChatGPT model was about 25% for zero-shot and few-shot\nevaluation. For GPT-4 model, a loss for semantic tasks is significantly lower\nthan for ChatGPT. We showed that the more difficult the task (lower SOTA\nperformance), the higher the ChatGPT loss. It especially refers to pragmatic\nNLP problems like emotion recognition. We also tested the ability to\npersonalize ChatGPT responses for selected subjective tasks via Random\nContextual Few-Shot Personalization, and we obtained significantly better\nuser-based predictions. Additional qualitative analysis revealed a ChatGPT\nbias, most likely due to the rules imposed on human trainers by OpenAI. Our\nresults provide the basis for a fundamental discussion of whether the high\nquality of recent predictive NLP models can indicate a tool's usefulness to\nsociety and how the learning and validation procedures for such systems should\nbe established.", "published": "2023-02-21 15:20:37", "link": "http://arxiv.org/abs/2302.10724v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Real-World Deployment and Evaluation of Kwame for Science, An AI\n  Teaching Assistant for Science Education in West Africa", "abstract": "Africa has a high student-to-teacher ratio which limits students' access to\nteachers for learning support such as educational question answering. In this\nwork, we extended Kwame, a bilingual AI teaching assistant for coding\neducation, adapted it for science education, and deployed it as a web app.\nKwame for Science provides passages from well-curated knowledge sources and\nrelated past national exam questions as answers to questions from students\nbased on the Integrated Science subject of the West African Senior Secondary\nCertificate Examination (WASSCE). Furthermore, students can view past national\nexam questions along with their answers and filter by year, question type, and\ntopics that were automatically categorized by a topic detection model which we\ndeveloped (91% unweighted average recall). We deployed Kwame for Science in the\nreal world over 8 months and had 750 users across 32 countries (15 in Africa)\nand 1.5K questions asked. Our evaluation showed an 87.2% top 3 accuracy (n=109\nquestions) implying that Kwame for Science has a high chance of giving at least\none useful answer among the 3 displayed. We categorized the reasons the model\nincorrectly answered questions to provide insights for future improvements. We\nalso share challenges and lessons with the development, deployment, and\nhuman-computer interaction component of such a tool to enable other researchers\nto deploy similar tools. With a first-of-its-kind tool within the African\ncontext, Kwame for Science has the potential to enable the delivery of\nscalable, cost-effective, and quality remote education to millions of people\nacross Africa.", "published": "2023-02-21 16:20:17", "link": "http://arxiv.org/abs/2302.10786v3", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "TherapyView: Visualizing Therapy Sessions with Temporal Topic Modeling\n  and AI-Generated Arts", "abstract": "We present the TherapyView, a demonstration system to help therapists\nvisualize the dynamic contents of past treatment sessions, enabled by the\nstate-of-the-art neural topic modeling techniques to analyze the topical\ntendencies of various psychiatric conditions and deep learning-based image\ngeneration engine to provide a visual summary. The system incorporates temporal\nmodeling to provide a time-series representation of topic similarities at a\nturn-level resolution and AI-generated artworks given the dialogue segments to\nprovide a concise representations of the contents covered in the session,\noffering interpretable insights for therapists to optimize their strategies and\nenhance the effectiveness of psychotherapy. This system provides a proof of\nconcept of AI-augmented therapy tools with e in-depth understanding of the\npatient's mental state and enabling more effective treatment.", "published": "2023-02-21 17:53:45", "link": "http://arxiv.org/abs/2302.10845v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management", "abstract": "Reinforcement learning (RL) has shown great promise for developing dialogue\nmanagement (DM) agents that are non-myopic, conduct rich conversations, and\nmaximize overall user satisfaction. Despite recent developments in RL and\nlanguage models (LMs), using RL to power conversational chatbots remains\nchallenging, in part because RL requires online exploration to learn\neffectively, whereas collecting novel human-bot interactions can be expensive\nand unsafe. This issue is exacerbated by the combinatorial action spaces facing\nthese algorithms, as most LM agents generate responses at the word level. We\ndevelop a variety of RL algorithms, specialized to dialogue planning, that\nleverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that\ncapture diverse semantics, generate utterances reflecting different intents,\nand are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods\nsignificantly reduce the size of the action space and improve the efficacy of\nRL-based DM. We evaluate our methods in open-domain dialogue to demonstrate\ntheir effectiveness w.r.t.\\ the diversity of intent in generated utterances and\noverall DM performance.", "published": "2023-02-21 18:02:20", "link": "http://arxiv.org/abs/2302.10850v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient CTC Regularization via Coarse Labels for End-to-End Speech\n  Translation", "abstract": "For end-to-end speech translation, regularizing the encoder with the\nConnectionist Temporal Classification (CTC) objective using the source\ntranscript or target translation as labels can greatly improve quality metrics.\nHowever, CTC demands an extra prediction layer over the vocabulary space,\nbringing in nonnegligible model parameters and computational overheads,\nalthough this layer is typically not used for inference. In this paper, we\nre-examine the need for genuine vocabulary labels for CTC for regularization\nand explore strategies to reduce the CTC label space, targeting improved\nefficiency without quality degradation. We propose coarse labeling for CTC\n(CoLaCTC), which merges vocabulary labels via simple heuristic rules, such as\nusing truncation, division or modulo (MOD) operations. Despite its simplicity,\nour experiments on 4 source and 8 target languages show that CoLaCTC with MOD\nparticularly can compress the label space aggressively to 256 and even further,\ngaining training efficiency (1.18x ~ 1.77x speedup depending on the original\nvocabulary size) yet still delivering comparable or better performance than the\nCTC baseline. We also show that CoLaCTC successfully generalizes to CTC\nregularization regardless of using transcript or translation for labeling.", "published": "2023-02-21 18:38:41", "link": "http://arxiv.org/abs/2302.10871v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning to Retrieve Engaging Follow-Up Queries", "abstract": "Open domain conversational agents can answer a broad range of targeted\nqueries. However, the sequential nature of interaction with these systems makes\nknowledge exploration a lengthy task which burdens the user with asking a chain\nof well phrased questions. In this paper, we present a retrieval based system\nand associated dataset for predicting the next questions that the user might\nhave. Such a system can proactively assist users in knowledge exploration\nleading to a more engaging dialog. The retrieval system is trained on a dataset\nwhich contains ~14K multi-turn information-seeking conversations with a valid\nfollow-up question and a set of invalid candidates. The invalid candidates are\ngenerated to simulate various syntactic and semantic confounders such as\nparaphrases, partial entity match, irrelevant entity, and ASR errors. We use\nconfounder specific techniques to simulate these negative examples on the\nOR-QuAC dataset and develop a dataset called the Follow-up Query Bank\n(FQ-Bank). Then, we train ranking models on FQ-Bank and present results\ncomparing supervised and unsupervised approaches. The results suggest that we\ncan retrieve the valid follow-ups by ranking them in higher positions compared\nto confounders, but further knowledge grounding can improve ranking\nperformance.", "published": "2023-02-21 20:26:23", "link": "http://arxiv.org/abs/2302.10978v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Edgeformers: Graph-Empowered Transformers for Representation Learning on\n  Textual-Edge Networks", "abstract": "Edges in many real-world social/information networks are associated with rich\ntext information (e.g., user-user communications or user-product reviews).\nHowever, mainstream network representation learning models focus on propagating\nand aggregating node attributes, lacking specific designs to utilize text\nsemantics on edges. While there exist edge-aware graph neural networks, they\ndirectly initialize edge attributes as a feature vector, which cannot fully\ncapture the contextualized text semantics of edges. In this paper, we propose\nEdgeformers, a framework built upon graph-enhanced Transformers, to perform\nedge and node representation learning by modeling texts on edges in a\ncontextualized way. Specifically, in edge representation learning, we inject\nnetwork information into each Transformer layer when encoding edge texts; in\nnode representation learning, we aggregate edge representations through an\nattention mechanism within each node's ego-graph. On five public datasets from\nthree different domains, Edgeformers consistently outperform state-of-the-art\nbaselines in edge classification and link prediction, demonstrating the\nefficacy in learning edge and node representations, respectively.", "published": "2023-02-21 23:09:17", "link": "http://arxiv.org/abs/2302.11050v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Time to Embrace Natural Language Processing (NLP)-based Digital\n  Pathology: Benchmarking NLP- and Convolutional Neural Network-based Deep\n  Learning Pipelines", "abstract": "NLP-based computer vision models, particularly vision transformers, have been\nshown to outperform CNN models in many imaging tasks. However, most digital\npathology artificial-intelligence models are based on CNN architectures,\nprobably owing to a lack of data regarding NLP models for pathology images. In\nthis study, we developed digital pathology pipelines to benchmark the five most\nrecently proposed NLP models (vision transformer (ViT), Swin Transformer,\nMobileViT, CMT, and Sequencer2D) and four popular CNN models (ResNet18,\nResNet50, MobileNetV2, and EfficientNet) to predict biomarkers in colorectal\ncancer (microsatellite instability, CpG island methylator phenotype, and BRAF\nmutation). Hematoxylin and eosin-stained whole-slide images from Molecular and\nCellular Oncology and The Cancer Genome Atlas were used as training and\nexternal validation datasets, respectively. Cross-study external validations\nrevealed that the NLP-based models significantly outperformed the CNN-based\nmodels in biomarker prediction tasks, improving the overall prediction and\nprecision up to approximately 10% and 26%, respectively. Notably, compared with\nexisting models in the current literature using large training datasets, our\nNLP models achieved state-of-the-art predictions for all three biomarkers using\na relatively small training dataset, suggesting that large training datasets\nare not a prerequisite for NLP models or transformers, and NLP may be more\nsuitable for clinical studies in which small training datasets are commonly\ncollected. The superior performance of Sequencer2D suggests that further\nresearch and innovation on both transformer and bidirectional long short-term\nmemory architectures are warranted in the field of digital pathology. NLP\nmodels can replace classic CNN architectures and become the new workhorse\nbackbone in the field of digital pathology.", "published": "2023-02-21 02:42:03", "link": "http://arxiv.org/abs/2302.10406v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "eess.IV", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Real-time speech enhancement with dynamic attention span", "abstract": "For real-time speech enhancement (SE) including noise suppression,\ndereverberation and acoustic echo cancellation, the time-variance of the audio\nsignals becomes a severe challenge. The causality and memory usage limit that\nonly the historical information can be used for the system to capture the\ntime-variant characteristics. We propose to adaptively change the receptive\nfield according to the input signal in deep neural network based SE model.\nSpecifically, in an encoder-decoder framework, a dynamic attention span\nmechanism is introduced to all the attention modules for controlling the size\nof historical content used for processing the current frame. Experimental\nresults verify that this dynamic mechanism can better track time-variant\nfactors and capture speech-related characteristics, benefiting to both\ninterference removing and speech quality retaining.", "published": "2023-02-21 00:53:34", "link": "http://arxiv.org/abs/2302.10377v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging phone-level linguistic-acoustic similarity for\n  utterance-level pronunciation scoring", "abstract": "Recent studies on pronunciation scoring have explored the effect of\nintroducing phone embeddings as reference pronunciation, but mostly in an\nimplicit manner, i.e., addition or concatenation of reference phone embedding\nand actual pronunciation of the target phone as the phone-level pronunciation\nquality representation. In this paper, we propose to use linguistic-acoustic\nsimilarity to explicitly measure the deviation of non-native production from\nits native reference for pronunciation assessment. Specifically, the deviation\nis first estimated by the cosine similarity between reference phone embedding\nand corresponding acoustic embedding. Next, a phone-level Goodness of\npronunciation (GOP) pre-training stage is introduced to guide this\nsimilarity-based learning for better initialization of the aforementioned two\nembeddings. Finally, a transformer-based hierarchical pronunciation scorer is\nused to map a sequence of phone embeddings, acoustic embeddings along with\ntheir similarity measures to predict the final utterance-level score.\nExperimental results on the non-native databases suggest that the proposed\nsystem significantly outperforms the baselines, where the acoustic and phone\nembeddings are simply added or concatenated. A further examination shows that\nthe phone embeddings learned in the proposed approach are able to capture\nlinguistic-acoustic attributes of native pronunciation as reference.", "published": "2023-02-21 05:14:19", "link": "http://arxiv.org/abs/2302.10444v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Nonparallel Emotional Voice Conversion For Unseen Speaker-Emotion Pairs\n  Using Dual Domain Adversarial Network & Virtual Domain Pairing", "abstract": "Primary goal of an emotional voice conversion (EVC) system is to convert the\nemotion of a given speech signal from one style to another style without\nmodifying the linguistic content of the signal. Most of the state-of-the-art\napproaches convert emotions for seen speaker-emotion combinations only. In this\npaper, we tackle the problem of converting the emotion of speakers whose only\nneutral data are present during the time of training and testing (i.e., unseen\nspeaker-emotion combinations). To this end, we extend a recently proposed\nStartGANv2-VC architecture by utilizing dual encoders for learning the speaker\nand emotion style embeddings separately along with dual domain source\nclassifiers. For achieving the conversion to unseen speaker-emotion\ncombinations, we propose a Virtual Domain Pairing (VDP) training strategy,\nwhich virtually incorporates the speaker-emotion pairs that are not present in\nthe real data without compromising the min-max game of a discriminator and\ngenerator in adversarial training. We evaluate the proposed method using a\nHindi emotional database.", "published": "2023-02-21 09:06:52", "link": "http://arxiv.org/abs/2302.10536v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DasFormer: Deep Alternating Spectrogram Transformer for\n  Multi/Single-Channel Speech Separation", "abstract": "For the task of speech separation, previous study usually treats\nmulti-channel and single-channel scenarios as two research tracks with\nspecialized solutions developed respectively. Instead, we propose a simple and\nunified architecture - DasFormer (Deep alternating spectrogram transFormer) to\nhandle both of them in the challenging reverberant environments. Unlike\nframe-wise sequence modeling, each TF-bin in the spectrogram is assigned with\nan embedding encoding spectral and spatial information. With such input,\nDasFormer is then formed by multiple repetition of simple blocks each of which\nintegrates 1) two multi-head self-attention (MHSA) modules alternately\nprocessing within each frequency bin & temporal frame of the spectrogram 2)\nMBConv before each MHSA for modeling local features on the spectrogram.\nExperiments show that DasFormer has a powerful ability to model the\ntime-frequency representation, whose performance far exceeds the current SOTA\nmodels in multi-channel speech separation, and also achieves single-channel\nSOTA in the more challenging yet realistic reverberation scenario.", "published": "2023-02-21 13:19:19", "link": "http://arxiv.org/abs/2302.10657v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpretable Spectrum Transformation Attacks to Speaker Recognition", "abstract": "The success of adversarial attacks to speaker recognition is mainly in\nwhite-box scenarios. When applying the adversarial voices that are generated by\nattacking white-box surrogate models to black-box victim models, i.e.\n\\textit{transfer-based} black-box attacks, the transferability of the\nadversarial voices is not only far from satisfactory, but also lacks\ninterpretable basis. To address these issues, in this paper, we propose a\ngeneral framework, named spectral transformation attack based on modified\ndiscrete cosine transform (STA-MDCT), to improve the transferability of the\nadversarial voices to a black-box victim model. Specifically, we first apply\nMDCT to the input voice. Then, we slightly modify the energy of different\nfrequency bands for capturing the salient regions of the adversarial noise in\nthe time-frequency domain that are critical to a successful attack. Unlike\nexisting approaches that operate voices in the time domain, the proposed\nframework operates voices in the time-frequency domain, which improves the\ninterpretability, transferability, and imperceptibility of the attack.\nMoreover, it can be implemented with any gradient-based attackers. To utilize\nthe advantage of model ensembling, we not only implement STA-MDCT with a single\nwhite-box surrogate model, but also with an ensemble of surrogate models.\nFinally, we visualize the saliency maps of adversarial voices by the class\nactivation maps (CAM), which offers an interpretable basis to transfer-based\nattacks in speaker recognition for the first time. Extensive comparison results\nwith five representative attackers show that the CAM visualization clearly\nexplains the effectiveness of STA-MDCT, and the weaknesses of the comparison\nmethods; the proposed method outperforms the comparison methods by a large\nmargin.", "published": "2023-02-21 14:12:29", "link": "http://arxiv.org/abs/2302.10686v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advancing Stuttering Detection via Data Augmentation, Class-Balanced\n  Loss and Multi-Contextual Deep Learning", "abstract": "Stuttering is a neuro-developmental speech impairment characterized by\nuncontrolled utterances (interjections) and core behaviors (blocks,\nrepetitions, and prolongations), and is caused by the failure of speech\nsensorimotors. Due to its complex nature, stuttering detection (SD) is a\ndifficult task. If detected at an early stage, it could facilitate speech\ntherapists to observe and rectify the speech patterns of persons who stutter\n(PWS). The stuttered speech of PWS is usually available in limited amounts and\nis highly imbalanced. To this end, we address the class imbalance problem in\nthe SD domain via a multibranching (MB) scheme and by weighting the\ncontribution of classes in the overall loss function, resulting in a huge\nimprovement in stuttering classes on the SEP-28k dataset over the baseline\n(StutterNet). To tackle data scarcity, we investigate the effectiveness of data\naugmentation on top of a multi-branched training scheme. The augmented training\noutperforms the MB StutterNet (clean) by a relative margin of 4.18% in macro\nF1-score (F1). In addition, we propose a multi-contextual (MC) StutterNet,\nwhich exploits different contexts of the stuttered speech, resulting in an\noverall improvement of 4.48% in F 1 over the single context based MB\nStutterNet. Finally, we have shown that applying data augmentation in the\ncross-corpora scenario can improve the overall SD performance by a relative\nmargin of 13.23% in F1 over the clean training.", "published": "2023-02-21 14:03:47", "link": "http://arxiv.org/abs/2302.11343v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Reinforcement Learning Framework for Online Speaker Diarization", "abstract": "Speaker diarization is a task to label an audio or video recording with the\nidentity of the speaker at each given time stamp. In this work, we propose a\nnovel machine learning framework to conduct real-time multi-speaker diarization\nand recognition without prior registration and pretraining in a fully online\nand reinforcement learning setting. Our framework combines embedding\nextraction, clustering, and resegmentation into the same problem as an online\ndecision-making problem. We discuss practical considerations and advanced\ntechniques such as the offline reinforcement learning, semi-supervision, and\ndomain adaptation to address the challenges of limited training data and\nout-of-distribution environments. Our approach considers speaker diarization as\na fully online learning problem of the speaker recognition task, where the\nagent receives no pretraining from any training set before deployment, and\nlearns to detect speaker identity on the fly through reward feedbacks. The\nparadigm of the reinforcement learning approach to speaker diarization presents\nan adaptive, lightweight, and generalizable system that is useful for\nmulti-user teleconferences, where many people might come and go without\nextensive pre-registration ahead of time. Lastly, we provide a desktop\napplication that uses our proposed approach as a proof of concept. To the best\nof our knowledge, this is the first approach to apply a reinforcement learning\napproach to the speaker diarization task.", "published": "2023-02-21 15:42:25", "link": "http://arxiv.org/abs/2302.10924v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
