{"title": "Can neural networks acquire a structural bias from raw linguistic data?", "abstract": "We evaluate whether BERT, a widely used neural network for sentence\nprocessing, acquires an inductive bias towards forming structural\ngeneralizations through pretraining on raw data. We conduct four experiments\ntesting its preference for structural vs. linear generalizations in different\nstructure-dependent phenomena. We find that BERT makes a structural\ngeneralization in 3 out of 4 empirical domains---subject-auxiliary inversion,\nreflexive binding, and verb tense detection in embedded clauses---but makes a\nlinear generalization when tested on NPI licensing. We argue that these results\nare the strongest evidence so far from artificial learners supporting the\nproposition that a structural bias can be acquired from raw data. If this\nconclusion is correct, it is tentative evidence that some linguistic universals\ncan be acquired by learners without innate biases. However, the precise\nimplications for human language acquisition are unclear, as humans learn\nlanguage from significantly less data than BERT.", "published": "2020-07-14 01:41:25", "link": "http://arxiv.org/abs/2007.06761v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emoji Prediction: Extensions and Benchmarking", "abstract": "Emojis are a succinct form of language which can express concrete meanings,\nemotions, and intentions. Emojis also carry signals that can be used to better\nunderstand communicative intent. They have become a ubiquitous part of our\ndaily lives, making them an important part of understanding user-generated\ncontent. The emoji prediction task aims at predicting the proper set of emojis\nassociated with a piece of text. Through emoji prediction, models can learn\nrich representations of the communicative intent of the written text. While\nexisting research on the emoji prediction task focus on a small subset of emoji\ntypes closely related to certain emotions, this setting oversimplifies the task\nand wastes the expressive power of emojis. In this paper, we extend the\nexisting setting of the emoji prediction task to include a richer set of emojis\nand to allow multi-label classification on the task. We propose novel models\nfor multi-class and multi-label emoji prediction based on Transformer networks.\nWe also construct multiple emoji prediction datasets from Twitter using\nheuristics. The BERT models achieve state-of-the-art performances on all our\ndatasets under all the settings, with relative improvements of 27.21% to\n236.36% in accuracy, 2.01% to 88.28% in top-5 accuracy and 65.19% to 346.79% in\nF-1 score, compared to the prior state-of-the-art. Our results demonstrate the\nefficacy of deep Transformer-based models on the emoji prediction task. We also\nrelease our datasets at\nhttps://github.com/hikari-NYU/Emoji_Prediction_Datasets_MMS for future\nresearchers.", "published": "2020-07-14 22:41:20", "link": "http://arxiv.org/abs/2007.07389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Coherency in Generated Emails by Leveraging Deep Neural\n  Learners", "abstract": "Advanced machine learning and natural language techniques enable attackers to\nlaunch sophisticated and targeted social engineering-based attacks. To counter\nthe active attacker issue, researchers have since resorted to proactive methods\nof detection. Email masquerading using targeted emails to fool the victim is an\nadvanced attack method. However automatic text generation requires controlling\nthe context and coherency of the generated content, which has been identified\nas an increasingly difficult problem. The method used leverages a hierarchical\ndeep neural model which uses a learned representation of the sentences in the\ninput document to generate structured written emails. We demonstrate the\ngeneration of short and targeted text messages using the deep model. The global\ncoherency of the synthesized text is evaluated using a qualitative study as\nwell as multiple quantitative measures.", "published": "2020-07-14 23:47:08", "link": "http://arxiv.org/abs/2007.07403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Robustness to Spurious Correlations using\n  Pre-trained Language Models", "abstract": "Recent work has shown that pre-trained language models such as BERT improve\nrobustness to spurious correlations in the dataset. Intrigued by these results,\nwe find that the key to their success is generalization from a small amount of\ncounterexamples where the spurious correlations do not hold. When such minority\nexamples are scarce, pre-trained models perform as poorly as models trained\nfrom scratch. In the case of extreme minority, we propose to use multi-task\nlearning (MTL) to improve generalization. Our experiments on natural language\ninference and paraphrase identification show that MTL with the right auxiliary\ntasks significantly improves performance on challenging examples without\nhurting the in-distribution performance. Further, we show that the gain from\nMTL mainly comes from improved generalization from the minority examples. Our\nresults highlight the importance of data diversity for overcoming spurious\ncorrelations.", "published": "2020-07-14 02:34:59", "link": "http://arxiv.org/abs/2007.06778v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring\n  Systems", "abstract": "Automatic scoring engines have been used for scoring approximately fifteen\nmillion test-takers in just the last three years. This number is increasing\nfurther due to COVID-19 and the associated automation of education and testing.\nDespite such wide usage, the AI-based testing literature of these \"intelligent\"\nmodels is highly lacking. Most of the papers proposing new models rely only on\nquadratic weighted kappa (QWK) based agreement with human raters for showing\nmodel efficacy. However, this effectively ignores the highly multi-feature\nnature of essay scoring. Essay scoring depends on features like coherence,\ngrammar, relevance, sufficiency and, vocabulary. To date, there has been no\nstudy testing Automated Essay Scoring: AES systems holistically on all these\nfeatures. With this motivation, we propose a model agnostic adversarial\nevaluation scheme and associated metrics for AES systems to test their natural\nlanguage understanding capabilities and overall robustness. We evaluate the\ncurrent state-of-the-art AES models using the proposed scheme and report the\nresults on five recent models. These models range from\nfeature-engineering-based approaches to the latest deep learning algorithms. We\nfind that AES models are highly overstable. Even heavy modifications(as much as\n25%) with content unrelated to the topic of the questions do not decrease the\nscore produced by the models. On the other hand, irrelevant content, on\naverage, increases the scores, thus showing that the model evaluation strategy\nand rubrics should be reconsidered. We also ask 200 human raters to score both\nan original and adversarial response to seeing if humans can detect differences\nbetween the two and whether they agree with the scores assigned by auto scores.", "published": "2020-07-14 03:49:43", "link": "http://arxiv.org/abs/2007.06796v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What's in a Name? Are BERT Named Entity Representations just as Good for\n  any other Name?", "abstract": "We evaluate named entity representations of BERT-based NLP models by\ninvestigating their robustness to replacements from the same typed class in the\ninput. We highlight that on several tasks while such perturbations are natural,\nstate of the art trained models are surprisingly brittle. The brittleness\ncontinues even with the recent entity-aware BERT models. We also try to discern\nthe cause of this non-robustness, considering factors such as tokenization and\nfrequency of occurrence. Then we provide a simple method that ensembles\npredictions from multiple replacements while jointly modeling the uncertainty\nof type annotations and label predictions. Experiments on three NLP tasks show\nthat our method enhances robustness and increases accuracy on both natural and\nadversarial datasets.", "published": "2020-07-14 08:14:00", "link": "http://arxiv.org/abs/2007.06897v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language, communication and society: a gender based linguistics analysis", "abstract": "The purpose of this study is to find evidence for supporting the hypothesis\nthat language is the mirror of our thinking, our prejudices and cultural\nstereotypes. In this analysis, a questionnaire was administered to 537 people.\nThe answers have been analysed to see if gender stereotypes were present such\nas the attribution of psychological and behavioural characteristics. In\nparticular, the aim was to identify, if any, what are the stereotyped images,\nwhich emerge in defining the roles of men and women in modern society.\nMoreover, the results given can be a good starting point to understand if\ngender stereotypes, and the expectations they produce, can result in\npenalization or inequality. If so, the language and its use would create\ninherently a gender bias, which influences evaluations both in work settings\nboth in everyday life.", "published": "2020-07-14 08:38:37", "link": "http://arxiv.org/abs/2007.06908v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Questionnaire analysis to define the most suitable survey for port-noise\n  investigation", "abstract": "The high level of noise pollution affecting the areas between ports and\nlogistic platforms represents a problem that can be faced from different points\nof view. Acoustic monitoring, mapping, short-term measurements, port and road\ntraffic flows analyses can give useful indications on the strategies to be\nproposed for a better management of the problem. A survey campaign through the\npreparation of questionnaires to be submitted to the population exposed to\nnoise in the back-port areas will help to better understand the subjective\npoint of view. The paper analyses a sample of questions suitable for the\nspecific research, chosen as part of the wide database of questionnaires\ninternationally proposed for subjective investigations. The preliminary results\nof a first data collection campaign are considered to verify the adequacy of\nthe number, the type of questions, and the type of sample noise used for the\nsurvey. The questionnaire will be optimized to be distributed in the TRIPLO\nproject (TRansports and Innovative sustainable connections between Ports and\nLOgistic platforms). The results of this survey will be the starting point for\nthe linguistic investigation carried out in combination with the acoustic\nmonitoring, to improve understanding the connections between personal feeling\nand technical aspects.", "published": "2020-07-14 08:52:55", "link": "http://arxiv.org/abs/2007.06915v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Modeling Voting for System Combination in Machine Translation", "abstract": "System combination is an important technique for combining the hypotheses of\ndifferent machine translation systems to improve translation performance.\nAlthough early statistical approaches to system combination have been proven\neffective in analyzing the consensus between hypotheses, they suffer from the\nerror propagation problem due to the use of pipelines. While this problem has\nbeen alleviated by end-to-end training of multi-source sequence-to-sequence\nmodels recently, these neural models do not explicitly analyze the relations\nbetween hypotheses and fail to capture their agreement because the attention to\na word in a hypothesis is calculated independently, ignoring the fact that the\nword might occur in multiple hypotheses. In this work, we propose an approach\nto modeling voting for system combination in machine translation. The basic\nidea is to enable words in hypotheses from different systems to vote on words\nthat are representative and should get involved in the generation process. This\ncan be done by quantifying the influence of each voter and its preference for\neach candidate. Our approach combines the advantages of statistical and neural\nmethods since it can not only analyze the relations between hypotheses but also\nallow for end-to-end training. Experiments show that our approach is capable of\nbetter taking advantage of the consensus between hypotheses and achieves\nsignificant improvements over state-of-the-art baselines on Chinese-English and\nEnglish-German machine translation tasks.", "published": "2020-07-14 09:59:38", "link": "http://arxiv.org/abs/2007.06943v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Transformer based Data Augmentation with Subword Units for\n  Morphologically Rich Online ASR", "abstract": "Recently Deep Transformer models have proven to be particularly powerful in\nlanguage modeling tasks for ASR. Their high complexity, however, makes them\nvery difficult to apply in the first (single) pass of an online system. Recent\nstudies showed that a considerable part of the knowledge of neural network\nLanguage Models (LM) can be transferred to traditional n-grams by using neural\ntext generation based data augmentation. In our paper, we pre-train a GPT-2\nTransformer LM on a general text corpus and fine-tune it on our Hungarian\nconversational call center ASR task. We show that although data augmentation\nwith Transformer-generated text works well for isolating languages, it causes a\nvocabulary explosion in a morphologically rich language. Therefore, we propose\na new method called subword-based neural text augmentation, where we retokenize\nthe generated text into statistically derived subwords. We compare Morfessor\nand BPE statistical subword tokenizers and show that both methods can\nsignificantly improve the WER while greatly reducing vocabulary size and memory\nrequirements. Finally, we also demonstrate that subword-based neural text\naugmentation outperforms the word-based approach not only in terms of overall\nWER but also in recognition of OOV words.", "published": "2020-07-14 10:22:05", "link": "http://arxiv.org/abs/2007.06949v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions\n  Attributes", "abstract": "This paper describes a large global dataset on people's discourse and\nresponses to the COVID-19 pandemic over the Twitter platform. From 28 January\n2020 to 1 June 2022, we collected and processed over 252 million Twitter posts\nfrom more than 29 million unique users using four keywords: \"corona\", \"wuhan\",\n\"nCov\" and \"covid\". Leveraging probabilistic topic modelling and pre-trained\nmachine learning-based emotion recognition algorithms, we labelled each tweet\nwith seventeen attributes, including a) ten binary attributes indicating the\ntweet's relevance (1) or irrelevance (0) to the top ten detected topics, b)\nfive quantitative emotion attributes indicating the degree of intensity of the\nvalence or sentiment (from 0: extremely negative to 1: extremely positive) and\nthe degree of intensity of fear, anger, sadness and happiness emotions (from 0:\nnot at all to 1: extremely intense), and c) two categorical attributes\nindicating the sentiment (very negative, negative, neutral or mixed, positive,\nvery positive) and the dominant emotion (fear, anger, sadness, happiness, no\nspecific emotion) the tweet is mainly expressing. We discuss the technical\nvalidity and report the descriptive statistics of these attributes, their\ntemporal distribution, and geographic representation. The paper concludes with\na discussion of the dataset's usage in communication, psychology, public\nhealth, economics, and epidemiology.", "published": "2020-07-14 10:30:47", "link": "http://arxiv.org/abs/2007.06954v8", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Using Holographically Compressed Embeddings in Question Answering", "abstract": "Word vector representations are central to deep learning natural language\nprocessing models. Many forms of these vectors, known as embeddings, exist,\nincluding word2vec and GloVe. Embeddings are trained on large corpora and learn\nthe word's usage in context, capturing the semantic relationship between words.\nHowever, the semantics from such training are at the level of distinct words\n(known as word types), and can be ambiguous when, for example, a word type can\nbe either a noun or a verb. In question answering, parts-of-speech and named\nentity types are important, but encoding these attributes in neural models\nexpands the size of the input. This research employs holographic compression of\npre-trained embeddings, to represent a token, its part-of-speech, and named\nentity type, in the same dimension as representing only the token. The\nimplementation, in a modified question answering recurrent deep learning\nnetwork, shows that semantic relationships are preserved, and yields strong\nperformance.", "published": "2020-07-14 18:29:49", "link": "http://arxiv.org/abs/2007.07287v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep learning models for representing out-of-vocabulary words", "abstract": "Communication has become increasingly dynamic with the popularization of\nsocial networks and applications that allow people to express themselves and\ncommunicate instantly. In this scenario, distributed representation models have\ntheir quality impacted by new words that appear frequently or that are derived\nfrom spelling errors. These words that are unknown by the models, known as\nout-of-vocabulary (OOV) words, need to be properly handled to not degrade the\nquality of the natural language processing (NLP) applications, which depend on\nthe appropriate vector representation of the texts. To better understand this\nproblem and finding the best techniques to handle OOV words, in this study, we\npresent a comprehensive performance evaluation of deep learning models for\nrepresenting OOV words. We performed an intrinsic evaluation using a benchmark\ndataset and an extrinsic evaluation using different NLP tasks: text\ncategorization, named entity recognition, and part-of-speech tagging. Although\nthe results indicated that the best technique for handling OOV words is\ndifferent for each task, Comick, a deep learning method that infers the\nembedding based on the context and the morphological structure of the OOV word,\nobtained promising results.", "published": "2020-07-14 19:31:25", "link": "http://arxiv.org/abs/2007.07318v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Intelligent requirements engineering from natural language and their\n  chaining toward CAD models", "abstract": "This paper assumes that design language plays an important role in how\ndesigners design and on the creativity of designers. Designers use and develop\nmodels as an aid to thinking, a focus for discussion and decision-making and a\nmeans of evaluating the reliability of the proposals. This paper proposes an\nintelligent method for requirements engineering from natural language and their\nchaining toward CAD models. The transition from linguistic analysis to the\nrepresentation of engineering requirements consists of the translation of the\nsyntactic structure into semantic form represented by conceptual graphs. Based\non the isomorphism between conceptual graphs and predicate logic, a formal\nlanguage of the specification is proposed. The outcome of this language is\nchained and translated in Computer Aided Three-Dimensional Interactive\nApplication (CATIA) models. The tool (EGEON: Engineering desiGn sEmantics\nelabOration and applicatioN) is developed to represent the semantic network of\nengineering requirements. A case study on the design of a car door hinge is\npresented to illustrates the proposed method.", "published": "2020-07-14 17:53:01", "link": "http://arxiv.org/abs/2007.07825v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Covidex: Neural Ranking Models and Keyword Search Infrastructure for the\n  COVID-19 Open Research Dataset", "abstract": "We present Covidex, a search engine that exploits the latest neural ranking\nmodels to provide information access to the COVID-19 Open Research Dataset\ncurated by the Allen Institute for AI. Our system has been online and serving\nusers since late March 2020. The Covidex is the user application component of\nour three-pronged strategy to develop technologies for helping domain experts\ntackle the ongoing global pandemic. In addition, we provide robust and\neasy-to-use keyword search infrastructure that exploits mature fusion-based\nmethods as well as standalone neural ranking models that can be incorporated\ninto other applications. These techniques have been evaluated in the ongoing\nTREC-COVID challenge: Our infrastructure and baselines have been adopted by\nmany participants, including some of the highest-scoring runs in rounds 1, 2,\nand 3. In round 3, we report the highest-scoring run that takes advantage of\nprevious training data and the second-highest fully automatic run.", "published": "2020-07-14 16:26:01", "link": "http://arxiv.org/abs/2007.07846v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Compare and Reweight: Distinctive Image Captioning Using Similar Images\n  Sets", "abstract": "A wide range of image captioning models has been developed, achieving\nsignificant improvement based on popular metrics, such as BLEU, CIDEr, and\nSPICE. However, although the generated captions can accurately describe the\nimage, they are generic for similar images and lack distinctiveness, i.e.,\ncannot properly describe the uniqueness of each image. In this paper, we aim to\nimprove the distinctiveness of image captions through training with sets of\nsimilar images. First, we propose a distinctiveness metric -- between-set CIDEr\n(CIDErBtw) to evaluate the distinctiveness of a caption with respect to those\nof similar images. Our metric shows that the human annotations of each image\nare not equivalent based on distinctiveness. Thus we propose several new\ntraining strategies to encourage the distinctiveness of the generated caption\nfor each image, which are based on using CIDErBtw in a weighted loss function\nor as a reinforcement learning reward. Finally, extensive experiments are\nconducted, showing that our proposed approach significantly improves both\ndistinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy\n(e.g., as measured by CIDEr) for a wide variety of image captioning baselines.\nThese results are further confirmed through a user study.", "published": "2020-07-14 07:40:39", "link": "http://arxiv.org/abs/2007.06877v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Our Evaluation Metric Needs an Update to Encourage Generalization", "abstract": "Models that surpass human performance on several popular benchmarks display\nsignificant degradation in performance on exposure to Out of Distribution (OOD)\ndata. Recent research has shown that models overfit to spurious biases and\n`hack' datasets, in lieu of learning generalizable features like humans. In\norder to stop the inflation in model performance -- and thus overestimation in\nAI systems' capabilities -- we propose a simple and novel evaluation metric,\nWOOD Score, that encourages generalization during evaluation.", "published": "2020-07-14 08:15:19", "link": "http://arxiv.org/abs/2007.06898v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoreGen: Contextualized Code Representation Learning for Commit Message\n  Generation", "abstract": "Automatic generation of high-quality commit messages for code commits can\nsubstantially facilitate software developers' works and coordination. However,\nthe semantic gap between source code and natural language poses a major\nchallenge for the task. Several studies have been proposed to alleviate the\nchallenge but none explicitly involves code contextual information during\ncommit message generation. Specifically, existing research adopts static\nembedding for code tokens, which maps a token to the same vector regardless of\nits context. In this paper, we propose a novel Contextualized code\nrepresentation learning strategy for commit message Generation (CoreGen).\nCoreGen first learns contextualized code representations which exploit the\ncontextual information behind code commit sequences. The learned\nrepresentations of code commits built upon Transformer are then fine-tuned for\ndownstream commit message generation. Experiments on the benchmark dataset\ndemonstrate the superior effectiveness of our model over the baseline models\nwith at least 28.18% improvement in terms of BLEU-4 score. Furthermore, we also\nhighlight the future opportunities in training contextualized code\nrepresentations on larger code corpus as a solution to low-resource tasks and\nadapting the contextualized code representation framework to other code-to-text\ngeneration tasks.", "published": "2020-07-14 09:43:26", "link": "http://arxiv.org/abs/2007.06934v3", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Extracting Structured Data from Physician-Patient Conversations By\n  Predicting Noteworthy Utterances", "abstract": "Despite diverse efforts to mine various modalities of medical data, the\nconversations between physicians and patients at the time of care remain an\nuntapped source of insights. In this paper, we leverage this data to extract\nstructured information that might assist physicians with post-visit\ndocumentation in electronic health records, potentially lightening the clerical\nburden. In this exploratory study, we describe a new dataset consisting of\nconversation transcripts, post-visit summaries, corresponding supporting\nevidence (in the transcript), and structured labels. We focus on the tasks of\nrecognizing relevant diagnoses and abnormalities in the review of organ systems\n(RoS). One methodological challenge is that the conversations are long (around\n1500 words), making it difficult for modern deep-learning models to use them as\ninput. To address this challenge, we extract noteworthy utterances---parts of\nthe conversation likely to be cited as evidence supporting some summary\nsentence. We find that by first filtering for (predicted) noteworthy\nutterances, we can significantly boost predictive performance for recognizing\nboth diagnoses and RoS abnormalities.", "published": "2020-07-14 16:10:37", "link": "http://arxiv.org/abs/2007.07151v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Explore and Explain: Self-supervised Navigation and Recounting", "abstract": "Embodied AI has been recently gaining attention as it aims to foster the\ndevelopment of autonomous and intelligent agents. In this paper, we devise a\nnovel embodied setting in which an agent needs to explore a previously unknown\nenvironment while recounting what it sees during the path. In this context, the\nagent needs to navigate the environment driven by an exploration goal, select\nproper moments for description, and output natural language descriptions of\nrelevant objects and scenes. Our model integrates a novel self-supervised\nexploration module with penalty, and a fully-attentive captioning model for\nexplanation. Also, we investigate different policies for selecting proper\nmoments for explanation, driven by information coming from both the environment\nand the navigation. Experiments are conducted on photorealistic environments\nfrom the Matterport3D dataset and investigate the navigation and explanation\ncapabilities of the agent as well as the role of their interactions.", "published": "2020-07-14 18:00:49", "link": "http://arxiv.org/abs/2007.07268v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Sudo rm -rf: Efficient Networks for Universal Audio Source Separation", "abstract": "In this paper, we present an efficient neural network for end-to-end general\npurpose audio source separation. Specifically, the backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRMRF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. In this way, we are able\nto obtain high quality audio source separation with limited number of floating\npoint operations, memory requirements, number of parameters and latency. Our\nexperiments on both speech and environmental sound separation datasets show\nthat SuDoRMRF performs comparably and even surpasses various state-of-the-art\napproaches with significantly higher computational resource requirements.", "published": "2020-07-14 05:46:38", "link": "http://arxiv.org/abs/2007.06833v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Deep Learning Approach for Low-Latency Packet Loss Concealment of\n  Audio Signals in Networked Music Performance Applications", "abstract": "Networked Music Performance (NMP) is envisioned as a potential game changer\namong Internet applications: it aims at revolutionizing the traditional concept\nof musical interaction by enabling remote musicians to interact and perform\ntogether through a telecommunication network. Ensuring realistic conditions for\nmusic performance, however, constitutes a significant engineering challenge due\nto extremely strict requirements in terms of audio quality and, most\nimportantly, network delay. To minimize the end-to-end delay experienced by the\nmusicians, typical implementations of NMP applications use un-compressed,\nbidirectional audio streams and leverage UDP as transport protocol. Being\nconnection less and unreliable,audio packets transmitted via UDP which become\nlost in transit are not re-transmitted and thus cause glitches in the receiver\naudio playout. This article describes a technique for predicting lost packet\ncontent in real-time using a deep learning approach. The ability of concealing\nerrors in real time can help mitigate audio impairments caused by packet\nlosses, thus improving the quality of audio playout in real-world scenarios.", "published": "2020-07-14 15:51:52", "link": "http://arxiv.org/abs/2007.07132v1", "categories": ["cs.SD", "cs.LG", "cs.NI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating Visually Aligned Sound from Videos", "abstract": "We focus on the task of generating sound from natural videos, and the sound\nshould be both temporally and content-wise aligned with visual signals. This\ntask is extremely challenging because some sounds generated \\emph{outside} a\ncamera can not be inferred from video content. The model may be forced to learn\nan incorrect mapping between visual content and these irrelevant sounds. To\naddress this challenge, we propose a framework named REGNET. In this framework,\nwe first extract appearance and motion features from video frames to better\ndistinguish the object that emits sound from complex background information. We\nthen introduce an innovative audio forwarding regularizer that directly\nconsiders the real sound as input and outputs bottlenecked sound features.\nUsing both visual and bottlenecked sound features for sound prediction during\ntraining provides stronger supervision for the sound prediction. The audio\nforwarding regularizer can control the irrelevant sound component and thus\nprevent the model from learning an incorrect mapping between video frames and\nsound emitted by the object that is out of the screen. During testing, the\naudio forwarding regularizer is removed to ensure that REGNET can produce\npurely aligned sound only from visual features. Extensive evaluations based on\nAmazon Mechanical Turk demonstrate that our method significantly improves both\ntemporal and content-wise alignment. Remarkably, our generated sound can fool\nthe human with a 68.12% success rate. Code and pre-trained models are publicly\navailable at https://github.com/PeihaoChen/regnet", "published": "2020-07-14 07:51:06", "link": "http://arxiv.org/abs/2008.00820v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Allpass Feedback Delay Networks", "abstract": "In the 1960s, Schroeder and Logan introduced delay line-based allpass\nfilters, which are still popular due to their computational efficiency and\nversatile applicability in artificial reverberation, decorrelation, and\ndispersive system design. In this work, we extend the theory of allpass systems\nto any arbitrary connection of delay lines, namely feedback delay networks\n(FDNs). We present a characterization of uniallpass FDNs, i.e., FDNs, which are\nallpass for an arbitrary choice of delays. Further, we develop a solution to\nthe completion problem, i.e., given an FDN feedback matrix to determine the\nremaining gain parameters such that the FDN is allpass. Particularly useful for\nthe completion problem are feedback matrices, which yield a homogeneous decay\nof all system modes. Finally, we apply the uniallpass characterization to\nprevious FDN designs, namely, Schroeder's series allpass and Gardner's nested\nallpass for single-input, single-output systems, and, Poletti's unitary\nreverberator for multi-input, multi-output systems and demonstrate the\nsignificant extension of the design space.", "published": "2020-07-14 20:18:34", "link": "http://arxiv.org/abs/2007.07337v2", "categories": ["eess.SP", "cs.SD", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "eess.SP"}
