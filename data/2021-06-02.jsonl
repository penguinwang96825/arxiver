{"title": "Evaluating Word Embeddings with Categorical Modularity", "abstract": "We introduce categorical modularity, a novel low-resource intrinsic metric to\nevaluate word embedding quality. Categorical modularity is a graph modularity\nmetric based on the $k$-nearest neighbor graph constructed with embedding\nvectors of words from a fixed set of semantic categories, in which the goal is\nto measure the proportion of words that have nearest neighbors within the same\ncategories. We use a core set of 500 words belonging to 59 neurobiologically\nmotivated semantic categories in 29 languages and analyze three word embedding\nmodels per language (FastText, MUSE, and subs2vec). We find moderate to strong\npositive correlations between categorical modularity and performance on the\nmonolingual tasks of sentiment analysis and word similarity calculation and on\nthe cross-lingual task of bilingual lexicon induction both to and from English.\nOverall, we suggest that categorical modularity provides non-trivial predictive\ninformation about downstream task performance, with breakdowns of correlations\nby model suggesting some meta-predictive properties about semantic information\nloss as well.", "published": "2021-06-02 01:29:11", "link": "http://arxiv.org/abs/2106.00877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving Arithmetic Word Problems with Transformers and Preprocessing of\n  Problem Text", "abstract": "This paper outlines the use of Transformer networks trained to translate math\nword problems to equivalent arithmetic expressions in infix, prefix, and\npostfix notations. We compare results produced by many neural configurations\nand find that most configurations outperform previously reported approaches on\nthree of four datasets with significant increases in accuracy of over 20\npercentage points. The best neural approaches boost accuracy by 30% when\ncompared to the previous state-of-the-art on some datasets.", "published": "2021-06-02 02:12:45", "link": "http://arxiv.org/abs/2106.00893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12\n  More Genres", "abstract": "SOTA coreference resolution produces increasingly impressive scores on the\nOntoNotes benchmark. However lack of comparable data following the same scheme\nfor more genres makes it difficult to evaluate generalizability to open domain\ndata. This paper provides a dataset and comprehensive evaluation showing that\nthe latest neural LM based end-to-end systems degrade very substantially out of\ndomain. We make an OntoNotes-like coreference dataset called OntoGUM publicly\navailable, converted from GUM, an English corpus covering 12 genres, using\ndeterministic rules, which we evaluate. Thanks to the rich syntactic and\ndiscourse annotations in GUM, we are able to create the largest human-annotated\ncoreference corpus following the OntoNotes guidelines, and the first to be\nevaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation\nacross 12 genres shows nearly 15-20% degradation for both deterministic and\ndeep learning systems, indicating a lack of generalizability or covert\noverfitting in existing coreference resolution models.", "published": "2021-06-02 04:42:51", "link": "http://arxiv.org/abs/2106.00933v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discrete Cosine Transform as Universal Sentence Encoder", "abstract": "Modern sentence encoders are used to generate dense vector representations\nthat capture the underlying linguistic characteristics for a sequence of words,\nincluding phrases, sentences, or paragraphs. These kinds of representations are\nideal for training a classifier for an end task such as sentiment analysis,\nquestion answering and text classification. Different models have been proposed\nto efficiently generate general purpose sentence representations to be used in\npretraining protocols. While averaging is the most commonly used efficient\nsentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an\nalternative that captures the underlying syntactic characteristics of a given\ntext without compromising practical efficiency compared to averaging. However,\nas with most other sentence encoders, the DCT sentence encoder was only\nevaluated in English. To this end, we utilize DCT encoder to generate universal\nsentence representation for different languages such as German, French, Spanish\nand Russian. The experimental results clearly show the superior effectiveness\nof DCT encoding in which consistent performance improvements are achieved over\nstrong baselines on multiple standardized datasets.", "published": "2021-06-02 04:43:54", "link": "http://arxiv.org/abs/2106.00934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Level Attention Model for Evidence-Based Fact Checking", "abstract": "Evidence-based fact checking aims to verify the truthfulness of a claim\nagainst evidence extracted from textual sources. Learning a representation that\neffectively captures relations between a claim and evidence can be challenging.\nRecent state-of-the-art approaches have developed increasingly sophisticated\nmodels based on graph structures. We present a simple model that can be trained\non sequence structures. Our model enables inter-sentence attentions at\ndifferent levels and can benefit from joint training. Results on a large-scale\ndataset for Fact Extraction and VERification (FEVER) show that our model\noutperforms the graph-based approaches and yields 1.09% and 1.42% improvements\nin label accuracy and FEVER score, respectively, over the best published model.", "published": "2021-06-02 05:40:12", "link": "http://arxiv.org/abs/2106.00950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answer Generation for Retrieval-based Question Answering Systems", "abstract": "Recent advancements in transformer-based models have greatly improved the\nability of Question Answering (QA) systems to provide correct answers; in\nparticular, answer sentence selection (AS2) models, core components of\nretrieval-based systems, have achieved impressive results. While generally\neffective, these models fail to provide a satisfying answer when all retrieved\ncandidates are of poor quality, even if they contain correct information. In\nAS2, models are trained to select the best answer sentence among a set of\ncandidates retrieved for a given question. In this work, we propose to generate\nanswers from a set of AS2 top candidates. Rather than selecting the best\ncandidate, we train a sequence to sequence transformer model to generate an\nanswer from a candidate set. Our tests on three English AS2 datasets show\nimprovement up to 32 absolute points in accuracy over the state of the art.", "published": "2021-06-02 05:45:49", "link": "http://arxiv.org/abs/2106.00955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RevCore: Review-augmented Conversational Recommendation", "abstract": "Existing conversational recommendation (CR) systems usually suffer from\ninsufficient item information when conducted on short dialogue history and\nunfamiliar items. Incorporating external information (e.g., reviews) is a\npotential solution to alleviate this problem. Given that reviews often provide\na rich and detailed user experience on different interests, they are potential\nideal resources for providing high-quality recommendations within an\ninformative conversation. In this paper, we design a novel end-to-end\nframework, namely, Review-augmented Conversational Recommender (RevCore), where\nreviews are seamlessly incorporated to enrich item information and assist in\ngenerating both coherent and informative responses. In detail, we extract\nsentiment-consistent reviews, perform review-enriched and entity-based\nrecommendations for item suggestions, as well as use a review-attentive\nencoder-decoder for response generation. Experimental results demonstrate the\nsuperiority of our approach in yielding better performance on both\nrecommendation and conversation responding.", "published": "2021-06-02 05:46:01", "link": "http://arxiv.org/abs/2106.00957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Discourse Structures for Argument Impact Classification", "abstract": "Discourse relations among arguments reveal logical structures of a debate\nconversation. However, no prior work has explicitly studied how the sequence of\ndiscourse relations influence a claim's impact. This paper empirically shows\nthat the discourse relations between two arguments along the context path are\nessential factors for identifying the persuasive power of an argument. We\nfurther propose DisCOC to inject and fuse the sentence-level structural\ndiscourse information with contextualized features derived from large-scale\nlanguage models. Experimental results and extensive analysis show that the\nattention and gate mechanisms that explicitly model contexts and texts can\nindeed help the argument impact classification task defined by Durmus et al.\n(2019), and discourse structures among the context path of the claim to be\nclassified can further boost the performance.", "published": "2021-06-02 06:49:19", "link": "http://arxiv.org/abs/2106.00976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Teacher is Enough? Pre-trained Language Model Distillation from\n  Multiple Teachers", "abstract": "Pre-trained language models (PLMs) achieve great success in NLP. However,\ntheir huge model sizes hinder their applications in many practical systems.\nKnowledge distillation is a popular technique to compress PLMs, which learns a\nsmall student model from a large teacher PLM. However, the knowledge learned\nfrom a single teacher may be limited and even biased, resulting in low-quality\nstudent model. In this paper, we propose a multi-teacher knowledge distillation\nframework named MT-BERT for pre-trained language model compression, which can\ntrain high-quality student model from multiple teacher PLMs. In MT-BERT we\ndesign a multi-teacher co-finetuning method to jointly finetune multiple\nteacher PLMs in downstream tasks with shared pooling and prediction layers to\nalign their output space for better collaborative teaching. In addition, we\npropose a multi-teacher hidden loss and a multi-teacher distillation loss to\ntransfer the useful knowledge in both hidden states and soft labels from\nmultiple teacher PLMs to the student model. Experiments on three benchmark\ndatasets validate the effectiveness of MT-BERT in compressing PLMs.", "published": "2021-06-02 08:42:33", "link": "http://arxiv.org/abs/2106.01023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Machine Reading Comprehension Models Learn Shortcuts?", "abstract": "Recent studies report that many machine reading comprehension (MRC) models\ncan perform closely to or even better than humans on benchmark datasets.\nHowever, existing works indicate that many MRC models may learn shortcuts to\noutwit these benchmarks, but the performance is unsatisfactory in real-world\napplications. In this work, we attempt to explore, instead of the expected\ncomprehension skills, why these models learn the shortcuts. Based on the\nobservation that a large portion of questions in current datasets have shortcut\nsolutions, we argue that larger proportion of shortcut questions in training\ndata make models rely on shortcut tricks excessively. To investigate this\nhypothesis, we carefully design two synthetic datasets with annotations that\nindicate whether a question can be answered using shortcut solutions. We\nfurther propose two new methods to quantitatively analyze the learning\ndifficulty regarding shortcut and challenging questions, and revealing the\ninherent learning mechanism behind the different performance between the two\nkinds of questions. A thorough empirical analysis shows that MRC models tend to\nlearn shortcut questions earlier than challenging questions, and the high\nproportions of shortcut questions in training sets hinder models from exploring\nthe sophisticated reasoning skills in the later stage of training.", "published": "2021-06-02 08:43:12", "link": "http://arxiv.org/abs/2106.01024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and\n  Effective Long Document Modeling", "abstract": "Transformer is important for text modeling. However, it has difficulty in\nhandling long documents due to the quadratic complexity with input text length.\nIn order to handle this problem, we propose a hierarchical interactive\nTransformer (Hi-Transformer) for efficient and effective long document\nmodeling. Hi-Transformer models documents in a hierarchical way, i.e., first\nlearns sentence representations and then learns document representations. It\ncan effectively reduce the complexity and meanwhile capture global document\ncontext in the modeling of each sentence. More specifically, we first use a\nsentence Transformer to learn the representations of each sentence. Then we use\na document Transformer to model the global document context from these sentence\nrepresentations. Next, we use another sentence Transformer to enhance sentence\nmodeling using the global document context. Finally, we use hierarchical\npooling method to obtain document embedding. Extensive experiments on three\nbenchmark datasets validate the efficiency and effectiveness of Hi-Transformer\nin long document modeling.", "published": "2021-06-02 09:30:29", "link": "http://arxiv.org/abs/2106.01040v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining the Inductive Bias of Neural Language Models with Artificial\n  Languages", "abstract": "Since language models are used to model a wide variety of languages, it is\nnatural to ask whether the neural architectures used for the task have\ninductive biases towards modeling particular types of languages. Investigation\nof these biases has proved complicated due to the many variables that appear in\nthe experimental setup. Languages vary in many typological dimensions, and it\nis difficult to single out one or two to investigate without the others acting\nas confounders. We propose a novel method for investigating the inductive\nbiases of language models using artificial languages. These languages are\nconstructed to allow us to create parallel corpora across languages that differ\nonly in the typological feature being investigated, such as word order. We then\nuse them to train and test language models. This constitutes a fully controlled\ncausal framework, and demonstrates how grammar engineering can serve as a\nuseful tool for analyzing neural models. Using this method, we find that\ncommonly used neural architectures exhibit different inductive biases: LSTMs\ndisplay little preference with respect to word ordering, while transformers\ndisplay a clear preference for some orderings over others. Further, we find\nthat neither the inductive bias of the LSTM nor that of the transformer appears\nto reflect any tendencies that we see in attested natural languages.", "published": "2021-06-02 09:34:32", "link": "http://arxiv.org/abs/2106.01044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cascade versus Direct Speech Translation: Do the Differences Still Make\n  a Difference?", "abstract": "Five years after the first published proofs of concept, direct approaches to\nspeech translation (ST) are now competing with traditional cascade solutions.\nIn light of this steady progress, can we claim that the performance gap between\nthe two is closed? Starting from this question, we present a systematic\ncomparison between state-of-the-art systems representative of the two\nparadigms. Focusing on three language directions\n(English-German/Italian/Spanish), we conduct automatic and manual evaluations,\nexploiting high-quality professional post-edits and annotations. Our\nmulti-faceted analysis on one of the few publicly available ST benchmarks\nattests for the first time that: i) the gap between the two paradigms is now\nclosed, and ii) the subtle differences observed in their behavior are not\nsufficient for humans neither to distinguish them nor to prefer one over the\nother.", "published": "2021-06-02 09:37:37", "link": "http://arxiv.org/abs/2106.01045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimax and Neyman-Pearson Meta-Learning for Outlier Languages", "abstract": "Model-agnostic meta-learning (MAML) has been recently put forth as a strategy\nto learn resource-poor languages in a sample-efficient fashion. Nevertheless,\nthe properties of these languages are often not well represented by those\navailable during training. Hence, we argue that the i.i.d. assumption ingrained\nin MAML makes it ill-suited for cross-lingual NLP. In fact, under a\ndecision-theoretic framework, MAML can be interpreted as minimising the\nexpected risk across training languages (with a uniform prior), which is known\nas Bayes criterion. To increase its robustness to outlier languages, we create\ntwo variants of MAML based on alternative criteria: Minimax MAML reduces the\nmaximum risk across languages, while Neyman-Pearson MAML constrains the risk in\neach language to a maximum threshold. Both criteria constitute fully\ndifferentiable two-player games. In light of this, we propose a new adaptive\noptimiser solving for a local approximation to their Nash equilibrium. We\nevaluate both model variants on two popular NLP tasks, part-of-speech tagging\nand question answering. We report gains for their average and minimum\nperformance across low-resource languages in zero- and few-shot settings,\ncompared to joint multi-source transfer and vanilla MAML.", "published": "2021-06-02 09:53:06", "link": "http://arxiv.org/abs/2106.01051v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "John praised Mary because he? Implicit Causality Bias and Its\n  Interaction with Explicit Cues in LMs", "abstract": "Some interpersonal verbs can implicitly attribute causality to either their\nsubject or their object and are therefore said to carry an implicit causality\n(IC) bias. Through this bias, causal links can be inferred from a narrative,\naiding language comprehension. We investigate whether pre-trained language\nmodels (PLMs) encode IC bias and use it at inference time. We find that to be\nthe case, albeit to different degrees, for three distinct PLM architectures.\nHowever, causes do not always need to be implicit -- when a cause is explicitly\nstated in a subordinate clause, an incongruent IC bias associated with the verb\nin the main clause leads to a delay in human processing. We hypothesize that\nthe temporary challenge humans face in integrating the two contradicting\nsignals, one from the lexical semantics of the verb, one from the\nsentence-level semantics, would be reflected in higher error rates for models\non tasks dependent on causal links. The results of our study lend support to\nthis hypothesis, suggesting that PLMs tend to prioritize lexical patterns over\nhigher-order signals.", "published": "2021-06-02 10:26:07", "link": "http://arxiv.org/abs/2106.01060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Informative Conclusions for Argumentative Texts", "abstract": "The purpose of an argumentative text is to support a certain conclusion. Yet,\nthey are often omitted, expecting readers to infer them rather. While\nappropriate when reading an individual text, this rhetorical device limits\naccessibility when browsing many texts (e.g., on a search engine or on social\nmedia). In these scenarios, an explicit conclusion makes for a good candidate\nsummary of an argumentative text. This is especially true if the conclusion is\ninformative, emphasizing specific concepts from the text. With this paper we\nintroduce the task of generating informative conclusions: First,\nWebis-ConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of\nargumentative texts and their conclusions. Second, two paradigms for conclusion\ngeneration are investigated; one extractive, the other abstractive in nature.\nThe latter exploits argumentative knowledge that augment the data via control\ncodes and finetuning the BART model on several subsets of the corpus. Third,\ninsights are provided into the suitability of our corpus for the task, the\ndifferences between the two generation paradigms, the trade-off between\ninformativeness and conciseness, and the impact of encoding argumentative\nknowledge. The corpus, code, and the trained models are publicly available.", "published": "2021-06-02 10:35:59", "link": "http://arxiv.org/abs/2106.01064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robustness of Text-to-SQL Models against Synonym Substitution", "abstract": "Recently, there has been significant progress in studying neural networks to\ntranslate text descriptions into SQL queries. Despite achieving good\nperformance on some public benchmarks, existing text-to-SQL models typically\nrely on the lexical matching between words in natural language (NL) questions\nand tokens in table schemas, which may render the models vulnerable to attacks\nthat break the schema linking mechanism. In this work, we investigate the\nrobustness of text-to-SQL models to synonym substitution. In particular, we\nintroduce Spider-Syn, a human-curated dataset based on the Spider benchmark for\ntext-to-SQL translation. NL questions in Spider-Syn are modified from Spider,\nby replacing their schema-related words with manually selected synonyms that\nreflect real-world question paraphrases. We observe that the accuracy\ndramatically drops by eliminating such explicit correspondence between NL\nquestions and table schemas, even if the synonyms are not adversarially\nselected to conduct worst-case adversarial attacks. Finally, we present two\ncategories of approaches to improve the model robustness. The first category of\napproaches utilizes additional synonym annotations for table schemas by\nmodifying the model input, while the second category is based on adversarial\ntraining. We demonstrate that both categories of approaches significantly\noutperform their counterparts without the defense, and the first category of\napproaches are more effective.", "published": "2021-06-02 10:36:23", "link": "http://arxiv.org/abs/2106.01065v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion\n  Detection", "abstract": "Emotion detection in dialogues is challenging as it often requires the\nidentification of thematic topics underlying a conversation, the relevant\ncommonsense knowledge, and the intricate transition patterns between the\naffective states. In this paper, we propose a Topic-Driven Knowledge-Aware\nTransformer to handle the challenges above. We firstly design a topic-augmented\nlanguage model (LM) with an additional layer specialized for topic detection.\nThe topic-augmented LM is then combined with commonsense statements derived\nfrom a knowledge base based on the dialogue contextual information. Finally, a\ntransformer-based encoder-decoder architecture fuses the topical and\ncommonsense information, and performs the emotion label sequence prediction.\nThe model has been experimented on four datasets in dialogue emotion detection,\ndemonstrating its superiority empirically over the existing state-of-the-art\napproaches. Quantitative and qualitative results show that the model can\ndiscover topics which help in distinguishing emotion categories.", "published": "2021-06-02 10:57:44", "link": "http://arxiv.org/abs/2106.01071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SyGNS: A Systematic Generalization Testbed Based on Natural Language\n  Semantics", "abstract": "Recently, deep neural networks (DNNs) have achieved great success in\nsemantically challenging NLP tasks, yet it remains unclear whether DNN models\ncan capture compositional meanings, those aspects of meaning that have been\nlong studied in formal semantics. To investigate this issue, we propose a\nSystematic Generalization testbed based on Natural language Semantics (SyGNS),\nwhose challenge is to map natural language sentences to multiple forms of\nscoped meaning representations, designed to account for various semantic\nphenomena. Using SyGNS, we test whether neural networks can systematically\nparse sentences involving novel combinations of logical expressions such as\nquantifiers and negation. Experiments show that Transformer and GRU models can\ngeneralize to unseen combinations of quantifiers, negations, and modifiers that\nare similar to given training instances in form, but not to the others. We also\nfind that the generalization performance to unseen combinations is better when\nthe form of meaning representations is simpler. The data and code for SyGNS are\npublicly available at https://github.com/verypluming/SyGNS.", "published": "2021-06-02 11:24:41", "link": "http://arxiv.org/abs/2106.01077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Sparse Attention more Interpretable?", "abstract": "Sparse attention has been claimed to increase model interpretability under\nthe assumption that it highlights influential inputs. Yet the attention\ndistribution is typically over representations internal to the model rather\nthan the inputs themselves, suggesting this assumption may not have merit. We\nbuild on the recent work exploring the interpretability of attention; we design\na set of experiments to help us understand how sparsity affects our ability to\nuse attention as an explainability tool. On three text classification tasks, we\nverify that only a weak relationship between inputs and co-indexed intermediate\nrepresentations exists -- under sparse attention and otherwise. Further, we do\nnot find any plausible mappings from sparse attention distributions to a sparse\nset of influential inputs through other avenues. Rather, we observe in this\nsetting that inducing sparsity may make it less plausible that attention can be\nused as a tool for understanding model behavior.", "published": "2021-06-02 11:42:56", "link": "http://arxiv.org/abs/2106.01087v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "belabBERT: a Dutch RoBERTa-based language model applied to psychiatric\n  classification", "abstract": "Natural language processing (NLP) is becoming an important means for\nautomatic recognition of human traits and states, such as intoxication,\npresence of psychiatric disorders, presence of airway disorders and states of\nstress. Such applications have the potential to be an important pillar for\nonline help lines, and may gradually be introduced into eHealth modules.\nHowever, NLP is language specific and for languages such as Dutch, NLP models\nare scarce. As a result, recent Dutch NLP models have a low capture of long\nrange semantic dependencies over sentences. To overcome this, here we present\nbelabBERT, a new Dutch language model extending the RoBERTa architecture.\nbelabBERT is trained on a large Dutch corpus (+32 GB) of web crawled texts. We\napplied belabBERT to the classification of psychiatric illnesses. First, we\nevaluated the strength of text-based classification using belabBERT, and\ncompared the results to the existing RobBERT model. Then, we compared the\nperformance of belabBERT to audio classification for psychiatric disorders.\nFinally, a brief exploration was performed, extending the framework to a hybrid\ntext- and audio-based classification. Our results show that belabBERT\noutperformed the current best text classification network for Dutch, RobBERT.\nbelabBERT also outperformed classification based on audio alone.", "published": "2021-06-02 11:50:49", "link": "http://arxiv.org/abs/2106.01091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and\n  Non-Local Relations", "abstract": "This work aims to tackle the challenging heterogeneous graph encoding problem\nin the text-to-SQL task. Previous methods are typically node-centric and merely\nutilize different weight matrices to parameterize edge types, which 1) ignore\nthe rich semantics embedded in the topological structure of edges, and 2) fail\nto distinguish local and non-local relations for each node. To this end, we\npropose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying\nrelational features without constructing meta-paths. By virtue of the line\ngraph, messages propagate more efficiently through not only connections between\nnodes, but also the topology of directed edges. Furthermore, both local and\nnon-local relations are integrated distinctively during the graph iteration. We\nalso design an auxiliary task called graph pruning to improve the\ndiscriminative capability of the encoder. Our framework achieves\nstate-of-the-art results (62.8% with Glove, 72.0% with Electra) on the\ncross-domain text-to-SQL benchmark Spider at the time of writing.", "published": "2021-06-02 11:53:35", "link": "http://arxiv.org/abs/2106.01093v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Use of Formal Ethical Reviews in NLP Literature: Historical Trends and\n  Current Practices", "abstract": "Ethical aspects of research in language technologies have received much\nattention recently. It is a standard practice to get a study involving human\nsubjects reviewed and approved by a professional ethics committee/board of the\ninstitution. How commonly do we see mention of ethical approvals in NLP\nresearch? What types of research or aspects of studies are usually subject to\nsuch reviews? With the rising concerns and discourse around the ethics of NLP,\ndo we also observe a rise in formal ethical reviews of NLP studies? And, if so,\nwould this imply that there is a heightened awareness of ethical issues that\nwas previously lacking? We aim to address these questions by conducting a\ndetailed quantitative and qualitative analysis of the ACL Anthology, as well as\ncomparing the trends in our field to those of other related disciplines, such\nas cognitive science, machine learning, data mining, and systems.", "published": "2021-06-02 12:12:59", "link": "http://arxiv.org/abs/2106.01105v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DynaEval: Unifying Turn and Dialogue Level Evaluation", "abstract": "A dialogue is essentially a multi-turn interaction among interlocutors.\nEffective evaluation metrics should reflect the dynamics of such interaction.\nExisting automatic metrics are focused very much on the turn-level quality,\nwhile ignoring such dynamics. To this end, we propose DynaEval, a unified\nautomatic evaluation framework which is not only capable of performing\nturn-level evaluation, but also holistically considers the quality of the\nentire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted\nto model a dialogue in totality, where the graph nodes denote each individual\nutterance and the edges represent the dependency between pairs of utterances. A\ncontrastive loss is then applied to distinguish well-formed dialogues from\ncarefully constructed negative samples. Experiments show that DynaEval\nsignificantly outperforms the state-of-the-art dialogue coherence model, and\ncorrelates strongly with human judgements across multiple dialogue evaluation\naspects at both turn and dialogue level.", "published": "2021-06-02 12:23:18", "link": "http://arxiv.org/abs/2106.01112v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Emotional Support Dialog Systems", "abstract": "Emotional support is a crucial ability for many conversation scenarios,\nincluding social interactions, mental health support, and customer service\nchats. Following reasonable procedures and using various support skills can\nhelp to effectively provide support. However, due to the lack of a\nwell-designed task and corpora of effective emotional support conversations,\nresearch on building emotional support into dialog systems remains untouched.\nIn this paper, we define the Emotional Support Conversation (ESC) task and\npropose an ESC Framework, which is grounded on the Helping Skills Theory. We\nconstruct an Emotion Support Conversation dataset (ESConv) with rich annotation\n(especially support strategy) in a help-seeker and supporter mode. To ensure a\ncorpus of high-quality conversations that provide examples of effective\nemotional support, we take extensive effort to design training tutorials for\nsupporters and several mechanisms for quality control during data collection.\nFinally, we evaluate state-of-the-art dialog models with respect to the ability\nto provide emotional support. Our results show the importance of support\nstrategies in providing effective emotional support and the utility of ESConv\nin training more emotional support systems.", "published": "2021-06-02 13:30:43", "link": "http://arxiv.org/abs/2106.01144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End NLP Knowledge Graph Construction", "abstract": "This paper studies the end-to-end construction of an NLP Knowledge Graph (KG)\nfrom scientific papers. We focus on extracting four types of relations:\nevaluatedOn between tasks and datasets, evaluatedBy between tasks and\nevaluation metrics, as well as coreferent and related relations between the\nsame type of entities. For instance, F1-score is coreferent with F-measure. We\nintroduce novel methods for each of these relation types and apply our final\nframework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a\nlarge-scale KG, which can facilitate automatically constructing scientific\nleaderboards for the NLP community. The results of our experiments indicate\nthat the resulting KG contains high-quality information.", "published": "2021-06-02 14:03:06", "link": "http://arxiv.org/abs/2106.01167v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Bot-Generated Text by Characterizing Linguistic Accommodation\n  in Human-Bot Interactions", "abstract": "Language generation models' democratization benefits many domains, from\nanswering health-related questions to enhancing education by providing\nAI-driven tutoring services. However, language generation models'\ndemocratization also makes it easier to generate human-like text at-scale for\nnefarious activities, from spreading misinformation to targeting specific\ngroups with hate speech. Thus, it is essential to understand how people\ninteract with bots and develop methods to detect bot-generated text. This paper\nshows that bot-generated text detection methods are more robust across datasets\nand models if we use information about how people respond to it rather than\nusing the bot's text directly. We also analyze linguistic alignment, providing\ninsight into differences between human-human and human-bot conversations.", "published": "2021-06-02 14:10:28", "link": "http://arxiv.org/abs/2106.01170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cluster-based Approach for Improving Isotropy in Contextual Embedding\n  Space", "abstract": "The representation degeneration problem in Contextual Word Representations\n(CWRs) hurts the expressiveness of the embedding space by forming an\nanisotropic cone where even unrelated words have excessively positive\ncorrelations. Existing techniques for tackling this issue require a learning\nprocess to re-train models with additional objectives and mostly employ a\nglobal assessment to study isotropy. Our quantitative analysis over isotropy\nshows that a local assessment could be more accurate due to the clustered\nstructure of CWRs. Based on this observation, we propose a local cluster-based\nmethod to address the degeneration issue in contextual embedding spaces. We\nshow that in clusters including punctuations and stop words, local dominant\ndirections encode structural information, removing which can improve CWRs\nperformance on semantic tasks. Moreover, we find that tense information in verb\nrepresentations dominates sense semantics. We show that removing dominant\ndirections of verb representations can transform the space to better suit\nsemantic applications. Our experiments demonstrate that the proposed\ncluster-based method can mitigate the degeneration problem on multiple tasks.", "published": "2021-06-02 14:26:37", "link": "http://arxiv.org/abs/2106.01183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Document Similarity Ranking via Contextualized Language\n  Models and Hierarchical Inference", "abstract": "We present a novel model for the problem of ranking a collection of documents\naccording to their semantic similarity to a source (query) document. While the\nproblem of document-to-document similarity ranking has been studied, most\nmodern methods are limited to relatively short documents or rely on the\nexistence of \"ground-truth\" similarity labels. Yet, in most common real-world\ncases, similarity ranking is an unsupervised problem as similarity labels are\nunavailable. Moreover, an ideal model should not be restricted by documents'\nlength. Hence, we introduce SDR, a self-supervised method for document\nsimilarity that can be applied to documents of arbitrary length. Importantly,\nSDR can be effectively applied to extremely long documents, exceeding the 4,096\nmaximal token limits of Longformer. Extensive evaluations on large document\ndatasets show that SDR significantly outperforms its alternatives across all\nmetrics. To accelerate future research on unlabeled long document similarity\nranking, and as an additional contribution to the community, we herein publish\ntwo human-annotated test sets of long documents similarity evaluation. The SDR\ncode and datasets are publicly available.", "published": "2021-06-02 14:29:35", "link": "http://arxiv.org/abs/2106.01186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IrEne: Interpretable Energy Prediction for Transformers", "abstract": "Existing software-based energy measurements of NLP models are not accurate\nbecause they do not consider the complex interactions between energy\nconsumption and model execution. We present IrEne, an interpretable and\nextensible energy prediction system that accurately predicts the inference\nenergy consumption of a wide range of Transformer-based NLP models. IrEne\nconstructs a model tree graph that breaks down the NLP model into modules that\nare further broken down into low-level machine learning (ML) primitives. IrEne\npredicts the inference energy consumption of the ML primitives as a function of\ngeneralizable features and fine-grained runtime resource usage. IrEne then\naggregates these low-level predictions recursively to predict the energy of\neach module and finally of the entire model. Experiments across multiple\nTransformer models show IrEne predicts inference energy consumption of\ntransformer models with an error of under 7% compared to the ground truth. In\ncontrast, existing energy models see an error of over 50%. We also show how\nIrEne can be used to conduct energy bottleneck analysis and to easily evaluate\nthe energy impact of different architectural choices. We release the code and\ndata at https://github.com/StonyBrookNLP/irene.", "published": "2021-06-02 14:43:51", "link": "http://arxiv.org/abs/2106.01199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Constraint-Based Behavior in Neural Models via Targeted\n  Fine-Tuning", "abstract": "A growing body of literature has focused on detailing the linguistic\nknowledge embedded in large, pretrained language models. Existing work has\nshown that non-linguistic biases in models can drive model behavior away from\nlinguistic generalizations. We hypothesized that competing linguistic processes\nwithin a language, rather than just non-linguistic model biases, could obscure\nunderlying linguistic knowledge. We tested this claim by exploring a single\nphenomenon in four languages: English, Chinese, Spanish, and Italian. While\nhuman behavior has been found to be similar across languages, we find\ncross-linguistic variation in model behavior. We show that competing processes\nin a language act as constraints on model behavior and demonstrate that\ntargeted fine-tuning can re-weight the learned constraints, uncovering\notherwise dormant linguistic knowledge in models. Our results suggest that\nmodels need to learn both the linguistic constraints in a language and their\nrelative ranking, with mismatches in either producing non-human-like behavior.", "published": "2021-06-02 14:52:11", "link": "http://arxiv.org/abs/2106.01207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-document Coreference Resolution over Predicted Mentions", "abstract": "Coreference resolution has been mostly investigated within a single document\nscope, showing impressive progress in recent years based on end-to-end models.\nHowever, the more challenging task of cross-document (CD) coreference\nresolution remained relatively under-explored, with the few recent models\napplied only to gold mentions. Here, we introduce the first end-to-end model\nfor CD coreference resolution from raw text, which extends the prominent model\nfor within-document coreference to the CD setting. Our model achieves\ncompetitive results for event and entity coreference resolution on gold\nmentions. More importantly, we set first baseline results, on the standard ECB+\ndataset, for CD coreference resolution over predicted mentions. Further, our\nmodel is simpler and more efficient than recent CD coreference resolution\nsystems, while not using any external resources.", "published": "2021-06-02 14:56:28", "link": "http://arxiv.org/abs/2106.01210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Generative Framework for Various NER Subtasks", "abstract": "Named Entity Recognition (NER) is the task of identifying spans that\nrepresent entities in sentences. Whether the entity spans are nested or\ndiscontinuous, the NER task can be categorized into the flat NER, nested NER,\nand discontinuous NER subtasks. These subtasks have been mainly solved by the\ntoken-level sequence labelling or span-level classification. However, these\nsolutions can hardly tackle the three kinds of NER subtasks concurrently. To\nthat end, we propose to formulate the NER subtasks as an entity span sequence\ngeneration task, which can be solved by a unified sequence-to-sequence\n(Seq2Seq) framework. Based on our unified framework, we can leverage the\npre-trained Seq2Seq model to solve all three kinds of NER subtasks without the\nspecial design of the tagging schema or ways to enumerate spans. We exploit\nthree types of entity representations to linearize entities into a sequence.\nOur proposed framework is easy-to-implement and achieves state-of-the-art\n(SoTA) or near SoTA performance on eight English NER datasets, including two\nflat NER datasets, three nested NER datasets, and three discontinuous NER\ndatasets.", "published": "2021-06-02 15:19:23", "link": "http://arxiv.org/abs/2106.01223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metaphor Generation with Conceptual Mappings", "abstract": "Generating metaphors is a difficult task as it requires understanding nuanced\nrelationships between abstract concepts. In this paper, we aim to generate a\nmetaphoric sentence given a literal expression by replacing relevant verbs.\nGuided by conceptual metaphor theory, we propose to control the generation\nprocess by encoding conceptual mappings between cognitive domains to generate\nmeaningful metaphoric expressions. To achieve this, we develop two methods: 1)\nusing FrameNet-based embeddings to learn mappings between domains and applying\nthem at the lexical level (CM-Lex), and 2) deriving source/target pairs to\ntrain a controlled seq-to-seq generation model (CM-BART). We assess our methods\nthrough automatic and human evaluation for basic metaphoricity and conceptual\nmetaphor presence. We show that the unsupervised CM-Lex model is competitive\nwith recent deep learning metaphor generation systems, and CM-BART outperforms\nall other models both in automatic and human evaluations.", "published": "2021-06-02 15:27:05", "link": "http://arxiv.org/abs/2106.01228v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Lower Perplexity is Not Always Human-Like", "abstract": "In computational psycholinguistics, various language models have been\nevaluated against human reading behavior (e.g., eye movement) to build\nhuman-like computational models. However, most previous efforts have focused\nalmost exclusively on English, despite the recent trend towards linguistic\nuniversal within the general community. In order to fill the gap, this paper\ninvestigates whether the established results in computational psycholinguistics\ncan be generalized across languages. Specifically, we re-examine an established\ngeneralization -- the lower perplexity a language model has, the more\nhuman-like the language model is -- in Japanese with typologically different\nstructures from English. Our experiments demonstrate that this established\ngeneralization exhibits a surprising lack of universality; namely, lower\nperplexity is not always human-like. Moreover, this discrepancy between English\nand Japanese is further explored from the perspective of (non-)uniform\ninformation density. Overall, our results suggest that a cross-lingual\nevaluation will be necessary to construct human-like computational models.", "published": "2021-06-02 15:27:29", "link": "http://arxiv.org/abs/2106.01229v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Identifiable yet Equally Performant Transformers for Text\n  Classification", "abstract": "Interpretability is an important aspect of the trustworthiness of a model's\npredictions. Transformer's predictions are widely explained by the attention\nweights, i.e., a probability distribution generated at its self-attention unit\n(head). Current empirical studies provide shreds of evidence that attention\nweights are not explanations by proving that they are not unique. A recent\nstudy showed theoretical justifications to this observation by proving the\nnon-identifiability of attention weights. For a given input to a head and its\noutput, if the attention weights generated in it are unique, we call the\nweights identifiable. In this work, we provide deeper theoretical analysis and\nempirical observations on the identifiability of attention weights. Ignored in\nthe previous works, we find the attention weights are more identifiable than we\ncurrently perceive by uncovering the hidden role of the key vector. However,\nthe weights are still prone to be non-unique attentions that make them unfit\nfor interpretation. To tackle this issue, we provide a variant of the encoder\nlayer that decouples the relationship between key and value vector and provides\nidentifiable weights up to the desired length of the input. We prove the\napplicability of such variations by providing empirical justifications on\nvaried text classification tasks. The implementations are available at\nhttps://github.com/declare-lab/identifiable-transformers.", "published": "2021-06-02 16:21:38", "link": "http://arxiv.org/abs/2106.01269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Distribution, Sparsity, and Inference-time Quantization of\n  Attention Values in Transformers", "abstract": "How much information do NLP tasks really need from a transformer's attention\nmechanism at application-time (inference)? From recent work, we know that there\nis sparsity in transformers and that the floating-points within its computation\ncan be discretized to fewer values with minimal loss to task accuracies.\nHowever, this requires retraining or even creating entirely new models, both of\nwhich can be expensive and carbon-emitting. Focused on optimizations that do\nnot require training, we systematically study the full range of typical\nattention values necessary. This informs the design of an inference-time\nquantization technique using both pruning and log-scaled mapping which produces\nonly a few (e.g. $2^3$) unique values. Over the tasks of question answering and\nsentiment analysis, we find nearly 80% of attention values can be pruned to\nzeros with minimal ($< 1.0\\%$) relative loss in accuracy. We use this pruning\ntechnique in conjunction with quantizing the attention values to only a 3-bit\nformat, without retraining, resulting in only a 0.8% accuracy reduction on\nquestion answering with fine-tuned RoBERTa.", "published": "2021-06-02 17:45:47", "link": "http://arxiv.org/abs/2106.01335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lightweight Adapter Tuning for Multilingual Speech Translation", "abstract": "Adapter modules were recently introduced as an efficient alternative to\nfine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters\nof a model and injecting lightweight modules between layers, resulting in the\naddition of only a small number of task-specific trainable parameters. While\nadapter tuning was investigated for multilingual neural machine translation,\nthis paper proposes a comprehensive analysis of adapters for multilingual\nspeech translation (ST). Starting from different pre-trained models (a\nmultilingual ST trained on parallel data or a multilingual BART (mBART) trained\non non-parallel multilingual data), we show that adapters can be used to: (a)\nefficiently specialize ST to specific language pairs with a low extra cost in\nterms of parameters, and (b) transfer from an automatic speech recognition\n(ASR) task and an mBART pre-trained model to a multilingual ST task.\nExperiments show that adapter tuning offer competitive results to full\nfine-tuning, while being much more parameter-efficient.", "published": "2021-06-02 20:51:42", "link": "http://arxiv.org/abs/2106.01463v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Efficacy of Summarization Evaluation across Languages", "abstract": "While automatic summarization evaluation methods developed for English are\nroutinely applied to other languages, this is the first attempt to\nsystematically quantify their panlinguistic efficacy. We take a summarization\ncorpus for eight different languages, and manually annotate generated summaries\nfor focus (precision) and coverage (recall). Based on this, we evaluate 19\nsummarization evaluation metrics, and find that using multilingual BERT within\nBERTScore performs well across all languages, at a level above that for\nEnglish.", "published": "2021-06-02 21:28:01", "link": "http://arxiv.org/abs/2106.01478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Passage Retrieval with Hashing for Open-domain Question\n  Answering", "abstract": "Most state-of-the-art open-domain question answering systems use a neural\nretrieval model to encode passages into continuous vectors and extract them\nfrom a knowledge source. However, such retrieval models often require large\nmemory to run because of the massive size of their passage index. In this\npaper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural\nretrieval model that integrates a learning-to-hash technique into the\nstate-of-the-art Dense Passage Retriever (DPR) to represent the passage index\nusing compact binary codes rather than continuous vectors. BPR is trained with\na multi-task objective over two tasks: efficient candidate generation based on\nbinary codes and accurate reranking based on continuous vectors. Compared with\nDPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss\nof accuracy on two standard open-domain question answering benchmarks: Natural\nQuestions and TriviaQA. Our code and trained models are available at\nhttps://github.com/studio-ousia/bpr.", "published": "2021-06-02 01:34:42", "link": "http://arxiv.org/abs/2106.00882v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploiting Global Contextual Information for Document-level Named Entity\n  Recognition", "abstract": "Most existing named entity recognition (NER) approaches are based on sequence\nlabeling models, which focus on capturing the local context dependencies.\nHowever, the way of taking one sentence as input prevents the modeling of\nnon-sequential global context, which is useful especially when local context\ninformation is limited or ambiguous. To this end, we propose a model called\nGlobal Context enhanced Document-level NER (GCDoc) to leverage global\ncontextual information from two levels, i.e., both word and sentence. At\nword-level, a document graph is constructed to model a wider range of\ndependencies between words, then obtain an enriched contextual representation\nfor each word via graph neural networks (GNN). To avoid the interference of\nnoise information, we further propose two strategies. First we apply the\nepistemic uncertainty theory to find out tokens whose representations are less\nreliable, thereby helping prune the document graph. Then a selective auxiliary\nclassifier is proposed to effectively learn the weight of edges in document\ngraph and reduce the importance of noisy neighbour nodes. At sentence-level,\nfor appropriately modeling wider context beyond single sentence, we employ a\ncross-sentence module which encodes adjacent sentences and fuses it with the\ncurrent sentence representation via attention and gating mechanisms. Extensive\nexperiments on two benchmark NER datasets (CoNLL 2003 and Ontonotes 5.0 English\ndataset) demonstrate the effectiveness of our proposed model. Our model reaches\nF1 score of 92.22 (93.40 with BERT) on CoNLL 2003 dataset and 88.32 (90.49 with\nBERT) on Ontonotes 5.0 dataset, achieving new state-of-the-art performance.", "published": "2021-06-02 01:52:07", "link": "http://arxiv.org/abs/2106.00887v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "High-Quality Diversification for Task-Oriented Dialogue Systems", "abstract": "Many task-oriented dialogue systems use deep reinforcement learning (DRL) to\nlearn policies that respond to the user appropriately and complete the tasks\nsuccessfully. Training DRL agents with diverse dialogue trajectories prepare\nthem well for rare user requests and unseen situations. One effective\ndiversification method is to let the agent interact with a diverse set of\nlearned user models. However, trajectories created by these artificial user\nmodels may contain generation errors, which can quickly propagate into the\nagent's policy. It is thus important to control the quality of the\ndiversification and resist the noise. In this paper, we propose a novel\ndialogue diversification method for task-oriented dialogue systems trained in\nsimulators. Our method, Intermittent Short Extension Ensemble (I-SEE),\nconstrains the intensity to interact with an ensemble of diverse user models\nand effectively controls the quality of the diversification. Evaluations on the\nMultiwoz dataset show that I-SEE successfully boosts the performance of several\nstate-of-the-art DRL dialogue agents.", "published": "2021-06-02 02:10:07", "link": "http://arxiv.org/abs/2106.00891v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in\n  Non-Autoregressive Translation", "abstract": "Knowledge distillation (KD) is commonly used to construct synthetic data for\ntraining non-autoregressive translation (NAT) models. However, there exists a\ndiscrepancy on low-frequency words between the distilled and the original data,\nleading to more errors on predicting low-frequency words. To alleviate the\nproblem, we directly expose the raw data into NAT by leveraging pretraining. By\nanalyzing directed alignments, we found that KD makes low-frequency source\nwords aligned with targets more deterministically but fails to align sufficient\nlow-frequency words from target to source. Accordingly, we propose reverse KD\nto rejuvenate more alignments for low-frequency target words. To make the most\nof authentic and synthetic data, we combine these complementary approaches as a\nnew training strategy for further boosting NAT performance. We conduct\nexperiments on five translation benchmarks over two advanced architectures.\nResults demonstrate that the proposed approach can significantly and\nuniversally improve translation quality by reducing translation errors on\nlow-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU\npoints on the WMT14 English-German and WMT16 Romanian-English datasets,\nrespectively. Our code, data, and trained models are available at\n\\url{https://github.com/alphadl/RLFW-NAT}.", "published": "2021-06-02 02:41:40", "link": "http://arxiv.org/abs/2106.00903v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Out-of-Domain Detection via Pre-trained Transformers", "abstract": "Deployed real-world machine learning applications are often subject to\nuncontrolled and even potentially malicious inputs. Those out-of-domain inputs\ncan lead to unpredictable outputs and sometimes catastrophic safety issues.\nPrior studies on out-of-domain detection require in-domain task labels and are\nlimited to supervised classification scenarios. Our work tackles the problem of\ndetecting out-of-domain samples with only unsupervised in-domain data. We\nutilize the latent representations of pre-trained transformers and propose a\nsimple yet effective method to transform features across all layers to\nconstruct out-of-domain detectors efficiently. Two domain-specific fine-tuning\napproaches are further proposed to boost detection accuracy. Our empirical\nevaluations of related methods on two datasets validate that our method greatly\nimproves out-of-domain detection ability in a more general scenario.", "published": "2021-06-02 05:21:25", "link": "http://arxiv.org/abs/2106.00948v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When and Why does a Model Fail? A Human-in-the-loop Error Detection\n  Framework for Sentiment Analysis", "abstract": "Although deep neural networks have been widely employed and proven effective\nin sentiment analysis tasks, it remains challenging for model developers to\nassess their models for erroneous predictions that might exist prior to\ndeployment. Once deployed, emergent errors can be hard to identify in\nprediction run-time and impossible to trace back to their sources. To address\nsuch gaps, in this paper we propose an error detection framework for sentiment\nanalysis based on explainable features. We perform global-level feature\nvalidation with human-in-the-loop assessment, followed by an integration of\nglobal and local-level feature contribution analysis. Experimental results show\nthat, given limited human-in-the-loop intervention, our method is able to\nidentify erroneous model predictions on unseen data with high precision.", "published": "2021-06-02 05:45:42", "link": "http://arxiv.org/abs/2106.00954v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "COM2SENSE: A Commonsense Reasoning Benchmark with Complementary\n  Sentences", "abstract": "Commonsense reasoning is intuitive for humans but has been a long-term\nchallenge for artificial intelligence (AI). Recent advancements in pretrained\nlanguage models have shown promising results on several commonsense benchmark\ndatasets. However, the reliability and comprehensiveness of these benchmarks\ntowards assessing model's commonsense reasoning ability remains unclear. To\nthis end, we introduce a new commonsense reasoning benchmark dataset comprising\nnatural language true/false statements, with each sample paired with its\ncomplementary counterpart, resulting in 4k sentence pairs. We propose a\npairwise accuracy metric to reliably measure an agent's ability to perform\ncommonsense reasoning over a given situation. The dataset is crowdsourced and\nenhanced with an adversarial model-in-the-loop setup to incentivize challenging\nsamples. To facilitate a systematic analysis of commonsense capabilities, we\ndesign our dataset along the dimensions of knowledge domains, reasoning\nscenarios and numeracy. Experimental results demonstrate that our strongest\nbaseline (UnifiedQA-3B), after fine-tuning, achieves ~71% standard accuracy and\n~51% pairwise accuracy, well below human performance (~95% for both metrics).\nThe dataset is available at https://github.com/PlusLabNLP/Com2Sense.", "published": "2021-06-02 06:31:55", "link": "http://arxiv.org/abs/2106.00969v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SocAoG: Incremental Graph Parsing for Social Relation Inference in\n  Dialogues", "abstract": "Inferring social relations from dialogues is vital for building emotionally\nintelligent robots to interpret human language better and act accordingly. We\nmodel the social network as an And-or Graph, named SocAoG, for the consistency\nof relations among a group and leveraging attributes as inference cues.\nMoreover, we formulate a sequential structure prediction task, and propose an\n$\\alpha$-$\\beta$-$\\gamma$ strategy to incrementally parse SocAoG for the\ndynamic inference upon any incoming utterance: (i) an $\\alpha$ process\npredicting attributes and relations conditioned on the semantics of dialogues,\n(ii) a $\\beta$ process updating the social relations based on related\nattributes, and (iii) a $\\gamma$ process updating individual's attributes based\non interpersonal social relations. Empirical results on DialogRE and MovieGraph\nshow that our model infers social relations more accurately than the\nstate-of-the-art methods. Moreover, the ablation study shows the three\nprocesses complement each other, and the case study demonstrates the dynamic\nrelational inference.", "published": "2021-06-02 08:07:42", "link": "http://arxiv.org/abs/2106.01006v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "T-BERT -- Model for Sentiment Analysis of Micro-blogs Integrating Topic\n  Model and BERT", "abstract": "Sentiment analysis (SA) has become an extensive research area in recent years\nimpacting diverse fields including ecommerce, consumer business, and politics,\ndriven by increasing adoption and usage of social media platforms. It is\nchallenging to extract topics and sentiments from unsupervised short texts\nemerging in such contexts, as they may contain figurative words, strident data,\nand co-existence of many possible meanings for a single word or phrase, all\ncontributing to obtaining incorrect topics. Most prior research is based on a\nspecific theme/rhetoric/focused-content on a clean dataset. In the work\nreported here, the effectiveness of BERT(Bidirectional Encoder Representations\nfrom Transformers) in sentiment classification tasks from a raw live dataset\ntaken from a popular microblogging platform is demonstrated. A novel T-BERT\nframework is proposed to show the enhanced performance obtainable by combining\nlatent topics with contextual BERT embeddings. Numerical experiments were\nconducted on an ensemble with about 42000 datasets using NimbleBox.ai platform\nwith a hardware configuration consisting of Nvidia Tesla K80(CUDA), 4 core CPU,\n15GB RAM running on an isolated Google Cloud Platform instance. The empirical\nresults show that the model improves in performance while adding topics to BERT\nand an accuracy rate of 90.81% on sentiment classification using BERT with the\nproposed approach.", "published": "2021-06-02 12:01:47", "link": "http://arxiv.org/abs/2106.01097v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact\n  Verification", "abstract": "Fact verification is a challenging task that requires simultaneously\nreasoning and aggregating over multiple retrieved pieces of evidence to\nevaluate the truthfulness of a claim. Existing approaches typically (i) explore\nthe semantic interaction between the claim and evidence at different\ngranularity levels but fail to capture their topical consistency during the\nreasoning process, which we believe is crucial for verification; (ii) aggregate\nmultiple pieces of evidence equally without considering their implicit stances\nto the claim, thereby introducing spurious information. To alleviate the above\nissues, we propose a novel topic-aware evidence reasoning and stance-aware\naggregation model for more accurate fact verification, with the following four\nkey properties: 1) checking topical consistency between the claim and evidence;\n2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring\nsemantic similarity between the global topic information and the semantic\nrepresentation of evidence; 4) aggregating evidence based on their implicit\nstances to the claim. Extensive experiments conducted on the two benchmark\ndatasets demonstrate the superiority of the proposed model over several\nstate-of-the-art approaches for fact verification. The source code can be\nobtained from https://github.com/jasenchn/TARSA.", "published": "2021-06-02 14:33:12", "link": "http://arxiv.org/abs/2106.01191v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Figurative Language in Recognizing Textual Entailment", "abstract": "We introduce a collection of recognizing textual entailment (RTE) datasets\nfocused on figurative language. We leverage five existing datasets annotated\nfor a variety of figurative language -- simile, metaphor, and irony -- and\nframe them into over 12,500 RTE examples.We evaluate how well state-of-the-art\nmodels trained on popular RTE datasets capture different aspects of figurative\nlanguage. Our results and analyses indicate that these models might not\nsufficiently capture figurative language, struggling to perform pragmatic\ninference and reasoning about world knowledge. Ultimately, our datasets provide\na challenging testbed for evaluating RTE models.", "published": "2021-06-02 14:37:32", "link": "http://arxiv.org/abs/2106.01195v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Differential Privacy for Text Analytics via Natural Text Sanitization", "abstract": "Texts convey sophisticated knowledge. However, texts also convey sensitive\ninformation. Despite the success of general-purpose language models and\ndomain-specific mechanisms with differential privacy (DP), existing text\nsanitization mechanisms still provide low utility, as cursed by the\nhigh-dimensional text representation. The companion issue of utilizing\nsanitized texts for downstream analytics is also under-explored. This paper\ntakes a direct approach to text sanitization. Our insight is to consider both\nsensitivity and similarity via our new local DP notion. The sanitized texts\nalso contribute to our sanitization-aware pretraining and fine-tuning, enabling\nprivacy-preserving natural language processing over the BERT language model\nwith promising utility. Surprisingly, the high utility does not boost up the\nsuccess rate of inference attacks.", "published": "2021-06-02 15:15:10", "link": "http://arxiv.org/abs/2106.01221v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Multilingual Medical Question Answering and Information Retrieval for\n  Rural Health Intelligence Access", "abstract": "In rural regions of several developing countries, access to quality\nhealthcare, medical infrastructure, and professional diagnosis is largely\nunavailable. Many of these regions are gradually gaining access to internet\ninfrastructure, although not with a strong enough connection to allow for\nsustained communication with a medical practitioner. Several deaths resulting\nfrom this lack of medical access, absence of patient's previous health records,\nand the unavailability of information in indigenous languages can be easily\nprevented. In this paper, we describe an approach leveraging the phenomenal\nprogress in Machine Learning and NLP (Natural Language Processing) techniques\nto design a model that is low-resource, multilingual, and a preliminary\nfirst-point-of-contact medical assistant. Our contribution includes defining\nthe NLP pipeline required for named-entity-recognition, language-agnostic\nsentence embedding, natural language translation, information retrieval,\nquestion answering, and generative pre-training for final query processing. We\nobtain promising results for this pipeline and preliminary results for EHR\n(Electronic Health Record) analysis with text summarization for medical\npractitioners to peruse for their diagnosis. Through this NLP pipeline, we aim\nto provide preliminary medical information to the user and do not claim to\nsupplant diagnosis from qualified medical practitioners. Using the input from\nsubject matter experts, we have compiled a large corpus to pre-train and\nfine-tune our BioBERT based NLP model for the specific tasks. We expect recent\nadvances in NLP architectures, several of which are efficient and\nprivacy-preserving models, to further the impact of our solution and improve on\nindividual task performance.", "published": "2021-06-02 16:05:24", "link": "http://arxiv.org/abs/2106.01251v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Uni-Encoder: A Fast and Accurate Response Selection Paradigm for\n  Generation-Based Dialogue Systems", "abstract": "Sample-and-rank is a key decoding strategy for modern generation-based\ndialogue systems. It helps achieve diverse and high-quality responses by\nselecting an answer from a small pool of generated candidates. The current\nstate-of-the-art ranking methods mainly use an encoding paradigm called\nCross-Encoder, which separately encodes each context-candidate pair and ranks\nthe candidates according to their fitness scores. However, Cross-Encoder\nrepeatedly encodes the same lengthy context for each candidate, resulting in\nhigh computational costs. Poly-Encoder addresses the above problems by reducing\nthe interaction between context and candidates, but with a price of performance\ndrop. In this work, we develop a new paradigm called Uni-Encoder, that keeps\nthe full attention over each pair as in Cross-Encoder while only encoding the\ncontext once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with\nthe context in one forward pass. We use the same positional embedding for all\ncandidates to ensure they are treated equally and design a new attention\nmechanism to avoid confusion. Our Uni-Encoder can simulate other ranking\nparadigms using different attention and response concatenation methods.\nExtensive experiments show that our proposed paradigm achieves new\nstate-of-the-art results on four benchmark datasets with high computational\nefficiency. For instance, it improves R10@1 by 2.9% with an approximately 4X\nfaster inference speed on the Ubuntu V2 dataset.", "published": "2021-06-02 16:14:51", "link": "http://arxiv.org/abs/2106.01263v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Select: A Fully Attentive Approach for Novel Object\n  Captioning", "abstract": "Image captioning models have lately shown impressive results when applied to\nstandard datasets. Switching to real-life scenarios, however, constitutes a\nchallenge due to the larger variety of visual concepts which are not covered in\nexisting training sets. For this reason, novel object captioning (NOC) has\nrecently emerged as a paradigm to test captioning models on objects which are\nunseen during the training phase. In this paper, we present a novel approach\nfor NOC that learns to select the most relevant objects of an image, regardless\nof their adherence to the training set, and to constrain the generative process\nof a language model accordingly. Our architecture is fully-attentive and\nend-to-end trainable, also when incorporating constraints. We perform\nexperiments on the held-out COCO dataset, where we demonstrate improvements\nover the state of the art, both in terms of adaptability to novel objects and\ncaption quality.", "published": "2021-06-02 19:11:21", "link": "http://arxiv.org/abs/2106.01424v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption\n  Evaluation via Typicality Analysis", "abstract": "The open-ended nature of visual captioning makes it a challenging area for\nevaluation. The majority of proposed models rely on specialized training to\nimprove human-correlation, resulting in limited adoption, generalizability, and\nexplainabilty. We introduce \"typicality\", a new formulation of evaluation\nrooted in information theory, which is uniquely suited for problems lacking a\ndefinite ground truth. Typicality serves as our framework to develop a novel\nsemantic comparison, SPARCS, as well as referenceless fluency evaluation\nmetrics. Over the course of our analysis, two separate dimensions of fluency\nnaturally emerge: style, captured by metric SPURTS, and grammar, captured in\nthe form of grammatical outlier penalties. Through extensive experiments and\nablation studies on benchmark datasets, we show how these decomposed dimensions\nof semantics and fluency provide greater system-level insight into captioner\ndifferences. Our proposed metrics along with their combination, SMURF, achieve\nstate-of-the-art correlation with human judgment when compared with other\nrule-based evaluation metrics.", "published": "2021-06-02 19:58:20", "link": "http://arxiv.org/abs/2106.01444v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Attention-based Contextual Language Model Adaptation for Speech\n  Recognition", "abstract": "Language modeling (LM) for automatic speech recognition (ASR) does not\nusually incorporate utterance level contextual information. For some domains\nlike voice assistants, however, additional context, such as the time at which\nan utterance was spoken, provides a rich input signal. We introduce an\nattention mechanism for training neural speech recognition language models on\nboth text and non-linguistic contextual data. When applied to a large\nde-identified dataset of utterances collected by a popular voice assistant\nplatform, our method reduces perplexity by 7.0% relative over a standard LM\nthat does not incorporate contextual information. When evaluated on utterances\nextracted from the long tail of the dataset, our method improves perplexity by\n9.0% relative over a standard LM and by over 2.8% relative when compared to a\nstate-of-the-art model for contextual LM.", "published": "2021-06-02 20:19:57", "link": "http://arxiv.org/abs/2106.01451v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively\n  Inspired Orthographic Adversarial Attacks", "abstract": "Adversarial attacks expose important blind spots of deep learning systems.\nWhile word- and sentence-level attack scenarios mostly deal with finding\nsemantic paraphrases of the input that fool NLP models, character-level attacks\ntypically insert typos into the input stream. It is commonly thought that these\nare easier to defend via spelling correction modules. In this work, we show\nthat both a standard spellchecker and the approach of Pruthi et al. (2019),\nwhich trains to defend against insertions, deletions and swaps, perform poorly\non the character-level benchmark recently proposed in Eger and Benz (2020)\nwhich includes more challenging attacks such as visual and phonetic\nperturbations and missing word segmentations. In contrast, we show that an\nuntrained iterative approach which combines context-independent character-level\ninformation with context-dependent information from BERT's masked language\nmodeling can perform on par with human crowd-workers from Amazon Mechanical\nTurk (AMT) supervised via 3-shot learning.", "published": "2021-06-02 20:21:03", "link": "http://arxiv.org/abs/2106.01452v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedNLI Is Not Immune: Natural Language Inference Artifacts in the\n  Clinical Domain", "abstract": "Crowdworker-constructed natural language inference (NLI) datasets have been\nfound to contain statistical artifacts associated with the annotation process\nthat allow hypothesis-only classifiers to achieve better-than-random\nperformance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018).\nWe investigate whether MedNLI, a physician-annotated dataset with premises\nextracted from clinical notes, contains such artifacts (Romanov and Shivade,\n2018). We find that entailed hypotheses contain generic versions of specific\nconcepts in the premise, as well as modifiers related to responsiveness,\nduration, and probability. Neutral hypotheses feature conditions and behaviors\nthat co-occur with, or cause, the condition(s) in the premise. Contradiction\nhypotheses feature explicit negation of the premise and implicit negation via\nassertion of good health. Adversarial filtering demonstrates that performance\ndegrades when evaluated on the difficult subset. We provide partition\ninformation and recommendations for alternative dataset construction strategies\nfor knowledge-intensive domains.", "published": "2021-06-02 22:12:39", "link": "http://arxiv.org/abs/2106.01491v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowing More About Questions Can Help: Improving Calibration in Question\n  Answering", "abstract": "We study calibration in question answering, estimating whether model\ncorrectly predicts answer for each question. Unlike prior work which mainly\nrely on the model's confidence score, our calibrator incorporates information\nabout the input example (e.g., question and the evidence context). Together\nwith data augmentation via back translation, our simple approach achieves 5-10%\ngains in calibration accuracy on reading comprehension benchmarks. Furthermore,\nwe present the first calibration study in the open retrieval setting, comparing\nthe calibration accuracy of retrieval-based span prediction models and answer\ngeneration models. Here again, our approach shows consistent gains over\ncalibrators relying on the model confidence. Our simple and efficient\ncalibrator can be easily adapted to many tasks and model architectures, showing\nrobust gains in all settings.", "published": "2021-06-02 22:22:52", "link": "http://arxiv.org/abs/2106.01494v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in\n  Mathematics Education", "abstract": "Since the introduction of the original BERT (i.e., BASE BERT), researchers\nhave developed various customized BERT models with improved performance for\nspecific domains and tasks by exploiting the benefits of transfer learning. Due\nto the nature of mathematical texts, which often use domain specific vocabulary\nalong with equations and math symbols, we posit that the development of a new\nBERT model for mathematics would be useful for many mathematical downstream\ntasks. In this resource paper, we introduce our multi-institutional effort\n(i.e., two learning platforms and three academic institutions in the US) toward\nthis need: MathBERT, a model created by pre-training the BASE BERT model on a\nlarge mathematical corpus ranging from pre-kindergarten (pre-k), to\nhigh-school, to college graduate level mathematical content. In addition, we\nselect three general NLP tasks that are often used in mathematics education:\nprediction of knowledge component, auto-grading open-ended Q&A, and knowledge\ntracing, to demonstrate the superiority of MathBERT over BASE BERT. Our\nexperiments show that MathBERT outperforms prior best methods by 1.2-22% and\nBASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific\nvocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT\npre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT\nvocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the\nparticipated leaning platforms: Stride, Inc, a commercial educational resource\nprovider, and ASSISTments.org, a free online educational platform. We release\nMathBERT for public usage at: https://github.com/tbs17/MathBERT.", "published": "2021-06-02 02:43:18", "link": "http://arxiv.org/abs/2106.07340v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Efficacy of Adversarial Data Collection for Question Answering:\n  Results from a Large-Scale Randomized Study", "abstract": "In adversarial data collection (ADC), a human workforce interacts with a\nmodel in real time, attempting to produce examples that elicit incorrect\npredictions. Researchers hope that models trained on these more challenging\ndatasets will rely less on superficial patterns, and thus be less brittle.\nHowever, despite ADC's intuitive appeal, it remains unclear when training on\nadversarial datasets produces more robust models. In this paper, we conduct a\nlarge-scale controlled study focused on question answering, assigning workers\nat random to compose questions either (i) adversarially (with a model in the\nloop); or (ii) in the standard fashion (without a model). Across a variety of\nmodels and datasets, we find that models trained on adversarial data usually\nperform better on other adversarial datasets but worse on a diverse collection\nof out-of-domain evaluation sets. Finally, we provide a qualitative analysis of\nadversarial (vs standard) data, identifying key differences and offering\nguidance for future research.", "published": "2021-06-02 00:48:33", "link": "http://arxiv.org/abs/2106.00872v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conversational Question Answering: A Survey", "abstract": "Question answering (QA) systems provide a way of querying the information\navailable in various formats including, but not limited to, unstructured and\nstructured data in natural languages. It constitutes a considerable part of\nconversational artificial intelligence (AI) which has led to the introduction\nof a special research topic on Conversational Question Answering (CQA), wherein\na system is required to understand the given context and then engages in\nmulti-turn QA to satisfy the user's information needs. Whilst the focus of most\nof the existing research work is subjected to single-turn QA, the field of\nmulti-turn QA has recently grasped attention and prominence owing to the\navailability of large-scale, multi-turn QA datasets and the development of\npre-trained language models. With a good amount of models and research papers\nadding to the literature every year recently, there is a dire need of arranging\nand presenting the related work in a unified manner to streamline future\nresearch. This survey, therefore, is an effort to present a comprehensive\nreview of the state-of-the-art research trends of CQA primarily based on\nreviewed papers from 2016-2021. Our findings show that there has been a trend\nshift from single-turn to multi-turn QA which empowers the field of\nConversational AI from different perspectives. This survey is intended to\nprovide an epitome for the research community with the hope of laying a strong\nfoundation for the field of CQA.", "published": "2021-06-02 01:06:34", "link": "http://arxiv.org/abs/2106.00874v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DialoGraph: Incorporating Interpretable Strategy-Graph Networks into\n  Negotiation Dialogues", "abstract": "To successfully negotiate a deal, it is not enough to communicate fluently:\npragmatic planning of persuasive negotiation strategies is essential. While\nmodern dialogue agents excel at generating fluent sentences, they still lack\npragmatic grounding and cannot reason strategically. We present DialoGraph, a\nnegotiation system that incorporates pragmatic strategies in a negotiation\ndialogue using graph neural networks. DialoGraph explicitly incorporates\ndependencies between sequences of strategies to enable improved and\ninterpretable prediction of next optimal strategies, given the dialogue\ncontext. Our graph-based method outperforms prior state-of-the-art negotiation\nmodels both in the accuracy of strategy/dialogue act prediction and in the\nquality of downstream dialogue response generation. We qualitatively show\nfurther benefits of learned strategy-graphs in providing explicit associations\nbetween effective negotiation strategies over the course of the dialogue,\nleading to interpretable and strategic dialogues.", "published": "2021-06-02 03:34:36", "link": "http://arxiv.org/abs/2106.00920v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Training Sampling with Monolingual Data Uncertainty for Neural\n  Machine Translation", "abstract": "Self-training has proven effective for improving NMT performance by\naugmenting model training with synthetic parallel data. The common practice is\nto construct synthetic data based on a randomly sampled subset of large-scale\nmonolingual data, which we empirically show is sub-optimal. In this work, we\npropose to improve the sampling procedure by selecting the most informative\nmonolingual sentences to complement the parallel data. To this end, we compute\nthe uncertainty of monolingual sentences using the bilingual dictionary\nextracted from the parallel data. Intuitively, monolingual sentences with lower\nuncertainty generally correspond to easy-to-translate patterns which may not\nprovide additional gains. Accordingly, we design an uncertainty-based sampling\nstrategy to efficiently exploit the monolingual data for self-training, in\nwhich monolingual sentences with higher uncertainty would be sampled with\nhigher probability. Experimental results on large-scale WMT\nEnglish$\\Rightarrow$German and English$\\Rightarrow$Chinese datasets demonstrate\nthe effectiveness of the proposed approach. Extensive analyses suggest that\nemphasizing the learning on uncertain monolingual sentences by our approach\ndoes improve the translation quality of high-uncertainty sentences and also\nbenefits the prediction of low-frequency words at the target side.", "published": "2021-06-02 05:01:36", "link": "http://arxiv.org/abs/2106.00941v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Few-Shot Partial-Label Learning", "abstract": "Partial-label learning (PLL) generally focuses on inducing a noise-tolerant\nmulti-class classifier by training on overly-annotated samples, each of which\nis annotated with a set of labels, but only one is the valid label. A basic\npromise of existing PLL solutions is that there are sufficient partial-label\n(PL) samples for training. However, it is more common than not to have just few\nPL samples at hand when dealing with new tasks. Furthermore, existing few-shot\nlearning algorithms assume precise labels of the support set; as such,\nirrelevant labels may seriously mislead the meta-learner and thus lead to a\ncompromised performance. How to enable PLL under a few-shot learning setting is\nan important problem, but not yet well studied. In this paper, we introduce an\napproach called FsPLL (Few-shot PLL). FsPLL first performs adaptive distance\nmetric learning by an embedding network and rectifying prototypes on the tasks\npreviously encountered. Next, it calculates the prototype of each class of a\nnew task in the embedding network. An unseen example can then be classified via\nits distance to each prototype. Experimental results on widely-used few-shot\ndatasets (Omniglot and miniImageNet) demonstrate that our FsPLL can achieve a\nsuperior performance than the state-of-the-art methods across different\nsettings, and it needs fewer samples for quickly adapting to new tasks.", "published": "2021-06-02 07:03:54", "link": "http://arxiv.org/abs/2106.00984v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment\n  Extraction in News Text", "abstract": "Understanding who blames or supports whom in news text is a critical research\nquestion in computational social science. Traditional methods and datasets for\nsentiment analysis are, however, not suitable for the domain of political text\nas they do not consider the direction of sentiments expressed between entities.\nIn this paper, we propose a novel NLP task of identifying directed sentiment\nrelationship between political entities from a given news document, which we\ncall directed sentiment extraction. From a million-scale news corpus, we\nconstruct a dataset of news sentences where sentiment relations of political\nentities are manually annotated. We present a simple but effective approach for\nutilizing a pretrained transformer, which infers the target class by predicting\nmultiple question-answering tasks and combining the outcomes. We demonstrate\nthe utility of our proposed method for social science research questions by\nanalyzing positive and negative opinions between political entities in two\nmajor events: 2016 U.S. presidential election and COVID-19. The newly proposed\nproblem, data, and method will facilitate future studies on interdisciplinary\nNLP methods and applications.", "published": "2021-06-02 09:02:14", "link": "http://arxiv.org/abs/2106.01033v2", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Evidence-based Factual Error Correction", "abstract": "This paper introduces the task of factual error correction: performing edits\nto a claim so that the generated rewrite is better supported by evidence. This\nextends the well-studied task of fact verification by providing a mechanism to\ncorrect written texts that are refuted or only partially supported by evidence.\nWe demonstrate that it is feasible to train factual error correction systems\nfrom existing fact checking datasets which only contain labeled claims\naccompanied by evidence, but not the correction. We achieve this by employing a\ntwo-stage distant supervision approach that incorporates evidence into masked\nclaims when generating corrections. Our approach, based on the T5 transformer\nand using retrieved evidence, achieved better results than existing work which\nused a pointer copy network and gold evidence, producing accurate factual error\ncorrections for 5x more instances in human evaluation and a .125 increase in\nSARI score. The evaluation is conducted on a dataset of 65,000 instances based\non a recent fact verification shared task and we release it to enable further\nwork on the task.", "published": "2021-06-02 11:00:17", "link": "http://arxiv.org/abs/2106.01072v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Database Reasoning Over Text", "abstract": "Neural models have shown impressive performance gains in answering queries\nfrom natural language text. However, existing works are unable to support\ndatabase queries, such as \"List/Count all female athletes who were born in 20th\ncentury\", which require reasoning over sets of relevant facts with operations\nsuch as join, filtering and aggregation. We show that while state-of-the-art\ntransformer models perform very well for small databases, they exhibit\nlimitations in processing noisy data, numerical operations, and queries that\naggregate facts. We propose a modular architecture to answer these\ndatabase-style queries over multiple spans from text and aggregating these at\nscale. We evaluate the architecture using WikiNLDB, a novel dataset for\nexploring such queries. Our architecture scales to databases containing\nthousands of facts whereas contemporary models are limited by how many facts\ncan be encoded. In direct comparison on small databases, our approach increases\noverall answer accuracy from 85% to 90%. On larger databases, our approach\nretains its accuracy whereas transformer baselines could not encode the\ncontext.", "published": "2021-06-02 11:09:40", "link": "http://arxiv.org/abs/2106.01074v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Improving low-resource ASR performance with untranscribed out-of-domain\n  data", "abstract": "Semi-supervised training (SST) is a common approach to leverage\nuntranscribed/unlabeled speech data to improve automatic speech recognition\nperformance in low-resource languages. However, if the available unlabeled\nspeech is mismatched to the target domain, SST is not as effective, and in many\ncases performs worse than the original system. In this paper, we address the\nissue of low-resource ASR when only untranscribed out-of-domain speech data is\nreadily available in the target language. Specifically, we look to improve\nperformance on conversational/telephony speech (target domain) using web\nresources, in particular YouTube data, which more closely resembles\nnews/topical broadcast data. Leveraging SST, we show that while in some cases\nsimply pooling the out-of-domain data with the training data lowers word error\nrate (WER), in all cases, we see improvements if we train first with the\nout-of-domain data and then fine-tune the resulting model with the original\ntraining data. Using 2000 hours of speed perturbed YouTube audio in each target\nlanguage, with semi-supervised transcripts, we show improvements on multiple\nlanguages/data sets, of up to 16.3% relative improvement in WER over the\nbaseline systems and up to 7.4% relative improvement in WER over a system that\nsimply pools the out-of-domain data with the training data.", "published": "2021-06-02 15:23:34", "link": "http://arxiv.org/abs/2106.01227v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enriching Transformers with Structured Tensor-Product Representations\n  for Abstractive Summarization", "abstract": "Abstractive summarization, the task of generating a concise summary of input\ndocuments, requires: (1) reasoning over the source document to determine the\nsalient pieces of information scattered across the long document, and (2)\ncomposing a cohesive text by reconstructing these salient facts into a shorter\nsummary that faithfully reflects the complex relations connecting these facts.\nIn this paper, we adapt TP-TRANSFORMER (Schlag et al., 2019), an architecture\nthat enriches the original Transformer (Vaswani et al., 2017) with the\nexplicitly compositional Tensor Product Representation (TPR), for the task of\nabstractive summarization. The key feature of our model is a structural bias\nthat we introduce by encoding two separate representations for each token to\nrepresent the syntactic structure (with role vectors) and semantic content\n(with filler vectors) separately. The model then binds the role and filler\nvectors into the TPR as the layer output. We argue that the structured\nintermediate representations enable the model to take better control of the\ncontents (salient facts) and structures (the syntax that connects the facts)\nwhen generating the summary. Empirically, we show that our TP-TRANSFORMER\noutperforms the Transformer and the original TP-TRANSFORMER significantly on\nseveral abstractive summarization datasets based on both automatic and human\nevaluations. On several syntactic and semantic probing tasks, we demonstrate\nthe emergent structural information in the role vectors and improved syntactic\ninterpretability in the TPR layer outputs. Code and models are available at\nhttps://github.com/jiangycTarheel/TPT-Summ.", "published": "2021-06-02 17:32:33", "link": "http://arxiv.org/abs/2106.01317v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "multiPRover: Generating Multiple Proofs for Improved Interpretability in\n  Rule Reasoning", "abstract": "We focus on a type of linguistic formal reasoning where the goal is to reason\nover explicit knowledge in the form of natural language facts and rules (Clark\net al., 2020). A recent work, named PRover (Saha et al., 2020), performs such\nreasoning by answering a question and also generating a proof graph that\nexplains the answer. However, compositional reasoning is not always unique and\nthere may be multiple ways of reaching the correct answer. Thus, in our work,\nwe address a new and challenging problem of generating multiple proof graphs\nfor reasoning over natural language rule-bases. Each proof provides a different\nrationale for the answer, thereby improving the interpretability of such\nreasoning systems. In order to jointly learn from all proof graphs and exploit\nthe correlations between multiple proofs for a question, we pose this task as a\nset generation problem over structured output spaces where each proof is\nrepresented as a directed graph. We propose two variants of a proof-set\ngeneration model, multiPRover. Our first model, Multilabel-multiPRover,\ngenerates a set of proofs via multi-label classification and implicit\nconditioning between the proofs; while the second model, Iterative-multiPRover,\ngenerates proofs iteratively by explicitly conditioning on the previously\ngenerated proofs. Experiments on multiple synthetic, zero-shot, and\nhuman-paraphrased datasets reveal that both multiPRover models significantly\noutperform PRover on datasets containing multiple gold proofs.\nIterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios\nwhere all examples have single correct proofs. It also generalizes better to\nquestions requiring higher depths of reasoning where multiple proofs are more\nfrequent. Our code and models are publicly available at\nhttps://github.com/swarnaHub/multiPRover", "published": "2021-06-02 17:58:35", "link": "http://arxiv.org/abs/2106.01354v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker\n  Identity in Dysarthric Voice Conversion", "abstract": "We propose a new paradigm for maintaining speaker identity in dysarthric\nvoice conversion (DVC). The poor quality of dysarthric speech can be greatly\nimproved by statistical VC, but as the normal speech utterances of a dysarthria\npatient are nearly impossible to collect, previous work failed to recover the\nindividuality of the patient. In light of this, we suggest a novel, two-stage\napproach for DVC, which is highly flexible in that no normal speech of the\npatient is required. First, a powerful parallel sequence-to-sequence model\nconverts the input dysarthric speech into a normal speech of a reference\nspeaker as an intermediate product, and a nonparallel, frame-wise VC model\nrealized with a variational autoencoder then converts the speaker identity of\nthe reference speech back to that of the patient while assumed to be capable of\npreserving the enhanced quality. We investigate several design options.\nExperimental evaluation results demonstrate the potential of our approach to\nimproving the quality of the dysarthric speech while maintaining the speaker\nidentity.", "published": "2021-06-02 18:41:03", "link": "http://arxiv.org/abs/2106.01415v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ethical-Advice Taker: Do Language Models Understand Natural Language\n  Interventions?", "abstract": "Is it possible to use natural language to intervene in a model's behavior and\nalter its prediction in a desired way? We investigate the effectiveness of\nnatural language interventions for reading-comprehension systems, studying this\nin the context of social stereotypes. Specifically, we propose a new language\nunderstanding task, Linguistic Ethical Interventions (LEI), where the goal is\nto amend a question-answering (QA) model's unethical behavior by communicating\ncontext-specific principles of ethics and equity to it. To this end, we build\nupon recent methods for quantifying a system's social stereotypes, augmenting\nthem with different kinds of ethical interventions and the desired model\nbehavior under such interventions. Our zero-shot evaluation finds that even\ntoday's powerful neural language models are extremely poor ethical-advice\ntakers, that is, they respond surprisingly little to ethical interventions even\nthough these interventions are stated as simple sentences. Few-shot learning\nimproves model behavior but remains far from the desired outcome, especially\nwhen evaluated for various types of generalization. Our new task thus poses a\nnovel language understanding challenge for the community.", "published": "2021-06-02 20:57:58", "link": "http://arxiv.org/abs/2106.01465v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying language changes surrounding mental health on Twitter", "abstract": "Mental health challenges are thought to afflict around 10% of the global\npopulation each year, with many going untreated due to stigma and limited\naccess to services. Here, we explore trends in words and phrases related to\nmental health through a collection of 1- , 2-, and 3-grams parsed from a data\nstream of roughly 10% of all English tweets since 2012. We examine temporal\ndynamics of mental health language, finding that the popularity of the phrase\n'mental health' increased by nearly two orders of magnitude between 2012 and\n2018. We observe that mentions of 'mental health' spike annually and reliably\ndue to mental health awareness campaigns, as well as unpredictably in response\nto mass shootings, celebrities dying by suicide, and popular fictional stories\nportraying suicide. We find that the level of positivity of messages containing\n'mental health', while stable through the growth period, has declined recently.\nFinally, we use the ratio of original tweets to retweets to quantify the\nfraction of appearances of mental health language due to social amplification.\nSince 2015, mentions of mental health have become increasingly due to retweets,\nsuggesting that stigma associated with discussion of mental health on Twitter\nhas diminished with time.", "published": "2021-06-02 21:35:53", "link": "http://arxiv.org/abs/2106.01481v1", "categories": ["physics.soc-ph", "cs.CL", "cs.SI"], "primary_category": "physics.soc-ph"}
{"title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and\n  Modelling Insights", "abstract": "Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the\nvarious linguistic peculiarities present in the language. The Sanskrit language\nis lexically productive, undergoes euphonic assimilation of phones at the word\nboundaries and exhibits variations in spelling conventions and in\npronunciations. In this work, we propose the first large scale study of\nautomatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact\nof unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR\ndataset for Sanskrit, which faithfully captures several of the linguistic\ncharacteristics expressed by the language. We investigate the role of different\nacoustic model and language model units in ASR systems for Sanskrit. We also\npropose a new modelling unit, inspired by the syllable level unit selection,\nthat captures character sequences from one vowel in the word to the next vowel.\nWe also highlight the importance of choosing graphemic representations for\nSanskrit and show the impact of this choice on word error rates (WER). Finally,\nwe extend these insights from Sanskrit ASR for building ASR systems in two\nother Indic languages, Gujarati and Telugu. For both these languages, our\nexperimental results show that the use of phonetic based graphemic\nrepresentations in ASR results in performance improvements as compared to ASR\nsystems that use native scripts.", "published": "2021-06-02 18:06:32", "link": "http://arxiv.org/abs/2106.05852v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation\n  Network", "abstract": "We present an instance-based nearest neighbor approach to entity linking. In\ncontrast to most prior entity retrieval systems which represent each entity\nwith a single vector, we build a contextualized mention-encoder that learns to\nplace similar mentions of the same entity closer in vector space than mentions\nof different entities. This approach allows all mentions of an entity to serve\nas \"class prototypes\" as inference involves retrieving from the full set of\nlabeled entity mentions in the training set and applying the nearest mention\nneighbor's entity label. Our model is trained on a large multilingual corpus of\nmention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor\ninference on an index of 700 million mentions. It is simpler to train, gives\nmore interpretable predictions, and outperforms all other systems on two\nmultilingual entity linking benchmarks.", "published": "2021-06-02 15:54:36", "link": "http://arxiv.org/abs/2106.07352v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Posthoc Verification and the Fallibility of the Ground Truth", "abstract": "Classifiers commonly make use of pre-annotated datasets, wherein a model is\nevaluated by pre-defined metrics on a held-out test set typically made of\nhuman-annotated labels. Metrics used in these evaluations are tied to the\navailability of well-defined ground truth labels, and these metrics typically\ndo not allow for inexact matches. These noisy ground truth labels and strict\nevaluation metrics may compromise the validity and realism of evaluation\nresults. In the present work, we discuss these concerns and conduct a\nsystematic posthoc verification experiment on the entity linking (EL) task.\nUnlike traditional methodologies, which asks annotators to provide free-form\nannotations, we ask annotators to verify the correctness of annotations after\nthe fact (i.e., posthoc). Compared to pre-annotation evaluation,\nstate-of-the-art EL models performed extremely well according to the posthoc\nevaluation methodology. Posthoc validation also permits the validation of the\nground truth dataset. Surprisingly, we find predictions from EL models had a\nsimilar or higher verification rate than the ground truth. We conclude with a\ndiscussion on these findings and recommendations for future evaluations.", "published": "2021-06-02 17:57:09", "link": "http://arxiv.org/abs/2106.07353v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Should We Always Separate?: Switching Between Enhanced and Observed\n  Signals for Overlapping Speech Recognition", "abstract": "Although recent advances in deep learning technology improved automatic\nspeech recognition (ASR), it remains difficult to recognize speech when it\noverlaps other people's voices. Speech separation or extraction is often used\nas a front-end to ASR to handle such overlapping speech. However, deep neural\nnetwork-based speech enhancement can generate `processing artifacts' as a side\neffect of the enhancement, which degrades ASR performance. For example, it is\nwell known that single-channel noise reduction for non-speech noise\n(non-overlapping speech) often does not improve ASR. Likewise, the processing\nartifacts may also be detrimental to ASR in some conditions when processing\noverlapping speech with a separation/extraction method, although it is usually\nbelieved that separation/extraction improves ASR. In order to answer the\nquestion `Do we always have to separate/extract speech from mixtures?', we\nanalyze ASR performance on observed and enhanced speech at various noise and\ninterference conditions, and show that speech enhancement degrades ASR under\nsome conditions even for overlapping speech. Based on these findings, we\npropose a simple switching algorithm between observed and enhanced speech based\non the estimated signal-to-interference ratio and signal-to-noise ratio. We\ndemonstrated experimentally that such a simple switching mechanism can improve\nrecognition performance when processing artifacts are detrimental to ASR.", "published": "2021-06-02 05:31:45", "link": "http://arxiv.org/abs/2106.00949v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NVC-Net: End-to-End Adversarial Voice Conversion", "abstract": "Voice conversion has gained increasing popularity in many applications of\nspeech synthesis. The idea is to change the voice identity from one speaker\ninto another while keeping the linguistic content unchanged. Many voice\nconversion approaches rely on the use of a vocoder to reconstruct the speech\nfrom acoustic features, and as a consequence, the speech quality heavily\ndepends on such a vocoder. In this paper, we propose NVC-Net, an end-to-end\nadversarial network, which performs voice conversion directly on the raw audio\nwaveform of arbitrary length. By disentangling the speaker identity from the\nspeech content, NVC-Net is able to perform non-parallel traditional\nmany-to-many voice conversion as well as zero-shot voice conversion from a\nshort utterance of an unseen target speaker. Importantly, NVC-Net is\nnon-autoregressive and fully convolutional, achieving fast inference. Our model\nis capable of producing samples at a rate of more than 3600 kHz on an NVIDIA\nV100 GPU, being orders of magnitude faster than state-of-the-art methods under\nthe same hardware configurations. Objective and subjective evaluations on\nnon-parallel many-to-many voice conversion tasks show that NVC-Net obtains\ncompetitive results with significantly fewer parameters.", "published": "2021-06-02 07:19:58", "link": "http://arxiv.org/abs/2106.00992v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Refinement of Direction of Arrival Estimators by\n  Majorization-Minimization Optimization on the Array Manifold", "abstract": "We propose a generalized formulation of direction of arrival estimation that\nincludes many existing methods such as steered response power, subspace,\ncoherent and incoherent, as well as speech sparsity-based methods. Unlike most\nconventional methods that rely exclusively on grid search, we introduce a\ncontinuous optimization algorithm to refine DOA estimates beyond the resolution\nof the initial grid. The algorithm is derived from the\nmajorization-minimization (MM) technique. We derive two surrogate functions,\none quadratic and one linear. Both lead to efficient iterative algorithms that\ndo not require hyperparameters, such as step size, and ensure that the DOA\nestimates never leave the array manifold, without the need for a projection\nstep. In numerical experiments, we show that the accuracy after a few\niterations of the MM algorithm nearly removes dependency on the resolution of\nthe initial grid used. We find that the quadratic surrogate function leads to\nvery fast convergence, but the simplicity of the linear algorithm is very\nattractive, and the performance gap small.", "published": "2021-06-02 08:18:14", "link": "http://arxiv.org/abs/2106.01011v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "math.OC"], "primary_category": "eess.SP"}
{"title": "Exploring modality-agnostic representations for music classification", "abstract": "Music information is often conveyed or recorded across multiple data\nmodalities including but not limited to audio, images, text and scores.\nHowever, music information retrieval research has almost exclusively focused on\nsingle modality recognition, requiring development of separate models for each\nmodality. Some multi-modal works require multiple coexisting modalities given\nto the model as inputs, constraining the use of these models to the few cases\nwhere data from all modalities are available. To the best of our knowledge, no\nexisting model has the ability to take inputs from varying modalities, e.g.\nimages or sounds, and classify them into unified music categories. We explore\nthe use of cross-modal retrieval as a pretext task to learn modality-agnostic\nrepresentations, which can then be used as inputs to classifiers that are\nindependent of modality. We select instrument classification as an example task\nfor our study as both visual and audio components provide relevant semantic\ninformation. We train music instrument classifiers that can take both images or\nsounds as input, and perform comparably to sound-only or image-only\nclassifiers. Furthermore, we explore the case when there is limited labeled\ndata for a given modality, and the impact in performance by using labeled data\nfrom other modalities. We are able to achieve almost 70% of best performing\nsystem in a zero-shot setting. We provide a detailed analysis of experimental\nresults to understand the potential and limitations of the approach, and\ndiscuss future steps towards modality-agnostic classifiers.", "published": "2021-06-02 13:39:42", "link": "http://arxiv.org/abs/2106.01149v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-To-End Deep Learning-Based Adaptation Control for Frequency-Domain\n  Adaptive System Identification", "abstract": "We present a novel end-to-end deep learning-based adaptation control\nalgorithm for frequency-domain adaptive system identification. The proposed\nmethod exploits a deep neural network to map observed signal features to\ncorresponding step-sizes which control the filter adaptation. The parameters of\nthe network are optimized in an end-to-end fashion by minimizing the average\nnormalized system distance of the adaptive filter. This avoids the need of\nexplicit signal power spectral density estimation as required for model-based\nadaptation control and further auxiliary mechanisms to deal with model\ninaccuracies. The proposed algorithm achieves fast convergence and robust\nsteady-state performance for scenarios characterized by high-level, non-white\nand non-stationary additive noise signals, abrupt environment changes and\nadditional model inaccuracies.", "published": "2021-06-02 16:14:43", "link": "http://arxiv.org/abs/2106.01262v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Dual Script E2E framework for Multilingual and Code-Switching ASR", "abstract": "India is home to multiple languages, and training automatic speech\nrecognition (ASR) systems for languages is challenging. Over time, each\nlanguage has adopted words from other languages, such as English, leading to\ncode-mixing. Most Indian languages also have their own unique scripts, which\nposes a major limitation in training multilingual and code-switching ASR\nsystems.\n  Inspired by results in text-to-speech synthesis, in this work, we use an\nin-house rule-based phoneme-level common label set (CLS) representation to\ntrain multilingual and code-switching ASR for Indian languages. We propose two\nend-to-end (E2E) ASR systems. In the first system, the E2E model is trained on\nthe CLS representation, and we use a novel data-driven back-end to recover the\nnative language script. In the second system, we propose a modification to the\nE2E model, wherein the CLS representation and the native language characters\nare used simultaneously for training. We show our results on the multilingual\nand code-switching tasks of the Indic ASR Challenge 2021. Our best results\nachieve 6% and 5% improvement (approx) in word error rate over the baseline\nsystem for the multilingual and code-switching tasks, respectively, on the\nchallenge development data.", "published": "2021-06-02 18:08:27", "link": "http://arxiv.org/abs/2106.01400v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sound-to-Imagination: An Exploratory Study on Unsupervised Crossmodal\n  Translation Using Diverse Audiovisual Data", "abstract": "The motivation of our research is to explore the possibilities of automatic\nsound-to-image (S2I) translation for enabling a human receiver to visually\ninfer the occurrence of sound related events. We expect the computer to\n'imagine' the scene from the captured sound, generating original images that\npicture the sound emitting source. Previous studies on similar topics opted for\nsimplified approaches using data with low content diversity and/or sound class\nsupervision. Differently, we propose to perform unsupervised S2I translation\nusing thousands of distinct and unknown scenes, with slightly pre-cleaned data,\njust enough to guarantee aural-visual semantic coherence. To that end, we\nemploy conditional generative adversarial networks (GANs) with a deep densely\nconnected generator. Additionally, we present a solution using informativity\nclassifiers to perform quantitative evaluation of the generated images. This\nenabled us to analyze the influence of network bottleneck variation over the\ntranslation, observing a potential trade-off between informativity and pixel\nspace convergence. Despite the complexity of the specified S2I translation\ntask, we were able to generalize the model enough to obtain more than 14%, in\naverage, of interpretable and semantically coherent images translated from\nunknown sounds.", "published": "2021-06-02 16:20:43", "link": "http://arxiv.org/abs/2106.01266v2", "categories": ["cs.SD", "cs.GR", "cs.MM", "eess.AS", "eess.IV", "68T07, 68T20, 68T45", "H.5.1; H.5.5; I.4.5; I.5.4"], "primary_category": "cs.SD"}
