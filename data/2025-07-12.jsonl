{"title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "abstract": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "published": "2025-07-12 23:29:56", "link": "http://arxiv.org/abs/2507.09424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching", "abstract": "Generating spoken dialogue is more challenging than monologue text-to-speech\n(TTS) due to the need for realistic turn-taking and distinct speaker timbres.\nExisting spoken dialogue generation models, being auto-regressive, suffer from\nslow and unstable inference. To overcome these limitations, we introduce\nZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation\nmodel built upon flow matching. Key designs include: 1) speaker-turn embeddings\nfor precise speaker turn-taking; 2) a curriculum learning strategy for stable\nspeech-text alignment; 3) specialized strategies to enable stereo dialogue\ngeneration. Additionally, recognizing the lack of open-source large-scale\nspoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue\ndataset from in-the-wild speech data. Furthermore, we established a benchmark\nto comprehensively evaluate various models. Experimental results demonstrate\nthat ZipVoice-Dialog achieves superior performance in intelligibility, speaker\nturn-taking accuracy, speaker similarity, and inference speed. Our codes, model\ncheckpoints, demo samples, and the OpenDialog dataset are all publicly\navailable at https://github.com/k2-fsa/ZipVoice.", "published": "2025-07-12 15:18:47", "link": "http://arxiv.org/abs/2507.09318v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning", "abstract": "Text-to-Speech (TTS) systems in Lombard speaking style can improve the\noverall intelligibility of speech, useful for hearing loss and noisy\nconditions. However, training those models requires a large amount of data and\nthe Lombard effect is challenging to record due to speaker and noise\nvariability and tiring recording conditions. Voice conversion (VC) has been\nshown to be a useful augmentation technique to train TTS systems in the absence\nof recorded data from the target speaker in the target speaking style. In this\npaper, we are concerned with Lombard speaking style transfer. Our goal is to\nconvert speaker identity while preserving the acoustic attributes that define\nthe Lombard speaking style. We compare voice conversion models with implicit\nand explicit acoustic feature conditioning. We observe that our proposed\nimplicit conditioning strategy achieves an intelligibility gain comparable to\nthe model conditioned on explicit acoustic features, while also preserving\nspeaker similarity.", "published": "2025-07-12 14:57:04", "link": "http://arxiv.org/abs/2507.09310v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ClaritySpeech: Dementia Obfuscation in Speech", "abstract": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "published": "2025-07-12 13:39:25", "link": "http://arxiv.org/abs/2507.09282v1", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.", "published": "2025-07-12 13:21:10", "link": "http://arxiv.org/abs/2507.09279v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Psychology-Driven Enhancement of Humour Translation", "abstract": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "published": "2025-07-12 11:44:41", "link": "http://arxiv.org/abs/2507.09259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "abstract": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "published": "2025-07-12 10:54:30", "link": "http://arxiv.org/abs/2507.09245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "abstract": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "published": "2025-07-12 09:49:30", "link": "http://arxiv.org/abs/2507.09225v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "abstract": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "published": "2025-07-12 08:54:05", "link": "http://arxiv.org/abs/2507.09205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "abstract": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "published": "2025-07-12 08:10:10", "link": "http://arxiv.org/abs/2507.09185v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "abstract": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})", "published": "2025-07-12 07:48:02", "link": "http://arxiv.org/abs/2507.09176v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "abstract": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "published": "2025-07-12 07:46:51", "link": "http://arxiv.org/abs/2507.09174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "abstract": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "published": "2025-07-12 06:27:46", "link": "http://arxiv.org/abs/2507.09157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "abstract": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "published": "2025-07-12 06:25:22", "link": "http://arxiv.org/abs/2507.09155v1", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "primary_category": "cs.CL"}
{"title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "abstract": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "published": "2025-07-12 01:34:24", "link": "http://arxiv.org/abs/2507.09104v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "abstract": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work.", "published": "2025-07-12 00:59:41", "link": "http://arxiv.org/abs/2507.09100v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.0"], "primary_category": "cs.HC"}
{"title": "DS@GT at Touch\u00e9: Large Language Models for Retrieval-Augmented Debate", "abstract": "Large Language Models (LLMs) demonstrate strong conversational abilities. In\nthis Working Paper, we study them in the context of debating in two ways: their\nability to perform in a structured debate along with a dataset of arguments to\nuse and their ability to evaluate utterances throughout the debate. We deploy\nsix leading publicly available models from three providers for the\nRetrieval-Augmented Debate and Evaluation. The evaluation is performed by\nmeasuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout\nthis task, we found that although LLMs perform well in debates when given\nrelated arguments, they tend to be verbose in responses yet consistent in\nevaluation. The accompanying source code for this paper is located at\nhttps://github.com/dsgt-arc/touche-2025-rad.", "published": "2025-07-12 00:20:00", "link": "http://arxiv.org/abs/2507.09090v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Simultaneous Network Design with Restricted Link Usage", "abstract": "Given a digraph with two terminal vertices $s$ and $t$ as well as a\nconservative cost function and several not necessarily disjoint color classes\non its arc set, our goal is to find a minimum-cost subset of the arcs such that\nits intersection with each color class contains an $s$-$t$ dipath. Problems of\nthis type arise naturally in multi-commodity network design settings where each\ncommodity is restricted to use links of its own color only.\n  We study several variants of the problem, deriving strong hardness results\neven for restricted cases, but we also identify cases that can be solved in\npolynomial time. The latter ones include the cases where the color classes form\na laminar family, or where the underlying digraph is acyclic and the number of\ncolor classes is constant. We also present an FPT algorithm for the general\ncase parameterized by the number of multi-colored arcs.", "published": "2025-07-12 23:34:58", "link": "http://arxiv.org/abs/2507.09426v1", "categories": ["cs.DS", "cs.CC", "cs.DM", "math.OC"], "primary_category": "cs.DS"}
{"title": "m-Eternal Domination and Variants on Some Classes of Finite and Infinite Graphs", "abstract": "We study the m-Eternal Domination problem, which is the following two-player\ngame between a defender and an attacker on a graph: initially, the defender\npositions k guards on vertices of the graph; the game then proceeds in turns\nbetween the defender and the attacker, with the attacker selecting a vertex and\nthe defender responding to the attack by moving a guard to the attacked vertex.\nThe defender may move more than one guard on their turn, but guards can only\nmove to neighboring vertices. The defender wins a game on a graph G with k\nguards if the defender has a strategy such that at every point of the game the\nvertices occupied by guards form a dominating set of G and the attacker wins\notherwise. The m-eternal domination number of a graph G is the smallest value\nof k for which (G,k) is a defender win.\n  We show that m-Eternal Domination is NP-hard, as well as some of its\nvariants, even on special classes of graphs. We also show structural results\nfor the Domination and m-Eternal Domination problems in the context of four\ntypes of infinite regular grids: square, octagonal, hexagonal, and triangular,\nestablishing tight bounds.", "published": "2025-07-12 13:43:19", "link": "http://arxiv.org/abs/2507.09283v1", "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO", "05C85, 68R10", "G.2.2"], "primary_category": "cs.DM"}
{"title": "Item-centric Exploration for Cold Start Problem", "abstract": "Recommender systems face a critical challenge in the item cold-start problem,\nwhich limits content diversity and exacerbates popularity bias by struggling to\nrecommend new items. While existing solutions often rely on auxiliary data, but\nthis paper illuminates a distinct, yet equally pressing, issue stemming from\nthe inherent user-centricity of many recommender systems. We argue that in\nenvironments with large and rapidly expanding item inventories, the traditional\nfocus on finding the \"best item for a user\" can inadvertently obscure the ideal\naudience for nascent content. To counter this, we introduce the concept of\nitem-centric recommendations, shifting the paradigm to identify the optimal\nusers for new items. Our initial realization of this vision involves an\nitem-centric control integrated into an exploration system. This control\nemploys a Bayesian model with Beta distributions to assess candidate items\nbased on a predicted balance between user satisfaction and the item's inherent\nquality. Empirical online evaluations reveal that this straightforward control\nmarkedly improves cold-start targeting efficacy, enhances user satisfaction\nwith newly explored content, and significantly increases overall exploration\nefficiency.", "published": "2025-07-12 23:22:23", "link": "http://arxiv.org/abs/2507.09423v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Balancing Semantic Relevance and Engagement in Related Video Recommendations", "abstract": "Related video recommendations commonly use collaborative filtering (CF)\ndriven by co-engagement signals, often resulting in recommendations lacking\nsemantic coherence and exhibiting strong popularity bias. This paper introduces\na novel multi-objective retrieval framework, enhancing standard two-tower\nmodels to explicitly balance semantic relevance and user engagement. Our\napproach uniquely combines: (a) multi-task learning (MTL) to jointly optimize\nco-engagement and semantic relevance, explicitly prioritizing topical\ncoherence; (b) fusion of multimodal content features (textual and visual\nembeddings) for richer semantic understanding; and (c) off-policy correction\n(OPC) via inverse propensity weighting to effectively mitigate popularity bias.\nEvaluation on industrial-scale data and a two-week live A/B test reveals our\nframework's efficacy. We observed significant improvements in semantic\nrelevance (from 51% to 63% topic match rate), a reduction in popular item\ndistribution (-13.8% popular video recommendations), and a +0.04% improvement\nin our topline user engagement metric. Our method successfully achieves better\nsemantic coherence, balanced engagement, and practical scalability for\nreal-world deployment.", "published": "2025-07-12 21:04:25", "link": "http://arxiv.org/abs/2507.09403v1", "categories": ["cs.IR", "cs.MM"], "primary_category": "cs.IR"}
{"title": "Knowledge Conceptualization Impacts RAG Efficacy", "abstract": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.", "published": "2025-07-12 20:10:26", "link": "http://arxiv.org/abs/2507.09389v1", "categories": ["cs.AI", "cs.CY", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval", "abstract": "Two-tower neural networks are a popular architecture for the retrieval stage\nin recommender systems. These models are typically trained with a softmax loss\nover the item catalog. However, in web-scale settings, the item catalog is\noften prohibitively large, making full softmax infeasible. A common solution is\nsampled softmax, which approximates the full softmax using a small number of\nsampled negatives.\n  One practical and widely adopted approach is to use in-batch negatives, where\nnegatives are drawn from items in the current mini-batch. However, this\nintroduces a bias: items that appear more frequently in the batch (i.e.,\npopular items) are penalized more heavily.\n  To mitigate this issue, a popular industry technique known as logQ correction\nadjusts the logits during training by subtracting the log-probability of an\nitem appearing in the batch. This correction is derived by analyzing the bias\nin the gradient and applying importance sampling, effectively twice, using the\nin-batch distribution as a proposal distribution. While this approach improves\nmodel quality, it does not fully eliminate the bias.\n  In this work, we revisit the derivation of logQ correction and show that it\noverlooks a subtle but important detail: the positive item in the denominator\nis not Monte Carlo-sampled - it is always present with probability 1. We\npropose a refined correction formula that accounts for this. Notably, our loss\nintroduces an interpretable sample weight that reflects the model's uncertainty\n- the probability of misclassification under the current parameters. We\nevaluate our method on both public and proprietary datasets, demonstrating\nconsistent improvements over the standard logQ correction.", "published": "2025-07-12 16:16:11", "link": "http://arxiv.org/abs/2507.09331v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching", "abstract": "Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .", "published": "2025-07-12 11:30:32", "link": "http://arxiv.org/abs/2507.09256v1", "categories": ["cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Retrieval-Augmented Recommendation Explanation Generation with Hierarchical Aggregation", "abstract": "Explainable Recommender System (ExRec) provides transparency to the\nrecommendation process, increasing users' trust and boosting the operation of\nonline services. With the rise of large language models (LLMs), whose extensive\nworld knowledge and nuanced language understanding enable the generation of\nhuman-like, contextually grounded explanations, LLM-powered ExRec has gained\ngreat momentum. However, existing LLM-based ExRec models suffer from profile\ndeviation and high retrieval overhead, hindering their deployment. To address\nthese issues, we propose Retrieval-Augmented Recommendation Explanation\nGeneration with Hierarchical Aggregation (REXHA). Specifically, we design a\nhierarchical aggregation based profiling module that comprehensively considers\nuser and item review information, hierarchically summarizing and constructing\nholistic profiles. Furthermore, we introduce an efficient retrieval module\nusing two types of pseudo-document queries to retrieve relevant reviews to\nenhance the generation of recommendation explanations, effectively reducing\nretrieval latency and improving the recall of relevant reviews. Extensive\nexperiments demonstrate that our method outperforms existing approaches by up\nto 12.6% w.r.t. the explanation quality while achieving high retrieval\nefficiency.", "published": "2025-07-12 08:15:05", "link": "http://arxiv.org/abs/2507.09188v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Joint Access Point Activation and Power Allocation for Cell-Free Massive MIMO Aided ISAC Systems", "abstract": "Cell-free massive multiple-input multiple-output (MIMO)-aided integrated\nsensing and communication (ISAC) systems are investigated where distributed\naccess points jointly serve users and sensing targets. We demonstrate that only\na subset of access points (APs) has to be activated for both tasks, while\ndeactivating redundant APs is essential for power savings. This motivates joint\nactive AP selection and power control for optimizing energy efficiency. The\nresultant problem is a mixed-integer nonlinear program (MINLP). To address\nthis, we propose a model-based Branch-and-Bound approach as a strong baseline\nto guide a semi-supervised heterogeneous graph neural network (HetGNN) for\nselecting the best active APs and the power allocation. Comprehensive numerical\nresults demonstrate that the proposed HetGNN reduces power consumption by\n20-25\\% and runs nearly 10,000 times faster than model-based benchmarks.", "published": "2025-07-12 23:30:18", "link": "http://arxiv.org/abs/2507.09425v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Asymptotically optimal cyclic subspace codes", "abstract": "Subspace codes, and in particular cyclic subspace codes, have gained\nsignificant attention in recent years due to their applications in error\ncorrection for random network coding. In this paper, we introduce a new\ntechnique for constructing cyclic subspace codes with large cardinality and\nprescribed minimum distance. Using this new method, we provide new\nconstructions of cyclic subspace codes in the Grassmannian $\\mathcal{G}_q(n,k)$\nof all $k$-dimensional $\\mathbb{F}_q$-subspaces of an $n$-dimensional vector\nspace over $\\mathbb{F}_q$, when $k\\mid n$ and $n/k$ is a composite number, with\nminimum distance $2k-2$ and large size. We prove that the resulting codes have\nsizes larger than those obtained from previously known constructions with the\nsame parameters. Furthermore, we show that our constructions of cyclic subspace\ncodes asymptotically reach the Johnson type bound II for infinite values of\n$n/k$.", "published": "2025-07-12 14:00:11", "link": "http://arxiv.org/abs/2507.09290v1", "categories": ["cs.IT", "math.CO", "math.IT", "11T71, 11T99, 94B99"], "primary_category": "cs.IT"}
{"title": "On Lattice Isomorphism Problems for Lattices from LCD Codes over Finite Rings", "abstract": "These days, post-quantum cryptography based on the lattice isomorphism\nproblem has been proposed. Ducas-Gibbons introduced the hull attack, which\nsolves the lattice isomorphism problem for lattices obtained by Construction A\nfrom an LCD code over a finite field. Using this attack, they showed that the\nlattice isomorphism problem for such lattices can be reduced to the lattice\nisomorphism problem with the trivial lattice $\\mathbb{Z}^n$ and the graph\nisomorphism problem. While the previous work by Ducas-Gibbons only considered\nlattices constructed by a code over a \\textit{finite field}, this paper\nconsiders lattices constructed by a code over a \\textit{finite ring}\n$\\mathbb{Z}/k\\mathbb{Z}$, which is a more general case. In particular, when $k$\nis odd, an odd prime power, or not divisible by $4$, we show that the lattice\nisomorphism problem can be reduced to the lattice isomorphism problem for\n$\\mathbb{Z}^n$ and the graph isomorphism problem.", "published": "2025-07-12 11:33:38", "link": "http://arxiv.org/abs/2507.09257v1", "categories": ["cs.IT", "math.CO", "math.IT", "Primary 11T71, Secondary 14G50"], "primary_category": "cs.IT"}
{"title": "A CLuP algorithm to practically achieve $\\sim 0.76$ SK--model ground state free energy", "abstract": "We consider algorithmic determination of the $n$-dimensional\nSherrington-Kirkpatrick (SK) spin glass model ground state free energy. It\ncorresponds to a binary maximization of an indefinite quadratic form and under\nthe \\emph{worst case} principles of the classical NP complexity theory it is\nhard to approximate within a $\\log(n)^{const.}$ factor. On the other hand, the\nSK's random nature allows (polynomial) spectral methods to \\emph{typically}\napproach the optimum within a constant factor. Naturally one is left with the\nfundamental question: can the residual (constant) \\emph{computational gap} be\nerased?\n  Following the success of \\emph{Controlled Loosening-up} (CLuP) algorithms in\nplanted models, we here devise a simple practical CLuP-SK algorithmic procedure\nfor (non-planted) SK models. To analyze the \\emph{typical} success of the\nalgorithm we associate to it (random) CLuP-SK models. Further connecting to\nrecent random processes studies [94,97], we characterize the models and CLuP-SK\nalgorithm via fully lifted random duality theory (fl RDT) [98]. Moreover,\nrunning the algorithm we demonstrate that its performance is in an excellent\nagrement with theoretical predictions. In particular, already for $n$ on the\norder of a few thousands CLuP-SK achieves $\\sim 0.76$ ground state free energy\nand remarkably closely approaches theoretical $n\\rightarrow\\infty$ limit\n$\\approx 0.763$. For all practical purposes, this renders computing SK model's\nnear ground state free energy as a \\emph{typically} easy problem.", "published": "2025-07-12 10:58:50", "link": "http://arxiv.org/abs/2507.09247v1", "categories": ["cond-mat.dis-nn", "cs.IT", "math.IT", "math.OC", "stat.ML"], "primary_category": "cond-mat.dis-nn"}
{"title": "Data Fusion and Aggregation Methods to Develop Composite Indexes for a Sustainable Future", "abstract": "Research on environmental risk modeling relies on numerous indicators to\nquantify the magnitude and frequency of extreme climate events, their\necological, economic, and social impacts, and the coping mechanisms that can\nreduce or mitigate their adverse effects. Index-based approaches significantly\nsimplify the process of quantifying, comparing, and monitoring risks associated\nwith other natural hazards, as a large set of indicators can be condensed into\na few key performance indicators. Data fusion techniques are often used in\nconjunction with expert opinions to develop key performance indicators. This\npaper discusses alternative methods to combine data from multiple indicators,\nwith an emphasis on their use-case scenarios, underlying assumptions, data\nrequirements, advantages, and limitations. The paper demonstrates the\napplication of these data fusion methods through examples from current risk and\nresilience models and simplified datasets. Simulations are conducted to\nidentify their strengths and weaknesses under various scenarios. Finally, a\nreal-life example illustrates how these data fusion techniques can be applied\nto inform policy recommendations in the context of drought resilience and\nsustainability.", "published": "2025-07-12 08:53:52", "link": "http://arxiv.org/abs/2507.09204v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Fundamental Limits of Bistatic Integrated Sensing and Communications over Memoryless Relay Channels", "abstract": "The problem of bistatic integrated sensing and communications over memoryless\nrelay channels is considered, where destination concurrently decodes the\nmessage sent by the source and estimates unknown parameters from received\nsignals with the help of a relay. A state-dependent discrete memoryless relay\nchannel is considered to model this setup, and the fundamental limits of the\ncommunication-sensing performance tradeoff are characterized by the\ncapacity-distortion function. An upper bound on the capacity-distortion\nfunction is derived, extending the cut-set bound results to address the sensing\noperation at the destination. A hybrid-partial-decode-and-compress-forward\ncoding scheme is also proposed to facilitate source-relay cooperation for both\nmessage transmission and sensing, establishing a lower bound on the\ncapacity-distortion function. It is found that the\nhybrid-partial-decode-and-compress-forward scheme achieves optimal sensing\nperformance when the communication task is ignored. Furthermore, the upper and\nlower bounds are shown to coincide for three specific classes of relay\nchannels. Numerical examples are provided to illustrate the\ncommunication-sensing tradeoff and demonstrate the benefits of integrated\ndesign.", "published": "2025-07-12 08:27:09", "link": "http://arxiv.org/abs/2507.09193v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Adaptive Social Learning using Theory of Mind", "abstract": "Social learning is a powerful mechanism through which agents learn about the\nworld from others. However, humans don't always choose to observe others, since\nsocial learning can carry time and cognitive resource costs. How do people\nbalance social and non-social learning? In this paper, we propose a rational\nmentalizing model of the decision to engage in social learning. This model\nestimates the utility of social learning by reasoning about the other agent's\ngoal and the informativity of their future actions. It then weighs the utility\nof social learning against the utility of self-exploration (non-social\nlearning). Using a multi-player treasure hunt game, we show that our model can\nquantitatively capture human trade-offs between social and non-social learning.\nFurthermore, our results indicate that these two components allow agents to\nflexibly apply social learning to achieve their goals more efficiently.", "published": "2025-07-12 21:55:06", "link": "http://arxiv.org/abs/2507.09409v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Simulation for All: A Step-by-Step Cookbook for Developing Human-Centered Multi-Agent Transportation Simulators", "abstract": "As cities evolve toward more complex and multimodal transportation systems,\nthe need for human-centered multi-agent simulation tools has never been more\nurgent. Yet most existing platforms remain limited - they often separate\ndifferent types of road users, rely on scripted or pre-defined behaviors,\noverlook public transit users as active participants, and are rarely designed\nwith accessibility in mind for non-technical users. To address this gap, this\npaper presents the specifications of a multi-agent simulation platform designed\nto support real-time, human-centered, and immersive studies of all road users,\naccompanied by open-source scripts for replication. Using high-fidelity\nimmersive virtual environments, our platform enables interaction across public\ntransit users, pedestrians, cyclists, automated vehicles, and drivers. The\narchitecture is modular, extensible, and designed for accessibility. The system\nintegrates hardware-specific modules - including an omnidirectional treadmill,\na seating arrangement, a smart trainer, and an actuated cockpit. Additionally,\nthe platform collects multimodal physiological, neurological, and behavioral\ndata through embedded sensing devices such as functional near-infrared\nspectroscopy (fNIRS), eye tracking, and wrist-based biosensors. To show the\nusability of this system, we present three use cases. Simulation for All aims\nto lower the barrier to entry for high-fidelity transportation simulation,\nsupport experimentation across disciplines, and advance our understanding of\nmultimodal mobility in complex urban environments.", "published": "2025-07-12 18:07:19", "link": "http://arxiv.org/abs/2507.09367v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets", "abstract": "We present StockSim, an open-source simulation platform for systematic\nevaluation of large language models (LLMs) in realistic financial\ndecision-making scenarios. Unlike previous toolkits that offer limited scope,\nStockSim delivers a comprehensive system that fully models market dynamics and\nsupports diverse simulation modes of varying granularity. It incorporates\ncritical real-world factors, such as latency, slippage, and order-book\nmicrostructure, that were previously neglected, enabling more faithful and\ninsightful assessment of LLM-based trading agents. An extensible, role-based\nagent framework supports heterogeneous trading strategies and multi-agent\ncoordination, making StockSim a uniquely capable testbed for NLP research on\nreasoning under uncertainty and sequential decision-making. We open-source all\nour code at https: //github.com/harrypapa2002/StockSim.", "published": "2025-07-12 11:29:44", "link": "http://arxiv.org/abs/2507.09255v1", "categories": ["cs.CE", "cs.MA"], "primary_category": "cs.CE"}
{"title": "Coordinated Communication and Inventory Optimization in Multi-Retailer Supply Chains", "abstract": "We consider a multi-retailer supply chain where each retailer can dynamically\nchoose when to share information (e.g., local inventory levels or demand\nobservations) with other retailers, incurring a communication cost for each\nsharing event. This flexible information exchange mechanism contrasts with\nfixed protocols such as always sharing or never sharing. We formulate a joint\noptimization of inventory control and communication strategies, aiming to\nbalance the trade-off between communication overhead and operational\nperformance (service levels, holding, and stockout costs). We adopt a common\ninformation framework and derive a centralized Partially Observable Markov\nDecision Process (POMDP) model for a supply chain coordinator. Solving this\ncoordinator's POMDP via dynamic programming characterizes the structure of\noptimal policies, determining when retailers should communicate and how they\nshould adjust orders based on available information. We show that, in this\nsetting, retailers can often act optimally by sharing only limited summaries of\ntheir private data, reducing communication frequency without compromising\nperformance. We also incorporate practical constraints on communication\nfrequency and propose an approximate point-based POMDP solution method\n(PBVI/SARSOP) to address computational complexity. Numerical experiments on\nmulti-retailer inventory scenarios demonstrate that our approach significantly\nimproves the cost-service trade-off compared to static information sharing\npolicies, effectively optimizing the schedule of information exchange for\ncooperative inventory control.", "published": "2025-07-12 09:39:44", "link": "http://arxiv.org/abs/2507.09223v1", "categories": ["math.OC", "cs.MA"], "primary_category": "math.OC"}
{"title": "A discontinuous Galerkin method for one-dimensional nonlocal wave problems", "abstract": "This paper presents a fully discrete numerical scheme for one-dimensional\nnonlocal wave equations and provides a rigorous theoretical analysis. To\nfacilitate the spatial discretization, we introduce an auxiliary variable\nanalogous to the gradient field in local discontinuous Galerkin (DG) methods\nfor classical partial differential equations (PDEs) and reformulate the\nequation into a system of equations. The proposed scheme then uses a DG method\nfor spatial discretization and the Crank-Nicolson method for time integration.\nWe prove optimal L2 error convergence for both the solution and the auxiliary\nvariable under a special class of radial kernels at the semi-discrete level. In\naddition, for general kernels, we demonstrate the asymptotic compatibility of\nthe scheme, ensuring that it recovers the classical DG approximation of the\nlocal wave equation in the zero-horizon limit. Furthermore, we prove that the\nfully discrete scheme preserves the energy of the nonlocal wave equation. A\nseries of numerical experiments are presented to validate the theoretical\nfindings.", "published": "2025-07-12 20:56:20", "link": "http://arxiv.org/abs/2507.09401v1", "categories": ["math.NA", "cs.NA", "45A05, 65M12, 65M60, 65R20"], "primary_category": "math.NA"}
{"title": "ORCHA -- A Performance Portability System for Post-Exascale Systems", "abstract": "Heterogeneity is the prevalent trend in the rapidly evolving high-performance\ncomputing (HPC) landscape in both hardware and application software. The\ndiversity in hardware platforms, currently comprising various accelerators and\na future possibility of specializable chiplets, poses a significant challenge\nfor scientific software developers aiming to harness optimal performance across\ndifferent computing platforms while maintaining the quality of solutions when\ntheir applications are simultaneously growing more complex. Code synthesis and\ncode generation can provide mechanisms to mitigate this challenge. We have\ndeveloped a toolchain, ORCHA, which arises from the needs of a large\nmultiphysics simulation software, Flash-X, which were not met by any of the\nexisting solutions. ORCHA is composed of three stand-alone tools -- one to\nexpress high-level control flow and a map of what to execute where on the\nplatform, a second one to express variants of data structures and arithmetic\noperations in the solvers in a unified fashion, and a third one that manages\nthe runtime orchestration of the data and computation. We use an\napplication-specific interface layer that uses code generation and code\nsynthesis to stitch together the application. In this paper, we describe the\ninterface layer for the application Flash-X and demonstrate the use of ORCHA in\nexploring possible configurations from which the optimal one can be selected\nfor production, including a case study in which a single simulation recipe is\nrealized on three distinct hardware mappings -- a GPU-centric, a CPU/GPU\nbalanced, and a CPU/GPU concurrent layouts -- highlighting the breadth of\nconfigurations ORCHA enables.", "published": "2025-07-12 16:35:32", "link": "http://arxiv.org/abs/2507.09337v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Finite element modeling of V-notched thermoelastic strain-limiting solids containing inclusions", "abstract": "A precise domain triangulation is recognized as indispensable for the\naccurate numerical approximation of differential operators within collocation\nmethods, leading to a substantial reduction in discretization errors. An\nefficient finite element method (FEM) is presented in this paper, meticulously\ndeveloped to solve a complex mathematical model. This model governs the\nbehavior of thermoelastic solids containing both a V-notch and inclusions. The\nsystem of partial differential equations underlying this model consists of two\nprimary components: a linear elliptic equation, which is used to describe the\ntemperature distribution, and a quasilinear equation, which governs the\nmechanical behavior of the body. Through the application of this specifically\ntailored FEM, accurate and efficient solutions are able to be obtained for\nthese intricate thermoelastic problems. The algebraically nonlinear\nconstitutive equation, alongside the balance of linear momentum, is effectively\nreduced to a second-order quasi-linear elliptic partial differential equation.\nComplex curved boundaries are represented through the application of a smooth,\ndistinctive point transformation. Furthermore, higher-order shape functions are\nemployed to ensure the accurate computation of entries within the FEM matrices\nand vectors, from which a highly precise approximate solution to the BVP is\nsubsequently obtained. The inherent nonlinearities in the governing\ndifferential equation are addressed through the implementation of a Picard-type\nlinearization scheme. Numerical results, derived from a series of test cases,\nhave consistently demonstrated a significant enhancement in accuracy, a crucial\nachievement for the nuanced analysis of thermoelastic solids.", "published": "2025-07-12 14:28:16", "link": "http://arxiv.org/abs/2507.09300v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Discrete reaction-diffusion system with stochastic dynamical boundary conditions: convergence results", "abstract": "A space discrete approximation to a highly nonlinear reaction-diffusion\nsystem endowed with a stochastic dynamical boundary condition is analyzed and\nthe convergence of the discrete scheme to the solution to the corresponding\ncontinuum random system is established. A splitting strategy allows us to\ndecompose the random system into a space-discrete heat equation with a\nstochastic boundary condition, and a nonlinear and nonlocal space-discrete\ndifferential system coupled with the first one and with deterministic initial\nand boundary conditions. The convergence result is obtained by first\nestablishing some a priori estimates for both space-discrete splitted variables\nand then exploiting compact embedding theorems for time-space Besov spaces on\nthe positive lattice. The convergence of a fully discrete approximation of the\nrandom system is also discussed.", "published": "2025-07-12 13:18:43", "link": "http://arxiv.org/abs/2507.09278v1", "categories": ["math.PR", "cs.NA", "math.NA", "60H35, 65M06, 65M12"], "primary_category": "math.PR"}
{"title": "Benchmark stress tests for flow past a cylinder at higher Reynolds numbers using EMAC", "abstract": "We consider a test problem for Navier-Stokes solvers based on the flow around\na cylinder at Reynolds numbers 500 and 1000, where the solution is observed to\nbe periodic when the problem is sufficiently resolved. Computing the resulting\nflow is a challenge, even for exactly divergence-free discretization methods,\nwhen the scheme does not include sufficient numerical dissipation. We examine\nthe performance of the energy, momentum and angular momentum conserving (EMAC)\nformulation of the Navier-Stokes equations. This incorporates more physical\nconservation into the finite element method even when the numerical solution is\nnot exactly divergence-free. Consequently, it has a chance to outperform\nstandard methods, especially for long-time simulations. We find that for\nlowest-order Taylor-Hood elements, EMAC outperforms the standard convective\nformulations. However, for higher-order elements, EMAC can become unstable on\nunder-resolved meshes.", "published": "2025-07-12 13:03:25", "link": "http://arxiv.org/abs/2507.09274v1", "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "primary_category": "math.NA"}
{"title": "Crack-tip field characterization in nonlinearly constituted and geometrically linear elastoporous solid containing a star-shaped crack: A finite element study", "abstract": "This paper introduces a three-dimensional (3-D) mathematical and\ncomputational framework for the characterization of crack-tip fields in\nstar-shaped cracks within porous elastic solids. A core emphasis of this model\nis its direct integration of density-dependent elastic moduli, offering a more\nphysically realistic representation of engineering materials where intrinsic\nporosity and density profoundly influence mechanical behavior. The governing\nboundary value problem, formulated for the static equilibrium of a 3-D,\nhomogeneous, and isotropic material, manifests as a system of second-order,\nquasilinear partial differential equations. This system is meticulously coupled\nwith classical traction-free boundary conditions imposed at the complex crack\nsurface. For the robust numerical solution of this intricate nonlinear problem,\nwe employ a continuous trilinear Galerkin-type finite element discretization.\nThe inherent strong nonlinearities arising within the discrete system are\neffectively managed through a powerful and stable {Picard-type linearization\nscheme}. The proposed model demonstrates a remarkable ability to accurately\ndescribe the full stress and strain states in a diverse range of materials,\ncrucially recovering the well-established classical singularities observed in\nlinearized elastic fracture mechanics. A comprehensive numerical examination of\ntensile stress, strain, and strain energy density fields consistently reveals\nthat these quantities attain their peak values in the immediate vicinity of the\ncrack tip, an observation that remarkably aligns with established findings in\nstandard linearized elastic fracture mechanics.", "published": "2025-07-12 12:00:13", "link": "http://arxiv.org/abs/2507.09263v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Investigation of Shock-Capturing with Bound-Preserving Limiters for the Nonlinearly Stable Flux Reconstruction Method", "abstract": "Nonlinearly stable flux reconstruction (NSFR) combines the key properties of\nprovable nonlinear stability with the increased time step from energy-stable\nflux reconstruction. The NSFR scheme has been successfully applied to unsteady\ncompressible flows. Through the use of a bound-preserving limiter, positivity\nof thermodynamic quantities is preserved, and this enables the extension of\nNSFR to hyperbolic conservation laws. We extend the limiter of Zhang and Shu\n[1] to ensure robustness for the proposed scheme. The limiter is modified to\nconsider the minimum density and pressure at the solution nodes when\ndetermining the value to scale the solution. The modifications are thoroughly\ntested with a suite of test cases. In addition to these modifications, this\npaper conducts a thorough investigation into the shock-capturing capabilities\nof the NSFR scheme and the advantages it presents over standard discontinuous\nGalerkin (DG) methods, where, on select variants of the flux reconstruction\n(FR) scheme, essentially oscillation-free solutions are demonstrated. Various\nparameters of the scheme are extensively tested and analyzed through several 1D\nand 2D compressible Euler tests that verify the high-order accuracy, entropy\nstability, time step advantage and shock-capturing capabilities of the NSFR\nscheme. These parameters include the two-point flux, quadrature nodes and the\nstrength of the FR parameter. In addition to investigating the impact of the\nvarious two-point fluxes, this paper also presents numerical studies to\ndetermine the CFL condition required to maintain positivity for the two-point\nflux of choice. The investigation yields insightful results for all parameters,\nwith the results pertaining to the type of FR scheme being of special interest.\nThe tests showcase increased robustness, time step advantages and\noscillation/overshoot mitigation when employing a stronger FR parameter.", "published": "2025-07-12 04:05:36", "link": "http://arxiv.org/abs/2507.09131v1", "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "primary_category": "math.NA"}
{"title": "Joint deep calibration of the 4-factor PDV model", "abstract": "Joint calibration to SPX and VIX market data is a delicate task that requires\nsophisticated modeling and incurs significant computational costs. The latter\nis especially true when pricing of volatility derivatives hinges on nested\nMonte Carlo simulation. One such example is the 4-factor Markov Path-Dependent\nVolatility (PDV) model of Guyon and Lekeufack (2023). Nonetheless, its realism\nhas earned it considerable attention in recent years. Gazzani and Guyon (2025)\nmarked a relevant contribution by learning the VIX as a random variable, i.e.,\na measurable function of the model parameters and the Markovian factors. A\nneural network replaces the inner simulation and makes the joint calibration\nproblem accessible. However, the minimization loop remains slow due to\nexpensive outer simulation. The present paper overcomes this limitation by\nlearning SPX implied volatilities, VIX futures, and VIX call option prices. The\npricing functions reduce to simple matrix-vector products that can be evaluated\non the fly, shrinking calibration times to just a few seconds.", "published": "2025-07-12 22:27:18", "link": "http://arxiv.org/abs/2507.09412v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Generalized Orlicz premia", "abstract": "We introduce a generalized version of Orlicz premia, based on possibly\nnon-convex loss functions. We show that this generalized definition covers a\nvariety of relevant examples, such as the geometric mean and the expectiles,\nwhile at the same time retaining a number of relevant properties. We establish\nthat cash-additivity leads to $L^p$-quantiles, extending a classical result on\n'collapse to the mean' for convex Orlicz premia.\n  We then focus on the geometrically convex case, discussing the dual\nrepresentation of generalized Orlicz premia and comparing it with a\nmultiplicative form of the standard dual representation for the convex case.\nFinally, we show that generalized Orlicz premia arise naturally as the only\nelicitable, positively homogeneous, monotone and normalized functionals.", "published": "2025-07-12 08:04:00", "link": "http://arxiv.org/abs/2507.09181v1", "categories": ["q-fin.RM", "math.PR", "math.ST", "q-fin.MF", "stat.TH"], "primary_category": "q-fin.RM"}
{"title": "A Framework for Predictive Directional Trading Based on Volatility and Causal Inference", "abstract": "Purpose: This study introduces a novel framework for identifying and\nexploiting predictive lead-lag relationships in financial markets. We propose\nan integrated approach that combines advanced statistical methodologies with\nmachine learning models to enhance the identification and exploitation of\npredictive relationships between equities. Methods: We employed a Gaussian\nMixture Model (GMM) to cluster nine prominent stocks based on their mid-range\nhistorical volatility profiles over a three-year period. From the resulting\nclusters, we constructed a multi-stage causal inference pipeline, incorporating\nthe Granger Causality Test (GCT), a customised Peter-Clark Momentary\nConditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to\nidentify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW)\nand a K-Nearest Neighbours (KNN) classifier were utilised to determine the\noptimal time lag for trade execution. The resulting strategy was rigorously\nbacktested. Results: The proposed volatility-based trading strategy, tested\nfrom 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The\nportfolio yielded a total return of 15.38%, significantly outperforming the\n10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics,\nincluding a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain\npairs, confirmed the strategy's viability. Conclusion: This research\ncontributes a systematic and robust methodology for identifying profitable\ntrading opportunities derived from volatility-based causal relationships. The\nfindings have significant implications for both academic research in financial\nmodelling and the practical application of algorithmic trading, offering a\nstructured approach to developing resilient, data-driven strategies.", "published": "2025-07-12 16:53:32", "link": "http://arxiv.org/abs/2507.09347v1", "categories": ["q-fin.ST", "cs.AI", "stat.ML"], "primary_category": "q-fin.ST"}
{"title": "Optimal Differentially Private Ranking from Pairwise Comparisons", "abstract": "Data privacy is a central concern in many applications involving ranking from\nincomplete and noisy pairwise comparisons, such as recommendation systems,\neducational assessments, and opinion surveys on sensitive topics. In this work,\nwe propose differentially private algorithms for ranking based on pairwise\ncomparisons. Specifically, we develop and analyze ranking methods under two\nprivacy notions: edge differential privacy, which protects the confidentiality\nof individual comparison outcomes, and individual differential privacy, which\nsafeguards potentially many comparisons contributed by a single individual. Our\nalgorithms--including a perturbed maximum likelihood estimator and a noisy\ncount-based method--are shown to achieve minimax optimal rates of convergence\nunder the respective privacy constraints. We further demonstrate the practical\neffectiveness of our methods through experiments on both simulated and\nreal-world data.", "published": "2025-07-12 20:09:28", "link": "http://arxiv.org/abs/2507.09388v1", "categories": ["math.ST", "stat.ME", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "Robust Spatiotemporal Epidemic Modeling with Integrated Adaptive Outlier Detection", "abstract": "In epidemic modeling, outliers can distort parameter estimation and\nultimately lead to misguided public health decisions. Although there are\nexisting robust methods that can mitigate this distortion, the ability to\nsimultaneously detect outliers is equally vital for identifying potential\ndisease hotspots. In this work, we introduce a robust spatiotemporal\ngeneralized additive model (RST-GAM) to address this need. We accomplish this\nwith a mean-shift parameter to quantify and adjust for the effects of outliers\nand rely on adaptive Lasso regularization to model the sparsity of outlying\nobservations. We use univariate polynomial splines and bivariate penalized\nsplines over triangulations to estimate the functional forms and a\ndata-thinning approach for data-adaptive weight construction. We derive a\nscalable proximal algorithm to estimate model parameters by minimizing a convex\nnegative log-quasi-likelihood function. Our algorithm uses adaptive step-sizes\nto ensure global convergence of the resulting iterate sequence. We establish\nerror bounds and selection consistency for the estimated parameters and\ndemonstrate our model's effectiveness through numerical studies under various\noutlier scenarios. Finally, we demonstrate the practical utility of RST-GAM by\nanalyzing county-level COVID-19 infection data in the United States,\nhighlighting its potential to inform public health decision-making.", "published": "2025-07-12 19:23:25", "link": "http://arxiv.org/abs/2507.09380v1", "categories": ["stat.ME", "physics.soc-ph", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation", "abstract": "Time series data with missing values is common across many domains.\nHealthcare presents special challenges due to prolonged periods of sensor\ndisconnection. In such cases, having a confidence measure for imputed values is\ncritical. Most existing methods either overlook model uncertainty or lack\nmechanisms to estimate it. To address this gap, we introduce a general\nframework that quantifies and leverages uncertainty for selective imputation.\nBy focusing on values the model is most confident in, highly unreliable\nimputations are avoided. Our experiments on multiple EHR datasets, covering\ndiverse types of missingness, demonstrate that selectively imputing\nless-uncertain values not only reduces imputation errors but also improves\ndownstream tasks. Specifically, we show performance gains in a 24-hour\nmortality prediction task, underscoring the practical benefit of incorporating\nuncertainty into time series imputation.", "published": "2025-07-12 17:11:00", "link": "http://arxiv.org/abs/2507.09353v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Uncovering symmetric and asymmetric species associations from community and environmental data", "abstract": "There is no much doubt that biotic interactions shape community assembly and\nultimately the spatial co-variations between species. There is a hope that the\nsignal of these biotic interactions can be observed and retrieved by\ninvestigating the spatial associations between species while accounting for the\ndirect effects of the environment. By definition, biotic interactions can be\nboth symmetric and asymmetric. Yet, most models that attempt to retrieve\nspecies associations from co-occurrence or co-abundance data internally assume\nsymmetric relationships between species. Here, we propose and validate a\nmachine-learning framework able to retrieve bidirectional associations by\nanalyzing species community and environmental data.\n  Our framework (1) models pairwise species associations as directed influences\nfrom a source to a target species, parameterized with two species-specific\nlatent embeddings: the effect of the source species on the community, and the\nresponse of the target species to the community; and (2) jointly fits these\nassociations within a multi-species conditional generative model with different\nmodes of interactions between environmental drivers and biotic associations.\nUsing both simulated and empirical data, we demonstrate the ability of our\nframework to recover known asymmetric and symmetric associations and highlight\nthe properties of the learned association networks. By comparing our approach\nto other existing models such as joint species distribution models and\nprobabilistic graphical models, we show its superior capacity at retrieving\nsymmetric and asymmetric interactions. The framework is intuitive, modular and\nbroadly applicable across various taxonomic groups.", "published": "2025-07-12 15:18:36", "link": "http://arxiv.org/abs/2507.09317v1", "categories": ["stat.ML", "cs.LG", "q-bio.PE", "68T07, 62H22, 92D40", "I.2.3; I.2.6; I.5.1"], "primary_category": "stat.ML"}
{"title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding", "abstract": "We propose TPP-SD, a novel approach that accelerates Transformer temporal\npoint process (TPP) sampling by adapting speculative decoding (SD) techniques\nfrom language models. By identifying the structural similarities between\nthinning algorithms for TPPs and speculative decoding for language models, we\ndevelop an efficient sampling framework that leverages a smaller draft model to\ngenerate multiple candidate events, which are then verified by the larger\ntarget model in parallel. TPP-SD maintains the same output distribution as\nautoregressive sampling while achieving significant acceleration. Experiments\non both synthetic and real datasets demonstrate that our approach produces\nsamples from identical distributions as standard methods, but with 2-6$\\times$\nspeedup. Our ablation studies analyze the impact of hyperparameters such as\ndraft length and draft model size on sampling efficiency. TPP-SD bridges the\ngap between powerful Transformer TPP models and the practical need for rapid\nsequence sampling.", "published": "2025-07-12 11:18:07", "link": "http://arxiv.org/abs/2507.09252v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications", "abstract": "Wavelet neural network (WNN), which learns an unknown nonlinear mapping from\nthe data, has been widely used in signal processing, and time-series analysis.\nHowever, challenges in constructing accurate wavelet bases and high\ncomputational costs limit their application. This study introduces a\nconstructive WNN that selects initial bases and trains functions by introducing\nnew bases for predefined accuracy while reducing computational costs. For the\nfirst time, we analyze the frequency of unknown nonlinear functions and select\nappropriate initial wavelets based on their primary frequency components by\nestimating the energy of the spatial frequency component. This leads to a novel\nconstructive framework consisting of a frequency estimator and a wavelet-basis\nincrease mechanism to prioritize high-energy bases, significantly improving\ncomputational efficiency. The theoretical foundation defines the necessary\ntime-frequency range for high-dimensional wavelets at a given accuracy. The\nframework's versatility is demonstrated through four examples: estimating\nunknown static mappings from offline data, combining two offline datasets,\nidentifying time-varying mappings from time-series data, and capturing\nnonlinear dependencies in real time-series data. These examples showcase the\nframework's broad applicability and practicality. All the code will be released\nat https://github.com/dshuangdd/CWNN.", "published": "2025-07-12 09:09:26", "link": "http://arxiv.org/abs/2507.09213v1", "categories": ["cs.LG", "stat.ML", "68T07"], "primary_category": "cs.LG"}
{"title": "Warm Starts Accelerate Generative Modelling", "abstract": "Iterative generative models, like diffusion and flow-matching, create\nhigh-fidelity samples by progressively refining a noise vector into data.\nHowever, this process is notoriously slow, often requiring hundreds of function\nevaluations. We introduce the warm-start model, a simple, deterministic model\nthat dramatically accelerates conditional generation by providing a better\nstarting point. Instead of starting generation from an uninformed N(0, I)\nprior, our warm-start model predicts an informed prior N(mu, sigma), whose\nmoments are conditioned on the input context. This \"warm start\" substantially\nreduces the distance the generative process must traverse, particularly when\nthe conditioning information is strongly informative. On tasks like image\ninpainting, our method achieves results competitive with a 1000-step DDPM\nbaseline using only 11 total function evaluations (1 for the warm start, 10 for\ngeneration). A simple conditional normalization trick makes our method\ncompatible with any standard generative model and sampler without modification,\nallowing it to be combined with other efficient sampling techniques for further\nacceleration. Our implementation is available at\nhttps://github.com/jonas-scholz123/warm-start-model.", "published": "2025-07-12 09:07:05", "link": "http://arxiv.org/abs/2507.09212v1", "categories": ["cs.LG", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling", "abstract": "Observed records of climate extremes provide an incomplete picture of risk,\nmissing \"unseen\" extremes that exceed historical bounds. In parallel,\nneglecting spatial dependence undervalues the risk of synchronized hazards that\namplify impacts. To address these challenges, we develop DeepX-GAN\n(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial\nNetwork), a knowledge-informed deep generative model designed to better capture\nthe spatial structure of rare extremes. The zero-shot generalizability of\nDeepX-GAN enables simulation of unseen extremes that fall outside historical\nexperience yet remain statistically plausible. We define two types of unseen\nextremes: \"checkmate\" extremes that directly hit targets, and \"stalemate\"\nextremes that narrowly miss. These unrealized scenarios expose latent risks in\nfragile systems and may reinforce a false sense of resilience if overlooked.\nNear misses, in particular, can prompt either proactive adaptation or dangerous\ncomplacency, depending on how they are interpreted. Applying DeepX-GAN to the\nMiddle East and North Africa (MENA), we find that these unseen extremes\ndisproportionately affect regions with high vulnerability and low socioeconomic\nreadiness, but differ in urgency and interpretation. Future warming could\nexpand and redistribute these unseen extremes, with emerging exposure hotspots\nin Indo-Pakistan and Central Africa. This distributional shift highlights\ncritical blind spots in conventional hazard planning and underscores the need\nto develop spatially adaptive policies that anticipate emergent risk hotspots\nrather than simply extrapolating from historical patterns.", "published": "2025-07-12 09:06:45", "link": "http://arxiv.org/abs/2507.09211v1", "categories": ["cs.LG", "physics.ao-ph", "physics.data-an", "physics.geo-ph", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The BdryMat\u00e9rn GP: Reliable incorporation of boundary information on irregular domains for Gaussian process modeling", "abstract": "Gaussian processes (GPs) are broadly used as surrogate models for expensive\ncomputer simulators of complex phenomena. However, a key bottleneck is that its\ntraining data are generated from this expensive simulator and thus can be\nhighly limited. A promising solution is to supplement the learning model with\nboundary information from scientific knowledge. However, despite recent work on\nboundary-integrated GPs, such models largely cannot accommodate boundary\ninformation on irregular (i.e., non-hypercube) domains, and do not provide\nsample path smoothness control or approximation error analysis, both of which\nare important for reliable surrogate modeling. We thus propose a novel\nBdryMat\\'ern GP modeling framework, which can reliably integrate Dirichlet,\nNeumann and Robin boundaries on an irregular connected domain with a boundary\nset that is twice-differentiable almost everywhere. Our model leverages a new\nBdryMat\\'ern covariance kernel derived in path integral form via a stochastic\npartial differential equation formulation. Similar to the GP with Mat\\'ern\nkernel, we prove that sample paths from the BdryMat\\'ern GP satisfy the desired\nboundaries with smoothness control on its derivatives. We further present an\nefficient approximation procedure for the BdryMat\\'ern kernel using finite\nelement modeling with rigorous error analysis. Finally, we demonstrate the\neffectiveness of the BdryMat\\'ern GP in a suite of numerical experiments on\nincorporating broad boundaries on irregular domains.", "published": "2025-07-12 07:53:49", "link": "http://arxiv.org/abs/2507.09178v1", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "Continual Reinforcement Learning by Planning with Online World Models", "abstract": "Continual reinforcement learning (CRL) refers to a naturalistic setting where\nan agent needs to endlessly evolve, by trial and error, to solve multiple tasks\nthat are presented sequentially. One of the largest obstacles to CRL is that\nthe agent may forget how to solve previous tasks when learning a new task,\nknown as catastrophic forgetting. In this paper, we propose to address this\nchallenge by planning with online world models. Specifically, we learn a\nFollow-The-Leader shallow model online to capture the world dynamics, in which\nwe plan using model predictive control to solve a set of tasks specified by any\nreward functions. The online world model is immune to forgetting by\nconstruction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$\nunder mild assumptions. The planner searches actions solely based on the latest\nonline model, thus forming a FTL Online Agent (OA) that updates incrementally.\nTo assess OA, we further design Continual Bench, a dedicated environment for\nCRL, and compare with several strong baselines under the same model-planning\nalgorithmic framework. The empirical results show that OA learns continuously\nto solve new tasks while not forgetting old skills, outperforming agents built\non deep world models with various continual learning techniques.", "published": "2025-07-12 07:52:31", "link": "http://arxiv.org/abs/2507.09177v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation", "abstract": "Sparse Principal Component Analysis (SPCA) is a fundamental technique for\ndimensionality reduction, and is NP-hard. In this paper, we introduce a\nrandomized approximation algorithm for SPCA, which is based on the basic SDP\nrelaxation. Our algorithm has an approximation ratio of at most the sparsity\nconstant with high probability, if called enough times. Under a technical\nassumption, which is consistently satisfied in our numerical tests, the average\napproximation ratio is also bounded by $\\mathcal{O}(\\log{d})$, where $d$ is the\nnumber of features. We show that this technical assumption is satisfied if the\nSDP solution is low-rank, or has exponentially decaying eigenvalues. We then\npresent a broad class of instances for which this technical assumption holds.\nWe also demonstrate that in a covariance model, which generalizes the spiked\nWishart model, our proposed algorithm achieves a near-optimal approximation\nratio. We demonstrate the efficacy of our algorithm through numerical results\non real-world datasets.", "published": "2025-07-12 05:43:56", "link": "http://arxiv.org/abs/2507.09148v1", "categories": ["stat.ML", "cs.LG", "math.OC"], "primary_category": "stat.ML"}
{"title": "A Generalization Theory for Zero-Shot Prediction", "abstract": "A modern paradigm for generalization in machine learning and AI consists of\npre-training a task-agnostic foundation model, generally obtained using\nself-supervised and multimodal contrastive learning. The resulting\nrepresentations can be used for prediction on a downstream task for which no\nlabeled data is available. We present a theoretical framework to better\nunderstand this approach, called zero-shot prediction. We identify the target\nquantities that zero-shot prediction aims to learn, or learns in passing, and\nthe key conditional independence relationships that enable its generalization\nability.", "published": "2025-07-12 03:37:57", "link": "http://arxiv.org/abs/2507.09128v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Study of Value-Aware Eigenoptions", "abstract": "Options, which impose an inductive bias toward temporal and hierarchical\nstructure, offer a powerful framework for reinforcement learning (RL). While\neffective in sequential decision-making, they are often handcrafted rather than\nlearned. Among approaches for discovering options, eigenoptions have shown\nstrong performance in exploration, but their role in credit assignment remains\nunderexplored. In this paper, we investigate whether eigenoptions can\naccelerate credit assignment in model-free RL, evaluating them in tabular and\npixel-based gridworlds. We find that pre-specified eigenoptions aid not only\nexploration but also credit assignment, whereas online discovery can bias the\nagent's experience too strongly and hinder learning. In the context of deep RL,\nwe also propose a method for learning option-values under non-linear function\napproximation, highlighting the impact of termination conditions on\nperformance. Our findings reveal both the promise and complexity of using\neigenoptions, and options more broadly, to simultaneously support credit\nassignment and exploration in reinforcement learning.", "published": "2025-07-12 03:29:59", "link": "http://arxiv.org/abs/2507.09127v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Moment-Based Generalization to Post-Prediction Inference", "abstract": "Artificial intelligence (AI) and machine learning (ML) are increasingly used\nto generate data for downstream analyses, yet naively treating these\npredictions as true observations can lead to biased results and incorrect\ninference. Wang et al. (2020) proposed a method, post-prediction inference,\nwhich calibrates inference by modeling the relationship between AI/ML-predicted\nand observed outcomes in a small, gold-standard sample. Since then, several\nmethods have been developed for inference with predicted data. We revisit Wang\net al. in light of these recent developments. We reflect on their assumptions\nand offer a simple extension of their method which relaxes these assumptions.\nOur extension (1) yields unbiased point estimates under standard conditions and\n(2) incorporates a simple scaling factor to preserve calibration variability.\nIn extensive simulations, we show that our method maintains nominal Type I\nerror rates, reduces bias, and achieves proper coverage.", "published": "2025-07-12 02:33:45", "link": "http://arxiv.org/abs/2507.09119v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Sharp Trade-Offs in High-Dimensional Inference via 2-Level SLOPE", "abstract": "Among techniques for high-dimensional linear regression, Sorted L-One\nPenalized Estimation (SLOPE) generalizes the LASSO via an adaptive $l_1$\nregularization that applies heavier penalties to larger coefficients in the\nmodel. To achieve such adaptivity, SLOPE requires the specification of a\ncomplex hierarchy of penalties, i.e., a monotone penalty sequence in $R^p$, in\ncontrast to a single penalty scalar for LASSO. Tuning this sequence when $p$ is\nlarge poses a challenge, as brute force search over a grid of values is\ncomputationally prohibitive. In this work, we study the 2-level SLOPE, an\nimportant subclass of SLOPE, with only three hyperparameters. We demonstrate\nboth empirically and analytically that 2-level SLOPE not only preserves the\nadvantages of general SLOPE -- such as improved mean squared error and\novercoming the Donoho-Tanner power limit -- but also exhibits computational\nbenefits by reducing the penalty hyperparameter space. In particular, we prove\nthat 2-level SLOPE admits a sharp, theoretically tight characterization of the\ntrade-off between true positive proportion (TPP) and false discovery proportion\n(FDP), contrasting with general SLOPE where only upper and lower bounds are\nknown. Empirical evaluations further underscore the effectiveness of 2-level\nSLOPE in settings where predictors exhibit high correlation, when the noise is\nlarge, or when the underlying signal is not sparse. Our results suggest that\n2-level SLOPE offers a robust, scalable alternative to both LASSO and general\nSLOPE, making it particularly suited for practical high-dimensional data\nanalysis.", "published": "2025-07-12 01:57:10", "link": "http://arxiv.org/abs/2507.09110v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "CoVAE: Consistency Training of Variational Autoencoders", "abstract": "Current state-of-the-art generative approaches frequently rely on a two-stage\ntraining procedure, where an autoencoder (often a VAE) first performs\ndimensionality reduction, followed by training a generative model on the\nlearned latent space. While effective, this introduces computational overhead\nand increased sampling times. We challenge this paradigm by proposing\nConsistency Training of Variational AutoEncoders (CoVAE), a novel single-stage\ngenerative autoencoding framework that adopts techniques from consistency\nmodels to train a VAE architecture. The CoVAE encoder learns a progressive\nseries of latent representations with increasing encoding noise levels,\nmirroring the forward processes of diffusion and flow matching models. This\nsequence of representations is regulated by a time dependent $\\beta$ parameter\nthat scales the KL loss. The decoder is trained using a consistency loss with\nvariational regularization, which reduces to a conventional VAE loss at the\nearliest latent time. We show that CoVAE can generate high-quality samples in\none or few steps without the use of a learned prior, significantly\noutperforming equivalent VAEs and other single-stage VAEs methods. Our approach\nprovides a unified framework for autoencoding and diffusion-style generative\nmodeling and provides a viable route for one-step generative high-performance\nautoencoding. Our code is publicly available at\nhttps://github.com/gisilvs/covae.", "published": "2025-07-12 01:32:08", "link": "http://arxiv.org/abs/2507.09103v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization", "abstract": "We study convergence in high-probability of SGD-type methods in non-convex\noptimization and the presence of heavy-tailed noise. To combat the heavy-tailed\nnoise, a general black-box nonlinear framework is considered, subsuming\nnonlinearities like sign, clipping, normalization and their smooth\ncounterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the\nrate $\\widetilde{\\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments\nand a symmetric probability density function (PDF). Crucially, N-SGD has\nexponentially decaying tails, matching the performance of linear SGD under\nlight-tailed noise. To handle non-symmetric noise, we propose two novel\nestimators, based on the idea of noise symmetrization. The first, dubbed\nSymmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any\nreference point is available at the start of training, while the second, dubbed\nMini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient.\nCombined with the nonlinear framework, we get N-SGE and N-MSGE methods,\nrespectively, both achieving the same convergence rate and exponentially\ndecaying tails as N-SGD, while allowing for non-symmetric noise with unbounded\nmoments and PDF satisfying a mild technical condition, with N-MSGE additionally\nrequiring bounded noise moment of order $p \\in (1,2]$. Compared to works\nassuming noise with bounded $p$-th moment, our results: 1) are based on a novel\nsymmetrization approach; 2) provide a unified framework and relaxed moment\nconditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly\nbetter than existing works when $p < 2$, while the complexity of N-MSGE is\nclose to existing works. Compared to works assuming symmetric noise with\nunbounded moments, we: 1) provide a sharper analysis and improved rates; 2)\nfacilitate state-dependent symmetric noise; 3) extend the strong guarantees to\nnon-symmetric noise.", "published": "2025-07-12 00:31:13", "link": "http://arxiv.org/abs/2507.09093v1", "categories": ["stat.ML", "cs.LG", "math.OC"], "primary_category": "stat.ML"}
{"title": "Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA", "abstract": "We generalize the low-rank decomposition problem, such as principal and\nindependent component analysis (PCA, ICA) for continuous-time vector-valued\nsignals and provide a model-agnostic implicit neural signal representation\nframework to learn numerical approximations to solve the problem. Modeling\nsignals as continuous-time stochastic processes, we unify the approaches to\nboth the PCA and ICA problems in the continuous setting through a contrast\nfunction term in the network loss, enforcing the desired statistical properties\nof the source signals (decorrelation, independence) learned in the\ndecomposition. This extension to a continuous domain allows the application of\nsuch decompositions to point clouds and irregularly sampled signals where\nstandard techniques are not applicable.", "published": "2025-07-12 00:20:16", "link": "http://arxiv.org/abs/2507.09091v1", "categories": ["cs.LG", "eess.SP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Reinforcement Learning with Gradient Eligibility Traces", "abstract": "Achieving fast and stable off-policy learning in deep reinforcement learning\n(RL) is challenging. Most existing methods rely on semi-gradient\ntemporal-difference (TD) methods for their simplicity and efficiency, but are\nconsequently susceptible to divergence. While more principled approaches like\nGradient TD (GTD) methods have strong convergence guarantees, they have rarely\nbeen used in deep RL. Recent work introduced the Generalized Projected Bellman\nError ($\\GPBE$), enabling GTD methods to work efficiently with nonlinear\nfunction approximation. However, this work is only limited to one-step methods,\nwhich are slow at credit assignment and require a large number of samples. In\nthis paper, we extend the $\\GPBE$ objective to support multistep credit\nassignment based on the $\\lambda$-return and derive three gradient-based\nmethods that optimize this new objective. We provide both a forward-view\nformulation compatible with experience replay and a backward-view formulation\ncompatible with streaming algorithms. Finally, we evaluate the proposed\nalgorithms and show that they outperform both PPO and StreamQ in MuJoCo and\nMinAtar environments, respectively. Code available at\nhttps://github.com/esraaelelimy/gtd\\_algos", "published": "2025-07-12 00:12:05", "link": "http://arxiv.org/abs/2507.09087v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering", "abstract": "Accurate sound propagation simulation is essential for delivering immersive\nexperiences in virtual applications, yet industry methods for acoustic modeling\noften do not account for the full breadth of acoustic wave phenomena. This\npaper proposes a novel two-dimensional (2D) finite-difference time-domain\n(FDTD) framework that simulates sound propagation as a wave-based model in\nUnreal Engine, with an emphasis on capturing lower frequency wave phenomena,\nembedding occlusion, diffraction, reflection and interference in generated\nimpulse responses. The process begins by discretizing the scene geometry into a\n2D grid via a top-down projection from which obstacle masks and boundary\nconditions are derived. A Python-based FDTD solver injects a sine sweep at a\nsource position, and virtual quadraphonic microphone arrays record pressure\nfield responses at pre-defined listener positions. De-convolution of the\npressure responses yields multi-channel impulse responses that retain spatial\ndirectionality which are then integrated into Unreal Engine's audio pipeline\nfor dynamic playback. Benchmark tests confirm agreement with analytical\nexpectations, and the paper outlines hybrid extensions aimed at commercial\nviability.", "published": "2025-07-12 18:46:26", "link": "http://arxiv.org/abs/2507.09376v1", "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model", "abstract": "Deep learning-based hearing loss compensation (HLC) seeks to enhance speech\nintelligibility and quality for hearing impaired listeners using neural\nnetworks. One major challenge of HLC is the lack of a ground-truth target.\nRecent works have used neural networks to emulate non-differentiable auditory\nperipheral models in closed-loop frameworks, but this approach lacks\nflexibility. Alternatively, differentiable auditory models allow direct\noptimization, yet previous studies focused on individual listener profiles, or\njoint noise reduction (NR) and HLC without balancing each task. This work\nformulates NR and HLC as a multi-task learning problem, training a system to\nsimultaneously predict denoised and compensated signals from noisy speech and\naudiograms using a differentiable auditory model. Results show the system\nachieves similar objective metric performance to systems trained for each task\nseparately, while being able to adjust the balance between NR and HLC during\ninference.", "published": "2025-07-12 18:37:18", "link": "http://arxiv.org/abs/2507.09372v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Microphone Occlusion Mitigation for Own-Voice Enhancement in Head-Worn Microphone Arrays Using Switching-Adaptive Beamforming", "abstract": "Enhancing the user's own-voice for head-worn microphone arrays is an\nimportant task in noisy environments to allow for easier speech communication\nand user-device interaction. However, a rarely addressed challenge is the\nchange of the microphones' transfer functions when one or more of the\nmicrophones gets occluded by skin, clothes or hair. The underlying problem for\nbeamforming-based speech enhancement is the (potentially rapidly) changing\ntransfer functions of both the own-voice and the noise component that have to\nbe accounted for to achieve optimal performance. In this paper, we address the\nproblem of an occluded microphone in a head-worn microphone array. We\ninvestigate three alternative mitigation approaches by means of (i)\nconventional adaptive beamforming, (ii) switching between a-priori estimates of\nthe beamformer coefficients for the occluded and unoccluded state, and (iii) a\nhybrid approach using a switching-adaptive beamformer. In an evaluation with\nreal-world recordings and simulated occlusion, we demonstrate the advantages of\nthe different approaches in terms of noise reduction, own-voice distortion and\nrobustness against voice activity detection errors.", "published": "2025-07-12 17:00:18", "link": "http://arxiv.org/abs/2507.09350v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct Speech-to-Speech Translation Corpus", "abstract": "There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for\nhigh resource-to-low resource language pairs such as English-to-Yoruba. Thus,\nin this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech\nTranslation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a\nhybrid architecture we developed for large-scale direct S2ST corpus creation at\nreduced cost. To achieve this, we leveraged non speech-to-speech Standard\nYoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as\nthe corresponding Standard English (SE) transcripts. YORULECT Corpus is small\nscale(1,504) samples, and it does not have paired English audios. Therefore, we\ngenerated the SE audios using pre-trained AI models (i.e. Facebook MMS). We\nalso developed an audio augmentation algorithm named AcoustAug based on three\nlatent acoustic features to generate augmented audios from the raw audios of\nthe two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language,\nwhich gives a total of 24,064 sample size. The total audio duration for the two\nlanguages is 41.20 hours. This size is quite significant. Beyond building S2ST\nmodels, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve\nexisting ones. The created corpus and Coqui framework were used to build a\npretrained Yoruba TTS model (named YoruTTS-0.5) as a proof of concept. The\nYoruTTS-0.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates\nmoderate fundamental pitch similarity with the reference real-time audio.\nUltimately, the corpus architecture in this study can be leveraged by\nresearchers and developers to curate datasets for multilingual\nhigh-resource-to-low-resource African languages. This will bridge the huge\ndigital divides in translations among high and low-resource language pairs.\nBENYO-S2ST-Corpus-1 and YoruTTS-0.5 are publicly available at\n(https://bit.ly/40bGMwi).", "published": "2025-07-12 16:43:04", "link": "http://arxiv.org/abs/2507.09342v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can We Really Repurpose Multi-Speaker ASR Corpus for Speaker Diarization?", "abstract": "Neural speaker diarization is widely used for overlap-aware speaker\ndiarization, but it requires large multi-speaker datasets for training. To meet\nthis data requirement, large datasets are often constructed by combining\nmultiple corpora, including those originally designed for multi-speaker\nautomatic speech recognition (ASR). However, ASR datasets often feature loosely\ndefined segment boundaries that do not align with the stricter conventions of\ndiarization benchmarks. In this work, we show that such boundary looseness\nsignificantly impacts the diarization error rate, reducing evaluation\nreliability. We also reveal that models trained on data with varying boundary\nprecision tend to learn dataset-specific looseness, leading to poor\ngeneralization across out-of-domain datasets. Training with standardized tight\nboundaries via forced alignment improves not only diarization performance,\nespecially in streaming scenarios, but also ASR performance when combined with\nsimple post-processing.", "published": "2025-07-12 09:50:30", "link": "http://arxiv.org/abs/2507.09226v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Spatial Audio Understanding via Question Answering", "abstract": "In this paper, we introduce a novel framework for spatial audio understanding\nof first-order ambisonic (FOA) signals through a question answering (QA)\nparadigm, aiming to extend the scope of sound event localization and detection\n(SELD) towards spatial scene understanding and reasoning. First, we curate and\nrelease fine-grained spatio-temporal textual descriptions for the STARSS23\ndataset using a rule-based approach, and further enhance linguistic diversity\nusing large language model (LLM)-based rephrasing. We also introduce a QA\ndataset aligned with the STARSS23 scenes, covering various aspects such as\nevent presence, localization, spatial, and temporal relationships. To increase\nlanguage variety, we again leverage LLMs to generate multiple rephrasings per\nquestion. Finally, we develop a baseline spatial audio QA model that takes FOA\nsignals and natural language questions as input and provides answers regarding\nvarious occurrences, temporal, and spatial relationships of sound events in the\nscene formulated as a classification task. Despite being trained solely with\nscene-level question answering supervision, our model achieves performance that\nis comparable to a fully supervised sound event localization and detection\nmodel trained with frame-level spatiotemporal annotations. The results\nhighlight the potential of language-guided approaches for spatial audio\nunderstanding and open new directions for integrating linguistic supervision\ninto spatial scene analysis.", "published": "2025-07-12 08:29:09", "link": "http://arxiv.org/abs/2507.09195v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large Language Models and Non-Negative Matrix Factorization for Bioacoustic Signal Decomposition", "abstract": "Large language models have shown a remarkable ability to extract meaning from\nunstructured data, offering new ways to interpret biomedical signals beyond\ntraditional numerical methods. In this study, we present a matrix factorization\nframework for bioacoustic signal analysis which is enhanced by large language\nmodels. The focus is on separating bioacoustic signals that commonly overlap in\nclinical recordings, using matrix factorization to decompose the mixture into\ninterpretable components. A large language model is then applied to the\nseparated signals to associate distinct acoustic patterns with potential\nmedical conditions such as cardiac rhythm disturbances or respiratory\nabnormalities. Recordings were obtained from a digital stethoscope applied to a\nclinical manikin to ensure a controlled and high-fidelity acquisition\nenvironment. This hybrid approach does not require labeled data or prior\nknowledge of source types, and it provides a more interpretable and accessible\nframework for clinical decision support. The method demonstrates promise for\nintegration into future intelligent diagnostic tools.", "published": "2025-07-12 06:44:43", "link": "http://arxiv.org/abs/2507.09161v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition", "abstract": "Despite substantial improvements in ASR, performance tends to degrade when\nfaced with adverse conditions such as speaker accents. Generative error\ncorrection (GER) leverages the rich linguistic knowledge and exceptional\nreasoning ability of LLMs, significantly outperforming typical LM methods.\nHowever, it lacks specificity in accented speech scenarios. In this study, we\nleverage GER to improve the accuracy of transcription predictions by addressing\nthe two primary features of accented speech recognition. To fully leverage\npronunciation information, we propose the multi-modal GER, which integrates\npronunciation information from the speech modality, and the multi-granularity\nGER, which incorporates fine-grained phoneme-level information related to\npronunciation. These two methods enable the LLM to utilize the pronunciation\ninformation of accented speech and the semantic information from word-level\nhypotheses for accurate transcription predictions through LoRA fine-tuning. On\nthe one hand, we employ a three-stage training strategy to train separate\nmulti-modal GER models for each accent to obtain mono-accent LoRA experts. By\nadopting our proposed HDMoLE method, which incorporates hierarchical routing\nand dynamic thresholds within the mixture of LoRA experts, we effectively merge\nmultiple mono-accent LoRA experts within a single multi-modal GER to overcome\nthe challenges posed by accent diversity. On the other hand, multi-granularity\nGER leverages the N-best word-level and phoneme-level hypotheses generated by\nthe HDMoLE model to predict the final accented speech transcriptions.\nExperimental results on the multi-accent English dataset demonstrate the\nefficacy of our proposed methods. Our methods achieve a remarkable relative WER\nreduction of 67.35% compared to the Whisper-large-v3 baseline.", "published": "2025-07-12 02:14:50", "link": "http://arxiv.org/abs/2507.09116v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lightweight Graph Neural Networks for Enhanced 5G NR Channel Estimation", "abstract": "Effective channel estimation CE is critical for optimizing the performance of\n5G New Radio NR systems particularly in dynamic environments where traditional\nmethods struggle with complexity and adaptability This paper introduces\nGraphNet a novel lightweight Graph Neural Network GNNbased estimator designed\nto enhance CE in 5G NR Our proposed method utilizes a GNN architecture that\nminimizes computational overhead while capturing essential features necessary\nfor accurate CE We evaluate GraphNet across various channel conditions from\nslowvarying to highly dynamic environments and compare its performance to\nChannelNet a wellknown deep learningbased CE method GraphNet not only matches\nChannelNets performance in stable conditions but significantly outperforms it\nin highvariation scenarios particularly in terms of Block Error Rate It also\nincludes builtin noise estimation that enhances robustness in challenging\nchannel conditions Furthermore its significantly lighter computational\nfootprint makes GraphNet highly suitable for realtime deployment especially on\nedge devices with limited computational resources By underscoring the potential\nof GNNs to transform CE processes GraphNet offers a scalable and robust\nsolution that aligns with the evolving demands of 5G technologies highlighting\nits efficiency and performance as a nextgeneration solution for wireless\ncommunication systems", "published": "2025-07-12 21:48:23", "link": "http://arxiv.org/abs/2507.09408v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Free-running vs. Synchronous: Single-Photon Lidar for High-flux 3D Imaging", "abstract": "Conventional wisdom suggests that single-photon lidar (SPL) should operate in\nlow-light conditions to minimize dead-time effects. Many methods have been\ndeveloped to mitigate these effects in synchronous SPL systems. However,\nsolutions for free-running SPL remain limited despite the advantage of reduced\nhistogram distortion from dead times. To improve the accuracy of free-running\nSPL, we propose a computationally efficient joint maximum likelihood estimator\nof the signal flux, the background flux, and the depth using only histograms,\nalong with a complementary regularization framework that incorporates a learned\npoint cloud score model as a prior. Simulations and experiments demonstrate\nthat free-running SPL yields lower estimation errors than its synchronous\ncounterpart under identical conditions, with our regularization further\nimproving accuracy.", "published": "2025-07-12 20:06:00", "link": "http://arxiv.org/abs/2507.09386v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Reliable Task Offloading in MEC through Transmission Diversity and Jamming-Aware Scheduling", "abstract": "Mobile Edge Computing (MEC) enables low-latency applications by bringing\ncomputation closer to the user, but dynamic task arrivals and communication\nthreats like jamming complicate reliable task offloading and resource\nallocation. In this paper, we formulate a dynamic MEC framework considering the\ntransmission diversity that jointly addresses task scheduling and resource\nblock (RB) assignment in the presence of jamming. First, we define and evaluate\nkey network metrics-including dropped task ratio and bandwidth\nutilization-while maintaining service continuity by accounting for the existing\ncommitments of the edge server to previously offloaded tasks. Then, we propose\na jamming-aware offloading and RB allocation framework that leverages\ntransmission diversity and optimal scheduling across distributed gNBs. The\nproposed solution is compared to a similar scenario without transmission\ndiversity and two baseline strategies of first-come-first-served (FCFS) and\nshortest task first (STF). The proposed algorithm effectively mitigates the\nimpact of jamming while enhancing resource utilization and minimizing task drop\nrates, making it highly suitable for mission-critical MEC applications. At\nsignal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves\na $0.26$ task drop rate, outperforming the scenario without transmission\ndiversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52\nand 0.63 task drop rates, respectively.", "published": "2025-07-12 17:08:27", "link": "http://arxiv.org/abs/2507.09352v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Matched Filtering-Based Channel Estimation for AFDM Systems in Doubly Selective Channels", "abstract": "Affine frequency division multiplexing (AFDM) has recently emerged as an\nexcellent backward-compatible 6G waveform. In this paper, an enhanced AFDM is\nproposed whereby the delay-Doppler (DD) coupling phase is considered.\nSpecifically, we study matched filtering (MF) assisted channel estimation (CE)\nfor AFDM systems in complex doubly selective channels. By deriving the complete\ninput-output relationship, the inter-chirp-carrier interference,\nsignal-to-interference-plus-noise ratio (SINR), and the effective SINR loss of\nAFDM, are investigated in discrete affine Fourier transform (DAFT) domain.\nFurther, we look into the path ambiguity problem and show that it may lead to\nsevere performance deterioration in fractional-delay fractional-Doppler\nchannels. To address such a problem, we introduce an MF assisted CE scheme\nbuilding upon a novel pilot arrangement across two consecutive AFDM\ntransmissions. This allows us to sequentially estimate the parameters of each\npath by exploiting the separability and approximate orthogonality of different\npaths in the DAFT domain, thus leading to significantly reduced complexity.\nFurthermore, based on generalized Fibonacci search (GFS), an MF-GFS scheme is\nproposed to avoid significantly redundant computation, which can be extended to\ntypical wide-band systems. Extensive simulation results indicate that the\nproposed schemes offer superior advantages in terms of their improved\ncommunication performance and lower complexity.", "published": "2025-07-12 12:21:33", "link": "http://arxiv.org/abs/2507.09268v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Deep Learning for sub-THz Radio Unit Selection using sub-10 GHz Channel Information and Inferred Device Beamforming", "abstract": "The dense and distributed deployment of sub-THz radio units (RUs) alongside\nsub-10 GHz access point (AP) is a promising approach to provide high data rate\nand reliable coverage for future 6G applications. However, beam search or RU\nselection for the sub-THz RUs incurs significant overhead and high power\nconsumption. To address this, we introduce a method that leverages deep\nlearning to infer a suitable sub-THz RU candidate from a set of sub-THz RUs\nusing the sub-10 GHz channel characteristics. A novel aspect of this work is\nthe consideration of inter-band beam configuration (IBBC), defined as the\nbroadside angle between the low-band and high-band antenna patterns of the user\nequipment (UE). Since IBBC indicates the beamforming information or UE's\norientation, it is typically not shared with the network as a part of\nsignalling. Therefore, we propose a solution strategy to infer a suitable\nsub-THz RU even when UEs do not share their IBBC information. Simulation\nresults illustrate the performance of the inferred sub-THz RU and highlights\nthe detrimental impact of neglecting UE orientation on the systems performance.", "published": "2025-07-12 10:43:12", "link": "http://arxiv.org/abs/2507.09244v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Image Super-Resolution-Based Signal Enhancement in Bistatic ISAC", "abstract": "Bistatic Integrated Sensing and Communication (ISAC) is poised to become a\ncornerstone technology in next-generation communication networks, such as\nBeyond 5G (B5G) and 6G, by enabling the concurrent execution of sensing and\ncommunication functions without requiring significant modifications to existing\ninfrastructure. Despite its promising potential, a major challenge in bistatic\ncooperative sensing lies in the degradation of sensing accuracy, primarily\ncaused by the inherently weak received signals resulting from high reflection\nlosses in complex environments. Traditional methods have predominantly relied\non adaptive filtering techniques to enhance the Signal-to-Noise Ratio (SNR) by\ndynamically adjusting the filter coefficients. However, these methods often\nstruggle to adapt effectively to the increasingly complex and diverse network\ntopologies. To address these challenges, we propose a novel Image\nSuper-Resolution-based Signal Enhancement (ISR-SE) framework that significantly\nimproves the recognition and recovery capabilities of ISAC signals.\nSpecifically, we first perform a time-frequency analysis by applying the\nShort-Time Fourier Transform (STFT) to the received signals, generating\nspectrograms that capture the frequency, magnitude, and phase components. These\ncomponents are then mapped into RGB images, where each channel represents one\nof the extracted features, enabling a more intuitive and informative\nvisualization of the signal structure. To enhance these RGB images, we design\nan improved denoising network that combines the strengths of the UNet\narchitecture and diffusion models. This hybrid architecture leverages UNet's\nmulti-scale feature extraction and the generative capacity of diffusion models\nto perform effective image denoising, thereby improving the quality and clarity\nof signal representations under low-SNR conditions.", "published": "2025-07-12 09:29:13", "link": "http://arxiv.org/abs/2507.09218v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Time-Varying Offset Estimation for Clock-Asynchronous Bistatic ISAC Systems", "abstract": "The bistatic Integrated Sensing and Communication (ISAC) is poised to become\na key application for next generation communication networks (e.g., B5G/6G),\nproviding simultaneous sensing and communication services with minimal changes\nto existing network infrastructure and hardware. However, a significant\nchallenge in bistatic cooperative sensing is clock asynchronism, arising from\nthe use of different clocks at far separated transmitters and receivers. This\nasynchrony leads to Timing Offsets (TOs) and Carrier Frequency Offsets (CFOs),\npotentially causing sensing ambiguity. Traditional synchronization methods\ntypically rely on static reference links or GNSS-based timing sources, both of\nwhich are often unreliable or unavailable in UAVbased bistatic ISAC scenarios.\nTo overcome these limitations, we propose a Time-Varying Offset Estimation\n(TVOE) framework tailored for clock-asynchronous bistatic ISAC systems, which\nleverages the geometrically predictable characteristics of the Line-of-Sight\n(LoS) path to enable robust, infrastructure-free\n  synchronization. The framework treats the LoS delay and the Doppler shift as\ndynamic observations and models their evolution as a hidden stochastic process.\nA state-space formulation is developed to jointly estimate TO and CFO via an\nExtended Kalman Filter (EKF), enabling real-time tracking of clock offsets\nacross successive frames. Furthermore, the estimated offsets are subsequently\napplied to correct the timing misalignment of all Non-Line-of-Sight (NLoS)\ncomponents, thereby enhancing the high-resolution target sensing performance.\nExtensive simulation results demonstrate that the proposed TVOE method improves\nthe estimation accuracy by 60%.", "published": "2025-07-12 09:18:21", "link": "http://arxiv.org/abs/2507.09215v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning", "abstract": "In this paper, a novel Three dimensional (3D) positioning framework of fluid\nantenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In\nthe proposed framework, a set of controlled UAVs cooperatively estimate the\nreal-time 3D position of a target UAV. Here, the active UAV transmits a\nmeasurement signal to the passive UAVs via the reflection from the target UAV.\nEach passive UAV estimates the distance of the active-target-passive UAV link\nand selects an antenna port to share the distance information with the base\nstation (BS) that calculates the real-time position of the target UAV. As the\ntarget UAV is moving due to its task operation, the controlled UAVs must\noptimize their trajectories and select optimal antenna port, aiming to estimate\nthe real-time position of the target UAV. We formulate this problem as an\noptimization problem to minimize the target UAV positioning error via\noptimizing the trajectories of all controlled UAVs and antenna port selection\nof passive UAVs. Here, an attention-based recurrent multi-agent reinforcement\nlearning (AR-MARL) scheme is proposed, which enables each controlled UAV to use\nthe local Q function to determine its trajectory and antenna port while\noptimizing the target UAV positioning performance without knowing the\ntrajectories and antenna port selections of other controlled UAVs. Different\nfrom current MARL methods, the proposed method uses a recurrent neural network\n(RNN) that incorporates historical state-action pairs of each controlled UAV,\nand an attention mechanism to analyze the importance of these historical\nstate-action pairs, thus improving the global Q function approximation accuracy\nand the target UAV positioning accuracy. Simulation results show that the\nproposed AR-MARL scheme can reduce the average positioning error by up to 17.5%\nand 58.5% compared to the VD-MARL scheme and the proposed method without FAS.", "published": "2025-07-12 00:31:15", "link": "http://arxiv.org/abs/2507.09094v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/prompt4trust.", "published": "2025-07-12 13:21:10", "link": "http://arxiv.org/abs/2507.09279v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Simulation for All: A Step-by-Step Cookbook for Developing Human-Centered Multi-Agent Transportation Simulators", "abstract": "As cities evolve toward more complex and multimodal transportation systems,\nthe need for human-centered multi-agent simulation tools has never been more\nurgent. Yet most existing platforms remain limited - they often separate\ndifferent types of road users, rely on scripted or pre-defined behaviors,\noverlook public transit users as active participants, and are rarely designed\nwith accessibility in mind for non-technical users. To address this gap, this\npaper presents the specifications of a multi-agent simulation platform designed\nto support real-time, human-centered, and immersive studies of all road users,\naccompanied by open-source scripts for replication. Using high-fidelity\nimmersive virtual environments, our platform enables interaction across public\ntransit users, pedestrians, cyclists, automated vehicles, and drivers. The\narchitecture is modular, extensible, and designed for accessibility. The system\nintegrates hardware-specific modules - including an omnidirectional treadmill,\na seating arrangement, a smart trainer, and an actuated cockpit. Additionally,\nthe platform collects multimodal physiological, neurological, and behavioral\ndata through embedded sensing devices such as functional near-infrared\nspectroscopy (fNIRS), eye tracking, and wrist-based biosensors. To show the\nusability of this system, we present three use cases. Simulation for All aims\nto lower the barrier to entry for high-fidelity transportation simulation,\nsupport experimentation across disciplines, and advance our understanding of\nmultimodal mobility in complex urban environments.", "published": "2025-07-12 18:07:19", "link": "http://arxiv.org/abs/2507.09367v2", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition", "abstract": "Despite substantial improvements in ASR, performance tends to degrade when\nfaced with adverse conditions such as speaker accents. Generative error\ncorrection (GER) leverages the rich linguistic knowledge and exceptional\nreasoning ability of LLMs, significantly outperforming typical LM methods.\nHowever, it lacks specificity in accented speech scenarios. In this study, we\nleverage GER to improve the accuracy of transcription predictions by addressing\nthe two primary features of accented speech recognition. To fully leverage\npronunciation information, we propose the multi-modal GER, which integrates\npronunciation information from the speech modality, and the multi-granularity\nGER, which incorporates fine-grained phoneme-level information related to\npronunciation. These two methods enable the LLM to utilize the pronunciation\ninformation of accented speech and the semantic information from word-level\nhypotheses for accurate transcription predictions through LoRA fine-tuning. On\nthe one hand, we employ a three-stage training strategy to train separate\nmulti-modal GER models for each accent to obtain mono-accent LoRA experts. By\nadopting our proposed HDMoLE method, which incorporates hierarchical routing\nand dynamic thresholds within the mixture of LoRA experts, we effectively merge\nmultiple mono-accent LoRA experts within a single multi-modal GER to overcome\nthe challenges posed by accent diversity. On the other hand, multi-granularity\nGER leverages the N-best word-level and phoneme-level hypotheses generated by\nthe HDMoLE model to predict the final accented speech transcriptions.\nExperimental results on the multi-accent English dataset demonstrate the\nefficacy of our proposed methods. Our methods achieve a remarkable relative WER\nreduction of 67.35% compared to the Whisper-large-v3 baseline.", "published": "2025-07-12 02:14:50", "link": "http://arxiv.org/abs/2507.09116v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
