{"title": "Graph Neural Networks for Antisocial Behavior Detection on Twitter", "abstract": "Social media resurgence of antisocial behavior has exerted a downward spiral\non stereotypical beliefs, and hateful comments towards individuals and social\ngroups, as well as false or distorted news. The advances in graph neural\nnetworks employed on massive quantities of graph-structured data raise high\nhopes for the future of mediating communication on social media platforms. An\napproach based on graph convolutional data was employed to better capture the\ndependencies between the heterogeneous types of data.\n  Utilizing past and present experiences on the topic, we proposed and\nevaluated a graph-based approach for antisocial behavior detection, with\ngeneral applicability that is both language- and context-independent. In this\nresearch, we carried out an experimental validation of our graph-based approach\non several PAN datasets provided as part of their shared tasks, that enable the\ndiscussion of the results obtained by the proposed solution.", "published": "2023-12-28 00:25:12", "link": "http://arxiv.org/abs/2312.16755v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Representation with Intra-Modal and Inter-Modal Graph\n  Contrastive Learning for Multimodal Emotion Recognition", "abstract": "With the release of increasing open-source emotion recognition datasets on\nsocial media platforms and the rapid development of computing resources,\nmultimodal emotion recognition tasks (MER) have begun to receive widespread\nresearch attention. The MER task extracts and fuses complementary semantic\ninformation from different modalities, which can classify the speaker's\nemotions. However, the existing feature fusion methods have usually mapped the\nfeatures of different modalities into the same feature space for information\nfusion, which can not eliminate the heterogeneity between different modalities.\nTherefore, it is challenging to make the subsequent emotion class boundary\nlearning. To tackle the above problems, we have proposed a novel Adversarial\nRepresentation with Intra-Modal and Inter-Modal Graph Contrastive for\nMultimodal Emotion Recognition (AR-IIGCN) method. Firstly, we input video,\naudio, and text features into a multi-layer perceptron (MLP) to map them into\nseparate feature spaces. Secondly, we build a generator and a discriminator for\nthe three modal features through adversarial representation, which can achieve\ninformation interaction between modalities and eliminate heterogeneity among\nmodalities. Thirdly, we introduce contrastive graph representation learning to\ncapture intra-modal and inter-modal complementary semantic information and\nlearn intra-class and inter-class boundary information of emotion categories.\nSpecifically, we construct a graph structure for three modal features and\nperform contrastive representation learning on nodes with different emotions in\nthe same modality and the same emotion in different modalities, which can\nimprove the feature representation ability of nodes. Extensive experimental\nworks show that the ARL-IIGCN method can significantly improve emotion\nrecognition accuracy on IEMOCAP and MELD datasets.", "published": "2023-12-28 01:57:26", "link": "http://arxiv.org/abs/2312.16778v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OmniDialog: An Omnipotent Pre-training Model for Task-Oriented Dialogue\n  System", "abstract": "Pre-trained conversation models (PCMs) have demonstrated remarkable results\nin task-oriented dialogue (TOD) systems. Many PCMs focus predominantly on\ndialogue management tasks like dialogue state tracking, dialogue generation\ntasks like response generation, or both. However, the existing PCMs seldom\nconsider dialogue comprehension tasks, such as dialogue question answering and\nsummarization tasks. These tasks allow PCMs to glean dialogue context from\nvarious angles. This observation naturally raises the question: Can the\nperformance of downstream dialogue tasks be enhanced if a PCM is pre-trained on\ndialogue management, generation, and comprehension tasks?\n  To investigate this, we proposed an Omnipotent Dialogue pre-training model\n(OmniDialog). It unifies these three dialogue tasks into a monolithic framework\nby multi-task learning, fostering inter-task communication. The pre-training\ncorpus of OmniDialog spans $\\mathbf{7}$ dialogue-focused tasks, drawing from\n$\\mathbf{15}$ datasets and encompassing over $\\mathbf{3.2}$ million dialogue\nutterances. To our knowledge, OmniDialog is a pioneering PCM pre-trained across\ndialogue management, generation, and comprehension domains. We evaluated its\nperformance across four tasks: dialogue summarization, end-to-end dialogue\nmodeling, dialogue state tracking, and intent classification. The results\nunderscore its efficacy in domain transfer learning, low-resource, and\nfull-dataset scenarios. Furthermore, to glean a nuanced understanding of\nOmniDialog's strengths and potential pitfalls, we designed a fine-grained\nanalysis framework for dialogue-centric tasks. Experimental results show that\nthe OmniDialog is good at hard samples, such as long dialogues and lengthy\nresponses.", "published": "2023-12-28 07:20:49", "link": "http://arxiv.org/abs/2312.16864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Lattice Graph Fusion for Chinese Named Entity Recognition", "abstract": "Integrating lexicon into character-level sequence has been proven effective\nto leverage word boundary and semantic information in Chinese named entity\nrecognition (NER). However, prior approaches usually utilize feature weighting\nand position coupling to integrate word information, but ignore the semantic\nand contextual correspondence between the fine-grained semantic units in the\ncharacter-word space. To solve this issue, we propose a Unified Lattice Graph\nFusion (ULGF) approach for Chinese NER. ULGF can explicitly capture various\nsemantic and boundary relations across different semantic units with the\nadjacency matrix by converting the lattice structure into a unified graph. We\nstack multiple graph-based intra-source self-attention and inter-source\ncross-gating fusion layers that iteratively carry out semantic interactions to\nlearn node representations. To alleviate the over-reliance on word information,\nwe further propose to leverage lexicon entity classification as an auxiliary\ntask. Experiments on four Chinese NER benchmark datasets demonstrate the\nsuperiority of our ULGF approach.", "published": "2023-12-28 09:31:25", "link": "http://arxiv.org/abs/2312.16917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Length Extrapolation of Transformers: A Survey from the Perspective of\n  Positional Encoding", "abstract": "Built upon the Transformer, large language models (LLMs) have captured\nworldwide attention due to their remarkable abilities. Nevertheless, all\nTransformer-based models including LLMs suffer from a preset length limit and\ncan hardly generalize from short training sequences to longer inference ones,\nnamely, they cannot perform length extrapolation to handle long sequences,\nwhich severely hinders their application in scenarios demanding long input\nsequences such as legal or scientific documents. Thus, numerous methods have\nemerged to enhance the length extrapolation of Transformers. Despite the great\nresearch efforts, a systematic survey is still lacking. To fill this gap, we\ndelve into these advances in a unified notation from the perspective of\npositional encoding (PE), as it has been considered the primary factor on\nlength extrapolation. Specifically, we begin with extrapolatable PEs that have\ndominated this research field. Then, we dive into extrapolation methods based\non them, covering position interpolation and randomized position methods.\nFinally, several challenges and future directions in this area are highlighted.\nThrough this survey, we aim to enable the reader to gain a deep understanding\nof existing methods and provide stimuli for future research.", "published": "2023-12-28 14:42:24", "link": "http://arxiv.org/abs/2312.17044v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving In-context Learning via Bidirectional Alignment", "abstract": "Large language models (LLMs) have shown impressive few-shot generalization on\nmany tasks via in-context learning (ICL). Despite their success in showing such\nemergent abilities, the scale and complexity of larger models also lead to\nunprecedentedly high computational demands and deployment challenges. In\nreaction, researchers explore transferring the powerful capabilities of larger\nmodels to more efficient and compact models by typically aligning the output of\nsmaller (student) models with that of larger (teacher) models. Existing methods\neither train student models on the generated outputs of teacher models or\nimitate their token-level probability distributions. However, these\ndistillation methods pay little to no attention to the input, which also plays\na crucial role in ICL. Based on the finding that the performance of ICL is\nhighly sensitive to the selection of demonstration examples, we propose\nBidirectional Alignment (BiAlign) to fully leverage the models' preferences for\nICL examples to improve the ICL abilities of student models. Specifically, we\nintroduce the alignment of input preferences between student and teacher models\nby incorporating a novel ranking loss, in addition to aligning the token-level\noutput distribution. With extensive experiments and analysis, we demonstrate\nthat BiAlign can consistently outperform existing baselines on a variety of\ntasks involving language understanding, reasoning, and coding.", "published": "2023-12-28 15:02:03", "link": "http://arxiv.org/abs/2312.17055v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation", "abstract": "In this work, we introduce a novel evaluation paradigm for Large Language\nModels (LLMs) that compels them to transition from a traditional\nquestion-answering role, akin to a student, to a solution-scoring role, akin to\na teacher. This paradigm, focusing on \"reasoning about reasoning,\" hence termed\nmeta-reasoning, shifts the emphasis from result-oriented assessments, which\noften neglect the reasoning process, to a more comprehensive evaluation that\neffectively distinguishes between the cognitive capabilities of different\nmodels. By applying this paradigm in the GSM8K dataset, we have developed the\nMR-GSM8K benchmark. Our extensive analysis includes several state-of-the-art\nmodels from both open-source and commercial domains, uncovering fundamental\ndeficiencies in their training and evaluation methodologies. Notably, while\nmodels like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in\nGSM8K, their performance disparities expanded dramatically in MR-GSM8K, with\ndifferences widening to over 20 absolute points, underscoring the significant\nchallenge posed by our meta-reasoning approach.", "published": "2023-12-28 15:49:43", "link": "http://arxiv.org/abs/2312.17080v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Text in Arbitrary Writing Styles", "abstract": "Prior work in style-controlled text generation has focused on tasks such as\nemulating the style of prolific literary authors, producing formal or informal\ntext, and mitigating toxicity of generated text. Plentiful demonstrations of\nthese styles are available, and as a result modern language models are often\nable to emulate them, either via prompting or discriminative control. However,\nin applications such as writing assistants, it is desirable for language models\nto produce text in an author-specific style on the basis of a potentially small\nwriting sample. For example, someone writing in a particular dialect may prefer\nwriting suggestions that retain the same dialect. We find that\ninstruction-tuned language models can struggle to reproduce author-specific\nstyle demonstrated in a prompt. Instead, we propose to guide a language model\nto generate text in a target style using contrastively-trained representations\nthat capture stylometric features. Our approach (StyleMC) combines an\nauthor-adapted language model with sequence-level inference to improve\nstylistic consistency, and is found to be effective in a variety of conditions,\nincluding unconditional generation and style transfer. Additionally, we find\nthat the proposed approach can serve as an effective anonymization method, by\nediting a document to mask authorship while preserving the original meaning", "published": "2023-12-28 18:58:52", "link": "http://arxiv.org/abs/2312.17242v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effect of dimensionality change on the bias of word embeddings", "abstract": "Word embedding methods (WEMs) are extensively used for representing text\ndata. The dimensionality of these embeddings varies across various tasks and\nimplementations. The effect of dimensionality change on the accuracy of the\ndownstream task is a well-explored question. However, how the dimensionality\nchange affects the bias of word embeddings needs to be investigated. Using the\nEnglish Wikipedia corpus, we study this effect for two static (Word2Vec and\nfastText) and two context-sensitive (ElMo and BERT) WEMs. We have two\nobservations. First, there is a significant variation in the bias of word\nembeddings with the dimensionality change. Second, there is no uniformity in\nhow the dimensionality change affects the bias of word embeddings. These\nfactors should be considered while selecting the dimensionality of word\nembeddings.", "published": "2023-12-28 13:01:10", "link": "http://arxiv.org/abs/2312.17292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Packing in LLM Training Improves Long Context Utilization", "abstract": "Recent advancements in long-context large language models have attracted\nsignificant attention, yet their practical applications often suffer from\nsuboptimal context utilization. This study investigates structuring training\ndata to enhance semantic interdependence, demonstrating that this approach\neffectively improves context utilization. To this end, we introduce the\nStructured Packing for Long Context (SPLiCe) method, which utilizes retrieval\nto collate mutually relevant documents into long and coherent training\nexamples. We validate SPLiCe empirically across models of varying sizes -- 3B,\n7B, and 13B -- achieving improved performance in long-context tasks, such as\nQasper and HotpotQA. Remarkably, even brief fine-tuning with SPLiCe is\nsufficient to realize these benefits. Additionally, SPLiCe effectively\nmitigates the lost-in-middle phenomenon often observed in large models. Our\ncomprehensive analysis of SPLiCe explores its design choices and reveals\nintriguing transfer effects; for instance, training on programming code\nenhances performance on natural language tasks.", "published": "2023-12-28 16:25:52", "link": "http://arxiv.org/abs/2312.17296v9", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model as an Annotator: Unsupervised Context-aware Quality\n  Phrase Generation", "abstract": "Phrase mining is a fundamental text mining task that aims to identify quality\nphrases from context. Nevertheless, the scarcity of extensive gold labels\ndatasets, demanding substantial annotation efforts from experts, renders this\ntask exceptionally challenging. Furthermore, the emerging, infrequent, and\ndomain-specific nature of quality phrases presents further challenges in\ndealing with this task. In this paper, we propose LMPhrase, a novel\nunsupervised context-aware quality phrase mining framework built upon large\npre-trained language models (LMs). Specifically, we first mine quality phrases\nas silver labels by employing a parameter-free probing technique called\nPerturbed Masking on the pre-trained language model BERT (coined as Annotator).\nIn contrast to typical statistic-based or distantly-supervised methods, our\nsilver labels, derived from large pre-trained language models, take into\naccount rich contextual information contained in the LMs. As a result, they\nbring distinct advantages in preserving informativeness, concordance, and\ncompleteness of quality phrases. Secondly, training a discriminative span\nprediction model heavily relies on massive annotated data and is likely to face\nthe risk of overfitting silver labels. Alternatively, we formalize phrase\ntagging task as the sequence generation problem by directly fine-tuning on the\nSequence-to-Sequence pre-trained language model BART with silver labels (coined\nas Generator). Finally, we merge the quality phrases from both the Annotator\nand Generator as the final predictions, considering their complementary nature\nand distinct characteristics. Extensive experiments show that our LMPhrase\nconsistently outperforms all the existing competitors across two different\ngranularity phrase mining tasks, where each task is tested on two different\ndomain datasets.", "published": "2023-12-28 20:32:44", "link": "http://arxiv.org/abs/2312.17349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Syntactic Structures: Modeling Syntax for Various Natural\n  Languages", "abstract": "We aim to provide an explanation for how the human brain might connect words\nfor sentence formation. A novel approach to modeling syntactic representation\nis introduced, potentially showing the existence of universal syntactic\nstructures for all natural languages. As the discovery of DNA's double helix\nstructure shed light on the inner workings of genetics, we wish to introduce a\nbasic understanding of how language might work in the human brain. It could be\nthe brain's way of encoding and decoding knowledge. It also brings some insight\ninto theories in linguistics, psychology, and cognitive science. After looking\ninto the logic behind universal syntactic structures and the methodology of the\nmodeling technique, we attempt to analyze corpora that showcase universality in\nthe language process of different natural languages such as English and Korean.\nLastly, we discuss the critical period hypothesis, universal grammar, and a few\nother assertions on language for the purpose of advancing our understanding of\nthe human brain.", "published": "2023-12-28 20:44:26", "link": "http://arxiv.org/abs/2402.01641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hiding in Plain Sight: Towards the Science of Linguistic Steganography", "abstract": "Covert communication (also known as steganography) is the practice of\nconcealing a secret inside an innocuous-looking public object (cover) so that\nthe modified public object (covert code) makes sense to everyone but only\nsomeone who knows the code can extract the secret (message). Linguistic\nsteganography is the practice of encoding a secret message in natural language\ntext such as spoken conversation or short public communications such as\ntweets.. While ad hoc methods for covert communications in specific domains\nexist ( JPEG images, Chinese poetry, etc), there is no general model for\nlinguistic steganography specifically. We present a novel mathematical\nformalism for creating linguistic steganographic codes, with three parameters:\nDecodability (probability that the receiver of the coded message will decode\nthe cover correctly), density (frequency of code words in a cover code), and\ndetectability (probability that an attacker can tell the difference between an\nuntampered cover compared to its steganized version). Verbal or linguistic\nsteganography is most challenging because of its lack of artifacts to hide the\nsecret message in. We detail a practical construction in Python of a\nsteganographic code for Tweets using inserted words to encode hidden digits\nwhile using n-gram frequency distortion as the measure of detectability of the\ninsertions. Using the publicly accessible Stanford Sentiment Analysis dataset\nwe implemented the tweet steganization scheme -- a codeword (an existing word\nin the data set) inserted in random positions in random existing tweets to find\nthe tweet that has the least possible n-gram distortion. We argue that this\napproximates KL distance in a localized manner at low cost and thus we get a\nlinguistic steganography scheme that is both formal and practical and permits a\ntradeoff between codeword density and detectability of the covert message.", "published": "2023-12-28 06:00:55", "link": "http://arxiv.org/abs/2312.16840v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Evaluating the Performance of Large Language Models for Spanish Language\n  in Undergraduate Admissions Exams", "abstract": "This study evaluates the performance of large language models, specifically\nGPT-3.5 and BARD (supported by Gemini Pro model), in undergraduate admissions\nexams proposed by the National Polytechnic Institute in Mexico. The exams cover\nEngineering/Mathematical and Physical Sciences, Biological and Medical\nSciences, and Social and Administrative Sciences. Both models demonstrated\nproficiency, exceeding the minimum acceptance scores for respective academic\nprograms to up to 75% for some academic programs. GPT-3.5 outperformed BARD in\nMathematics and Physics, while BARD performed better in History and questions\nrelated to factual information. Overall, GPT-3.5 marginally surpassed BARD with\nscores of 60.94% and 60.42%, respectively.", "published": "2023-12-28 06:23:39", "link": "http://arxiv.org/abs/2312.16845v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones", "abstract": "In recent years, multimodal large language models (MLLMs) such as GPT-4V have\ndemonstrated remarkable advancements, excelling in a variety of vision-language\ntasks. Despite their prowess, the closed-source nature and computational\ndemands of such models limit their accessibility and applicability. This study\nintroduces TinyGPT-V, a novel open-source MLLM, designed for efficient training\nand inference across various vision-language tasks, including image captioning\n(IC) and visual question answering (VQA). Leveraging a compact yet powerful\narchitecture, TinyGPT-V integrates the Phi-2 language model with pre-trained\nvision encoders, utilizing a unique mapping module for visual and linguistic\ninformation fusion. With a training regimen optimized for small backbones and\nemploying a diverse dataset amalgam, TinyGPT-V requires significantly lower\ncomputational resources 24GB for training and as little as 8GB for inference\nwithout compromising on performance. Our experiments demonstrate that\nTinyGPT-V, with its language model 2.8 billion parameters, achieves comparable\nresults in VQA and image inference tasks to its larger counterparts while being\nuniquely suited for deployment on resource-constrained devices through\ninnovative quantization techniques. This work not only paves the way for more\naccessible and efficient MLLMs but also underscores the potential of smaller,\noptimized models in bridging the gap between high performance and computational\nefficiency in real-world applications. Additionally, this paper introduces a\nnew approach to multimodal large language models using smaller backbones. Our\ncode and training weights are available in the supplementary material.", "published": "2023-12-28 07:11:41", "link": "http://arxiv.org/abs/2312.16862v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence", "abstract": "Measuring the coherence of text is a vital aspect of evaluating the quality\nof written content. Recent advancements in neural coherence modeling have\ndemonstrated their efficacy in capturing entity coreference and discourse\nrelations, thereby enhancing coherence evaluation. However, many existing\nmethods heavily depend on static embeddings or focus narrowly on nearby\ncontext, constraining their capacity to measure the overarching coherence of\nlong texts. In this paper, we posit that coherent texts inherently manifest a\nsequential and cohesive interplay among sentences, effectively conveying the\ncentral theme, purpose, or standpoint. To explore this abstract relationship,\nwe introduce the \"BBScore,\" a novel reference-free metric grounded in Brownian\nbridge theory for assessing text coherence. Our findings showcase that when\nsynergized with a simple additional classification component, this metric\nattains a performance level comparable to state-of-the-art techniques on\nstandard artificial discrimination tasks. We also establish in downstream tasks\nthat this metric effectively differentiates between human-written documents and\ntext generated by large language models under a specific domain. Furthermore,\nwe illustrate the efficacy of this approach in detecting written styles\nattributed to diverse large language models, underscoring its potential for\ngeneralizability. In summary, we present a novel Brownian bridge coherence\nmetric capable of measuring both local and global text coherence, while\ncircumventing the need for end-to-end model training. This flexibility allows\nfor its application in various downstream tasks.", "published": "2023-12-28 08:34:17", "link": "http://arxiv.org/abs/2312.16893v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spike No More: Stabilizing the Pre-training of Large Language Models", "abstract": "Loss spikes often occur during pre-training of large language models. The\nspikes degrade the performance of large language models and sometimes ruin the\npre-training. Since the pre-training needs a vast computational budget, we\nshould avoid such spikes. Based on the assumption that the loss spike is caused\nby the sudden growth of the gradient norm, we explore factors to keep the\ngradient norm small through an analysis of the spectral norms of the Jacobian\nmatrices for the sub-layers. Our findings suggest that stabilizing the\npre-training process requires two conditions: small sub-layers and large\nshortcut. We conduct various experiments to empirically verify our theoretical\nanalyses. Experimental results demonstrate that methods satisfying the\nconditions effectively prevent loss spikes during pre-training.", "published": "2023-12-28 08:53:27", "link": "http://arxiv.org/abs/2312.16903v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot learning for automated content analysis: Efficient coding of\n  arguments and claims in the debate on arms deliveries to Ukraine", "abstract": "Pre-trained language models (PLM) based on transformer neural networks\ndeveloped in the field of natural language processing (NLP) offer great\nopportunities to improve automatic content analysis in communication science,\nespecially for the coding of complex semantic categories in large datasets via\nsupervised machine learning. However, three characteristics so far impeded the\nwidespread adoption of the methods in the applying disciplines: the dominance\nof English language models in NLP research, the necessary computing resources,\nand the effort required to produce training data to fine-tune PLMs. In this\nstudy, we address these challenges by using a multilingual transformer model in\ncombination with the adapter extension to transformers, and few-shot learning\nmethods. We test our approach on a realistic use case from communication\nscience to automatically detect claims and arguments together with their stance\nin the German news debate on arms deliveries to Ukraine. In three experiments,\nwe evaluate (1) data preprocessing strategies and model variants for this task,\n(2) the performance of different few-shot learning methods, and (3) how well\nthe best setup performs on varying training set sizes in terms of validity,\nreliability, replicability and reproducibility of the results. We find that our\nproposed combination of transformer adapters with pattern exploiting training\nprovides a parameter-efficient and easily shareable alternative to fully\nfine-tuning PLMs. It performs on par in terms of validity, while overall,\nprovides better properties for application in communication studies. The\nresults also show that pre-fine-tuning for a task on a near-domain dataset\nleads to substantial improvement, in particular in the few-shot setting.\nFurther, the results indicate that it is useful to bias the dataset away from\nthe viewpoints of specific prominent individuals.", "published": "2023-12-28 11:39:08", "link": "http://arxiv.org/abs/2312.16975v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "How Far Are LLMs from Believable AI? A Benchmark for Evaluating the\n  Believability of Human Behavior Simulation", "abstract": "In recent years, AI has demonstrated remarkable capabilities in simulating\nhuman behaviors, particularly those implemented with large language models\n(LLMs). However, due to the lack of systematic evaluation of LLMs' simulated\nbehaviors, the believability of LLMs among humans remains ambiguous, i.e., it\nis unclear which behaviors of LLMs are convincingly human-like and which need\nfurther improvements. In this work, we design SimulateBench to evaluate the\nbelievability of LLMs when simulating human behaviors. In specific, we evaluate\nthe believability of LLMs based on two critical dimensions: 1) consistency: the\nextent to which LLMs can behave consistently with the given information of a\nhuman to simulate; and 2) robustness: the ability of LLMs' simulated behaviors\nto remain robust when faced with perturbations. SimulateBench includes 65\ncharacter profiles and a total of 8,400 questions to examine LLMs' simulated\nbehaviors. Based on SimulateBench, we evaluate the performances of 10 widely\nused LLMs when simulating characters. The experimental results reveal that\ncurrent LLMs struggle to align their behaviors with assigned characters and are\nvulnerable to perturbations in certain factors.", "published": "2023-12-28 16:51:11", "link": "http://arxiv.org/abs/2312.17115v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Virtual Scientific Companion for Synchrotron Beamlines: A Prototype", "abstract": "The extraordinarily high X-ray flux and specialized instrumentation at\nsynchrotron beamlines have enabled versatile in-situ and high throughput\nstudies that are impossible elsewhere. Dexterous and efficient control of\nexperiments are thus crucial for efficient beamline operation. Artificial\nintelligence and machine learning methods are constantly being developed to\nenhance facility performance, but the full potential of these developments can\nonly be reached with efficient human-computer-interaction. Natural language is\nthe most intuitive and efficient way for humans to communicate. However, the\nlow credibility and reproducibility of existing large language models and tools\ndemand extensive development to be made for robust and reliable performance for\nscientific purposes. In this work, we introduce the prototype of virtual\nscientific companion (VISION) and demonstrate that it is possible to control\nbasic beamline operations through natural language with open-source language\nmodel and the limited computational resources at beamline. The human-AI nature\nof VISION leverages existing automation systems and data framework at\nsynchrotron beamlines.", "published": "2023-12-28 18:12:42", "link": "http://arxiv.org/abs/2312.17180v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The LLM Surgeon", "abstract": "State-of-the-art language models are becoming increasingly large in an effort\nto achieve the highest performance on large corpora of available textual data.\nHowever, the sheer size of the Transformer architectures makes it difficult to\ndeploy models within computational, environmental or device-specific\nconstraints. We explore data-driven compression of existing pretrained models\nas an alternative to training smaller models from scratch. To do so, we scale\nKronecker-factored curvature approximations of the target loss landscape to\nlarge language models. In doing so, we can compute both the dynamic allocation\nof structures that can be removed as well as updates of remaining weights that\naccount for the removal. We provide a general framework for unstructured,\nsemi-structured and structured pruning and improve upon weight updates to\ncapture more correlations between weights, while remaining computationally\nefficient. Experimentally, our method can prune rows and columns from a range\nof OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance,\nand achieve state-of-the-art results in unstructured and semi-structured\npruning of large language models.", "published": "2023-12-28 18:59:09", "link": "http://arxiv.org/abs/2312.17244v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AI Content Self-Detection for Transformer-based Large Language Models", "abstract": "$ $The usage of generative artificial intelligence (AI) tools based on large\nlanguage models, including ChatGPT, Bard, and Claude, for text generation has\nmany exciting applications with the potential for phenomenal productivity\ngains. One issue is authorship attribution when using AI tools. This is\nespecially important in an academic setting where the inappropriate use of\ngenerative AI tools may hinder student learning or stifle research by creating\na large amount of automatically generated derivative work. Existing plagiarism\ndetection systems can trace the source of submitted text but are not yet\nequipped with methods to accurately detect AI-generated text. This paper\nintroduces the idea of direct origin detection and evaluates whether generative\nAI systems can recognize their output and distinguish it from human-written\ntexts. We argue why current transformer-based models may be able to self-detect\ntheir own generated text and perform a small empirical study using zero-shot\nlearning to investigate if that is the case. Results reveal varying\ncapabilities of AI systems to identify their generated text. Google's Bard\nmodel exhibits the largest capability of self-detection with an accuracy of\n94\\%, followed by OpenAI's ChatGPT with 83\\%. On the other hand, Anthropic's\nClaude model seems to be not able to self-detect.", "published": "2023-12-28 10:08:57", "link": "http://arxiv.org/abs/2312.17289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Experiential Co-Learning of Software-Developing Agents", "abstract": "Recent advancements in large language models (LLMs) have brought significant\nchanges to various domains, especially through LLM-driven autonomous agents. A\nrepresentative scenario is in software development, where LLM agents\ndemonstrate efficient collaboration, task division, and assurance of software\nquality, markedly reducing the need for manual involvement. However, these\nagents frequently perform a variety of tasks independently, without benefiting\nfrom past experiences, which leads to repeated mistakes and inefficient\nattempts in multi-step task execution. To this end, we introduce Experiential\nCo-Learning, a novel LLM-agent learning framework in which instructor and\nassistant agents gather shortcut-oriented experiences from their historical\ntrajectories and use these past experiences for future task execution. The\nextensive experiments demonstrate that the framework enables agents to tackle\nunseen software-developing tasks more effectively. We anticipate that our\ninsights will guide LLM agents towards enhanced autonomy and contribute to\ntheir evolutionary growth in cooperative learning. The code and data are\navailable at https://github.com/OpenBMB/ChatDev.", "published": "2023-12-28 13:50:42", "link": "http://arxiv.org/abs/2312.17025v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined\n  Levels", "abstract": "The explosion of visual content available online underscores the requirement\nfor an accurate machine assessor to robustly evaluate scores across diverse\ntypes of visual contents. While recent studies have demonstrated the\nexceptional potentials of large multi-modality models (LMMs) on a wide range of\nrelated fields, in this work, we explore how to teach them for visual rating\naligned with human opinions. Observing that human raters only learn and judge\ndiscrete text-defined levels in subjective studies, we propose to emulate this\nsubjective process and teach LMMs with text-defined rating levels instead of\nscores. The proposed Q-Align achieves state-of-the-art performance on image\nquality assessment (IQA), image aesthetic assessment (IAA), as well as video\nquality assessment (VQA) tasks under the original LMM structure. With the\nsyllabus, we further unify the three tasks into one model, termed the OneAlign.\nIn our experiments, we demonstrate the advantage of the discrete-level-based\nsyllabus over direct-score-based variants for LMMs. Our code and the\npre-trained weights are released at https://github.com/Q-Future/Q-Align.", "published": "2023-12-28 16:10:25", "link": "http://arxiv.org/abs/2312.17090v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MIVC: Multiple Instance Visual Component for Visual-Language Models", "abstract": "Vision-language models have been widely explored across a wide range of tasks\nand achieve satisfactory performance. However, it's under-explored how to\nconsolidate entity understanding through a varying number of images and to\nalign it with the pre-trained language models for generative tasks. In this\npaper, we propose MIVC, a general multiple instance visual component to bridge\nthe gap between various image inputs with off-the-shelf vision-language models\nby aggregating visual representations in a permutation-invariant fashion\nthrough a neural network. We show that MIVC could be plugged into the\nvisual-language models to improve the model performance consistently on visual\nquestion answering, classification and captioning tasks on a public available\ne-commerce dataset with multiple images per product. Furthermore, we show that\nthe component provides insight into the contribution of each image to the\ndownstream tasks.", "published": "2023-12-28 16:33:32", "link": "http://arxiv.org/abs/2312.17109v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MathPile: A Billion-Token-Scale Pretraining Corpus for Math", "abstract": "High-quality, large-scale corpora are the cornerstone of building foundation\nmodels. In this work, we introduce MathPile, a diverse and high-quality\nmath-centric corpus comprising about 9.5 billion tokens. Throughout its\ncreation, we adhered to the principle of \"less is more\", firmly believing in\nthe supremacy of data quality over quantity, even in the pre-training phase.\nOur meticulous data collection and processing efforts included a complex suite\nof preprocessing, prefiltering, language identification, cleaning, filtering,\nand deduplication, ensuring the high quality of our corpus. Furthermore, we\nperformed data contamination detection on downstream benchmark test sets to\neliminate duplicates and conducted continual pre-training experiments, booting\nthe performance on common mathematical reasoning benchmarks. We aim for our\nMathPile to boost language models' mathematical reasoning abilities and\nopen-source its different versions and processing scripts to advance the field.", "published": "2023-12-28 16:55:40", "link": "http://arxiv.org/abs/2312.17120v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM4Causal: Democratized Causal Tools for Everyone via Large Language\n  Model", "abstract": "Large Language Models (LLMs) have shown their success in language\nunderstanding and reasoning on general topics. However, their capability to\nperform inference based on user-specified structured data and knowledge in\ncorpus-rare concepts, such as causal decision-making is still limited. In this\nwork, we explore the possibility of fine-tuning an open-sourced LLM into\nLLM4Causal, which can identify the causal task, execute a corresponding\nfunction, and interpret its numerical results based on users' queries and the\nprovided dataset. Meanwhile, we propose a data generation process for more\ncontrollable GPT prompting and present two instruction-tuning datasets: (1)\nCausal-Retrieval-Bench for causal problem identification and input parameter\nextraction for causal function calling and (2) Causal-Interpret-Bench for\nin-context causal interpretation. By conducting end-to-end evaluations and two\nablation studies, we showed that LLM4Causal can deliver end-to-end solutions\nfor causal problems and provide easy-to-understand answers, which significantly\noutperforms the baselines.", "published": "2023-12-28 16:59:06", "link": "http://arxiv.org/abs/2312.17122v4", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision,\n  Language, Audio, and Action", "abstract": "We present Unified-IO 2, the first autoregressive multimodal model that is\ncapable of understanding and generating image, text, audio, and action. To\nunify different modalities, we tokenize inputs and outputs -- images, text,\naudio, action, bounding boxes, etc., into a shared semantic space and then\nprocess them with a single encoder-decoder transformer model. Since training\nwith such diverse modalities is challenging, we propose various architectural\nimprovements to stabilize model training. We train our model from scratch on a\nlarge multimodal pre-training corpus from diverse sources with a multimodal\nmixture of denoisers objective. To learn an expansive set of skills, such as\nfollowing multimodal instructions, we construct and finetune on an ensemble of\n120 datasets with prompts and augmentations. With a single unified model,\nUnified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and\nstrong results in more than 35 benchmarks, including image generation and\nunderstanding, natural language understanding, video and audio understanding,\nand robotic manipulation. We release all our models to the research community.", "published": "2023-12-28 17:57:06", "link": "http://arxiv.org/abs/2312.17172v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Do Androids Know They're Only Dreaming of Electric Sheep?", "abstract": "We design probes trained on the internal representations of a transformer\nlanguage model to predict its hallucinatory behavior on three grounded\ngeneration tasks. To train the probes, we annotate for span-level hallucination\non both sampled (organic) and manually edited (synthetic) reference outputs.\nOur probes are narrowly trained and we find that they are sensitive to their\ntraining domain: they generalize poorly from one task to another or from\nsynthetic to organic hallucinations. However, on in-domain data, they can\nreliably detect hallucinations at many transformer layers, achieving 95% of\ntheir peak performance as early as layer 4. Here, probing proves accurate for\nevaluating hallucination, outperforming several contemporary baselines and even\nsurpassing an expert human annotator in response-level detection F1. Similarly,\non span-level labeling, probes are on par or better than the expert annotator\non two out of three generation tasks. Overall, we find that probing is a\nfeasible and efficient alternative to language model hallucination evaluation\nwhen model states are available.", "published": "2023-12-28 18:59:50", "link": "http://arxiv.org/abs/2312.17249v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing watermarks for large language models", "abstract": "With the rise of large language models (LLMs) and concerns about potential\nmisuse, watermarks for generative LLMs have recently attracted much attention.\nAn important aspect of such watermarks is the trade-off between their\nidentifiability and their impact on the quality of the generated text. This\npaper introduces a systematic approach to this trade-off in terms of a\nmulti-objective optimization problem. For a large class of robust, efficient\nwatermarks, the associated Pareto optimal solutions are identified and shown to\noutperform the currently default watermark.", "published": "2023-12-28 16:10:51", "link": "http://arxiv.org/abs/2312.17295v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Exploring Nature: Datasets and Models for Analyzing Nature-Related\n  Disclosures", "abstract": "Nature is an amorphous concept. Yet, it is essential for the planet's\nwell-being to understand how the economy interacts with it. To address the\ngrowing demand for information on corporate nature disclosure, we provide\ndatasets and classifiers to detect nature communication by companies. We ground\nour approach in the guidelines of the Taskforce on Nature-related Financial\nDisclosures (TNFD). Particularly, we focus on the specific dimensions of water,\nforest, and biodiversity. For each dimension, we create an expert-annotated\ndataset with 2,200 text samples and train classifier models. Furthermore, we\nshow that nature communication is more prevalent in hotspot areas and directly\neffected industries like agriculture and utilities. Our approach is the first\nto respond to calls to assess corporate nature communication on a large scale.", "published": "2023-12-28 19:42:14", "link": "http://arxiv.org/abs/2312.17337v1", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language\n  Models for Private and Secure Inference", "abstract": "This paper addresses the privacy and security concerns associated with deep\nneural language models, which serve as crucial components in various modern\nAI-based applications. These models are often used after being pre-trained and\nfine-tuned for specific tasks, with deployment on servers accessed through the\ninternet. However, this introduces two fundamental risks: (a) the transmission\nof user inputs to the server via the network gives rise to interception\nvulnerabilities, and (b) privacy concerns emerge as organizations that deploy\nsuch models store user data with restricted context. To address this, we\npropose a novel method to adapt and fine-tune transformer-based language models\non passkey-encrypted user-specific text. The original pre-trained language\nmodel first undergoes a quick adaptation (without any further pre-training)\nwith a series of irreversible transformations applied to the tokenizer and\ntoken embeddings. This enables the model to perform inference on encrypted\ninputs while preventing reverse engineering of text from model parameters and\nintermediate outputs. After adaptation, models are fine-tuned on encrypted\nversions of existing training datasets. Experimental evaluation employing\nadapted versions of renowned models (e.g., BERT, RoBERTa) across established\nbenchmark English and multilingual datasets for text classification and\nsequence labeling shows that encrypted models achieve performance parity with\ntheir original counterparts. This serves to safeguard performance, privacy, and\nsecurity cohesively.", "published": "2023-12-28 19:55:11", "link": "http://arxiv.org/abs/2312.17342v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "DrugAssist: A Large Language Model for Molecule Optimization", "abstract": "Recently, the impressive performance of large language models (LLMs) on a\nwide range of tasks has attracted an increasing number of attempts to apply\nLLMs in drug discovery. However, molecule optimization, a critical task in the\ndrug discovery pipeline, is currently an area that has seen little involvement\nfrom LLMs. Most of existing approaches focus solely on capturing the underlying\npatterns in chemical structures provided by the data, without taking advantage\nof expert feedback. These non-interactive approaches overlook the fact that the\ndrug discovery process is actually one that requires the integration of expert\nexperience and iterative refinement. To address this gap, we propose\nDrugAssist, an interactive molecule optimization model which performs\noptimization through human-machine dialogue by leveraging LLM's strong\ninteractivity and generalizability. DrugAssist has achieved leading results in\nboth single and multiple property optimization, simultaneously showcasing\nimmense potential in transferability and iterative optimization. In addition,\nwe publicly release a large instruction-based dataset called\nMolOpt-Instructions for fine-tuning language models on molecule optimization\ntasks. We have made our code and data publicly available at\nhttps://github.com/blazerye/DrugAssist, which we hope to pave the way for\nfuture research in LLMs' application for drug discovery.", "published": "2023-12-28 10:46:56", "link": "http://arxiv.org/abs/2401.10334v1", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.QM"}
{"title": "Multimodal Sentiment Analysis with Missing Modality: A\n  Knowledge-Transfer Approach", "abstract": "Multimodal sentiment analysis aims to identify the emotions expressed by\nindividuals through visual, language, and acoustic cues. However, most of the\nexisting research efforts assume that all modalities are available during both\ntraining and testing, making their algorithms susceptible to the missing\nmodality scenario. In this paper, we propose a novel knowledge-transfer network\nto translate between different modalities to reconstruct the missing audio\nmodalities. Moreover, we develop a cross-modality attention mechanism to retain\nthe maximal information of the reconstructed and observed modalities for\nsentiment prediction. Extensive experiments on three publicly available\ndatasets demonstrate significant improvements over baselines and achieve\ncomparable results to the previous methods with complete multi-modality\nsupervision.", "published": "2023-12-28 06:47:18", "link": "http://arxiv.org/abs/2401.10747v4", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AQUALLM: Audio Question Answering Data Generation Using Large Language\n  Models", "abstract": "Audio Question Answering (AQA) constitutes a pivotal task in which machines\nanalyze both audio signals and natural language questions to produce precise\nnatural language answers. The significance of possessing high-quality, diverse,\nand extensive AQA datasets cannot be overstated when aiming for the precision\nof an AQA system. While there has been notable focus on developing accurate and\nefficient AQA models, the creation of high-quality, diverse, and extensive\ndatasets for the specific task at hand has not garnered considerable attention.\nTo address this challenge, this work makes several contributions. We introduce\na scalable AQA data generation pipeline, denoted as the AQUALLM framework,\nwhich relies on Large Language Models (LLMs). This framework utilizes existing\naudio-caption annotations and incorporates state-of-the-art LLMs to generate\nexpansive, high-quality AQA datasets. Additionally, we present three extensive\nand high-quality benchmark datasets for AQA, contributing significantly to the\nprogression of AQA research. AQA models trained on the proposed datasets set\nsuperior benchmarks compared to the existing state-of-the-art. Moreover, models\ntrained on our datasets demonstrate enhanced generalizability when compared to\nmodels trained using human-annotated AQA data. Code and datasets will be\naccessible on GitHub~\\footnote{\\url{https://github.com/swarupbehera/AQUALLM}}.", "published": "2023-12-28 20:01:27", "link": "http://arxiv.org/abs/2312.17343v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Uncertainty Quantification in Machine Learning for Joint Speaker\n  Diarization and Identification", "abstract": "This paper studies modulation spectrum features ($\\Phi$) and mel-frequency\ncepstral coefficients ($\\Psi$) in joint speaker diarization and identification\n(JSID). JSID is important as speaker diarization on its own to distinguish\nspeakers is insufficient for many applications, it is often necessary to\nidentify speakers as well. Machine learning models are set up using\nconvolutional neural networks (CNNs) on $\\Phi$ and recurrent neural networks\n$\\unicode{x2013}$ long short-term memory (LSTMs) on $\\Psi$, then concatenating\ninto fully connected layers.\n  Experiment 1 shows models on both $\\Phi$ and $\\Psi$ have better diarization\nerror rates (DERs) than models on either alone; a CNN on $\\Phi$ has DER\n29.09\\%, compared to 27.78\\% for a LSTM on $\\Psi$ and 19.44\\% for a model on\nboth. Experiment 1 also investigates aleatoric uncertainties and shows the\nmodel on both $\\Phi$ and $\\Psi$ has mean entropy 0.927~bits (out of 4~bits) for\ncorrect predictions compared to 1.896~bits for incorrect predictions which,\nalong with entropy histogram shapes, shows the model helpfully indicates where\nit is uncertain.\n  Experiment 2 investigates epistemic uncertainties as well as aleatoric using\nMonte Carlo dropout (MCD). It compares models on both $\\Phi$ and $\\Psi$ with\nmodels trained on x-vectors ($X$), before applying Kalman filter smoothing on\nepistemic uncertainties for resegmentation and model ensembles. While the two\nmodels on $X$ (DERs 10.23\\% and 9.74\\%) outperform those on $\\Phi$ and $\\Psi$\n(DER 17.85\\%) after their individual Kalman filter smoothing, combining them\nusing a Kalman filter smoothing method improves the DER to 9.29\\%. Aleatoric\nuncertainties are higher for incorrect predictions.\n  Both Experiments show models on $\\Phi$ do not distinguish overlapping\nspeakers as well as anticipated. However, Experiment 2 shows model ensembles do\nbetter with overlapping speakers than individual models do.", "published": "2023-12-28 01:17:11", "link": "http://arxiv.org/abs/2312.16763v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A New Perspective on Speaker Verification: Joint Modeling with DFSMN and\n  Transformer", "abstract": "Speaker verification is to judge the similarity between two unknown voices in\nan open set, where the ideal speaker embedding should be able to condense\ndiscriminant information into a compact utterance-level representation that has\nsmall intra-speaker distances and large inter-speaker distances. We propose\nVoice Transformer (VOT), a novel model for speaker verification, which\nintegrates parallel transformers at multiple scales. A deep feedforward\nsequential memory network (DFSMN) is incorporated into the attention part of\nthese transformers to increase feature granularity. The attentive statistics\npooling layer is added to focus on important frames and form utterance-level\nfeatures. We propose Additive Angular Margin Focal Loss (AAMF) to address the\nhard samples problem. We evaluate the proposed approach on the VoxCeleb1 and\nCN-Celeb2 datasets, demonstrating that VOT surpasses most mainstream models.\nThe code is available on\nGitHub\\footnote{\\url{https://github.com/luckyerr/Voice-Transformer_Speaker-Verification}}.", "published": "2023-12-28 04:55:59", "link": "http://arxiv.org/abs/2312.16826v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Remixed2Remixed: Domain adaptation for speech enhancement by Noise2Noise\n  learning with Remixing", "abstract": "This paper proposes Remixed2Remixed, a domain adaptation method for speech\nenhancement, which adopts Noise2Noise (N2N) learning to adapt models trained on\nartificially generated (out-of-domain: OOD) noisy-clean pair data to better\nseparate real-world recorded (in-domain) noisy data. The proposed method uses a\nteacher model trained on OOD data to acquire pseudo-in-domain speech and noise\nsignals, which are shuffled and remixed twice in each batch to generate two\nbootstrapped mixtures. The student model is then trained by optimizing an\nN2N-based cost function computed using these two bootstrapped mixtures. As the\ntraining strategy is similar to the recently proposed RemixIT, we also\ninvestigate the effectiveness of N2N-based loss as a regularization of RemixIT.\nExperimental results on the CHiME-7 unsupervised domain adaptation for\nconversational speech enhancement (UDASE) task revealed that the proposed\nmethod outperformed the challenge baseline system, RemixIT, and reduced the\nblurring of performance caused by teacher models.", "published": "2023-12-28 05:45:03", "link": "http://arxiv.org/abs/2312.16836v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accent-VITS:accent transfer for end-to-end TTS", "abstract": "Accent transfer aims to transfer an accent from a source speaker to synthetic\nspeech in the target speaker's voice. The main challenge is how to effectively\ndisentangle speaker timbre and accent which are entangled in speech. This paper\npresents a VITS-based end-to-end accent transfer model named Accent-VITS.Based\non the main structure of VITS, Accent-VITS makes substantial improvements to\nenable effective and stable accent transfer.We leverage a hierarchical CVAE\nstructure to model accent pronunciation information and acoustic features,\nrespectively, using bottleneck features and mel spectrums as\nconstraints.Moreover, the text-to-wave mapping in VITS is decomposed into\ntext-to-accent and accent-to-wave mappings in Accent-VITS. In this way, the\ndisentanglement of accent and speaker timbre becomes be more stable and\neffective.Experiments on multi-accent and Mandarin datasets show that\nAccent-VITS achieves higher speaker similarity, accent similarity and speech\nnaturalness as compared with a strong baseline.", "published": "2023-12-28 06:37:52", "link": "http://arxiv.org/abs/2312.16850v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binaural recording methods with analysis on inter-aural time, level, and\n  phase differences", "abstract": "Binaural recordings are a form of stereophonic recording method that\nreplicates how human ears perceive sound, these types of recordings create a 3D\naural image around the listener and are extremely immersive when well recorded\nand listened to appropriately with headphones. It has wide applications in\nvideo, podcast, and gaming formats -- allowing the listener to feel like they\nare there. Although binaural formats are seldom used for music applications,\nthey have also been utilized in music ranging from Rock, Jazz, Acoustic, and\nClassical. In this paper, we will investigate the acoustical phenomenon that\nproduces the binaural effect in audio recordings -- including the ITD\n(Inter-aural time difference), the ILD (inter-aural level difference), IPD\n(inter-aural phase difference) as well as the monaural spectral difference that\noccurs between two ears so we can better understand the replication of human\nhearing in binaural recordings. Binaural recordings differ from regular\nstereophonic recordings as they are arranged in a specific way to account for\nHRTF (Head-related transfer function). The most common method of binaural\nrecordings is with two high-quality omni-directional microphones affixed on a\ndummy head where the ears are located, although other methods exist without the\nuse of a full dummy head.", "published": "2023-12-28 08:18:04", "link": "http://arxiv.org/abs/2312.16884v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Jeffreys divergence-based regularization of neural network output\n  distribution applied to speaker recognition", "abstract": "A new loss function for speaker recognition with deep neural network is\nproposed, based on Jeffreys Divergence. Adding this divergence to the\ncross-entropy loss function allows to maximize the target value of the output\ndistribution while smoothing the non-target values. This objective function\nprovides highly discriminative features. Beyond this effect, we propose a\ntheoretical justification of its effectiveness and try to understand how this\nloss function affects the model, in particular the impact on dataset types\n(i.e. in-domain or out-of-domain w.r.t the training corpus). Our experiments\nshow that Jeffreys loss consistently outperforms the state-of-the-art for\nspeaker recognition, especially on out-of-domain data, and helps limit false\nalarms.", "published": "2023-12-28 08:18:48", "link": "http://arxiv.org/abs/2312.16885v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BEAST: Online Joint Beat and Downbeat Tracking Based on Streaming\n  Transformer", "abstract": "Many deep learning models have achieved dominant performance on the offline\nbeat tracking task. However, online beat tracking, in which only the past and\npresent input features are available, still remains challenging. In this paper,\nwe propose BEAt tracking Streaming Transformer (BEAST), an online joint beat\nand downbeat tracking system based on the streaming Transformer. To deal with\nonline scenarios, BEAST applies contextual block processing in the Transformer\nencoder. Moreover, we adopt relative positional encoding in the attention layer\nof the streaming Transformer encoder to capture relative timing position which\nis critically important information in music. Carrying out beat and downbeat\nexperiments on benchmark datasets for a low latency scenario with maximum\nlatency under 50 ms, BEAST achieves an F1-measure of 80.04% in beat and 46.78%\nin downbeat, which is a substantial improvement of about 5 percentage points\nover the state-of-the-art online beat tracking model.", "published": "2023-12-28 17:43:39", "link": "http://arxiv.org/abs/2312.17156v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Revolutionizing Personalized Voice Synthesis: The Journey towards\n  Emotional and Individual Authenticity with DIVSE (Dynamic Individual Voice\n  Synthesis Engine)", "abstract": "This comprehensive paper delves into the forefront of personalized voice\nsynthesis within artificial intelligence (AI), spotlighting the Dynamic\nIndividual Voice Synthesis Engine (DIVSE). DIVSE represents a groundbreaking\nleap in text-to-voice (TTS) technology, uniquely focusing on adapting and\npersonalizing voice outputs to match individual vocal characteristics. The\nresearch underlines the gap in current AI-generated voices, which, while\ntechnically advanced, fall short in replicating the unique individuality and\nexpressiveness intrinsic to human speech. It outlines the challenges and\nadvancements in personalized voice synthesis, emphasizing the importance of\nemotional expressiveness, accent and dialect variability, and capturing\nindividual voice traits. The architecture of DIVSE is meticulously detailed,\nshowcasing its three core components: Voice Characteristic Learning Module\n(VCLM), Emotional Tone and Accent Adaptation Module (ETAAM), and Dynamic Speech\nSynthesis Engine (DSSE). The innovative approach of DIVSE lies in its adaptive\nlearning capability, which evolves over time to tailor voice outputs to\nspecific user traits. The paper presents a rigorous experimental setup,\nutilizing accepted datasets and personalization metrics like Mean Opinion Score\n(MOS) and Emotional Alignment Score, to validate DIVSE's superiority over\nmainstream models. The results depict a clear advancement in achieving higher\npersonalization and emotional resonance in AI-generated voices.", "published": "2023-12-28 00:59:13", "link": "http://arxiv.org/abs/2312.17281v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Arrow of Time in Music -- Revisiting the Temporal Structure of Music\n  with Distinguishability and Unique Orientability as the Anchor Point", "abstract": "Driven by the term \"the arrow of time\" as a general topic, the article\ndevelops a musical discussion by referring to the etymological origin of the\nterm: philosophy (epistemology) and physics (thermodynamics). In particular,\nthe article explores two specific conditions: distinguishability and unique\norientability, from which the article derives respective musical propositions\nand case studies. For the distinguishability condition, the article focuses on\nthe \"recurrence\" in music and tries to interpret Bach's Christmas Oratorio from\nthe perspective of \"birth/resurrection\". For the unique orientability\ncondition, the article discusses the process of delaying the climax, thereby\nproposing \"AB-AAB left-replication\" model, implying an organicist view by\ntreating the temporal structure of music (e.g. form) as the product of a\ndynamic process: organic growth.", "published": "2023-12-28 10:14:00", "link": "http://arxiv.org/abs/2312.17633v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
