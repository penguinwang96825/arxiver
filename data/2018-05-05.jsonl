{"title": "Various Approaches to Aspect-based Sentiment Analysis", "abstract": "The problem of aspect-based sentiment analysis deals with classifying\nsentiments (negative, neutral, positive) for a given aspect in a sentence. A\ntraditional sentiment classification task involves treating the entire sentence\nas a text document and classifying sentiments based on all the words. Let us\nassume, we have a sentence such as \"the acceleration of this car is fast, but\nthe reliability is horrible\". This can be a difficult sentence because it has\ntwo aspects with conflicting sentiments about the same entity. Considering\nmachine learning techniques (or deep learning), how do we encode the\ninformation that we are interested in one aspect and its sentiment but not the\nother? Let us explore various pre-processing steps, features, and methods used\nto facilitate in solving this task.", "published": "2018-05-05 02:44:51", "link": "http://arxiv.org/abs/1805.01984v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese NER Using Lattice LSTM", "abstract": "We investigate a lattice-structured LSTM model for Chinese NER, which encodes\na sequence of input characters as well as all potential words that match a\nlexicon. Compared with character-based methods, our model explicitly leverages\nword and word sequence information. Compared with word-based methods, lattice\nLSTM does not suffer from segmentation errors. Gated recurrent cells allow our\nmodel to choose the most relevant characters and words from a sentence for\nbetter NER results. Experiments on various datasets show that lattice LSTM\noutperforms both word-based and character-based LSTM baselines, achieving the\nbest results.", "published": "2018-05-05 08:48:32", "link": "http://arxiv.org/abs/1805.02023v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Representation of Morphologically-Rich Input for Neural\n  Machine Translation", "abstract": "Neural machine translation (NMT) models are typically trained with fixed-size\ninput and output vocabularies, which creates an important bottleneck on their\naccuracy and generalization capability. As a solution, various studies proposed\nsegmenting words into sub-word units and performing translation at the\nsub-lexical level. However, statistical word segmentation methods have recently\nshown to be prone to morphological errors, which can lead to inaccurate\ntranslations. In this paper, we propose to overcome this problem by replacing\nthe source-language embedding layer of NMT with a bi-directional recurrent\nneural network that generates compositional representations of the input at any\ndesired level of granularity. We test our approach in a low-resource setting\nwith five languages from different morphological typologies, and under\ndifferent composition assumptions. By training NMT to compose word\nrepresentations from character n-grams, our approach consistently outperforms\n(from 1.71 to 2.48 BLEU points) NMT learning embeddings of statistically\ngenerated sub-word units.", "published": "2018-05-05 10:27:32", "link": "http://arxiv.org/abs/1805.02036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Hyper-Parameter Optimization for Neural Machine Translation on\n  GPU Architectures", "abstract": "Neural machine translation (NMT) has been accelerated by deep learning neural\nnetworks over statistical-based approaches, due to the plethora and\nprogrammability of commodity heterogeneous computing architectures such as\nFPGAs and GPUs and the massive amount of training corpuses generated from news\noutlets, government agencies and social media. Training a learning classifier\nfor neural networks entails tuning hyper-parameters that would yield the best\nperformance. Unfortunately, the number of parameters for machine translation\ninclude discrete categories as well as continuous options, which makes for a\ncombinatorial explosive problem. This research explores optimizing\nhyper-parameters when training deep learning neural networks for machine\ntranslation. Specifically, our work investigates training a language model with\nMarian NMT. Results compare NMT under various hyper-parameter settings across a\nvariety of modern GPU architecture generations in single node and multi-node\nsettings, revealing insights on which hyper-parameters matter most in terms of\nperformance, such as words processed per second, convergence rates, and\ntranslation accuracy, and provides insights on how to best achieve\nhigh-performing NMT systems.", "published": "2018-05-05 18:13:41", "link": "http://arxiv.org/abs/1805.02094v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Patient Representations from Text", "abstract": "Mining electronic health records for patients who satisfy a set of predefined\ncriteria is known in medical informatics as phenotyping. Phenotyping has\nnumerous applications such as outcome prediction, clinical trial recruitment,\nand retrospective studies. Supervised machine learning for phenotyping\ntypically relies on sparse patient representations such as bag-of-words. We\nconsider an alternative that involves learning patient representations. We\ndevelop a neural network model for learning patient representations and show\nthat the learned representations are general enough to obtain state-of-the-art\nperformance on a standard comorbidity detection task.", "published": "2018-05-05 18:20:48", "link": "http://arxiv.org/abs/1805.02096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning of Artist Group Factors to Musical Genre\n  Classification", "abstract": "The automated recognition of music genres from audio information is a\nchallenging problem, as genre labels are subjective and noisy. Artist labels\nare less subjective and less noisy, while certain artists may relate more\nstrongly to certain genres. At the same time, at prediction time, it is not\nguaranteed that artist labels are available for a given audio segment.\nTherefore, in this work, we propose to apply the transfer learning framework,\nlearning artist-related information which will be used at inference time for\ngenre classification. We consider different types of artist-related\ninformation, expressed through artist group factors, which will allow for more\nefficient learning and stronger robustness to potential label noise.\nFurthermore, we investigate how to achieve the highest validation accuracy on\nthe given FMA dataset, by experimenting with various kinds of transfer methods,\nincluding single-task transfer, multi-task transfer and finally multi-task\nlearning.", "published": "2018-05-05 11:17:49", "link": "http://arxiv.org/abs/1805.02043v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
