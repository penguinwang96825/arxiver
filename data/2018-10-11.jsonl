{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).", "published": "2018-10-11 00:50:01", "link": "http://arxiv.org/abs/1810.04805v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eyes are the Windows to the Soul: Predicting the Rating of Text Quality\n  Using Gaze Behaviour", "abstract": "Predicting a reader's rating of text quality is a challenging task that\ninvolves estimating different subjective aspects of the text, like structure,\nclarity, etc. Such subjective aspects are better handled using cognitive\ninformation. One such source of cognitive information is gaze behaviour. In\nthis paper, we show that gaze behaviour does indeed help in effectively\npredicting the rating of text quality. To do this, we first model text quality\nas a function of three properties - organization, coherence and cohesion. Then,\nwe demonstrate how capturing gaze behaviour helps in predicting each of these\nproperties, and hence the overall quality, by reporting improvements obtained\nby adding gaze features to traditional textual features for score prediction.\nWe also hypothesize that if a reader has fully understood the text, the\ncorresponding gaze behaviour would give a better indication of the assigned\nrating, as opposed to partial understanding. Our experiments validate this\nhypothesis by showing greater agreement between the given rating and the\npredicted rating when the reader has a full understanding of the text.", "published": "2018-10-11 04:34:31", "link": "http://arxiv.org/abs/1810.04839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Models for Data-to-Text Natural Language\n  Generation: Word- vs. Character-based Processing and Output Diversity", "abstract": "We present a comparison of word-based and character-based\nsequence-to-sequence models for data-to-text natural language generation, which\ngenerate natural language descriptions for structured inputs. On the datasets\nof two recent generation challenges, our models achieve comparable or better\nautomatic evaluation results than the best challenge submissions. Subsequent\ndetailed statistical and human analyses shed light on the differences between\nthe two input representations and the diversity of the generated texts. In a\ncontrolled experiment with synthetic training data generated from templates, we\ndemonstrate the ability of neural models to learn novel combinations of the\ntemplates and thereby generalize beyond the linguistic structures they were\ntrained on.", "published": "2018-10-11 06:43:28", "link": "http://arxiv.org/abs/1810.04864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Linear Word Analogies", "abstract": "A surprising property of word vectors is that word analogies can often be\nsolved with vector arithmetic. However, it is unclear why arithmetic operators\ncorrespond to non-linear embedding models such as skip-gram with negative\nsampling (SGNS). We provide a formal explanation of this phenomenon without\nmaking the strong assumptions that past theories have made about the vector\nspace and word distribution. Our theory has several implications. Past work has\nconjectured that linear substructures exist in vector spaces because relations\ncan be represented as ratios; we prove that this holds for SGNS. We provide\nnovel justification for the addition of SGNS word vectors by showing that it\nautomatically down-weights the more frequent word, as weighting schemes do ad\nhoc. Lastly, we offer an information theoretic interpretation of Euclidean\ndistance in vector spaces, justifying its use in capturing word dissimilarity.", "published": "2018-10-11 08:08:40", "link": "http://arxiv.org/abs/1810.04882v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Structural Evaluation for Text Simplification", "abstract": "Current measures for evaluating text simplification systems focus on\nevaluating lexical text aspects, neglecting its structural aspects. In this\npaper we propose the first measure to address structural aspects of text\nsimplification, called SAMSA. It leverages recent advances in semantic parsing\nto assess simplification quality by decomposing the input based on its semantic\nstructure and comparing it to the output. SAMSA provides a reference-less\nautomatic evaluation procedure, avoiding the problems that reference-based\nmethods face due to the vast space of valid simplifications for a given\nsentence. Our human evaluation experiments show both SAMSA's substantial\ncorrelation with human judgments, as well as the deficiency of existing\nreference-based measures in evaluating structural simplification.", "published": "2018-10-11 13:54:50", "link": "http://arxiv.org/abs/1810.05022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Text Simplification Using Semantic and Neural\n  Methods", "abstract": "Sentence splitting is a major simplification operator. Here we present a\nsimple and efficient splitting algorithm based on an automatic semantic parser.\nAfter splitting, the text is amenable for further fine-tuned simplification\noperations. In particular, we show that neural Machine Translation can be\neffectively used in this situation. Previous application of Machine Translation\nfor simplification suffers from a considerable disadvantage in that they are\nover-conservative, often failing to modify the source in any way. Splitting\nbased on semantic parsing, as proposed here, alleviates this issue. Extensive\nautomatic and human evaluation shows that the proposed method compares\nfavorably to the state-of-the-art in combined lexical and structural\nsimplification.", "published": "2018-10-11 16:14:24", "link": "http://arxiv.org/abs/1810.05104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns", "abstract": "Coreference resolution is an important task for natural language\nunderstanding, and the resolution of ambiguous pronouns a longstanding\nchallenge. Nonetheless, existing corpora do not capture ambiguous pronouns in\nsufficient volume or diversity to accurately indicate the practical utility of\nmodels. Furthermore, we find gender bias in existing corpora and systems\nfavoring masculine entities. To address this, we present and release GAP, a\ngender-balanced labeled corpus of 8,908 ambiguous pronoun-name pairs sampled to\nprovide diverse coverage of challenges posed by real-world text. We explore a\nrange of baselines which demonstrate the complexity of the challenge, the best\nachieving just 66.9% F1. We show that syntactic structure and continuous neural\nmodels provide promising, complementary cues for approaching the challenge.", "published": "2018-10-11 18:52:42", "link": "http://arxiv.org/abs/1810.05201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SyntaxSQLNet: Syntax Tree Networks for Complex and\n  Cross-DomainText-to-SQL Task", "abstract": "Most existing studies in text-to-SQL tasks do not require generating complex\nSQL queries with multiple clauses or sub-queries, and generalizing to new,\nunseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network\nto address the complex and cross-domain text-to-SQL generation task.\nSyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL\ngeneration path history and table-aware column attention encoders. We evaluate\nSyntaxSQLNet on the Spider text-to-SQL task, which contains databases with\nmultiple tables and complex SQL queries with multiple SQL clauses and nested\nqueries. We use a database split setting where databases in the test set are\nunseen during training. Experimental results show that SyntaxSQLNet can handle\na significantly greater number of complex SQL examples than prior work,\noutperforming the previous state-of-the-art model by 7.3% in exact matching\naccuracy. We also show that SyntaxSQLNet can further improve the performance by\nan additional 7.5% using a cross-domain augmentation method, resulting in a\n14.8% improvement in total. To our knowledge, we are the first to study this\ncomplex and cross-domain text-to-SQL task.", "published": "2018-10-11 20:24:13", "link": "http://arxiv.org/abs/1810.05237v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One Size Does Not Fit All: Generating and Evaluating Variable Number of\n  Keyphrases", "abstract": "Different texts shall by nature correspond to different number of keyphrases.\nThis desideratum is largely missing from existing neural keyphrase generation\nmodels. In this study, we address this problem from both modeling and\nevaluation perspectives.\n  We first propose a recurrent generative model that generates multiple\nkeyphrases as delimiter-separated sequences. Generation diversity is further\nenhanced with two novel techniques by manipulating decoder hidden states. In\ncontrast to previous approaches, our model is capable of generating diverse\nkeyphrases and controlling number of outputs.\n  We further propose two evaluation metrics tailored towards the\nvariable-number generation. We also introduce a new dataset StackEx that\nexpands beyond the only existing genre (i.e., academic writing) in keyphrase\ngeneration tasks. With both previous and new evaluation metrics, our model\noutperforms strong baselines on all datasets.", "published": "2018-10-11 20:40:15", "link": "http://arxiv.org/abs/1810.05241v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Relation Extraction Within and Across Sentence Boundaries", "abstract": "Past work in relation extraction mostly focuses on binary relation between\nentity pairs within single sentence. Recently, the NLP community has gained\ninterest in relation extraction in entity pairs spanning multiple sentences. In\nthis paper, we propose a novel architecture for this task: inter-sentential\ndependency-based neural networks (iDepNN). iDepNN models the shortest and\naugmented dependency paths via recurrent and recursive neural networks to\nextract relationships within (intra-) and across (inter-) sentence boundaries.\nCompared to SVM and neural network baselines, iDepNN is more robust to false\npositives in relationships spanning sentences.\n  We evaluate our models on four datasets from newswire (MUC6) and medical\n(BioNLP shared task) domains that achieve state-of-the-art performance and show\na better balance in precision and recall for inter-sentential relationships. We\nperform better than 11 teams participating in the BioNLP shared task 2016 and\nachieve a gain of 5.2% (0.587 vs 0.558) in F1 over the winning team. We also\nrelease the crosssentence annotations for MUC6.", "published": "2018-10-11 16:07:20", "link": "http://arxiv.org/abs/1810.05102v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Text Generation Without Reinforcement Learning", "abstract": "Generative Adversarial Networks (GANs) have experienced a recent surge in\npopularity, performing competitively in a variety of tasks, especially in\ncomputer vision. However, GAN training has shown limited success in natural\nlanguage processing. This is largely because sequences of text are discrete,\nand thus gradients cannot propagate from the discriminator to the generator.\nRecent solutions use reinforcement learning to propagate approximate gradients\nto the generator, but this is inefficient to train. We propose to utilize an\nautoencoder to learn a low-dimensional representation of sentences. A GAN is\nthen trained to generate its own vectors in this space, which decode to\nrealistic utterances. We report both random and interpolated samples from the\ngenerator. Visualization of sentence vectors indicate our model correctly\nlearns the latent space of the autoencoder. Both human ratings and BLEU scores\nshow that our model generates realistic text against competitive baselines.", "published": "2018-10-11 22:50:38", "link": "http://arxiv.org/abs/1810.06640v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "VoiceFilter: Targeted Voice Separation by Speaker-Conditioned\n  Spectrogram Masking", "abstract": "In this paper, we present a novel system that separates the voice of a target\nspeaker from multi-speaker signals, by making use of a reference signal from\nthe target speaker. We achieve this by training two separate neural networks:\n(1) A speaker recognition network that produces speaker-discriminative\nembeddings; (2) A spectrogram masking network that takes both noisy spectrogram\nand speaker embedding as input, and produces a mask. Our system significantly\nreduces the speech recognition WER on multi-speaker signals, with minimal WER\ndegradation on single-speaker signals.", "published": "2018-10-11 02:57:14", "link": "http://arxiv.org/abs/1810.04826v6", "categories": ["eess.AS", "cs.LG", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Novel Chaotic Uniform Quantizer for Speech Coding", "abstract": "Quantization is an essential step in the analog-to-digital conversion process\nand it is very important in all modern telecommunication systems. In this\npaper, a novel chaotic uniform quantizer is proposed and its application for\nspeech coding is presented. The proposed system consists of three stages: two\nPCM coders separated by an XOR operation with a chaotic sequence, where the\nfirst step is used for continuous signal sampling and second stage performs\ndata encryption, while the third stage provides additional data compression.\nThe performance of the presented quantizer for Laplacian distributed signals\nand real speech signals is investigated and compared with that of the\nwell-known uniform and non-uniform quantizers. Simulation results show that the\nproposed quantizer provides secured data with higher levels of SQNR compared to\nothers.", "published": "2018-10-11 21:34:24", "link": "http://arxiv.org/abs/1810.05260v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Piano Genie", "abstract": "We present Piano Genie, an intelligent controller which allows non-musicians\nto improvise on the piano. With Piano Genie, a user performs on a simple\ninterface with eight buttons, and their performance is decoded into the space\nof plausible piano music in real time. To learn a suitable mapping procedure\nfor this problem, we train recurrent neural network autoencoders with discrete\nbottlenecks: an encoder learns an appropriate sequence of buttons corresponding\nto a piano piece, and a decoder learns to map this sequence back to the\noriginal piece. During performance, we substitute a user's input for the\nencoder output, and play the decoder's prediction each time the user presses a\nbutton. To improve the intuitiveness of Piano Genie's performance behavior, we\nimpose musically meaningful constraints over the encoder's outputs.", "published": "2018-10-11 21:00:44", "link": "http://arxiv.org/abs/1810.05246v2", "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
