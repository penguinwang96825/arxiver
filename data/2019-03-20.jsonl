{"title": "Left-to-Right Dependency Parsing with Pointer Networks", "abstract": "We propose a novel transition-based algorithm that straightforwardly parses\nsentences from left to right by building $n$ attachments, with $n$ being the\nlength of the input sentence. Similarly to the recent stack-pointer parser by\nMa et al. (2018), we use the pointer network framework that, given a word, can\ndirectly point to a position from the sentence. However, our left-to-right\napproach is simpler than the original top-down stack-pointer parser (not\nrequiring a stack) and reduces transition sequence length in half, from 2$n$-1\nactions to $n$. This results in a quadratic non-projective parser that runs\ntwice as fast as the original while achieving the best accuracy to date on the\nEnglish PTB dataset (96.04% UAS, 94.43% LAS) among fully-supervised\nsingle-model dependency parsers, and improves over the former top-down\ntransition system in the majority of languages tested.", "published": "2019-03-20 11:19:58", "link": "http://arxiv.org/abs/1903.08445v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Decay-Function-Free Time-Aware Attention to Context and Speaker\n  Indicator for Spoken Language Understanding", "abstract": "To capture salient contextual information for spoken language understanding\n(SLU) of a dialogue, we propose time-aware models that automatically learn the\nlatent time-decay function of the history without a manual time-decay function.\nWe also propose a method to identify and label the current speaker to improve\nthe SLU accuracy. In experiments on the benchmark dataset used in Dialog State\nTracking Challenge 4, the proposed models achieved significantly higher F1\nscores than the state-of-the-art contextual models. Finally, we analyze the\neffectiveness of the introduced models in detail. The analysis demonstrates\nthat the proposed methods were effective to improve SLU accuracy individually.", "published": "2019-03-20 11:28:06", "link": "http://arxiv.org/abs/1903.08450v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing the Need for Visual Context in Multimodal Machine Translation", "abstract": "Current work on multimodal machine translation (MMT) has suggested that the\nvisual modality is either unnecessary or only marginally beneficial. We posit\nthat this is a consequence of the very simple, short and repetitive sentences\nused in the only available dataset for the task (Multi30K), rendering the\nsource text sufficient as context. In the general case, however, we believe\nthat it is possible to combine visual and textual information in order to\nground translations. In this paper we probe the contribution of the visual\nmodality to state-of-the-art MMT models by conducting a systematic analysis\nwhere we partially deprive the models from source-side textual context. Our\nresults show that under limited textual context, models are capable of\nleveraging the visual input to generate better translations. This contradicts\nthe current belief that MMT models disregard the visual modality because of\neither the quality of the image features or the way they are integrated into\nthe model.", "published": "2019-03-20 18:11:59", "link": "http://arxiv.org/abs/1903.08678v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Compositionality Detection with External Knowledge Bases\n  andWord Embeddings", "abstract": "When the meaning of a phrase cannot be inferred from the individual meanings\nof its words (e.g., hot dog), that phrase is said to be non-compositional.\nAutomatic compositionality detection in multi-word phrases is critical in any\napplication of semantic processing, such as search engines; failing to detect\nnon-compositional phrases can hurt system effectiveness notably. Existing\nresearch treats phrases as either compositional or non-compositional in a\ndeterministic manner. In this paper, we operationalize the viewpoint that\ncompositionality is contextual rather than deterministic, i.e., that whether a\nphrase is compositional or non-compositional depends on its context. For\nexample, the phrase `green card' is compositional when referring to a green\ncolored card, whereas it is non-compositional when meaning permanent residence\nauthorization. We address the challenge of detecting this type of contextual\ncompositionality as follows: given a multi-word phrase, we enrich the word\nembedding representing its semantics with evidence about its global context\n(terms it often collocates with) as well as its local context (narratives where\nthat phrase is used, which we call usage scenarios). We further extend this\nrepresentation with information extracted from external knowledge bases. The\nresulting representation incorporates both localized context and more general\nusage of the phrase and allows to detect its compositionality in a\nnon-deterministic and contextual way. Empirical evaluation of our model on a\ndataset of phrase compositionality, manually collected by crowdsourcing\ncontextual compositionality assessments, shows that our model outperforms\nstate-of-the-art baselines notably on detecting phrase compositionality.", "published": "2019-03-20 08:53:05", "link": "http://arxiv.org/abs/1903.08389v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prospection: Interpretable Plans From Language By Predicting the Future", "abstract": "High-level human instructions often correspond to behaviors with multiple\nimplicit steps. In order for robots to be useful in the real world, they must\nbe able to to reason over both motions and intermediate goals implied by human\ninstructions. In this work, we propose a framework for learning representations\nthat convert from a natural-language command to a sequence of intermediate\ngoals for execution on a robot. A key feature of this framework is prospection,\ntraining an agent not just to correctly execute the prescribed command, but to\npredict a horizon of consequences of an action before taking it. We demonstrate\nthe fidelity of plans generated by our framework when interpreting real,\ncrowd-sourced natural language commands for a robot in simulated scenes.", "published": "2019-03-20 01:52:37", "link": "http://arxiv.org/abs/1903.08309v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Neural Check-Worthiness Ranking with Weak Supervision: Finding Sentences\n  for Fact-Checking", "abstract": "Automatic fact-checking systems detect misinformation, such as fake news, by\n(i) selecting check-worthy sentences for fact-checking, (ii) gathering related\ninformation to the sentences, and (iii) inferring the factuality of the\nsentences. Most prior research on (i) uses hand-crafted features to select\ncheck-worthy sentences, and does not explicitly account for the recent finding\nthat the top weighted terms in both check-worthy and non-check-worthy sentences\nare actually overlapping [15]. Motivated by this, we present a neural\ncheck-worthiness sentence ranking model that represents each word in a sentence\nby \\textit{both} its embedding (aiming to capture its semantics) and its\nsyntactic dependencies (aiming to capture its role in modifying the semantics\nof other terms in the sentence). Our model is an end-to-end trainable neural\nnetwork for check-worthiness ranking, which is trained on large amounts of\nunlabelled data through weak supervision. Thorough experimental evaluation\nagainst state of the art baselines, with and without weak supervision, shows\nour model to be superior at all times (+13% in MAP and +28% at various\nPrecision cut-offs from the best baseline with statistical significance).\nEmpirical analysis of the use of weak supervision, word embedding pretraining\non domain-specific data, and the use of syntactic dependencies of our model\nreveals that check-worthy sentences contain notably more identical syntactic\ndependencies than non-check-worthy sentences.", "published": "2019-03-20 09:40:19", "link": "http://arxiv.org/abs/1903.08404v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Neural Speed Reading with Structural-Jump-LSTM", "abstract": "Recurrent neural networks (RNNs) can model natural language by sequentially\n'reading' input tokens and outputting a distributed representation of each\ntoken. Due to the sequential nature of RNNs, inference time is linearly\ndependent on the input length, and all inputs are read regardless of their\nimportance. Efforts to speed up this inference, known as 'neural speed\nreading', either ignore or skim over part of the input. We present\nStructural-Jump-LSTM: the first neural speed reading model to both skip and\njump text during inference. The model consists of a standard LSTM and two\nagents: one capable of skipping single words when reading, and one capable of\nexploiting punctuation structure (sub-sentence separators (,:), sentence end\nsymbols (.!?), or end of text markers) to jump ahead after reading a word. A\ncomprehensive experimental evaluation of our model against all five\nstate-of-the-art neural reading models shows that Structural-Jump-LSTM achieves\nthe best overall floating point operations (FLOP) reduction (hence is faster),\nwhile keeping the same accuracy or even improving it compared to a vanilla LSTM\nthat reads the whole text.", "published": "2019-03-20 12:01:46", "link": "http://arxiv.org/abs/1904.00761v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Affect in Tweets Using Experts Model", "abstract": "Estimating the intensity of emotion has gained significance as modern textual\ninputs in potential applications like social media, e-retail markets,\npsychology, advertisements etc., carry a lot of emotions, feelings, expressions\nalong with its meaning. However, the approaches of traditional sentiment\nanalysis primarily focuses on classifying the sentiment in general (positive or\nnegative) or at an aspect level(very positive, low negative, etc.) and cannot\nexploit the intensity information. Moreover, automatically identifying emotions\nlike anger, fear, joy, sadness, disgust etc., from text introduces challenging\nscenarios where single tweet may contain multiple emotions with different\nintensities and some emotions may even co-occur in some of the tweets. In this\npaper, we propose an architecture, Experts Model, inspired from the standard\nMixture of Experts (MoE) model. The key idea here is each expert learns\ndifferent sets of features from the feature vector which helps in better\nemotion detection from the tweet. We compared the results of our Experts Model\nwith both baseline results and top five performers of SemEval-2018 Task-1,\nAffect in Tweets (AIT). The experimental results show that our proposed\napproach deals with the emotion detection problem and stands at top-5 results.", "published": "2019-03-20 11:10:29", "link": "http://arxiv.org/abs/1904.00762v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Distributed Vector Representations of Folksong Motifs", "abstract": "This article presents a distributed vector representation model for learning\nfolksong motifs. A skip-gram version of word2vec with negative sampling is used\nto represent high quality embeddings. Motifs from the Essen Folksong collection\nare compared based on their cosine similarity. A new evaluation method for\ntesting the quality of the embeddings based on a melodic similarity task is\npresented to show how the vector space can represent complex contextual\nfeatures, and how it can be utilized for the study of folksong variation.", "published": "2019-03-20 21:52:13", "link": "http://arxiv.org/abs/1903.08756v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.SD", "eess.AS", "H.5.5"], "primary_category": "cs.IR"}
{"title": "Smart Edition of MIDI Files", "abstract": "We address the issue of editing musical performance data, in particular MIDI\nfiles representing human musical performances. Editing such sequences raises\nspecific issues due to the ambiguous nature of musical objects. The first\nsource of ambiguity is that musicians naturally produce many deviations from\nthe metrical frame. These deviations may be intentional or subconscious, but\nthey play an important role in conveying the groove or feeling of a\nperformance. Relations between musical elements are also usually implicit,\ncreating even more ambiguity. A note is in relation with the surrounding notes\nin many possible ways: it can be part of a melodic pattern, it can also play a\nharmonic role with the simultaneous notes, or be a pedal-tone. All these\naspects play an essential role that should be preserved, as much as possible,\nwhen editing musical sequences.\n  In this paper, we contribute specifically to the problem of editing\nnon-quantized, metrical musical sequences represented as MIDI files. We first\nlist of number of problems caused by the use of naive edition operations\napplied to performance data, using a motivating example. We then introduce a\nmodel, called Dancing MIDI, based on 1) two desirable, well-defined properties\nfor edit operations and 2) two well-defined operations, Split and Concat, with\nan implementation. We show that our model formally satisfies the two\nproperties, and that it prevents most of the problems that occur with naive\nedit operations on our motivating example, as well as on a real-world example\nusing an automatic harmonizer.", "published": "2019-03-20 11:51:12", "link": "http://arxiv.org/abs/1903.08459v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
