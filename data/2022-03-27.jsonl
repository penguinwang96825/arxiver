{"title": "Text Adversarial Purification as Defense against Adversarial Attacks", "abstract": "Adversarial purification is a successful defense mechanism against\nadversarial attacks without requiring knowledge of the form of the incoming\nattack. Generally, adversarial purification aims to remove the adversarial\nperturbations therefore can make correct predictions based on the recovered\nclean samples. Despite the success of adversarial purification in the computer\nvision field that incorporates generative models such as energy-based models\nand diffusion models, using purification as a defense strategy against textual\nadversarial attacks is rarely explored. In this work, we introduce a novel\nadversarial purification method that focuses on defending against textual\nadversarial attacks. With the help of language models, we can inject noise by\nmasking input texts and reconstructing the masked texts based on the masked\nlanguage models. In this way, we construct an adversarial purification process\nfor textual models against the most widely used word-substitution adversarial\nattacks. We test our proposed adversarial purification method on several strong\nadversarial attack methods including Textfooler and BERT-Attack and\nexperimental results indicate that the purification algorithm can successfully\ndefend against strong word-substitution attacks.", "published": "2022-03-27 04:41:55", "link": "http://arxiv.org/abs/2203.14207v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BARCOR: Towards A Unified Framework for Conversational Recommendation\n  Systems", "abstract": "Recommendation systems focus on helping users find items of interest in the\nsituations of information overload, where users' preferences are typically\nestimated by the past observed behaviors. In contrast, conversational\nrecommendation systems (CRS) aim to understand users' preferences via\ninteractions in conversation flows. CRS is a complex problem that consists of\ntwo main tasks: (1) recommendation and (2) response generation. Previous work\noften tried to solve the problem in a modular manner, where recommenders and\nresponse generators are separate neural models. Such modular architectures\noften come with a complicated and unintuitive connection between the modules,\nleading to inefficient learning and other issues. In this work, we propose a\nunified framework based on BART for conversational recommendation, which\ntackles two tasks in a single model. Furthermore, we also design and collect a\nlightweight knowledge graph for CRS in the movie domain. The experimental\nresults show that the proposed methods achieve the state-of-the-art performance\nin terms of both automatic and human evaluation.", "published": "2022-03-27 09:42:16", "link": "http://arxiv.org/abs/2203.14257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Aspect-Based Sentiment Classification", "abstract": "With the constantly growing number of reviews and other sentiment-bearing\ntexts on the Web, the demand for automatic sentiment analysis algorithms\ncontinues to expand. Aspect-based sentiment classification (ABSC) allows for\nthe automatic extraction of highly fine-grained sentiment information from text\ndocuments or sentences. In this survey, the rapidly evolving state of the\nresearch on ABSC is reviewed. A novel taxonomy is proposed that categorizes the\nABSC models into three major categories: knowledge-based, machine learning, and\nhybrid models. This taxonomy is accompanied with summarizing overviews of the\nreported model performances, and both technical and intuitive explanations of\nthe various ABSC models. State-of-the-art ABSC models are discussed, such as\nmodels based on the transformer model, and hybrid deep learning models that\nincorporate knowledge bases. Additionally, various techniques for representing\nthe model inputs and evaluating the model outputs are reviewed. Furthermore,\ntrends in the research on ABSC are identified and a discussion is provided on\nthe ways in which the field of ABSC can be advanced in the future.", "published": "2022-03-27 10:15:00", "link": "http://arxiv.org/abs/2203.14266v1", "categories": ["cs.CL", "A.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Pyramid-BERT: Reducing Complexity via Successive Core-set based Token\n  Selection", "abstract": "Transformer-based language models such as BERT have achieved the\nstate-of-the-art performance on various NLP tasks, but are computationally\nprohibitive. A recent line of works use various heuristics to successively\nshorten sequence length while transforming tokens through encoders, in tasks\nsuch as classification and ranking that require a single token embedding for\nprediction. We present a novel solution to this problem, called Pyramid-BERT\nwhere we replace previously used heuristics with a {\\em core-set} based token\nselection method justified by theoretical results. The core-set based token\nselection technique allows us to avoid expensive pre-training, gives a\nspace-efficient fine tuning, and thus makes it suitable to handle longer\nsequence lengths. We provide extensive experiments establishing advantages of\npyramid BERT over several baselines and existing works on the GLUE benchmarks\nand Long Range Arena datasets.", "published": "2022-03-27 19:52:01", "link": "http://arxiv.org/abs/2203.14380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Design and Development of Rule-based open-domain Question-Answering\n  System on SQuAD v2.0 Dataset", "abstract": "Human mind is the palace of curious questions that seek answers.\nComputational resolution of this challenge is possible through Natural Language\nProcessing techniques. Statistical techniques like machine learning and deep\nlearning require a lot of data to train and despite that they fail to tap into\nthe nuances of language. Such systems usually perform best on close-domain\ndatasets. We have proposed development of a rule-based open-domain\nquestion-answering system which is capable of answering questions of any domain\nfrom a corresponding context passage. We have used 1000 questions from SQuAD\n2.0 dataset for testing the developed system and it gives satisfactory results.\nIn this paper, we have described the structure of the developed system and have\nanalyzed the performance.", "published": "2022-03-27 07:51:18", "link": "http://arxiv.org/abs/2204.09659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Educational Question Generation of Children Storybooks via Question Type\n  Distribution Learning and Event-Centric Summarization", "abstract": "Generating educational questions of fairytales or storybooks is vital for\nimproving children's literacy ability. However, it is challenging to generate\nquestions that capture the interesting aspects of a fairytale story with\neducational meaningfulness. In this paper, we propose a novel question\ngeneration method that first learns the question type distribution of an input\nstory paragraph, and then summarizes salient events which can be used to\ngenerate high-cognitive-demand questions. To train the event-centric\nsummarizer, we finetune a pre-trained transformer-based sequence-to-sequence\nmodel using silver samples composed by educational question-answer pairs. On a\nnewly proposed educational question answering dataset FairytaleQA, we show good\nperformance of our method on both automatic and human evaluation metrics. Our\nwork indicates the necessity of decomposing question type distribution learning\nand event-centric summary generation for educational question generation.", "published": "2022-03-27 02:21:19", "link": "http://arxiv.org/abs/2203.14187v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene\n  Graphs with Language Structures via Dependency Relationships", "abstract": "Understanding realistic visual scene images together with language\ndescriptions is a fundamental task towards generic visual understanding.\nPrevious works have shown compelling comprehensive results by building\nhierarchical structures for visual scenes (e.g., scene graphs) and natural\nlanguages (e.g., dependency trees), individually. However, how to construct a\njoint vision-language (VL) structure has barely been investigated. More\nchallenging but worthwhile, we introduce a new task that targets on inducing\nsuch a joint VL structure in an unsupervised manner. Our goal is to bridge the\nvisual scene graphs and linguistic dependency trees seamlessly. Due to the lack\nof VL structural data, we start by building a new dataset VLParse. Rather than\nusing labor-intensive labeling from scratch, we propose an automatic alignment\nprocedure to produce coarse structures followed by human refinement to produce\nhigh-quality ones. Moreover, we benchmark our dataset by proposing a\ncontrastive learning (CL)-based framework VLGAE, short for Vision-Language\nGraph Autoencoder. Our model obtains superior performance on two derived tasks,\ni.e., language grammar induction and VL phrase grounding. Ablations show the\neffectiveness of both visual cues and dependency relationships on fine-grained\nVL structure construction.", "published": "2022-03-27 09:51:34", "link": "http://arxiv.org/abs/2203.14260v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "bitsa_nlp@LT-EDI-ACL2022: Leveraging Pretrained Language Models for\n  Detecting Homophobia and Transphobia in Social Media Comments", "abstract": "Online social networks are ubiquitous and user-friendly. Nevertheless, it is\nvital to detect and moderate offensive content to maintain decency and empathy.\nHowever, mining social media texts is a complex task since users don't adhere\nto any fixed patterns. Comments can be written in any combination of languages\nand many of them may be low-resource.\n  In this paper, we present our system for the LT-EDI shared task on detecting\nhomophobia and transphobia in social media comments. We experiment with a\nnumber of monolingual and multilingual transformer based models such as mBERT\nalong with a data augmentation technique for tackling class imbalance. Such\npretrained large models have recently shown tremendous success on a variety of\nbenchmark tasks in natural language processing. We observe their performance on\na carefully annotated, real life dataset of YouTube comments in English as well\nas Tamil.\n  Our submission achieved ranks 9, 6 and 3 with a macro-averaged F1-score of\n0.42, 0.64 and 0.58 in the English, Tamil and Tamil-English subtasks\nrespectively. The code for the system has been open sourced.", "published": "2022-03-27 10:15:34", "link": "http://arxiv.org/abs/2203.14267v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diagonal State Spaces are as Effective as Structured State Spaces", "abstract": "Modeling long range dependencies in sequential data is a fundamental step\ntowards attaining human-level performance in many modalities such as text,\nvision, audio and video. While attention-based models are a popular and\neffective choice in modeling short-range interactions, their performance on\ntasks requiring long range reasoning has been largely inadequate. In an\nexciting result, Gu et al. (ICLR 2022) proposed the $\\textit{Structured State\nSpace}$ (S4) architecture delivering large gains over state-of-the-art models\non several long-range tasks across various modalities. The core proposition of\nS4 is the parameterization of state matrices via a diagonal plus low rank\nstructure, allowing efficient computation. In this work, we show that one can\nmatch the performance of S4 even without the low rank correction and thus\nassuming the state matrices to be diagonal. Our $\\textit{Diagonal State Space}$\n(DSS) model matches the performance of S4 on Long Range Arena tasks, speech\nclassification on Speech Commands dataset, while being conceptually simpler and\nstraightforward to implement.", "published": "2022-03-27 16:30:33", "link": "http://arxiv.org/abs/2203.14343v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Listen, Adapt, Better WER: Source-free Single-utterance Test-time\n  Adaptation for Automatic Speech Recognition", "abstract": "Although deep learning-based end-to-end Automatic Speech Recognition (ASR)\nhas shown remarkable performance in recent years, it suffers severe performance\nregression on test samples drawn from different data distributions. Test-time\nAdaptation (TTA), previously explored in the computer vision area, aims to\nadapt the model trained on source domains to yield better predictions for test\nsamples, often out-of-domain, without accessing the source data. Here, we\npropose the Single-Utterance Test-time Adaptation (SUTA) framework for ASR,\nwhich is the first TTA study on ASR to our best knowledge. The single-utterance\nTTA is a more realistic setting that does not assume test data are sampled from\nidentical distribution and does not delay on-demand inference due to\npre-collection for the batch of adaptation data. SUTA consists of unsupervised\nobjectives with an efficient adaptation strategy. Empirical results demonstrate\nthat SUTA effectively improves the performance of the source ASR model\nevaluated on multiple out-of-domain target corpora and in-domain test samples.", "published": "2022-03-27 06:38:39", "link": "http://arxiv.org/abs/2203.14222v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Example-based Hypernetworks for Out-of-Distribution Generalization", "abstract": "As Natural Language Processing (NLP) algorithms continually achieve new\nmilestones, out-of-distribution generalization remains a significant challenge.\nThis paper addresses the issue of multi-source adaptation for unfamiliar\ndomains: We leverage labeled data from multiple source domains to generalize to\nunknown target domains at training. Our innovative framework employs\nexample-based Hypernetwork adaptation: a T5 encoder-decoder initially generates\na unique signature from an input example, embedding it within the source\ndomains' semantic space. This signature is subsequently utilized by a\nHypernetwork to generate the task classifier's weights. We evaluated our method\nacross two tasks - sentiment classification and natural language inference - in\n29 adaptation scenarios, where it outpaced established algorithms. In an\nadvanced version, the signature also enriches the input example's\nrepresentation. We also compare our finetuned architecture to few-shot GPT-3,\ndemonstrating its effectiveness in essential use cases. To our knowledge, this\nmarks the first application of Hypernetworks to the adaptation for unknown\ndomains.", "published": "2022-03-27 11:10:10", "link": "http://arxiv.org/abs/2203.14276v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcement Guided Multi-Task Learning Framework for Low-Resource\n  Stereotype Detection", "abstract": "As large Pre-trained Language Models (PLMs) trained on large amounts of data\nin an unsupervised manner become more ubiquitous, identifying various types of\nbias in the text has come into sharp focus. Existing \"Stereotype Detection\"\ndatasets mainly adopt a diagnostic approach toward large PLMs. Blodgett et. al\n(2021a) show that there are significant reliability issues with the existing\nbenchmark datasets. Annotating a reliable dataset requires a precise\nunderstanding of the subtle nuances of how stereotypes manifest in text. In\nthis paper, we annotate a focused evaluation set for \"Stereotype Detection\"\nthat addresses those pitfalls by de-constructing various ways in which\nstereotypes manifest in text. Further, we present a multi-task model that\nleverages the abundance of data-rich neighboring tasks such as hate speech\ndetection, offensive language detection, misogyny detection, etc., to improve\nthe empirical performance on \"Stereotype Detection\". We then propose a\nreinforcement-learning agent that guides the multi-task learning model by\nlearning to identify the training examples from the neighboring tasks that help\nthe target task the most. We show that the proposed models achieve significant\nempirical gains over existing baselines on all the tasks.", "published": "2022-03-27 17:16:11", "link": "http://arxiv.org/abs/2203.14349v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical\n  domain Question Answering", "abstract": "This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question\nAnswering (MCQA) dataset designed to address real-world medical entrance exam\nquestions. More than 194k high-quality AIIMS \\& NEET PG entrance exam MCQs\ncovering 2.4k healthcare topics and 21 medical subjects are collected with an\naverage token length of 12.77 and high topical diversity. Each sample contains\na question, correct answer(s), and other options which requires a deeper\nlanguage understanding as it tests the 10+ reasoning abilities of a model\nacross a wide range of medical subjects \\& topics. A detailed explanation of\nthe solution, along with the above information, is provided in this study.", "published": "2022-03-27 18:59:16", "link": "http://arxiv.org/abs/2203.14371v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Dataset for Speech Emotion Recognition in Greek Theatrical Plays", "abstract": "Machine learning methodologies can be adopted in cultural applications and\npropose new ways to distribute or even present the cultural content to the\npublic. For instance, speech analytics can be adopted to automatically generate\nsubtitles in theatrical plays, in order to (among other purposes) help people\nwith hearing loss. Apart from a typical speech-to-text transcription with\nAutomatic Speech Recognition (ASR), Speech Emotion Recognition (SER) can be\nused to automatically predict the underlying emotional content of speech\ndialogues in theatrical plays, and thus to provide a deeper understanding how\nthe actors utter their lines. However, real-world datasets from theatrical\nplays are not available in the literature. In this work we present GreThE, the\nGreek Theatrical Emotion dataset, a new publicly available data collection for\nspeech emotion recognition in Greek theatrical plays. The dataset contains\nutterances from various actors and plays, along with respective valence and\narousal annotations. Towards this end, multiple annotators have been asked to\nprovide their input for each speech recording and inter-annotator agreement is\ntaken into account in the final ground truth generation. In addition, we\ndiscuss the results of some indicative experiments that have been conducted\nwith machine and deep learning frameworks, using the dataset, along with some\nwidely used databases in the field of speech emotion recognition.", "published": "2022-03-27 21:55:59", "link": "http://arxiv.org/abs/2203.15568v1", "categories": ["cs.SD", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "SMP-PHAT: Lightweight DoA Estimation by Merging Microphone Pairs", "abstract": "This paper introduces SMP-PHAT, which performs direction of arrival (DoA) of\nsound estimation with a microphone array by merging pairs of microphones that\nare parallel in space. This approach reduces the number of pairwise\ncross-correlation computations, and brings down the number of flops and memory\nlookups when searching for DoA. Experiments on low-cost hardware with commonly\nused microphone arrays show that the proposed method provides the same accuracy\nas the former SRP-PHAT approach, while reducing the computational load by 39%\nin some cases.", "published": "2022-03-27 22:44:16", "link": "http://arxiv.org/abs/2203.14409v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Active Speaker Detection", "abstract": "Recent advances in the Active Speaker Detection (ASD) problem build upon a\ntwo-stage process: feature extraction and spatio-temporal context aggregation.\nIn this paper, we propose an end-to-end ASD workflow where feature learning and\ncontextual predictions are jointly learned. Our end-to-end trainable network\nsimultaneously learns multi-modal embeddings and aggregates spatio-temporal\ncontext. This results in more suitable feature representations and improved\nperformance in the ASD task. We also introduce interleaved graph neural network\n(iGNN) blocks, which split the message passing according to the main sources of\ncontext in the ASD problem. Experiments show that the aggregated features from\nthe iGNN blocks are more suitable for ASD, resulting in state-of-the art\nperformance. Finally, we design a weakly-supervised strategy, which\ndemonstrates that the ASD problem can also be approached by utilizing\naudiovisual data but relying exclusively on audio annotations. We achieve this\nby modelling the direct relationship between the audio signal and the possible\nsound sources (speakers), as well as introducing a contrastive loss. All the\nresources of this project will be made available at:\nhttps://github.com/fuankarion/end-to-end-asd.", "published": "2022-03-27 08:55:28", "link": "http://arxiv.org/abs/2203.14250v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Bunched LPCNet2: Efficient Neural Vocoders Covering Devices from Cloud\n  to Edge", "abstract": "Text-to-Speech (TTS) services that run on edge devices have many advantages\ncompared to cloud TTS, e.g., latency and privacy issues. However, neural\nvocoders with a low complexity and small model footprint inevitably generate\nannoying sounds. This study proposes a Bunched LPCNet2, an improved LPCNet\narchitecture that provides highly efficient performance in high-quality for\ncloud servers and in a low-complexity for low-resource edge devices. Single\nlogistic distribution achieves computational efficiency, and insightful tricks\nreduce the model footprint while maintaining speech quality. A DualRate\narchitecture, which generates a lower sampling rate from a prosody model, is\nalso proposed to reduce maintenance costs. The experiments demonstrate that\nBunched LPCNet2 generates satisfactory speech quality with a model footprint of\n1.1MB while operating faster than real-time on a RPi 3B. Our audio samples are\navailable at https://srtts.github.io/bunchedLPCNet2.", "published": "2022-03-27 23:56:52", "link": "http://arxiv.org/abs/2203.14416v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Blind Source Separation for Mixture of Sinusoids with Near-Linear\n  Computational Complexity", "abstract": "We propose a multi-tone decomposition algorithm that can find the\nfrequencies, amplitudes and phases of the fundamental sinusoids in a noisy\nobservation sequence. Under independent identically distributed Gaussian noise,\nour method utilizes a maximum likelihood approach to estimate the relevant tone\nparameters from the contaminated observations. When estimating $M$ number of\nsinusoidal sources, our algorithm successively estimates their frequencies and\njointly optimizes their amplitudes and phases. Our method can also be\nimplemented as a blind source separator in the absence of the information about\n$M$. The computational complexity of our algorithm is near-linear, i.e.,\n$\\tilde{O}(N)$.", "published": "2022-03-27 15:16:07", "link": "http://arxiv.org/abs/2203.14324v1", "categories": ["eess.SP", "cs.LG", "eess.AS", "math.OC", "stat.ML"], "primary_category": "eess.SP"}
