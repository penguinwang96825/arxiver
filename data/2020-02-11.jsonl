{"title": "Training with Streaming Annotation", "abstract": "In this paper, we address a practical scenario where training data is\nreleased in a sequence of small-scale batches and annotation in earlier phases\nhas lower quality than the later counterparts. To tackle the situation, we\nutilize a pre-trained transformer network to preserve and integrate the most\nsalient document information from the earlier batches while focusing on the\nannotation (presumably with higher quality) from the current batch. Using event\nextraction as a case study, we demonstrate in the experiments that our proposed\nframework can perform better than conventional approaches (the improvement\nranges from 3.6 to 14.9% absolute F-score gain), especially when there is more\nnoise in the early annotation; and our approach spares 19.1% time with regard\nto the best conventional method.", "published": "2020-02-11 01:52:05", "link": "http://arxiv.org/abs/2002.04165v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Neural Dialogue Generation", "abstract": "Maximum Mutual information (MMI), which models the bidirectional dependency\nbetween responses ($y$) and contexts ($x$), i.e., the forward probability $\\log\np(y|x)$ and the backward probability $\\log p(x|y)$, has been widely used as the\nobjective in the \\sts model to address the dull-response issue in open-domain\ndialog generation. Unfortunately, under the framework of the \\sts model, direct\ndecoding from $\\log p(y|x) + \\log p(x|y)$ is infeasible since the second part\n(i.e., $p(x|y)$) requires the completion of target generation before it can be\ncomputed, and the search space for $y$ is enormous. Empirically, an N-best list\nis first generated given $p(y|x)$, and $p(x|y)$ is then used to rerank the\nN-best list, which inevitably results in non-globally-optimal solutions. In\nthis paper, we propose to use non-autoregressive (non-AR) generation model to\naddress this non-global optimality issue. Since target tokens are generated\nindependently in non-AR generation, $p(x|y)$ for each target word can be\ncomputed as soon as it's generated, and does not have to wait for the\ncompletion of the whole sequence. This naturally resolves the non-global\noptimal issue in decoding. Experimental results demonstrate that the proposed\nnon-AR strategy produces more diverse, coherent, and appropriate responses,\nyielding substantive gains in BLEU scores and in human evaluations.", "published": "2020-02-11 08:19:28", "link": "http://arxiv.org/abs/2002.04250v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adjusting Image Attributes of Localized Regions with Low-level Dialogue", "abstract": "Natural Language Image Editing (NLIE) aims to use natural language\ninstructions to edit images. Since novices are inexperienced with image editing\ntechniques, their instructions are often ambiguous and contain high-level\nabstractions that tend to correspond to complex editing steps to accomplish.\nMotivated by this inexperience aspect, we aim to smooth the learning curve by\nteaching the novices to edit images using low-level commanding terminologies.\nTowards this end, we develop a task-oriented dialogue system to investigate\nlow-level instructions for NLIE. Our system grounds language on the level of\nedit operations, and suggests options for a user to choose from. Though\ncompelled to express in low-level terms, a user evaluation shows that 25% of\nusers found our system easy-to-use, resonating with our motivation. An analysis\nshows that users generally adapt to utilizing the proposed low-level language\ninterface. In this study, we identify that object segmentation as the key\nfactor to the user satisfaction. Our work demonstrates the advantages of the\nlow-level, direct language-action mapping approach that can be applied to other\nproblem domains beyond image editing such as audio editing or industrial\ndesign.", "published": "2020-02-11 20:59:34", "link": "http://arxiv.org/abs/2002.04678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Rumour Mill: Making the Spread of Misinformation Explicit and\n  Tangible", "abstract": "Misinformation spread presents a technological and social threat to society.\nWith the advance of AI-based language models, automatically generated texts\nhave become difficult to identify and easy to create at scale. We present \"The\nRumour Mill\", a playful art piece, designed as a commentary on the spread of\nrumours and automatically-generated misinformation. The mill is a tabletop\ninteractive machine, which invites a user to experience the process of creating\nbelievable text by interacting with different tangible controls on the mill.\nThe user manipulates visible parameters to adjust the genre and type of an\nautomatically generated text rumour. The Rumour Mill is a physical\ndemonstration of the state of current technology and its ability to generate\nand manipulate natural language text, and of the act of starting and spreading\nrumours.", "published": "2020-02-11 15:49:32", "link": "http://arxiv.org/abs/2002.04494v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Two Huge Title and Keyword Generation Corpora of Research Articles", "abstract": "Recent developments in sequence-to-sequence learning with neural networks\nhave considerably improved the quality of automatically generated text\nsummaries and document keywords, stipulating the need for even bigger training\ncorpora. Metadata of research articles are usually easy to find online and can\nbe used to perform research on various tasks. In this paper, we introduce two\nhuge datasets for text summarization (OAGSX) and keyword generation (OAGKX)\nresearch, containing 34 million and 23 million records, respectively. The data\nwere retrieved from the Open Academic Graph which is a network of research\nprofiles and publications. We carefully processed each record and also tried\nseveral extractive and abstractive methods of both tasks to create performance\nbaselines for other researchers. We further illustrate the performance of those\nmethods previewing their outputs. In the near future, we would like to apply\ntopic modeling on the two sets to derive subsets of research articles from more\nspecific disciplines.", "published": "2020-02-11 21:17:29", "link": "http://arxiv.org/abs/2002.04689v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Performance Comparison of Crowdworkers and NLP Tools on Named-Entity\n  Recognition and Sentiment Analysis of Political Tweets", "abstract": "We report results of a comparison of the accuracy of crowdworkers and seven\nNatural Language Processing (NLP) toolkits in solving two important NLP tasks,\nnamed-entity recognition (NER) and entity-level sentiment (ELS) analysis. We\nhere focus on a challenging dataset, 1,000 political tweets that were collected\nduring the U.S. presidential primary election in February 2016. Each tweet\nrefers to at least one of four presidential candidates, i.e., four named\nentities. The groundtruth, established by experts in political communication,\nhas entity-level sentiment information for each candidate mentioned in the\ntweet. We tested several commercial and open-source tools. Our experiments show\nthat, for our dataset of political tweets, the most accurate NER system, Google\nCloud NL, performed almost on par with crowdworkers, but the most accurate ELS\nanalysis system, TensiStrength, did not match the accuracy of crowdworkers by a\nlarge margin of more than 30 percent points.", "published": "2020-02-11 03:03:20", "link": "http://arxiv.org/abs/2002.04181v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning Coupled Policies for Simultaneous Machine Translation using\n  Imitation Learning", "abstract": "We present a novel approach to efficiently learn a simultaneous translation\nmodel with coupled programmer-interpreter policies. First, wepresent an\nalgorithmic oracle to produce oracle READ/WRITE actions for training bilingual\nsentence-pairs using the notion of word alignments. This oracle actions are\ndesigned to capture enough information from the partial input before writing\nthe output. Next, we perform a coupled scheduled sampling to effectively\nmitigate the exposure bias when learning both policies jointly with imitation\nlearning. Experiments on six language-pairs show our method outperforms strong\nbaselines in terms of translation quality while keeping the translation delay\nlow.", "published": "2020-02-11 10:56:42", "link": "http://arxiv.org/abs/2002.04306v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning", "abstract": "Recent powerful pre-trained language models have achieved remarkable\nperformance on most of the popular datasets for reading comprehension. It is\ntime to introduce more challenging datasets to push the development of this\nfield towards more comprehensive reasoning of text. In this paper, we introduce\na new Reading Comprehension dataset requiring logical reasoning (ReClor)\nextracted from standardized graduate admission examinations. As earlier studies\nsuggest, human-annotated datasets usually contain biases, which are often\nexploited by models to achieve high accuracy without truly understanding the\ntext. In order to comprehensively evaluate the logical reasoning ability of\nmodels on ReClor, we propose to identify biased data points and separate them\ninto EASY set while the rest as HARD set. Empirical results show that\nstate-of-the-art models have an outstanding ability to capture biases contained\nin the dataset with high accuracy on EASY set. However, they struggle on HARD\nset with poor performance near that of random guess, indicating more research\nis needed to essentially enhance the logical reasoning ability of current\nmodels.", "published": "2020-02-11 11:54:29", "link": "http://arxiv.org/abs/2002.04326v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Convolutional Neural Networks and a Transfer Learning Strategy to\n  Classify Parkinson's Disease from Speech in Three Different Languages", "abstract": "Parkinson's disease patients develop different speech impairments that affect\ntheir communication capabilities. The automatic assessment of the speech of the\npatients allows the development of computer aided tools to support the\ndiagnosis and the evaluation of the disease severity. This paper introduces a\nmethodology to classify Parkinson's disease from speech in three different\nlanguages: Spanish, German, and Czech. The proposed approach considers\nconvolutional neural networks trained with time frequency representations and a\ntransfer learning strategy among the three languages. The transfer learning\nscheme aims to improve the accuracy of the models when the weights of the\nneural network are initialized with utterances from a different language than\nthe used for the test set. The results suggest that the proposed strategy\nimproves the accuracy of the models in up to 8\\% when the base model used to\ninitialize the weights of the classifier is robust enough. In addition, the\nresults obtained after the transfer learning are in most cases more balanced in\nterms of specificity-sensitivity than those trained without the transfer\nlearning strategy.", "published": "2020-02-11 13:48:38", "link": "http://arxiv.org/abs/2002.04374v1", "categories": ["cs.LG", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Superbloom: Bloom filter meets Transformer", "abstract": "We extend the idea of word pieces in natural language models to machine\nlearning tasks on opaque ids. This is achieved by applying hash functions to\nmap each id to multiple hash tokens in a much smaller space, similarly to a\nBloom filter. We show that by applying a multi-layer Transformer to these Bloom\nfilter digests, we are able to obtain models with high accuracy. They\noutperform models of a similar size without hashing and, to a large degree,\nmodels of a much larger size trained using sampled softmax with the same\ncomputational budget. Our key observation is that it is important to use a\nmulti-layer Transformer for Bloom filter digests to remove ambiguity in the\nhashed input. We believe this provides an alternative method to solving\nproblems with large vocabulary size.", "published": "2020-02-11 22:52:40", "link": "http://arxiv.org/abs/2002.04723v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Mask & Focus: Conversation Modelling by Learning Concepts", "abstract": "Sequence to sequence models attempt to capture the correlation between all\nthe words in the input and output sequences. While this is quite useful for\nmachine translation where the correlation among the words is indeed quite\nstrong, it becomes problematic for conversation modelling where the correlation\nis often at a much abstract level. In contrast, humans tend to focus on the\nessential concepts discussed in the conversation context and generate responses\naccordingly. In this paper, we attempt to mimic this response generating\nmechanism by learning the essential concepts in the context and response in an\nunsupervised manner. The proposed model, referred to as Mask \\& Focus maps the\ninput context to a sequence of concepts which are then used to generate the\nresponse concepts. Together, the context and the response concepts generate the\nfinal response. In order to learn context concepts from the training data\nautomatically, we \\emph{mask} words in the input and observe the effect of\nmasking on response generation. We train our model to learn those response\nconcepts that have high mutual information with respect to the context\nconcepts, thereby guiding the model to \\emph{focus} on the context concepts.\nMask \\& Focus achieves significant improvement over the existing baselines in\nseveral established metrics for dialogues.", "published": "2020-02-11 15:11:55", "link": "http://arxiv.org/abs/2003.04976v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust Acoustic Scene Classification using a Multi-Spectrogram\n  Encoder-Decoder Framework", "abstract": "This article proposes an encoder-decoder network model for Acoustic Scene\nClassification (ASC), the task of identifying the scene of an audio recording\nfrom its acoustic signature. We make use of multiple low-level spectrogram\nfeatures at the front-end, transformed into higher level features through a\nwell-trained CNN-DNN front-end encoder. The high level features and their\ncombination (via a trained feature combiner) are then fed into different\ndecoder models comprising random forest regression, DNNs and a mixture of\nexperts, for back-end classification. We report extensive experiments to\nevaluate the accuracy of this framework for various ASC datasets, including\nLITIS Rouen and IEEE AASP Challenge on Detection and Classification of Acoustic\nScenes and Events (DCASE) 2016 Task 1, 2017 Task 1, 2018 Tasks 1A & 1B and 2019\nTasks 1A & 1B. The experimental results highlight two main contributions; the\nfirst is an effective method for high-level feature extraction from\nmulti-spectrogram input via the novel C-DNN architecture encoder network, and\nthe second is the proposed decoder which enables the framework to achieve\ncompetitive results on various datasets. The fact that a single framework is\nhighly competitive for several different challenges is an indicator of its\nrobustness for performing general ASC tasks.", "published": "2020-02-11 16:02:43", "link": "http://arxiv.org/abs/2002.04502v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CGCNN: Complex Gabor Convolutional Neural Network on raw speech", "abstract": "Convolutional Neural Networks (CNN) have been used in Automatic Speech\nRecognition (ASR) to learn representations directly from the raw signal instead\nof hand-crafted acoustic features, providing a richer and lossless input\nsignal. Recent researches propose to inject prior acoustic knowledge to the\nfirst convolutional layer by integrating the shape of the impulse responses in\norder to increase both the interpretability of the learnt acoustic model, and\nits performances. We propose to combine the complex Gabor filter with\ncomplex-valued deep neural networks to replace usual CNN weights kernels, to\nfully take advantage of its optimal time-frequency resolution and of the\ncomplex domain. The conducted experiments on the TIMIT phoneme recognition task\nshows that the proposed approach reaches top-of-the-line performances while\nremaining interpretable.", "published": "2020-02-11 17:48:17", "link": "http://arxiv.org/abs/2002.04569v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Periodicity Pitch Detection in Complex Harmonies on EEG Timeline Data", "abstract": "An acoustic stimulus, e.g., a musical harmony, is transformed in a highly\nnon-linear way during the hearing process in ear and brain. We study this by\ncomparing the frequency spectrum of an input stimulus and its response spectrum\nin the auditory processing stream using the frequency following response (FFR).\n  Using electroencephalography (EEG), we investigate whether the periodicity\npitches of complex harmonies (which are related to their missing fundamentals)\nare added in the auditory brainstem by analyzing the FFR. While other\nexperiments focus on common musical harmonies like the major and the minor\ntriad and dyads, we also consider the suspended chord. The suspended chord\ncauses tension foreign to the common triads and therefore holds a special role\namong the triads.\n  While watching a muted nature documentary, the participants hear synthesized\nclassic piano triads and single tones with a duration of 300ms for the stimulus\nand 100ms interstimulus interval. We acquired EEG data of 64 electrodes with a\nsampling rate of 5kHz to get a detailed enough resolution of the perception\nprocess in the human brain.\n  Applying a fast Fourier transformation (FFT) on the EEG response, starting\n50ms after stimulus onset, the evaluation of the frequency spectra shows that\nthe periodicity pitch frequencies calculated beforehand +/-3Hz occur with some\naccuracy. However, jitter turned out as a problem here. Note that the\nsought-for periodicity pitch frequencies do not physically exist in the\nfrequency spectra of the stimuli.", "published": "2020-02-11 14:39:44", "link": "http://arxiv.org/abs/2002.04990v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning with Out-of-Distribution Data for Audio Classification", "abstract": "In supervised machine learning, the assumption that training data is labelled\ncorrectly is not always satisfied. In this paper, we investigate an instance of\nlabelling error for classification tasks in which the dataset is corrupted with\nout-of-distribution (OOD) instances: data that does not belong to any of the\ntarget classes, but is labelled as such. We show that detecting and relabelling\ncertain OOD instances, rather than discarding them, can have a positive effect\non learning. The proposed method uses an auxiliary classifier, trained on data\nthat is known to be in-distribution, for detection and relabelling. The amount\nof data required for this is shown to be small. Experiments are carried out on\nthe FSDnoisy18k audio dataset, where OOD instances are very prevalent. The\nproposed method is shown to improve the performance of convolutional neural\nnetworks by a significant margin. Comparisons with other noise-robust\ntechniques are similarly encouraging.", "published": "2020-02-11 21:08:06", "link": "http://arxiv.org/abs/2002.04683v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phoneme Boundary Detection using Learnable Segmental Features", "abstract": "Phoneme boundary detection plays an essential first step for a variety of\nspeech processing applications such as speaker diarization, speech science,\nkeyword spotting, etc. In this work, we propose a neural architecture coupled\nwith a parameterized structured loss function to learn segmental\nrepresentations for the task of phoneme boundary detection. First, we evaluated\nour model when the spoken phonemes were not given as input. Results on the\nTIMIT and Buckeye corpora suggest that the proposed model is superior to the\nbaseline models and reaches state-of-the-art performance in terms of F1 and\nR-value. We further explore the use of phonetic transcription as additional\nsupervision and show this yields minor improvements in performance but\nsubstantially better convergence rates. We additionally evaluate the model on a\nHebrew corpus and demonstrate such phonetic supervision can be beneficial in a\nmulti-lingual setting.", "published": "2020-02-11 14:03:08", "link": "http://arxiv.org/abs/2002.04992v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
