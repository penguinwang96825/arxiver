{"title": "PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning", "abstract": "To build a high-quality open-domain chatbot, we introduce the effective\ntraining process of PLATO-2 via curriculum learning. There are two stages\ninvolved in the learning process. In the first stage, a coarse-grained\ngeneration model is trained to learn response generation under the simplified\nframework of one-to-one mapping. In the second stage, a fine-grained generative\nmodel augmented with latent variables and an evaluation model are further\ntrained to generate diverse responses and to select the best response,\nrespectively. PLATO-2 was trained on both Chinese and English data, whose\neffectiveness and superiority are verified through comprehensive evaluations,\nachieving new state-of-the-art results.", "published": "2020-06-30 13:36:10", "link": "http://arxiv.org/abs/2006.16779v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Segmentation Approach for Coreference Resolution Task", "abstract": "In coreference resolution, it is important to consider all members of a\ncoreference cluster and decide about all of them at once. This technique can\nhelp to avoid losing precision and also in finding long-distance relations. The\npresented paper is a report of an ongoing study on an idea which proposes a new\napproach for coreference resolution which can resolve all coreference mentions\nto a given mention in the document in one pass. This has been accomplished by\ndefining an embedding method for the position of all members of a coreference\ncluster in a document and resolving all of them for a given mention. In the\nproposed method, the BERT model has been used for encoding the documents and a\nhead network designed to capture the relations between the embedded tokens.\nThese are then converted to the proposed span position embedding matrix which\nembeds the position of all coreference mentions in the document. We tested this\nidea on CoNLL 2012 dataset and although the preliminary results from this\nmethod do not quite meet the state-of-the-art results, they are promising and\nthey can capture features like long-distance relations better than the other\napproaches.", "published": "2020-06-30 16:44:28", "link": "http://arxiv.org/abs/2007.04301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correction of Faulty Background Knowledge based on Condition Aware and\n  Revise Transformer for Question Answering", "abstract": "The study of question answering has received increasing attention in recent\nyears. This work focuses on providing an answer that compatible with both user\nintent and conditioning information corresponding to the question, such as\ndelivery status and stock information in e-commerce. However, these conditions\nmay be wrong or incomplete in real-world applications. Although existing\nquestion answering systems have considered the external information, such as\ncategorical attributes and triples in knowledge base, they all assume that the\nexternal information is correct and complete. To alleviate the effect of\ndefective condition values, this paper proposes condition aware and revise\nTransformer (CAR-Transformer). CAR-Transformer (1) revises each condition value\nbased on the whole conversation and original conditions values, and (2) it\nencodes the revised conditions and utilizes the conditions embedding to select\nan answer. Experimental results on a real-world customer service dataset\ndemonstrate that the CAR-Transformer can still select an appropriate reply when\nconditions corresponding to the question exist wrong or missing values, and\nsubstantially outperforms baseline models on automatic and human evaluations.\nThe proposed CAR-Transformer can be extended to other NLP tasks which need to\nconsider conditioning information.", "published": "2020-06-30 12:24:35", "link": "http://arxiv.org/abs/2006.16722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through\n  Scene Graph", "abstract": "We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates\nstructured knowledge obtained from scene graphs to learn joint representations\nof vision-language. ERNIE-ViL tries to build the detailed semantic connections\n(objects, attributes of objects and relationships between objects) across\nvision and language, which are essential to vision-language cross-modal tasks.\nUtilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph\nPrediction tasks, i.e., Object Prediction, Attribute Prediction and\nRelationship Prediction tasks in the pre-training phase. Specifically, these\nprediction tasks are implemented by predicting nodes of different types in the\nscene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint\nrepresentations characterizing the alignments of the detailed semantics across\nvision and language. After pre-training on large scale image-text aligned\ndatasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal\ndownstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these\ntasks and ranks the first place on the VCR leaderboard with an absolute\nimprovement of 3.7%.", "published": "2020-06-30 16:03:12", "link": "http://arxiv.org/abs/2006.16934v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning to Read through Machine Teaching", "abstract": "Learning to read words aloud is a major step towards becoming a reader. Many\nchildren struggle with the task because of the inconsistencies of English\nspelling-sound correspondences. Curricula vary enormously in how these patterns\nare taught. Children are nonetheless expected to master the system in limited\ntime (by grade 4). We used a cognitively interesting neural network\narchitecture to examine whether the sequence of learning trials could be\nstructured to facilitate learning. This is a hard combinatorial optimization\nproblem even for a modest number of learning trials (e.g., 10K). We show how\nthis sequence optimization problem can be posed as optimizing over a time\nvarying distribution i.e., defining probability distributions over words at\ndifferent steps in training. We then use stochastic gradient descent to find an\noptimal time-varying distribution and a corresponding optimal training\nsequence. We observed significant improvement on generalization accuracy\ncompared to baseline conditions (random sequences; sequences biased by word\nfrequency). These findings suggest an approach to improving learning outcomes\nin domains where performance depends on ability to generalize beyond limited\ntraining experience.", "published": "2020-06-30 02:04:52", "link": "http://arxiv.org/abs/2006.16470v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Data-driven Neural Network Architecture for Sentiment Analysis", "abstract": "The fabulous results of convolution neural networks in image-related tasks,\nattracted attention of text mining, sentiment analysis and other text analysis\nresearchers. It is however difficult to find enough data for feeding such\nnetworks, optimize their parameters, and make the right design choices when\nconstructing network architectures. In this paper we present the creation steps\nof two big datasets of song emotions. We also explore usage of convolution and\nmax-pooling neural layers on song lyrics, product and movie review text\ndatasets. Three variants of a simple and flexible neural network architecture\nare also compared. Our intention was to spot any important patterns that can\nserve as guidelines for parameter optimization of similar models. We also\nwanted to identify architecture design choices which lead to high performing\nsentiment analysis models. To this end, we conducted a series of experiments\nwith neural architectures of various configurations. Our results indicate that\nparallel convolutions of filter lengths up to three are usually enough for\ncapturing relevant text features. Also, max-pooling region size should be\nadapted to the length of text documents for producing the best feature maps.\nTop results we got are obtained with feature maps of lengths 6 to 18. An\nimprovement on future neural network models for sentiment analysis, could be\ngenerating sentiment polarity prediction of documents using aggregation of\npredictions on smaller excerpt of the entire text.", "published": "2020-06-30 10:08:36", "link": "http://arxiv.org/abs/2006.16642v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GShard: Scaling Giant Models with Conditional Computation and Automatic\n  Sharding", "abstract": "Neural network scaling has been critical for improving the model quality in\nmany real-world machine learning applications with vast amounts of training\ndata and compute. Although this trend of scaling is affirmed to be a sure-fire\napproach for better model quality, there are challenges on the path such as the\ncomputation cost, ease of programming, and efficient implementation on parallel\ndevices. GShard is a module composed of a set of lightweight annotation APIs\nand an extension to the XLA compiler. It provides an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the\nexisting model code. GShard enabled us to scale up multilingual neural machine\ntranslation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600\nbillion parameters using automatic sharding. We demonstrate that such a giant\nmodel can efficiently be trained on 2048 TPU v3 accelerators in 4 days to\nachieve far superior quality for translation from 100 languages to English\ncompared to the prior art.", "published": "2020-06-30 10:42:02", "link": "http://arxiv.org/abs/2006.16668v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Technical Report: Auxiliary Tuning and its Application to Conditional\n  Text Generation", "abstract": "We introduce a simple and efficient method, called Auxiliary Tuning, for\nadapting a pre-trained Language Model to a novel task; we demonstrate this\napproach on the task of conditional text generation. Our approach supplements\nthe original pre-trained model with an auxiliary model that shifts the output\ndistribution according to the target task. The auxiliary model is trained by\nadding its logits to the pre-trained model logits and maximizing the likelihood\nof the target task output. Our method imposes no constraints on the auxiliary\narchitecture. In particular, the auxiliary model can ingest additional input\nrelevant to the target task, independently from the pre-trained model's input.\nFurthermore, mixing the models at the logits level provides a natural\nprobabilistic interpretation of the method. Our method achieved similar results\nto training from scratch for several different tasks, while using significantly\nfewer resources for training; we share a specific example of text generation\nconditioned on keywords.", "published": "2020-06-30 14:00:48", "link": "http://arxiv.org/abs/2006.16823v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "SE3M: A Model for Software Effort Estimation Using Pre-trained Embedding\n  Models", "abstract": "Estimating effort based on requirement texts presents many challenges,\nespecially in obtaining viable features to infer effort. Aiming to explore a\nmore effective technique for representing textual requirements to infer effort\nestimates by analogy, this paper proposes to evaluate the effectiveness of\npre-trained embeddings models. For this, two embeddings approach, context-less\nand contextualized models are used. Generic pre-trained models for both\napproaches went through a fine-tuning process. The generated models were used\nas input in the applied deep learning architecture, with linear output. The\nresults were very promising, realizing that pre-trained incorporation models\ncan be used to estimate software effort based only on requirements texts. We\nhighlight the results obtained to apply the pre-trained BERT model with\nfine-tuning in a single project repository, whose value is the Mean Absolute\nError (MAE) is 4.25 and the standard deviation of only 0.17, which represents a\nresult very positive when compared to similar works. The main advantages of the\nproposed estimation method are reliability, the possibility of generalization,\nspeed, and low computational cost provided by the fine-tuning process, and the\npossibility to infer new or existing requirements.", "published": "2020-06-30 14:15:38", "link": "http://arxiv.org/abs/2006.16831v1", "categories": ["cs.SE", "cs.CL", "cs.LG", "68T50", "K.6.3; I.2.7; I.5.4"], "primary_category": "cs.SE"}
{"title": "OSCaR: Orthogonal Subspace Correction and Rectification of Biases in\n  Word Embeddings", "abstract": "Language representations are known to carry stereotypical biases and, as a\nresult, lead to biased predictions in downstream tasks. While existing methods\nare effective at mitigating biases by linear projection, such methods are too\naggressive: they not only remove bias, but also erase valuable information from\nword embeddings. We develop new measures for evaluating specific information\nretention that demonstrate the tradeoff between bias removal and information\nretention. To address this challenge, we propose OSCaR (Orthogonal Subspace\nCorrection and Rectification), a bias-mitigating method that focuses on\ndisentangling biased associations between concepts instead of removing concepts\nwholesale. Our experiments on gender biases show that OSCaR is a well-balanced\napproach that ensures that semantic information is retained in the embeddings\nand bias is also effectively mitigated.", "published": "2020-06-30 18:18:13", "link": "http://arxiv.org/abs/2007.00049v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Mutual Information for Text Generation", "abstract": "Recent advances in maximizing mutual information (MI) between the source and\ntarget have demonstrated its effectiveness in text generation. However,\nprevious works paid little attention to modeling the backward network of MI\n(i.e., dependency from the target to the source), which is crucial to the\ntightness of the variational information maximization lower bound. In this\npaper, we propose Adversarial Mutual Information (AMI): a text generation\nframework which is formed as a novel saddle point (min-max) optimization aiming\nto identify joint interactions between the source and target. Within this\nframework, the forward and backward networks are able to iteratively promote or\ndemote each other's generated instances by comparing the real and synthetic\ndata distributions. We also develop a latent noise sampling strategy that\nleverages random variations at the high-level semantic space to enhance the\nlong term dependency in the generation process. Extensive experiments based on\ndifferent text generation tasks demonstrate that the proposed AMI framework can\nsignificantly outperform several strong baselines, and we also show that AMI\nhas potential to lead to a tighter lower bound of maximum mutual information\nfor the variational information maximization problem.", "published": "2020-06-30 19:11:51", "link": "http://arxiv.org/abs/2007.00067v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multi-view Frequency LSTM: An Efficient Frontend for Automatic Speech\n  Recognition", "abstract": "Acoustic models in real-time speech recognition systems typically stack\nmultiple unidirectional LSTM layers to process the acoustic frames over time.\nPerformance improvements over vanilla LSTM architectures have been reported by\nprepending a stack of frequency-LSTM (FLSTM) layers to the time LSTM. These\nFLSTM layers can learn a more robust input feature to the time LSTM layers by\nmodeling time-frequency correlations in the acoustic input signals. A drawback\nof FLSTM based architectures however is that they operate at a predefined, and\ntuned, window size and stride, referred to as 'view' in this paper. We present\na simple and efficient modification by combining the outputs of multiple FLSTM\nstacks with different views, into a dimensionality reduced feature\nrepresentation. The proposed multi-view FLSTM architecture allows to model a\nwider range of time-frequency correlations compared to an FLSTM model with\nsingle view. When trained on 50K hours of English far-field speech data with\nCTC loss followed by sMBR sequence training, we show that the multi-view FLSTM\nacoustic model provides relative Word Error Rate (WER) improvements of 3-7% for\ndifferent speaker and acoustic environment scenarios over an optimized single\nFLSTM model, while retaining a similar computational footprint.", "published": "2020-06-30 22:19:53", "link": "http://arxiv.org/abs/2007.00131v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modality-Agnostic Attention Fusion for visual search with text feedback", "abstract": "Image retrieval with natural language feedback offers the promise of catalog\nsearch based on fine-grained visual features that go beyond objects and binary\nattributes, facilitating real-world applications such as e-commerce. Our\nModality-Agnostic Attention Fusion (MAAF) model combines image and text\nfeatures and outperforms existing approaches on two visual search with\nmodifying phrase datasets, Fashion IQ and CSS, and performs competitively on a\ndataset with only single-word modifications, Fashion200k. We also introduce two\nnew challenging benchmarks adapted from Birds-to-Words and Spot-the-Diff, which\nprovide new settings with rich language inputs, and we show that our approach\nwithout modification outperforms strong baselines. To better understand our\nmodel, we conduct detailed ablations on Fashion IQ and provide visualizations\nof the surprising phenomenon of words avoiding \"attending\" to the image region\nthey refer to.", "published": "2020-06-30 22:55:02", "link": "http://arxiv.org/abs/2007.00145v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BERTERS: Multimodal Representation Learning for Expert Recommendation\n  System with Transformer", "abstract": "The objective of an expert recommendation system is to trace a set of\ncandidates' expertise and preferences, recognize their expertise patterns, and\nidentify experts. In this paper, we introduce a multimodal classification\napproach for expert recommendation system (BERTERS). In our proposed system,\nthe modalities are derived from text (articles published by candidates) and\ngraph (their co-author connections) information. BERTERS converts text into a\nvector using the Bidirectional Encoder Representations from Transformer (BERT).\nAlso, a graph Representation technique called ExEm is used to extract the\nfeatures of candidates from the co-author network. Final representation of a\ncandidate is the concatenation of these vectors and other features. Eventually,\na classifier is built on the concatenation of features. This multimodal\napproach can be used in both the academic community and the community question\nanswering. To verify the effectiveness of BERTERS, we analyze its performance\non multi-label classification and visualization tasks.", "published": "2020-06-30 12:30:16", "link": "http://arxiv.org/abs/2007.07229v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A Speech Enhancement Algorithm based on Non-negative Hidden Markov Model\n  and Kullback-Leibler Divergence", "abstract": "In this paper, we propose a novel supervised single-channel speech\nenhancement method combing the the Kullback-Leibler divergence-based\nnon-negative matrix factorization (NMF) and hidden Markov model (NMF-HMM). With\nthe application of HMM, the temporal dynamics information of speech signals can\nbe taken into account. In the training stage, the sum of Poisson, leading to\nthe KL divergence measure, is used as the observation model for each state of\nHMM. This ensures that a computationally efficient multiplicative update can be\nused for the parameter update of the proposed model. In the online enhancement\nstage, we propose a novel minimum mean-square error (MMSE) estimator for the\nproposed NMF-HMM. This estimator can be implemented using parallel computing,\nsaving the time complexity. The performance of the proposed algorithm is\nverified by objective measures. The experimental results show that the proposed\nstrategy achieves better speech enhancement performance than state-of-the-art\nspeech enhancement methods. More specifically, compared with the traditional\nNMF-based speech enhancement methods, our proposed algorithm achieves a 5\\%\nimprovement for short-time objective intelligibility (STOI) and 0.18\nimprovement for perceptual evaluation of speech quality (PESQ).", "published": "2020-06-30 11:26:48", "link": "http://arxiv.org/abs/2006.16689v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Joint-Diagonalizability-Constrained Multichannel Nonnegative Matrix\n  Factorization Based on Multivariate Complex Sub-Gaussian Distribution", "abstract": "In this paper, we address a statistical model extension of multichannel\nnonnegative matrix factorization (MNMF) for blind source separation, and we\npropose a new parameter update algorithm used in the sub-Gaussian model. MNMF\nemploys full-rank spatial covariance matrices and can simulate situations in\nwhich the reverberation is strong and the sources are not point sources. In\nconventional MNMF, spectrograms of observed signals are assumed to follow a\nmultivariate Gaussian distribution. In this paper, first, to extend the MNMF\nmodel, we introduce the multivariate generalized Gaussian distribution as the\nmultivariate sub-Gaussian distribution. Since the cost function of MNMF based\non this multivariate sub-Gaussian model is difficult to minimize, we\nadditionally introduce the joint-diagonalizability constraint in spatial\ncovariance matrices to MNMF similarly to FastMNMF, and transform the cost\nfunction to the form to which we can apply the auxiliary functions to derive\nthe valid parameter update rules. Finally, from blind source separation\nexperiments, we show that the proposed method outperforms the conventional\nmethods in source-separation accuracy.", "published": "2020-06-30 11:37:03", "link": "http://arxiv.org/abs/2007.00416v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Sequential Self Teaching Approach for Improving Generalization in\n  Sound Event Recognition", "abstract": "An important problem in machine auditory perception is to recognize and\ndetect sound events. In this paper, we propose a sequential self-teaching\napproach to learning sounds. Our main proposition is that it is harder to learn\nsounds in adverse situations such as from weakly labeled and/or noisy labeled\ndata, and in these situations a single stage of learning is not sufficient. Our\nproposal is a sequential stage-wise learning process that improves\ngeneralization capabilities of a given modeling system. We justify this method\nvia technical results and on Audioset, the largest sound events dataset, our\nsequential learning approach can lead to up to 9% improvement in performance. A\ncomprehensive evaluation also shows that the method leads to improved\ntransferability of knowledge from previously trained models, thereby leading to\nimproved generalization capabilities on transfer learning tasks.", "published": "2020-06-30 22:53:43", "link": "http://arxiv.org/abs/2007.00144v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
