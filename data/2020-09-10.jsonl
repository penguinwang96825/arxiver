{"title": "Improving Coreference Resolution by Leveraging Entity-Centric Features\n  with Graph Neural Networks and Second-order Inference", "abstract": "One of the major challenges in coreference resolution is how to make use of\nentity-level features defined over clusters of mentions rather than mention\npairs. However, coreferent mentions usually spread far apart in an entire text,\nwhich makes it extremely difficult to incorporate entity-level features. We\npropose a graph neural network-based coreference resolution method that can\ncapture the entity-centric information by encouraging the sharing of features\nacross all mentions that probably refer to the same real-world entity. Mentions\nare linked to each other via the edges modeling how likely two linked mentions\npoint to the same entity. Modeling by such graphs, the features between\nmentions can be shared by message passing operations in an entity-centric\nmanner. A global inference algorithm up to second-order features is also\npresented to optimally cluster mentions into consistent groups. Experimental\nresults show our graph neural network-based method combing with the\nsecond-order decoding algorithm (named GNNCR) achieved close to\nstate-of-the-art performance on the English CoNLL-2012 Shared Task dataset.", "published": "2020-09-10 02:22:21", "link": "http://arxiv.org/abs/2009.04639v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Universal Representations from Word to Sentence", "abstract": "Despite the well-developed cut-edge representation learning for language,\nmost language representation models usually focus on specific level of\nlinguistic unit, which cause great inconvenience when being confronted with\nhandling multiple layers of linguistic objects in a unified way. Thus this work\nintroduces and explores the universal representation learning, i.e., embeddings\nof different levels of linguistic unit in a uniform vector space through a\ntask-independent evaluation. We present our approach of constructing analogy\ndatasets in terms of words, phrases and sentences and experiment with multiple\nrepresentation models to examine geometric properties of the learned vector\nspace. Then we empirically verify that well pre-trained Transformer models\nincorporated with appropriate training settings may effectively yield universal\nrepresentation. Especially, our implementation of fine-tuning ALBERT on NLI and\nPPDB datasets achieves the highest accuracy on analogy tasks in different\nlanguage levels. Further experiments on the insurance FAQ task show\neffectiveness of universal representation models in real-world applications.", "published": "2020-09-10 03:53:18", "link": "http://arxiv.org/abs/2009.04656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Target Segmentation for Direct Speech Translation", "abstract": "Recent studies on direct speech translation show continuous improvements by\nmeans of data augmentation techniques and bigger deep learning models. While\nthese methods are helping to close the gap between this new approach and the\nmore traditional cascaded one, there are many incongruities among different\nstudies that make it difficult to assess the state of the art. Surprisingly,\none point of discussion is the segmentation of the target text. Character-level\nsegmentation has been initially proposed to obtain an open vocabulary, but it\nresults on long sequences and long training time. Then, subword-level\nsegmentation became the state of the art in neural machine translation as it\nproduces shorter sequences that reduce the training time, while being superior\nto word-level models. As such, recent works on speech translation started using\ntarget subwords despite the initial use of characters and some recent claims of\nbetter results at the character level. In this work, we perform an extensive\ncomparison of the two methods on three benchmarks covering 8 language\ndirections and multilingual training. Subword-level segmentation compares\nfavorably in all settings, outperforming its character-level counterpart in a\nrange of 1 to 3 BLEU points.", "published": "2020-09-10 07:47:01", "link": "http://arxiv.org/abs/2009.04707v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyze the Effects of Weighting Functions on Cost Function in the Glove\n  Model", "abstract": "When dealing with the large vocabulary size and corpus size, the run-time for\ntraining Glove model is long, it can even be up to several dozen hours for\ndata, which is approximately 500MB in size. As a result, finding and selecting\nthe optimal parameters for the weighting function create many difficulties for\nweak hardware. Of course, to get the best results, we need to test benchmarks\nmany times. In order to solve this problem, we derive a weighting function,\nwhich can save time for choosing parameters and making benchmarks. It also\nallows one to obtain nearly similar accuracy at the same given time without\nconcern for experimentation.", "published": "2020-09-10 08:55:25", "link": "http://arxiv.org/abs/2009.04732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Grievance Dictionary: Understanding Threatening Language Use", "abstract": "This paper introduces the Grievance Dictionary, a psycholinguistic dictionary\nwhich can be used to automatically understand language use in the context of\ngrievance-fuelled violence threat assessment. We describe the development the\ndictionary, which was informed by suggestions from experienced threat\nassessment practitioners. These suggestions and subsequent human and\ncomputational word list generation resulted in a dictionary of 20,502 words\nannotated by 2,318 participants. The dictionary was validated by applying it to\ntexts written by violent and non-violent individuals, showing strong evidence\nfor a difference between populations in several dictionary categories. Further\nclassification tasks showed promising performance, but future improvements are\nstill needed. Finally, we provide instructions and suggestions for the use of\nthe Grievance Dictionary by security professionals and (violence) researchers.", "published": "2020-09-10 12:06:48", "link": "http://arxiv.org/abs/2009.04798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Gender Bias in BERT", "abstract": "Contextual language models (CLMs) have pushed the NLP benchmarks to a new\nheight. It has become a new norm to utilize CLM provided word embeddings in\ndownstream tasks such as text classification. However, unless addressed, CLMs\nare prone to learn intrinsic gender-bias in the dataset. As a result,\npredictions of downstream NLP models can vary noticeably by varying gender\nwords, such as replacing \"he\" to \"she\", or even gender-neutral words. In this\npaper, we focus our analysis on a popular CLM, i.e., BERT. We analyse the\ngender-bias it induces in five downstream tasks related to emotion and\nsentiment intensity prediction. For each task, we train a simple regressor\nutilizing BERT's word embeddings. We then evaluate the gender-bias in\nregressors using an equity evaluation corpus. Ideally and from the specific\ndesign, the models should discard gender informative features from the input.\nHowever, the results show a significant dependence of the system's predictions\non gender-particular words and phrases. We claim that such biases can be\nreduced by removing genderspecific features from word embedding. Hence, for\neach layer in BERT, we identify directions that primarily encode gender\ninformation. The space formed by such directions is referred to as the gender\nsubspace in the semantic space of word embeddings. We propose an algorithm that\nfinds fine-grained gender directions, i.e., one primary direction for each BERT\nlayer. This obviates the need of realizing gender subspace in multiple\ndimensions and prevents other crucial information from being omitted.\nExperiments show that removing embedding components in such directions achieves\ngreat success in reducing BERT-induced bias in the downstream tasks.", "published": "2020-09-10 17:38:32", "link": "http://arxiv.org/abs/2009.05021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Relation Extraction with Document-level Heterogeneous Graph\n  Attention Networks", "abstract": "Dialogue relation extraction (DRE) aims to detect the relation between two\nentities mentioned in a multi-party dialogue. It plays an important role in\nconstructing knowledge graphs from conversational data increasingly abundant on\nthe internet and facilitating intelligent dialogue system development. The\nprior methods of DRE do not meaningfully leverage speaker information-they just\nprepend the utterances with the respective speaker names. Thus, they fail to\nmodel the crucial inter-speaker relations that may give additional context to\nrelevant argument entities through pronouns and triggers. We, however, present\na graph attention network-based method for DRE where a graph, that contains\nmeaningfully connected speaker, entity, entity-type, and utterance nodes, is\nconstructed. This graph is fed to a graph attention network for context\npropagation among relevant nodes, which effectively captures the dialogue\ncontext. We empirically show that this graph-based approach quite effectively\ncaptures the relations between different entity pairs in a dialogue as it\noutperforms the state-of-the-art approaches by a significant margin on the\nbenchmark dataset DialogRE. Our code is released at:\nhttps://github.com/declare-lab/dialog-HGAT", "published": "2020-09-10 18:51:48", "link": "http://arxiv.org/abs/2009.05092v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RadLex Normalization in Radiology Reports", "abstract": "Radiology reports have been widely used for extraction of various clinically\nsignificant information about patients' imaging studies. However, limited\nresearch has focused on standardizing the entities to a common\nradiology-specific vocabulary. Further, no study to date has attempted to\nleverage RadLex for standardization. In this paper, we aim to normalize a\ndiverse set of radiological entities to RadLex terms. We manually construct a\nnormalization corpus by annotating entities from three types of reports. This\ncontains 1706 entity mentions. We propose two deep learning-based NLP methods\nbased on a pre-trained language model (BERT) for automatic normalization.\nFirst, we employ BM25 to retrieve candidate concepts for the BERT-based models\n(re-ranker and span detector) to predict the normalized concept. The results\nare promising, with the best accuracy (78.44%) obtained by the span detector.\nAdditionally, we discuss the challenges involved in corpus construction and\npropose new RadLex terms.", "published": "2020-09-10 19:59:08", "link": "http://arxiv.org/abs/2009.05128v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FILTER: An Enhanced Fusion Method for Cross-lingual Language\n  Understanding", "abstract": "Large-scale cross-lingual language models (LM), such as mBERT, Unicoder and\nXLM, have achieved great success in cross-lingual representation learning.\nHowever, when applied to zero-shot cross-lingual transfer tasks, most existing\nmethods use only single-language input for LM finetuning, without leveraging\nthe intrinsic cross-lingual alignment between different languages that proves\nessential for multilingual tasks. In this paper, we propose FILTER, an enhanced\nfusion method that takes cross-lingual data as input for XLM finetuning.\nSpecifically, FILTER first encodes text input in the source language and its\ntranslation in the target language independently in the shallow layers, then\nperforms cross-language fusion to extract multilingual knowledge in the\nintermediate layers, and finally performs further language-specific encoding.\nDuring inference, the model makes predictions based on the text input in the\ntarget language and its translation in the source language. For simple tasks\nsuch as classification, translated text in the target language shares the same\nlabel as the source language. However, this shared label becomes less accurate\nor even unavailable for more complex tasks such as question answering, NER and\nPOS tagging. To tackle this issue, we further propose an additional\nKL-divergence self-teaching loss for model training, based on auto-generated\nsoft pseudo-labels for translated text in the target language. Extensive\nexperiments demonstrate that FILTER achieves new state of the art on two\nchallenging multilingual multi-task benchmarks, XTREME and XGLUE.", "published": "2020-09-10 22:42:15", "link": "http://arxiv.org/abs/2009.05166v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Real-Time Question Answering via Question Generation", "abstract": "Although deep neural networks have achieved tremendous success for question\nanswering (QA), they are still suffering from heavy computational and energy\ncost for real product deployment. Further, existing QA systems are bottlenecked\nby the encoding time of real-time questions with neural networks, thus\nsuffering from detectable latency in deployment for large-volume traffic. To\nreduce the computational cost and accelerate real-time question answering\n(RTQA) for practical usage, we propose to remove all the neural networks from\nonline QA systems, and present Ocean-Q (an Ocean of Questions), which\nintroduces a new question generation (QG) model to generate a large pool of QA\npairs offline, then in real time matches an input question with the candidate\nQA pool to predict the answer without question encoding. Ocean-Q can be readily\ndeployed in existing distributed database systems or search engine for\nlarge-scale query usage, and much greener with no additional cost for\nmaintaining large neural networks. Experiments on SQuAD(-open) and HotpotQA\nbenchmarks demonstrate that Ocean-Q is able to accelerate the fastest\nstate-of-the-art RTQA system by 4X times, with only a 3+% accuracy drop.", "published": "2020-09-10 22:44:29", "link": "http://arxiv.org/abs/2009.05167v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emora: An Inquisitive Social Chatbot Who Cares For You", "abstract": "Inspired by studies on the overwhelming presence of experience-sharing in\nhuman-human conversations, Emora, the social chatbot developed by Emory\nUniversity, aims to bring such experience-focused interaction to the current\nfield of conversational AI. The traditional approach of information-sharing\ntopic handlers is balanced with a focus on opinion-oriented exchanges that\nEmora delivers, and new conversational abilities are developed that support\ndialogues that consist of a collaborative understanding and learning process of\nthe partner's life experiences. We present a curated dialogue system that\nleverages highly expressive natural language templates, powerful intent\nclassification, and ontology resources to provide an engaging and interesting\nconversational experience to every user.", "published": "2020-09-10 00:42:59", "link": "http://arxiv.org/abs/2009.04617v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Response Selection Models Really Know What's Next? Utterance\n  Manipulation Strategies for Multi-turn Response Selection", "abstract": "In this paper, we study the task of selecting the optimal response given a\nuser and system utterance history in retrieval-based multi-turn dialog systems.\nRecently, pre-trained language models (e.g., BERT, RoBERTa, and ELECTRA) showed\nsignificant improvements in various natural language processing tasks. This and\nsimilar response selection tasks can also be solved using such language models\nby formulating the tasks as dialog--response binary classification tasks.\nAlthough existing works using this approach successfully obtained\nstate-of-the-art results, we observe that language models trained in this\nmanner tend to make predictions based on the relatedness of history and\ncandidates, ignoring the sequential nature of multi-turn dialog systems. This\nsuggests that the response selection task alone is insufficient for learning\ntemporal dependencies between utterances. To this end, we propose utterance\nmanipulation strategies (UMS) to address this problem. Specifically, UMS\nconsist of several strategies (i.e., insertion, deletion, and search), which\naid the response selection model towards maintaining dialog coherence. Further,\nUMS are self-supervised methods that do not require additional annotation and\nthus can be easily incorporated into existing approaches. Extensive evaluation\nacross multiple languages and models shows that UMS are highly effective in\nteaching dialog consistency, which leads to models pushing the state-of-the-art\nwith significant margins on multiple public benchmark datasets.", "published": "2020-09-10 07:39:05", "link": "http://arxiv.org/abs/2009.04703v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta-Learning with Sparse Experience Replay for Lifelong Language\n  Learning", "abstract": "Lifelong learning requires models that can continuously learn from sequential\nstreams of data without suffering catastrophic forgetting due to shifts in data\ndistributions. Deep learning models have thrived in the non-sequential learning\nparadigm; however, when used to learn a sequence of tasks, they fail to retain\npast knowledge and learn incrementally. We propose a novel approach to lifelong\nlearning of language tasks based on meta-learning with sparse experience replay\nthat directly optimizes to prevent forgetting. We show that under the realistic\nsetting of performing a single pass on a stream of tasks and without any task\nidentifiers, our method obtains state-of-the-art results on lifelong text\nclassification and relation extraction. We analyze the effectiveness of our\napproach and further demonstrate its low computational and space complexity.", "published": "2020-09-10 14:36:38", "link": "http://arxiv.org/abs/2009.04891v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classification of descriptions and summary using multiple passes of\n  statistical and natural language toolkits", "abstract": "This document describes a possible approach that can be used to check the\nrelevance of a summary / definition of an entity with respect to its name. This\nclassifier focuses on the relevancy of an entity's name to its summary /\ndefinition, in other words, it is a name relevance check. The percentage score\nobtained from this approach can be used either on its own or used to supplement\nscores obtained from other metrics to arrive upon a final classification; at\nthe end of the document, potential improvements have also been outlined. The\ndataset that this document focuses on achieving an objective score is a list of\npackage names and their respective summaries (sourced from pypi.org).", "published": "2020-09-10 15:49:24", "link": "http://arxiv.org/abs/2009.04953v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.5.1; I.5.2; I.7.0"], "primary_category": "cs.CL"}
{"title": "Modern Methods for Text Generation", "abstract": "Synthetic text generation is challenging and has limited success. Recently, a\nnew architecture, called Transformers, allow machine learning models to\nunderstand better sequential data, such as translation or summarization. BERT\nand GPT-2, using Transformers in their cores, have shown a great performance in\ntasks such as text classification, translation and NLI tasks. In this article,\nwe analyse both algorithms and compare their output quality in text generation\ntasks.", "published": "2020-09-10 16:17:10", "link": "http://arxiv.org/abs/2009.04968v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dialogue-adaptive Language Model Pre-training From Quality Estimation", "abstract": "Pre-trained language models (PrLMs) have achieved great success on a wide\nrange of natural language processing tasks by virtue of the universal language\nrepresentation ability obtained by self-supervised learning on a large corpus.\nThese models are pre-trained on standard plain texts with general language\nmodel (LM) training objectives, which would be insufficient to model\ndialogue-exclusive attributes like specificity and informativeness reflected in\nthese tasks that are not explicitly captured by the pre-trained universal\nlanguage representations. In this work, we propose dialogue-adaptive\npre-training objectives (DAPO) derived from quality estimation to simulate\ndialogue-specific features, namely coherence, specificity, and informativeness.\nAs the foundation for model pre-training, we synthesize a new dialogue corpus\nand build our training set with two unsupervised methods: 1) coherence-oriented\ncontext corruption, including utterance ordering, insertion, and replacement,\nto help the model capture the coherence inside the dialogue contexts; and 2)\nspecificity-oriented automatic rescoring, which encourages the model to measure\nthe quality of the synthesized data for dialogue-adaptive pre-training by\nconsidering specificity and informativeness. Experimental results on widely\nused open-domain response selection and quality estimation benchmarks show that\nDAPO significantly improves the baseline models and achieves state-of-the-art\nperformance on the MuTual leaderboard, verifying the effectiveness of\nestimating quality evaluation factors into pre-training.", "published": "2020-09-10 16:46:46", "link": "http://arxiv.org/abs/2009.04984v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-modal embeddings using multi-task learning for emotion recognition", "abstract": "General embeddings like word2vec, GloVe and ELMo have shown a lot of success\nin natural language tasks. The embeddings are typically extracted from models\nthat are built on general tasks such as skip-gram models and natural language\ngeneration. In this paper, we extend the work from natural language\nunderstanding to multi-modal architectures that use audio, visual and textual\ninformation for machine learning tasks. The embeddings in our network are\nextracted using the encoder of a transformer model trained using multi-task\ntraining. We use person identification and automatic speech recognition as the\ntasks in our embedding generation framework. We tune and evaluate the\nembeddings on the downstream task of emotion recognition and demonstrate that\non the CMU-MOSEI dataset, the embeddings can be used to improve over previous\nstate of the art results.", "published": "2020-09-10 17:33:16", "link": "http://arxiv.org/abs/2009.05019v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GeoSPARQL+: Syntax, Semantics and System for Integrated Querying of\n  Graph, Raster and Vector Data -- Technical Report", "abstract": "We introduce an approach to semantically represent and query raster data in a\nSemantic Web graph. We extend the GeoSPARQL vocabulary and query language to\nsupport raster data as a new type of geospatial data. We define new filter\nfunctions and illustrate our approach using several use cases on real-world\ndata sets. Finally, we describe a prototypical implementation and validate the\nfeasibility of our approach.", "published": "2020-09-10 17:53:19", "link": "http://arxiv.org/abs/2009.05032v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Patient Cohort Retrieval using Transformer Language Models", "abstract": "We apply deep learning-based language models to the task of patient cohort\nretrieval (CR) with the aim to assess their efficacy. The task of CR requires\nthe extraction of relevant documents from the electronic health records (EHRs)\non the basis of a given query. Given the recent advancements in the field of\ndocument retrieval, we map the task of CR to a document retrieval task and\napply various deep neural models implemented for the general domain tasks. In\nthis paper, we propose a framework for retrieving patient cohorts using neural\nlanguage models without the need of explicit feature engineering and domain\nexpertise. We find that a majority of our models outperform the BM25 baseline\nmethod on various evaluation metrics.", "published": "2020-09-10 19:40:41", "link": "http://arxiv.org/abs/2009.05121v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Rank over Class: The Untapped Potential of Ranking in Natural Language\n  Processing", "abstract": "Text classification has long been a staple within Natural Language Processing\n(NLP) with applications spanning across diverse areas such as sentiment\nanalysis, recommender systems and spam detection. With such a powerful\nsolution, it is often tempting to use it as the go-to tool for all NLP problems\nsince when you are holding a hammer, everything looks like a nail. However, we\nargue here that many tasks which are currently addressed using classification\nare in fact being shoehorned into a classification mould and that if we instead\naddress them as a ranking problem, we not only improve the model, but we\nachieve better performance. We propose a novel end-to-end ranking approach\nconsisting of a Transformer network responsible for producing representations\nfor a pair of text sequences, which are in turn passed into a context\naggregating network outputting ranking scores used to determine an ordering to\nthe sequences based on some notion of relevance. We perform numerous\nexperiments on publicly-available datasets and investigate the applications of\nranking in problems often solved using classification. In an experiment on a\nheavily-skewed sentiment analysis dataset, converting ranking results to\nclassification labels yields an approximately 22% improvement over\nstate-of-the-art text classification, demonstrating the efficacy of text\nranking over text classification in certain scenarios.", "published": "2020-09-10 22:18:57", "link": "http://arxiv.org/abs/2009.05160v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparsifying Transformer Models with Trainable Representation Pooling", "abstract": "We propose a novel method to sparsify attention in the Transformer model by\nlearning to select the most-informative token representations during the\ntraining process, thus focusing on the task-specific parts of an input. A\nreduction of quadratic time and memory complexity to sublinear was achieved due\nto a robust trainable top-$k$ operator. Our experiments on a challenging long\ndocument summarization task show that even our simple baseline performs\ncomparably to the current SOTA, and with trainable pooling, we can retain its\ntop quality, while being $1.8\\times$ faster during training, $4.5\\times$ faster\nduring inference, and up to $13\\times$ more computationally efficient in the\ndecoder.", "published": "2020-09-10 22:49:39", "link": "http://arxiv.org/abs/2009.05169v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Denoising Large-Scale Image Captioning from Alt-text Data using Content\n  Selection Models", "abstract": "Training large-scale image captioning (IC) models demands access to a rich\nand diverse set of training examples, gathered from the wild, often from noisy\nalt-text data. However, recent modeling approaches to IC often fall short in\nterms of performance in this case, because they assume a clean annotated\ndataset (as opposed to the noisier alt-text--based annotations), and employ an\nend-to-end generation approach, which often lacks both controllability and\ninterpretability. We address these problems by breaking down the task into two\nsimpler, more controllable tasks -- skeleton prediction and skeleton-based\ncaption generation. Specifically, we show that selecting content words as\nskeletons} helps in generating improved and denoised captions when leveraging\nrich yet noisy alt-text--based uncurated datasets. We also show that the\npredicted English skeletons can be further cross-lingually leveraged to\ngenerate non-English captions, and present experimental results covering\ncaption generation in French, Italian, German, Spanish and Hindi. We also show\nthat skeleton-based prediction allows for better control of certain caption\nproperties, such as length, content, and gender expression, providing a handle\nto perform human-in-the-loop semi-automatic corrections.", "published": "2020-09-10 23:31:38", "link": "http://arxiv.org/abs/2009.05175v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Non-Pharmaceutical Intervention Discovery with Topic Modeling", "abstract": "We consider the task of discovering categories of non-pharmaceutical\ninterventions during the evolving COVID-19 pandemic. We explore topic modeling\non two corpora with national and international scope. These models discover\nexisting categories when compared with human intervention labels while reduced\nhuman effort needed.", "published": "2020-09-10 11:37:00", "link": "http://arxiv.org/abs/2009.13602v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Brain2Word: Decoding Brain Activity for Language Generation", "abstract": "Brain decoding, understood as the process of mapping brain activities to the\nstimuli that generated them, has been an active research area in the last\nyears. In the case of language stimuli, recent studies have shown that it is\npossible to decode fMRI scans into an embedding of the word a subject is\nreading. However, such word embeddings are designed for natural language\nprocessing tasks rather than for brain decoding. Therefore, they limit our\nability to recover the precise stimulus. In this work, we propose to directly\nclassify an fMRI scan, mapping it to the corresponding word within a fixed\nvocabulary. Unlike existing work, we evaluate on scans from previously unseen\nsubjects. We argue that this is a more realistic setup and we present a model\nthat can decode fMRI data from unseen subjects. Our model achieves 5.22% Top-1\nand 13.59% Top-5 accuracy in this challenging task, significantly outperforming\nall the considered competitive baselines. Furthermore, we use the decoded words\nto guide language generation with the GPT-2 model. This way, we advance the\nquest for a system that translates brain activities into coherent text.", "published": "2020-09-10 10:47:36", "link": "http://arxiv.org/abs/2009.04765v3", "categories": ["cs.CL", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Visual Relationship Detection with Visual-Linguistic Knowledge from\n  Multimodal Representations", "abstract": "Visual relationship detection aims to reason over relationships among salient\nobjects in images, which has drawn increasing attention over the past few\nyears. Inspired by human reasoning mechanisms, it is believed that external\nvisual commonsense knowledge is beneficial for reasoning visual relationships\nof objects in images, which is however rarely considered in existing methods.\nIn this paper, we propose a novel approach named Relational Visual-Linguistic\nBidirectional Encoder Representations from Transformers (RVL-BERT), which\nperforms relational reasoning with both visual and language commonsense\nknowledge learned via self-supervised pre-training with multimodal\nrepresentations. RVL-BERT also uses an effective spatial module and a novel\nmask attention module to explicitly capture spatial information among the\nobjects. Moreover, our model decouples object detection from visual\nrelationship recognition by taking in object names directly, enabling it to be\nused on top of any object detection system. We show through quantitative and\nqualitative experiments that, with the transferred knowledge and novel modules,\nRVL-BERT achieves competitive results on two challenging visual relationship\ndetection datasets. The source code is available at\nhttps://github.com/coldmanck/RVL-BERT.", "published": "2020-09-10 16:15:09", "link": "http://arxiv.org/abs/2009.04965v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-Hop Fact Checking of Political Claims", "abstract": "Recent work has proposed multi-hop models and datasets for studying complex\nnatural language reasoning. One notable task requiring multi-hop reasoning is\nfact checking, where a set of connected evidence pieces leads to the final\nverdict of a claim. However, existing datasets either do not provide\nannotations for gold evidence pages, or the only dataset which does (FEVER)\nmostly consists of claims which can be fact-checked with simple reasoning and\nis constructed artificially. Here, we study more complex claim verification of\nnaturally occurring claims with multiple hops over interconnected evidence\nchunks. We: 1) construct a small annotated dataset, PolitiHop, of evidence\nsentences for claim verification; 2) compare it to existing multi-hop datasets;\nand 3) study how to transfer knowledge from more extensive in- and\nout-of-domain resources to PolitiHop. We find that the task is complex and\nachieve the best performance with an architecture that specifically models\nreasoning over evidence pieces in combination with in-domain transfer learning.", "published": "2020-09-10 13:54:15", "link": "http://arxiv.org/abs/2009.06401v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Time-Aware Evidence Ranking for Fact-Checking", "abstract": "Truth can vary over time. Fact-checking decisions on claim veracity should\ntherefore take into account temporal information of both the claim and\nsupporting or refuting evidence. In this work, we investigate the hypothesis\nthat the timestamp of a Web page is crucial to how it should be ranked for a\ngiven claim. We delineate four temporal ranking methods that constrain evidence\nranking differently and simulate hypothesis-specific evidence rankings given\nthe evidence timestamps as gold standard. Evidence ranking in three\nfact-checking models is ultimately optimized using a learning-to-rank loss\nfunction. Our study reveals that time-aware evidence ranking not only surpasses\nrelevance assumptions based purely on semantic similarity or position in a\nsearch results list, but also improves veracity predictions of time-sensitive\nclaims in particular.", "published": "2020-09-10 13:39:49", "link": "http://arxiv.org/abs/2009.06402v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "ICASSP 2021 Acoustic Echo Cancellation Challenge: Datasets, Testing\n  Framework, and Results", "abstract": "The ICASSP 2021 Acoustic Echo Cancellation Challenge is intended to stimulate\nresearch in the area of acoustic echo cancellation (AEC), which is an important\npart of speech enhancement and still a top issue in audio communication and\nconferencing systems. Many recent AEC studies report good performance on\nsynthetic datasets where the train and test samples come from the same\nunderlying distribution. However, the AEC performance often degrades\nsignificantly on real recordings. Also, most of the conventional objective\nmetrics such as echo return loss enhancement (ERLE) and perceptual evaluation\nof speech quality (PESQ) do not correlate well with subjective speech quality\ntests in the presence of background noise and reverberation found in realistic\nenvironments. In this challenge, we open source two large datasets to train AEC\nmodels under both single talk and double talk scenarios. These datasets consist\nof recordings from more than 2,500 real audio devices and human speakers in\nreal environments, as well as a synthetic dataset. We open source two large\ntest sets, and we open source an online subjective test framework for\nresearchers to quickly test their results. The winners of this challenge will\nbe selected based on the average Mean Opinion Score (MOS) achieved across all\ndifferent single talk and double talk scenarios.", "published": "2020-09-10 16:25:51", "link": "http://arxiv.org/abs/2009.04972v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploration of End-to-end Synthesisers forZero Resource Speech Challenge\n  2020", "abstract": "A Spoken dialogue system for an unseen language is referred to as Zero\nresource speech. It is especially beneficial for developing applications for\nlanguages that have low digital resources. Zero resource speech synthesis is\nthe task of building text-to-speech (TTS) models in the absence of\ntranscriptions. In this work, speech is modelled as a sequence of transient and\nsteady-state acoustic units, and a unique set of acoustic units is discovered\nby iterative training. Using the acoustic unit sequence, TTS models are\ntrained. The main goal of this work is to improve the synthesis quality of zero\nresource TTS system. Four different systems are proposed. All the systems\nconsist of three stages: unit discovery, followed by unit sequence to\nspectrogram mapping, and finally spectrogram to speech inversion. Modifications\nare proposed to the spectrogram mapping stage. These modifications include\ntraining the mapping on voice data, using x-vectors to improve the mapping,\ntwo-stage learning, and gender-specific modelling. Evaluation of the proposed\nsystems in the Zerospeech 2020 challenge shows that quite good quality\nsynthesis can be achieved.", "published": "2020-09-10 16:46:33", "link": "http://arxiv.org/abs/2009.04983v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Utterance Clustering Using Stereo Audio Channels", "abstract": "Utterance clustering is one of the actively researched topics in audio signal\nprocessing and machine learning. This study aims to improve the performance of\nutterance clustering by processing multichannel (stereo) audio signals.\nProcessed audio signals were generated by combining left- and right-channel\naudio signals in a few different ways and then extracted embedded features\n(also called d-vectors) from those processed audio signals. This study applied\nthe Gaussian mixture model for supervised utterance clustering. In the training\nphase, a parameter sharing Gaussian mixture model was conducted to train the\nmodel for each speaker. In the testing phase, the speaker with the maximum\nlikelihood was selected as the detected speaker. Results of experiments with\nreal audio recordings of multi-person discussion sessions showed that the\nproposed method that used multichannel audio signals achieved significantly\nbetter performance than a conventional method with mono audio signals in more\ncomplicated conditions.", "published": "2020-09-10 18:25:33", "link": "http://arxiv.org/abs/2009.05076v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
