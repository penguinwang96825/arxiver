{"title": "Decision-informed Neural Networks with Large Language Model Integration for Portfolio Optimization", "abstract": "This paper addresses the critical disconnect between prediction and decision\nquality in portfolio optimization by integrating Large Language Models (LLMs)\nwith decision-focused learning. We demonstrate both theoretically and\nempirically that minimizing the prediction error alone leads to suboptimal\nportfolio decisions. We aim to exploit the representational power of LLMs for\ninvestment decisions. An attention mechanism processes asset relationships,\ntemporal dependencies, and macro variables, which are then directly integrated\ninto a portfolio optimization layer. This enables the model to capture complex\nmarket dynamics and align predictions with the decision objectives. Extensive\nexperiments on S\\&P100 and DOW30 datasets show that our model consistently\noutperforms state-of-the-art deep learning models. In addition, gradient-based\nanalyses show that our model prioritizes the assets most crucial to decision\nmaking, thus mitigating the effects of prediction errors on portfolio\nperformance. These findings underscore the value of integrating decision\nobjectives into predictions for more robust and context-aware portfolio\nmanagement.", "published": "2025-02-02 15:45:21", "link": "http://arxiv.org/abs/2502.00828v1", "categories": ["q-fin.PM", "cs.AI", "q-fin.CP"], "primary_category": "q-fin.PM"}
{"title": "Floating exercise boundaries for American options in time-inhomogeneous models", "abstract": "This paper examines a semi-analytical approach for pricing American options\nin time-inhomogeneous models characterized by negative interest rates (for\nequity, FX) or negative convenience yields (for commodities, cryptocurrencies).\nUnder such conditions, exercise boundaries may exhibit a \"floating\" structure -\ndynamically appearing and disappearing. For example, a second exercise boundary\ncould emerge within the computational domain and subsequently both could\ncollapse, demanding specialized pricing methodologies.", "published": "2025-02-02 10:21:22", "link": "http://arxiv.org/abs/2502.00740v1", "categories": ["q-fin.PR", "q-fin.CP", "q-fin.MF"], "primary_category": "q-fin.PR"}
{"title": "On Noncommutative Quantum Mechanics and the Black-Scholes Model", "abstract": "Two novel and direct quantum mechanical representations of the Black-Scholes\nmodel are constructed based on the (Wick-rotated) quantization of two specific\nmechanical systems. The quantum setup is achieved by means of the associated\nLaplace-Beltrami operator (one for each model), and not by merely applying the\nusual naive rule. Additionally, the clear identification of the geometric\ncontent of the underlying classical framework is exploited in order to arrive\nat a noncommutative quantum mechanics generalization of the Black-Scholes\nmodel. We also consider a system consisting of two degrees of freedom whose\n(Wick-rotated) quantization leads to a model which can be seen as related to\nthe Merton-Garman family. This model is also generalized via noncommutative\nquantum mechanics.", "published": "2025-02-02 22:10:45", "link": "http://arxiv.org/abs/2502.00938v1", "categories": ["q-fin.MF", "quant-ph"], "primary_category": "q-fin.MF"}
{"title": "Trade Dynamics of the Global Dry Bulk Shipping Network", "abstract": "This study investigates the inherently random structures of dry bulk shipping\nnetworks, often likened to a taxi service, and identifies the underlying trade\ndynamics that contribute to this randomness within individual cargo\nsub-networks. By analysing micro-level trade flow data from 2015 to 2023, we\nexplore the evolution of dry commodity networks, including grain, coal, and\niron ore, and suggest that the Giant Strongly Connected Components exhibit\nsmall-world phenomena, indicative of efficient bilateral trade. The significant\nheterogeneity of in-degree and out-degree within these sub-networks, primarily\ndriven by importing ports, underscores the complexity of their dynamics. Our\ntemporal analysis shows that while the Covid-19 pandemic profoundly impacted\nthe coal network, the Ukraine conflict significantly altered the grain network,\nresulting in changes in community structures. Notably, grain sub-networks\ndisplay periodic changes, suggesting distinct life cycles absent in coal and\niron ore networks. These findings illustrate that the randomness in dry bulk\nshipping networks is a reflection of real-world trade dynamics, providing\nvaluable insights for stakeholders in navigating and predicting network\nbehaviours.", "published": "2025-02-02 19:01:22", "link": "http://arxiv.org/abs/2502.00877v1", "categories": ["q-fin.MF", "physics.soc-ph"], "primary_category": "q-fin.MF"}
{"title": "Efficient Language Modeling for Low-Resource Settings with Hybrid\n  RNN-Transformer Architectures", "abstract": "Transformer-based language models have recently been at the forefront of\nactive research in text generation. However, these models' advances come at the\nprice of prohibitive training costs, with parameter counts in the billions and\ncompute requirements measured in petaflop/s-decades. In this paper, we\ninvestigate transformer-based architectures for improving model performance in\na low-data regime by selectively replacing attention layers with feed-forward\nand quasi-recurrent neural network layers. We test these architectures on the\nstandard Enwik8 and Wikitext-103 corpora. Our results show that our reduced\narchitectures outperform existing models with a comparable number of\nparameters, and obtain comparable performance to larger models while\nsignificantly reducing the number of parameters.", "published": "2025-02-02 01:05:09", "link": "http://arxiv.org/abs/2502.00617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction,\n  and Column Exploration", "abstract": "Text-to-SQL systems have unlocked easier access to critical data insights by\nenabling natural language queries over structured databases. However, deploying\nsuch systems in enterprise environments remains challenging due to factors such\nas large, complex schemas (> 3000 columns), diverse SQL dialects (e.g.,\nBigQuery, Snowflake) and sophisticated query requirements (e.g.,\ntransformation, analytics). Current state-of-the-art performance on the Spider\n2.0 dataset -- a benchmark built to mimic such complex environments -- remains\nlimited at 20%. Key limitations include inadequate instruction-following, poor\nlong-context comprehension, weak self-refinement, and insufficient\ndialect-specific knowledge. To address these gaps, we propose ReFoRCE\n(Self-Refinement Agent with Format Restriction and Column Exploration) which\nintroduces (1) table compression to mitigate long-context limitations (2)\nformat restriction to ensure accurate answer format, and (3) iterative column\nexploration for enhanced schema understanding. Additionally, it employs\nself-refinement pipeline consisting of (1) parallelized workflows with voting\nmechanisms and (2) a Common Table Expression (CTE) based refinement approach to\nhandle unresolved cases. ReFoRCE achieves state-of-the-art results scoring\n31.26 on the Spider 2.0-Snow and scoring 30.35 on the Spider 2.0-Lite tasks.", "published": "2025-02-02 05:25:03", "link": "http://arxiv.org/abs/2502.00675v2", "categories": ["cs.CL", "I.2.7; I.2.0; H.2.0"], "primary_category": "cs.CL"}
{"title": "Structural Latency Perturbation in Large Language Models Through\n  Recursive State Induction", "abstract": "Computational efficiency has remained a critical consideration in scaling\nhigh-capacity language models, with inference latency and resource consumption\npresenting significant constraints on real-time applications. The study has\nintroduced a structured latency perturbation mechanism that modifies\ncomputational pathways through recursive state induction, enabling dynamic\nsuppression of redundant activations while preserving generative fidelity. A\nformal mathematical framework has been established to describe recursive\nperturbations, ensuring that modifications remain adaptive rather than\nstatically imposed. Experiments have demonstrated that applying recursive state\nadjustments reduces inference latency across varying sequence lengths, with\nlonger text generations benefiting from cumulative efficiency improvements.\nComparative evaluations against structured pruning and quantization have\nindicated that latency gains can be achieved without compromising token\nretention or memory utilization. The analysis of computational overhead has\nsuggested that selectively suppressing redundant activations contributes to\nimproved power efficiency, particularly in scenarios requiring extended text\ngeneration. An assessment of linguistic stability has shown that token-level\nconsistency remains largely intact under controlled perturbation thresholds,\nreinforcing the viability of structural latency modifications as an alternative\nto weight-centric optimization techniques. The results have supported the\nhypothesis that recursive state induction offers an effective method for\nreducing computational complexity without requiring architectural modifications\nor external augmentation.", "published": "2025-02-02 11:45:35", "link": "http://arxiv.org/abs/2502.00758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FIRE: Flexible Integration of Data Quality Ratings for Effective\n  Pre-Training", "abstract": "Selecting high-quality data can significantly improve the pretraining\nefficiency of large language models (LLMs). Existing methods generally rely on\nheuristic techniques and single-quality signals, limiting their ability to\nevaluate data quality comprehensively. In this work, we propose FIRE, a\nflexible and scalable framework for integrating multiple data quality raters,\nwhich allows for a comprehensive assessment of data quality across various\ndimensions. FIRE aligns multiple quality signals into a unified space, and\nintegrates diverse data quality raters to provide a comprehensive quality\nsignal for each data point. Further, we introduce a progressive data selection\nscheme based on FIRE that iteratively refines the selection of high-quality\ndata points. Experiments on the SlimPajama dataset reveal that FIRE outperforms\nother data selection methods and significantly enhances the pretrained model\nacross a wide range of downstream tasks, with a 2.9% average performance\nimprovement over Random and reducing the FLOPs necessary to achieve a certain\nperformance level by more than half.", "published": "2025-02-02 11:52:26", "link": "http://arxiv.org/abs/2502.00761v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Large Language Models in Reasoning and Translating Complex\n  Linguistic Puzzles", "abstract": "This paper investigates the utilization of Large Language Models (LLMs) for\nsolving complex linguistic puzzles, a domain requiring advanced reasoning and\nadept translation capabilities akin to human cognitive processes. We explore\nspecific prompting techniques designed to enhance ability of LLMs to reason and\nelucidate their decision-making pathways, with a focus on Input-Output\nPrompting (IO), Chain-of-Thought Prompting (CoT), and Solo Performance\nPrompting (SPP). Utilizing datasets from the Puzzling Machine Competition and\nvarious Linguistics Olympiads, we employ a comprehensive set of metrics to\nassess the performance of GPT-4 0603, a prominent LLM, across these prompting\nmethods. Our findings illuminate the potential of LLMs in linguistic reasoning\nand complex translation tasks, highlighting their capabilities and identifying\nlimitations in the context of linguistic puzzles. This research contributes\nsignificantly to the broader field of Natural Language Processing (NLP) by\nproviding insights into the optimization of LLM applications for improved\nreasoning and translation accuracy, thereby enriching the ongoing dialogue in\nNLP advancements.", "published": "2025-02-02 14:53:14", "link": "http://arxiv.org/abs/2502.00817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weak Supervision Dynamic KL-Weighted Diffusion Models Guided by Large\n  Language Models", "abstract": "In this paper, we presents a novel method for improving text-to-image\ngeneration by combining Large Language Models (LLMs) with diffusion models, a\nhybrid approach aimed at achieving both higher quality and efficiency in image\nsynthesis from text descriptions. Our approach introduces a new dynamic\nKL-weighting strategy to optimize the diffusion process, along with\nincorporating semantic understanding from pre-trained LLMs to guide the\ngeneration process. The proposed method significantly improves both the visual\nquality and alignment of generated images with text descriptions, addressing\nchallenges such as computational inefficiency, instability in training, and\nrobustness to textual variability. We evaluate our method on the COCO dataset\nand demonstrate its superior performance over traditional GAN-based models,\nboth quantitatively and qualitatively. Extensive experiments, including\nablation studies and human evaluations, confirm that our method outperforms\nexisting approaches in terms of image realism, relevance to the input text, and\noverall aesthetic quality. Our approach also shows promise in scalability to\nother multimodal tasks, making it a versatile solution for a wide range of\ngenerative applications.", "published": "2025-02-02 15:43:13", "link": "http://arxiv.org/abs/2502.00826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalization of Medical Large Language Models through Cross-Domain\n  Weak Supervision", "abstract": "The advancement of large language models (LLMs) has opened new frontiers in\nnatural language processing, particularly in specialized domains like\nhealthcare. In this paper, we propose the Incremental Curriculum-Based\nFine-Tuning (ICFT) framework to enhance the generative capabilities of medical\nlarge language models (MLLMs). ICFT combines curriculum-based learning,\ndual-stage memory coordination, and parameter-efficient fine-tuning to enable a\nprogressive transition from general linguistic knowledge to strong\ndomain-specific expertise. Experimental results across diverse medical NLP\ntasks, including question answering, preference classification, and response\ngeneration, demonstrate that ICFT consistently outperforms state-of-the-art\nbaselines, achieving improvements in both accuracy and efficiency. Further\nanalysis reveals the framework's ability to generalize to unseen data, reduce\nerrors, and deliver diverse, contextually relevant medical responses. These\nfindings establish ICFT as a robust and scalable solution for adapting LLMs to\nthe medical domain, offering practical benefits for real-world healthcare\napplications.", "published": "2025-02-02 16:05:23", "link": "http://arxiv.org/abs/2502.00832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Accuracy, Robustness, and Readability of LLM-Generated\n  Sustainability-Related Word Definitions", "abstract": "A common language with standardized definitions is crucial for effective\nclimate discussions. However, concerns exist about LLMs misrepresenting climate\nterms. We compared 300 official IPCC glossary definitions with those generated\nby GPT-4o-mini, Llama3.1 8B, and Mistral 7B, analyzing adherence, robustness,\nand readability using SBERT sentence embeddings. The LLMs scored an average\nadherence of $0.57-0.59 \\pm 0.15$, and their definitions proved harder to read\nthan the originals. Model-generated definitions vary mainly among words with\nmultiple or ambiguous definitions, showing the potential to highlight terms\nthat need standardization. The results show how LLMs could support\nenvironmental discourse while emphasizing the need to align model outputs with\nestablished terminology for clarity and consistency.", "published": "2025-02-02 21:05:21", "link": "http://arxiv.org/abs/2502.00916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Abstraction: Harnessing Frontier Models to Structure\n  Real-World Data at Scale", "abstract": "The vast majority of real-world patient information resides in unstructured\nclinical text, and the process of medical abstraction seeks to extract and\nnormalize structured information from this unstructured input. However,\ntraditional medical abstraction methods can require significant manual efforts\nthat can include crafting rules or annotating training labels, limiting\nscalability. In this paper, we propose UniMedAbstractor (UMA), a zero-shot\nmedical abstraction framework leveraging Large Language Models (LLMs) through a\nmodular and customizable prompt template. We refer to our approach as universal\nabstraction as it can quickly scale to new attributes through its universal\nprompt template without curating attribute-specific training labels or rules.\nWe evaluate UMA for oncology applications, focusing on fifteen key attributes\nrepresenting the cancer patient journey, from short-context attributes (e.g.,\nperformance status, treatment) to complex long-context attributes requiring\nlongitudinal reasoning (e.g., tumor site, histology, TNM staging). Experiments\non real-world data show UMA's strong performance and generalizability. Compared\nto supervised and heuristic baselines, UMA with GPT-4o achieves on average an\nabsolute 2-point F1/accuracy improvement for both short-context and\nlong-context attribute abstraction. For pathologic T staging, UMA even\noutperforms the supervised model by 20 points in accuracy.", "published": "2025-02-02 22:31:54", "link": "http://arxiv.org/abs/2502.00943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Multi-Agent System Training with Data Influence-Oriented Tree\n  Search", "abstract": "Monte Carlo Tree Search (MCTS) based methods provide promising approaches for\ngenerating synthetic data to enhance the self-training of Large Language Model\n(LLM) based multi-agent systems (MAS). These methods leverage Q-values to\nestimate individual agent contributions. However, relying solely on Q-values to\nidentify informative data may misalign with the data synthesis objective, as\nthe focus should be on selecting data that best enhances model training. To\naddress this discrepancy, we propose Data Influence-oriented Tree Search\n(DITS), a novel framework that incorporates influence scores to guide both tree\nsearch and data selection. By leveraging influence scores, we effectively\nidentify the most impactful data for system improvement, thereby enhancing\nmodel performance. Furthermore, we derive influence score estimation methods\ntailored for non-differentiable metrics, significantly reducing computational\noverhead by utilizing inference computations. Extensive experiments on eight\nmulti-agent datasets demonstrate the robustness and effectiveness of the\nproposed methods. Notably, our findings reveal that allocating more inference\nresources to estimate influence scores, rather than Q-values, during data\nsynthesis can more effectively and efficiently enhance model training.", "published": "2025-02-02 23:20:16", "link": "http://arxiv.org/abs/2502.00955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The exception of humour: Iconicity, Phonemic Surprisal, Memory Recall,\n  and Emotional Associations", "abstract": "This meta-study explores the relationships between humor, phonemic bigram\nsurprisal, emotional valence, and memory recall. Prior research indicates that\nwords with higher phonemic surprisal are more readily remembered, suggesting\nthat unpredictable phoneme sequences promote long-term memory recall. Emotional\nvalence is another well-documented factor influencing memory, with negative\nexperiences and stimuli typically being remembered more easily than positive\nones. Building on existing findings, this study highlights that words with\nnegative associations often exhibit greater surprisal and are easier to recall.\nHumor, however, presents an exception: while associated with positive emotions,\nhumorous words also display heightened surprisal and enhanced memorability.", "published": "2025-02-02 05:31:46", "link": "http://arxiv.org/abs/2502.01682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing", "abstract": "Large language models (LLMs) have achieved remarkable performance on various\nnatural language tasks. However, they are trained on static corpora and their\nknowledge can become outdated quickly in the fast-changing world. This\nmotivates the development of knowledge editing (KE) to update specific\nknowledge in LLMs without changing unrelated others or compromising their\npre-trained capabilities. Previous efforts sought to update a small amount of\nparameters of a LLM and proved effective for making selective updates.\nNonetheless, the edited LLM often exhibits degraded ability to reason about the\nnew knowledge. In this work, we identify a key issue: heterogeneous token\noverfitting (HTO), where the LLM overfits different tokens in the provided\nknowledge at varying rates. To tackle this, we propose OVERTONE, a token-level\nsmoothing method that mitigates HTO by adaptively refining the target\ndistribution. Theoretically, OVERTONE offers better parameter updates with\nnegligible computation overhead. It also induces an implicit DPO but does not\nrequire preference data pairs. Extensive experiments across four editing\nmethods, two LLMs, and diverse scenarios demonstrate the effectiveness and\nversatility of our method.", "published": "2025-02-02 00:10:51", "link": "http://arxiv.org/abs/2502.00602v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SimulPL: Aligning Human Preferences in Simultaneous Machine Translation", "abstract": "Simultaneous Machine Translation (SiMT) generates translations while\nreceiving streaming source inputs. This requires the SiMT model to learn a\nread/write policy, deciding when to translate and when to wait for more source\ninput. Numerous linguistic studies indicate that audiences in SiMT scenarios\nhave distinct preferences, such as accurate translations, simpler syntax, and\nno unnecessary latency. Aligning SiMT models with these human preferences is\ncrucial to improve their performances. However, this issue still remains\nunexplored. Additionally, preference optimization for SiMT task is also\nchallenging. Existing methods focus solely on optimizing the generated\nresponses, ignoring human preferences related to latency and the optimization\nof read/write policy during the preference optimization phase. To address these\nchallenges, we propose Simultaneous Preference Learning (SimulPL), a preference\nlearning framework tailored for the SiMT task. In the SimulPL framework, we\ncategorize SiMT human preferences into five aspects: \\textbf{translation\nquality preference}, \\textbf{monotonicity preference}, \\textbf{key point\npreference}, \\textbf{simplicity preference}, and \\textbf{latency preference}.\nBy leveraging the first four preferences, we construct human preference prompts\nto efficiently guide GPT-4/4o in generating preference data for the SiMT task.\nIn the preference optimization phase, SimulPL integrates \\textbf{latency\npreference} into the optimization objective and enables SiMT models to improve\nthe read/write policy, thereby aligning with human preferences more\neffectively. Experimental results indicate that SimulPL exhibits better\nalignment with human preferences across all latency levels in\nZh$\\rightarrow$En, De$\\rightarrow$En and En$\\rightarrow$Zh SiMT tasks. Our data\nand code will be available at https://github.com/EurekaForNLP/SimulPL.", "published": "2025-02-02 02:47:09", "link": "http://arxiv.org/abs/2502.00634v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Small Language Models for News Summarization: Implications\n  and Factors Influencing Performance", "abstract": "The increasing demand for efficient summarization tools in\nresource-constrained environments highlights the need for effective solutions.\nWhile large language models (LLMs) deliver superior summarization quality,\ntheir high computational resource requirements limit practical use\napplications. In contrast, small language models (SLMs) present a more\naccessible alternative, capable of real-time summarization on edge devices.\nHowever, their summarization capabilities and comparative performance against\nLLMs remain underexplored. This paper addresses this gap by presenting a\ncomprehensive evaluation of 19 SLMs for news summarization across 2,000 news\nsamples, focusing on relevance, coherence, factual consistency, and summary\nlength. Our findings reveal significant variations in SLM performance, with\ntop-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results\ncomparable to those of 70B LLMs while generating more concise summaries.\nNotably, SLMs are better suited for simple prompts, as overly complex prompts\nmay lead to a decline in summary quality. Additionally, our analysis indicates\nthat instruction tuning does not consistently enhance the news summarization\ncapabilities of SLMs. This research not only contributes to the understanding\nof SLMs but also provides practical insights for researchers seeking efficient\nsummarization solutions that balance performance and resource use.", "published": "2025-02-02 03:07:45", "link": "http://arxiv.org/abs/2502.00641v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?", "abstract": "Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves $6.6\\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of $3.8\\%$ improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.", "published": "2025-02-02 05:23:29", "link": "http://arxiv.org/abs/2502.00674v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Universal Post-Processing Networks for Joint Optimization of Modules in\n  Task-Oriented Dialogue Systems", "abstract": "Post-processing networks (PPNs) are components that modify the outputs of\narbitrary modules in task-oriented dialogue systems and are optimized using\nreinforcement learning (RL) to improve the overall task completion capability\nof the system. However, previous PPN-based approaches have been limited to\nhandling only a subset of modules within a system, which poses a significant\nlimitation in improving the system performance. In this study, we propose a\njoint optimization method for post-processing the outputs of all modules using\nuniversal post-processing networks (UniPPNs), which are language-model-based\nnetworks that can modify the outputs of arbitrary modules in a system as a\nsequence-transformation task. Moreover, our RL algorithm, which employs a\nmodule-level Markov decision process, enables fine-grained value and advantage\nestimation for each module, thereby stabilizing joint learning for\npost-processing the outputs of all modules. Through both simulation-based and\nhuman evaluation experiments using the MultiWOZ dataset, we demonstrated that\nUniPPN outperforms conventional PPNs in the task completion capability of\ntask-oriented dialogue systems.", "published": "2025-02-02 10:46:37", "link": "http://arxiv.org/abs/2502.00747v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vision-centric Token Compression in Large Language Model", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nexcelling in handling longer sequences. However, the inefficiency and\nredundancy in processing extended in-context tokens remain a challenge. Many\nattempts to address this rely on compressing tokens with smaller text encoders,\nyet we question whether text encoders are truly indispensable. Our journey\nleads to an unexpected discovery-a much smaller vision encoder, applied\ndirectly to sequences of text tokens, can rival text encoders on text tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small text understanding benchmarks, VIST leads to comparable results with\n16% fewer FLOPs and 50% less memory usage. We further uncover significant token\nredundancy and devise a frequency-based masking strategy to guide the focus of\nthe visual encoder toward the most critical tokens. Interestingly, we observe\nthe trained visual encoder performs like a summarizer, selectively ignoring\nless important words such as prepositions and conjunctions. This approach\ndelivers remarkable results, outperforming traditional text encoder-based\nmethods by 5.7% on average over benchmarks like TriviaQA, NQ, PopQA, TREF,\nSST2, and SST5, setting a new standard for token efficiency in LLMs.", "published": "2025-02-02 13:10:06", "link": "http://arxiv.org/abs/2502.00791v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Disentangling Length Bias In Preference Learning Via\n  Response-Conditioned Modeling", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has achieved considerable\nsuccess in aligning large language models (LLMs) by modeling human preferences\nwith a learnable reward model and employing a reinforcement learning algorithm\nto maximize the reward model's scores. However, these reward models are\nsusceptible to exploitation through various superficial confounding factors,\nwith length bias emerging as a particularly significant concern. Moreover,\nwhile the pronounced impact of length bias on preference modeling suggests that\nLLMs possess an inherent sensitivity to length perception, our preliminary\ninvestigations reveal that fine-tuned LLMs consistently struggle to adhere to\nexplicit length instructions. To address these two limitations, we propose a\nnovel framework wherein the reward model explicitly differentiates between\nhuman semantic preferences and response length requirements. Specifically, we\nintroduce a Response-conditioned Bradley-Terry (Rc-BT) model that enhances the\nreward model's capability in length bias mitigating and length instruction\nfollowing, through training on our augmented dataset. Furthermore, we propose\nthe Rc-DPO algorithm to leverage the Rc-BT model for direct policy optimization\n(DPO) of LLMs, simultaneously mitigating length bias and promoting adherence to\nlength instructions. Extensive evaluations demonstrate that our approach\nsubstantially improves both preference modeling and length instruction\ncompliance, with its effectiveness validated across various foundational models\nand preference datasets.", "published": "2025-02-02 14:50:25", "link": "http://arxiv.org/abs/2502.00814v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Explainability in Practice: A Survey of Explainable NLP Across Various\n  Domains", "abstract": "Natural Language Processing (NLP) has become a cornerstone in many critical\nsectors, including healthcare, finance, and customer relationship management.\nThis is especially true with the development and use of advanced models such as\nGPT-based architectures and BERT, which are widely used in decision-making\nprocesses. However, the black-box nature of these advanced NLP models has\ncreated an urgent need for transparency and explainability. This review\nexplores explainable NLP (XNLP) with a focus on its practical deployment and\nreal-world applications, examining its implementation and the challenges faced\nin domain-specific contexts. The paper underscores the importance of\nexplainability in NLP and provides a comprehensive perspective on how XNLP can\nbe designed to meet the unique demands of various sectors, from healthcare's\nneed for clear insights to finance's emphasis on fraud detection and risk\nassessment. Additionally, this review aims to bridge the knowledge gap in XNLP\nliterature by offering a domain-specific exploration and discussing\nunderrepresented areas such as real-world applicability, metric evaluation, and\nthe role of human interaction in model assessment. The paper concludes by\nsuggesting future research directions that could enhance the understanding and\nbroader application of XNLP.", "published": "2025-02-02 16:18:44", "link": "http://arxiv.org/abs/2502.00837v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HintEval: A Comprehensive Framework for Hint Generation and Evaluation\n  for Questions", "abstract": "Large Language Models (LLMs) are transforming how people find information,\nand many users turn nowadays to chatbots to obtain answers to their questions.\nDespite the instant access to abundant information that LLMs offer, it is still\nimportant to promote critical thinking and problem-solving skills. Automatic\nhint generation is a new task that aims to support humans in answering\nquestions by themselves by creating hints that guide users toward answers\nwithout directly revealing them. In this context, hint evaluation focuses on\nmeasuring the quality of hints, helping to improve the hint generation\napproaches. However, resources for hint research are currently spanning\ndifferent formats and datasets, while the evaluation tools are missing or\nincompatible, making it hard for researchers to compare and test their models.\nTo overcome these challenges, we introduce HintEval, a Python library that\nmakes it easy to access diverse datasets and provides multiple approaches to\ngenerate and evaluate hints. HintEval aggregates the scattered resources into a\nsingle toolkit that supports a range of research goals and enables a clear,\nmulti-faceted, and reliable evaluation. The proposed library also includes\ndetailed online documentation, helping users quickly explore its features and\nget started. By reducing barriers to entry and encouraging consistent\nevaluation practices, HintEval offers a major step forward for facilitating\nhint generation and analysis research within the NLP/IR community.", "published": "2025-02-02 17:07:18", "link": "http://arxiv.org/abs/2502.00857v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SimPER: A Minimalist Approach to Preference Alignment without\n  Hyperparameters", "abstract": "Existing preference optimization objectives for language model alignment\nrequire additional hyperparameters that must be extensively tuned to achieve\noptimal performance, increasing both the complexity and time required for\nfine-tuning large language models. In this paper, we propose a simple yet\neffective hyperparameter-free preference optimization algorithm for alignment.\nWe observe that promising performance can be achieved simply by optimizing\ninverse perplexity, which is calculated as the inverse of the exponentiated\naverage log-likelihood of the chosen and rejected responses in the preference\ndataset. The resulting simple learning objective, SimPER, is easy to implement\nand eliminates the need for expensive hyperparameter tuning and a reference\nmodel, making it both computationally and memory efficient. Extensive\nexperiments on widely used real-world benchmarks, including MT-Bench,\nAlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base\nmodels, demonstrate that SimPER consistently and significantly outperforms\nexisting approaches-even without any hyperparameters or a reference model . For\nexample, despite its simplicity, SimPER outperforms state-of-the-art methods by\nup to 5.7 points on AlpacaEval 2 and achieves the highest average ranking\nacross 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is\npublicly available at: https://github.com/tengxiao1/SimPER.", "published": "2025-02-02 19:25:41", "link": "http://arxiv.org/abs/2502.00883v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MorphBPE: A Morpho-Aware Tokenizer Bridging Linguistic Complexity for\n  Efficient LLM Training Across Morphologies", "abstract": "Tokenization is fundamental to Natural Language Processing (NLP), directly\nimpacting model efficiency and linguistic fidelity. While Byte Pair Encoding\n(BPE) is widely used in Large Language Models (LLMs), it often disregards\nmorpheme boundaries, leading to suboptimal segmentation, particularly in\nmorphologically rich languages. We introduce MorphBPE, a morphology-aware\nextension of BPE that integrates linguistic structure into subword tokenization\nwhile preserving statistical efficiency. Additionally, we propose two\nmorphology-based evaluation metrics: (i) Morphological Consistency F1-Score,\nwhich quantifies the consistency between morpheme sharing and token sharing,\ncontributing to LLM training convergence, and (ii) Morphological Edit Distance,\nwhich measures alignment between morphemes and tokens concerning\ninterpretability. Experiments on English, Russian, Hungarian, and Arabic across\n300M and 1B parameter LLMs demonstrate that MorphBPE consistently reduces\ncross-entropy loss, accelerates convergence, and improves morphological\nalignment scores. Fully compatible with existing LLM pipelines, MorphBPE\nrequires minimal modifications for integration. The MorphBPE codebase and\ntokenizer playground will be available at:\nhttps://github.com/llm-lab-org/MorphBPE and https://tokenizer.llm-lab.org", "published": "2025-02-02 20:06:39", "link": "http://arxiv.org/abs/2502.00894v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Powered Benchmark Factory: Reliable, Generic, and Efficient", "abstract": "The rapid advancement of large language models (LLMs) has led to a surge in\nboth model supply and application demands. To facilitate effective matching\nbetween them, reliable, generic and efficient benchmark generators are widely\nneeded. However, human annotators are constrained by inefficiency, and current\nLLM benchmark generators not only lack generalizability but also struggle with\nlimited reliability, as they lack a comprehensive evaluation framework for\nvalidation and optimization. To fill this gap, we first propose an automated\nand unbiased evaluation framework, structured around four dimensions and ten\ncriteria. Under this framework, we carefully analyze the advantages and\nweaknesses of directly prompting LLMs as generic benchmark generators. To\nenhance the reliability, we introduce a series of methods to address the\nidentified weaknesses and integrate them as BenchMaker. Experiments across\nmultiple LLMs and tasks confirm that BenchMaker achieves superior or comparable\nperformance to human-annotated benchmarks on all metrics, highlighting its\ngeneralizability and reliability. More importantly, it delivers highly\nconsistent evaluation results across 12 LLMs (0.967 Pearson correlation against\nMMLU-Pro), while taking only $0.005 and 0.38 minutes per sample.", "published": "2025-02-02 06:36:01", "link": "http://arxiv.org/abs/2502.01683v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Agent-Based Uncertainty Awareness Improves Automated Radiology Report\n  Labeling with an Open-Source Large Language Model", "abstract": "Reliable extraction of structured data from radiology reports using Large\nLanguage Models (LLMs) remains challenging, especially for complex, non-English\ntexts like Hebrew. This study introduces an agent-based uncertainty-aware\napproach to improve the trustworthiness of LLM predictions in medical\napplications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease\npatients (from 2010 to 2023) across three medical centers. A subset of 512\nreports was manually annotated for six gastrointestinal organs and 15\npathological findings, while the remaining reports were automatically annotated\nusing HSMP-BERT. Structured data extraction was performed using Llama 3.1\n(Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed\nsix semantically equivalent prompts to estimate uncertainty. An Agent-Based\nDecision Model integrated multiple prompt outputs into five confidence levels\nfor calibrated uncertainty and was compared against three entropy-based models.\nPerformance was evaluated using accuracy, F1 score, precision, recall, and\nCohen's Kappa before and after filtering high-uncertainty cases. The\nagent-based model outperformed the baseline across all metrics, achieving an F1\nscore of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering\nhigh-uncertainty cases (greater than or equal to 0.5), the F1 score improved to\n0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated\nclear separation between correct and incorrect predictions, with the\nagent-based model providing the most well-calibrated uncertainty estimates. By\nincorporating uncertainty-aware prompt ensembles and an agent-based decision\nmodel, this approach enhances the performance and reliability of LLMs in\nstructured data extraction from radiology reports, offering a more\ninterpretable and trustworthy solution for high-stakes medical applications.", "published": "2025-02-02 16:57:03", "link": "http://arxiv.org/abs/2502.01691v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reformulation is All You Need: Addressing Malicious Text Features in\n  DNNs", "abstract": "Human language encompasses a wide range of intricate and diverse implicit\nfeatures, which attackers can exploit to launch adversarial or backdoor\nattacks, compromising DNN models for NLP tasks. Existing model-oriented\ndefenses often require substantial computational resources as model size\nincreases, whereas sample-oriented defenses typically focus on specific attack\nvectors or schemes, rendering them vulnerable to adaptive attacks. We observe\nthat the root cause of both adversarial and backdoor attacks lies in the\nencoding process of DNN models, where subtle textual features, negligible for\nhuman comprehension, are erroneously assigned significant weight by less robust\nor trojaned models. Based on it we propose a unified and adaptive defense\nframework that is effective against both adversarial and backdoor attacks. Our\napproach leverages reformulation modules to address potential malicious\nfeatures in textual inputs while preserving the original semantic integrity.\nExtensive experiments demonstrate that our framework outperforms existing\nsample-oriented defense baselines across a diverse range of malicious textual\nfeatures.", "published": "2025-02-02 03:39:43", "link": "http://arxiv.org/abs/2502.00652v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with\n  Multi-modal Prototypes and Image Bias Estimation", "abstract": "Existing vision-language model (VLM)-based methods for out-of-distribution\n(OOD) detection typically rely on similarity scores between input images and\nin-distribution (ID) text prototypes. However, the modality gap between image\nand text often results in high false positive rates, as OOD samples can exhibit\nhigh similarity to ID text prototypes. To mitigate the impact of this modality\ngap, we propose incorporating ID image prototypes along with ID text\nprototypes. We present theoretical analysis and empirical evidence indicating\nthat this approach enhances VLM-based OOD detection performance without any\nadditional training. To further reduce the gap between image and text, we\nintroduce a novel few-shot tuning framework, SUPREME, comprising biased prompts\ngeneration (BPG) and image-text consistency (ITC) modules. BPG enhances\nimage-text fusion and improves generalization by conditioning ID text\nprototypes on the Gaussian-based estimated image domain bias; ITC reduces the\nmodality gap by minimizing intra- and inter-modal distances. Moreover, inspired\nby our theoretical and empirical findings, we introduce a novel OOD score\n$S_{\\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we\npresent extensive experiments to demonstrate that SUPREME consistently\noutperforms existing VLM-based OOD detection methods.", "published": "2025-02-02 04:30:51", "link": "http://arxiv.org/abs/2502.00662v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large\n  Language Models with Kernel Divergence", "abstract": "Dataset contamination, where evaluation datasets overlap with pre-training\ncorpora, inflates performance metrics and undermines the reliability of model\nevaluations. Quantifying dataset contamination thus becomes essential to ensure\nthat performance evaluations genuinely reflect a model's ability to generalize\nto unseen data, rather than relying on memorized examples. To address this\nproblem, we propose Kernel Divergence Score (KDS), a novel method that\nquantifies dataset contamination by computing the divergence between the kernel\nsimilarity matrix of sample embeddings, before and after fine-tuning on the\nbenchmark dataset. Leveraging the insight that fine-tuning affects unseen\nsamples more significantly than seen ones, KDS provides a reliable measure of\ncontamination. Through extensive experiments on controlled contamination\nscenarios, KDS demonstrates a near-perfect correlation with contamination\nlevels and outperforms existing baselines. Additionally, we perform\ncomprehensive ablation studies to analyze the impact of key design choices,\nproviding deeper insights into the components and effectiveness of KDS. These\nablations highlight the importance of leveraging fine-grained kernel-based\ninformation and confirm the reliability of the proposed framework across\ndiverse datasets and settings.", "published": "2025-02-02 05:50:39", "link": "http://arxiv.org/abs/2502.00678v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Survey of Quantized Graph Representation Learning: Connecting Graph\n  Structures with Large Language Models", "abstract": "Recent years have witnessed rapid advances in graph representation learning,\nwith the continuous embedding approach emerging as the dominant paradigm.\nHowever, such methods encounter issues regarding parameter efficiency,\ninterpretability, and robustness. Thus, Quantized Graph Representation (QGR)\nlearning has recently gained increasing interest, which represents the graph\nstructure with discrete codes instead of conventional continuous embeddings.\nGiven its analogous representation form to natural language, QGR also possesses\nthe capability to seamlessly integrate graph structures with large language\nmodels (LLMs). As this emerging paradigm is still in its infancy yet holds\nsignificant promise, we undertake this thorough survey to promote its rapid\nfuture prosperity. We first present the background of the general quantization\nmethods and their merits. Moreover, we provide an in-depth demonstration of\ncurrent QGR studies from the perspectives of quantized strategies, training\nobjectives, distinctive designs, knowledge graph quantization, and\napplications. We further explore the strategies for code dependence learning\nand integration with LLMs. At last, we give discussions and conclude future\ndirections, aiming to provide a comprehensive picture of QGR and inspire future\nresearch.", "published": "2025-02-02 05:57:34", "link": "http://arxiv.org/abs/2502.00681v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning Autonomous Code Integration for Math Language Models", "abstract": "Recent advances in mathematical problem-solving with language models (LMs)\nintegrate chain-of-thought (CoT) reasoning and code execution to harness their\ncomplementary strengths. However, existing hybrid frameworks exhibit a critical\nlimitation: they depend on externally dictated instructions or rigid\ncode-integration templates, lacking metacognitive awareness -- the capacity to\ndynamically evaluate intrinsic capabilities and autonomously determine when and\nhow to integrate tools. This rigidity motivates our study of autonomous code\nintegration, enabling models to adapt tool-usage strategies as their reasoning\nabilities evolve during training.\n  While reinforcement learning (RL) shows promise for boosting LLM reasoning at\nscale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning\nautonomous code integration due to inadequate exploration of the vast\ncombinatorial space of CoT-code interleaving patterns. To address this\nchallenge, we propose a novel Expectation-Maximization (EM) framework that\nsynergizes structured exploration (E-step) with off-policy RL optimization\n(M-step), creating a self-reinforcing cycle between metacognitive tool-use\ndecisions and evolving capabilities. Experiments reveal our method achieves\nsuperior results through improved exploration. Notably, our 7B model improves\nover 11% on MATH500 and 9.4% on AIME without o1-like CoT.", "published": "2025-02-02 06:32:23", "link": "http://arxiv.org/abs/2502.00691v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Model Provenance Testing for Large Language Models", "abstract": "Large language models are increasingly customized through fine-tuning and\nother adaptations, creating challenges in enforcing licensing terms and\nmanaging downstream impacts. Tracking model origins is crucial both for\nprotecting intellectual property and for identifying derived models when biases\nor vulnerabilities are discovered in foundation models. We address this\nchallenge by developing a framework for testing model provenance: Whether one\nmodel is derived from another. Our approach is based on the key observation\nthat real-world model derivations preserve significant similarities in model\noutputs that can be detected through statistical analysis. Using only black-box\naccess to models, we employ multiple hypothesis testing to compare model\nsimilarities against a baseline established by unrelated models. On two\ncomprehensive real-world benchmarks spanning models from 30M to 4B parameters\nand comprising over 600 models, our tester achieves 90-95% precision and 80-90%\nrecall in identifying derived models. These results demonstrate the viability\nof systematic provenance verification in production environments even when only\nAPI access is available.", "published": "2025-02-02 07:39:37", "link": "http://arxiv.org/abs/2502.00706v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit\n  Classifiers as Experts", "abstract": "Early Exit (EE) techniques have emerged as a means to reduce inference\nlatency in Deep Neural Networks (DNNs). The latency improvement and accuracy in\nthese techniques crucially depend on the criteria used to make exit decisions.\nWe propose a new decision criterion where exit classifiers are treated as\nexperts BEEM and aggregate their confidence scores. The confidence scores are\naggregated only if neighbouring experts are consistent in prediction as the\nsamples pass through them, thus capturing their ensemble effect. A sample exits\nwhen the aggregated confidence value exceeds a threshold. The threshold is set\nusing the error rates of the intermediate exits aiming to surpass the\nperformance of conventional DNN inference. Experimental results on the COCO\ndataset for Image captioning and GLUE datasets for various language tasks\ndemonstrate that our method enhances the performance of state-of-the-art EE\nmethods, achieving improvements in speed-up by a factor 1.5x to 2.1x. When\ncompared to the final layer, its accuracy is comparable in harder Image\nCaptioning and improves in the easier language tasks. The source code for this\nwork is publicly available at https://github.com/Div290/BEEM1/tree/main", "published": "2025-02-02 10:35:19", "link": "http://arxiv.org/abs/2502.00745v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Zero-Shot Warning Generation for Misinformative Multimodal Content", "abstract": "The widespread prevalence of misinformation poses significant societal\nconcerns. Out-of-context misinformation, where authentic images are paired with\nfalse text, is particularly deceptive and easily misleads audiences. Most\nexisting detection methods primarily evaluate image-text consistency but often\nlack sufficient explanations, which are essential for effectively debunking\nmisinformation. We present a model that detects multimodal misinformation\nthrough cross-modality consistency checks, requiring minimal training time.\nAdditionally, we propose a lightweight model that achieves competitive\nperformance using only one-third of the parameters. We also introduce a\ndual-purpose zero-shot learning task for generating contextualized warnings,\nenabling automated debunking and enhancing user comprehension. Qualitative and\nhuman evaluations of the generated warnings highlight both the potential and\nlimitations of our approach.", "published": "2025-02-02 11:18:05", "link": "http://arxiv.org/abs/2502.00752v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Predicting potentially unfair clauses in Chilean terms of services with\n  natural language processing", "abstract": "This study addresses the growing concern of information asymmetry in consumer\ncontracts, exacerbated by the proliferation of online services with complex\nTerms of Service that are rarely even read. Even though research on automatic\nanalysis methods is conducted, the problem is aggravated by the general focus\non English-language Machine Learning approaches and on major jurisdictions,\nsuch as the European Union. We introduce a new methodology and a substantial\ndataset addressing this gap. We propose a novel annotation scheme with four\ncategories and a total of 20 classes, and apply it on 50 online Terms of\nService used in Chile. Our evaluation of transformer-based models highlights\nhow factors like language- and/or domain-specific pre-training, few-shot sample\nsize, and model architecture affect the detection and classification of\npotentially abusive clauses. Results show a large variability in performance\nfor the different tasks and models, with the highest macro-F1 scores for the\ndetection task ranging from 79% to 89% and micro-F1 scores up to 96%, while\nmacro-F1 scores for the classification task range from 60% to 70% and micro-F1\nscores from 64% to 80%. Notably, this is the first Spanish-language multi-label\nclassification dataset for legal clauses, applying Chilean law and offering a\ncomprehensive evaluation of Spanish-language models in the legal domain. Our\nwork lays the ground for future research in method development for rarely\nconsidered legal analysis and potentially leads to practical applications to\nsupport consumers in Chile and Latin America as a whole.", "published": "2025-02-02 18:01:39", "link": "http://arxiv.org/abs/2502.00865v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models Use Trigonometry to Do Addition", "abstract": "Mathematical reasoning is an increasingly important indicator of large\nlanguage model (LLM) capabilities, yet we lack understanding of how LLMs\nprocess even simple mathematical tasks. To address this, we reverse engineer\nhow three mid-sized LLMs compute addition. We first discover that numbers are\nrepresented in these LLMs as a generalized helix, which is strongly causally\nimplicated for the tasks of addition and subtraction, and is also causally\nrelevant for integer division, multiplication, and modular arithmetic. We then\npropose that LLMs compute addition by manipulating this generalized helix using\nthe \"Clock\" algorithm: to solve $a+b$, the helices for $a$ and $b$ are\nmanipulated to produce the $a+b$ answer helix which is then read out to model\nlogits. We model influential MLP outputs, attention head outputs, and even\nindividual neuron preactivations with these helices and verify our\nunderstanding with causal interventions. By demonstrating that LLMs represent\nnumbers on a helix and manipulate this helix to perform addition, we present\nthe first representation-level explanation of an LLM's mathematical capability.", "published": "2025-02-02 18:55:26", "link": "http://arxiv.org/abs/2502.00873v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Embracing Dialectic Intersubjectivity: Coordination of Different\n  Perspectives in Content Analysis with LLM Persona Simulation", "abstract": "This study attempts to advancing content analysis methodology from\nconsensus-oriented to coordination-oriented practices, thereby embracing\ndiverse coding outputs and exploring the dynamics among differential\nperspectives. As an exploratory investigation of this approach, we evaluate six\nGPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on\nBiden and Trump during the 2020 U.S. presidential campaign, examining patterns\nacross these models. By assessing each model's alignment with ideological\nperspectives, we explore how partisan selective processing could be identified\nin LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona\nLLMs exhibit stronger ideological biases when processing politically congruent\ncontent. Additionally, intercoder reliability is higher among same-partisan\npersonas compared to cross-partisan pairs. This approach enhances the nuanced\nunderstanding of LLM outputs and advances the integrity of AI-driven social\nscience research, enabling simulations of real-world implications.", "published": "2025-02-02 20:29:10", "link": "http://arxiv.org/abs/2502.00903v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Attention Sinks and Outlier Features: A 'Catch, Tag, and Release'\n  Mechanism for Embeddings", "abstract": "Two prominent features of large language models (LLMs) is the presence of\nlarge-norm (outlier) features and the tendency for tokens to attend very\nstrongly to a select few tokens. Despite often having no semantic relevance,\nthese select tokens, called attention sinks, along with the large outlier\nfeatures, have proven important for model performance, compression, and\nstreaming. Consequently, investigating the roles of these phenomena within\nmodels and exploring how they might manifest in the model parameters has become\nan area of active interest. Through an empirical investigation, we demonstrate\nthat attention sinks utilize outlier features to: catch a sequence of tokens,\ntag the captured tokens by applying a common perturbation, and then release the\ntokens back into the residual stream, where the tagged tokens are eventually\nretrieved. We prove that simple tasks, like averaging, necessitate the 'catch,\ntag, release' mechanism hence explaining why it would arise organically in\nmodern LLMs. Our experiments also show that the creation of attention sinks can\nbe completely captured in the model parameters using low-rank matrices, which\nhas important implications for model compression and substantiates the success\nof recent approaches that incorporate a low-rank term to offset performance\ndegradation.", "published": "2025-02-02 21:15:07", "link": "http://arxiv.org/abs/2502.00919v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LIBRA: Measuring Bias of Large Language Model from a Local Context", "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing applications, yet their widespread use raises concerns regarding\ninherent biases that may reduce utility or harm for particular social groups.\nDespite the advancement in addressing LLM bias, existing research has two major\nlimitations. First, existing LLM bias evaluation focuses on the U.S. cultural\ncontext, making it challenging to reveal stereotypical biases of LLMs toward\nother cultures, leading to unfair development and use of LLMs. Second, current\nbias evaluation often assumes models are familiar with the target social\ngroups. When LLMs encounter words beyond their knowledge boundaries that are\nunfamiliar in their training data, they produce irrelevant results in the local\ncontext due to hallucinations and overconfidence, which are not necessarily\nindicative of inherent bias. This research addresses these limitations with a\nLocal Integrated Bias Recognition and Assessment Framework (LIBRA) for\nmeasuring bias using datasets sourced from local corpora without crowdsourcing.\nImplementing this framework, we develop a dataset comprising over 360,000 test\ncases in the New Zealand context. Furthermore, we propose the Enhanced\nIdealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge\nboundary score (bbs) and a distribution divergence-based bias measurement to\ntackle the challenge of LLMs encountering words beyond knowledge boundaries.\nOur results show that the BERT family, GPT-2, and Llama-3 models seldom\nunderstand local words in different contexts. While Llama-3 exhibits larger\nbias, it responds better to different cultural contexts. The code and dataset\nare available at: https://github.com/ipangbo/LIBRA.", "published": "2025-02-02 04:24:57", "link": "http://arxiv.org/abs/2502.01679v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "abstract": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.", "published": "2025-02-02 05:14:22", "link": "http://arxiv.org/abs/2502.05206v3", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.CR"}
{"title": "\"Would You Want an AI Tutor?\" Understanding Stakeholder Perceptions of\n  LLM-based Chatbots in the Classroom", "abstract": "In recent years, Large Language Models (LLMs) rapidly gained popularity\nacross all parts of society, including education. After initial skepticism and\nbans, many schools have chosen to embrace this new technology by integrating it\ninto their curricula in the form of virtual tutors and teaching assistants.\nHowever, neither the companies developing this technology nor the public\ninstitutions involved in its implementation have set up a formal system to\ncollect feedback from the stakeholders impacted by them. In this paper, we\nargue that understanding the perceptions of those directly affected by LLMS in\nthe classroom, such as students and teachers, as well as those indirectly\nimpacted, like parents and school staff, is essential for ensuring responsible\nuse of AI in this critical domain. Our contributions are two-fold. First, we\npresent results of a literature review focusing on the perceptions of LLM-based\nchatbots in education. We highlight important gaps in the literature, such as\nthe exclusion of key educational agents (e.g., parents or school\nadministrators) when analyzing the role of stakeholders, and the frequent\nomission of the learning contexts in which the AI systems are implemented.\nThus, we present a taxonomy that organizes existing literature on stakeholder\nperceptions. Second, we propose the Contextualized Perceptions for the Adoption\nof Chatbots in Education (Co-PACE) framework, which can be used to\nsystematically elicit perceptions and inform whether and how LLM-based chatbots\nshould be designed, developed, and deployed in the classroom.", "published": "2025-02-02 16:50:08", "link": "http://arxiv.org/abs/2503.02885v1", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "What can large language models do for sustainable food?", "abstract": "Food systems are responsible for a third of human-caused greenhouse gas\nemissions. We investigate what Large Language Models (LLMs) can contribute to\nreducing the environmental impacts of food production. We define a typology of\ndesign and prediction tasks based on the sustainable food literature and\ncollaboration with domain experts, and evaluate six LLMs on four tasks in our\ntypology. For example, for a sustainable protein design task, food science\nexperts estimated that collaboration with an LLM can reduce time spent by 45%\non average, compared to 22% for collaboration with another expert human food\nscientist. However, for a sustainable menu design task, LLMs produce suboptimal\nsolutions when instructed to consider both human satisfaction and climate\nimpacts. We propose a general framework for integrating LLMs with combinatorial\noptimization to improve reasoning capabilities. Our approach decreases\nemissions of food choices by 79% in a hypothetical restaurant while maintaining\nparticipants' satisfaction with their set of choices. Our results demonstrate\nLLMs' potential, supported by optimization techniques, to accelerate\nsustainable food development and adoption.", "published": "2025-02-02 18:12:16", "link": "http://arxiv.org/abs/2503.04734v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive\n  Impairment", "abstract": "Existing methods for analyzing linguistic content from picture descriptions\nfor assessment of cognitive-linguistic impairment often overlook the\nparticipant's visual narrative path, which typically requires eye tracking to\nassess. Spatio-semantic graphs are a useful tool for analyzing this narrative\npath from transcripts alone, however they are limited by the need for manual\ntagging of content information units (CIUs). In this paper, we propose an\nautomated approach for estimation of spatio-semantic graphs (via automated\nextraction of CIUs) from the Cookie Theft picture commonly used in\ncognitive-linguistic analyses. The method enables the automatic\ncharacterization of the visual semantic path during picture description.\nExperiments demonstrate that the automatic spatio-semantic graphs effectively\ndifferentiate between cognitively impaired and unimpaired speakers. Statistical\nanalyses reveal that the features derived by the automated method produce\ncomparable results to the manual method, with even greater group differences\nbetween clinical groups of interest. These results highlight the potential of\nthe automated approach for extracting spatio-semantic features in developing\nclinical speech models for cognitive impairment assessment.", "published": "2025-02-02 10:25:19", "link": "http://arxiv.org/abs/2502.01685v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks\n  in Audio-Language Models", "abstract": "The rise of multimodal large language models has introduced innovative\nhuman-machine interaction paradigms but also significant challenges in machine\nlearning safety. Audio-Language Models (ALMs) are especially relevant due to\nthe intuitive nature of spoken communication, yet little is known about their\nfailure modes. This paper explores audio jailbreaks targeting ALMs, focusing on\ntheir ability to bypass alignment mechanisms. We construct adversarial\nperturbations that generalize across prompts, tasks, and even base audio\nsamples, demonstrating the first universal jailbreaks in the audio modality,\nand show that these remain effective in simulated real-world conditions. Beyond\ndemonstrating attack feasibility, we analyze how ALMs interpret these audio\nadversarial examples and reveal them to encode imperceptible first-person toxic\nspeech - suggesting that the most effective perturbations for eliciting toxic\noutputs specifically embed linguistic features within the audio signal. These\nresults have important implications for understanding the interactions between\ndifferent modalities in multimodal models, and offer actionable insights for\nenhancing defenses against adversarial audio attacks.", "published": "2025-02-02 08:36:23", "link": "http://arxiv.org/abs/2502.00718v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "CycleGuardian: A Framework for Automatic RespiratorySound classification\n  Based on Improved Deep clustering and Contrastive Learning", "abstract": "Auscultation plays a pivotal role in early respiratory and pulmonary disease\ndiagnosis. Despite the emergence of deep learning-based methods for automatic\nrespiratory sound classification post-Covid-19, limited datasets impede\nperformance enhancement. Distinguishing between normal and abnormal respiratory\nsounds poses challenges due to the coexistence of normal respiratory components\nand noise components in both types. Moreover, different abnormal respiratory\nsounds exhibit similar anomalous features, hindering their differentiation.\nBesides, existing state-of-the-art models suffer from excessive parameter size,\nimpeding deployment on resource-constrained mobile platforms. To address these\nissues, we design a lightweight network CycleGuardian and propose a framework\nbased on an improved deep clustering and contrastive learning. We first\ngenerate a hybrid spectrogram for feature diversity and grouping spectrograms\nto facilitating intermittent abnormal sound capture.Then, CycleGuardian\nintegrates a deep clustering module with a similarity-constrained clustering\ncomponent to improve the ability to capture abnormal features and a contrastive\nlearning module with group mixing for enhanced abnormal feature discernment.\nMulti-objective optimization enhances overall performance during training. In\nexperiments we use the ICBHI2017 dataset, following the official split method\nand without any pre-trained weights, our method achieves Sp: 82.06 $\\%$, Se:\n44.47$\\%$, and Score: 63.26$\\%$ with a network model size of 38M, comparing to\nthe current model, our method leads by nearly 7$\\%$, achieving the current best\nperformances. Additionally, we deploy the network on Android devices,\nshowcasing a comprehensive intelligent respiratory sound auscultation system.", "published": "2025-02-02 09:56:47", "link": "http://arxiv.org/abs/2502.00734v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "neuro2voc: Decoding Vocalizations from Neural Activity", "abstract": "Accurate decoding of neural spike trains and relating them to motor output is\na challenging task due to the inherent sparsity and length in neural spikes and\nthe complexity of brain circuits. This master project investigates experimental\nmethods for decoding zebra finch motor outputs (in both discrete syllables and\ncontinuous spectrograms), from invasive neural recordings obtained from\nNeuropixels.\n  There are three major achievements: (1) XGBoost with SHAP analysis trained on\nspike rates revealed neuronal interaction patterns crucial for syllable\nclassification. (2) Novel method (tokenizing neural data with GPT2) and\narchitecture (Mamba2) demonstrated potential for decoding of syllables using\nspikes. (3) A combined contrastive learning-VAE framework successfully\ngenerated spectrograms from binned neural data.\n  This work establishes a promising foundation for neural decoding of complex\nmotor outputs and offers several novel methodological approaches for processing\nsparse neural data.", "published": "2025-02-02 11:09:31", "link": "http://arxiv.org/abs/2502.07800v1", "categories": ["q-bio.NC", "cs.LG", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "CardioLive: Empowering Video Streaming with Online Cardiac Monitoring", "abstract": "Online Cardiac Monitoring (OCM) emerges as a compelling enhancement for the\nnext-generation video streaming platforms. It enables various applications\nincluding remote health, online affective computing, and deepfake detection.\nYet the physiological information encapsulated in the video streams has been\nlong neglected. In this paper, we present the design and implementation of\nCardioLive, the first online cardiac monitoring system in video streaming\nplatforms. We leverage the naturally co-existed video and audio streams and\ndevise CardioNet, the first audio-visual network to learn the cardiac series.\nIt incorporates multiple unique designs to extract temporal and spectral\nfeatures, ensuring robust performance under realistic video streaming\nconditions. To enable the Service-On-Demand online cardiac monitoring, we\nimplement CardioLive as a plug-and-play middleware service and develop\nsystematic solutions to practical issues including changing FPS and\nunsynchronized streams. Extensive experiments have been done to demonstrate the\neffectiveness of our system. We achieve a Mean Square Error (MAE) of 1.79 BPM\nerror, outperforming the video-only and audio-only solutions by 69.2% and\n81.2%, respectively. Our CardioLive service achieves average throughputs of\n115.97 and 98.16 FPS when implemented in Zoom and YouTube. We believe our work\nopens up new applications for video stream systems. We will release the code\nsoon.", "published": "2025-02-02 07:26:05", "link": "http://arxiv.org/abs/2502.00702v1", "categories": ["cs.HC", "cs.NI", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.HC"}
