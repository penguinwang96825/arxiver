{"title": "Idioms-Proverbs Lexicon for Modern Standard Arabic and Colloquial\n  Sentiment Analysis", "abstract": "Although, the fair amount of works in sentiment analysis (SA) and opinion\nmining (OM) systems in the last decade and with respect to the performance of\nthese systems, but it still not desired performance, especially for\nmorphologically-Rich Language (MRL) such as Arabic, due to the complexities and\nchallenges exist in the nature of the languages itself. One of these challenges\nis the detection of idioms or proverbs phrases within the writer text or\ncomment. An idiom or proverb is a form of speech or an expression that is\npeculiar to itself. Grammatically, it cannot be understood from the individual\nmeanings of its elements and can yield different sentiment when treats as\nseparate words. Consequently, In order to facilitate the task of detection and\nclassification of lexical phrases for automated SA systems, this paper presents\nAIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic\n(MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentence\nlevel with semantic orientation (positive or negative). The efforts of manually\nbuilding and annotating the lexicon are reported. Moreover, we build a\nclassifier that extracts idioms and proverbs, phrases from text using n-gram\nand similarity measure methods. Finally, several experiments were carried out\non various data, including Arabic tweets and Arabic microblogs (hotel\nreservation, product reviews, and TV program comments) from publicly available\nArabic online reviews websites (social media, blogs, forums, e-commerce web\nsites) to evaluate the coverage and accuracy of AIPSeLEX.", "published": "2015-06-05 13:29:33", "link": "http://arxiv.org/abs/1506.01906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Content Translation: Computer-assisted translation tool for Wikipedia\n  articles", "abstract": "The quality and quantity of articles in each Wikipedia language varies\ngreatly. Translating from another Wikipedia is a natural way to add more\ncontent, but the translation process is not properly supported in the software\nused by Wikipedia. Past computer-assisted translation tools built for Wikipedia\nare not commonly used. We created a tool that adapts to the specific needs of\nan open community and to the kind of content in Wikipedia. Qualitative and\nquantitative data indicates that the new tool helps users translate articles\neasier and faster.", "published": "2015-06-05 13:48:56", "link": "http://arxiv.org/abs/1506.01914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Overcomplete Word Vector Representations", "abstract": "Current distributed representations of words show little resemblance to\ntheories of lexical semantics. The former are dense and uninterpretable, the\nlatter largely based on familiar, discrete classes (e.g., supersenses) and\nrelations (e.g., synonymy and hypernymy). We propose methods that transform\nword vectors into sparse (and optionally binary) vectors. The resulting\nrepresentations are more similar to the interpretable features typically used\nin NLP, though they are discovered automatically from raw corpora. Because the\nvectors are highly sparse, they are computationally easy to work with. Most\nimportantly, we find that they outperform the original vectors on benchmark\ntasks.", "published": "2015-06-05 18:20:43", "link": "http://arxiv.org/abs/1506.02004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-scale Simple Question Answering with Memory Networks", "abstract": "Training large-scale question answering systems is complicated because\ntraining sources usually cover a small portion of the range of possible\nquestions. This paper studies the impact of multitask and transfer learning for\nsimple question answering; a setting for which the reasoning required to answer\nis quite easy, as long as one can retrieve the correct evidence given a\nquestion, which can be difficult in large-scale conditions. To this end, we\nintroduce a new dataset of 100k questions that we use in conjunction with\nexisting benchmarks. We conduct our study within the framework of Memory\nNetworks (Weston et al., 2015) because this perspective allows us to eventually\nscale up to more complex reasoning, and show that Memory Networks can be\nsuccessfully trained to achieve excellent performance.", "published": "2015-06-05 21:48:39", "link": "http://arxiv.org/abs/1506.02075v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Visualizing and Understanding Recurrent Networks", "abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study.", "published": "2015-06-05 22:33:04", "link": "http://arxiv.org/abs/1506.02078v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
