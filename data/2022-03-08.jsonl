{"title": "A Variational Hierarchical Model for Neural Cross-Lingual Summarization", "abstract": "The goal of the cross-lingual summarization (CLS) is to convert a document in\none language (e.g., English) to a summary in another one (e.g., Chinese).\nEssentially, the CLS task is the combination of machine translation (MT) and\nmonolingual summarization (MS), and thus there exists the hierarchical\nrelationship between MT\\&MS and CLS. Existing studies on CLS mainly focus on\nutilizing pipeline methods or jointly training an end-to-end model through an\nauxiliary MT or MS objective. However, it is very challenging for the model to\ndirectly conduct CLS as it requires both the abilities to translate and\nsummarize. To address this issue, we propose a hierarchical model for the CLS\ntask, based on the conditional variational auto-encoder. The hierarchical model\ncontains two kinds of latent variables at the local and global levels,\nrespectively. At the local level, there are two latent variables, one for\ntranslation and the other for summarization. As for the global level, there is\nanother latent variable for cross-lingual summarization conditioned on the two\nlocal-level variables. Experiments on two language directions (English-Chinese)\nverify the effectiveness and superiority of the proposed approach. In addition,\nwe show that our model is able to generate better cross-lingual summaries than\ncomparison models in the few-shot setting.", "published": "2022-03-08 02:46:11", "link": "http://arxiv.org/abs/2203.03820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Framework of Medical Information Annotation and Extraction for\n  Chinese Clinical Text", "abstract": "Medical information extraction consists of a group of natural language\nprocessing (NLP) tasks, which collaboratively convert clinical text to\npre-defined structured formats. Current state-of-the-art (SOTA) NLP models are\nhighly integrated with deep learning techniques and thus require massive\nannotated linguistic data. This study presents an engineering framework of\nmedical entity recognition, relation extraction and attribute extraction, which\nare unified in annotation, modeling and evaluation. Specifically, the\nannotation scheme is comprehensive, and compatible between tasks, especially\nfor the medical relations. The resulted annotated corpus includes 1,200 full\nmedical records (or 18,039 broken-down documents), and achieves inter-annotator\nagreements (IAAs) of 94.53%, 73.73% and 91.98% F 1 scores for the three tasks.\nThree task-specific neural network models are developed within a shared\nstructure, and enhanced by SOTA NLP techniques, i.e., pre-trained language\nmodels. Experimental results show that the system can retrieve medical\nentities, relations and attributes with F 1 scores of 93.47%, 67.14% and\n90.89%, respectively. This study, in addition to our publicly released\nannotation scheme and code, provides solid and practical engineering experience\nof developing an integrated medical information extraction system.", "published": "2022-03-08 03:19:16", "link": "http://arxiv.org/abs/2203.03823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Hierarchy into Text Encoder: a Contrastive Learning\n  Approach for Hierarchical Text Classification", "abstract": "Hierarchical text classification is a challenging subtask of multi-label\nclassification due to its complex label hierarchy. Existing methods encode text\nand label hierarchy separately and mix their representations for\nclassification, where the hierarchy remains unchanged for all input text.\nInstead of modeling them separately, in this work, we propose Hierarchy-guided\nContrastive Learning (HGCLR) to directly embed the hierarchy into a text\nencoder. During training, HGCLR constructs positive samples for input text\nunder the guidance of the label hierarchy. By pulling together the input text\nand its positive sample, the text encoder can learn to generate the\nhierarchy-aware text representation independently. Therefore, after training,\nthe HGCLR enhanced text encoder can dispense with the redundant hierarchy.\nExtensive experiments on three benchmark datasets verify the effectiveness of\nHGCLR.", "published": "2022-03-08 03:21:45", "link": "http://arxiv.org/abs/2203.03825v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Building an Open-Domain Dialogue System Incorporated with\n  Internet Memes", "abstract": "In recent years, Internet memes have been widely used in online chatting.\nCompared with text-based communication, conversations become more expressive\nand attractive when Internet memes are incorporated. This paper presents our\nsolutions for the Meme incorporated Open-domain Dialogue (MOD) Challenge of\nDSTC10, where three tasks are involved: text response modeling, meme retrieval,\nand meme emotion classification. Firstly, we leverage a large-scale pre-trained\ndialogue model for coherent and informative response generation. Secondly,\nbased on interaction-based text-matching, our approach can retrieve appropriate\nmemes with good generalization ability. Thirdly, we propose to model the\nemotion flow (EF) in conversations and introduce an auxiliary task of emotion\ndescription prediction (EDP) to boost the performance of meme emotion\nclassification. Experimental results on the MOD dataset demonstrate that our\nmethods can incorporate Internet memes into dialogue systems effectively.", "published": "2022-03-08 03:54:02", "link": "http://arxiv.org/abs/2203.03835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DARER: Dual-task Temporal Relational Recurrent Reasoning Network for\n  Joint Dialog Sentiment Classification and Act Recognition", "abstract": "The task of joint dialog sentiment classification (DSC) and act recognition\n(DAR) aims to simultaneously predict the sentiment label and act label for each\nutterance in a dialog. In this paper, we put forward a new framework which\nmodels the explicit dependencies via integrating \\textit{prediction-level\ninteractions} other than semantics-level interactions, more consistent with\nhuman intuition. Besides, we propose a speaker-aware temporal graph (SATG) and\na dual-task relational temporal graph (DRTG) to introduce \\textit{temporal\nrelations} into dialog understanding and dual-task reasoning. To implement our\nframework, we propose a novel model dubbed DARER, which first generates the\ncontext-, speaker- and temporal-sensitive utterance representations via\nmodeling SATG, then conducts recurrent dual-task relational reasoning on DRTG,\nin which process the estimated label distributions act as key clues in\nprediction-level interactions. Experiment results show that DARER outperforms\nexisting models by large margins while requiring much less computation resource\nand costing less training time. Remarkably, on DSC task in Mastodon, DARER\ngains a relative improvement of about 25% over previous best model in terms of\nF1, with less than 50% parameters and about only 60% required GPU memory.", "published": "2022-03-08 05:19:18", "link": "http://arxiv.org/abs/2203.03856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both\n  Language and Vision-and-Language Tasks", "abstract": "The workflow of pretraining and fine-tuning has emerged as a popular paradigm\nfor solving various NLP and V&L (Vision-and-Language) downstream tasks. With\nthe capacity of pretrained models growing rapidly, how to perform\nparameter-efficient fine-tuning has become fairly important for quick transfer\nlearning and deployment. In this paper, we design a novel unified\nparameter-efficient transfer learning framework that works effectively on both\npure language and V&L tasks. In particular, we use a shared hypernetwork that\ntakes trainable hyper-embeddings as input, and outputs weights for fine-tuning\ndifferent small modules in a pretrained language model, such as tuning the\nparameters inserted into multi-head attention blocks (i.e., prefix-tuning) and\nfeed-forward blocks (i.e., adapter-tuning). We define a set of embeddings\n(e.g., layer, block, task and visual embeddings) as the key components to\ncalculate hyper-embeddings, which thus can support both pure language and V&L\ntasks. Our proposed framework adds fewer trainable parameters in multi-task\nlearning while achieving superior performances and transfer ability compared to\nstate-of-the-art methods. Empirical results on the GLUE benchmark and multiple\nV&L tasks confirm the effectiveness of our framework on both textual and visual\nmodalities.", "published": "2022-03-08 06:51:33", "link": "http://arxiv.org/abs/2203.03878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InstructionNER: A Multi-Task Instruction-Based Generative Framework for\n  Few-shot NER", "abstract": "Recently, prompt-based methods have achieved significant performance in\nfew-shot learning scenarios by bridging the gap between language model\npre-training and fine-tuning for downstream tasks. However, existing prompt\ntemplates are mostly designed for sentence-level tasks and are inappropriate\nfor sequence labeling objectives. To address the above issue, we propose a\nmulti-task instruction-based generative framework, named InstructionNER, for\nlow-resource named entity recognition. Specifically, we reformulate the NER\ntask as a generation problem, which enriches source sentences with\ntask-specific instructions and answer options, then inferences the entities and\ntypes in natural language. We further propose two auxiliary tasks, including\nentity extraction and entity typing, which enable the model to capture more\nboundary information of entities and deepen the understanding of entity type\nsemantics, respectively. Experimental results show that our method consistently\noutperforms other baselines on five datasets in few-shot settings.", "published": "2022-03-08 07:56:36", "link": "http://arxiv.org/abs/2203.03903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Generalized Models for Task-oriented Dialogue Modeling on Spoken\n  Conversations", "abstract": "Building robust and general dialogue models for spoken conversations is\nchallenging due to the gap in distributions of spoken and written data. This\npaper presents our approach to build generalized models for the\nKnowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations\nChallenge of DSTC-10. In order to mitigate the discrepancies between spoken and\nwritten text, we mainly employ extensive data augmentation strategies on\nwritten data, including artificial error injection and round-trip text-speech\ntransformation. To train robust models for spoken conversations, we improve\npre-trained language models, and apply ensemble algorithms for each sub-task.\nTypically, for the detection task, we fine-tune \\roberta and ELECTRA, and run\nan error-fixing ensemble algorithm. For the selection task, we adopt a\ntwo-stage framework that consists of entity tracking and knowledge ranking, and\npropose a multi-task learning method to learn multi-level semantic information\nby domain classification and entity selection. For the generation task, we\nadopt a cross-validation data process to improve pre-trained generative\nlanguage models, followed by a consensus decoding algorithm, which can add\narbitrary features like relative \\rouge metric, and tune associated feature\nweights toward \\bleu directly. Our approach ranks third on the objective\nevaluation and second on the final official human evaluation.", "published": "2022-03-08 12:26:57", "link": "http://arxiv.org/abs/2203.04045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the Mixing of Contextual Information in the Transformer", "abstract": "The Transformer architecture aggregates input information through the\nself-attention mechanism, but there is no clear understanding of how this\ninformation is mixed across the entire model. Additionally, recent works have\ndemonstrated that attention weights alone are not enough to describe the flow\nof information. In this paper, we consider the whole attention block --\nmulti-head attention, residual connection, and layer normalization -- and\ndefine a metric to measure token-to-token interactions within each layer. Then,\nwe aggregate layer-wise interpretations to provide input attribution scores for\nmodel predictions. Experimentally, we show that our method, ALTI (Aggregation\nof Layer-wise Token-to-token Interactions), provides more faithful explanations\nand increased robustness than gradient-based methods.", "published": "2022-03-08 17:21:27", "link": "http://arxiv.org/abs/2203.04212v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which side are you on? Insider-Outsider classification in\n  conspiracy-theoretic social media", "abstract": "Social media is a breeding ground for threat narratives and related\nconspiracy theories. In these, an outside group threatens the integrity of an\ninside group, leading to the emergence of sharply defined group identities:\nInsiders -- agents with whom the authors identify and Outsiders -- agents who\nthreaten the insiders. Inferring the members of these groups constitutes a\nchallenging new NLP task: (i) Information is distributed over many\npoorly-constructed posts; (ii) Threats and threat agents are highly contextual,\nwith the same post potentially having multiple agents assigned to membership in\neither group; (iii) An agent's identity is often implicit and transitive; and\n(iv) Phrases used to imply Outsider status often do not follow common negative\nsentiment patterns. To address these challenges, we define a novel\nInsider-Outsider classification task. Because we are not aware of any\nappropriate existing datasets or attendant models, we introduce a labeled\ndataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages\npretrained language modeling to classify Insiders and Outsiders. NP2IO is shown\nto be robust, generalizing to noun phrases not seen during training, and\nexceeding the performance of non-trivial baseline models by $20\\%$.", "published": "2022-03-08 19:29:53", "link": "http://arxiv.org/abs/2203.04356v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating the Uncertainty in Emotion Class Labels with\n  Utterance-Specific Dirichlet Priors", "abstract": "Emotion recognition is a key attribute for artificial intelligence systems\nthat need to naturally interact with humans. However, the task definition is\nstill an open problem due to the inherent ambiguity of emotions. In this paper,\na novel Bayesian training loss based on per-utterance Dirichlet prior\ndistributions is proposed for verbal emotion recognition, which models the\nuncertainty in one-hot labels created when human annotators assign the same\nutterance to different emotion classes. An additional metric is used to\nevaluate the performance by detection test utterances with high labelling\nuncertainty. This removes a major limitation that emotion classification\nsystems only consider utterances with labels where the majority of annotators\nagree on the emotion class. Furthermore, a frequentist approach is studied to\nleverage the continuous-valued \"soft\" labels obtained by averaging the one-hot\nlabels. We propose a two-branch model structure for emotion classification on a\nper-utterance basis, which achieves state-of-the-art classification results on\nthe widely used IEMOCAP dataset. Based on this, uncertainty estimation\nexperiments were performed. The best performance in terms of the area under the\nprecision-recall curve when detecting utterances with high uncertainty was\nachieved by interpolating the Bayesian training loss with the Kullback-Leibler\ndivergence training loss for the soft labels. The generality of the proposed\napproach was verified using the MSP-Podcast dataset which yielded the same\npattern of results.", "published": "2022-03-08 23:30:01", "link": "http://arxiv.org/abs/2203.04443v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic-Preserving Linguistic Steganography by Pivot Translation and\n  Semantic-Aware Bins Coding", "abstract": "Linguistic steganography (LS) aims to embed secret information into a highly\nencoded text for covert communication. It can be roughly divided to two main\ncategories, i.e., modification based LS (MLS) and generation based LS (GLS).\nUnlike MLS that hides secret data by slightly modifying a given text without\nimpairing the meaning of the text, GLS uses a trained language model to\ndirectly generate a text carrying secret data. A common disadvantage for MLS\nmethods is that the embedding payload is very low, whose return is well\npreserving the semantic quality of the text. In contrast, GLS allows the data\nhider to embed a high payload, which has to pay the high price of\nuncontrollable semantics. In this paper, we propose a novel LS method to modify\na given text by pivoting it between two different languages and embed secret\ndata by applying a GLS-like information encoding strategy. Our purpose is to\nalter the expression of the given text, enabling a high payload to be embedded\nwhile keeping the semantic information unchanged. Experimental results have\nshown that the proposed work not only achieves a high embedding payload, but\nalso shows superior performance in maintaining the semantic consistency and\nresisting linguistic steganalysis.", "published": "2022-03-08 01:35:05", "link": "http://arxiv.org/abs/2203.03795v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Understanding Iterative Revision from Human-Written Text", "abstract": "Writing is, by nature, a strategic, adaptive, and more importantly, an\niterative process. A crucial part of writing is editing and revising the text.\nPrevious works on text revision have focused on defining edit intention\ntaxonomies within a single domain or developing computational models with a\nsingle level of edit granularity, such as sentence-level edits, which differ\nfrom human's revision cycles. This work describes IteraTeR: the first\nlarge-scale, multi-domain, edit-intention annotated corpus of iteratively\nrevised text. In particular, IteraTeR is collected based on a new framework to\ncomprehensively model the iterative text revisions that generalize to various\ndomains of formal writing, edit intentions, revision depths, and granularities.\nWhen we incorporate our annotated edit intentions, both generative and\nedit-based text revision models significantly improve automatic evaluations.\nThrough our work, we better understand the text revision process, making vital\nconnections between edit intentions and writing quality, enabling the creation\nof diverse corpora to support computational modeling of iterative text\nrevisions.", "published": "2022-03-08 01:47:42", "link": "http://arxiv.org/abs/2203.03802v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Image Search with Text Feedback by Additive Attention Compositional\n  Learning", "abstract": "Effective image retrieval with text feedback stands to impact a range of\nreal-world applications, such as e-commerce. Given a source image and text\nfeedback that describes the desired modifications to that image, the goal is to\nretrieve the target images that resemble the source yet satisfy the given\nmodifications by composing a multi-modal (image-text) query. We propose a novel\nsolution to this problem, Additive Attention Compositional Learning (AACL),\nthat uses a multi-modal transformer-based architecture and effectively models\nthe image-text contexts. Specifically, we propose a novel image-text\ncomposition module based on additive attention that can be seamlessly plugged\ninto deep neural networks. We also introduce a new challenging benchmark\nderived from the Shopping100k dataset. AACL is evaluated on three large-scale\ndatasets (FashionIQ, Fashion200k, and Shopping100k), each with strong\nbaselines. Extensive experiments show that AACL achieves new state-of-the-art\nresults on all three datasets.", "published": "2022-03-08 02:03:49", "link": "http://arxiv.org/abs/2203.03809v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced\n  Training for Neural Machine Translation", "abstract": "Neural networks tend to gradually forget the previously learned knowledge\nwhen learning multiple tasks sequentially from dynamic data distributions. This\nproblem is called \\textit{catastrophic forgetting}, which is a fundamental\nchallenge in the continual learning of neural networks. In this work, we\nobserve that catastrophic forgetting not only occurs in continual learning but\nalso affects the traditional static training. Neural networks, especially\nneural machine translation models, suffer from catastrophic forgetting even if\nthey learn from a static training set. To be specific, the final model pays\nimbalanced attention to training samples, where recently exposed samples\nattract more attention than earlier samples. The underlying cause is that\ntraining samples do not get balanced training in each model update, so we name\nthis problem \\textit{imbalanced training}. To alleviate this problem, we\npropose Complementary Online Knowledge Distillation (COKD), which uses\ndynamically updated teacher models trained on specific data orders to\niteratively provide complementary knowledge to the student model. Experimental\nresults on multiple machine translation tasks show that our method successfully\nalleviates the problem of imbalanced training and achieves substantial\nimprovements over strong baseline systems.", "published": "2022-03-08 08:08:45", "link": "http://arxiv.org/abs/2203.03910v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Visual-Language Navigation Pretraining via Prompt-based Environmental\n  Self-exploration", "abstract": "Vision-language navigation (VLN) is a challenging task due to its large\nsearching space in the environment. To address this problem, previous works\nhave proposed some methods of fine-tuning a large model that pretrained on\nlarge-scale datasets. However, the conventional fine-tuning methods require\nextra human-labeled navigation data and lack self-exploration capabilities in\nenvironments, which hinders their generalization of unseen scenes. To improve\nthe ability of fast cross-domain adaptation, we propose Prompt-based\nEnvironmental Self-exploration (ProbES), which can self-explore the\nenvironments by sampling trajectories and automatically generates structured\ninstructions via a large-scale cross-modal pretrained model (CLIP). Our method\nfully utilizes the knowledge learned from CLIP to build an in-domain dataset by\nself-exploration without human labeling. Unlike the conventional approach of\nfine-tuning, we introduce prompt-based learning to achieve fast adaptation for\nlanguage embeddings, which substantially improves the learning efficiency by\nleveraging prior knowledge. By automatically synthesizing\ntrajectory-instruction pairs in any environment without human supervision and\nefficient prompt-based learning, our model can adapt to diverse vision-language\nnavigation tasks, including VLN and REVERIE. Both qualitative and quantitative\nresults show that our ProbES significantly improves the generalization ability\nof the navigation model.", "published": "2022-03-08 11:01:24", "link": "http://arxiv.org/abs/2203.04006v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Semantic Distillation Guided Salient Object Detection", "abstract": "Most existing CNN-based salient object detection methods can identify local\nsegmentation details like hair and animal fur, but often misinterpret the real\nsaliency due to the lack of global contextual information caused by the\nsubjectiveness of the SOD task and the locality of convolution layers.\nMoreover, due to the unrealistically expensive labeling costs, the current\nexisting SOD datasets are insufficient to cover the real data distribution. The\nlimitation and bias of the training data add additional difficulty to fully\nexploring the semantic association between object-to-object and\nobject-to-environment in a given image. In this paper, we propose a semantic\ndistillation guided SOD (SDG-SOD) method that produces accurate results by\nfusing semantically distilled knowledge from generated image captioning into\nthe Vision-Transformer-based SOD framework. SDG-SOD can better uncover\ninter-objects and object-to-environment saliency and cover the gap between the\nsubjective nature of SOD and its expensive labeling. Comprehensive experiments\non five benchmark datasets demonstrate that the SDG-SOD outperforms the\nstate-of-the-art approaches on four evaluation metrics, and largely improves\nthe model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.", "published": "2022-03-08 13:40:51", "link": "http://arxiv.org/abs/2203.04076v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "It's AI Match: A Two-Step Approach for Schema Matching Using Embeddings", "abstract": "Since data is often stored in different sources, it needs to be integrated to\ngather a global view that is required in order to create value and derive\nknowledge from it. A critical step in data integration is schema matching which\naims to find semantic correspondences between elements of two schemata. In\norder to reduce the manual effort involved in schema matching, many solutions\nfor the automatic determination of schema correspondences have already been\ndeveloped.\n  In this paper, we propose a novel end-to-end approach for schema matching\nbased on neural embeddings. The main idea is to use a two-step approach\nconsisting of a table matching step followed by an attribute matching step. In\nboth steps we use embeddings on different levels either representing the whole\ntable or single attributes. Our results show that our approach is able to\ndetermine correspondences in a robust and reliable way and compared to\ntraditional schema matching approaches can find non-trivial correspondences.", "published": "2022-03-08 19:42:28", "link": "http://arxiv.org/abs/2203.04366v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "iSEA: An Interactive Pipeline for Semantic Error Analysis of NLP Models", "abstract": "Error analysis in NLP models is essential to successful model development and\ndeployment. One common approach for diagnosing errors is to identify\nsubpopulations in the dataset where the model produces the most errors.\nHowever, existing approaches typically define subpopulations based on\npre-defined features, which requires users to form hypotheses of errors in\nadvance. To complement these approaches, we propose iSEA, an Interactive\nPipeline for Semantic Error Analysis in NLP Models, which automatically\ndiscovers semantically-grounded subpopulations with high error rates in the\ncontext of a human-in-the-loop interactive system. iSEA enables model\ndevelopers to learn more about their model errors through discovered\nsubpopulations, validate the sources of errors through interactive analysis on\nthe discovered subpopulations, and test hypotheses about model errors by\ndefining custom subpopulations. The tool supports semantic descriptions of\nerror-prone subpopulations at the token and concept level, as well as\npre-defined higher-level features. Through use cases and expert interviews, we\ndemonstrate how iSEA can assist error understanding and analysis.", "published": "2022-03-08 21:31:15", "link": "http://arxiv.org/abs/2203.04408v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Extraction of Sleep Information from Clinical Notes of Patients with\n  Alzheimer's Disease Using Natural Language Processing", "abstract": "Alzheimer's Disease (AD) is the most common form of dementia in the United\nStates. Sleep is one of the lifestyle-related factors that has been shown\ncritical for optimal cognitive function in old age. However, there is a lack of\nresearch studying the association between sleep and AD incidence. A major\nbottleneck for conducting such research is that the traditional way to acquire\nsleep information is time-consuming, inefficient, non-scalable, and limited to\npatients' subjective experience. A gold standard dataset is created from manual\nannotation of 570 randomly sampled clinical note documents from the adSLEEP, a\ncorpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved\nfrom the University of Pittsburgh Medical Center (UPMC). We developed a\nrule-based Natural Language Processing (NLP) algorithm, machine learning\nmodels, and Large Language Model(LLM)-based NLP algorithms to automate the\nextraction of sleep-related concepts, including snoring, napping, sleep\nproblem, bad sleep quality, daytime sleepiness, night wakings, and sleep\nduration, from the gold standard dataset. Rule-based NLP algorithm achieved the\nbest performance of F1 across all sleep-related concepts. In terms of Positive\nPredictive Value (PPV), rule-based NLP algorithm achieved 1.00 for daytime\nsleepiness and sleep duration, machine learning models: 0.95 and for napping,\n0.86 for bad sleep quality and 0.90 for snoring; and LLAMA2 with finetuning\nachieved PPV of 0.93 for Night Wakings, 0.89 for sleep problem, and 1.00 for\nsleep duration. The results show that the rule-based NLP algorithm consistently\nachieved the best performance for all sleep concepts. This study focused on the\nclinical notes of patients with AD, but could be extended to general sleep\ninformation extraction for other diseases.", "published": "2022-03-08 21:20:19", "link": "http://arxiv.org/abs/2204.09601v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "abstract": "Pre-trained models for programming languages have recently demonstrated great\nsuccess on code intelligence. To support both code-related understanding and\ngeneration tasks, recent works attempt to pre-train unified encoder-decoder\nmodels. However, such encoder-decoder framework is sub-optimal for\nauto-regressive tasks, especially code completion that requires a decoder-only\nmanner for efficient inference. In this paper, we present UniXcoder, a unified\ncross-modal pre-trained model for programming language. The model utilizes mask\nattention matrices with prefix adapters to control the behavior of the model\nand leverages cross-modal contents like AST and code comment to enhance code\nrepresentation. To encode AST that is represented as a tree in parallel, we\npropose a one-to-one mapping method to transform AST in a sequence structure\nthat retains all structural information from the tree. Furthermore, we propose\nto utilize multi-modal contents to learn representation of code fragment with\ncontrastive learning, and then align representations among programming\nlanguages using a cross-modal generation task. We evaluate UniXcoder on five\ncode-related tasks over nine datasets. To further evaluate the performance of\ncode fragment representation, we also construct a dataset for a new task,\ncalled zero-shot code-to-code search. Results show that our model achieves\nstate-of-the-art performance on most tasks and analysis reveals that comment\nand AST can both enhance UniXcoder.", "published": "2022-03-08 04:48:07", "link": "http://arxiv.org/abs/2203.03850v1", "categories": ["cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Where Does the Performance Improvement Come From? -- A Reproducibility\n  Concern about Image-Text Retrieval", "abstract": "This article aims to provide the information retrieval community with some\nreflections on recent advances in retrieval learning by analyzing the\nreproducibility of image-text retrieval models. Due to the increase of\nmultimodal data over the last decade, image-text retrieval has steadily become\na major research direction in the field of information retrieval. Numerous\nresearchers train and evaluate image-text retrieval algorithms using benchmark\ndatasets such as MS-COCO and Flickr30k. Research in the past has mostly focused\non performance, with multiple state-of-the-art methodologies being suggested in\na variety of ways. According to their assertions, these techniques provide\nimproved modality interactions and hence more precise multimodal\nrepresentations. In contrast to previous works, we focus on the reproducibility\nof the approaches and the examination of the elements that lead to improved\nperformance by pretrained and nonpretrained models in retrieving images and\ntext. To be more specific, we first examine the related reproducibility\nconcerns and explain why our focus is on image-text retrieval tasks. Second, we\nsystematically summarize the current paradigm of image-text retrieval models\nand the stated contributions of those approaches. Third, we analyze various\naspects of the reproduction of pretrained and nonpretrained retrieval models.\nTo complete this, we conducted ablation experiments and obtained some\ninfluencing factors that affect retrieval recall more than the improvement\nclaimed in the original paper. Finally, we present some reflections and\nchallenges that the retrieval community should consider in the future. Our\nsource code is publicly available at\nhttps://github.com/WangFei-2019/Image-text-Retrieval.", "published": "2022-03-08 05:01:43", "link": "http://arxiv.org/abs/2203.03853v3", "categories": ["cs.IR", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "Geodesic Multi-Modal Mixup for Robust Fine-Tuning", "abstract": "Pre-trained multi-modal models, such as CLIP, provide transferable embeddings\nand show promising results in diverse applications. However, the analysis of\nlearned multi-modal embeddings is relatively unexplored, and the embedding\ntransferability can be improved. In this work, we observe that CLIP holds\nseparated embedding subspaces for two different modalities, and then we\ninvestigate it through the lens of uniformity-alignment to measure the quality\nof learned representation. Both theoretically and empirically, we show that\nCLIP retains poor uniformity and alignment even after fine-tuning. Such a lack\nof alignment and uniformity might restrict the transferability and robustness\nof embeddings. To this end, we devise a new fine-tuning method for robust\nrepresentation equipping better alignment and uniformity. First, we propose a\nGeodesic Multi-Modal Mixup that mixes the embeddings of image and text to\ngenerate hard negative samples on the hypersphere. Then, we fine-tune the model\non hard negatives as well as original negatives and positives with contrastive\nloss. Based on the theoretical analysis about hardness guarantee and limiting\nbehavior, we justify the use of our method. Extensive experiments on retrieval,\ncalibration, few- or zero-shot classification (under distribution shift),\nembedding arithmetic, and image captioning further show that our method\nprovides transferable representations, enabling robust model adaptation on\ndiverse tasks. Code: https://github.com/changdaeoh/multimodal-mixup", "published": "2022-03-08 07:34:52", "link": "http://arxiv.org/abs/2203.03897v4", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Adaptor: Objective-Centric Adaptation Framework for Language Models", "abstract": "Progress in natural language processing research is catalyzed by the\npossibilities given by the widespread software frameworks. This paper\nintroduces Adaptor library that transposes the traditional model-centric\napproach composed of pre-training + fine-tuning steps to objective-centric\napproach, composing the training process by applications of selected\nobjectives. We survey research directions that can benefit from enhanced\nobjective-centric experimentation in multitask training, custom objectives\ndevelopment, dynamic training curricula, or domain adaptation. Adaptor aims to\nease reproducibility of these research directions in practice. Finally, we\ndemonstrate the practical applicability of Adaptor in selected unsupervised\ndomain adaptation scenarios.", "published": "2022-03-08 10:34:52", "link": "http://arxiv.org/abs/2203.03989v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Plumeria at SemEval-2022 Task 6: Robust Approaches for Sarcasm Detection\n  for English and Arabic Using Transformers and Data Augmentation", "abstract": "This paper describes our submission to SemEval-2022 Task 6 on sarcasm\ndetection and its five subtasks for English and Arabic. Sarcasm conveys a\nmeaning which contradicts the literal meaning, and it is mainly found on social\nnetworks. It has a significant role in understanding the intention of the user.\nFor detecting sarcasm, we used deep learning techniques based on transformers\ndue to its success in the field of Natural Language Processing (NLP) without\nthe need for feature engineering. The datasets were taken from tweets. We\ncreated new datasets by augmenting with external data or by using word\nembeddings and repetition of instances. Experiments were done on the datasets\nwith different types of preprocessing because it is crucial in this task. The\nrank of our team was consistent across four subtasks (fourth rank in three\nsubtasks and sixth rank in one subtask); whereas other teams might be in the\ntop ranks for some subtasks but rank drastically less in other subtasks. This\nimplies the robustness and stability of the models and the techniques we used.", "published": "2022-03-08 14:33:45", "link": "http://arxiv.org/abs/2203.04111v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Bidirectional Translation between Descriptions and Actions with\n  Small Paired Data", "abstract": "This study achieved bidirectional translation between descriptions and\nactions using small paired data from different modalities. The ability to\nmutually generate descriptions and actions is essential for robots to\ncollaborate with humans in their daily lives, which generally requires a large\ndataset that maintains comprehensive pairs of both modality data. However, a\npaired dataset is expensive to construct and difficult to collect. To address\nthis issue, this study proposes a two-stage training method for bidirectional\ntranslation. In the proposed method, we train recurrent autoencoders (RAEs) for\ndescriptions and actions with a large amount of non-paired data. Then, we\nfinetune the entire model to bind their intermediate representations using\nsmall paired data. Because the data used for pre-training do not require\npairing, behavior-only data or a large language corpus can be used. We\nexperimentally evaluated our method using a paired dataset consisting of\nmotion-captured actions and descriptions. The results showed that our method\nperformed well, even when the amount of paired data to train was small. The\nvisualization of the intermediate representations of each RAE showed that\nsimilar actions were encoded in a clustered position and the corresponding\nfeature vectors were well aligned.", "published": "2022-03-08 17:39:16", "link": "http://arxiv.org/abs/2203.04218v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "QCRI's COVID-19 Disinformation Detector: A System to Fight the COVID-19\n  Infodemic in Social Media", "abstract": "Fighting the ongoing COVID-19 infodemic has been declared as one of the most\nimportant focus areas by the World Health Organization since the onset of the\nCOVID-19 pandemic. While the information that is consumed and disseminated\nconsists of promoting fake cures, rumors, and conspiracy theories to spreading\nxenophobia and panic, at the same time there is information (e.g., containing\nadvice, promoting cure) that can help different stakeholders such as\npolicy-makers. Social media platforms enable the infodemic and there has been\nan effort to curate the content on such platforms, analyze and debunk them.\nWhile a majority of the research efforts consider one or two aspects (e.g.,\ndetecting factuality) of such information, in this study we focus on a\nmultifaceted approach, including an\nAPI,\\url{https://app.swaggerhub.com/apis/yifan2019/Tanbih/0.8.0/} and a demo\nsystem,\\url{https://covid19.tanbih.org}, which we made freely and publicly\navailable. We believe that this will facilitate researchers and different\nstakeholders. A screencast of the API services and demo is\navailable.\\url{https://youtu.be/zhbcSvxEKMk}", "published": "2022-03-08 12:30:35", "link": "http://arxiv.org/abs/2204.03506v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.SI", "68T50", "I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "SpeechFormer: A Hierarchical Efficient Framework Incorporating the\n  Characteristics of Speech", "abstract": "Transformer has obtained promising results on cognitive speech signal\nprocessing field, which is of interest in various applications ranging from\nemotion to neurocognitive disorder analysis. However, most works treat speech\nsignal as a whole, leading to the neglect of the pronunciation structure that\nis unique to speech and reflects the cognitive process. Meanwhile, Transformer\nhas heavy computational burden due to its full attention operation. In this\npaper, a hierarchical efficient framework, called SpeechFormer, which considers\nthe structural characteristics of speech, is proposed and can be served as a\ngeneral-purpose backbone for cognitive speech signal processing. The proposed\nSpeechFormer consists of frame, phoneme, word and utterance stages in\nsuccession, each performing a neighboring attention according to the structural\npattern of speech with high computational efficiency. SpeechFormer is evaluated\non speech emotion recognition (IEMOCAP & MELD) and neurocognitive disorder\ndetection (Pitt & DAIC-WOZ) tasks, and the results show that SpeechFormer\noutperforms the standard Transformer-based framework while greatly reducing the\ncomputational cost. Furthermore, our SpeechFormer achieves comparable results\nto the state-of-the-art approaches.", "published": "2022-03-08 02:22:28", "link": "http://arxiv.org/abs/2203.03812v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Practical cognitive speech compression", "abstract": "This paper presents a new neural speech compression method that is practical\nin the sense that it operates at low bitrate, introduces a low latency, is\ncompatible in computational complexity with current mobile devices, and\nprovides a subjective quality that is comparable to that of standard\nmobile-telephony codecs. Other recently proposed neural vocoders also have the\nability to operate at low bitrate. However, they do not produce the same level\nof subjective quality as standard codecs. On the other hand, standard codecs\nrely on objective and short-term metrics such as the segmental signal-to-noise\nratio that correlate only weakly with perception. Furthermore, standard codecs\nare less efficient than unsupervised neural networks at capturing speech\nattributes, especially long-term ones. The proposed method combines a\ncognitive-coding encoder that extracts an interpretable unsupervised\nhierarchical representation with a multi stage decoder that has a GAN-based\narchitecture. We observe that this method is very robust to the quantization of\nrepresentation features. An AB test was conducted on a subset of the Harvard\nsentences that are commonly used to evaluate standard mobile-telephony codecs.\nThe results show that the proposed method outperforms the standard AMR-WB codec\nin terms of delay, bitrate and subjective quality.", "published": "2022-03-08 21:44:38", "link": "http://arxiv.org/abs/2203.04415v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Digital Speech Algorithms for Speaker De-Identification", "abstract": "The present work is based on the COST Action IC1206 for De-identification in\nmultimedia content. It was performed to test four algorithms of voice\nmodifications on a speech gender recognizer to find the degree of modification\nof pitch when the speech recognizer have the probability of success equal to\nthe probability of failure. The purpose of this analysis is to assess the\nintensity of the speech tone modification, the quality, the reversibility and\nnot-reversibility of the changes made.", "published": "2022-03-08 08:57:11", "link": "http://arxiv.org/abs/2203.03932v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer", "abstract": "This paper presents an audio-visual approach for voice separation which\nproduces state-of-the-art results at a low latency in two scenarios: speech and\nsinging voice. The model is based on a two-stage network. Motion cues are\nobtained with a lightweight graph convolutional network that processes face\nlandmarks. Then, both audio and motion features are fed to an audio-visual\ntransformer which produces a fairly good estimation of the isolated target\nsource. In a second stage, the predominant voice is enhanced with an audio-only\nnetwork. We present different ablation studies and comparison to\nstate-of-the-art methods. Finally, we explore the transferability of models\ntrained for speech separation in the task of singing voice separation. The\ndemos, code, and weights are available in https://ipcv.github.io/VoViT/", "published": "2022-03-08 14:08:47", "link": "http://arxiv.org/abs/2203.04099v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Locate This, Not That: Class-Conditioned Sound Event DOA Estimation", "abstract": "Existing systems for sound event localization and detection (SELD) typically\noperate by estimating a source location for all classes at every time instant.\nIn this paper, we propose an alternative class-conditioned SELD model for\nsituations where we may not be interested in localizing all classes all of the\ntime. This class-conditioned SELD model takes as input the spatial and spectral\nfeatures from the sound file, and also a one-hot vector indicating the class we\nare currently interested in localizing. We inject the conditioning information\nat several points in our model using feature-wise linear modulation (FiLM)\nlayers. Through experiments on the DCASE 2020 Task 3 dataset, we show that the\nproposed class-conditioned SELD model performs better in terms of common SELD\nmetrics than the baseline model that locates all classes simultaneously, and\nalso outperforms specialist models that are trained to locate only a single\nclass of interest. We also evaluate performance on the DCASE 2021 Task 3\ndataset, which includes directional interference (sound events from classes we\nare not interested in localizing) and notice especially strong improvement from\nthe class-conditioned model.", "published": "2022-03-08 16:49:15", "link": "http://arxiv.org/abs/2203.04197v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Harmonicity Plays a Critical Role in DNN Based Versus in\n  Biologically-Inspired Monaural Speech Segregation Systems", "abstract": "Recent advancements in deep learning have led to drastic improvements in\nspeech segregation models. Despite their success and growing applicability, few\nefforts have been made to analyze the underlying principles that these networks\nlearn to perform segregation. Here we analyze the role of harmonicity on two\nstate-of-the-art Deep Neural Networks (DNN)-based models- Conv-TasNet and\nDPT-Net. We evaluate their performance with mixtures of natural speech versus\nslightly manipulated inharmonic speech, where harmonics are slightly frequency\njittered. We find that performance deteriorates significantly if one source is\neven slightly harmonically jittered, e.g., an imperceptible 3% harmonic jitter\ndegrades performance of Conv-TasNet from 15.4 dB to 0.70 dB. Training the model\non inharmonic speech does not remedy this sensitivity, instead resulting in\nworse performance on natural speech mixtures, making inharmonicity a powerful\nadversarial factor in DNN models. Furthermore, additional analyses reveal that\nDNN algorithms deviate markedly from biologically inspired algorithms that rely\nprimarily on timing cues and not harmonicity to segregate speech.", "published": "2022-03-08 21:51:06", "link": "http://arxiv.org/abs/2203.04420v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
