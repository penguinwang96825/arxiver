{"title": "Semi-Supervised Sequence Modeling with Cross-View Training", "abstract": "Unsupervised representation learning algorithms such as word2vec and ELMo\nimprove the accuracy of many supervised NLP models, mainly because they can\ntake advantage of large amounts of unlabeled text. However, the supervised\nmodels only learn from task-specific labeled data during the main training\nphase. We therefore propose Cross-View Training (CVT), a semi-supervised\nlearning algorithm that improves the representations of a Bi-LSTM sentence\nencoder using a mix of labeled and unlabeled data. On labeled examples,\nstandard supervised learning is used. On unlabeled examples, CVT teaches\nauxiliary prediction modules that see restricted views of the input (e.g., only\npart of a sentence) to match the predictions of the full model seeing the whole\ninput. Since the auxiliary modules and the full model share intermediate\nrepresentations, this in turn improves the full model. Moreover, we show that\nCVT is particularly effective when combined with multi-task learning. We\nevaluate CVT on five sequence tagging tasks, machine translation, and\ndependency parsing, achieving state-of-the-art results.", "published": "2018-09-22 02:39:32", "link": "http://arxiv.org/abs/1809.08370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Byte-sized Approach to Named Entity Recognition", "abstract": "In biomedical literature, it is common for entity boundaries to not align\nwith word boundaries. Therefore, effective identification of entity spans\nrequires approaches capable of considering tokens that are smaller than words.\nWe introduce a novel, subword approach for named entity recognition (NER) that\nuses byte-pair encodings (BPE) in combination with convolutional and recurrent\nneural networks to produce byte-level tags of entities. We present experimental\nresults on several standard biomedical datasets, namely the BioCreative VI\nBio-ID, JNLPBA, and GENETAG datasets. We demonstrate competitive performance\nwhile bypassing the specialized domain expertise needed to create biomedical\ntext tokenization rules.", "published": "2018-09-22 06:03:29", "link": "http://arxiv.org/abs/1809.08386v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Financial Sentimental Factors in Chinese Market Using\n  Natural Language Processing", "abstract": "In this paper, we design an integrated algorithm to evaluate the sentiment of\nChinese market. Firstly, with the help of the web browser automation, we crawl\na lot of news and comments from several influential financial websites\nautomatically. Secondly, we use techniques of Natural Language Processing(NLP)\nunder Chinese context, including tokenization, Word2vec word embedding and\nsemantic database WordNet, to compute Senti-scores of these news and comments,\nand then construct the sentimental factor. Here, we build a finance-specific\nsentimental lexicon so that the sentimental factor can reflect the sentiment of\nfinancial market but not the general sentiments as happiness, sadness, etc.\nThirdly, we also implement an adjustment of the standard sentimental factor.\nOur experimental performance shows that there is a significant correlation\nbetween our standard sentimental factor and the Chinese market, and the\nadjusted factor is even more informative, having a stronger correlation with\nthe Chinese market. Therefore, our sentimental factors can be important\nreferences when making investment decisions. Especially during the Chinese\nmarket crash in 2015, the Pearson correlation coefficient of adjusted\nsentimental factor with SSE is 0.5844, which suggests that our model can\nprovide a solid guidance, especially in the special period when the market is\ninfluenced greatly by public sentiment.", "published": "2018-09-22 06:35:07", "link": "http://arxiv.org/abs/1809.08390v1", "categories": ["q-fin.CP", "cs.CL"], "primary_category": "q-fin.CP"}
{"title": "The Privacy Policy Landscape After the GDPR", "abstract": "The EU General Data Protection Regulation (GDPR) is one of the most demanding\nand comprehensive privacy regulations of all time. A year after it went into\neffect, we study its impact on the landscape of privacy policies online. We\nconduct the first longitudinal, in-depth, and at-scale assessment of privacy\npolicies before and after the GDPR. We gauge the complete consumption cycle of\nthese policies, from the first user impressions until the compliance\nassessment. We create a diverse corpus of two sets of 6,278 unique\nEnglish-language privacy policies from inside and outside the EU, covering\ntheir pre-GDPR and the post-GDPR versions. The results of our tests and\nanalyses suggest that the GDPR has been a catalyst for a major overhaul of the\nprivacy policies inside and outside the EU. This overhaul of the policies,\nmanifesting in extensive textual changes, especially for the EU-based websites,\ncomes at mixed benefits to the users. While the privacy policies have become\nconsiderably longer, our user study with 470 participants on Amazon MTurk\nindicates a significant improvement in the visual representation of privacy\npolicies from the users' perspective for the EU websites. We further develop a\nnew workflow for the automated assessment of requirements in privacy policies.\nUsing this workflow, we show that privacy policies cover more data practices\nand are more consistent with seven compliance requirements post the GDPR. We\nalso assess how transparent the organizations are with their privacy practices\nby performing specificity analysis. In this analysis, we find evidence for\npositive changes triggered by the GDPR, with the specificity level improving on\naverage. Still, we find the landscape of privacy policies to be in a\ntransitional phase; many policies still do not meet several key GDPR\nrequirements or their improved coverage comes with reduced specificity.", "published": "2018-09-22 07:08:20", "link": "http://arxiv.org/abs/1809.08396v3", "categories": ["cs.CR", "cs.CL", "cs.CY"], "primary_category": "cs.CR"}
{"title": "Relating Zipf's law to textual information", "abstract": "Zipf's law is the main regularity of quantitative linguistics. Despite of\nmany works devoted to foundations of this law, it is still unclear whether it\nis only a statistical regularity, or it has deeper relations with\ninformation-carrying structures of the text. This question relates to that of\ndistinguishing a meaningful text (written in an unknown system) from a\nmeaningless set of symbols that mimics statistical features of a text. Here we\ncontribute to resolving these questions by comparing features of the first half\nof a text (from the beginning to the middle) to its second half. This\ncomparison can uncover hidden effects, because the halves have the same values\nof many parameters (style, genre, author's vocabulary {\\it etc}). In all\nstudied texts we saw that for the first half Zipf's law applies from smaller\nranks than in the second half, i.e. the law applies better to the first half.\nAlso, words that hold Zipf's law in the first half are distributed more\nhomogeneously over the text. These features do allow to distinguish a\nmeaningful text from a random sequence of words. Our findings correlate with a\nnumber of textual characteristics that hold in most cases we studied: the first\nhalf is lexically richer, has longer and less repetitive words, more and\nshorter sentences, more punctuation signs and more paragraphs. These\ndifferences between the halves indicate on a higher hierarchic level of text\norganization that so far went unnoticed in text linguistics. They relate the\nvalidity of Zipf's law to textual information. A complete description of this\neffect requires new models, though one existing model can account for some of\nits aspects.", "published": "2018-09-22 07:37:39", "link": "http://arxiv.org/abs/1809.08399v1", "categories": ["cs.CL", "nlin.AO", "physics.soc-ph"], "primary_category": "cs.CL"}
