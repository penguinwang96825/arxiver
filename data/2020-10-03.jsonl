{"title": "Mining Knowledge for Natural Language Inference from Wikipedia\n  Categories", "abstract": "Accurate lexical entailment (LE) and natural language inference (NLI) often\nrequire large quantities of costly annotations. To alleviate the need for\nlabeled data, we introduce WikiNLI: a resource for improving model performance\non NLI and LE tasks. It contains 428,899 pairs of phrases constructed from\nnaturally annotated category hierarchies in Wikipedia. We show that we can\nimprove strong baselines such as BERT and RoBERTa by pretraining them on\nWikiNLI and transferring the models on downstream tasks. We conduct systematic\ncomparisons with phrases extracted from other knowledge bases such as WordNet\nand Wikidata to find that pretraining on WikiNLI gives the best performance. In\naddition, we construct WikiNLI in other languages, and show that pretraining on\nthem improves performance on NLI tasks of corresponding languages.", "published": "2020-10-03 00:45:01", "link": "http://arxiv.org/abs/2010.01239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilevel Text Alignment with Cross-Document Attention", "abstract": "Text alignment finds application in tasks such as citation recommendation and\nplagiarism detection. Existing alignment methods operate at a single,\npredefined level and cannot learn to align texts at, for example, sentence and\ndocument levels. We propose a new learning approach that equips previously\nestablished hierarchical attention encoders for representing documents with a\ncross-document attention component, enabling structural comparisons across\ndifferent levels (document-to-document and sentence-to-document). Our component\nis weakly supervised from document pairs and can align at multiple levels. Our\nevaluation on predicting document-to-document relationships and\nsentence-to-document relationships on the tasks of citation recommendation and\nplagiarism detection shows that our approach outperforms previously established\nhierarchical, attention encoders based on recurrent and transformer\ncontextualization that are unaware of structural correspondence between\ndocuments.", "published": "2020-10-03 02:52:28", "link": "http://arxiv.org/abs/2010.01263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Partially-Aligned Data-to-Text Generation with Distant Supervision", "abstract": "The Data-to-Text task aims to generate human-readable text for describing\nsome given structured data enabling more interpretability. However, the typical\ngeneration task is confined to a few particular domains since it requires\nwell-aligned data which is difficult and expensive to obtain. Using\npartially-aligned data is an alternative way of solving the dataset scarcity\nproblem. This kind of data is much easier to obtain since it can be produced\nautomatically. However, using this kind of data induces the over-generation\nproblem posing difficulties for existing models, which tends to add unrelated\nexcerpts during the generation procedure. In order to effectively utilize\nautomatically annotated partially-aligned datasets, we extend the traditional\ngeneration task to a refined task called Partially-Aligned Data-to-Text\nGeneration (PADTG) which is more practical since it utilizes automatically\nannotated data for training and thus considerably expands the application\ndomains. To tackle this new task, we propose a novel distant supervision\ngeneration framework. It firstly estimates the input data's supportiveness for\neach target word with an estimator and then applies a supportiveness adaptor\nand a rebalanced beam search to harness the over-generation problem in the\ntraining and generation phases respectively. We also contribute a\npartially-aligned dataset (The data and source code of this paper can be\nobtained from https://github.com/fuzihaofzh/distant_supervision_nlg by sampling\nsentences from Wikipedia and automatically extracting corresponding KB triples\nfor each sentence from Wikidata. The experimental results show that our\nframework outperforms all baseline models as well as verify the feasibility of\nutilizing partially-aligned data.", "published": "2020-10-03 03:18:52", "link": "http://arxiv.org/abs/2010.01268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Interpretable Reasoning over Paragraph Effects in Situation", "abstract": "We focus on the task of reasoning over paragraph effects in situation, which\nrequires a model to understand the cause and effect described in a background\nparagraph, and apply the knowledge to a novel situation. Existing works ignore\nthe complicated reasoning process and solve it with a one-step \"black box\"\nmodel. Inspired by human cognitive processes, in this paper we propose a\nsequential approach for this task which explicitly models each step of the\nreasoning process with neural network modules. In particular, five reasoning\nmodules are designed and learned in an end-to-end manner, which leads to a more\ninterpretable model. Experimental results on the ROPES dataset demonstrate the\neffectiveness and explainability of our proposed approach.", "published": "2020-10-03 04:03:52", "link": "http://arxiv.org/abs/2010.01272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNISON: Unpaired Cross-lingual Image Captioning", "abstract": "Image captioning has emerged as an interesting research field in recent years\ndue to its broad application scenarios. The traditional paradigm of image\ncaptioning relies on paired image-caption datasets to train the model in a\nsupervised manner. However, creating such paired datasets for every target\nlanguage is prohibitively expensive, which hinders the extensibility of\ncaptioning technology and deprives a large part of the world population of its\nbenefit. In this work, we present a novel unpaired cross-lingual method to\ngenerate image captions without relying on any caption corpus in the source or\nthe target language. Specifically, our method consists of two phases: (i) a\ncross-lingual auto-encoding process, which utilizing a sentence parallel\n(bitext) corpus to learn the mapping from the source to the target language in\nthe scene graph encoding space and decode sentences in the target language, and\n(ii) a cross-modal unsupervised feature mapping, which seeks to map the encoded\nscene graph features from image modality to language modality. We verify the\neffectiveness of our proposed method on the Chinese image caption generation\ntask. The comparisons against several existing methods demonstrate the\neffectiveness of our approach.", "published": "2020-10-03 06:14:06", "link": "http://arxiv.org/abs/2010.01288v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Geometry-Inspired Attack for Generating Natural Language Adversarial\n  Examples", "abstract": "Generating adversarial examples for natural language is hard, as natural\nlanguage consists of discrete symbols, and examples are often of variable\nlengths. In this paper, we propose a geometry-inspired attack for generating\nnatural language adversarial examples. Our attack generates adversarial\nexamples by iteratively approximating the decision boundary of Deep Neural\nNetworks (DNNs). Experiments on two datasets with two different models show\nthat our attack fools natural language models with high success rates, while\nonly replacing a few words. Human evaluation shows that adversarial examples\ngenerated by our attack are hard for humans to recognize. Further experiments\nshow that adversarial training can improve model robustness against our attack.", "published": "2020-10-03 12:58:47", "link": "http://arxiv.org/abs/2010.01345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code to Comment \"Translation\": Data, Metrics, Baselining & Evaluation", "abstract": "The relationship of comments to code, and in particular, the task of\ngenerating useful comments given the code, has long been of interest. The\nearliest approaches have been based on strong syntactic theories of\ncomment-structures, and relied on textual templates. More recently, researchers\nhave applied deep learning methods to this task, and specifically, trainable\ngenerative translation models which are known to work very well for Natural\nLanguage translation (e.g., from German to English). We carefully examine the\nunderlying assumption here: that the task of generating comments sufficiently\nresembles the task of translating between natural languages, and so similar\nmodels and evaluation metrics could be used. We analyze several recent\ncode-comment datasets for this task: CodeNN, DeepCom, FunCom, and DocString. We\ncompare them with WMT19, a standard dataset frequently used to train state of\nthe art natural language translators. We found some interesting differences\nbetween the code-comment data and the WMT19 natural language data. Next, we\ndescribe and conduct some studies to calibrate BLEU (which is commonly used as\na measure of comment quality). using \"affinity pairs\" of methods, from\ndifferent projects, in the same project, in the same class, etc; Our study\nsuggests that the current performance on some datasets might need to be\nimproved substantially. We also argue that fairly naive information retrieval\n(IR) methods do well enough at this task to be considered a reasonable\nbaseline. Finally, we make some suggestions on how our findings might be used\nin future research in this area.", "published": "2020-10-03 18:57:26", "link": "http://arxiv.org/abs/2010.01410v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Semantic Role Labeling Guided Multi-turn Dialogue ReWriter", "abstract": "For multi-turn dialogue rewriting, the capacity of effectively modeling the\nlinguistic knowledge in dialog context and getting rid of the noises is\nessential to improve its performance. Existing attentive models attend to all\nwords without prior focus, which results in inaccurate concentration on some\ndispensable words. In this paper, we propose to use semantic role labeling\n(SRL), which highlights the core semantic information of who did what to whom,\nto provide additional guidance for the rewriter model. Experiments show that\nthis information significantly improves a RoBERTa-based model that already\noutperforms previous state-of-the-art systems.", "published": "2020-10-03 19:50:04", "link": "http://arxiv.org/abs/2010.01417v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Aspect-Based Sentiment Analysis in Education Domain", "abstract": "Analysis of a large amount of data has always brought value to institutions\nand organizations. Lately, people's opinions expressed through text have become\na very important aspect of this analysis. In response to this challenge, a\nnatural language processing technique known as Aspect-Based Sentiment Analysis\n(ABSA) has emerged. Having the ability to extract the polarity for each aspect\nof opinions separately, ABSA has found itself useful in a wide range of\ndomains. Education is one of the domains in which ABSA can be successfully\nutilized. Being able to understand and find out what students like and don't\nlike most about a course, professor, or teaching methodology can be of great\nimportance for the respective institutions. While this task represents a unique\nNLP challenge, many studies have proposed different approaches to tackle the\nproblem. In this work, we present a comprehensive review of the existing work\nin ABSA with a focus in the education domain. A wide range of methodologies are\ndiscussed and conclusions are drawn.", "published": "2020-10-03 21:51:47", "link": "http://arxiv.org/abs/2010.01429v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personality Trait Detection Using Bagged SVM over BERT Word Embedding\n  Ensembles", "abstract": "Recently, the automatic prediction of personality traits has received\nincreasing attention and has emerged as a hot topic within the field of\naffective computing. In this work, we present a novel deep learning-based\napproach for automated personality detection from text. We leverage state of\nthe art advances in natural language understanding, namely the BERT language\nmodel to extract contextualized word embeddings from textual data for automated\nauthor personality detection. Our primary goal is to develop a computationally\nefficient, high-performance personality prediction model which can be easily\nused by a large number of people without access to huge computation resources.\nOur extensive experiments with this ideology in mind, led us to develop a novel\nmodel which feeds contextualized embeddings along with psycholinguistic\nfeatures toa Bagged-SVM classifier for personality trait prediction. Our model\noutperforms the previous state of the art by 1.04% and, at the same time is\nsignificantly more computationally efficient to train. We report our results on\nthe famous gold standard Essays dataset for personality detection.", "published": "2020-10-03 09:25:51", "link": "http://arxiv.org/abs/2010.01309v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CardioXNet: A Novel Lightweight Deep Learning Framework for\n  Cardiovascular Disease Classification Using Heart Sound Recordings", "abstract": "The alarmingly high mortality rate and increasing global prevalence of\ncardiovascular diseases signify the crucial need for early detection schemes.\nPhonocardiogram (PCG) signals have been historically applied in this domain\nowing to its simplicity and cost-effectiveness. In this paper, we propose\nCardioXNet, a novel lightweight end-to-end CRNN architecture for automatic\ndetection of five classes of cardiac auscultation namely normal, aortic\nstenosis, mitral stenosis, mitral regurgitation and mitral valve prolapse using\nraw PCG signal. The process has been automated by the involvement of two\nlearning phases. Three parallel CNN pathways have been implemented in the\nrepresentation learning phase to learn the coarse and fine-grained features\nfrom the PCG and to explore the salient features from variable receptive fields\ninvolving 2D-CNN based squeeze-expansion. Thus, in the representation learning\nphase, the network extracts efficient time-invariant features and converges\nwith great rapidity. In the sequential residual learning phase, with the\nbidirectional-LSTMs and the skip connection, the network can proficiently\nextract temporal features without performing any feature extraction on the\nsignal. The obtained results demonstrate that the proposed end-to-end\narchitecture yields outstanding performance in all the evaluation metrics\ncompared to the previous state-of-the-art methods with up to 99.60% accuracy,\n99.56% precision, 99.52% recall and 99.68% F1- score on an average while being\ncomputationally comparable. This model outperforms the previous works using the\nsame dataset by a considerable margin. The high accuracy metrics on both\nprimary and secondary dataset combined with a significantly low number of\nparameters and end-to-end prediction approach makes the proposed network\nsuitable for point of care CVD screening in low resource setups using memory\nconstraint mobile devices.", "published": "2020-10-03 17:07:42", "link": "http://arxiv.org/abs/2010.01392v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of\n  Alzheimer's Dementia", "abstract": "Reliability in Neural Networks (NNs) is crucial in safety-critical\napplications like healthcare, and uncertainty estimation is a widely researched\nmethod to highlight the confidence of NNs in deployment. In this work, we\npropose an uncertainty-aware boosting technique for multi-modal ensembling to\npredict Alzheimer's Dementia Severity. The propagation of uncertainty across\nacoustic, cognitive, and linguistic features produces an ensemble system robust\nto heteroscedasticity in the data. Weighing the different modalities based on\nthe uncertainty estimates, we experiment on the benchmark ADReSS dataset, a\nsubject-independent and balanced dataset, to show that our method outperforms\nthe state-of-the-art methods while also reducing the overall entropy of the\nsystem. This work aims to encourage fair and aware models. The source code is\navailable at https://github.com/wazeerzulfikar/alzheimers-dementia", "published": "2020-10-03 23:08:00", "link": "http://arxiv.org/abs/2010.01440v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.LG"}
