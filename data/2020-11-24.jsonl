{"title": "Acoustic span embeddings for multilingual query-by-example search", "abstract": "Query-by-example (QbE) speech search is the task of matching spoken queries\nto utterances within a search collection. In low- or zero-resource settings,\nQbE search is often addressed with approaches based on dynamic time warping\n(DTW). Recent work has found that methods based on acoustic word embeddings\n(AWEs) can improve both performance and search speed. However, prior work on\nAWE-based QbE has primarily focused on English data and with single-word\nqueries. In this work, we generalize AWE training to spans of words, producing\nacoustic span embeddings (ASE), and explore the application of ASE to QbE with\narbitrary-length queries in multiple unseen languages. We consider the commonly\nused setting where we have access to labeled data in other languages (in our\ncase, several low-resource languages) distinct from the unseen test languages.\nWe evaluate our approach on the QUESST 2015 QbE tasks, finding that\nmultilingual ASE-based search is much faster than DTW-based search and\noutperforms the best previously published results on this task.", "published": "2020-11-24 00:28:22", "link": "http://arxiv.org/abs/2011.11807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GLGE: A New General Language Generation Evaluation Benchmark", "abstract": "Multi-task benchmarks such as GLUE and SuperGLUE have driven great progress\nof pretraining and transfer learning in Natural Language Processing (NLP).\nThese benchmarks mostly focus on a range of Natural Language Understanding\n(NLU) tasks, without considering the Natural Language Generation (NLG) models.\nIn this paper, we present the General Language Generation Evaluation (GLGE), a\nnew multi-task benchmark for evaluating the generalization capabilities of NLG\nmodels across eight language generation tasks. For each task, we continue to\ndesign three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and\nGLGE-Hard). This introduces 24 subtasks to comprehensively compare model\nperformance. To encourage research on pretraining and transfer learning on NLG\nmodels, we make GLGE publicly available and build a leaderboard with strong\nbaselines including MASS, BART, and ProphetNet (The source code and dataset are\npublicly available at https://github.com/microsoft/glge).", "published": "2020-11-24 06:59:45", "link": "http://arxiv.org/abs/2011.11928v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Picking BERT's Brain: Probing for Linguistic Dependencies in\n  Contextualized Embeddings Using Representational Similarity Analysis", "abstract": "As the name implies, contextualized representations of language are typically\nmotivated by their ability to encode context. Which aspects of context are\ncaptured by such representations? We introduce an approach to address this\nquestion using Representational Similarity Analysis (RSA). As case studies, we\ninvestigate the degree to which a verb embedding encodes the verb's subject, a\npronoun embedding encodes the pronoun's antecedent, and a full-sentence\nrepresentation encodes the sentence's head word (as determined by a dependency\nparse). In all cases, we show that BERT's contextualized embeddings reflect the\nlinguistic dependency being studied, and that BERT encodes these dependencies\nto a greater degree than it encodes less linguistically-salient controls. These\nresults demonstrate the ability of our approach to adjudicate between\nhypotheses about which aspects of context are encoded in representations of\nlanguage.", "published": "2020-11-24 13:19:06", "link": "http://arxiv.org/abs/2011.12073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unequal Representations: Analyzing Intersectional Biases in Word\n  Embeddings Using Representational Similarity Analysis", "abstract": "We present a new approach for detecting human-like social biases in word\nembeddings using representational similarity analysis. Specifically, we probe\ncontextualized and non-contextualized embeddings for evidence of intersectional\nbiases against Black women. We show that these embeddings represent Black women\nas simultaneously less feminine than White women, and less Black than Black\nmen. This finding aligns with intersectionality theory, which argues that\nmultiple identity categories (such as race or sex) layer on top of each other\nin order to create unique modes of discrimination that are not shared by any\nindividual category.", "published": "2020-11-24 13:45:14", "link": "http://arxiv.org/abs/2011.12086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Transferable Method for Named Entity Recognition Task", "abstract": "Named Entity Recognition (NER) is a fundamental task in the fields of natural\nlanguage processing and information extraction. NER has been widely used as a\nstandalone tool or an essential component in a variety of applications such as\nquestion answering, dialogue assistants and knowledge graphs development.\nHowever, training reliable NER models requires a large amount of labelled data\nwhich is expensive to obtain, particularly in specialized domains. This paper\ndescribes a method to learn a domain-specific NER model for an arbitrary set of\nnamed entities when domain-specific supervision is not available. We assume\nthat the supervision can be obtained with no human effort, and neural models\ncan learn from each other. The code, data and models are publicly available.", "published": "2020-11-24 15:45:52", "link": "http://arxiv.org/abs/2011.12170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Intelligible Plumitifs Descriptions: Use Case Application\n  with Ethical Considerations", "abstract": "Plumitifs (dockets) were initially a tool for law clerks. Nowadays, they are\nused as summaries presenting all the steps of a judicial case. Information\nconcerning parties' identity, jurisdiction in charge of administering the case,\nand some information relating to the nature and the course of the preceding are\navailable through plumitifs. They are publicly accessible but barely\nunderstandable; they are written using abbreviations and referring to\nprovisions from the Criminal Code of Canada, which makes them hard to reason\nabout. In this paper, we propose a simple yet efficient multi-source language\ngeneration architecture that leverages both the plumitif and the Criminal\nCode's content to generate intelligible plumitifs descriptions. It goes without\nsaying that ethical considerations rise with these sensitive documents made\nreadable and available at scale, legitimate concerns that we address in this\npaper.", "published": "2020-11-24 16:02:36", "link": "http://arxiv.org/abs/2011.12183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Text Classification by Jointly Learning to Cluster and Align", "abstract": "Distributional text clustering delivers semantically informative\nrepresentations and captures the relevance between each word and semantic\nclustering centroids. We extend the neural text clustering approach to text\nclassification tasks by inducing cluster centers via a latent variable model\nand interacting with distributional word embeddings, to enrich the\nrepresentation of tokens and measure the relatedness between tokens and each\nlearnable cluster centroid. The proposed method jointly learns word clustering\ncentroids and clustering-token alignments, achieving the state of the art\nresults on multiple benchmark datasets and proving that the proposed\ncluster-token alignment mechanism is indeed favorable to text classification.\nNotably, our qualitative analysis has conspicuously illustrated that text\nrepresentations learned by the proposed model are in accord well with our\nintuition.", "published": "2020-11-24 16:07:18", "link": "http://arxiv.org/abs/2011.12184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizing Cross-Document Event Coreference Resolution Across Multiple\n  Corpora", "abstract": "Cross-document event coreference resolution (CDCR) is an NLP task in which\nmentions of events need to be identified and clustered throughout a collection\nof documents. CDCR aims to benefit downstream multi-document applications, but\ndespite recent progress on corpora and system development, downstream\nimprovements from applying CDCR have not been shown yet. We make the\nobservation that every CDCR system to date was developed, trained, and tested\nonly on a single respective corpus. This raises strong concerns on their\ngeneralizability -- a must-have for downstream applications where the magnitude\nof domains or event mentions is likely to exceed those found in a curated\ncorpus. To investigate this assumption, we define a uniform evaluation setup\ninvolving three CDCR corpora: ECB+, the Gun Violence Corpus and the Football\nCoreference Corpus (which we reannotate on token level to make our analysis\npossible). We compare a corpus-independent, feature-based system against a\nrecent neural system developed for ECB+. Whilst being inferior in absolute\nnumbers, the feature-based system shows more consistent performance across all\ncorpora whereas the neural system is hit-and-miss. Via model introspection, we\nfind that the importance of event actions, event time, etc. for resolving\ncoreference in practice varies greatly between the corpora. Additional analysis\nshows that several systems overfit on the structure of the ECB+ corpus. We\nconclude with recommendations on how to achieve generally applicable CDCR\nsystems in the future -- the most important being that evaluation on multiple\nCDCR corpora is strongly necessary. To facilitate future research, we release\nour dataset, annotation guidelines, and system implementation to the public.", "published": "2020-11-24 17:45:03", "link": "http://arxiv.org/abs/2011.12249v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Pattern-mining Driven Study on Differences of Newspapers in Expressing\n  Temporal Information", "abstract": "This paper studies the differences between different types of newspapers in\nexpressing temporal information, which is a topic that has not received much\nattention. Techniques from the fields of temporal processing and pattern mining\nare employed to investigate this topic. First, a corpus annotated with temporal\ninformation is created by the author. Then, sequences of temporal information\ntags mixed with part-of-speech tags are extracted from the corpus. The TKS\nalgorithm is used to mine skip-gram patterns from the sequences. With these\npatterns, the signatures of the four newspapers are obtained. In order to make\nthe signatures uniquely characterize the newspapers, we revise the signatures\nby removing reference patterns. Through examining the number of patterns in the\nsignatures and revised signatures, the proportion of patterns containing\ntemporal information tags and the specific patterns containing temporal\ninformation tags, it is found that newspapers differ in ways of expressing\ntemporal information.", "published": "2020-11-24 18:20:24", "link": "http://arxiv.org/abs/2011.12265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing deep neural networks with morphological information", "abstract": "Deep learning approaches are superior in NLP due to their ability to extract\ninformative features and patterns from languages. The two most successful\nneural architectures are LSTM and transformers, used in large pretrained\nlanguage models such as BERT. While cross-lingual approaches are on the rise,\nmost current NLP techniques are designed and applied to English, and\nless-resourced languages are lagging behind. In morphologically rich languages,\ninformation is conveyed through morphology, e.g., through affixes modifying\nstems of words. Existing neural approaches do not explicitly use the\ninformation on word morphology. We analyse the effect of adding morphological\nfeatures to LSTM and BERT models. As a testbed, we use three tasks available in\nmany less-resourced languages: named entity recognition (NER), dependency\nparsing (DP), and comment filtering (CF). We construct baselines involving LSTM\nand BERT models, which we adjust by adding additional input in the form of part\nof speech (POS) tags and universal features. We compare models across several\nlanguages from different language families. Our results suggest that adding\nmorphological features has mixed effects depending on the quality of features\nand the task. The features improve the performance of LSTM-based models on the\nNER and DP tasks, while they do not benefit the performance on the CF task. For\nBERT-based models, the morphological features only improve the performance on\nDP when they are of high quality while not showing practical improvement when\nthey are predicted. Even for high-quality features, the improvements are less\npronounced in language-specific BERT variants compared to massively\nmultilingual BERT models. As in NER and CF datasets manually checked features\nare not available, we only experiment with predicted features and find that\nthey do not cause any practical improvement in performance.", "published": "2020-11-24 22:35:44", "link": "http://arxiv.org/abs/2011.12432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Supervision Framework for Relation Extraction with Distant\n  Supervision and Human Annotation", "abstract": "Relation extraction (RE) has been extensively studied due to its importance\nin real-world applications such as knowledge base construction and question\nanswering. Most of the existing works train the models on either distantly\nsupervised data or human-annotated data. To take advantage of the high accuracy\nof human annotation and the cheap cost of distant supervision, we propose the\ndual supervision framework which effectively utilizes both types of data.\nHowever, simply combining the two types of data to train a RE model may\ndecrease the prediction accuracy since distant supervision has labeling bias.\nWe employ two separate prediction networks HA-Net and DS-Net to predict the\nlabels by human annotation and distant supervision, respectively, to prevent\nthe degradation of accuracy by the incorrect labeling of distant supervision.\nFurthermore, we propose an additional loss term called disagreement penalty to\nenable HA-Net to learn from distantly supervised labels. In addition, we\nexploit additional networks to adaptively assess the labeling bias by\nconsidering contextual information. Our performance study on sentence-level and\ndocument-level REs confirms the effectiveness of the dual supervision\nframework.", "published": "2020-11-24 02:35:24", "link": "http://arxiv.org/abs/2011.11851v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Robotic Dating Coaching System Leveraging Online Communities Posts", "abstract": "Can a robot be a personal dating coach? Even with the increasing amount of\nconversational data on the internet, the implementation of conversational\nrobots remains a challenge. In particular, a detailed and professional\ncounseling log is expensive and not publicly accessible. In this paper, we\ndevelop a robot dating coaching system leveraging corpus from online\ncommunities. We examine people's perceptions of the dating coaching robot with\na dialogue module. 97 participants joined to have a conversation with the\nrobot, and 30 of them evaluated the robot. The results indicate that\nparticipants thought the robot could become a dating coach while considering\nthe robot is entertaining rather than helpful.", "published": "2020-11-24 02:46:02", "link": "http://arxiv.org/abs/2011.11855v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Argument from Old Man's View: Assessing Social Bias in Argumentation", "abstract": "Social bias in language - towards genders, ethnicities, ages, and other\nsocial groups - poses a problem with ethical impact for many NLP applications.\nRecent research has shown that machine learning models trained on respective\ndata may not only adopt, but even amplify the bias. So far, however, little\nattention has been paid to bias in computational argumentation. In this paper,\nwe study the existence of social biases in large English debate portals. In\nparticular, we train word embedding models on portal-specific corpora and\nsystematically evaluate their bias using WEAT, an existing metric to measure\nbias in word embeddings. In a word co-occurrence analysis, we then investigate\ncauses of bias. The results suggest that all tested debate corpora contain\nunbalanced and biased data, mostly in favor of male people with\nEuropean-American names. Our empirical insights contribute towards an\nunderstanding of bias in argumentative data sources.", "published": "2020-11-24 10:39:44", "link": "http://arxiv.org/abs/2011.12014v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Fuzzy Stochastic Timed Petri Nets for Causal properties representation", "abstract": "Imagery is frequently used to model, represent and communicate knowledge. In\nparticular, graphs are one of the most powerful tools, being able to represent\nrelations between objects. Causal relations are frequently represented by\ndirected graphs, with nodes denoting causes and links denoting causal\ninfluence. A causal graph is a skeletal picture, showing causal associations\nand impact between entities. Common methods used for graphically representing\ncausal scenarios are neurons, truth tables, causal Bayesian networks, cognitive\nmaps and Petri Nets. Causality is often defined in terms of precedence (the\ncause precedes the effect), concurrency (often, an effect is provoked\nsimultaneously by two or more causes), circularity (a cause provokes the effect\nand the effect reinforces the cause) and imprecision (the presence of the cause\nfavors the effect, but not necessarily causes it). We will show that, even\nthough the traditional graphical models are able to represent separately some\nof the properties aforementioned, they fail trying to illustrate indistinctly\nall of them. To approach that gap, we will introduce Fuzzy Stochastic Timed\nPetri Nets as a graphical tool able to represent time, co-occurrence, looping\nand imprecision in causal flow.", "published": "2020-11-24 13:22:34", "link": "http://arxiv.org/abs/2011.12075v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Tackling Domain-Specific Winograd Schemas with Knowledge-Based Reasoning\n  and Machine Learning", "abstract": "The Winograd Schema Challenge (WSC) is a common-sense reasoning task that\nrequires background knowledge. In this paper, we contribute to tackling WSC in\nfour ways. Firstly, we suggest a keyword method to define a restricted domain\nwhere distinctive high-level semantic patterns can be found. A thanking domain\nwas defined by key-words, and the data set in this domain is used in our\nexperiments. Secondly, we develop a high-level knowledge-based reasoning method\nusing semantic roles which is based on the method of Sharma [2019]. Thirdly, we\npropose an ensemble method to combine knowledge-based reasoning and machine\nlearning which shows the best performance in our experiments. As a machine\nlearning method, we used Bidirectional Encoder Representations from\nTransformers (BERT) [Kocijan et al., 2019]. Lastly, in terms of evaluation, we\nsuggest a \"robust\" accuracy measurement by modifying that of Trichelair et al.\n[2018]. As with their switching method, we evaluate a model by considering its\nperformance on trivial variants of each sentence in the test set.", "published": "2020-11-24 13:34:38", "link": "http://arxiv.org/abs/2011.12081v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gender bias in magazines oriented to men and women: a computational\n  approach", "abstract": "Cultural products are a source to acquire individual values and behaviours.\nTherefore, the differences in the content of the magazines aimed specifically\nat women or men are a means to create and reproduce gender stereotypes. In this\nstudy, we compare the content of a women-oriented magazine with that of a\nmen-oriented one, both produced by the same editorial group, over a decade\n(2008-2018). With Topic Modelling techniques we identify the main themes\ndiscussed in the magazines and quantify how much the presence of these topics\ndiffers between magazines over time. Then, we performed a word-frequency\nanalysis to validate this methodology and extend the analysis to other subjects\nthat did not emerge automatically. Our results show that the frequency of\nappearance of the topics Family, Business and Women as sex objects, present an\ninitial bias that tends to disappear over time. Conversely, in Fashion and\nScience topics, the initial differences between both magazines are maintained.\nBesides, we show that in 2012, the content associated with horoscope increased\nin the women-oriented magazine, generating a new gap that remained open over\ntime. Also, we show a strong increase in the use of words associated with\nfeminism since 2015 and specifically the word abortion in 2018. Overall, these\ncomputational tools allowed us to analyse more than 24,000 articles. Up to our\nknowledge, this is the first study to compare magazines in such a large\ndataset, a task that would have been prohibitive using manual content analysis\nmethodologies.", "published": "2020-11-24 14:02:49", "link": "http://arxiv.org/abs/2011.12096v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Two-Way Neural Machine Translation: A Proof of Concept for Bidirectional\n  Translation Modeling using a Two-Dimensional Grid", "abstract": "Neural translation models have proven to be effective in capturing sufficient\ninformation from a source sentence and generating a high-quality target\nsentence. However, it is not easy to get the best effect for bidirectional\ntranslation, i.e., both source-to-target and target-to-source translation using\na single model. If we exclude some pioneering attempts, such as multilingual\nsystems, all other bidirectional translation approaches are required to train\ntwo individual models. This paper proposes to build a single end-to-end\nbidirectional translation model using a two-dimensional grid, where the\nleft-to-right decoding generates source-to-target, and the bottom-to-up\ndecoding creates target-to-source output. Instead of training two models\nindependently, our approach encourages a single network to jointly learn to\ntranslate in both directions. Experiments on the WMT 2018\nGerman$\\leftrightarrow$English and Turkish$\\leftrightarrow$English translation\ntasks show that the proposed model is capable of generating a good translation\nquality and has sufficient potential to direct the research.", "published": "2020-11-24 15:42:32", "link": "http://arxiv.org/abs/2011.12165v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tight Integrated End-to-End Training for Cascaded Speech Translation", "abstract": "A cascaded speech translation model relies on discrete and non-differentiable\ntranscription, which provides a supervision signal from the source side and\nhelps the transformation between source speech and target text. Such modeling\nsuffers from error propagation between ASR and MT models. Direct speech\ntranslation is an alternative method to avoid error propagation; however, its\nperformance is often behind the cascade system. To use an intermediate\nrepresentation and preserve the end-to-end trainability, previous studies have\nproposed using two-stage models by passing the hidden vectors of the recognizer\ninto the decoder of the MT model and ignoring the MT encoder. This work\nexplores the feasibility of collapsing the entire cascade components into a\nsingle end-to-end trainable model by optimizing all parameters of ASR and MT\nmodels jointly without ignoring any learned parameters. It is a tightly\nintegrated method that passes renormalized source word posterior distributions\nas a soft decision instead of one-hot vectors and enables backpropagation.\nTherefore, it provides both transcriptions and translations and achieves strong\nconsistency between them. Our experiments on four tasks with different data\nscenarios show that the model outperforms cascade models up to 1.8% in BLEU and\n2.0% in TER and is superior compared to direct models.", "published": "2020-11-24 15:43:49", "link": "http://arxiv.org/abs/2011.12167v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Generation via Combinatorial Constraint Satisfaction: A Tree\n  Search Enhanced Monte-Carlo Approach", "abstract": "Generating natural language under complex constraints is a principled\nformulation towards controllable text generation. We present a framework to\nallow specification of combinatorial constraints for sentence generation. We\npropose TSMH, an efficient method to generate high likelihood sentences with\nrespect to a pre-trained language model while satisfying the constraints. Our\napproach is highly flexible, requires no task-specific training, and leverages\nefficient constraint satisfaction solving techniques. To better handle the\ncombinatorial constraints, a tree search algorithm is embedded into the\nproposal process of the Markov chain Monte Carlo (MCMC) to explore candidates\nthat satisfy more constraints. Compared to existing MCMC approaches, our\nsampling approach has a better mixing performance. Experiments show that TSMH\nachieves consistent and significant improvement on multiple language generation\ntasks.", "published": "2020-11-24 19:21:00", "link": "http://arxiv.org/abs/2011.12334v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Experiments on transfer learning architectures for biomedical relation\n  extraction", "abstract": "Relation extraction (RE) consists in identifying and structuring\nautomatically relations of interest from texts. Recently, BERT improved the top\nperformances for several NLP tasks, including RE. However, the best way to use\nBERT, within a machine learning architecture, and within a transfer learning\nstrategy is still an open question since it is highly dependent on each\nspecific task and domain. Here, we explore various BERT-based architectures and\ntransfer learning strategies (i.e., frozen or fine-tuned) for the task of\nbiomedical RE on two corpora. Among tested architectures and strategies, our\n*BERT-segMCNN with finetuning reaches performances higher than the\nstate-of-the-art on the two corpora (1.73 % and 32.77 % absolute improvement on\nChemProt and PGxCorpus corpora respectively). More generally, our experiments\nillustrate the expected interest of fine-tuning with BERT, but also the\nunexplored advantage of using structural information (with sentence\nsegmentation), in addition to the context classically leveraged by BERT.", "published": "2020-11-24 20:56:47", "link": "http://arxiv.org/abs/2011.12380v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Clinical Outcome Predictions Using Convolution over Medical\n  Entities with Multimodal Learning", "abstract": "Early prediction of mortality and length of stay(LOS) of a patient is vital\nfor saving a patient's life and management of hospital resources. Availability\nof electronic health records(EHR) makes a huge impact on the healthcare domain\nand there has seen several works on predicting clinical problems. However, many\nstudies did not benefit from the clinical notes because of the sparse, and high\ndimensional nature. In this work, we extract medical entities from clinical\nnotes and use them as additional features besides time-series features to\nimprove our predictions. We propose a convolution based multimodal\narchitecture, which not only learns effectively combining medical entities and\ntime-series ICU signals of patients, but also allows us to compare the effect\nof different embedding techniques such as Word2vec, FastText on medical\nentities. In the experiments, our proposed method robustly outperforms all\nother baseline models including different multimodal architectures for all\nclinical tasks. The code for the proposed method is available at\nhttps://github.com/tanlab/ConvolutionMedicalNer.", "published": "2020-11-24 20:08:39", "link": "http://arxiv.org/abs/2011.12349v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Towards Zero-shot Cross-lingual Image Retrieval", "abstract": "There has been a recent spike in interest in multi-modal Language and Vision\nproblems. On the language side, most of these models primarily focus on English\nsince most multi-modal datasets are monolingual. We try to bridge this gap with\na zero-shot approach for learning multi-modal representations using\ncross-lingual pre-training on the text side. We present a simple yet practical\napproach for building a cross-lingual image retrieval model which trains on a\nmonolingual training dataset but can be used in a zero-shot cross-lingual\nfashion during inference. We also introduce a new objective function which\ntightens the text embedding clusters by pushing dissimilar texts from each\nother. Finally, we introduce a new 1K multi-lingual MSCOCO2014 caption test\ndataset (XTD10) in 7 languages that we collected using a crowdsourcing\nplatform. We use this as the test set for evaluating zero-shot model\nperformance across languages. XTD10 dataset is made publicly available here:\nhttps://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10", "published": "2020-11-24 22:13:21", "link": "http://arxiv.org/abs/2012.05107v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ensemble Distillation Approaches for Grammatical Error Correction", "abstract": "Ensemble approaches are commonly used techniques to improving a system by\ncombining multiple model predictions. Additionally these schemes allow the\nuncertainty, as well as the source of the uncertainty, to be derived for the\nprediction. Unfortunately these benefits come at a computational and memory\ncost. To address this problem ensemble distillation (EnD) and more recently\nensemble distribution distillation (EnDD) have been proposed that compress the\nensemble into a single model, representing either the ensemble average\nprediction or prediction distribution respectively. This paper examines the\napplication of both these distillation approaches to a sequence prediction\ntask, grammatical error correction (GEC). This is an important application area\nfor language learning tasks as it can yield highly useful feedback to the\nlearner. It is, however, more challenging than the standard tasks investigated\nfor distillation as the prediction of any grammatical correction to a word will\nbe highly dependent on both the input sequence and the generated output history\nfor the word. The performance of both EnD and EnDD are evaluated on both\npublicly available GEC tasks as well as a spoken language task.", "published": "2020-11-24 15:00:45", "link": "http://arxiv.org/abs/2012.07535v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integration of variational autoencoder and spatial clustering for\n  adaptive multi-channel neural speech separation", "abstract": "In this paper, we propose a method combining variational autoencoder model of\nspeech with a spatial clustering approach for multi-channel speech separation.\nThe advantage of integrating spatial clustering with a spectral model was shown\nin several works. As the spectral model, previous works used either factorial\ngenerative models of the mixed speech or discriminative neural networks. In our\nwork, we combine the strengths of both approaches, by building a factorial\nmodel based on a generative neural network, a variational autoencoder. By doing\nso, we can exploit the modeling power of neural networks, but at the same time,\nkeep a structured model. Such a model can be advantageous when adapting to new\nnoise conditions as only the noise part of the model needs to be modified. We\nshow experimentally, that our model significantly outperforms previous\nfactorial model based on Gaussian mixture model (DOLPHIN), performs comparably\nto integration of permutation invariant training with spatial clustering, and\nenables us to easily adapt to new noise conditions. The code for the method is\navailable at https://github.com/BUTSpeechFIT/vae_dolphin", "published": "2020-11-24 09:28:46", "link": "http://arxiv.org/abs/2011.11984v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Audio Classification via Semantic Embeddings", "abstract": "In this paper, we study zero-shot learning in audio classification via\nsemantic embeddings extracted from textual labels and sentence descriptions of\nsound classes. Our goal is to obtain a classifier that is capable of\nrecognizing audio instances of sound classes that have no available training\nsamples, but only semantic side information. We employ a bilinear compatibility\nframework to learn an acoustic-semantic projection between intermediate-level\nrepresentations of audio instances and sound classes, i.e., acoustic embeddings\nand semantic embeddings. We use VGGish to extract deep acoustic embeddings from\naudio clips, and pre-trained language models (Word2Vec, GloVe, BERT) to\ngenerate either label embeddings from textual labels or sentence embeddings\nfrom sentence descriptions of sound classes. Audio classification is performed\nby a linear compatibility function that measures how compatible an acoustic\nembedding and a semantic embedding are. We evaluate the proposed method on a\nsmall balanced dataset ESC-50 and a large-scale unbalanced audio subset of\nAudioSet. The experimental results show that classification performance is\nsignificantly improved by involving sound classes that are semantically close\nto the test classes in training. Meanwhile, we demonstrate that both label\nembeddings and sentence embeddings are useful for zero-shot learning.\nClassification performance is improved by concatenating label/sentence\nembeddings generated with different language models. With their hybrid\nconcatenations, the results are improved further.", "published": "2020-11-24 14:42:22", "link": "http://arxiv.org/abs/2011.12133v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TFGAN: Time and Frequency Domain Based Generative Adversarial Network\n  for High-fidelity Speech Synthesis", "abstract": "Recently, GAN based speech synthesis methods, such as MelGAN, have become\nvery popular. Compared to conventional autoregressive based methods, parallel\nstructures based generators make waveform generation process fast and stable.\nHowever, the quality of generated speech by autoregressive based neural\nvocoders, such as WaveRNN, is still higher than GAN. To address this issue, we\npropose a novel vocoder model: TFGAN, which is adversarially learned both in\ntime and frequency domain. On one hand, we propose to discriminate ground-truth\nwaveform from synthetic one in frequency domain for offering more consistency\nguarantees instead of only in time domain. On the other hand, in contrast to\nthe conventionally frequency-domain STFT loss approach or feature map loss by\ndiscriminator to learn waveform, we propose a set of time-domain loss that\nencourage the generator to capture the waveform directly. TFGAN has nearly same\nsynthesis speed as MelGAN, but the fidelity is significantly improved by our\nnovel learning method. In our experiments, TFGAN shows the ability to achieve\ncomparable mean opinion score (MOS) than autoregressive vocoder under speech\nsynthesis context.", "published": "2020-11-24 16:55:48", "link": "http://arxiv.org/abs/2011.12206v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A light transformer for speech-to-intent applications", "abstract": "Spoken language understanding (SLU) systems can make life more agreeable,\nsafer (e.g. in a car) or can increase the independence of physically challenged\nusers. However, due to the many sources of variation in speech, a well-trained\nsystem is hard to transfer to other conditions like a different language or to\nspeech impaired users. A remedy is to design a user-taught SLU system that can\nlearn fully from scratch from users' demonstrations, which in turn requires\nthat the system's model quickly converges after only a few training samples. In\nthis paper, we propose a light transformer structure by using a simplified\nrelative position encoding with the goal to reduce the model size and improve\nefficiency. The light transformer works as an alternative speech encoder for an\nexisting user-taught multitask SLU system. Experimental results on three\ndatasets with challenging speech conditions prove our approach outperforms the\nexisted system and other state-of-art models with half of the original model\nsize and training time.", "published": "2020-11-24 17:13:27", "link": "http://arxiv.org/abs/2011.12221v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "How Far Are We from Robust Voice Conversion: A Survey", "abstract": "Voice conversion technologies have been greatly improved in recent years with\nthe help of deep learning, but their capabilities of producing natural sounding\nutterances in different conditions remain unclear. In this paper, we gave a\nthorough study of the robustness of known VC models. We also modified these\nmodels, such as the replacement of speaker embeddings, to further improve their\nperformances. We found that the sampling rate and audio duration greatly\ninfluence voice conversion. All the VC models suffer from unseen data, but\nAdaIN-VC is relatively more robust. Also, the speaker embedding jointly trained\nis more suitable for voice conversion than those trained on speaker\nidentification.", "published": "2020-11-24 12:34:36", "link": "http://arxiv.org/abs/2011.12063v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synth2Aug: Cross-domain speaker recognition with TTS synthesized speech", "abstract": "In recent years, Text-To-Speech (TTS) has been used as a data augmentation\ntechnique for speech recognition to help complement inadequacies in the\ntraining data. Correspondingly, we investigate the use of a multi-speaker TTS\nsystem to synthesize speech in support of speaker recognition. In this study we\nfocus the analysis on tasks where a relatively small number of speakers is\navailable for training. We observe on our datasets that TTS synthesized speech\nimproves cross-domain speaker recognition performance and can be combined\neffectively with multi-style training. Additionally, we explore the\neffectiveness of different types of text transcripts used for TTS synthesis.\nResults suggest that matching the textual content of the target domain is a\ngood practice, and if that is not feasible, a transcript with a sufficiently\nlarge vocabulary is recommended.", "published": "2020-11-24 00:48:54", "link": "http://arxiv.org/abs/2011.11818v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Novel Multimodal Music Genre Classifier using Hierarchical Attention\n  and Convolutional Neural Network", "abstract": "Music genre classification is one of the trending topics in regards to the\ncurrent Music Information Retrieval (MIR) Research. Since, the dependency of\ngenre is not only limited to the audio profile, we also make use of textual\ncontent provided as lyrics of the corresponding song. We implemented a CNN\nbased feature extractor for spectrograms in order to incorporate the acoustic\nfeatures and a Hierarchical Attention Network based feature extractor for\nlyrics. We then go on to classify the music track based upon the resulting\nfused feature vector.", "published": "2020-11-24 09:02:35", "link": "http://arxiv.org/abs/2011.11970v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Decoder DPRNN: High Accuracy Source Counting and Separation", "abstract": "We propose an end-to-end trainable approach to single-channel speech\nseparation with unknown number of speakers. Our approach extends the MulCat\nsource separation backbone with additional output heads: a count-head to infer\nthe number of speakers, and decoder-heads for reconstructing the original\nsignals. Beyond the model, we also propose a metric on how to evaluate source\nseparation with variable number of speakers. Specifically, we cleared up the\nissue on how to evaluate the quality when the ground-truth hasmore or less\nspeakers than the ones predicted by the model. We evaluate our approach on the\nWSJ0-mix datasets, with mixtures up to five speakers. We demonstrate that our\napproach outperforms state-of-the-art in counting the number of speakers and\nremains competitive in quality of reconstructed signals.", "published": "2020-11-24 11:00:21", "link": "http://arxiv.org/abs/2011.12022v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
