{"title": "Retrieve-and-Fill for Scenario-based Task-Oriented Semantic Parsing", "abstract": "Task-oriented semantic parsing models have achieved strong results in recent\nyears, but unfortunately do not strike an appealing balance between model size,\nruntime latency, and cross-domain generalizability. We tackle this problem by\nintroducing scenario-based semantic parsing: a variant of the original task\nwhich first requires disambiguating an utterance's \"scenario\" (an intent-slot\ntemplate with variable leaf spans) before generating its frame, complete with\nontology and utterance tokens. This formulation enables us to isolate\ncoarse-grained and fine-grained aspects of the task, each of which we solve\nwith off-the-shelf neural modules, also optimizing for the axes outlined above.\nConcretely, we create a Retrieve-and-Fill (RAF) architecture comprised of (1) a\nretrieval module which ranks the best scenario given an utterance and (2) a\nfilling module which imputes spans into the scenario to create the frame. Our\nmodel is modular, differentiable, interpretable, and allows us to garner extra\nsupervision from scenarios. RAF achieves strong results in high-resource,\nlow-resource, and multilingual settings, outperforming recent approaches by\nwide margins despite, using base pre-trained encoders, small sequence lengths,\nand parallel decoding.", "published": "2022-02-02 08:00:21", "link": "http://arxiv.org/abs/2202.00901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Retrieval-Augmented Text Generation", "abstract": "Recently, retrieval-augmented text generation attracted increasing attention\nof the computational linguistics community. Compared with conventional\ngeneration models, retrieval-augmented text generation has remarkable\nadvantages and particularly has achieved state-of-the-art performance in many\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks\nincluding dialogue response generation, machine translation, and other\ngeneration tasks. Finally, it points out some important directions on top of\nrecent methods to facilitate future research.", "published": "2022-02-02 16:18:41", "link": "http://arxiv.org/abs/2202.01110v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relative Position Prediction as Pre-training for Text Encoders", "abstract": "Meaning is defined by the company it keeps. However, company is two-fold:\nIt's based on the identity of tokens and also on their position (topology). We\nargue that a position-centric perspective is more general and useful. The\nclassic MLM and CLM objectives in NLP are easily phrased as position\npredictions over the whole vocabulary. Adapting the relative position encoding\nparadigm in NLP to create relative labels for self-supervised learning, we seek\nto show superior pre-training judged by performance on downstream tasks.", "published": "2022-02-02 17:13:31", "link": "http://arxiv.org/abs/2202.01145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The slurk Interaction Server Framework: Better Data for Better Dialog\n  Models", "abstract": "This paper presents the slurk software, a lightweight interaction server for\nsetting up dialog data collections and running experiments. Slurk enables a\nmultitude of settings including text-based, speech and video interaction\nbetween two or more humans or humans and bots, and a multimodal display area\nfor presenting shared or private interactive context. The software is\nimplemented in Python with an HTML and JS frontend that can easily be adapted\nto individual needs. It also provides a setup for pairing participants on\ncommon crowdworking platforms such as Amazon Mechanical Turk and some example\nbot scripts for common interaction scenarios.", "published": "2022-02-02 17:30:33", "link": "http://arxiv.org/abs/2202.01155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Explain Word Reading Times Better Than Empirical\n  Predictability", "abstract": "Though there is a strong consensus that word length and frequency are the\nmost important single-word features determining visual-orthographic access to\nthe mental lexicon, there is less agreement as how to best capture syntactic\nand semantic factors. The traditional approach in cognitive reading research\nassumes that word predictability from sentence context is best captured by\ncloze completion probability (CCP) derived from human performance data. We\nreview recent research suggesting that probabilistic language models provide\ndeeper explanations for syntactic and semantic effects than CCP. Then we\ncompare CCP with (1) Symbolic n-gram models consolidate syntactic and semantic\nshort-range relations by computing the probability of a word to occur, given\ntwo preceding words. (2) Topic models rely on subsymbolic representations to\ncapture long-range semantic similarity by word co-occurrence counts in\ndocuments. (3) In recurrent neural networks (RNNs), the subsymbolic units are\ntrained to predict the next word, given all preceding words in the sentences.\nTo examine lexical retrieval, these models were used to predict single fixation\ndurations and gaze durations to capture rapidly successful and standard lexical\naccess, and total viewing time to capture late semantic integration. The linear\nitem-level analyses showed greater correlations of all language models with all\neye-movement measures than CCP. Then we examined non-linear relations between\nthe different types of predictability and the reading times using generalized\nadditive models. N-gram and RNN probabilities of the present word more\nconsistently predicted reading performance compared with topic models or CCP.", "published": "2022-02-02 16:38:43", "link": "http://arxiv.org/abs/2202.01128v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Error Correction in ASR using Sequence-to-Sequence Models", "abstract": "Post-editing in Automatic Speech Recognition (ASR) entails automatically\ncorrecting common and systematic errors produced by the ASR system. The outputs\nof an ASR system are largely prone to phonetic and spelling errors. In this\npaper, we propose to use a powerful pre-trained sequence-to-sequence model,\nBART, further adaptively trained to serve as a denoising model, to correct\nerrors of such types. The adaptive training is performed on an augmented\ndataset obtained by synthetically inducing errors as well as by incorporating\nactual errors from an existing ASR system. We also propose a simple approach to\nrescore the outputs using word level alignments. Experimental results on\naccented speech data demonstrate that our strategy effectively rectifies a\nsignificant number of ASR errors and produces improved WER results when\ncompared against a competitive baseline. We also highlight a negative result\nobtained on the related grammatical error correction task in Hindi language\nshowing the limitation in capturing wider context by our proposed model.", "published": "2022-02-02 17:32:59", "link": "http://arxiv.org/abs/2202.01157v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT\n  Language Models, and Resources", "abstract": "We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from\ndifferent internet sources. We expand the existing Marathi monolingual corpus\nwith 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT,\nand MahaRoBerta all BERT-based masked language models, and MahaFT, the fast\ntext word embeddings both trained on full Marathi corpus with 752M tokens. We\nshow the effectiveness of these resources on downstream Marathi sentiment\nanalysis, text classification, and named entity recognition (NER) tasks. We\nalso release MahaGPT, a generative Marathi GPT model trained on Marathi corpus.\nMarathi is a popular language in India but still lacks these resources. This\nwork is a step forward in building open resources for the Marathi language. The\ndata and models are available at https://github.com/l3cube-pune/MarathiNLP .", "published": "2022-02-02 17:35:52", "link": "http://arxiv.org/abs/2202.01159v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Scaling Laws for Routed Language Models", "abstract": "The performance of a language model has been shown to be effectively modeled\nas a power-law in its parameter count. Here we study the scaling behaviors of\nRouting Networks: architectures that conditionally use only a subset of their\nparameters while processing an input. For these models, parameter count and\ncomputational requirement form two independent axes along which an increase\nleads to better performance. In this work we derive and justify scaling laws\ndefined on these two variables which generalize those known for standard\nlanguage models and describe the performance of a wide range of routing\narchitectures trained via three different techniques. Afterwards we provide two\napplications of these laws: first deriving an Effective Parameter Count along\nwhich all models scale at the same rate, and then using the scaling\ncoefficients to give a quantitative comparison of the three routing techniques\nconsidered. Our analysis derives from an extensive evaluation of Routing\nNetworks across five orders of magnitude of size, including models with\nhundreds of experts and hundreds of billions of parameters.", "published": "2022-02-02 17:58:52", "link": "http://arxiv.org/abs/2202.01169v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PromptSource: An Integrated Development Environment and Repository for\n  Natural Language Prompts", "abstract": "PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.", "published": "2022-02-02 20:48:54", "link": "http://arxiv.org/abs/2202.01279v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Co-training Improves Prompt-based Learning for Large Language Models", "abstract": "We demonstrate that co-training (Blum & Mitchell, 1998) can improve the\nperformance of prompt-based learning by using unlabeled data. While prompting\nhas emerged as a promising paradigm for few-shot and zero-shot learning, it is\noften brittle and requires much larger models compared to the standard\nsupervised setup. We find that co-training makes it possible to improve the\noriginal prompt model and at the same time learn a smaller, downstream\ntask-specific model. In the case where we only have partial access to a prompt\nmodel (e.g., output probabilities from GPT-3 (Brown et al., 2020)) we learn a\ncalibration model over the prompt outputs. When we have full access to the\nprompt model's gradients but full finetuning remains prohibitively expensive\n(e.g., T0 (Sanh et al., 2021)), we learn a set of soft prompt continuous\nvectors to iteratively update the prompt model. We find that models trained in\nthis manner can significantly improve performance on challenging datasets where\nthere is currently a large gap between prompt-based learning and\nfully-supervised models.", "published": "2022-02-02 00:48:26", "link": "http://arxiv.org/abs/2202.00828v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Streaming Multi-Talker ASR with Token-Level Serialized Output Training", "abstract": "This paper proposes a token-level serialized output training (t-SOT), a novel\nframework for streaming multi-talker automatic speech recognition (ASR). Unlike\nexisting streaming multi-talker ASR models using multiple output branches, the\nt-SOT model has only a single output branch that generates recognition tokens\n(e.g., words, subwords) of multiple speakers in chronological order based on\ntheir emission times. A special token that indicates the change of ``virtual''\noutput channels is introduced to keep track of the overlapping utterances.\nCompared to the prior streaming multi-talker ASR models, the t-SOT model has\nthe advantages of less inference cost and a simpler model architecture.\nMoreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the\nt-SOT-based transformer transducer model achieves the state-of-the-art word\nerror rates by a significant margin to the prior results. For non-overlapping\nspeech, the t-SOT model is on par with a single-talker ASR model in terms of\nboth accuracy and computational cost, opening the door for deploying one model\nfor both single- and multi-talker scenarios.", "published": "2022-02-02 01:27:21", "link": "http://arxiv.org/abs/2202.00842v5", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Some Reflections on Drawing Causal Inference using Textual Data:\n  Parallels Between Human Subjects and Organized Texts", "abstract": "We examine the role of textual data as study units when conducting causal\ninference by drawing parallels between human subjects and organized texts. %in\nhuman population research. We elaborate on key causal concepts and principles,\nand expose some ambiguity and sometimes fallacies. To facilitate better framing\na causal query, we discuss two strategies: (i) shifting from immutable traits\nto perceptions of them, and (ii) shifting from some abstract concept/property\nto its constituent parts, i.e., adopting a constructivist perspective of an\nabstract concept. We hope this article would raise the awareness of the\nimportance of articulating and clarifying fundamental concepts before delving\ninto developing methodologies when drawing causal inference using textual data.", "published": "2022-02-02 01:54:04", "link": "http://arxiv.org/abs/2202.00848v1", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "What Has Been Enhanced in my Knowledge-Enhanced Language Model?", "abstract": "Pretrained language models (LMs) do not capture factual knowledge very well.\nThis has led to the development of a number of knowledge integration (KI)\nmethods which aim to incorporate external knowledge into pretrained LMs. Even\nthough KI methods show some performance gains over vanilla LMs, the\ninner-workings of these methods are not well-understood. For instance, it is\nunclear how and what kind of knowledge is effectively integrated into these\nmodels and if such integration may lead to catastrophic forgetting of already\nlearned knowledge. This paper revisits the KI process in these models with an\ninformation-theoretic view and shows that KI can be interpreted using a graph\nconvolution operation. We propose a probe model called \\textit{Graph\nConvolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and\nexposing what kind of knowledge is integrated into these models. We conduct\nexperiments to verify that our GCS can indeed be used to correctly interpret\nthe KI process, and we use it to analyze two well-known knowledge-enhanced LMs:\nERNIE and K-Adapter, and find that only a small amount of factual knowledge is\nintegrated in them. We stratify knowledge in terms of various relation types\nand find that ERNIE and K-Adapter integrate different kinds of knowledge to\ndifferent extent. Our analysis also shows that simply increasing the size of\nthe KI corpus may not lead to better KI; fundamental advances may be needed.", "published": "2022-02-02 11:23:36", "link": "http://arxiv.org/abs/2202.00964v7", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "RescoreBERT: Discriminative Speech Recognition Rescoring with BERT", "abstract": "Second-pass rescoring is an important component in automatic speech\nrecognition (ASR) systems that is used to improve the outputs from a first-pass\ndecoder by implementing a lattice rescoring or $n$-best re-ranking. While\npretraining with a masked language model (MLM) objective has received great\nsuccess in various natural language understanding (NLU) tasks, it has not\ngained traction as a rescoring model for ASR. Specifically, training a\nbidirectional model like BERT on a discriminative objective such as minimum WER\n(MWER) has not been explored. Here we show how to train a BERT-based rescoring\nmodel with MWER loss, to incorporate the improvements of a discriminative loss\ninto fine-tuning of deep bidirectional pretrained models for ASR. Specifically,\nwe propose a fusion strategy that incorporates the MLM into the discriminative\ntraining process to effectively distill knowledge from a pretrained model. We\nfurther propose an alternative discriminative loss. This approach, which we\ncall RescoreBERT, reduces WER by 6.6%/3.4% relative on the LibriSpeech\nclean/other test sets over a BERT baseline without discriminative objective. We\nalso evaluate our method on an internal dataset from a conversational agent and\nfind that it reduces both latency and WER (by 3 to 8% relative) over an LSTM\nrescoring model.", "published": "2022-02-02 15:45:26", "link": "http://arxiv.org/abs/2202.01094v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Keyword localisation in untranscribed speech using visually grounded\n  speech models", "abstract": "Keyword localisation is the task of finding where in a speech utterance a\ngiven query keyword occurs. We investigate to what extent keyword localisation\nis possible using a visually grounded speech (VGS) model. VGS models are\ntrained on unlabelled images paired with spoken captions. These models are\ntherefore self-supervised -- trained without any explicit textual label or\nlocation information. To obtain training targets, we first tag training images\nwith soft text labels using a pretrained visual classifier with a fixed\nvocabulary. This enables a VGS model to predict the presence of a written\nkeyword in an utterance, but not its location. We consider four ways to equip\nVGS models with localisations capabilities. Two of these -- a saliency approach\nand input masking -- can be applied to an arbitrary prediction model after\ntraining, while the other two -- attention and a score aggregation approach --\nare incorporated directly into the structure of the model. Masked-based\nlocalisation gives some of the best reported localisation scores from a VGS\nmodel, with an accuracy of 57% when the system knows that a keyword occurs in\nan utterance and need to predict its location. In a setting where localisation\nis performed after detection, an $F_1$ of 25% is achieved, and in a setting\nwhere a keyword spotting ranking pass is first performed, we get a localisation\nP@10 of 32%. While these scores are modest compared to the idealised setting\nwith unordered bag-of-word-supervision (from transcriptions), these models do\nnot receive any textual or location supervision. Further analyses show that\nthese models are limited by the first detection or ranking pass. Moreover,\nindividual keyword localisation performance is correlated with the tagging\nperformance from the visual classifier. We also show qualitatively how and\nwhere semantic mistakes occur, e.g. that the model locates surfer when queried\nwith ocean.", "published": "2022-02-02 16:14:29", "link": "http://arxiv.org/abs/2202.01107v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Epidemic Dreams: Dreaming about health during the COVID-19 pandemic", "abstract": "The continuity hypothesis of dreams suggests that the content of dreams is\ncontinuous with the dreamer's waking experiences. Given the unprecedented\nnature of the experiences during COVID-19, we studied the continuity hypothesis\nin the context of the pandemic. We implemented a deep-learning algorithm that\ncan extract mentions of medical conditions from text and applied it to two\ndatasets collected during the pandemic: 2,888 dream reports (dreaming life\nexperiences), and 57M tweets mentioning the pandemic (waking life experiences).\nThe health expressions common to both sets were typical COVID-19 symptoms\n(e.g., cough, fever, and anxiety), suggesting that dreams reflected people's\nreal-world experiences. The health expressions that distinguished the two sets\nreflected differences in thought processes: expressions in waking life\nreflected a linear and logical thought process and, as such, described\nrealistic symptoms or related disorders (e.g., nasal pain, SARS, H1N1); those\nin dreaming life reflected a thought process closer to the visual and emotional\nspheres and, as such, described either conditions unrelated to the virus (e.g.,\nmaggots, deformities, snakebites), or conditions of surreal nature (e.g., teeth\nfalling out, body crumbling into sand). Our results confirm that dream reports\nrepresent an understudied yet valuable source of people's health experiences in\nthe real world.", "published": "2022-02-02 18:09:06", "link": "http://arxiv.org/abs/2202.01176v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "ASR-Aware End-to-end Neural Diarization", "abstract": "We present a Conformer-based end-to-end neural diarization (EEND) model that\nuses both acoustic input and features derived from an automatic speech\nrecognition (ASR) model. Two categories of features are explored: features\nderived directly from ASR output (phones, position-in-word and word boundaries)\nand features derived from a lexical speaker change detection model, trained by\nfine-tuning a pretrained BERT model on the ASR output. Three modifications to\nthe Conformer-based EEND architecture are proposed to incorporate the features.\nFirst, ASR features are concatenated with acoustic features. Second, we propose\na new attention mechanism called contextualized self-attention that utilizes\nASR features to build robust speaker representations. Finally, multi-task\nlearning is used to train the model to minimize classification loss for the ASR\nfeatures along with diarization loss. Experiments on the two-speaker English\nconversations of Switchboard+SRE data sets show that multi-task learning with\nposition-in-word information is the most effective way of utilizing ASR\nfeatures, reducing the diarization error rate (DER) by 20% relative to the\nbaseline.", "published": "2022-02-02 21:17:14", "link": "http://arxiv.org/abs/2202.01286v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Comparison of Online Hate on Reddit and 4chan: A Case Study of the\n  2020 US Election", "abstract": "The rapid integration of the Internet into our daily lives has led to many\nbenefits but also to a number of new, wide-spread threats such as online hate,\ntrolling, bullying, and generally aggressive behaviours. While research has\ntraditionally explored online hate, in particular, on one platform, the reality\nis that such hate is a phenomenon that often makes use of multiple online\nnetworks. In this article, we seek to advance the discussion into online hate\nby harnessing a comparative approach, where we make use of various Natural\nLanguage Processing (NLP) techniques to computationally analyse hateful content\nfrom Reddit and 4chan relating to the 2020 US Presidential Elections. Our\nfindings show how content and posting activity can differ depending on the\nplatform being used. Through this, we provide initial comparison into the\nplatform-specific behaviours of online hate, and how different platforms can\nserve specific purposes. We further provide several avenues for future research\nutilising a cross-platform approach so as to gain a more comprehensive\nunderstanding of the global hate ecosystem.", "published": "2022-02-02 21:48:56", "link": "http://arxiv.org/abs/2202.01302v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding The Robustness of Self-supervised Learning Through Topic\n  Modeling", "abstract": "Self-supervised learning has significantly improved the performance of many\nNLP tasks. However, how can self-supervised learning discover useful\nrepresentations, and why is it better than traditional approaches such as\nprobabilistic models are still largely unknown. In this paper, we focus on the\ncontext of topic modeling and highlight a key advantage of self-supervised\nlearning - when applied to data generated by topic models, self-supervised\nlearning can be oblivious to the specific model, and hence is less susceptible\nto model misspecification. In particular, we prove that commonly used\nself-supervised objectives based on reconstruction or contrastive samples can\nboth recover useful posterior information for general topic models.\nEmpirically, we show that the same objectives can perform on par with posterior\ninference using the correct model, while outperforming posterior inference\nusing misspecified models.", "published": "2022-02-02 06:20:59", "link": "http://arxiv.org/abs/2203.03539v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "GatorTron: A Large Clinical Language Model to Unlock Patient Information\n  from Unstructured Electronic Health Records", "abstract": "There is an increasing interest in developing artificial intelligence (AI)\nsystems to process and interpret electronic health records (EHRs). Natural\nlanguage processing (NLP) powered by pretrained language models is the key\ntechnology for medical AI systems utilizing clinical narratives. However, there\nare few clinical language models, the largest of which trained in the clinical\ndomain is comparatively small at 110 million parameters (compared with billions\nof parameters in the general domain). It is not clear how large clinical\nlanguage models with billions of parameters can help medical AI systems utilize\nunstructured EHRs. In this study, we develop from scratch a large clinical\nlanguage model - GatorTron - using >90 billion words of text (including >82\nbillion words of de-identified clinical text) and systematically evaluate it on\n5 clinical NLP tasks including clinical concept extraction, medical relation\nextraction, semantic textual similarity, natural language inference (NLI), and\nmedical question answering (MQA). We examine how (1) scaling up the number of\nparameters and (2) scaling up the size of the training data could benefit these\nNLP tasks. GatorTron models scale up the clinical language model from 110\nmillion to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6%\nand 9.5% improvement in accuracy for NLI and MQA), which can be applied to\nmedical AI systems to improve healthcare delivery. The GatorTron models are\npublicly available at:\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og.", "published": "2022-02-02 14:28:51", "link": "http://arxiv.org/abs/2203.03540v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Longitudinal Dataset of Twitter ISIS Users", "abstract": "We present a large longitudinal dataset of tweets from two sets of users that\nare suspected to be affiliated with ISIS. These sets of users are identified\nbased on a prior study and a campaign aimed at shutting down ISIS Twitter\naccounts. These users have engaged with known ISIS accounts at least once\nduring 2014-2015 and are still active as of 2021. Some of them have directly\nsupported the ISIS users and their tweets by retweeting them, and some of the\nusers that have quoted tweets of ISIS, have uncertain connections to ISIS seed\naccounts. This study and the dataset represent a unique approach to analyzing\nISIS data. Although much research exists on ISIS online activities, few studies\nhave focused on individual accounts. Our approach to validating accounts as\nwell as developing a framework for differentiating accounts' functionality\n(e.g., propaganda versus operational planning) offers a foundation for future\nresearch. We perform some descriptive statistics and preliminary analyses on\nour collected data to provide deeper insight and highlight the significance and\npracticality of such analyses. We further discuss several cross-disciplinary\npotential use cases and research directions.", "published": "2022-02-02 05:03:05", "link": "http://arxiv.org/abs/2202.00878v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CR", "cs.LG", "68T50, 68-11", "J.4; K.4.1; I.2.7; H.4.3"], "primary_category": "cs.SI"}
{"title": "Automated Detection of Doxing on Twitter", "abstract": "Doxing refers to the practice of disclosing sensitive personal information\nabout a person without their consent. This form of cyberbullying is an\nunpleasant and sometimes dangerous phenomenon for online social networks.\nAlthough prior work exists on automated identification of other types of\ncyberbullying, a need exists for methods capable of detecting doxing on Twitter\nspecifically. We propose and evaluate a set of approaches for automatically\ndetecting second- and third-party disclosures on Twitter of sensitive private\ninformation, a subset of which constitutes doxing. We summarize our findings of\ncommon intentions behind doxing episodes and compare nine different approaches\nfor automated detection based on string-matching and one-hot encoded\nheuristics, as well as word and contextualized string embedding representations\nof tweets. We identify an approach providing 96.86% accuracy and 97.37% recall\nusing contextualized string embeddings and conclude by discussing the\npracticality of our proposed methods.", "published": "2022-02-02 05:04:34", "link": "http://arxiv.org/abs/2202.00879v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CR", "cs.CY", "68T01, 68T50, 91F20", "H.4.3; I.7.0; J.4; K.4.2"], "primary_category": "cs.SI"}
{"title": "ISS2: An Extension of Iterative Source Steering Algorithm for\n  Majorization-Minimization-Based Independent Vector Analysis", "abstract": "A majorization-minimization (MM) algorithm for independent vector analysis\noptimizes a separation matrix $W = [w_1, \\ldots, w_m]^h \\in \\mathbb{C}^{m\n\\times m}$ by minimizing a surrogate function of the form $\\mathcal{L}(W) =\n\\sum_{i = 1}^m w_i^h V_i w_i - \\log | \\det W |^2$, where $m \\in \\mathbb{N}$ is\nthe number of sensors and positive definite matrices $V_1,\\ldots,V_m \\in\n\\mathbb{C}^{m \\times m}$ are constructed in each MM iteration. For $m \\geq 3$,\nno algorithm has been found to obtain a global minimum of $\\mathcal{L}(W)$.\nInstead, block coordinate descent (BCD) methods with closed-form update\nformulas have been developed for minimizing $\\mathcal{L}(W)$ and shown to be\neffective. One such BCD is called iterative projection (IP) that updates one or\ntwo rows of $W$ in each iteration. Another BCD is called iterative source\nsteering (ISS) that updates one column of the mixing matrix $A = W^{-1}$ in\neach iteration. Although the time complexity per iteration of ISS is $m$ times\nsmaller than that of IP, the conventional ISS converges slower than the current\nfastest IP (called $\\text{IP}_2$) that updates two rows of $W$ in each\niteration. We here extend this ISS to $\\text{ISS}_2$ that can update two\ncolumns of $A$ in each iteration while maintaining its small time complexity.\nTo this end, we provide a unified way for developing new ISS type methods from\nwhich $\\text{ISS}_2$ as well as the conventional ISS can be immediately\nobtained in a systematic manner. Numerical experiments to separate reverberant\nspeech mixtures show that our $\\text{ISS}_2$ converges in fewer MM iterations\nthan the conventional ISS, and is comparable to $\\text{IP}_2$.", "published": "2022-02-02 04:50:08", "link": "http://arxiv.org/abs/2202.00875v3", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Melody Extraction from Polyphonic Music by Deep Learning Approaches: A\n  Review", "abstract": "Melody extraction is a vital music information retrieval task among music\nresearchers for its potential applications in education pedagogy and the music\nindustry. Melody extraction is a notoriously challenging task due to the\npresence of background instruments. Also, often melodic source exhibits similar\ncharacteristics to that of the other instruments. The interfering background\naccompaniment with the vocals makes extracting the melody from the mixture\nsignal much more challenging. Until recently, classical signal processing-based\nmelody extraction methods were quite popular among melody extraction\nresearchers. The ability of the deep learning models to model large-scale data\nand the ability of the models to learn automatic features by exploiting spatial\nand temporal dependencies inspired many researchers to adopt deep learning\nmodels for melody extraction. In this paper, an attempt has been made to review\nthe up-to-date data-driven deep learning approaches for melody extraction from\npolyphonic music. The available deep models have been categorized based on the\ntype of neural network used and the output representation they use for\npredicting melody. Further, the architectures of the 25 melody extraction\nmodels are briefly presented. The loss functions used to optimize the model\nparameters of the melody extraction models are broadly categorized into four\ncategories and briefly describe the loss functions used by various melody\nextraction models. Also, the various input representations adopted by the\nmelody extraction models and the parameter settings are deeply described. A\nsection describing the explainability of the block-box melody extraction deep\nneural networks is included. The performance of 25 melody extraction methods is\ncompared. The possible future directions to explore/improve the melody\nextraction methods are also presented in the paper.", "published": "2022-02-02 15:07:15", "link": "http://arxiv.org/abs/2202.01078v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The CORAL++ Algorithm for Unsupervised Domain Adaptation of Speaker\n  Recogntion", "abstract": "State-of-the-art speaker recognition systems are trained with a large amount\nof human-labeled training data set. Such a training set is usually composed of\nvarious data sources to enhance the modeling capability of models. However, in\npractical deployment, unseen condition is almost inevitable. Domain mismatch is\na common problem in real-life applications due to the statistical difference\nbetween the training and testing data sets. To alleviate the degradation caused\nby domain mismatch, we propose a new feature-based unsupervised domain\nadaptation algorithm. The algorithm we propose is a further optimization based\non the well-known CORrelation ALignment (CORAL), so we call it CORAL++. On the\nNIST 2019 Speaker Recognition Evaluation (SRE19), we use SRE18 CTS set as the\ndevelopment set to verify the effectiveness of CORAL++. With the typical\nx-vector/PLDA setup, the CORAL++ outperforms the CORAL by 9.40% relatively on\nEER.", "published": "2022-02-02 15:41:24", "link": "http://arxiv.org/abs/2202.01092v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Active Audio-Visual Separation of Dynamic Sound Sources", "abstract": "We explore active audio-visual separation for dynamic sound sources, where an\nembodied agent moves intelligently in a 3D environment to continuously isolate\nthe time-varying audio stream being emitted by an object of interest. The agent\nhears a mixed stream of multiple audio sources (e.g., multiple people\nconversing and a band playing music at a noisy party). Given a limited time\nbudget, it needs to extract the target sound accurately at every step using\negocentric audio-visual observations. We propose a reinforcement learning agent\nequipped with a novel transformer memory that learns motion policies to control\nits camera and microphone to recover the dynamic target audio, using\nself-attention to make high-quality estimates for current timesteps and also\nsimultaneously improve its past estimates. Using highly realistic acoustic\nSoundSpaces simulations in real-world scanned Matterport3D environments, we\nshow that our model is able to learn efficient behavior to carry out continuous\nseparation of a dynamic audio target. Project:\nhttps://vision.cs.utexas.edu/projects/active-av-dynamic-separation/.", "published": "2022-02-02 02:03:28", "link": "http://arxiv.org/abs/2202.00850v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound\n  Classification and Detection", "abstract": "Audio classification is an important task of mapping audio samples into their\ncorresponding labels. Recently, the transformer model with self-attention\nmechanisms has been adopted in this field. However, existing audio transformers\nrequire large GPU memories and long training time, meanwhile relying on\npretrained vision models to achieve high performance, which limits the model's\nscalability in audio tasks. To combat these problems, we introduce HTS-AT: an\naudio transformer with a hierarchical structure to reduce the model size and\ntraining time. It is further combined with a token-semantic module to map final\noutputs into class featuremaps, thus enabling the model for the audio event\ndetection (i.e. localization in time). We evaluate HTS-AT on three datasets of\naudio classification where it achieves new state-of-the-art (SOTA) results on\nAudioSet and ESC-50, and equals the SOTA on Speech Command V2. It also achieves\nbetter performance in event localization than the previous CNN-based models.\nMoreover, HTS-AT requires only 35% model parameters and 15% training time of\nthe previous audio transformer. These results demonstrate the high performance\nand high efficiency of HTS-AT.", "published": "2022-02-02 04:49:14", "link": "http://arxiv.org/abs/2202.00874v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TONet: Tone-Octave Network for Singing Melody Extraction from Polyphonic\n  Music", "abstract": "Singing melody extraction is an important problem in the field of music\ninformation retrieval. Existing methods typically rely on frequency-domain\nrepresentations to estimate the sung frequencies. However, this design does not\nlead to human-level performance in the perception of melody information for\nboth tone (pitch-class) and octave. In this paper, we propose TONet, a\nplug-and-play model that improves both tone and octave perceptions by\nleveraging a novel input representation and a novel network architecture.\nFirst, we present an improved input representation, the Tone-CFP, that\nexplicitly groups harmonics via a rearrangement of frequency-bins. Second, we\nintroduce an encoder-decoder architecture that is designed to obtain a salience\nfeature map, a tone feature map, and an octave feature map. Third, we propose a\ntone-octave fusion mechanism to improve the final salience feature map.\nExperiments are done to verify the capability of TONet with various baseline\nbackbone models. Our results show that tone-octave fusion with Tone-CFP can\nsignificantly improve the singing voice extraction performance across various\ndatasets -- with substantial gains in octave and tone accuracy.", "published": "2022-02-02 10:55:48", "link": "http://arxiv.org/abs/2202.00951v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
