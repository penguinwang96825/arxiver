{"title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading\n  Comprehension", "abstract": "In this paper, we present a novel approach to machine reading comprehension\nfor the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a\nquestion with exact text spans in a passage, the MS-MARCO dataset defines the\ntask as answering a question from multiple passages and the words in the answer\nare not necessary in the passages. We therefore develop an\nextraction-then-synthesis framework to synthesize answers from extraction\nresults. Specifically, the answer extraction model is first employed to predict\nthe most important sub-spans from the passage as evidence, and the answer\nsynthesis model takes the evidence as additional features along with the\nquestion and passage to further elaborate the final answers. We build the\nanswer extraction model with state-of-the-art neural networks for single\npassage reading comprehension, and propose an additional task of passage\nranking to help answer extraction in multiple passages. The answer synthesis\nmodel is based on the sequence-to-sequence neural networks with extracted\nevidences as features. Experiments show that our extraction-then-synthesis\nmethod outperforms state-of-the-art methods.", "published": "2017-06-15 11:10:33", "link": "http://arxiv.org/abs/1706.04815v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "German in Flux: Detecting Metaphoric Change via Word Entropy", "abstract": "This paper explores the information-theoretic measure entropy to detect\nmetaphoric change, transferring ideas from hypernym detection to research on\nlanguage change. We also build the first diachronic test set for German as a\nstandard for metaphoric change annotation. Our model shows high performance, is\nunsupervised, language-independent and generalizable to other processes of\nsemantic change.", "published": "2017-06-15 17:14:17", "link": "http://arxiv.org/abs/1706.04971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Formal Models from Normative Texts", "abstract": "We are concerned with the analysis of normative texts - documents based on\nthe deontic notions of obligation, permission, and prohibition. Our goal is to\nmake queries about these notions and verify that a text satisfies certain\nproperties concerning causality of actions and timing constraints. This\nrequires taking the original text and building a representation (model) of it\nin a formal language, in our case the C-O Diagram formalism. We present an\nexperimental, semi-automatic aid that helps to bridge the gap between a\nnormative text in natural language and its C-O Diagram representation. Our\napproach consists of using dependency structures obtained from the\nstate-of-the-art Stanford Parser, and applying our own rules and heuristics in\norder to extract the relevant components. The result is a tabular data\nstructure where each sentence is split into suitable fields, which can then be\nconverted into a C-O Diagram. The process is not fully automatic however, and\nsome post-editing is generally required of the user. We apply our tool and\nperform experiments on documents from different domains, and report an initial\nevaluation of the accuracy and feasibility of our approach.", "published": "2017-06-15 07:24:23", "link": "http://arxiv.org/abs/1706.04997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensembling Factored Neural Machine Translation Models for Automatic\n  Post-Editing and Quality Estimation", "abstract": "This work presents a novel approach to Automatic Post-Editing (APE) and\nWord-Level Quality Estimation (QE) using ensembles of specialized Neural\nMachine Translation (NMT) systems. Word-level features that have proven\neffective for QE are included as input factors, expanding the representation of\nthe original source and the machine translation hypothesis, which are used to\ngenerate an automatically post-edited hypothesis. We train a suite of NMT\nmodels that use different input representations, but share the same output\nspace. These models are then ensembled together, and tuned for both the APE and\nthe QE task. We thus attempt to connect the state-of-the-art approaches to APE\nand QE within a single framework. Our models achieve state-of-the-art results\nin both tasks, with the only difference in the tuning step which learns weights\nfor each component of the ensemble.", "published": "2017-06-15 20:47:03", "link": "http://arxiv.org/abs/1706.05083v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Mixture Model for Learning Multi-Sense Word Embeddings", "abstract": "Word embeddings are now a standard technique for inducing meaning\nrepresentations for words. For getting good representations, it is important to\ntake into account different senses of a word. In this paper, we propose a\nmixture model for learning multi-sense word embeddings. Our model generalizes\nthe previous works in that it allows to induce different weights of different\nsenses of a word. The experimental results show that our model outperforms\nprevious models on standard evaluation tasks.", "published": "2017-06-15 23:07:06", "link": "http://arxiv.org/abs/1706.05111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a theory of word order. Comment on \"Dependency distance: a new\n  perspective on syntactic patterns in natural language\" by Haitao Liu et al", "abstract": "Comment on \"Dependency distance: a new perspective on syntactic patterns in\nnatural language\" by Haitao Liu et al", "published": "2017-06-15 14:01:40", "link": "http://arxiv.org/abs/1706.04872v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "A Survey Of Cross-lingual Word Embedding Models", "abstract": "Cross-lingual representations of words enable us to reason about word meaning\nin multilingual contexts and are a key facilitator of cross-lingual transfer\nwhen developing natural language processing models for low-resource languages.\nIn this survey, we provide a comprehensive typology of cross-lingual word\nembedding models. We compare their data requirements and objective functions.\nThe recurring theme of the survey is that many of the models presented in the\nliterature optimize for the same objectives, and that seemingly different\nmodels are often equivalent modulo optimization strategies, hyper-parameters,\nand such. We also discuss the different ways cross-lingual word embeddings are\nevaluated, as well as future challenges and research horizons.", "published": "2017-06-15 14:46:56", "link": "http://arxiv.org/abs/1706.04902v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DSRIM: A Deep Neural Information Retrieval Model Enhanced by a Knowledge\n  Resource Driven Representation of Documents", "abstract": "The state-of-the-art solutions to the vocabulary mismatch in information\nretrieval (IR) mainly aim at leveraging either the relational semantics\nprovided by external resources or the distributional semantics, recently\ninvestigated by deep neural approaches. Guided by the intuition that the\nrelational semantics might improve the effectiveness of deep neural approaches,\nwe propose the Deep Semantic Resource Inference Model (DSRIM) that relies on:\n1) a representation of raw-data that models the relational semantics of text by\njointly considering objects and relations expressed in a knowledge resource,\nand 2) an end-to-end neural architecture that learns the query-document\nrelevance by leveraging the distributional and relational semantics of\ndocuments and queries. The experimental evaluation carried out on two TREC\ndatasets from TREC Terabyte and TREC CDS tracks relying respectively on WordNet\nand MeSH resources, indicates that our model outperforms state-of-the-art\nsemantic and deep neural IR models.", "published": "2017-06-15 15:24:32", "link": "http://arxiv.org/abs/1706.04922v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
