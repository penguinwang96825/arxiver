{"title": "SCOTT: Self-Consistent Chain-of-Thought Distillation", "abstract": "Large language models (LMs) beyond a certain scale, demonstrate the emergent\ncapability of generating free-text rationales for their predictions via\nchain-of-thought (CoT) prompting. While CoT can yield dramatically improved\nperformance, such gains are only observed for sufficiently large LMs. Even more\nconcerning, there is little guarantee that the generated rationales are\nconsistent with LM's predictions or faithfully justify the decisions. In this\nwork, we propose a faithful knowledge distillation method to learn a small,\nself-consistent CoT model from a teacher model that is orders of magnitude\nlarger. To form better supervision, we elicit rationales supporting the gold\nanswers from a large LM (teacher) by contrastive decoding, which encourages the\nteacher to generate tokens that become more plausible only when the answer is\nconsidered. To ensure faithful distillation, we use the teacher-generated\nrationales to learn a student LM with a counterfactual reasoning objective,\nwhich prevents the student from ignoring the rationales to make inconsistent\npredictions. Experiments show that, while yielding comparable end-task\nperformance, our method can generate CoT rationales that are more faithful than\nbaselines do. Further analysis suggests that such a model respects the\nrationales more when making decisions; thus, we can improve its performance\nmore by refining its rationales.", "published": "2023-05-03 03:47:00", "link": "http://arxiv.org/abs/2305.01879v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Interventions-based Few-Shot Named Entity Recognition", "abstract": "Few-shot named entity recognition (NER) systems aims at recognizing new\nclasses of entities based on a few labeled samples. A significant challenge in\nthe few-shot regime is prone to overfitting than the tasks with abundant\nsamples. The heavy overfitting in few-shot learning is mainly led by spurious\ncorrelation caused by the few samples selection bias. To alleviate the problem\nof the spurious correlation in the few-shot NER, in this paper, we propose a\ncausal intervention-based few-shot NER method. Based on the prototypical\nnetwork, the method intervenes in the context and prototype via backdoor\nadjustment during training. In particular, intervening in the context of the\none-shot scenario is very difficult, so we intervene in the prototype via\nincremental learning, which can also avoid catastrophic forgetting. Our\nexperiments on different benchmarks show that our approach achieves new\nstate-of-the-art results (achieving up to 29% absolute improvement and 12% on\naverage for all tasks).", "published": "2023-05-03 06:11:39", "link": "http://arxiv.org/abs/2305.01914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Meta-Learning for Zero-Shot Relation Triplet Extraction", "abstract": "The zero-shot relation triplet extraction (ZeroRTE) task aims to extract\nrelation triplets from a piece of text with unseen relation types. The seminal\nwork adopts the pre-trained generative model to generate synthetic samples for\nnew relations. However, current generative models lack the optimization process\nof model generalization on different tasks during training, and thus have\nlimited generalization capability. For this reason, we propose a novel\ngenerative meta-learning framework which exploits the `learning-to-learn'\nability of meta-learning to boost the generalization capability of generative\nmodels. Specifically, we first design a task-aware generative model which can\nlearn the general knowledge by forcing the optimization process to be conducted\nacross multiple tasks. Based on it, we then present three generative\nmeta-learning approaches designated for three typical meta-learning categories.\nExtensive experimental results demonstrate that our framework achieves a new\nstate-of-the-art performance for the ZeroRTE task.", "published": "2023-05-03 06:34:39", "link": "http://arxiv.org/abs/2305.01920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LMs Generalize to Future Data? An Empirical Analysis on Text\n  Summarization", "abstract": "Recent pre-trained language models (PLMs) achieve promising results in\nexisting abstractive summarization datasets. However, existing summarization\nbenchmarks overlap in time with the standard pre-training corpora and\nfinetuning datasets. Hence, the strong performance of PLMs may rely on the\nparametric knowledge that is memorized during pre-training and fine-tuning.\nMoreover, the knowledge memorized by PLMs may quickly become outdated, which\naffects the generalization performance of PLMs on future data. In this work, we\npropose TempoSum, a novel benchmark that contains data samples from 2010 to\n2022, to understand the temporal generalization ability of abstractive\nsummarization models. Through extensive human evaluation, we show that\nparametric knowledge stored in summarization models significantly affects the\nfaithfulness of the generated summaries on future data. Moreover, existing\nfaithfulness enhancement methods cannot reliably improve the faithfulness of\nsummarization models on future data. Finally, we discuss several\nrecommendations to the research community on how to evaluate and improve the\ntemporal generalization capability of text summarization models.", "published": "2023-05-03 08:08:07", "link": "http://arxiv.org/abs/2305.01951v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NorQuAD: Norwegian Question Answering Dataset", "abstract": "In this paper we present NorQuAD: the first Norwegian question answering\ndataset for machine reading comprehension. The dataset consists of 4,752\nmanually created question-answer pairs. We here detail the data collection\nprocedure and present statistics of the dataset. We also benchmark several\nmultilingual and Norwegian monolingual language models on the dataset and\ncompare them against human performance. The dataset will be made freely\navailable.", "published": "2023-05-03 08:17:07", "link": "http://arxiv.org/abs/2305.01957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural language processing on customer note data", "abstract": "Automatic analysis of customer data for businesses is an area that is of\ninterest to companies. Business to business data is studied rarely in academia\ndue to the sensitive nature of such information. Applying natural language\nprocessing can speed up the analysis of prohibitively large sets of data. This\npaper addresses this subject and applies sentiment analysis, topic modelling\nand keyword extraction to a B2B data set. We show that accurate sentiment can\nbe extracted from the notes automatically and the notes can be sorted by\nrelevance into different topics. We see that without clear separation topics\ncan lack relevance to a business context.", "published": "2023-05-03 10:36:56", "link": "http://arxiv.org/abs/2305.02029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What makes a good pause? Investigating the turn-holding effects of\n  fillers", "abstract": "Filled pauses (or fillers), such as \"uh\" and \"um\", are frequent in\nspontaneous speech and can serve as a turn-holding cue for the listener,\nindicating that the current speaker is not done yet. In this paper, we use the\nrecently proposed Voice Activity Projection (VAP) model, which is a deep\nlearning model trained to predict the dynamics of conversation, to analyse the\neffects of filled pauses on the expected turn-hold probability. The results\nshow that, while filled pauses do indeed have a turn-holding effect, it is\nperhaps not as strong as could be expected, probably due to the redundancy of\nother cues. We also find that the prosodic properties and position of the\nfiller has a significant effect on the turn-hold probability. However, contrary\nto what has been suggested in previous work, there is no difference between\n\"uh\" and \"um\" in this regard.", "published": "2023-05-03 13:15:37", "link": "http://arxiv.org/abs/2305.02101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Background Knowledge Grounding for Readable, Relevant, and Factual\n  Biomedical Lay Summaries", "abstract": "Communication of scientific findings to the public is important for keeping\nnon-experts informed of developments such as life-saving medical treatments.\nHowever, generating readable lay summaries from scientific documents is\nchallenging, and currently, these summaries suffer from critical factual\nerrors. One popular intervention for improving factuality is using additional\nexternal knowledge to provide factual grounding. However, it is unclear how\nthese grounding sources should be retrieved, selected, or integrated, and how\nsupplementary grounding documents might affect the readability or relevance of\nthe generated summaries. We develop a simple method for selecting grounding\nsources and integrating them with source documents. We then use the BioLaySum\nsummarization dataset to evaluate the effects of different grounding sources on\nsummary quality. We found that grounding source documents improves the\nrelevance and readability of lay summaries but does not improve factuality of\nlay summaries. This continues to be true in zero-shot summarization settings\nwhere we hypothesized that grounding might be even more important for factual\nlay summaries.", "published": "2023-05-03 13:24:49", "link": "http://arxiv.org/abs/2305.02104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-RE: In-context Learning for Relation Extraction using Large Language\n  Models", "abstract": "In spite of the potential for ground-breaking achievements offered by large\nlanguage models (LLMs) (e.g., GPT-3), they still lag significantly behind\nfully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).\nThis is due to the two major shortcomings of LLMs in RE: (1) low relevance\nregarding entity and relation in retrieved demonstrations for in-context\nlearning; and (2) the strong inclination to wrongly classify NULL examples into\nother pre-defined labels.\n  In this paper, we propose GPT-RE to bridge the gap between LLMs and\nfully-supervised baselines. GPT-RE successfully addresses the aforementioned\nissues by (1) incorporating task-specific entity representations in\ndemonstration retrieval; and (2) enriching the demonstrations with gold\nlabel-induced reasoning logic. We evaluate GPT-RE on four widely-used RE\ndatasets, and observe that GPT-RE achieves improvements over not only existing\nGPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE\nachieves SOTA performances on the Semeval and SciERC datasets, and competitive\nperformances on the TACRED and ACE05 datasets.", "published": "2023-05-03 13:28:08", "link": "http://arxiv.org/abs/2305.02105v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pay More Attention to Relation Exploration for Knowledge Base Question\n  Answering", "abstract": "Knowledge base question answering (KBQA) is a challenging task that aims to\nretrieve correct answers from large-scale knowledge bases. Existing attempts\nprimarily focus on entity representation and final answer reasoning, which\nresults in limited supervision for this task. Moreover, the relations, which\nempirically determine the reasoning path selection, are not fully considered in\nrecent advancements. In this study, we propose a novel framework, RE-KBQA, that\nutilizes relations in the knowledge base to enhance entity representation and\nintroduce additional supervision. We explore guidance from relations in three\naspects, including (1) distinguishing similar entities by employing a\nvariational graph auto-encoder to learn relation importance; (2) exploring\nextra supervision by predicting relation distributions as soft labels with a\nmulti-task scheme; (3) designing a relation-guided re-ranking algorithm for\npost-processing. Experimental results on two benchmark datasets demonstrate the\neffectiveness and superiority of our framework, improving the F1 score by 5.7%\nfrom 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on par\nwith state-of-the-art methods.", "published": "2023-05-03 13:48:30", "link": "http://arxiv.org/abs/2305.02118v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining Language Models' Predictions with High-Impact Concepts", "abstract": "The emergence of large-scale pretrained language models has posed\nunprecedented challenges in deriving explanations of why the model has made\nsome predictions. Stemmed from the compositional nature of languages, spurious\ncorrelations have further undermined the trustworthiness of NLP systems,\nleading to unreliable model explanations that are merely correlated with the\noutput predictions. To encourage fairness and transparency, there exists an\nurgent demand for reliable explanations that allow users to consistently\nunderstand the model's behavior. In this work, we propose a complete framework\nfor extending concept-based interpretability methods to NLP. Specifically, we\npropose a post-hoc interpretability method for extracting predictive high-level\nfeatures (concepts) from the pretrained model's hidden layer activations. We\noptimize for features whose existence causes the output predictions to change\nsubstantially, \\ie generates a high impact. Moreover, we devise several\nevaluation metrics that can be universally applied. Extensive experiments on\nreal and synthetic tasks demonstrate that our method achieves superior results\non {predictive impact}, usability, and faithfulness compared to the baselines.", "published": "2023-05-03 14:48:27", "link": "http://arxiv.org/abs/2305.02160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Statistical Exploration of Text Partition Into Constituents: The Case\n  of the Priestly Source in the Books of Genesis and Exodus", "abstract": "We present a pipeline for a statistical textual exploration, offering a\nstylometry-based explanation and statistical validation of a hypothesized\npartition of a text. Given a parameterization of the text, our pipeline: (1)\ndetects literary features yielding the optimal overlap between the hypothesized\nand unsupervised partitions, (2) performs a hypothesis-testing analysis to\nquantify the statistical significance of the optimal overlap, while conserving\nimplicit correlations between units of text that are more likely to be grouped,\nand (3) extracts and quantifies the importance of features most responsible for\nthe classification, estimates their statistical stability and cluster-wise\nabundance.\n  We apply our pipeline to the first two books in the Bible, where one\nstylistic component stands out in the eyes of biblical scholars, namely, the\nPriestly component. We identify and explore statistically significant stylistic\ndifferences between the Priestly and non-Priestly components.", "published": "2023-05-03 15:07:42", "link": "http://arxiv.org/abs/2305.02170v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Being Parameter-Efficient: A Stratified Sparsely Activated\n  Transformer with Dynamic Capacity", "abstract": "Mixture-of-experts (MoE) models that employ sparse activation have\ndemonstrated effectiveness in significantly increasing the number of parameters\nwhile maintaining low computational requirements per token. However, recent\nstudies have established that MoE models are inherently parameter-inefficient\nas the improvement in performance diminishes with an increasing number of\nexperts. We hypothesize this parameter inefficiency is a result of all experts\nhaving equal capacity, which may not adequately meet the varying complexity\nrequirements of different tokens or tasks. In light of this, we propose\nStratified Mixture of Experts (SMoE) models, which feature a stratified\nstructure and can assign dynamic capacity to different tokens. We demonstrate\nthe effectiveness of SMoE on three multilingual machine translation benchmarks,\ncontaining 4, 15, and 94 language pairs, respectively. We show that SMoE\noutperforms multiple state-of-the-art MoE models with the same or fewer\nparameters.", "published": "2023-05-03 15:18:18", "link": "http://arxiv.org/abs/2305.02176v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AttenWalker: Unsupervised Long-Document Question Answering via\n  Attention-based Graph Walking", "abstract": "Annotating long-document question answering (long-document QA) pairs is\ntime-consuming and expensive. To alleviate the problem, it might be possible to\ngenerate long-document QA pairs via unsupervised question answering (UQA)\nmethods. However, existing UQA tasks are based on short documents, and can\nhardly incorporate long-range information. To tackle the problem, we propose a\nnew task, named unsupervised long-document question answering (ULQA), aiming to\ngenerate high-quality long-document QA instances in an unsupervised manner.\nBesides, we propose AttenWalker, a novel unsupervised method to aggregate and\ngenerate answers with long-range dependency so as to construct long-document QA\npairs. Specifically, AttenWalker is composed of three modules, i.e., span\ncollector, span linker and answer aggregator. Firstly, the span collector takes\nadvantage of constituent parsing and reconstruction loss to select informative\ncandidate spans for constructing answers. Secondly, by going through the\nattention graph of a pre-trained long-document model, potentially interrelated\ntext spans (that might be far apart) could be linked together via an\nattention-walking algorithm. Thirdly, in the answer aggregator, linked spans\nare aggregated into the final answer via the mask-filling ability of a\npre-trained model. Extensive experiments show that AttenWalker outperforms\nprevious methods on Qasper and NarrativeQA. In addition, AttenWalker also shows\nstrong performance in the few-shot learning setting.", "published": "2023-05-03 16:16:14", "link": "http://arxiv.org/abs/2305.02235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from\n  Linguistically Complex Text", "abstract": "Pretrained Vision-Language Models (VLMs) have achieved remarkable performance\nin image retrieval from text. However, their performance drops drastically when\nconfronted with linguistically complex texts that they struggle to comprehend.\nInspired by the Divide-and-Conquer algorithm and dual-process theory, in this\npaper, we regard linguistically complex texts as compound proposition texts\ncomposed of multiple simple proposition sentences and propose an end-to-end\nNeural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three\nmain components: 1) Divide: a proposition generator divides the compound\nproposition text into simple proposition sentences and produces their\ncorresponding representations, 2) Conquer: a pretrained VLMs-based\nvisual-linguistic interactor achieves the interaction between decomposed\nproposition sentences and images, 3) Combine: a neural-symbolic reasoner\ncombines the above reasoning states to obtain the final solution via a neural\nlogic reasoning approach. According to the dual-process theory, the\nvisual-linguistic interactor and neural-symbolic reasoner could be regarded as\nanalogical reasoning System 1 and logical reasoning System 2. We conduct\nextensive experiments on a challenging image retrieval from contextual\ndescriptions data set. Experimental results and analyses indicate NDCR\nsignificantly improves performance in the complex image-text reasoning problem.\nCode link: https://github.com/YunxinLi/NDCR.", "published": "2023-05-03 16:55:00", "link": "http://arxiv.org/abs/2305.02265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Tracking in Language Models", "abstract": "Keeping track of how states of entities change as a text or dialog unfolds is\na key prerequisite to discourse understanding. Yet, there have been few\nsystematic investigations into the ability of large language models (LLMs) to\ntrack discourse entities. In this work, we present a task probing to what\nextent a language model can infer the final state of an entity given an English\ndescription of the initial state and a series of state-changing operations. We\nuse this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track\nthe state of entities, and find that only GPT-3.5 models, which have been\npretrained on large amounts of code, exhibit this ability. We then investigate\nwhether smaller models pretrained primarily on text can learn to track\nentities, through finetuning T5 on several training/evaluation splits. While\nperformance degrades for more complex splits, we find that even when evaluated\non a different set of entities from training or longer operation sequences, a\nfinetuned model can perform non-trivial entity tracking. Taken together, these\nresults suggest that language models can learn to track entities but\npretraining on text corpora alone does not make this capacity surface.", "published": "2023-05-03 18:01:13", "link": "http://arxiv.org/abs/2305.02363v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging\n  Narratives", "abstract": "Sustaining coherent and engaging narratives requires dialogue or storytelling\nagents to understand how the personas of speakers or listeners ground the\nnarrative. Specifically, these agents must infer personas of their listeners to\nproduce statements that cater to their interests. They must also learn to\nmaintain consistent speaker personas for themselves throughout the narrative,\nso that their counterparts feel involved in a realistic conversation or story.\n  However, personas are diverse and complex: they entail large quantities of\nrich interconnected world knowledge that is challenging to robustly represent\nin general narrative systems (e.g., a singer is good at singing, and may have\nattended conservatoire). In this work, we construct a new large-scale persona\ncommonsense knowledge graph, PeaCoK, containing ~100K human-validated persona\nfacts. Our knowledge graph schematizes five dimensions of persona knowledge\nidentified in previous studies of human interactive behaviours, and distils\nfacts in this schema from both existing commonsense knowledge graphs and\nlarge-scale pretrained language models. Our analysis indicates that PeaCoK\ncontains rich and precise world persona inferences that help downstream systems\ngenerate more consistent and engaging narratives.", "published": "2023-05-03 18:02:22", "link": "http://arxiv.org/abs/2305.02364v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Backdoor Learning on Sequence to Sequence Models", "abstract": "Backdoor learning has become an emerging research area towards building a\ntrustworthy machine learning system. While a lot of works have studied the\nhidden danger of backdoor attacks in image or text classification, there is a\nlimited understanding of the model's robustness on backdoor attacks when the\noutput space is infinite and discrete. In this paper, we study a much more\nchallenging problem of testing whether sequence-to-sequence (seq2seq) models\nare vulnerable to backdoor attacks. Specifically, we find by only injecting\n0.2\\% samples of the dataset, we can cause the seq2seq model to generate the\ndesignated keyword and even the whole sentence. Furthermore, we utilize Byte\nPair Encoding (BPE) to create multiple new triggers, which brings new\nchallenges to backdoor detection since these backdoors are not static.\nExtensive experiments on machine translation and text summarization have been\nconducted to show our proposed methods could achieve over 90\\% attack success\nrate on multiple datasets and models.", "published": "2023-05-03 20:31:13", "link": "http://arxiv.org/abs/2305.02424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "evaluating bert and parsbert for analyzing persian advertisement data", "abstract": "This paper discusses the impact of the Internet on modern trading and the\nimportance of data generated from these transactions for organizations to\nimprove their marketing efforts. The paper uses the example of Divar, an online\nmarketplace for buying and selling products and services in Iran, and presents\na competition to predict the percentage of a car sales ad that would be\npublished on the Divar website. Since the dataset provides a rich source of\nPersian text data, the authors use the Hazm library, a Python library designed\nfor processing Persian text, and two state-of-the-art language models, mBERT\nand ParsBERT, to analyze it. The paper's primary objective is to compare the\nperformance of mBERT and ParsBERT on the Divar dataset. The authors provide\nsome background on data mining, Persian language, and the two language models,\nexamine the dataset's composition and statistical features, and provide details\non their fine-tuning and training configurations for both approaches. They\npresent the results of their analysis and highlight the strengths and\nweaknesses of the two language models when applied to Persian text data. The\npaper offers valuable insights into the challenges and opportunities of working\nwith low-resource languages such as Persian and the potential of advanced\nlanguage models like BERT for analyzing such data. The paper also explains the\ndata mining process, including steps such as data cleaning and normalization\ntechniques. Finally, the paper discusses the types of machine learning\nproblems, such as supervised, unsupervised, and reinforcement learning, and the\npattern evaluation techniques, such as confusion matrix. Overall, the paper\nprovides an informative overview of the use of language models and data mining\ntechniques for analyzing text data in low-resource languages, using the example\nof the Divar dataset.", "published": "2023-05-03 20:50:05", "link": "http://arxiv.org/abs/2305.02426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explore the difficulty of words and its influential attributes based on\n  the Wordle game", "abstract": "We adopt the distribution and expectation of guessing times in game Wordle as\nmetrics to predict the difficulty of words and explore their influence factors.\nIn order to predictthe difficulty distribution, we use Monte Carlo to simulate\nthe guessing process of players and then narrow the gap between raw and actual\ndistribution of guessing times for each word with Markov which generates the\nassociativity of words. Afterwards, we take advantage of lasso regression to\npredict the deviation of guessing times expectation and quadratic programming\nto obtain the correction of the original distribution.To predict the difficulty\nlevels, we first use hierarchical clustering to classify the difficulty levels\nbased on the expectation of guessing times. Afterwards we downscale the\nvariables of lexical attributes based on factor analysis. Significant factors\ninclude the number of neighboring words, letter similarity, sub-string\nsimilarity, and word frequency. Finally, we build the relationship between\nlexical attributes and difficulty levels through ordered logistic regression.", "published": "2023-05-03 09:24:39", "link": "http://arxiv.org/abs/2305.03502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Imperceptible Document Manipulations against Neural Ranking\n  Models", "abstract": "Adversarial attacks have gained traction in order to identify potential\nvulnerabilities in neural ranking models (NRMs), but current attack methods\noften introduce grammatical errors, nonsensical expressions, or incoherent text\nfragments, which can be easily detected. Additionally, current methods rely\nheavily on the use of a well-imitated surrogate NRM to guarantee the attack\neffect, which makes them difficult to use in practice. To address these issues,\nwe propose a framework called Imperceptible DocumEnt Manipulation (IDEM) to\nproduce adversarial documents that are less noticeable to both algorithms and\nhumans. IDEM instructs a well-established generative language model, such as\nBART, to generate connection sentences without introducing easy-to-detect\nerrors, and employs a separate position-wise merging strategy to balance\nrelevance and coherence of the perturbed text. Experimental results on the\npopular MS MARCO benchmark demonstrate that IDEM can outperform strong\nbaselines while preserving fluency and correctness of the target documents as\nevidenced by automatic and human evaluations. Furthermore, the separation of\nadversarial text generation from the surrogate NRM makes IDEM more robust and\nless affected by the quality of the surrogate NRM.", "published": "2023-05-03 02:09:29", "link": "http://arxiv.org/abs/2305.01860v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Causality-aware Concept Extraction based on Knowledge-guided Prompting", "abstract": "Concepts benefit natural language understanding but are far from complete in\nexisting knowledge graphs (KGs). Recently, pre-trained language models (PLMs)\nhave been widely used in text-based concept extraction (CE). However, PLMs tend\nto mine the co-occurrence associations from massive corpus as pre-trained\nknowledge rather than the real causal effect between tokens. As a result, the\npre-trained knowledge confounds PLMs to extract biased concepts based on\nspurious co-occurrence correlations, inevitably resulting in low precision. In\nthis paper, through the lens of a Structural Causal Model (SCM), we propose\nequipping the PLM-based extractor with a knowledge-guided prompt as an\nintervention to alleviate concept bias. The prompt adopts the topic of the\ngiven entity from the existing knowledge in KGs to mitigate the spurious\nco-occurrence correlations between entities and biased concepts. Our extensive\nexperiments on representative multilingual KG datasets justify that our\nproposed prompt can effectively alleviate concept bias and improve the\nperformance of PLM-based CE models.The code has been released on\nhttps://github.com/siyuyuan/KPCE.", "published": "2023-05-03 03:36:20", "link": "http://arxiv.org/abs/2305.01876v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot Event Detection: An Empirical Study and a Unified View", "abstract": "Few-shot event detection (ED) has been widely studied, while this brings\nnoticeable discrepancies, e.g., various motivations, tasks, and experimental\nsettings, that hinder the understanding of models for future progress.This\npaper presents a thorough empirical study, a unified view of ED models, and a\nbetter unified baseline. For fair evaluation, we compare 12 representative\nmethods on three datasets, which are roughly grouped into prompt-based and\nprototype-based models for detailed analysis. Experiments consistently\ndemonstrate that prompt-based methods, including ChatGPT, still significantly\ntrail prototype-based methods in terms of overall performance. To investigate\ntheir superior performance, we break down their design elements along several\ndimensions and build a unified framework on prototype-based methods. Under such\nunified view, each prototype-method can be viewed a combination of different\nmodules from these design elements. We further combine all advantageous modules\nand propose a simple yet effective baseline, which outperforms existing methods\nby a large margin (e.g., 2.7% F1 gains under low-resource setting).", "published": "2023-05-03 05:31:48", "link": "http://arxiv.org/abs/2305.01901v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Multi-bit Natural Language Watermarking through Invariant\n  Features", "abstract": "Recent years have witnessed a proliferation of valuable original natural\nlanguage contents found in subscription-based media outlets, web novel\nplatforms, and outputs of large language models. However, these contents are\nsusceptible to illegal piracy and potential misuse without proper security\nmeasures. This calls for a secure watermarking system to guarantee copyright\nprotection through leakage tracing or ownership identification. To effectively\ncombat piracy and protect copyrights, a multi-bit watermarking framework should\nbe able to embed adequate bits of information and extract the watermarks in a\nrobust manner despite possible corruption. In this work, we explore ways to\nadvance both payload and robustness by following a well-known proposition from\nimage watermarking and identify features in natural language that are invariant\nto minor corruption. Through a systematic analysis of the possible sources of\nerrors, we further propose a corruption-resistant infill model. Our full method\nimproves upon the previous work on robustness by +16.8% point on average on\nfour datasets, three corruption types, and two corruption ratios. Code\navailable at https://github.com/bangawayoo/nlp-watermarking.", "published": "2023-05-03 05:37:30", "link": "http://arxiv.org/abs/2305.01904v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Contrastive Learning of Sentence Embeddings from AI Feedback", "abstract": "Contrastive learning has become a popular approach in natural language\nprocessing, particularly for the learning of sentence embeddings. However, the\ndiscrete nature of natural language makes it difficult to ensure the quality of\npositive and negative sample pairs generated through data augmentation methods.\nAlthough supervised contrastive learning can produce more accurate sample pairs\nwith human feedback labels, it still lacks fine-grained training signals. In\nthis paper, we propose to improve \\textbf{C}ontrastive \\textbf{L}earning of\nsentence embeddings from \\textbf{AI} \\textbf{F}eedback \\textbf{(CLAIF)}. Our\nmethod utilizes AI feedback from large pre-trained language models (LLMs) to\nconstruct sample pairs with fine-grained sample similarity scores to improve\ncontrastive learning. Besides, we combine human feedback and AI feedback to\nprovide better supervision signals for supervised contrastive learning of\nsentence embeddings. Experimental results show that our method achieves\nstate-of-the-art performance on several semantic textual similarity (STS) and\ntransfer learning tasks compared to other unsupervised and supervised\ncontrastive learning methods.", "published": "2023-05-03 06:26:13", "link": "http://arxiv.org/abs/2305.01918v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Be an Alternative to Human Evaluations?", "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of\ntexts generated by machine learning models or written by humans. However, human\nevaluation is very difficult to reproduce and its quality is notoriously\nunstable, hindering fair comparisons among different natural language\nprocessing (NLP) models and algorithms. Recently, large language models (LLMs)\nhave demonstrated exceptional performance on unseen tasks when only the task\ninstructions are provided. In this paper, we explore if such an ability of the\nLLMs can be used as an alternative to human evaluation. We present the LLMs\nwith the exact same instructions, samples to be evaluated, and questions used\nto conduct human evaluation, and then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation. We use human evaluation and LLM\nevaluation to evaluate the texts in two NLP tasks: open-ended story generation\nand adversarial attacks. We show that the result of LLM evaluation is\nconsistent with the results obtained by expert human evaluation: the texts\nrated higher by human experts are also rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable over different formatting of the\ntask instructions and the sampling algorithm used to generate the answer. We\nare the first to show the potential of using LLMs to assess the quality of\ntexts and discuss the limitations and ethical considerations of LLM evaluation.", "published": "2023-05-03 07:28:50", "link": "http://arxiv.org/abs/2305.01937v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text\n  Documents via Semantic-Oriented Hierarchical Graphs", "abstract": "Discrete reasoning over table-text documents (e.g., financial reports) gains\nincreasing attention in recent two years. Existing works mostly simplify this\nchallenge by manually selecting and transforming document pages to structured\ntables and paragraphs, hindering their practical application. In this work, we\nexplore a more realistic problem setting in the form of TAT-DQA, i.e. to answer\nthe question over a visually-rich table-text document. Specifically, we propose\na novel Doc2SoarGraph framework with enhanced discrete reasoning capability by\nharnessing the differences and correlations among different elements (e.g.,\nquantities, dates) of the given question and document with Semantic-oriented\nhierarchical Graph structures. We conduct extensive experiments on TAT-DQA\ndataset, and the results show that our proposed framework outperforms the best\nbaseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score\nrespectively on the test set, achieving the new state-of-the-art.", "published": "2023-05-03 07:30:32", "link": "http://arxiv.org/abs/2305.01938v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SeqAug: Sequential Feature Resampling as a modality agnostic\n  augmentation method", "abstract": "Data augmentation is a prevalent technique for improving performance in\nvarious machine learning applications. We propose SeqAug, a modality-agnostic\naugmentation method that is tailored towards sequences of extracted features.\nThe core idea of SeqAug is to augment the sequence by resampling from the\nunderlying feature distribution. Resampling is performed by randomly selecting\nfeature dimensions and permuting them along the temporal axis. Experiments on\nCMU-MOSEI verify that SeqAug is modality agnostic; it can be successfully\napplied to a single modality or multiple modalities. We further verify its\ncompatibility with both recurrent and transformer architectures, and also\ndemonstrate comparable to state-of-the-art results.", "published": "2023-05-03 08:11:25", "link": "http://arxiv.org/abs/2305.01954v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Systematic Study of Knowledge Distillation for Natural Language\n  Generation with Pseudo-Target Training", "abstract": "Modern Natural Language Generation (NLG) models come with massive\ncomputational and storage requirements. In this work, we study the potential of\ncompressing them, which is crucial for real-world applications serving millions\nof users. We focus on Knowledge Distillation (KD) techniques, in which a small\nstudent model learns to imitate a large teacher model, allowing to transfer\nknowledge from the teacher to the student. In contrast to much of the previous\nwork, our goal is to optimize the model for a specific NLG task and a specific\ndataset. Typically in real-world applications, in addition to labeled data\nthere is abundant unlabeled task-specific data, which is crucial for attaining\nhigh compression rates via KD. In this work, we conduct a systematic study of\ntask-specific KD techniques for various NLG tasks under realistic assumptions.\nWe discuss the special characteristics of NLG distillation and particularly the\nexposure bias problem. Following, we derive a family of Pseudo-Target (PT)\naugmentation methods, substantially extending prior work on sequence-level KD.\nWe propose the Joint-Teaching method, which applies word-level KD to multiple\nPTs generated by both the teacher and the student. Finally, we validate our\nfindings in an extreme setup with no labeled examples using GPT-4 as the\nteacher. Our study provides practical model design observations and\ndemonstrates the effectiveness of PT training for task-specific KD in NLG.", "published": "2023-05-03 10:49:38", "link": "http://arxiv.org/abs/2305.02031v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Response-conditioned Turn-taking Prediction", "abstract": "Previous approaches to turn-taking and response generation in conversational\nsystems have treated it as a two-stage process: First, the end of a turn is\ndetected (based on conversation history), then the system generates an\nappropriate response. Humans, however, do not take the turn just because it is\nlikely, but also consider whether what they want to say fits the position. In\nthis paper, we present a model (an extension of TurnGPT) that conditions the\nend-of-turn prediction on both conversation history and what the next speaker\nwants to say. We found that our model consistently outperforms the baseline\nmodel in a variety of metrics. The improvement is most prominent in two\nscenarios where turn predictions can be ambiguous solely from the conversation\nhistory: 1) when the current utterance contains a statement followed by a\nquestion; 2) when the end of the current utterance semantically matches the\nresponse. Treating the turn-prediction and response-ranking as a one-stage\nprocess, our findings suggest that our model can be used as an incremental\nresponse ranker, which can be applied in various settings.", "published": "2023-05-03 11:06:50", "link": "http://arxiv.org/abs/2305.02036v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Curriculum View of Robust Loss Functions", "abstract": "Robust loss functions are designed to combat the adverse impacts of label\nnoise, whose robustness is typically supported by theoretical bounds agnostic\nto the training dynamics. However, these bounds may fail to characterize the\nempirical performance as it remains unclear why robust loss functions can\nunderfit. We show that most loss functions can be rewritten into a form with\nthe same class-score margin and different sample-weighting functions. The\nresulting curriculum view provides a straightforward analysis of the training\ndynamics, which helps attribute underfitting to diminished average sample\nweights and noise robustness to larger weights for clean samples. We show that\nsimple fixes to the curriculums can make underfitting robust loss functions\ncompetitive with the state-of-the-art, and training schedules can substantially\naffect the noise robustness even with robust loss functions. Code is available\nat \\url{github}.", "published": "2023-05-03 14:13:03", "link": "http://arxiv.org/abs/2305.02139v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-Shot Listwise Document Reranking with a Large Language Model", "abstract": "Supervised ranking methods based on bi-encoder or cross-encoder architectures\nhave shown success in multi-stage text ranking tasks, but they require large\namounts of relevance judgments as training data. In this work, we propose\nListwise Reranker with a Large Language Model (LRL), which achieves strong\nreranking effectiveness without using any task-specific training data.\nDifferent from the existing pointwise ranking methods, where documents are\nscored independently and ranked according to the scores, LRL directly generates\na reordered list of document identifiers given the candidate documents.\nExperiments on three TREC web search datasets demonstrate that LRL not only\noutperforms zero-shot pointwise methods when reranking first-stage retrieval\nresults, but can also act as a final-stage reranker to improve the top-ranked\nresults of a pointwise method for improved efficiency. Additionally, we apply\nour approach to subsets of MIRACL, a recent multilingual retrieval dataset,\nwith results showing its potential to generalize across different languages.", "published": "2023-05-03 14:45:34", "link": "http://arxiv.org/abs/2305.02156v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring Linguistic Properties of Monolingual BERTs with Typological\n  Classification among Languages", "abstract": "The impressive achievements of transformers force NLP researchers to delve\ninto how these models represent the underlying structure of natural language.\nIn this paper, we propose a novel standpoint to investigate the above issue:\nusing typological similarities among languages to observe how their respective\nmonolingual models encode structural information. We aim to layer-wise compare\ntransformers for typologically similar languages to observe whether these\nsimilarities emerge for particular layers. For this investigation, we propose\nto use Centered Kernel Alignment to measure similarity among weight matrices.\nWe found that syntactic typological similarity is consistent with the\nsimilarity between the weights in the middle layers, which are the pretrained\nBERT layers to which syntax encoding is generally attributed. Moreover, we\nobserve that a domain adaptation on semantically equivalent texts enhances this\nsimilarity among weight matrices.", "published": "2023-05-03 15:52:17", "link": "http://arxiv.org/abs/2305.02215v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Benefits of Label-Description Training for Zero-Shot Text\n  Classification", "abstract": "Pretrained language models have improved zero-shot text classification by\nallowing the transfer of semantic knowledge from the training data in order to\nclassify among specific label sets in downstream tasks. We propose a simple way\nto further improve zero-shot accuracies with minimal effort. We curate small\nfinetuning datasets intended to describe the labels for a task. Unlike typical\nfinetuning data, which has texts annotated with labels, our data simply\ndescribes the labels in language, e.g., using a few related terms,\ndictionary/encyclopedia entries, and short templates. Across a range of topic\nand sentiment datasets, our method is more accurate than zero-shot by 17-19%\nabsolute. It is also more robust to choices required for zero-shot\nclassification, such as patterns for prompting the model to classify and\nmappings from labels to tokens in the model's vocabulary. Furthermore, since\nour data merely describes the labels but does not use input texts, finetuning\non it yields a model that performs strongly on multiple text domains for a\ngiven label set, even improving over few-shot out-of-domain classification in\nmultiple settings.", "published": "2023-05-03 16:19:31", "link": "http://arxiv.org/abs/2305.02239v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-end Training and Decoding for Pivot-based Cascaded Translation\n  Model", "abstract": "Utilizing pivot language effectively can significantly improve low-resource\nmachine translation. Usually, the two translation models, source-pivot and\npivot-target, are trained individually and do not utilize the limited (source,\ntarget) parallel data. This work proposes an end-to-end training method for the\ncascaded translation model and configures an improved decoding algorithm. The\ninput of the pivot-target model is modified to weighted pivot embedding based\non the probability distribution output by the source-pivot model. This allows\nthe model to be trained end-to-end. In addition, we mitigate the inconsistency\nbetween tokens and probability distributions while using beam search in pivot\ndecoding. Experiments demonstrate that our method enhances the quality of\ntranslation.", "published": "2023-05-03 16:48:43", "link": "http://arxiv.org/abs/2305.02261v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating BERT-based Scientific Relation Classifiers for Scholarly\n  Knowledge Graph Construction on Digital Library Collections", "abstract": "The rapid growth of research publications has placed great demands on digital\nlibraries (DL) for advanced information management technologies. To cater to\nthese demands, techniques relying on knowledge-graph structures are being\nadvocated. In such graph-based pipelines, inferring semantic relations between\nrelated scientific concepts is a crucial step. Recently, BERT-based pre-trained\nmodels have been popularly explored for automatic relation classification.\nDespite significant progress, most of them were evaluated in different\nscenarios, which limits their comparability. Furthermore, existing methods are\nprimarily evaluated on clean texts, which ignores the digitization context of\nearly scholarly publications in terms of machine scanning and optical character\nrecognition (OCR). In such cases, the texts may contain OCR noise, in turn\ncreating uncertainty about existing classifiers' performances. To address these\nlimitations, we started by creating OCR-noisy texts based on three clean\ncorpora. Given these parallel corpora, we conducted a thorough empirical\nevaluation of eight Bert-based classification models by focusing on three\nfactors: (1) Bert variants; (2) classification strategies; and, (3) OCR noise\nimpacts. Experiments on clean data show that the domain-specific pre-trained\nBert is the best variant to identify scientific relations. The strategy of\npredicting a single relation each time outperforms the one simultaneously\nidentifying multiple relations in general. The optimal classifier's performance\ncan decline by around 10% to 20% in F-score on the noisy corpora. Insights\ndiscussed in this study can help DL stakeholders select techniques for building\noptimal knowledge-graph-based systems.", "published": "2023-05-03 17:32:16", "link": "http://arxiv.org/abs/2305.02291v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Evaluating the Efficacy of Length-Controllable Machine Translation", "abstract": "Length-controllable machine translation is a type of constrained translation.\nIt aims to contain the original meaning as much as possible while controlling\nthe length of the translation. We can use automatic summarization or machine\ntranslation evaluation metrics for length-controllable machine translation, but\nthis is not necessarily suitable and accurate. This work is the first attempt\nto evaluate the automatic metrics for length-controllable machine translation\ntasks systematically. We conduct a rigorous human evaluation on two translation\ndirections and evaluate 18 summarization or translation evaluation metrics. We\nfind that BLEURT and COMET have the highest correlation with human evaluation\nand are most suitable as evaluation metrics for length-controllable machine\ntranslation.", "published": "2023-05-03 17:50:33", "link": "http://arxiv.org/abs/2305.02300v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal\n  Infillings", "abstract": "Recent advances in large language models elicit reasoning in a\nchain-of-thought that allows models to decompose problems in a human-like\nfashion. Though this paradigm improves multi-step reasoning ability in language\nmodels, it is limited by being unimodal and applied mainly to\nquestion-answering tasks. We claim that incorporating visual augmentation into\nreasoning is essential, especially for complex, imaginative tasks.\nConsequently, we introduce VCoT, a novel method that leverages chain-of-thought\nprompting with vision-language grounding to recursively bridge the logical gaps\nwithin sequential data. Our method uses visual guidance to generate synthetic\nmultimodal infillings that add consistent and novel information to reduce the\nlogical gaps for downstream tasks that can benefit from temporal reasoning, as\nwell as provide interpretability into models' multi-step reasoning. We apply\nVCoT to the Visual Storytelling and WikiHow summarization datasets and\ndemonstrate through human evaluation that VCoT offers novel and consistent\nsynthetic data augmentation beating chain-of-thought baselines, which can be\nused to enhance downstream performance.", "published": "2023-05-03 17:58:29", "link": "http://arxiv.org/abs/2305.02317v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Using Language Models on Low-end Hardware", "abstract": "This paper evaluates the viability of using fixed language models for\ntraining text classification networks on low-end hardware. We combine language\nmodels with a CNN architecture and put together a comprehensive benchmark with\n8 datasets covering single-label and multi-label classification of topic,\nsentiment, and genre. Our observations are distilled into a list of trade-offs,\nconcluding that there are scenarios, where not fine-tuning a language model\nyields competitive effectiveness at faster training, requiring only a quarter\nof the memory compared to fine-tuning.", "published": "2023-05-03 18:00:03", "link": "http://arxiv.org/abs/2305.02350v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Approximating CKY with Transformers", "abstract": "We investigate the ability of transformer models to approximate the CKY\nalgorithm, using them to directly predict a sentence's parse and thus avoid the\nCKY algorithm's cubic dependence on sentence length. We find that on standard\nconstituency parsing benchmarks this approach achieves competitive or better\nperformance than comparable parsers that make use of CKY, while being faster.\nWe also evaluate the viability of this approach for parsing under\n\\textit{random} PCFGs. Here we find that performance declines as the grammar\nbecomes more ambiguous, suggesting that the transformer is not fully capturing\nthe CKY computation. However, we also find that incorporating additional\ninductive bias is helpful, and we propose a novel approach that makes use of\ngradients with respect to chart representations in predicting the parse, in\nanalogy with the CKY algorithm being a subgradient of a partition function\nvariant with respect to the chart.", "published": "2023-05-03 18:55:09", "link": "http://arxiv.org/abs/2305.02386v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PTP: Boosting Stability and Performance of Prompt Tuning with\n  Perturbation-Based Regularizer", "abstract": "Recent studies show that prompt tuning can better leverage the power of large\nlanguage models than fine-tuning on downstream natural language understanding\ntasks. However, the existing prompt tuning methods have training instability\nissues, as the variance of scores under different random seeds is quite large.\nTo address this critical problem, we first investigate and find that the loss\nlandscape of vanilla prompt tuning is precipitous when it is visualized, where\na slight change of input data can cause a big fluctuation in the loss\nlandscape. This is an essential factor that leads to the instability of prompt\ntuning. Based on this observation, we introduce perturbation-based\nregularizers, which can smooth the loss landscape, into prompt tuning. We\npropose a new algorithm, called Prompt Tuning with Perturbation-based\nregularizer~(PTP), which can not only alleviate training instability\ndramatically but also boost the performance of prompt tuning. We design two\nkinds of perturbation-based regularizers, including random-noise-based and\nadversarial-based. In particular, our proposed perturbations are flexible on\nboth text space and embedding space. Extensive experiments show the\neffectiveness of our proposed methods in stabilizing the training. Our new\nalgorithms improve the state-of-the-art prompt tuning methods by 1.94\\% and\n2.34\\% on SuperGLUE and FewGLUE benchmarks, respectively.", "published": "2023-05-03 20:30:51", "link": "http://arxiv.org/abs/2305.02423v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory", "abstract": "With direct access to human-written reference as memory, retrieval-augmented\ngeneration has achieved much progress in a wide range of text generation tasks.\nSince better memory would typically prompt better generation~(we define this as\nprimal problem). The traditional approach for memory retrieval involves\nselecting memory that exhibits the highest similarity to the input. However,\nthis method is constrained by the quality of the fixed corpus from which memory\nis retrieved. In this paper, by exploring the duality of the primal problem:\nbetter generation also prompts better memory, we propose a novel framework,\nselfmem, which addresses this limitation by iteratively employing a\nretrieval-augmented generator to create an unbounded memory pool and using a\nmemory selector to choose one output as memory for the subsequent generation\nround. This enables the model to leverage its own output, referred to as\nself-memory, for improved generation. We evaluate the effectiveness of selfmem\non three distinct text generation tasks: neural machine translation,\nabstractive text summarization, and dialogue generation, under two generation\nparadigms: fine-tuned small model and few-shot LLM. Our approach achieves\nstate-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),\nand BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in\nenhancing retrieval-augmented generation models. Furthermore, we conduct\nthorough analyses of each component in the selfmem framework to identify\nbottlenecks and provide insights for future research.", "published": "2023-05-03 21:40:54", "link": "http://arxiv.org/abs/2305.02437v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transfer and Active Learning for Dissonance Detection: Addressing the\n  Rare-Class Challenge", "abstract": "While transformer-based systems have enabled greater accuracies with fewer\ntraining examples, data acquisition obstacles still persist for rare-class\ntasks -- when the class label is very infrequent (e.g. < 5% of samples). Active\nlearning has in general been proposed to alleviate such challenges, but choice\nof selection strategy, the criteria by which rare-class examples are chosen,\nhas not been systematically evaluated. Further, transformers enable iterative\ntransfer-learning approaches. We propose and investigate transfer- and active\nlearning solutions to the rare class problem of dissonance detection through\nutilizing models trained on closely related tasks and the evaluation of\nacquisition strategies, including a proposed probability-of-rare-class (PRC)\napproach. We perform these experiments for a specific rare class problem:\ncollecting language samples of cognitive dissonance from social media. We find\nthat PRC is a simple and effective strategy to guide annotations and ultimately\nimprove model accuracy while transfer-learning in a specific order can improve\nthe cold-start performance of the learner but does not benefit iterations of\nactive learning.", "published": "2023-05-03 23:29:05", "link": "http://arxiv.org/abs/2305.02459v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal\n  Assistive Writing", "abstract": "In legal document writing, one of the key elements is properly citing the\ncase laws and other sources to substantiate claims and arguments. Understanding\nthe legal domain and identifying appropriate citation context or cite-worthy\nsentences are challenging tasks that demand expensive manual annotation. The\npresence of jargon, language semantics, and high domain specificity makes legal\nlanguage complex, making any associated legal task hard for automation. The\ncurrent work focuses on the problem of citation-worthiness identification. It\nis designed as the initial step in today's citation recommendation systems to\nlighten the burden of extracting an adequate set of citation contexts. To\naccomplish this, we introduce a labeled dataset of 178M sentences for\ncitation-worthiness detection in the legal domain from the Caselaw Access\nProject (CAP). The performance of various deep learning models was examined on\nthis novel dataset. The domain-specific pre-trained model tends to outperform\nother models, with an 88% F1-score for the citation-worthiness detection task.", "published": "2023-05-03 04:20:56", "link": "http://arxiv.org/abs/2305.03508v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPTutor: a ChatGPT-powered programming tool for code explanation", "abstract": "Learning new programming skills requires tailored guidance. With the\nemergence of advanced Natural Language Generation models like the ChatGPT API,\nthere is now a possibility of creating a convenient and personalized tutoring\nsystem with AI for computer science education. This paper presents GPTutor, a\nChatGPT-powered programming tool, which is a Visual Studio Code extension using\nthe ChatGPT API to provide programming code explanations. By integrating Visual\nStudio Code API, GPTutor can comprehensively analyze the provided code by\nreferencing the relevant source codes. As a result, GPTutor can use designed\nprompts to explain the selected code with a pop-up message. GPTutor is now\npublished at the Visual Studio Code Extension Marketplace, and its source code\nis openly accessible on GitHub. Preliminary evaluation indicates that GPTutor\ndelivers the most concise and accurate explanations compared to vanilla ChatGPT\nand GitHub Copilot. Moreover, the feedback from students and teachers indicated\nthat GPTutor is user-friendly and can explain given codes satisfactorily.\nFinally, we discuss possible future research directions for GPTutor. This\nincludes enhancing its performance and personalization via further prompt\nprogramming, as well as evaluating the effectiveness of GPTutor with real\nusers.", "published": "2023-05-03 02:30:13", "link": "http://arxiv.org/abs/2305.01863v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.HC"}
{"title": "Analysing the Impact of Audio Quality on the Use of Naturalistic\n  Long-Form Recordings for Infant-Directed Speech Research", "abstract": "Modelling of early language acquisition aims to understand how infants\nbootstrap their language skills. The modelling encompasses properties of the\ninput data used for training the models, the cognitive hypotheses and their\nalgorithmic implementations being tested, and the evaluation methodologies to\ncompare models to human data. Recent developments have enabled the use of more\nnaturalistic training data for computational models. This also motivates\ndevelopment of more naturalistic tests of model behaviour. A crucial step\ntowards such an aim is to develop representative speech datasets consisting of\nspeech heard by infants in their natural environments. However, a major\ndrawback of such recordings is that they are typically noisy, and it is\ncurrently unclear how the sound quality could affect analyses and modelling\nexperiments conducted on such data. In this paper, we explore this aspect for\nthe case of infant-directed speech (IDS) and adult-directed speech (ADS)\nanalysis. First, we manually and automatically annotated audio quality of\nutterances extracted from two corpora of child-centred long-form recordings (in\nEnglish and French). We then compared acoustic features of IDS and ADS in an\nin-lab dataset and across different audio quality subsets of naturalistic data.\nFinally, we assessed how the audio quality and recording environment may change\nthe conclusions of a modelling analysis using a recent self-supervised learning\nmodel. Our results show that the use of modest and high audio quality\nnaturalistic speech data result in largely similar conclusions on IDS and ADS\nin terms of acoustic analyses and modelling experiments. We also found that an\nautomatic sound quality assessment tool can be used to screen out useful parts\nof long-form recordings for a closer analysis with comparable results to that\nof manual quality annotation.", "published": "2023-05-03 08:25:37", "link": "http://arxiv.org/abs/2305.01965v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Considerations for Ethical Speech Recognition Datasets", "abstract": "Speech AI Technologies are largely trained on publicly available datasets or\nby the massive web-crawling of speech. In both cases, data acquisition focuses\non minimizing collection effort, without necessarily taking the data subjects'\nprotection or user needs into consideration. This results to models that are\nnot robust when used on users who deviate from the dominant demographics in the\ntraining set, discriminating individuals having different dialects, accents,\nspeaking styles, and disfluencies. In this talk, we use automatic speech\nrecognition as a case study and examine the properties that ethical speech\ndatasets should possess towards responsible AI applications. We showcase\ndiversity issues, inclusion practices, and necessary considerations that can\nimprove trained models, while facilitating model explainability and protecting\nusers and data subjects. We argue for the legal & privacy protection of data\nsubjects, targeted data sampling corresponding to user demographics & needs,\nappropriate meta data that ensure explainability & accountability in cases of\nmodel failure, and the sociotechnical \\& situated model design. We hope this\ntalk can inspire researchers \\& practitioners to design and use more\nhuman-centric datasets in speech technologies and other domains, in ways that\nempower and respect users, while improving machine learning models' robustness\nand utility.", "published": "2023-05-03 12:38:14", "link": "http://arxiv.org/abs/2305.02081v1", "categories": ["cs.CY", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CY"}
{"title": "Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space", "abstract": "Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.", "published": "2023-05-03 14:33:23", "link": "http://arxiv.org/abs/2305.02151v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WangLab at MEDIQA-Chat 2023: Clinical Note Generation from\n  Doctor-Patient Conversations using Large Language Models", "abstract": "This paper describes our submission to the MEDIQA-Chat 2023 shared task for\nautomatic clinical note generation from doctor-patient conversations. We report\nresults for two approaches: the first fine-tunes a pre-trained language model\n(PLM) on the shared task data, and the second uses few-shot in-context learning\n(ICL) with a large language model (LLM). Both achieve high performance as\nmeasured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and\nfirst, respectively, of all submissions to the shared task. Expert human\nscrutiny indicates that notes generated via the ICL-based approach with GPT-4\nare preferred about as often as human-written notes, making it a promising path\ntoward automated note generation from doctor-patient conversations.", "published": "2023-05-03 15:58:28", "link": "http://arxiv.org/abs/2305.02220v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "M2-CTTS: End-to-End Multi-scale Multi-modal Conversational\n  Text-to-Speech Synthesis", "abstract": "Conversational text-to-speech (TTS) aims to synthesize speech with proper\nprosody of reply based on the historical conversation. However, it is still a\nchallenge to comprehensively model the conversation, and a majority of\nconversational TTS systems only focus on extracting global information and omit\nlocal prosody features, which contain important fine-grained information like\nkeywords and emphasis. Moreover, it is insufficient to only consider the\ntextual features, and acoustic features also contain various prosody\ninformation. Hence, we propose M2-CTTS, an end-to-end multi-scale multi-modal\nconversational text-to-speech system, aiming to comprehensively utilize\nhistorical conversation and enhance prosodic expression. More specifically, we\ndesign a textual context module and an acoustic context module with both\ncoarse-grained and fine-grained modeling. Experimental results demonstrate that\nour model mixed with fine-grained context information and additionally\nconsidering acoustic features achieves better prosody performance and\nnaturalness in CMOS tests.", "published": "2023-05-03 16:59:38", "link": "http://arxiv.org/abs/2305.02269v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less\n  Training Data and Smaller Model Sizes", "abstract": "Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for training small models\nwithin a multi-task framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to few-shot prompted LLMs, we achieve better\nperformance using substantially smaller model sizes. Third, we reduce both the\nmodel size and the amount of data required to outperform LLMs; our finetuned\n770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%\nof available data on a benchmark, whereas standard finetuning the same T5 model\nstruggles to match even by using 100% of the dataset. We release the code at:\nhttps://github.com/google-research/distilling-step-by-step .", "published": "2023-05-03 17:50:56", "link": "http://arxiv.org/abs/2305.02301v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Entity-Based Evaluation of Political Bias in Automatic Summarization", "abstract": "Growing literature has shown that NLP systems may encode social biases;\nhowever, the political bias of summarization models remains relatively unknown.\nIn this work, we use an entity replacement method to investigate the portrayal\nof politicians in automatically generated summaries of news articles. We\ndevelop an entity-based computational framework to assess the sensitivities of\nseveral extractive and abstractive summarizers to the politicians Donald Trump\nand Joe Biden. We find consistent differences in these summaries upon entity\nreplacement, such as reduced emphasis of Trump's presence in the context of the\nsame article and a more individualistic representation of Trump with respect to\nthe collective US government (i.e., administration). These summary\ndissimilarities are most prominent when the entity is heavily featured in the\nsource article. Our characterization provides a foundation for future studies\nof bias in summarization and for normative discussions on the ideal qualities\nof automatic summaries.", "published": "2023-05-03 17:59:59", "link": "http://arxiv.org/abs/2305.02321v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Novel Plagiarism Detection Approach Combining BERT-based Word\n  Embedding, Attention-based LSTMs and an Improved Differential Evolution\n  Algorithm", "abstract": "Detecting plagiarism involves finding similar items in two different sources.\nIn this article, we propose a novel method for detecting plagiarism that is\nbased on attention mechanism-based long short-term memory (LSTM) and\nbidirectional encoder representations from transformers (BERT) word embedding,\nenhanced with optimized differential evolution (DE) method for pre-training and\na focal loss function for training. BERT could be included in a downstream task\nand fine-tuned as a task-specific BERT can be included in a downstream task and\nfine-tuned as a task-specific structure, while the trained BERT model is\ncapable of detecting various linguistic characteristics. Unbalanced\nclassification is one of the primary issues with plagiarism detection. We\nsuggest a focal loss-based training technique that carefully learns minority\nclass instances to solve this. Another issue that we tackle is the training\nphase itself, which typically employs gradient-based methods like\nback-propagation for the learning process and thus suffers from some drawbacks,\nincluding sensitivity to initialization. To initiate the BP process, we suggest\na novel DE algorithm that makes use of a clustering-based mutation operator.\nHere, a winning cluster is identified for the current DE population, and a\nfresh updating method is used to produce potential answers. We evaluate our\nproposed approach on three benchmark datasets ( MSRP, SNLI, and SemEval2014)\nand demonstrate that it performs well when compared to both conventional and\npopulation-based methods.", "published": "2023-05-03 18:26:47", "link": "http://arxiv.org/abs/2305.02374v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Defending against Insertion-based Textual Backdoor Attacks via\n  Attribution", "abstract": "Textual backdoor attack, as a novel attack model, has been shown to be\neffective in adding a backdoor to the model during training. Defending against\nsuch backdoor attacks has become urgent and important. In this paper, we\npropose AttDef, an efficient attribution-based pipeline to defend against two\ninsertion-based poisoning attacks, BadNL and InSent. Specifically, we regard\nthe tokens with larger attribution scores as potential triggers since larger\nattribution words contribute more to the false prediction results and therefore\nare more likely to be poison triggers. Additionally, we further utilize an\nexternal pre-trained language model to distinguish whether input is poisoned or\nnot. We show that our proposed method can generalize sufficiently well in two\ncommon attack scenarios (poisoning training data and testing data), which\nconsistently improves previous methods. For instance, AttDef can successfully\nmitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34%\n(3.99% up) under pre-training and post-training attack defense respectively,\nachieving the new state-of-the-art performance on prediction recovery over four\nbenchmark datasets.", "published": "2023-05-03 19:29:26", "link": "http://arxiv.org/abs/2305.02394v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Plan, Eliminate, and Track -- Language Models are Good Teachers for\n  Embodied Agents", "abstract": "Pre-trained large language models (LLMs) capture procedural knowledge about\nthe world. Recent work has leveraged LLM's ability to generate abstract plans\nto simplify challenging control tasks, either by action scoring, or action\nmodeling (fine-tuning). However, the transformer architecture inherits several\nconstraints that make it difficult for the LLM to directly serve as the agent:\ne.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,\nand incompatibility with non-text environments. To maintain compatibility with\na low-level trainable actor, we propose to instead use the knowledge in LLMs to\nsimplify the control problem, rather than solving it. We propose the Plan,\nEliminate, and Track (PET) framework. The Plan module translates a task\ndescription into a list of high-level sub-tasks. The Eliminate module masks out\nirrelevant objects and receptacles from the observation for the current\nsub-task. Finally, the Track module determines whether the agent has\naccomplished each sub-task. On the AlfWorld instruction following benchmark,\nthe PET framework leads to a significant 15% improvement over SOTA for\ngeneralization to human goal specifications.", "published": "2023-05-03 20:11:22", "link": "http://arxiv.org/abs/2305.02412v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying the Dissimilarity of Texts", "abstract": "Quantifying the dissimilarity of two texts is an important aspect of a number\nof natural language processing tasks, including semantic information retrieval,\ntopic classification, and document clustering. In this paper, we compared the\nproperties and performance of different dissimilarity measures $D$ using three\ndifferent representations of texts -- vocabularies, word frequency\ndistributions, and vector embeddings -- and three simple tasks -- clustering\ntexts by author, subject, and time period. Using the Project Gutenberg\ndatabase, we found that the generalised Jensen--Shannon divergence applied to\nword frequencies performed strongly across all tasks, that $D$'s based on\nvector embedding representations led to stronger performance for smaller texts,\nand that the optimal choice of approach was ultimately task-dependent. We also\ninvestigated, both analytically and numerically, the behaviour of the different\n$D$'s when the two texts varied in length by a factor $h$. We demonstrated that\nthe (natural) estimator of the Jaccard distance between vocabularies was\ninconsistent and computed explicitly the $h$-dependency of the bias of the\nestimator of the generalised Jensen--Shannon divergence applied to word\nfrequencies. We also found numerically that the Jensen--Shannon divergence and\nembedding-based approaches were robust to changes in $h$, while the Jaccard\ndistance was not.", "published": "2023-05-03 22:59:08", "link": "http://arxiv.org/abs/2305.02457v1", "categories": ["cs.CL", "cond-mat.stat-mech", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Training Natural Language Processing Models on Encrypted Text for\n  Enhanced Privacy", "abstract": "With the increasing use of cloud-based services for training and deploying\nmachine learning models, data privacy has become a major concern. This is\nparticularly important for natural language processing (NLP) models, which\noften process sensitive information such as personal communications and\nconfidential documents. In this study, we propose a method for training NLP\nmodels on encrypted text data to mitigate data privacy concerns while\nmaintaining similar performance to models trained on non-encrypted data. We\ndemonstrate our method using two different architectures, namely\nDoc2Vec+XGBoost and Doc2Vec+LSTM, and evaluate the models on the 20 Newsgroups\ndataset. Our results indicate that both encrypted and non-encrypted models\nachieve comparable performance, suggesting that our encryption method is\neffective in preserving data privacy without sacrificing model accuracy. In\norder to replicate our experiments, we have provided a Colab notebook at the\nfollowing address: https://t.ly/lR-TP", "published": "2023-05-03 00:37:06", "link": "http://arxiv.org/abs/2305.03497v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "ChatGraph: Interpretable Text Classification by Converting ChatGPT\n  Knowledge to Graphs", "abstract": "ChatGPT, as a recently launched large language model (LLM), has shown\nsuperior performance in various natural language processing (NLP) tasks.\nHowever, two major limitations hinder its potential applications: (1) the\ninflexibility of finetuning on downstream tasks and (2) the lack of\ninterpretability in the decision-making process. To tackle these limitations,\nwe propose a novel framework that leverages the power of ChatGPT for specific\ntasks, such as text classification, while improving its interpretability. The\nproposed framework conducts a knowledge graph extraction task to extract\nrefined and structural knowledge from the raw data using ChatGPT. The rich\nknowledge is then converted into a graph, which is further used to train an\ninterpretable linear classifier to make predictions. To evaluate the\neffectiveness of our proposed method, we conduct experiments on four datasets.\nThe result shows that our method can significantly improve the performance\ncompared to directly utilizing ChatGPT for text classification tasks. And our\nmethod provides a more transparent decision-making process compared with\nprevious text classification methods.", "published": "2023-05-03 19:57:43", "link": "http://arxiv.org/abs/2305.03513v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Judgments of research co-created by generative AI: experimental evidence", "abstract": "The introduction of ChatGPT has fuelled a public debate on the use of\ngenerative AI (large language models; LLMs), including its use by researchers.\nIn the current work, we test whether delegating parts of the research process\nto LLMs leads people to distrust and devalue researchers and scientific output.\nParticipants (N=402) considered a researcher who delegates elements of the\nresearch process to a PhD student or LLM, and rated (1) moral acceptability,\n(2) trust in the scientist to oversee future projects, and (3) the accuracy and\nquality of the output. People judged delegating to an LLM as less acceptable\nthan delegating to a human (d = -0.78). Delegation to an LLM also decreased\ntrust to oversee future research projects (d = -0.80), and people thought the\nresults would be less accurate and of lower quality (d = -0.85). We discuss how\nthis devaluation might transfer into the underreporting of generative AI use.", "published": "2023-05-03 15:57:39", "link": "http://arxiv.org/abs/2305.11873v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "econ.GN", "q-fin.EC", "K.4.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Diverse and Vivid Sound Generation from Text Descriptions", "abstract": "Previous audio generation mainly focuses on specified sound classes such as\nspeech or music, whose form and content are greatly restricted. In this paper,\nwe go beyond specific audio generation by using natural language description as\na clue to generate broad sounds. Unlike visual information, a text description\nis concise by its nature but has rich hidden meanings beneath, which poses a\nhigher possibility and complexity on the audio to be generated. A\nVariation-Quantized GAN is used to train a codebook learning discrete\nrepresentations of spectrograms. For a given text description, its pre-trained\nembedding is fed to a Transformer to sample codebook indices to decode a\nspectrogram to be further transformed into waveform by a melgan vocoder. The\ngenerated waveform has high quality and fidelity while excellently\ncorresponding to the given text. Experiments show that our proposed method is\ncapable of generating natural, vivid audios, achieving superb quantitative and\nqualitative results.", "published": "2023-05-03 08:52:40", "link": "http://arxiv.org/abs/2305.01980v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improved Vocal Effort Transfer Vector Estimation for Vocal Effort-Robust\n  Speaker Verification", "abstract": "Despite the maturity of modern speaker verification technology, its\nperformance still significantly degrades when facing non-neutrally-phonated\n(e.g., shouted and whispered) speech. To address this issue, in this paper, we\npropose a new speaker embedding compensation method based on a minimum mean\nsquare error (MMSE) estimator. This method models the joint distribution of the\nvocal effort transfer vector and non-neutrally-phonated embedding spaces and\noperates in a principal component analysis domain to cope with\nnon-neutrally-phonated speech data scarcity. Experiments are carried out using\na cutting-edge speaker verification system integrating a powerful\nself-supervised pre-trained model for speech representation. In comparison with\na state-of-the-art embedding compensation method, the proposed MMSE estimator\nyields superior and competitive equal error rate results when tackling shouted\nand whispered speech, respectively.", "published": "2023-05-03 14:29:01", "link": "http://arxiv.org/abs/2305.02147v3", "categories": ["eess.AS", "cs.HC"], "primary_category": "eess.AS"}
{"title": "HappyQuokka System for ICASSP 2023 Auditory EEG Challenge", "abstract": "This report describes our submission to Task 2 of the Auditory EEG Decoding\nChallenge at ICASSP 2023 Signal Processing Grand Challenge (SPGC). Task 2 is a\nregression problem that focuses on reconstructing a speech envelope from an EEG\nsignal. For the task, we propose a pre-layer normalized feed-forward\ntransformer (FFT) architecture. For within-subjects generation, we additionally\nutilize an auxiliary global conditioner which provides our model with\nadditional information about seen individuals. Experimental results show that\nour proposed method outperforms the VLAAI baseline and all other submitted\nsystems. Notably, it demonstrates significant improvements on the\nwithin-subjects task, likely thanks to our use of the auxiliary global\nconditioner. In terms of evaluation metrics set by the challenge, we obtain\nPearson correlation values of 0.1895 0.0869 for the within-subjects generation\ntest and 0.0976 0.0444 for the heldout-subjects test. We release the training\ncode for our model online.", "published": "2023-05-03 07:57:11", "link": "http://arxiv.org/abs/2305.06806v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Improvement of Audio-Text Cross-Modal Representations", "abstract": "Recent advances in using language models to obtain cross-modal audio-text\nrepresentations have overcome the limitations of conventional training\napproaches that use predefined labels. This has allowed the community to make\nprogress in tasks like zero-shot classification, which would otherwise not be\npossible. However, learning such representations requires a large amount of\nhuman-annotated audio-text pairs. In this paper, we study unsupervised\napproaches to improve the learning framework of such representations with\nunpaired text and audio. We explore domain-unspecific and domain-specific\ncuration methods to create audio-text pairs that we use to further improve the\nmodel. We also show that when domain-specific curation is used in conjunction\nwith a soft-labeled contrastive loss, we are able to obtain significant\nimprovement in terms of zero-shot classification performance on downstream\nsound event classification or acoustic scene classification tasks.", "published": "2023-05-03 02:30:46", "link": "http://arxiv.org/abs/2305.01864v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Detect Novel and Fine-Grained Acoustic Sequences Using\n  Pretrained Audio Representations", "abstract": "This work investigates pretrained audio representations for few shot Sound\nEvent Detection. We specifically address the task of few shot detection of\nnovel acoustic sequences, or sound events with semantically meaningful temporal\nstructure, without assuming access to non-target audio. We develop procedures\nfor pretraining suitable representations, and methods which transfer them to\nour few shot learning scenario. Our experiments evaluate the general purpose\nutility of our pretrained representations on AudioSet, and the utility of\nproposed few shot methods via tasks constructed from real-world acoustic\nsequences. Our pretrained embeddings are suitable to the proposed task, and\nenable multiple aspects of our few shot framework.", "published": "2023-05-03 18:41:24", "link": "http://arxiv.org/abs/2305.02382v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Plug-and-Play Multilingual Few-shot Spoken Words Recognition", "abstract": "As technology advances and digital devices become prevalent, seamless\nhuman-machine communication is increasingly gaining significance. The growing\nadoption of mobile, wearable, and other Internet of Things (IoT) devices has\nchanged how we interact with these smart devices, making accurate spoken words\nrecognition a crucial component for effective interaction. However, building\nrobust spoken words detection system that can handle novel keywords remains\nchallenging, especially for low-resource languages with limited training data.\nHere, we propose PLiX, a multilingual and plug-and-play keyword spotting system\nthat leverages few-shot learning to harness massive real-world data and enable\nthe recognition of unseen spoken words at test-time. Our few-shot deep models\nare learned with millions of one-second audio clips across 20 languages,\nachieving state-of-the-art performance while being highly efficient. Extensive\nevaluations show that PLiX can generalize to novel spoken words given as few as\njust one support example and performs well on unseen languages out of the box.\nWe release models and inference code to serve as a foundation for future\nresearch and voice-enabled user interface development for emerging devices.", "published": "2023-05-03 18:58:14", "link": "http://arxiv.org/abs/2305.03058v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AV-SAM: Segment Anything Model Meets Audio-Visual Localization and\n  Segmentation", "abstract": "Segment Anything Model (SAM) has recently shown its powerful effectiveness in\nvisual segmentation tasks. However, there is less exploration concerning how\nSAM works on audio-visual tasks, such as visual sound localization and\nsegmentation. In this work, we propose a simple yet effective audio-visual\nlocalization and segmentation framework based on the Segment Anything Model,\nnamely AV-SAM, that can generate sounding object masks corresponding to the\naudio. Specifically, our AV-SAM simply leverages pixel-wise audio-visual fusion\nacross audio features and visual features from the pre-trained image encoder in\nSAM to aggregate cross-modal representations. Then, the aggregated cross-modal\nfeatures are fed into the prompt encoder and mask decoder to generate the final\naudio-visual segmentation masks. We conduct extensive experiments on\nFlickr-SoundNet and AVSBench datasets. The results demonstrate that the\nproposed AV-SAM can achieve competitive performance on sounding object\nlocalization and segmentation.", "published": "2023-05-03 00:33:52", "link": "http://arxiv.org/abs/2305.01836v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
