{"title": "Non-distributional Word Vector Representations", "abstract": "Data-driven representation learning for words is a technique of central\nimportance in NLP. While indisputably useful as a source of features in\ndownstream tasks, such vectors tend to consist of uninterpretable components\nwhose relationship to the categories of traditional lexical semantic theories\nis tenuous at best. We present a method for constructing interpretable word\nvectors from hand-crafted linguistic resources like WordNet, FrameNet etc.\nThese vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We\nanalyze their performance on state-of-the-art evaluation methods for\ndistributional models of word vectors and find they are competitive to standard\ndistributional approaches.", "published": "2015-06-17 07:40:14", "link": "http://arxiv.org/abs/1506.05230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pragmatic Side Effects", "abstract": "In the quest to give a formal compositional semantics to natural languages,\nsemanticists have started turning their attention to phenomena that have been\nalso considered as parts of pragmatics (e.g., discourse anaphora and\npresupposition projection). To account for these phenomena, the very kinds of\nmeanings assigned to words and phrases are often revisited. To be more\nspecific, in the prevalent paradigm of modeling natural language denotations\nusing the simply-typed lambda calculus (higher-order logic) this means\nrevisiting the types of denotations assigned to individual parts of speech.\nHowever, the lambda calculus also serves as a fundamental theory of\ncomputation, and in the study of computation, similar type shifts have been\nemployed to give a meaning to side effects. Side effects in programming\nlanguages correspond to actions that go beyond the lexical scope of an\nexpression (a thrown exception might propagate throughout a program, a variable\nmodified at one point might later be read at an another) or even beyond the\nscope of the program itself (a program might interact with the outside world by\ne.g., printing documents, making sounds, operating robotic limbs...).", "published": "2015-06-17 19:55:13", "link": "http://arxiv.org/abs/1506.05676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Editorial for the First Workshop on Mining Scientific Papers:\n  Computational Linguistics and Bibliometrics", "abstract": "The workshop \"Mining Scientific Papers: Computational Linguistics and\nBibliometrics\" (CLBib 2015), co-located with the 15th International Society of\nScientometrics and Informetrics Conference (ISSI 2015), brought together\nresearchers in Bibliometrics and Computational Linguistics in order to study\nthe ways Bibliometrics can benefit from large-scale text analytics and sense\nmining of scientific papers, thus exploring the interdisciplinarity of\nBibliometrics and Natural Language Processing (NLP). The goals of the workshop\nwere to answer questions like: How can we enhance author network analysis and\nBibliometrics using data obtained by text analytics? What insights can NLP\nprovide on the structure of scientific writing, on citation networks, and on\nin-text citation analysis? This workshop is the first step to foster the\nreflection on the interdisciplinarity and the benefits that the two disciplines\nBibliometrics and Natural Language Processing can drive from it.", "published": "2015-06-17 18:03:25", "link": "http://arxiv.org/abs/1506.05402v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning Contextualized Semantics from Co-occurring Terms via a Siamese\n  Architecture", "abstract": "One of the biggest challenges in Multimedia information retrieval and\nunderstanding is to bridge the semantic gap by properly modeling concept\nsemantics in context. The presence of out of vocabulary (OOV) concepts\nexacerbates this difficulty. To address the semantic gap issues, we formulate a\nproblem on learning contextualized semantics from descriptive terms and propose\na novel Siamese architecture to model the contextualized semantics from\ndescriptive terms. By means of pattern aggregation and probabilistic topic\nmodels, our Siamese architecture captures contextualized semantics from the\nco-occurring descriptive terms via unsupervised learning, which leads to a\nconcept embedding space of the terms in context. Furthermore, the co-occurring\nOOV concepts can be easily represented in the learnt concept embedding space.\nThe main properties of the concept embedding space are demonstrated via\nvisualization. Using various settings in semantic priming, we have carried out\na thorough evaluation by comparing our approach to a number of state-of-the-art\nmethods on six annotation corpora in different domains, i.e., MagTag5K, CAL500\nand Million Song Dataset in the music domain as well as Corel5K, LabelMe and\nSUNDatabase in the image domain. Experimental results on semantic priming\nsuggest that our approach outperforms those state-of-the-art methods\nconsiderably in various aspects.", "published": "2015-06-17 23:03:43", "link": "http://arxiv.org/abs/1506.05514v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "I.2.6"], "primary_category": "cs.IR"}
