{"title": "A Sentence Simplification System for Improving Relation Extraction", "abstract": "In this demo paper, we present a text simplification approach that is\ndirected at improving the performance of state-of-the-art Open Relation\nExtraction (RE) systems. As syntactically complex sentences often pose a\nchallenge for current Open RE approaches, we have developed a simplification\nframework that performs a pre-processing step by taking a single sentence as\ninput and using a set of syntactic-based transformation rules to create a\ntextual input that is easier to process for subsequently applied Open RE\nsystems.", "published": "2017-03-27 11:15:58", "link": "http://arxiv.org/abs/1703.09013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bootstrapping a Lexicon for Emotional Arousal in Software Engineering", "abstract": "Emotional arousal increases activation and performance but may also lead to\nburnout in software development. We present the first version of a Software\nEngineering Arousal lexicon (SEA) that is specifically designed to address the\nproblem of emotional arousal in the software developer ecosystem. SEA is built\nusing a bootstrapping approach that combines word embedding model trained on\nissue-tracking data and manual scoring of items in the lexicon. We show that\nour lexicon is able to differentiate between issue priorities, which are a\nsource of emotional activation and then act as a proxy for arousal. The best\nperformance is obtained by combining SEA (428 words) with a previously created\ngeneral purpose lexicon by Warriner et al. (13,915 words) and it achieves\nCohen's d effect sizes up to 0.5.", "published": "2017-03-27 13:11:33", "link": "http://arxiv.org/abs/1703.09046v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Tidy Data Model for Natural Language Processing using cleanNLP", "abstract": "The package cleanNLP provides a set of fast tools for converting a textual\ncorpus into a set of normalized tables. The underlying natural language\nprocessing pipeline utilizes Stanford's CoreNLP library, exposing a number of\nannotation tasks for text written in English, French, German, and Spanish.\nAnnotators include tokenization, part of speech tagging, named entity\nrecognition, entity linking, sentiment analysis, dependency parsing,\ncoreference resolution, and information extraction.", "published": "2017-03-27 02:18:36", "link": "http://arxiv.org/abs/1703.09570v2", "categories": ["cs.CL", "stat.CO"], "primary_category": "cs.CL"}
{"title": "Where to put the Image in an Image Caption Generator", "abstract": "When a recurrent neural network language model is used for caption\ngeneration, the image information can be fed to the neural network either by\ndirectly incorporating it in the RNN -- conditioning the language model by\n`injecting' image features -- or in a layer following the RNN -- conditioning\nthe language model by `merging' image features. While both options are attested\nin the literature, there is as yet no systematic comparison between the two. In\nthis paper we empirically show that it is not especially detrimental to\nperformance whether one architecture is used or another. The merge architecture\ndoes have practical advantages, as conditioning by merging allows the RNN's\nhidden state vector to shrink in size by up to four times. Our results suggest\nthat the visual and linguistic modalities for caption generation need not be\njointly encoded by the RNN as that yields large, memory-intensive models with\nfew tangible advantages in performance; rather, the multimodal integration\nshould be delayed to a subsequent stage.", "published": "2017-03-27 15:13:49", "link": "http://arxiv.org/abs/1703.09137v2", "categories": ["cs.NE", "cs.CL", "cs.CV"], "primary_category": "cs.NE"}
