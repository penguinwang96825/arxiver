{"title": "Designing the Business Conversation Corpus", "abstract": "While the progress of machine translation of written text has come far in the\npast several years thanks to the increasing availability of parallel corpora\nand corpora-based training technologies, automatic translation of spoken text\nand dialogues remains challenging even for modern systems. In this paper, we\naim to boost the machine translation quality of conversational texts by\nintroducing a newly constructed Japanese-English business conversation parallel\ncorpus. A detailed analysis of the corpus is provided along with challenging\nexamples for automatic translation. We also experiment with adding the corpus\nin a machine translation training scenario and show how the resulting system\nbenefits from its use.", "published": "2020-08-05 05:19:44", "link": "http://arxiv.org/abs/2008.01940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An exploration of the encoding of grammatical gender in word embeddings", "abstract": "The vector representation of words, known as word embeddings, has opened a\nnew research approach in linguistic studies. These representations can capture\ndifferent types of information about words. The grammatical gender of nouns is\na typical classification of nouns based on their formal and semantic\nproperties. The study of grammatical gender based on word embeddings can give\ninsight into discussions on how grammatical genders are determined. In this\nstudy, we compare different sets of word embeddings according to the accuracy\nof a neural classifier determining the grammatical gender of nouns. It is found\nthat there is an overlap in how grammatical gender is encoded in Swedish,\nDanish, and Dutch embeddings. Our experimental results on the contextualized\nembeddings pointed out that adding more contextual information to embeddings is\ndetrimental to the classifier's performance. We also observed that removing\nmorpho-syntactic features such as articles from the training corpora of\nembeddings decreases the classification performance dramatically, indicating a\nlarge portion of the information is encoded in the relationship between nouns\nand articles.", "published": "2020-08-05 06:01:46", "link": "http://arxiv.org/abs/2008.01946v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple Texts as a Limiting Factor in Online Learning: Quantifying\n  (Dis-)similarities of Knowledge Networks across Languages", "abstract": "We test the hypothesis that the extent to which one obtains information on a\ngiven topic through Wikipedia depends on the language in which it is consulted.\nControlling the size factor, we investigate this hypothesis for a number of 25\nsubject areas. Since Wikipedia is a central part of the web-based information\nlandscape, this indicates a language-related, linguistic bias. The article\ntherefore deals with the question of whether Wikipedia exhibits this kind of\nlinguistic relativity or not. From the perspective of educational science, the\narticle develops a computational model of the information landscape from which\nmultiple texts are drawn as typical input of web-based reading. For this\npurpose, it develops a hybrid model of intra- and intertextual similarity of\ndifferent parts of the information landscape and tests this model on the\nexample of 35 languages and corresponding Wikipedias. In this way the article\nbuilds a bridge between reading research, educational science, Wikipedia\nresearch and computational linguistics.", "published": "2020-08-05 11:11:55", "link": "http://arxiv.org/abs/2008.02047v1", "categories": ["cs.CL", "68T50 (Primary) 68T30, 91F20 (Secondary)", "I.2.7; J.5; K.3.m"], "primary_category": "cs.CL"}
{"title": "Computational linguistic assessment of textbook and online learning\n  media by means of threshold concepts in business education", "abstract": "Threshold concepts are key terms in domain-based knowledge acquisition. They\nare regarded as building blocks of the conceptual development of domain\nknowledge within particular learners. From a linguistic perspective, however,\nthreshold concepts are instances of specialized vocabularies, exhibiting\nparticular linguistic features. Threshold concepts are typically used in\nspecialized texts such as textbooks -- that is, within a formal learning\nenvironment. However, they also occur in informal learning environments like\nnewspapers. In this article, a first approach is taken to combine both lines\ninto an overarching research program - that is, to provide a computational\nlinguistic assessment of different resources, including in particular online\nresources, by means of threshold concepts. To this end, the distributive\nprofiles of 63 threshold concepts from business education (which have been\ncollected from threshold concept research) has been investigated in three kinds\nof (German) resources, namely textbooks, newspapers, and Wikipedia. Wikipedia\nis (one of) the largest and most widely used online resources. We looked at the\nthreshold concepts' frequency distribution, their compound distribution, and\ntheir network structure within the three kind of resources. The two main\nfindings can be summarized as follows: Firstly, the three kinds of resources\ncan indeed be distinguished in terms of their threshold concepts' profiles.\nSecondly, Wikipedia definitely appears to be a formal learning resource.", "published": "2020-08-05 12:56:16", "link": "http://arxiv.org/abs/2008.02096v1", "categories": ["cs.CL", "68T50 (Primary) 68T30, 91F20 (Secondary)", "I.2.7; J.5; K.3.m"], "primary_category": "cs.CL"}
{"title": "Contextualized Translation of Automatically Segmented Speech", "abstract": "Direct speech-to-text translation (ST) models are usually trained on corpora\nsegmented at sentence level, but at inference time they are commonly fed with\naudio split by a voice activity detector (VAD). Since VAD segmentation is not\nsyntax-informed, the resulting segments do not necessarily correspond to\nwell-formed sentences uttered by the speaker but, most likely, to fragments of\none or more sentences. This segmentation mismatch degrades considerably the\nquality of ST models' output. So far, researchers have focused on improving\naudio segmentation towards producing sentence-like splits. In this paper,\ninstead, we address the issue in the model, making it more robust to a\ndifferent, potentially sub-optimal segmentation. To this aim, we train our\nmodels on randomly segmented data and compare two approaches: fine-tuning and\nadding the previous segment as context. We show that our context-aware solution\nis more robust to VAD-segmented input, outperforming a strong base model and\nthe fine-tuning on different VAD segmentations of an English-German test set by\nup to 4.25 BLEU points.", "published": "2020-08-05 17:52:25", "link": "http://arxiv.org/abs/2008.02270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient MDI Adaptation for n-gram Language Models", "abstract": "This paper presents an efficient algorithm for n-gram language model\nadaptation under the minimum discrimination information (MDI) principle, where\nan out-of-domain language model is adapted to satisfy the constraints of\nmarginal probabilities of the in-domain data. The challenge for MDI language\nmodel adaptation is its computational complexity. By taking advantage of the\nbackoff structure of n-gram model and the idea of hierarchical training method,\noriginally proposed for maximum entropy (ME) language models, we show that MDI\nadaptation can be computed in linear-time complexity to the inputs in each\niteration. The complexity remains the same as ME models, although MDI is more\ngeneral than ME. This makes MDI adaptation practical for large corpus and\nvocabulary. Experimental results confirm the scalability of our algorithm on\nvery large datasets, while MDI adaptation gets slightly worse perplexity but\nbetter word error rate results compared to simple linear interpolation.", "published": "2020-08-05 22:21:03", "link": "http://arxiv.org/abs/2008.02385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Antibody Watch: Text Mining Antibody Specificity from the Literature", "abstract": "Antibodies are widely used reagents to test for expression of proteins and\nother antigens. However, they might not always reliably produce results when\nthey do not specifically bind to the target proteins that their providers\ndesigned them for, leading to unreliable research results. While many proposals\nhave been developed to deal with the problem of antibody specificity, it is\nstill challenging to cover the millions of antibodies that are available to\nresearchers. In this study, we investigate the feasibility of automatically\ngenerating alerts to users of problematic antibodies by extracting statements\nabout antibody specificity reported in the literature. The extracted alerts can\nbe used to construct an \"Antibody Watch\" knowledge base containing supporting\nstatements of problematic antibodies. We developed a deep neural network system\nand tested its performance with a corpus of more than two thousand articles\nthat reported uses of antibodies. We divided the problem into two tasks. Given\nan input article, the first task is to identify snippets about antibody\nspecificity and classify if the snippets report that any antibody exhibits\nnon-specificity, and thus is problematic. The second task is to link each of\nthese snippets to one or more antibodies mentioned in the snippet. The\nexperimental evaluation shows that our system can accurately perform both\nclassification and linking tasks with weighted F-scores over 0.925 and 0.923,\nrespectively, and 0.914 overall when combined to complete the joint task. We\nleveraged Research Resource Identifiers (RRID) to precisely identify antibodies\nlinked to the extracted specificity snippets. The result shows that it is\nfeasible to construct a reliable knowledge base about problematic antibodies by\ntext mining.", "published": "2020-08-05 05:14:11", "link": "http://arxiv.org/abs/2008.01937v2", "categories": ["cs.CL", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "Ontology-driven weak supervision for clinical entity classification in\n  electronic health records", "abstract": "In the electronic health record, using clinical notes to identify entities\nsuch as disorders and their temporality (e.g. the order of an event relative to\na time index) can inform many important analyses. However, creating training\ndata for clinical entity tasks is time consuming and sharing labeled data is\nchallenging due to privacy concerns. The information needs of the COVID-19\npandemic highlight the need for agile methods of training machine learning\nmodels for clinical notes. We present Trove, a framework for weakly supervised\nentity classification using medical ontologies and expert-generated rules. Our\napproach, unlike hand-labeled notes, is easy to share and modify, while\noffering performance comparable to learning from manually labeled training\ndata. In this work, we validate our framework on six benchmark tasks and\ndemonstrate Trove's ability to analyze the records of patients visiting the\nemergency department at Stanford Health Care for COVID-19 presenting symptoms\nand risk factors.", "published": "2020-08-05 07:42:09", "link": "http://arxiv.org/abs/2008.01972v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving End-to-End Speech-to-Intent Classification with Reptile", "abstract": "End-to-end spoken language understanding (SLU) systems have many advantages\nover conventional pipeline systems, but collecting in-domain speech data to\ntrain an end-to-end system is costly and time consuming. One question arises\nfrom this: how to train an end-to-end SLU with limited amounts of data? Many\nresearchers have explored approaches that make use of other related data\nresources, typically by pre-training parts of the model on high-resource speech\nrecognition. In this paper, we suggest improving the generalization performance\nof SLU models with a non-standard learning algorithm, Reptile. Though Reptile\nwas originally proposed for model-agnostic meta learning, we argue that it can\nalso be used to directly learn a target task and result in better\ngeneralization than conventional gradient descent. In this work, we employ\nReptile to the task of end-to-end spoken intent classification. Experiments on\nfour datasets of different languages and domains show improvement of intent\nprediction accuracy, both when Reptile is used alone and used in addition to\npre-training.", "published": "2020-08-05 08:32:15", "link": "http://arxiv.org/abs/2008.01994v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Glushkov's construction for functional subsequential transducers", "abstract": "Glushkov's construction has many interesting properties and they become even\nmore evident when applied to transducers. This article strives to show the wast\nrange of possible extensions and optimisations for this algorithm. Special\nflavour of regular expressions is introduced, which can be efficiently\nconverted to $\\epsilon$-free functional subsequential weighted finite state\ntransducers. Produced automata are very compact, as they contain only one state\nfor each symbol (from input alphabet) of original expression and only one\ntransition for each range of symbols, no matter how large. Such compactified\nranges of transitions allow for efficient binary search lookup during automaton\nevaluation. All the methods and algorithms presented here were used to\nimplement open-source compiler of regular expressions for multitape\ntransducers.", "published": "2020-08-05 17:09:58", "link": "http://arxiv.org/abs/2008.02239v4", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "6VecLM: Language Modeling in Vector Space for IPv6 Target Generation", "abstract": "Fast IPv6 scanning is challenging in the field of network measurement as it\nrequires exploring the whole IPv6 address space but limited by current\ncomputational power. Researchers propose to obtain possible active target\ncandidate sets to probe by algorithmically analyzing the active seed sets.\nHowever, IPv6 addresses lack semantic information and contain numerous\naddressing schemes, leading to the difficulty of designing effective\nalgorithms. In this paper, we introduce our approach 6VecLM to explore\nachieving such target generation algorithms. The architecture can map addresses\ninto a vector space to interpret semantic relationships and uses a Transformer\nnetwork to build IPv6 language models for predicting address sequence.\nExperiments indicate that our approach can perform semantic classification on\naddress space. By adding a new generation approach, our model possesses a\ncontrollable word innovation capability compared to conventional language\nmodels. The work outperformed the state-of-the-art target generation algorithms\non two active address datasets by reaching more quality candidate sets.", "published": "2020-08-05 16:26:50", "link": "http://arxiv.org/abs/2008.02213v1", "categories": ["cs.NI", "cs.CL", "cs.LG"], "primary_category": "cs.NI"}
{"title": "Generalized Word Shift Graphs: A Method for Visualizing and Explaining\n  Pairwise Comparisons Between Texts", "abstract": "A common task in computational text analyses is to quantify how two corpora\ndiffer according to a measurement like word frequency, sentiment, or\ninformation content. However, collapsing the texts' rich stories into a single\nnumber is often conceptually perilous, and it is difficult to confidently\ninterpret interesting or unexpected textual patterns without looming concerns\nabout data artifacts or measurement validity. To better capture fine-grained\ndifferences between texts, we introduce generalized word shift graphs,\nvisualizations which yield a meaningful and interpretable summary of how\nindividual words contribute to the variation between two texts for any measure\nthat can be formulated as a weighted average. We show that this framework\nnaturally encompasses many of the most commonly used approaches for comparing\ntexts, including relative frequencies, dictionary scores, and entropy-based\nmeasures like the Kullback-Leibler and Jensen-Shannon divergences. Through\nseveral case studies, we demonstrate how generalized word shift graphs can be\nflexibly applied across domains for diagnostic investigation, hypothesis\ngeneration, and substantive interpretation. By providing a detailed lens into\ntextual shifts between corpora, generalized word shift graphs help\ncomputational social scientists, digital humanists, and other text analysis\npractitioners fashion more robust scientific narratives.", "published": "2020-08-05 17:27:11", "link": "http://arxiv.org/abs/2008.02250v1", "categories": ["cs.CL", "cs.CY", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Aligning AI With Shared Human Values", "abstract": "We show how to assess a language model's knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.", "published": "2020-08-05 17:59:16", "link": "http://arxiv.org/abs/2008.02275v6", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "An Interpretable Deep Learning System for Automatically Scoring Request\n  for Proposals", "abstract": "The Managed Care system within Medicaid (US Healthcare) uses Request For\nProposals (RFP) to award contracts for various healthcare and related services.\nRFP responses are very detailed documents (hundreds of pages) submitted by\ncompeting organisations to win contracts. Subject matter expertise and domain\nknowledge play an important role in preparing RFP responses along with analysis\nof historical submissions. Automated analysis of these responses through\nNatural Language Processing (NLP) systems can reduce time and effort needed to\nexplore historical responses, and assisting in writing better responses. Our\nwork draws parallels between scoring RFPs and essay scoring models, while\nhighlighting new challenges and the need for interpretability. Typical scoring\nmodels focus on word level impacts to grade essays and other short write-ups.\nWe propose a novel Bi-LSTM based regression model, and provide deeper insight\ninto phrases which latently impact scoring of responses. We contend the merits\nof our proposed methodology using extensive quantitative experiments. We also\nqualitatively asses the impact of important phrases using human evaluators.\nFinally, we introduce a novel problem statement that can be used to further\nimprove the state of the art in NLP based automatic scoring systems.", "published": "2020-08-05 20:21:35", "link": "http://arxiv.org/abs/2008.02347v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Pain in Sickle Cell Disease using Clinical Text", "abstract": "Sickle Cell Disease (SCD) is a hereditary disorder of red blood cells in\nhumans. Complications such as pain, stroke, and organ failure occur in SCD as\nmalformed, sickled red blood cells passing through small blood vessels get\ntrapped. Particularly, acute pain is known to be the primary symptom of SCD.\nThe insidious and subjective nature of SCD pain leads to challenges in pain\nassessment among Medical Practitioners (MPs). Thus, accurate identification of\nmarkers of pain in patients with SCD is crucial for pain management.\nClassifying clinical notes of patients with SCD based on their pain level\nenables MPs to give appropriate treatment. We propose a binary classification\nmodel to predict pain relevance of clinical notes and a multiclass\nclassification model to predict pain level. While our four binary machine\nlearning (ML) classifiers are comparable in their performance, Decision Trees\nhad the best performance for the multiclass classification task achieving 0.70\nin F-measure. Our results show the potential clinical text analysis and machine\nlearning offer to pain management in sickle cell patients.", "published": "2020-08-05 23:39:57", "link": "http://arxiv.org/abs/2008.11081v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Learning to Denoise Historical Music", "abstract": "We propose an audio-to-audio neural network model that learns to denoise old\nmusic recordings. Our model internally converts its input into a time-frequency\nrepresentation by means of a short-time Fourier transform (STFT), and processes\nthe resulting complex spectrogram using a convolutional neural network. The\nnetwork is trained with both reconstruction and adversarial objectives on a\nsynthetic noisy music dataset, which is created by mixing clean music with real\nnoise samples extracted from quiet segments of old recordings. We evaluate our\nmethod quantitatively on held-out test examples of the synthetic dataset, and\nqualitatively by human rating on samples of actual historical recordings. Our\nresults show that the proposed method is effective in removing noise, while\npreserving the quality and details of the original music.", "published": "2020-08-05 10:05:44", "link": "http://arxiv.org/abs/2008.02027v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Recognition-Synthesis Based Non-Parallel Voice Conversion with\n  Adversarial Learning", "abstract": "This paper presents an adversarial learning method for recognition-synthesis\nbased non-parallel voice conversion. A recognizer is used to transform acoustic\nfeatures into linguistic representations while a synthesizer recovers output\nfeatures from the recognizer outputs together with the speaker identity. By\nseparating the speaker characteristics from the linguistic representations,\nvoice conversion can be achieved by replacing the speaker identity with the\ntarget one. In our proposed method, a speaker adversarial loss is adopted in\norder to obtain speaker-independent linguistic representations using the\nrecognizer. Furthermore, discriminators are introduced and a generative\nadversarial network (GAN) loss is used to prevent the predicted features from\nbeing over-smoothed. For training model parameters, a strategy of pre-training\non a multi-speaker dataset and then fine-tuning on the source-target speaker\npair is designed. Our method achieved higher similarity than the baseline model\nthat obtained the best performance in Voice Conversion Challenge 2018.", "published": "2020-08-05 21:25:32", "link": "http://arxiv.org/abs/2008.02371v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MusPy: A Toolkit for Symbolic Music Generation", "abstract": "In this paper, we present MusPy, an open source Python library for symbolic\nmusic generation. MusPy provides easy-to-use tools for essential components in\na music generation system, including dataset management, data I/O, data\npreprocessing and model evaluation. In order to showcase its potential, we\npresent statistical analysis of the eleven datasets currently supported by\nMusPy. Moreover, we conduct a cross-dataset generalizability experiment by\ntraining an autoregressive model on each dataset and measuring held-out\nlikelihood on the others---a process which is made easier by MusPy's dataset\nmanagement system. The results provide a map of domain overlap between various\ncommonly used datasets and show that some datasets contain more representative\ncross-genre samples than others. Along with the dataset analysis, these results\nmight serve as a guide for choosing datasets in future research. Source code\nand documentation are available at https://github.com/salu133445/muspy .", "published": "2020-08-05 06:16:13", "link": "http://arxiv.org/abs/2008.01951v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Neural Loop Combiner: Neural Network Models for Assessing the\n  Compatibility of Loops", "abstract": "Music producers who use loops may have access to thousands in loop libraries,\nbut finding ones that are compatible is a time-consuming process; we hope to\nreduce this burden with automation. State-of-the-art systems for estimating\ncompatibility, such as AutoMashUpper, are mostly rule-based and could be\nimproved on with machine learn-ing. To train a model, we need a large set of\nloops with ground truth compatibility values. No such dataset exists, so we\nextract loops from existing music to obtain positive examples of compatible\nloops, and propose and compare various strategies for choosing negative\nexamples. For re-producibility, we curate data from the Free Music\nArchive.Using this data, we investigate two types of model architectures for\nestimating the compatibility of loops: one based on a Siamese network, and the\nother a pure convolutional neural network (CNN). We conducted a user study in\nwhich participants rated the quality of the combinations suggested by each\nmodel, and found the CNN to outperform the Siamese network. Both model-based\napproaches outperformed the rule-based one. We have opened source the code for\nbuilding the models and the dataset.", "published": "2020-08-05 09:16:50", "link": "http://arxiv.org/abs/2008.02011v2", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Compact Graph Architecture for Speech Emotion Recognition", "abstract": "We propose a deep graph approach to address the task of speech emotion\nrecognition. A compact, efficient and scalable way to represent data is in the\nform of graphs. Following the theory of graph signal processing, we propose to\nmodel speech signal as a cycle graph or a line graph. Such graph structure\nenables us to construct a Graph Convolution Network (GCN)-based architecture\nthat can perform an accurate graph convolution in contrast to the approximate\nconvolution used in standard GCNs. We evaluated the performance of our model\nfor speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases.\nOur model outperforms standard GCN and other relevant deep graph architectures\nindicating the effectiveness of our approach. When compared with existing\nspeech emotion recognition methods, our model achieves comparable performance\nto the state-of-the-art with significantly fewer learnable parameters (~30K)\nindicating its applicability in resource-constrained devices.", "published": "2020-08-05 12:09:09", "link": "http://arxiv.org/abs/2008.02063v4", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Content based singing voice source separation via strong conditioning\n  using aligned phonemes", "abstract": "Informed source separation has recently gained renewed interest with the\nintroduction of neural networks and the availability of large multitrack\ndatasets containing both the mixture and the separated sources. These\napproaches use prior information about the target source to improve separation.\nHistorically, Music Information Retrieval researchers have focused primarily on\nscore-informed source separation, but more recent approaches explore\nlyrics-informed source separation. However, because of the lack of multitrack\ndatasets with time-aligned lyrics, models use weak conditioning with\nnon-aligned lyrics. In this paper, we present a multimodal multitrack dataset\nwith lyrics aligned in time at the word level with phonetic information as well\nas explore strong conditioning using the aligned phonemes. Our model follows a\nU-Net architecture and takes as input both the magnitude spectrogram of a\nmusical mixture and a matrix with aligned phonetic information. The phoneme\nmatrix is embedded to obtain the parameters that control Feature-wise Linear\nModulation (FiLM) layers. These layers condition the U-Net feature maps to\nadapt the separation process to the presence of different phonemes via affine\ntransformations. We show that phoneme conditioning can be successfully applied\nto improve singing voice source separation.", "published": "2020-08-05 12:25:24", "link": "http://arxiv.org/abs/2008.02070v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Characterization of Expressive Performance in Classical Music:\n  First Results of the Con Espressione Game", "abstract": "A piece of music can be expressively performed, or interpreted, in a variety\nof ways. With the help of an online questionnaire, the Con Espressione Game, we\ncollected some 1,500 descriptions of expressive character relating to 45\nperformances of 9 excerpts from classical piano pieces, played by different\nfamous pianists. More specifically, listeners were asked to describe, using\nfreely chosen words (preferably: adjectives), how they perceive the expressive\ncharacter of the different performances. In this paper, we offer a first\naccount of this new data resource for expressive performance research, and\nprovide an exploratory analysis, addressing three main questions: (1) how\nsimilarly do different listeners describe a performance of a piece? (2) what\nare the main dimensions (or axes) for expressive character emerging from this?;\nand (3) how do measurable parameters of a performance (e.g., tempo, dynamics)\nand mid- and high-level features that can be predicted by machine learning\nmodels (e.g., articulation, arousal) relate to these expressive dimensions? The\ndataset that we publish along with this paper was enriched by adding\nhand-corrected score-to-performance alignments, as well as descriptive audio\nfeatures such as tempo and dynamics curves.", "published": "2020-08-05 15:40:57", "link": "http://arxiv.org/abs/2008.02194v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hybrid Transformer/CTC Networks for Hardware Efficient Voice Triggering", "abstract": "We consider the design of two-pass voice trigger detection systems. We focus\non the networks in the second pass that are used to re-score candidate segments\nobtained from the first-pass. Our baseline is an acoustic model(AM), with\nBiLSTM layers, trained by minimizing the CTC loss. We replace the BiLSTM layers\nwith self-attention layers. Results on internal evaluation sets show that\nself-attention networks yield better accuracy while requiring fewer parameters.\nWe add an auto-regressive decoder network on top of the self-attention layers\nand jointly minimize the CTC loss on the encoder and the cross-entropy loss on\nthe decoder. This design yields further improvements over the baseline. We\nretrain all the models above in a multi-task learning(MTL) setting, where one\nbranch of a shared network is trained as an AM, while the second branch\nclassifies the whole sequence to be true-trigger or not. Results demonstrate\nthat networks with self-attention layers yield $\\sim$60% relative reduction in\nfalse reject rates for a given false-alarm rate, while requiring 10% fewer\nparameters. When trained in the MTL setup, self-attention networks yield\nfurther accuracy improvements. On-device measurements show that we observe 70%\nrelative reduction in inference time. Additionally, the proposed network\narchitectures are $\\sim$5X faster to train.", "published": "2020-08-05 19:16:33", "link": "http://arxiv.org/abs/2008.02323v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Data Cleansing with Contrastive Learning for Vocal Note Event\n  Annotations", "abstract": "Data cleansing is a well studied strategy for cleaning erroneous labels in\ndatasets, which has not yet been widely adopted in Music Information Retrieval.\nPreviously proposed data cleansing models do not consider structured (e.g. time\nvarying) labels, such as those common to music data. We propose a novel data\ncleansing model for time-varying, structured labels which exploits the local\nstructure of the labels, and demonstrate its usefulness for vocal note event\nannotations in music. %Our model is trained in a contrastive learning manner by\nautomatically creating local deformations of likely correct labels. Our model\nis trained in a contrastive learning manner by automatically contrasting likely\ncorrect labels pairs against local deformations of them. We demonstrate that\nthe accuracy of a transcription model improves greatly when trained using our\nproposed strategy compared with the accuracy when trained using the original\ndataset. Additionally we use our model to estimate the annotation error rates\nin the DALI dataset, and highlight other potential uses for this type of model.", "published": "2020-08-05 12:24:37", "link": "http://arxiv.org/abs/2008.02069v3", "categories": ["cs.LG", "cs.IR", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
