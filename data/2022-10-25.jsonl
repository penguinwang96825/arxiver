{"title": "Help me write a poem: Instruction Tuning as a Vehicle for Collaborative\n  Poetry Writing", "abstract": "Recent work in training large language models (LLMs) to follow natural\nlanguage instructions has opened up exciting opportunities for natural language\ninterface design. Building on the prior success of LLMs in the realm of\ncomputer-assisted creativity, we aim to study if LLMs can improve the quality\nof user-generated content through collaboration. We present CoPoet, a\ncollaborative poetry writing system. In contrast to auto-completing a user's\ntext, CoPoet is controlled by user instructions that specify the attributes of\nthe desired text, such as Write a sentence about `love' or Write a sentence\nending in `fly'. The core component of our system is a language model\nfine-tuned on a diverse collection of instructions for poetry writing. Our\nmodel is not only competitive with publicly available LLMs trained on\ninstructions (InstructGPT), but is also capable of satisfying unseen\ncompositional instructions. A study with 15 qualified crowdworkers shows that\nusers successfully write poems with CoPoet on diverse topics ranging from\nMonarchy to Climate change. Further, the collaboratively written poems are\npreferred by third-party evaluators over those written without the system.", "published": "2022-10-25 00:07:10", "link": "http://arxiv.org/abs/2210.13669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Parameter Efficient Learning for Generation", "abstract": "Parameter efficient learning methods (PERMs) have recently gained significant\nattention as they provide an efficient way for pre-trained language models\n(PLMs) to adapt to a downstream task. However, these conclusions are mostly\ndrawn from in-domain evaluations over the full training set. In this paper, we\npresent comparisons between PERMs and finetuning from three new perspectives:\n(1) the effect of sample and model size to in-domain evaluations, (2)\ngeneralization to unseen domains and new datasets, and (3) the faithfulness of\ngenerations. Our results show that for in-domain settings (a) there is a cross\npoint of sample size for which PERMs will perform better than finetuning when\ntraining with fewer samples, and (b) larger PLMs have larger cross points. For\ncross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al.,\n2019) performs the best amongst all the PERMs studied here, and (b) it\noutperforms finetuning if the task dataset is below a certain size. We also\ncompare the faithfulness of generations and show that PERMs can achieve better\nfaithfulness score than finetuning, especially for small training set, by as\nmuch as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and\nachieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all\nROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98).", "published": "2022-10-25 00:14:48", "link": "http://arxiv.org/abs/2210.13673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for\n  Cross-lingual Text-to-SQL Semantic Parsing", "abstract": "In-context learning using large language models has recently shown surprising\nresults for semantic parsing tasks such as Text-to-SQL translation. Prompting\nGPT-3 or Codex using several examples of question-SQL pairs can produce\nexcellent results, comparable to state-of-the-art finetuning-based models.\nHowever, existing work primarily focuses on English datasets, and it is unknown\nwhether large language models can serve as competitive semantic parsers for\nother languages. To bridge this gap, our work focuses on cross-lingual\nText-to-SQL semantic parsing for translating non-English utterances into SQL\nqueries based on an English schema. We consider a zero-shot transfer learning\nsetting with the assumption that we do not have any labeled examples in the\ntarget language (but have annotated examples in English). This work introduces\nthe XRICL framework, which learns to retrieve relevant English exemplars for a\ngiven query to construct prompts. We also include global translation exemplars\nfor a target language to facilitate the translation process for large language\nmodels. To systematically evaluate our model, we construct two new benchmark\ndatasets, XSpider and XKaggle-dbqa, which include questions in Chinese,\nVietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively\nleverages large pre-trained language models to outperform existing baselines.\nData and code are publicly available at https://github.com/Impavidity/XRICL.", "published": "2022-10-25 01:33:49", "link": "http://arxiv.org/abs/2210.13693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating\n  Models to Reflect Conflicting Evidence", "abstract": "Question answering models can use rich knowledge sources -- up to one hundred\nretrieved passages and parametric knowledge in the large-scale language model\n(LM). Prior work assumes information in such knowledge sources is consistent\nwith each other, paying little attention to how models blend information stored\nin their LM parameters with that from retrieved evidence documents. In this\npaper, we simulate knowledge conflicts (i.e., where parametric knowledge\nsuggests one answer and different passages suggest different answers) and\nexamine model behaviors. We find retrieval performance heavily impacts which\nsources models rely on, and current models mostly rely on non-parametric\nknowledge in their best-performing settings. We discover a troubling trend that\ncontradictions among knowledge sources affect model confidence only marginally.\nTo address this issue, we present a new calibration study, where models are\ndiscouraged from presenting any single answer when presented with multiple\nconflicting answer candidates in retrieved evidences.", "published": "2022-10-25 01:46:00", "link": "http://arxiv.org/abs/2210.13701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Legal Domain Adaptation", "abstract": "Seeking legal advice is often expensive. Recent advancements in machine\nlearning for solving complex problems can be leveraged to help make legal\nservices more accessible to the public. However, real-life applications\nencounter significant challenges. State-of-the-art language models are growing\nincreasingly large, making parameter-efficient learning increasingly important.\nUnfortunately, parameter-efficient methods perform poorly with small amounts of\ndata, which are common in the legal domain (where data labelling costs are\nhigh). To address these challenges, we propose parameter-efficient legal domain\nadaptation, which uses vast unsupervised legal data from public legal forums to\nperform legal pre-training. This method exceeds or matches the fewshot\nperformance of existing models such as LEGAL-BERT on various legal tasks while\ntuning only approximately 0.1% of model parameters. Additionally, we show that\nour method can achieve calibration comparable to existing methods across\nseveral tasks. To the best of our knowledge, this work is among the first to\nexplore parameter-efficient methods of tuning language models in the legal\ndomain.", "published": "2022-10-25 02:14:15", "link": "http://arxiv.org/abs/2210.13712v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Few-Shot Relation Extraction with Label Prompt Dropout", "abstract": "Few-shot relation extraction aims to learn to identify the relation between\ntwo entities based on very limited training examples. Recent efforts found that\ntextual labels (i.e., relation names and relation descriptions) could be\nextremely useful for learning class representations, which will benefit the\nfew-shot learning task. However, what is the best way to leverage such label\ninformation in the learning process is an important research question. Existing\nworks largely assume such textual labels are always present during both\nlearning and prediction. In this work, we argue that such approaches may not\nalways lead to optimal results. Instead, we present a novel approach called\nlabel prompt dropout, which randomly removes label descriptions in the learning\nprocess. Our experiments show that our approach is able to lead to improved\nclass representations, yielding significantly better results on the few-shot\nrelation extraction task.", "published": "2022-10-25 03:03:09", "link": "http://arxiv.org/abs/2210.13733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEMETR: Diagnosing Evaluation Metrics for Translation", "abstract": "While machine translation evaluation metrics based on string overlap (e.g.,\nBLEU) have their limitations, their computations are transparent: the BLEU\nscore assigned to a particular candidate translation can be traced back to the\npresence or absence of certain words. The operations of newer learned metrics\n(e.g., BLEURT, COMET), which leverage pretrained language models to achieve\nhigher correlations with human quality judgments than BLEU, are opaque in\ncomparison. In this paper, we shed light on the behavior of these learned\nmetrics by creating DEMETR, a diagnostic dataset with 31K English examples\n(translated from 10 source languages) for evaluating the sensitivity of MT\nevaluation metrics to 35 different linguistic perturbations spanning semantic,\nsyntactic, and morphological error categories. All perturbations were carefully\ndesigned to form minimal pairs with the actual translation (i.e., differ in\nonly one aspect). We find that learned metrics perform substantially better\nthan string-based metrics on DEMETR. Additionally, learned metrics differ in\ntheir sensitivity to various phenomena (e.g., BERTScore is sensitive to\nuntranslated words but relatively insensitive to gender manipulation, while\nCOMET is much more sensitive to word repetition than to aspectual changes). We\npublicly release DEMETR to spur more informed future development of machine\ntranslation evaluation metrics", "published": "2022-10-25 03:25:44", "link": "http://arxiv.org/abs/2210.13746v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IDK-MRC: Unanswerable Questions for Indonesian Machine Reading\n  Comprehension", "abstract": "Machine Reading Comprehension (MRC) has become one of the essential tasks in\nNatural Language Understanding (NLU) as it is often included in several NLU\nbenchmarks (Liang et al., 2020; Wilie et al., 2020). However, most MRC datasets\nonly have answerable question type, overlooking the importance of unanswerable\nquestions. MRC models trained only on answerable questions will select the span\nthat is most likely to be the answer, even when the answer does not actually\nexist in the given passage (Rajpurkar et al., 2018). This problem especially\nremains in medium- to low-resource languages like Indonesian. Existing\nIndonesian MRC datasets (Purwarianti et al., 2007; Clark et al., 2020) are\nstill inadequate because of the small size and limited question types, i.e.,\nthey only cover answerable questions. To fill this gap, we build a new\nIndonesian MRC dataset called I(n)don'tKnow- MRC (IDK-MRC) by combining the\nautomatic and manual unanswerable question generation to minimize the cost of\nmanual dataset construction while maintaining the dataset quality. Combined\nwith the existing answerable questions, IDK-MRC consists of more than 10K\nquestions in total. Our analysis shows that our dataset significantly improves\nthe performance of Indonesian MRC models, showing a large improvement for\nunanswerable questions.", "published": "2022-10-25 05:46:53", "link": "http://arxiv.org/abs/2210.13778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Chinese Spelling Check Framework Based on Reverse Contrastive Learning", "abstract": "Chinese spelling check is a task to detect and correct spelling mistakes in\nChinese text. Existing research aims to enhance the text representation and use\nmulti-source information to improve the detection and correction capabilities\nof models, but does not pay too much attention to improving their ability to\ndistinguish between confusable words. Contrastive learning, whose aim is to\nminimize the distance in representation space between similar sample pairs, has\nrecently become a dominant technique in natural language processing. Inspired\nby contrastive learning, we present a novel framework for Chinese spelling\nchecking, which consists of three modules: language representation, spelling\ncheck and reverse contrastive learning. Specifically, we propose a reverse\ncontrastive learning strategy, which explicitly forces the model to minimize\nthe agreement between the similar examples, namely, the phonetically and\nvisually confusable characters. Experimental results show that our framework is\nmodel-agnostic and could be combined with existing Chinese spelling check\nmodels to yield state-of-the-art performance.", "published": "2022-10-25 08:05:38", "link": "http://arxiv.org/abs/2210.13823v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IFDID: Information Filter upon Diversity-Improved Decoding for\n  Diversity-Faithfulness Tradeoff in NLG", "abstract": "Some Natural Language Generation (NLG) tasks require both faithfulness and\ndiversity. The decoding strategy is intensively related to the quality of the\ngenerated text. Strategies such as beam search, greedy search, etc., perform\nwith low diversity and high repetition. On the other hand, guided decoding, the\nsolution towards diversity, may generate unfaithful expressions. To this end,\nthis paper presents Information Filter upon Diversity-Improved Decoding (IFDID)\nto obtain the tradeoff between diversity and faithfulness. IFDID is a two-stage\ndecoding strategy leveraging the proposed Enhance-Filter framework, which\nachieves the tradeoff by increasing the probabilities of some typical tokens\nbeing selected and subsequently filtering them by their information amount. To\nverify the effectiveness, we compare our method with other baselines on related\nCommonGEN, RocStories and AdGen benchmarks, which cover Chinese and English\ndatasets. Our numerical experimental results and human evaluation outcomes\nverify the effectiveness of the proposed approach, as our approach achieves a\n1.24 higher ROUGE score describing faithfulness as well as higher diversity\nrepresented by 62.5% higher upon Dist-2 than traditional approaches,\ndemonstrating that IFDID is a novel SOTA decoding strategy for the tradeoff\nbetween diversity and faithfulness.", "published": "2022-10-25 08:14:20", "link": "http://arxiv.org/abs/2210.13829v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation", "abstract": "Recent model-based reference-free metrics for open-domain dialogue evaluation\nexhibit promising correlations with human judgment. However, they either\nperform turn-level evaluation or look at a single dialogue quality dimension.\nOne would expect a good evaluation metric to assess multiple quality dimensions\nat the dialogue level. To this end, we are motivated to propose a\nmulti-dimensional dialogue-level metric, which consists of three sub-metrics\nwith each targeting a specific dimension. The sub-metrics are trained with\nnovel self-supervised objectives and exhibit strong correlations with human\njudgment for their respective dimensions. Moreover, we explore two approaches\nto combine the sub-metrics: metric ensemble and multitask learning. Both\napproaches yield a holistic metric that significantly outperforms individual\nsub-metrics. Compared to the existing state-of-the-art metric, the combined\nmetrics achieve around 16% relative improvement on average across three\nhigh-quality dialogue-level evaluation benchmarks.", "published": "2022-10-25 08:26:03", "link": "http://arxiv.org/abs/2210.13832v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deconfounding Legal Judgment Prediction for European Court of Human\n  Rights Cases Towards Better Alignment with Experts", "abstract": "This work demonstrates that Legal Judgement Prediction systems without\nexpert-informed adjustments can be vulnerable to shallow, distracting surface\nsignals that arise from corpus construction, case distribution, and confounding\nfactors. To mitigate this, we use domain expertise to strategically identify\nstatistically predictive but legally irrelevant information. We adopt\nadversarial training to prevent the system from relying on it. We evaluate our\ndeconfounded models by employing interpretability techniques and comparing to\nexpert annotations. Quantitative experiments and qualitative analysis show that\nour deconfounded model consistently aligns better with expert rationales than\nbaselines trained for prediction only. We further contribute a set of reference\nexpert annotations to the validation and testing partitions of an existing\nbenchmark dataset of European Court of Human Rights cases.", "published": "2022-10-25 08:37:25", "link": "http://arxiv.org/abs/2210.13836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogConv: A Lightweight Fully Convolutional Network for Multi-view\n  Response Selection", "abstract": "Current end-to-end retrieval-based dialogue systems are mainly based on\nRecurrent Neural Networks or Transformers with attention mechanisms. Although\npromising results have been achieved, these models often suffer from slow\ninference or huge number of parameters. In this paper, we propose a novel\nlightweight fully convolutional architecture, called DialogConv, for response\nselection. DialogConv is exclusively built on top of convolution to extract\nmatching features of context and response. Dialogues are modeled in 3D views,\nwhere DialogConv performs convolution operations on embedding view, word view\nand utterance view to capture richer semantic information from multiple\ncontextual views. On the four benchmark datasets, compared with\nstate-of-the-art baselines, DialogConv is on average about 8.5x smaller in\nsize, and 79.39x and 10.64x faster on CPU and GPU devices, respectively. At the\nsame time, DialogConv achieves the competitive effectiveness of response\nselection.", "published": "2022-10-25 09:06:39", "link": "http://arxiv.org/abs/2210.13845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for\n  Misinformation", "abstract": "Misinformation emerges in times of uncertainty when credible information is\nlimited. This is challenging for NLP-based fact-checking as it relies on\ncounter-evidence, which may not yet be available. Despite increasing interest\nin automatic fact-checking, it is still unclear if automated approaches can\nrealistically refute harmful real-world misinformation. Here, we contrast and\ncompare NLP fact-checking with how professional fact-checkers combat\nmisinformation in the absence of counter-evidence. In our analysis, we show\nthat, by design, existing NLP task definitions for fact-checking cannot refute\nmisinformation as professional fact-checkers do for the majority of claims. We\nthen define two requirements that the evidence in datasets must fulfill for\nrealistic fact-checking: It must be (1) sufficient to refute the claim and (2)\nnot leaked from existing fact-checking articles. We survey existing\nfact-checking datasets and find that all of them fail to satisfy both criteria.\nFinally, we perform experiments to demonstrate that models trained on a\nlarge-scale fact-checking dataset rely on leaked evidence, which makes them\nunsuitable in real-world scenarios. Taken together, we show that current NLP\nfact-checking cannot realistically combat real-world misinformation because it\ndepends on unrealistic assumptions about counter-evidence in the data.", "published": "2022-10-25 09:40:48", "link": "http://arxiv.org/abs/2210.13865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Reuse Distractors to support Multiple Choice Question\n  Generation in Education", "abstract": "Multiple choice questions (MCQs) are widely used in digital learning systems,\nas they allow for automating the assessment process. However, due to the\nincreased digital literacy of students and the advent of social media\nplatforms, MCQ tests are widely shared online, and teachers are continuously\nchallenged to create new questions, which is an expensive and time-consuming\ntask. A particularly sensitive aspect of MCQ creation is to devise relevant\ndistractors, i.e., wrong answers that are not easily identifiable as being\nwrong. This paper studies how a large existing set of manually created answers\nand distractors for questions over a variety of domains, subjects, and\nlanguages can be leveraged to help teachers in creating new MCQs, by the smart\nreuse of existing distractors. We built several data-driven models based on\ncontext-aware question and distractor representations, and compared them with\nstatic feature-based models. The proposed models are evaluated with automated\nmetrics and in a realistic user test with teachers. Both automatic and human\nevaluations indicate that context-aware models consistently outperform a static\nfeature-based approach. For our best-performing context-aware model, on average\n3 distractors out of the 10 shown to teachers were rated as high-quality\ndistractors. We create a performance benchmark, and make it public, to enable\ncomparison between different approaches and to introduce a more standardized\nevaluation of the task. The benchmark contains a test of 298 educational\nquestions covering multiple subjects & languages and a 77k multilingual pool of\ndistractor vocabulary for future research.", "published": "2022-10-25 12:48:56", "link": "http://arxiv.org/abs/2210.13964v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From exemplar to copy: the scribal appropriation of a Hadewijch\n  manuscript computationally explored", "abstract": "This study is devoted to two of the oldest known manuscripts in which the\noeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,\nKBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of\ncodicological and contextual arguments, it is assumed that the scribe who\nproduced B used A as an exemplar. While the similarities in both layout and\ncontent between the two manuscripts are striking, the present article seeks to\nidentify the differences. After all, regardless of the intention to produce a\ncopy that closely follows the exemplar, subtle linguistic variation is\napparent. Divergences relate to spelling conventions, but also to the way in\nwhich words are abbreviated (and the extent to which abbreviations occur). The\npresent study investigates the spelling profiles of the scribes who produced\nmss. A and B in a computational way. In the first part of this study, we will\npresent both manuscripts in more detail, after which we will consider prior\nresearch carried out on scribal profiling. The current study both builds and\nexpands on Kestemont (2015). Next, we outline the methodology used to analyse\nand measure the degree of scribal appropriation that took place when ms. B was\ncopied off the exemplar ms. A. After this, we will discuss the results\nobtained, focusing on the scribal variation that can be found both at the level\nof individual words and n-grams. To this end, we use machine learning to\nidentify the most distinctive features that separate manuscript A from B.\nFinally, we look at possible diachronic trends in the appropriation by B's\nscribe of his exemplar. We argue that scribal takeovers in the exemplar impacts\nthe practice of the copying scribe, while transitions to a different content\nmatter cause little to no effect.", "published": "2022-10-25 14:40:25", "link": "http://arxiv.org/abs/2210.14061v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Search Is What You Need For Neural Text Generation", "abstract": "Generating text with autoregressive language models (LMs) is of great\nimportance to many natural language processing (NLP) applications. Previous\nsolutions for this task often produce text that contains degenerative\nexpressions or lacks semantic consistency. Recently, Su et al. introduced a new\ndecoding method, contrastive search, based on the isotropic representation\nspace of the language model and obtained new state of the art on various\nbenchmarks. Additionally, Su et al. argued that the representations of\nautoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also\nshared by previous studies. Therefore, to ensure the language model follows an\nisotropic distribution, Su et al. proposed a contrastive learning scheme,\nSimCTG, which calibrates the language model's representations through\nadditional training.\n  In this study, we first answer the question: \"Are autoregressive LMs really\nanisotropic?\". To this end, we extensively evaluate the isotropy of LMs across\n16 major languages. Surprisingly, we find that the anisotropic problem only\nexists in the two specific English GPT-2-small/medium models. On the other\nhand, all other evaluated LMs are naturally isotropic which is in contrast to\nthe conclusion drawn by previous studies. Based on our findings, we further\nassess the contrastive search decoding method using off-the-shelf LMs on four\ngeneration tasks across 16 languages. Our experimental results demonstrate that\ncontrastive search significantly outperforms previous decoding methods without\nany additional training. More notably, on 12 out of the 16 evaluated languages,\ncontrastive search performs comparably with human-level performances as judged\nby human evaluations. Our code and other related resources are publicly\navailable at https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need.", "published": "2022-10-25 16:40:48", "link": "http://arxiv.org/abs/2210.14140v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Interpretable Summary Evaluation via Allocation of Contextual\n  Embeddings to Reference Text Topics", "abstract": "Despite extensive recent advances in summary generation models, evaluation of\nauto-generated summaries still widely relies on single-score systems\ninsufficient for transparent assessment and in-depth qualitative analysis.\nTowards bridging this gap, we propose the multifaceted interpretable summary\nevaluation method (MISEM), which is based on allocation of a summary's\ncontextual token embeddings to semantic topics identified in the reference\ntext. We further contribute an interpretability toolbox for automated summary\nevaluation and interactive visual analysis of summary scoring, topic\nidentification, and token-topic allocation. MISEM achieves a promising .404\nPearson correlation with human judgment on the TAC'08 dataset.", "published": "2022-10-25 17:09:08", "link": "http://arxiv.org/abs/2210.14174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and\n  Summarization", "abstract": "Social media has increasingly played a key role in emergency response: first\nresponders can use public posts to better react to ongoing crisis events and\ndeploy the necessary resources where they are most needed. Timeline extraction\nand abstractive summarization are critical technical tasks to leverage large\nnumbers of social media posts about events. Unfortunately, there are few\ndatasets for benchmarking technical approaches for those tasks. This paper\npresents CrisisLTLSum, the largest dataset of local crisis event timelines\navailable to date. CrisisLTLSum contains 1,000 crisis event timelines across\nfour domains: wildfires, local fires, traffic, and storms. We built\nCrisisLTLSum using a semi-automated cluster-then-refine approach to collect\ndata from the public Twitter stream. Our initial experiments indicate a\nsignificant gap between the performance of strong baselines compared to the\nhuman performance on both tasks. Our dataset, code, and models are publicly\navailable.", "published": "2022-10-25 17:32:40", "link": "http://arxiv.org/abs/2210.14190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Document-Level Literary Machine Translation with Parallel\n  Paragraphs from World Literature", "abstract": "Literary translation is a culturally significant task, but it is bottlenecked\nby the small number of qualified literary translators relative to the many\nuntranslated works published around the world. Machine translation (MT) holds\npotential to complement the work of human translators by improving both\ntraining procedures and their overall efficiency. Literary translation is less\nconstrained than more traditional MT settings since translators must balance\nmeaning equivalence, readability, and critical interpretability in the target\nlanguage. This property, along with the complex discourse-level context present\nin literary texts, also makes literary MT more challenging to computationally\nmodel and evaluate. To explore this task, we collect a dataset (Par3) of\nnon-English language novels in the public domain, each aligned at the paragraph\nlevel to both human and automatic English translations. Using Par3, we discover\nthat expert literary translators prefer reference human translations over\nmachine-translated paragraphs at a rate of 84%, while state-of-the-art\nautomatic MT metrics do not correlate with those preferences. The experts note\nthat MT outputs contain not only mistranslations, but also discourse-disrupting\nerrors and stylistic inconsistencies. To address these problems, we train a\npost-editing model whose output is preferred over normal MT output at a rate of\n69% by experts. We publicly release Par3 at\nhttps://github.com/katherinethai/par3/ to spur future research into literary\nMT.", "published": "2022-10-25 18:03:34", "link": "http://arxiv.org/abs/2210.14250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Open Data and Task Augmentation to Automated Behavioral\n  Coding of Psychotherapy Conversations in Low-Resource Scenarios", "abstract": "In psychotherapy interactions, the quality of a session is assessed by\ncodifying the communicative behaviors of participants during the conversation\nthrough manual observation and annotation. Developing computational approaches\nfor automated behavioral coding can reduce the burden on human coders and\nfacilitate the objective evaluation of the intervention. In the real world,\nhowever, implementing such algorithms is associated with data sparsity\nchallenges since privacy concerns lead to limited available in-domain data. In\nthis paper, we leverage a publicly available conversation-based dataset and\ntransfer knowledge to the low-resource behavioral coding task by performing an\nintermediate language model training via meta-learning. We introduce a task\naugmentation method to produce a large number of \"analogy tasks\" - tasks\nsimilar to the target one - and demonstrate that the proposed framework\npredicts target behaviors more accurately than all the other baseline models.", "published": "2022-10-25 18:15:25", "link": "http://arxiv.org/abs/2210.14254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revision for Concision: A Constrained Paraphrase Generation Task", "abstract": "Academic writing should be concise as concise sentences better keep the\nreaders' attention and convey meaning clearly. Writing concisely is\nchallenging, for writers often struggle to revise their drafts. We introduce\nand formulate revising for concision as a natural language processing task at\nthe sentence level. Revising for concision requires algorithms to use only\nnecessary words to rewrite a sentence while preserving its meaning. The revised\nsentence should be evaluated according to its word choice, sentence structure,\nand organization. The revised sentence also needs to fulfil semantic retention\nand syntactic soundness. To aide these efforts, we curate and make available a\nbenchmark parallel dataset that can depict revising for concision. The dataset\ncontains 536 pairs of sentences before and after revising, and all pairs are\ncollected from college writing centres. We also present and evaluate the\napproaches to this problem, which may assist researchers in this area.", "published": "2022-10-25 18:20:54", "link": "http://arxiv.org/abs/2210.14257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Evasion Attacks on Summarization Scoring", "abstract": "The automatic scoring of summaries is important as it guides the development\nof summarizers. Scoring is also complex, as it involves multiple aspects such\nas fluency, grammar, and even textual entailment with the source text. However,\nsummary scoring has not been considered a machine learning task to study its\naccuracy and robustness. In this study, we place automatic scoring in the\ncontext of regression machine learning tasks and perform evasion attacks to\nexplore its robustness. Attack systems predict a non-summary string from each\ninput, and these non-summary strings achieve competitive scores with good\nsummarizers on the most popular metrics: ROUGE, METEOR, and BERTScore. Attack\nsystems also \"outperform\" state-of-the-art summarization methods on ROUGE-1 and\nROUGE-L, and score the second-highest on METEOR. Furthermore, a BERTScore\nbackdoor is observed: a simple trigger can score higher than any automatic\nsummarization method. The evasion attacks in this work indicate the low\nrobustness of current scoring systems at the system level. We hope that our\nhighlighting of these proposed attacks will facilitate the development of\nsummary scores.", "published": "2022-10-25 18:25:47", "link": "http://arxiv.org/abs/2210.14260v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Similarity between Units of Natural Language: The Transition from Coarse\n  to Fine Estimation", "abstract": "Capturing the similarities between human language units is crucial for\nexplaining how humans associate different objects, and therefore its\ncomputation has received extensive attention, research, and applications. With\nthe ever-increasing amount of information around us, calculating similarity\nbecomes increasingly complex, especially in many cases, such as legal or\nmedical affairs, measuring similarity requires extra care and precision, as\nsmall acts within a language unit can have significant real-world effects. My\nresearch goal in this thesis is to develop regression models that account for\nsimilarities between language units in a more refined way.\n  Computation of similarity has come a long way, but approaches to debugging\nthe measures are often based on continually fitting human judgment values. To\nthis end, my goal is to develop an algorithm that precisely catches loopholes\nin a similarity calculation. Furthermore, most methods have vague definitions\nof the similarities they compute and are often difficult to interpret. The\nproposed framework addresses both shortcomings. It constantly improves the\nmodel through catching different loopholes. In addition, every refinement of\nthe model provides a reasonable explanation. The regression model introduced in\nthis thesis is called progressively refined similarity computation, which\ncombines attack testing with adversarial training. The similarity regression\nmodel of this thesis achieves state-of-the-art performance in handling edge\ncases.", "published": "2022-10-25 18:54:32", "link": "http://arxiv.org/abs/2210.14275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenStance: Real-world Zero-shot Stance Detection", "abstract": "Prior studies of zero-shot stance detection identify the attitude of texts\ntowards unseen topics occurring in the same document corpus. Such task\nformulation has three limitations: (i) Single domain/dataset. A system is\noptimized on a particular dataset from a single domain; therefore, the\nresulting system cannot work well on other datasets; (ii) the model is\nevaluated on a limited number of unseen topics; (iii) it is assumed that part\nof the topics has rich annotations, which might be impossible in real-world\napplications. These drawbacks will lead to an impractical stance detection\nsystem that fails to generalize to open domains and open-form topics. This work\ndefines OpenStance: open-domain zero-shot stance detection, aiming to handle\nstance detection in an open world with neither domain constraints nor\ntopic-specific annotations. The key challenge of OpenStance lies in the\nopen-domain generalization: learning a system with fully unspecific supervision\nbut capable of generalizing to any dataset. To solve OpenStance, we propose to\ncombine indirect supervision, from textual entailment datasets, and weak\nsupervision, from data generated automatically by pre-trained Language Models.\nOur single system, without any topic-specific supervision, outperforms the\nsupervised method on three popular datasets. To our knowledge, this is the\nfirst work that studies stance detection under the open-domain zero-shot\nsetting. All data and code are publicly released.", "published": "2022-10-25 19:50:36", "link": "http://arxiv.org/abs/2210.14299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Analysis of Syntactic Agreement Neurons in Multilingual Language\n  Models", "abstract": "Structural probing work has found evidence for latent syntactic information\nin pre-trained language models. However, much of this analysis has focused on\nmonolingual models, and analyses of multilingual models have employed\ncorrelational methods that are confounded by the choice of probing tasks. In\nthis study, we causally probe multilingual language models (XGLM and\nmultilingual BERT) as well as monolingual BERT-based models across various\nlanguages; we do this by performing counterfactual perturbations on neuron\nactivations and observing the effect on models' subject-verb agreement\nprobabilities. We observe where in the model and to what extent syntactic\nagreement is encoded in each language. We find significant neuron overlap\nacross languages in autoregressive multilingual language models, but not masked\nlanguage models. We also find two distinct layer-wise effect patterns and two\ndistinct sets of neurons used for syntactic agreement, depending on whether the\nsubject and verb are separated by other tokens. Finally, we find that\nbehavioral analyses of language models are likely underestimating how sensitive\nmasked language models are to syntactic information.", "published": "2022-10-25 20:43:36", "link": "http://arxiv.org/abs/2210.14328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question\n  Answering", "abstract": "We introduce RoMQA, the first benchmark for robust, multi-evidence,\nmulti-answer question answering (QA). RoMQA contains clusters of questions that\nare derived from related constraints mined from the Wikidata knowledge graph.\nRoMQA evaluates robustness of QA models to varying constraints by measuring\nworst-case performance within each question cluster. Compared to prior QA\ndatasets, RoMQA has more human-written questions that require reasoning over\nmore evidence text and have, on average, many more correct answers. In\naddition, human annotators rate RoMQA questions as more natural or likely to be\nasked by people. We evaluate state-of-the-art large language models in\nzero-shot, few-shot, and fine-tuning settings, and find that RoMQA is\nchallenging: zero-shot and few-shot models perform similarly to naive\nbaselines, while supervised retrieval methods perform well below gold evidence\nupper bounds. Moreover, existing models are not robust to variations in\nquestion constraints, but can be made more robust by tuning on clusters of\nrelated questions. Our results show that RoMQA is a challenging benchmark for\nlarge language models, and provides a quantifiable test to build more robust QA\nmethods.", "published": "2022-10-25 21:39:36", "link": "http://arxiv.org/abs/2210.14353v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Product Safety in E-Commerce with NLP", "abstract": "Ensuring safety of the products offered to the customers is of paramount\nimportance to any e- commerce platform. Despite stringent quality and safety\nchecking of products listed on these platforms, occasionally customers might\nreceive a product that can pose a safety issue arising out of its use. In this\npaper, we present an innovative mechanism of how a large scale multinational\ne-commerce platform, Zalando, uses Natural Language Processing techniques to\nassist timely investigation of the potentially unsafe products mined directly\nfrom customer written claims in unstructured plain text. We systematically\ndescribe the types of safety issues that concern Zalando customers. We\ndemonstrate how we map this core business problem into a supervised text\nclassification problem with highly imbalanced, noisy, multilingual data in a\nAI-in-the-loop setup with a focus on Key Performance Indicator (KPI) driven\nevaluation. Finally, we present detailed ablation studies to show a\ncomprehensive comparison between different classification techniques. We\nconclude the work with how this NLP model was deployed.", "published": "2022-10-25 22:10:30", "link": "http://arxiv.org/abs/2210.14363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Monitor Model and its Misconceptions: A Clarification", "abstract": "Horizontal (automatic) and vertical (control) processes have been observed\nand reported for a long time in translation production. Schaeffer and Carl's\nMonitor Model integrates these two processes into one framework, assuming that\npriming mechanisms underlie horizontal/automatic processes, while\nvertical/monitoring processes implement consciously accessible control\nmechanisms. The Monitor Model has been criticized in various ways and several\nmisconceptions have accumulated over the past years. In this chapter, I update\nthe Monitor Model with additional evidence and argue that it is compatible with\nan enactivist approach to cognition. I address several misconceptions related\nto the Monitor Model.", "published": "2022-10-25 22:15:55", "link": "http://arxiv.org/abs/2210.14367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deploying a Retrieval based Response Model for Task Oriented Dialogues", "abstract": "Task-oriented dialogue systems in industry settings need to have high\nconversational capability, be easily adaptable to changing situations and\nconform to business constraints. This paper describes a 3-step procedure to\ndevelop a conversational model that satisfies these criteria and can\nefficiently scale to rank a large set of response candidates. First, we provide\na simple algorithm to semi-automatically create a high-coverage template set\nfrom historic conversations without any annotation. Second, we propose a neural\narchitecture that encodes the dialogue context and applicable business\nconstraints as profile features for ranking the next turn. Third, we describe a\ntwo-stage learning strategy with self-supervised training, followed by\nsupervised fine-tuning on limited data collected through a human-in-the-loop\nplatform. Finally, we describe offline experiments and present results of\ndeploying our model with human-in-the-loop to converse with live customers\nonline.", "published": "2022-10-25 23:10:19", "link": "http://arxiv.org/abs/2210.14379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Sentiment Analysis for Code-Switched Text Data", "abstract": "Multilingual transformer language models have recently attracted much\nattention from researchers and are used in cross-lingual transfer learning for\nmany NLP tasks such as text classification and named entity recognition.\nHowever, similar methods for transfer learning from monolingual text to\ncode-switched text have not been extensively explored mainly due to the\nfollowing challenges: (1) Code-switched corpus, unlike monolingual corpus,\nconsists of more than one language and existing methods can't be applied\nefficiently, (2) Code-switched corpus is usually made of resource-rich and\nlow-resource languages and upon using multilingual pre-trained language models,\nthe final model might bias towards resource-rich language. In this paper, we\nfocus on code-switched sentiment analysis where we have a labelled\nresource-rich language dataset and unlabelled code-switched data. We propose a\nframework that takes the distinction between resource-rich and low-resource\nlanguage into account. Instead of training on the entire code-switched corpus\nat once, we create buckets based on the fraction of words in the resource-rich\nlanguage and progressively train from resource-rich language dominated samples\nto low-resource language dominated samples. Extensive experiments across\nmultiple language pairs demonstrate that progressive training helps\nlow-resource language dominated samples.", "published": "2022-10-25 23:13:53", "link": "http://arxiv.org/abs/2210.14380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computer-Aided Modelling of the Bilingual Word Indices to the\n  Ninth-Century Uchitel'noe evangelie", "abstract": "The development of bilingual dictionaries to medieval translations presents\ndiverse difficulties. These result from two types of philological\ncircumstances: a) the asymmetry between the source language and the target\nlanguage; and b) the varying available sources of both the original and\ntranslated texts. In particular, the full critical edition of Tihova of\nConstantine of Preslav's Uchitel'noe evangelie ('Didactic Gospel') gives a\nrelatively good idea of the Old Church Slavonic translation but not of its\nGreek source text. This is due to the fact that Cramer's edition of the catenae\n- used as the parallel text in it - is based on several codices whose text does\nnot fully coincide with the Slavonic. This leads to the addition of the\nnewly-discovered parallels from Byzantine manuscripts and John Chrysostom's\nhomilies. Our approach to these issues is a step-wise process with two main\ngoals: a) to facilitate the philological annotation of input data and b) to\nconsider the manifestations of the mentioned challenges, first, separately in\norder to simplify their resolution, and, then, in their combination. We\ndemonstrate how we model various types of asymmetric translation correlates and\nthe variability resulting from the pluralism of sources. We also demonstrate\nhow all these constructions are being modelled and processed into the final\nindices. Our approach is designed with generalisation in mind and is intended\nto be applicable also for other translations from Greek into Old Church\nSlavonic.", "published": "2022-10-25 10:16:39", "link": "http://arxiv.org/abs/2211.05579v1", "categories": ["cs.CL", "J.5; I.7.3"], "primary_category": "cs.CL"}
{"title": "PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph\n  Completion", "abstract": "This paper presents a parameter-lite transfer learning approach of pretrained\nlanguage models (LM) for knowledge graph (KG) completion. Instead of\nfinetuning, which modifies all LM parameters, we only tune a few new parameters\nwhile keeping the original LM parameters fixed. We establish this via\nreformulating KG completion as a \"fill-in-the-blank\" task, and introducing a\nparameter-lite encoder on top of the original LMs. We show that, by tuning far\nfewer parameters than finetuning, LMs transfer non-trivially to most tasks and\nreach competitiveness with prior state-of-the-art approaches. For instance, we\noutperform the fully finetuning approaches on a KG completion benchmark by\ntuning only 1% of the parameters. The code and datasets are available at\n\\url{https://github.com/yuanyehome/PALT}.", "published": "2022-10-25 02:22:29", "link": "http://arxiv.org/abs/2210.13715v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SciFact-Open: Towards open-domain scientific claim verification", "abstract": "While research on scientific claim verification has led to the development of\npowerful systems that appear to approach human performance, these approaches\nhave yet to be tested in a realistic setting against large corpora of\nscientific literature. Moving to this open-domain evaluation setting, however,\nposes unique challenges; in particular, it is infeasible to exhaustively\nannotate all evidence documents. In this work, we present SciFact-Open, a new\ntest collection designed to evaluate the performance of scientific claim\nverification systems on a corpus of 500K research abstracts. Drawing upon\npooling techniques from information retrieval, we collect evidence for\nscientific claims by pooling and annotating the top predictions of four\nstate-of-the-art scientific claim verification models. We find that systems\ndeveloped on smaller corpora struggle to generalize to SciFact-Open, exhibiting\nperformance drops of at least 15 F1. In addition, analysis of the evidence in\nSciFact-Open reveals interesting phenomena likely to appear when claim\nverification systems are deployed in practice, e.g., cases where the evidence\nsupports only a special case of the claim. Our dataset is available at\nhttps://github.com/dwadden/scifact-open.", "published": "2022-10-25 05:45:00", "link": "http://arxiv.org/abs/2210.13777v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topical Segmentation of Spoken Narratives: A Test Case on Holocaust\n  Survivor Testimonies", "abstract": "The task of topical segmentation is well studied, but previous work has\nmostly addressed it in the context of structured, well-defined segments, such\nas segmentation into paragraphs, chapters, or segmenting text that originated\nfrom multiple sources. We tackle the task of segmenting running (spoken)\nnarratives, which poses hitherto unaddressed challenges. As a test case, we\naddress Holocaust survivor testimonies, given in English. Other than the\nimportance of studying these testimonies for Holocaust research, we argue that\nthey provide an interesting test case for topical segmentation, due to their\nunstructured surface level, relative abundance (tens of thousands of such\ntestimonies were collected), and the relatively confined domain that they\ncover. We hypothesize that boundary points between segments correspond to low\nmutual information between the sentences proceeding and following the boundary.\nBased on this hypothesis, we explore a range of algorithmic approaches to the\ntask, building on previous work on segmentation that uses generative Bayesian\nmodeling and state-of-the-art neural machinery. Compared to manually annotated\nreferences, we find that the developed approaches show considerable\nimprovements over previous work.", "published": "2022-10-25 06:02:28", "link": "http://arxiv.org/abs/2210.13783v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Relation Classification via Efficient and Effective\n  Prompting", "abstract": "Prompting pre-trained language models has achieved impressive performance on\nvarious NLP tasks, especially in low data regimes. Despite the success of\nprompting in monolingual settings, applying prompt-based methods in\nmultilingual scenarios has been limited to a narrow set of tasks, due to the\nhigh cost of handcrafting multilingual prompts. In this paper, we present the\nfirst work on prompt-based multilingual relation classification (RC), by\nintroducing an efficient and effective method that constructs prompts from\nrelation triples and involves only minimal translation for the class labels. We\nevaluate its performance in fully supervised, few-shot and zero-shot scenarios,\nand analyze its effectiveness across 14 languages, prompt variants, and\nEnglish-task training in cross-lingual settings. We find that in both fully\nsupervised and few-shot scenarios, our prompt method beats competitive\nbaselines: fine-tuning XLM-R_EM and null prompts. It also outperforms the\nrandom baseline by a large margin in zero-shot experiments. Our method requires\nlittle in-language knowledge and can be used as a strong baseline for similar\nmultilingual classification tasks.", "published": "2022-10-25 08:40:23", "link": "http://arxiv.org/abs/2210.13838v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "This joke is [MASK]: Recognizing Humor and Offense with Prompting", "abstract": "Humor is a magnetic component in everyday human interactions and\ncommunications. Computationally modeling humor enables NLP systems to entertain\nand engage with users. We investigate the effectiveness of prompting, a new\ntransfer learning paradigm for NLP, for humor recognition. We show that\nprompting performs similarly to finetuning when numerous annotations are\navailable, but gives stellar performance in low-resource humor recognition. The\nrelationship between humor and offense is also inspected by applying influence\nfunctions to prompting; we show that models could rely on offense to determine\nhumor during transfer.", "published": "2022-10-25 13:02:45", "link": "http://arxiv.org/abs/2210.13985v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Are All Spurious Features in Natural Language Alike? An Analysis through\n  a Causal Lens", "abstract": "The term `spurious correlations' has been used in NLP to informally denote\nany undesirable feature-label correlations. However, a correlation can be\nundesirable because (i) the feature is irrelevant to the label (e.g.\npunctuation in a review), or (ii) the feature's effect on the label depends on\nthe context (e.g. negation words in a review), which is ubiquitous in language\ntasks. In case (i), we want the model to be invariant to the feature, which is\nneither necessary nor sufficient for prediction. But in case (ii), even an\nideal model (e.g. humans) must rely on the feature, since it is necessary (but\nnot sufficient) for prediction. Therefore, a more fine-grained treatment of\nspurious features is needed to specify the desired model behavior. We formalize\nthis distinction using a causal model and probabilities of necessity and\nsufficiency, which delineates the causal relations between a feature and a\nlabel. We then show that this distinction helps explain results of existing\ndebiasing methods on different spurious features, and demystifies surprising\nresults such as the encoding of spurious features in model representations\nafter debiasing.", "published": "2022-10-25 13:31:28", "link": "http://arxiv.org/abs/2210.14011v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Softmax for Uncertainty Approximation in Text Classification", "abstract": "Uncertainty approximation in text classification is an important area with\napplications in domain adaptation and interpretability. One of the most widely\nused uncertainty approximation methods is Monte Carlo (MC) Dropout, which is\ncomputationally expensive as it requires multiple forward passes through the\nmodel. A cheaper alternative is to simply use the softmax based on a single\nforward pass without dropout to estimate model uncertainty. However, prior work\nhas indicated that these predictions tend to be overconfident. In this paper,\nwe perform a thorough empirical analysis of these methods on five datasets with\ntwo base neural architectures in order to identify the trade-offs between the\ntwo. We compare both softmax and an efficient version of MC Dropout on their\nuncertainty approximations and downstream text classification performance,\nwhile weighing their runtime (cost) against performance (benefit). We find\nthat, while MC dropout produces the best uncertainty approximations, using a\nsimple softmax leads to competitive and in some cases better uncertainty\nestimation for text classification at a much lower computational cost,\nsuggesting that softmax can in fact be a sufficient uncertainty estimate when\ncomputational resources are a concern.", "published": "2022-10-25 14:13:53", "link": "http://arxiv.org/abs/2210.14037v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring Mode Connectivity for Pre-trained Language Models", "abstract": "Recent years have witnessed the prevalent application of pre-trained language\nmodels (PLMs) in NLP. From the perspective of parameter space, PLMs provide\ngeneric initialization, starting from which high-performance minima could be\nfound. Although plenty of works have studied how to effectively and efficiently\nadapt PLMs to high-performance minima, little is known about the connection of\nvarious minima reached under different adaptation configurations. In this\npaper, we investigate the geometric connections of different minima through the\nlens of mode connectivity, which measures whether two minima can be connected\nwith a low-loss path. We conduct empirical analyses to investigate three\nquestions: (1) how could hyperparameters, specific tuning methods, and training\ndata affect PLM's mode connectivity? (2) How does mode connectivity change\nduring pre-training? (3) How does the PLM's task knowledge change along the\npath connecting two minima? In general, exploring the mode connectivity of PLMs\nconduces to understanding the geometric connection of different minima, which\nmay help us fathom the inner workings of PLM downstream adaptation.", "published": "2022-10-25 15:40:11", "link": "http://arxiv.org/abs/2210.14102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Better Intent Representations for Financial Open Intent\n  Classification", "abstract": "With the recent surge of NLP technologies in the financial domain, banks and\nother financial entities have adopted virtual agents (VA) to assist customers.\nA challenging problem for VAs in this domain is determining a user's reason or\nintent for contacting the VA, especially when the intent was unseen or open\nduring the VA's training. One method for handling open intents is adaptive\ndecision boundary (ADB) post-processing, which learns tight decision boundaries\nfrom intent representations to separate known and open intents. We propose\nincorporating two methods for supervised pre-training of intent\nrepresentations: prefix-tuning and fine-tuning just the last layer of a large\nlanguage model (LLM). With this proposal, our accuracy is 1.63% - 2.07% higher\nthan the prior state-of-the-art ADB method for open intent classification on\nthe banking77 benchmark amongst others. Notably, we only supplement the\noriginal ADB model with 0.1% additional trainable parameters. Ablation studies\nalso determine that our method yields better results than full fine-tuning the\nentire model. We hypothesize that our findings could stimulate a new optimal\nmethod of downstream tuning that combines parameter efficient tuning modules\nwith fine-tuning a subset of the base model's layers.", "published": "2022-10-25 20:01:13", "link": "http://arxiv.org/abs/2210.14304v1", "categories": ["cs.CL", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "On Robust Incremental Learning over Many Multilingual Steps", "abstract": "Recent work in incremental learning has introduced diverse approaches to\ntackle catastrophic forgetting from data augmentation to optimized training\nregimes. However, most of them focus on very few training steps. We propose a\nmethod for robust incremental learning over dozens of fine-tuning steps using\ndata from a variety of languages. We show that a combination of\ndata-augmentation and an optimized training regime allows us to continue\nimproving the model even for as many as fifty training steps. Crucially, our\naugmentation strategy does not require retaining access to previous training\ndata and is suitable in scenarios with privacy constraints.", "published": "2022-10-25 20:05:53", "link": "http://arxiv.org/abs/2210.14307v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthetic Text Generation with Differential Privacy: A Simple and\n  Practical Recipe", "abstract": "Privacy concerns have attracted increasing attention in data-driven products\ndue to the tendency of machine learning models to memorize sensitive training\ndata. Generating synthetic versions of such data with a formal privacy\nguarantee, such as differential privacy (DP), provides a promising path to\nmitigating these privacy concerns, but previous approaches in this direction\nhave typically failed to produce synthetic data of high quality. In this work,\nwe show that a simple and practical recipe in the text domain is effective:\nsimply fine-tuning a pretrained generative language model with DP enables the\nmodel to generate useful synthetic text with strong privacy protection. Through\nextensive empirical analyses on both benchmark and private customer data, we\ndemonstrate that our method produces synthetic text that is competitive in\nterms of utility with its non-private counterpart, meanwhile providing strong\nprotection against potential privacy leakages.", "published": "2022-10-25 21:21:17", "link": "http://arxiv.org/abs/2210.14348v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Bilingual Lexicon Induction for Low-Resource Languages using Graph\n  Matching via Optimal Transport", "abstract": "Bilingual lexicons form a critical component of various natural language\nprocessing applications, including unsupervised and semisupervised machine\ntranslation and crosslingual information retrieval. We improve bilingual\nlexicon induction performance across 40 language pairs with a graph-matching\nmethod based on optimal transport. The method is especially strong with low\namounts of supervision.", "published": "2022-10-25 23:09:20", "link": "http://arxiv.org/abs/2210.14378v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards standardizing Korean Grammatical Error Correction: Datasets and\n  Annotation", "abstract": "Research on Korean grammatical error correction (GEC) is limited, compared to\nother major languages such as English. We attribute this problematic\ncircumstance to the lack of a carefully designed evaluation benchmark for\nKorean GEC. In this work, we collect three datasets from different sources\n(Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean\ngrammatical errors. Considering the nature of Korean grammar, We then define 14\nerror types for Korean and provide KAGAS (Korean Automatic Grammatical error\nAnnotation System), which can automatically annotate error types from parallel\ncorpora. We use KAGAS on our datasets to make an evaluation benchmark for\nKorean, and present baseline models trained from our datasets. We show that the\nmodel trained with our datasets significantly outperforms the currently used\nstatistical Korean GEC system (Hanspell) on a wider range of error types,\ndemonstrating the diversity and usefulness of the datasets. The implementations\nand datasets are open-sourced.", "published": "2022-10-25 23:41:52", "link": "http://arxiv.org/abs/2210.14389v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Imbalanced Text Classification with Dynamic Curriculum\n  Learning", "abstract": "Recent advances in pre-trained language models have improved the performance\nfor text classification tasks. However, little attention is paid to the\npriority scheduling strategy on the samples during training. Humans acquire\nknowledge gradually from easy to complex concepts, and the difficulty of the\nsame material can also vary significantly in different learning stages.\nInspired by this insights, we proposed a novel self-paced dynamic curriculum\nlearning (SPDCL) method for imbalanced text classification, which evaluates the\nsample difficulty by both linguistic character and model capacity. Meanwhile,\nrather than using static curriculum learning as in the existing research, our\nSPDCL can reorder and resample training data by difficulty criterion with an\nadaptive from easy to hard pace. The extensive experiments on several\nclassification tasks show the effectiveness of SPDCL strategy, especially for\nthe imbalanced dataset.", "published": "2022-10-25 07:57:59", "link": "http://arxiv.org/abs/2210.14724v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Long Is Enough? Exploring the Optimal Intervals of Long-Range\n  Clinical Note Language Modeling", "abstract": "Large pre-trained language models (LMs) have been widely adopted in\nbiomedical and clinical domains, introducing many powerful LMs such as bio-lm\nand BioELECTRA. However, the applicability of these methods to real clinical\nuse cases is hindered, due to the limitation of pre-trained LMs in processing\nlong textual data with thousands of words, which is a common length for a\nclinical note. In this work, we explore long-range adaptation from such LMs\nwith Longformer, allowing the LMs to capture longer clinical notes context. We\nconduct experiments on three n2c2 challenges datasets and a longitudinal\nclinical dataset from Hong Kong Hospital Authority electronic health record\n(EHR) system to show the effectiveness and generalizability of this concept,\nachieving 10\\% F1-score improvement. Based on our experiments, we conclude that\ncapturing a longer clinical note interval is beneficial to the model\nperformance, but there are different cut-off intervals to achieve the optimal\nperformance for different target variables. Our code is available at\nhttps://github.com/HLTCHKUST/long-biomedical-model.", "published": "2022-10-25 09:21:28", "link": "http://arxiv.org/abs/2211.07713v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridging the Training-Inference Gap for Dense Phrase Retrieval", "abstract": "Building dense retrievers requires a series of standard procedures, including\ntraining and validating neural models and creating indexes for efficient\nsearch. However, these procedures are often misaligned in that training\nobjectives do not exactly reflect the retrieval scenario at inference time. In\nthis paper, we explore how the gap between training and inference in dense\nretrieval can be reduced, focusing on dense phrase retrieval (Lee et al., 2021)\nwhere billions of representations are indexed at inference. Since validating\nevery dense retriever with a large-scale index is practically infeasible, we\npropose an efficient way of validating dense retrievers using a small subset of\nthe entire corpus. This allows us to validate various training strategies\nincluding unifying contrastive loss terms and using hard negatives for phrase\nretrieval, which largely reduces the training-inference discrepancy. As a\nresult, we improve top-1 phrase retrieval accuracy by 2~3 points and top-20\npassage retrieval accuracy by 2~4 points for open-domain question answering.\nOur work urges modeling dense retrievers with careful consideration of training\nand inference via efficient validation while advancing phrase retrieval as a\ngeneral solution for dense retrieval.", "published": "2022-10-25 00:53:06", "link": "http://arxiv.org/abs/2210.13678v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Referee: Reference-Free Sentence Summarization with Sharper\n  Controllability through Symbolic Knowledge Distillation", "abstract": "We present Referee, a novel framework for sentence summarization that can be\ntrained reference-free (i.e., requiring no gold summaries for supervision),\nwhile allowing direct control for compression ratio. Our work is the first to\ndemonstrate that reference-free, controlled sentence summarization is feasible\nvia the conceptual framework of Symbolic Knowledge Distillation (West et al.,\n2022), where latent knowledge in pre-trained language models is distilled via\nexplicit examples sampled from the teacher models, further purified with three\ntypes of filters: length, fidelity, and Information Bottleneck. Moreover, we\nuniquely propose iterative distillation of knowledge, where student models from\nthe previous iteration of distillation serve as teacher models in the next\niteration. Starting off from a relatively modest set of GPT3-generated\nsummaries, we demonstrate how iterative knowledge distillation can lead to\nconsiderably smaller, but better summarizers with sharper controllability. A\nuseful by-product of this iterative distillation process is a high-quality\ndataset of sentence-summary pairs with varying degrees of compression ratios.\nEmpirical results demonstrate that the final student models vastly outperform\nthe much larger GPT3-Instruct model in terms of the controllability of\ncompression ratios, without compromising the quality of resulting\nsummarization.", "published": "2022-10-25 07:07:54", "link": "http://arxiv.org/abs/2210.13800v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapitch: Adaption Multi-Speaker Text-to-Speech Conditioned on Pitch\n  Disentangling with Untranscribed Data", "abstract": "In this paper, we proposed Adapitch, a multi-speaker TTS method that makes\nadaptation of the supervised module with untranscribed data. We design two self\nsupervised modules to train the text encoder and mel decoder separately with\nuntranscribed data to enhance the representation of text and mel. To better\nhandle the prosody information in a synthesized voice, a supervised TTS module\nis designed conditioned on content disentangling of pitch, text, and speaker.\nThe training phase was separated into two parts, pretrained and fixed the text\nencoder and mel decoder with unsupervised mode, then the supervised mode on the\ndisentanglement of TTS. Experiment results show that the Adaptich achieved much\nbetter quality than baseline methods.", "published": "2022-10-25 07:21:07", "link": "http://arxiv.org/abs/2210.13803v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Speech Representation Learning via Speech-level and\n  Phoneme-level Masking Approach", "abstract": "Recovering the masked speech frames is widely applied in speech\nrepresentation learning. However, most of these models use random masking in\nthe pre-training. In this work, we proposed two kinds of masking approaches:\n(1) speech-level masking, making the model to mask more speech segments than\nsilence segments, (2) phoneme-level masking, forcing the model to mask the\nwhole frames of the phoneme, instead of phoneme pieces. We pre-trained the\nmodel via these two approaches, and evaluated on two downstream tasks, phoneme\nclassification and speaker recognition. The experiments demonstrated that the\nproposed masking approaches are beneficial to improve the performance of speech\nrepresentation.", "published": "2022-10-25 07:26:47", "link": "http://arxiv.org/abs/2210.13805v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MetaSpeech: Speech Effects Switch Along with Environment for Metaverse", "abstract": "Metaverse expands the physical world to a new dimension, and the physical\nenvironment and Metaverse environment can be directly connected and entered.\nVoice is an indispensable communication medium in the real world and Metaverse.\nFusion of the voice with environment effects is important for user immersion in\nMetaverse. In this paper, we proposed using the voice conversion based method\nfor the conversion of target environment effect speech. The proposed method was\nnamed MetaSpeech, which introduces an environment effect module containing an\neffect extractor to extract the environment information and an effect encoder\nto encode the environment effect condition, in which gradient reversal layer\nwas used for adversarial training to keep the speech content and speaker\ninformation while disentangling the environmental effects. From the experiment\nresults on the public dataset of LJSpeech with four environment effects, the\nproposed model could complete the specific environment effect conversion and\noutperforms the baseline methods from the voice conversion task.", "published": "2022-10-25 07:37:48", "link": "http://arxiv.org/abs/2210.13811v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentially Private Language Models for Secure Data Sharing", "abstract": "To protect the privacy of individuals whose data is being shared, it is of\nhigh importance to develop methods allowing researchers and companies to\nrelease textual data while providing formal privacy guarantees to its\noriginators. In the field of NLP, substantial efforts have been directed at\nbuilding mechanisms following the framework of local differential privacy,\nthereby anonymizing individual text samples before releasing them. In practice,\nthese approaches are often dissatisfying in terms of the quality of their\noutput language due to the strong noise required for local differential\nprivacy. In this paper, we approach the problem at hand using global\ndifferential privacy, particularly by training a generative language model in a\ndifferentially private manner and consequently sampling data from it. Using\nnatural language prompts and a new prompt-mismatch loss, we are able to create\nhighly accurate and fluent textual datasets taking on specific desired\nattributes such as sentiment or topic and resembling statistical properties of\nthe training data. We perform thorough experiments indicating that our\nsynthetic datasets do not leak information from our original data and are of\nhigh language quality and highly suitable for training models for further\nanalysis on real-world data. Notably, we also demonstrate that training\nclassifiers on private synthetic data outperforms directly training classifiers\non real data with DP-SGD.", "published": "2022-10-25 11:12:56", "link": "http://arxiv.org/abs/2210.13918v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Dual Mechanism Priming Effects in Hindi Word Order", "abstract": "Word order choices during sentence production can be primed by preceding\nsentences. In this work, we test the DUAL MECHANISM hypothesis that priming is\ndriven by multiple different sources. Using a Hindi corpus of text productions,\nwe model lexical priming with an n-gram cache model and we capture more\nabstract syntactic priming with an adaptive neural language model. We permute\nthe preverbal constituents of corpus sentences, and then use a logistic\nregression model to predict which sentences actually occurred in the corpus\nagainst artificially generated meaning-equivalent variants. Our results\nindicate that lexical priming and lexically-independent syntactic priming\naffect complementary sets of verb classes. By showing that different priming\ninfluences are separable from one another, our results support the hypothesis\nthat multiple different cognitive mechanisms underlie priming.", "published": "2022-10-25 11:49:22", "link": "http://arxiv.org/abs/2210.13938v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Discourse Context Predictability Effects in Hindi Word Order", "abstract": "We test the hypothesis that discourse predictability influences Hindi\nsyntactic choice. While prior work has shown that a number of factors (e.g.,\ninformation status, dependency length, and syntactic surprisal) influence Hindi\nword order preferences, the role of discourse predictability is underexplored\nin the literature. Inspired by prior work on syntactic priming, we investigate\nhow the words and syntactic structures in a sentence influence the word order\nof the following sentences. Specifically, we extract sentences from the\nHindi-Urdu Treebank corpus (HUTB), permute the preverbal constituents of those\nsentences, and build a classifier to predict which sentences actually occurred\nin the corpus against artificially generated distractors. The classifier uses a\nnumber of discourse-based features and cognitive features to make its\npredictions, including dependency length, surprisal, and information status. We\nfind that information status and LSTM-based discourse predictability influence\nword order choices, especially for non-canonical object-fronted orders. We\nconclude by situating our results within the broader syntactic priming\nliterature.", "published": "2022-10-25 11:53:01", "link": "http://arxiv.org/abs/2210.13940v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Entity Divider with Language Grounding in Multi-Agent Reinforcement\n  Learning", "abstract": "We investigate the use of natural language to drive the generalization of\npolicies in multi-agent settings. Unlike single-agent settings, the\ngeneralization of policies should also consider the influence of other agents.\nBesides, with the increasing number of entities in multi-agent settings, more\nagent-entity interactions are needed for language grounding, and the enormous\nsearch space could impede the learning process. Moreover, given a simple\ngeneral instruction,e.g., beating all enemies, agents are required to decompose\nit into multiple subgoals and figure out the right one to focus on. Inspired by\nprevious work, we try to address these issues at the entity level and propose a\nnovel framework for language grounding in multi-agent reinforcement learning,\nentity divider (EnDi). EnDi enables agents to independently learn subgoal\ndivision at the entity level and act in the environment based on the associated\nentities. The subgoal division is regularized by opponent modeling to avoid\nsubgoal conflicts and promote coordinated strategies. Empirically, EnDi\ndemonstrates the strong generalization ability to unseen games with new\ndynamics and expresses the superiority over existing methods.", "published": "2022-10-25 11:53:52", "link": "http://arxiv.org/abs/2210.13942v1", "categories": ["cs.LG", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "KnowGL: Knowledge Generation and Linking from Text", "abstract": "We propose KnowGL, a tool that allows converting text into structured\nrelational data represented as a set of ABox assertions compliant with the TBox\nof a given Knowledge Graph (KG), such as Wikidata. We address this problem as a\nsequence generation task by leveraging pre-trained sequence-to-sequence\nlanguage models, e.g. BART. Given a sentence, we fine-tune such models to\ndetect pairs of entity mentions and jointly generate a set of facts consisting\nof the full set of semantic annotations for a KG, such as entity labels, entity\ntypes, and their relationships. To showcase the capabilities of our tool, we\nbuild a web application consisting of a set of UI widgets that help users to\nnavigate through the semantic data extracted from a given input text. We make\nthe KnowGL model available at https://huggingface.co/ibm/knowgl-large.", "published": "2022-10-25 12:12:36", "link": "http://arxiv.org/abs/2210.13952v5", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "IELM: An Open Information Extraction Benchmark for Pre-Trained Language\n  Models", "abstract": "We introduce a new open information extraction (OIE) benchmark for\npre-trained language models (LM). Recent studies have demonstrated that\npre-trained LMs, such as BERT and GPT, may store linguistic and relational\nknowledge. In particular, LMs are able to answer ``fill-in-the-blank''\nquestions when given a pre-defined relation category. Instead of focusing on\npre-defined relations, we create an OIE benchmark aiming to fully examine the\nopen relational information present in the pre-trained LMs. We accomplish this\nby turning pre-trained LMs into zero-shot OIE systems. Surprisingly,\npre-trained LMs are able to obtain competitive performance on both standard OIE\ndatasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets\n(TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For\ninstance, the zero-shot pre-trained LMs outperform the F1 score of the\nstate-of-the-art supervised OIE methods on our factual OIE datasets without\nneeding to use any training sets. Our code and datasets are available at\nhttps://github.com/cgraywang/IELM", "published": "2022-10-25 16:25:00", "link": "http://arxiv.org/abs/2210.14128v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PolyHope: Two-Level Hope Speech Detection from Tweets", "abstract": "Hope is characterized as openness of spirit toward the future, a desire,\nexpectation, and wish for something to happen or to be true that remarkably\naffects human's state of mind, emotions, behaviors, and decisions. Hope is\nusually associated with concepts of desired expectations and\npossibility/probability concerning the future. Despite its importance, hope has\nrarely been studied as a social media analysis task. This paper presents a hope\nspeech dataset that classifies each tweet first into \"Hope\" and \"Not Hope\",\nthen into three fine-grained hope categories: \"Generalized Hope\", \"Realistic\nHope\", and \"Unrealistic Hope\" (along with \"Not Hope\"). English tweets in the\nfirst half of 2022 were collected to build this dataset. Furthermore, we\ndescribe our annotation process and guidelines in detail and discuss the\nchallenges of classifying hope and the limitations of the existing hope speech\ndetection corpora. In addition, we reported several baselines based on\ndifferent learning approaches, such as traditional machine learning, deep\nlearning, and transformers, to benchmark our dataset. We evaluated our\nbaselines using weighted-averaged and macro-averaged F1-scores. Observations\nshow that a strict process for annotator selection and detailed annotation\nguidelines enhanced the dataset's quality. This strict annotation process\nresulted in promising performance for simple machine learning classifiers with\nonly bi-grams; however, binary and multiclass hope speech detection results\nreveal that contextual embedding models have higher performance in this\ndataset.", "published": "2022-10-25 16:34:03", "link": "http://arxiv.org/abs/2210.14136v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Data Augmentation Through Prompting for Dialogue\n  Understanding", "abstract": "Dialogue understanding tasks often necessitate abundant annotated data to\nachieve good performance and that presents challenges in low-resource settings.\nTo alleviate this barrier, we explore few-shot data augmentation for dialogue\nunderstanding by prompting large pre-trained language models and present a\nnovel approach that iterates on augmentation quality by applying\nweakly-supervised filters. We evaluate our methods on the emotion and act\nclassification tasks in DailyDialog and the intent classification task in\nFacebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our\naugmented data mixed with few-shot ground truth data are able to approach or\nsurpass existing state-of-the-art performance on both datasets. For DailyDialog\nspecifically, using 10% of the ground truth data we outperform the current\nstate-of-the-art model which uses 100% of the data.", "published": "2022-10-25 17:01:30", "link": "http://arxiv.org/abs/2210.14169v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Influence Functions for Sequence Tagging Models", "abstract": "Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging,\nand Semantic Role Labeling) are naturally framed as sequence tagging problems.\nHowever, there has been comparatively little work on interpretability methods\nfor sequence tagging models. In this paper, we extend influence functions -\nwhich aim to trace predictions back to the training points that informed them -\nto sequence tagging tasks. We define the influence of a training instance\nsegment as the effect that perturbing the labels within this segment has on a\ntest segment level prediction. We provide an efficient approximation to compute\nthis, and show that it tracks with the true segment influence, measured\nempirically. We show the practical utility of segment influence by using the\nmethod to identify systematic annotation errors in two named entity recognition\ncorpora. Code to reproduce our results is available at\nhttps://github.com/successar/Segment_Influence_Functions.", "published": "2022-10-25 17:13:11", "link": "http://arxiv.org/abs/2210.14177v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Linguistic-Enhanced Transformer with CTC Embedding for Speech\n  Recognition", "abstract": "The recent emergence of joint CTC-Attention model shows significant\nimprovement in automatic speech recognition (ASR). The improvement largely lies\nin the modeling of linguistic information by decoder. The decoder\njoint-optimized with an acoustic encoder renders the language model from\nground-truth sequences in an auto-regressive manner during training. However,\nthe training corpus of the decoder is limited to the speech transcriptions,\nwhich is far less than the corpus needed to train an acceptable language model.\nThis leads to poor robustness of decoder. To alleviate this problem, we propose\nlinguistic-enhanced transformer, which introduces refined CTC information to\ndecoder during training process, so that the decoder can be more robust. Our\nexperiments on AISHELL-1 speech corpus show that the character error rate (CER)\nis relatively reduced by up to 7%. We also find that in joint CTC-Attention ASR\nmodel, decoder is more sensitive to linguistic information than acoustic\ninformation.", "published": "2022-10-25 08:12:59", "link": "http://arxiv.org/abs/2210.14725v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cloning Ideology and Style using Deep Learning", "abstract": "Text generation tasks have gotten the attention of researchers in the last\nfew years because of their applications on a large scale.In the past, many\nresearchers focused on task-based text generations.Our research focuses on text\ngeneration based on the ideology and style of a specific author, and text\ngeneration on a topic that was not written by the same author in the past.Our\ntrained model requires an input prompt containing initial few words of text to\nproduce a few paragraphs of text based on the ideology and style of the author\non which the model is trained.Our methodology to accomplish this task is based\non Bi-LSTM.The Bi-LSTM model is used to make predictions at the character\nlevel, during the training corpus of a specific author is used along with the\nground truth corpus.A pre-trained model is used to identify the sentences of\nground truth having contradiction with the author's corpus to make our language\nmodel inclined.During training, we have achieved a perplexity score of 2.23 at\nthe character level. The experiments show a perplexity score of around 3 over\nthe test dataset.", "published": "2022-10-25 11:37:19", "link": "http://arxiv.org/abs/2211.07712v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mixed-EVC: Mixed Emotion Synthesis and Control in Voice Conversion", "abstract": "Emotional voice conversion (EVC) traditionally targets the transformation of\nspoken utterances from one emotional state to another, with previous research\nmainly focusing on discrete emotion categories. This paper departs from the\nnorm by introducing a novel perspective: a nuanced rendering of mixed emotions\nand enhancing control over emotional expression. To achieve this, we propose a\nnovel EVC framework, Mixed-EVC, which only leverages discrete emotion training\nlabels. We construct an attribute vector that encodes the relationships among\nthese discrete emotions, which is predicted using a ranking-based support\nvector machine and then integrated into a sequence-to-sequence (seq2seq) EVC\nframework. Mixed-EVC not only learns to characterize the input emotional style\nbut also quantifies its relevance to other emotions during training. As a\nresult, users have the ability to assign these attributes to achieve their\ndesired rendering of mixed emotions. Objective and subjective evaluations\nconfirm the effectiveness of our approach in terms of mixed emotion synthesis\nand control while surpassing traditional baselines in the conversion of\ndiscrete emotions from one to another.", "published": "2022-10-25 03:50:34", "link": "http://arxiv.org/abs/2210.13756v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Streaming Parrotron for on-device speech-to-speech conversion", "abstract": "We present a fully on-device streaming Speech2Speech conversion model that\nnormalizes a given input speech directly to synthesized output speech.\nDeploying such a model on mobile devices pose significant challenges in terms\nof memory footprint and computation requirements. We present a streaming-based\napproach to produce an acceptable delay, with minimal loss in speech conversion\nquality, when compared to a reference state of the art non-streaming approach.\nOur method consists of first streaming the encoder in real time while the\nspeaker is speaking. Then, as soon as the speaker stops speaking, we run the\nspectrogram decoder in streaming mode along the side of a streaming vocoder to\ngenerate output speech. To achieve an acceptable delay-quality trade-off, we\npropose a novel hybrid approach for look-ahead in the encoder which combines a\nlook-ahead feature stacker with a look-ahead self-attention. We show that our\nstreaming approach is almost 2x faster than real time on the Pixel4 CPU.", "published": "2022-10-25 04:33:00", "link": "http://arxiv.org/abs/2210.13761v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual\n  Voice Conversion Using $\u03b2$-VAE", "abstract": "We propose an unsupervised learning method to disentangle speech into content\nrepresentation and speaker identity representation. We apply this method to the\nchallenging one-shot cross-lingual voice conversion task to demonstrate the\neffectiveness of the disentanglement. Inspired by $\\beta$-VAE, we introduce a\nlearning objective that balances between the information captured by the\ncontent and speaker representations. In addition, the inductive biases from the\narchitectural design and the training dataset further encourage the desired\ndisentanglement. Both objective and subjective evaluations show the\neffectiveness of the proposed method in speech disentanglement and in one-shot\ncross-lingual voice conversion.", "published": "2022-10-25 05:12:47", "link": "http://arxiv.org/abs/2210.13771v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhanced Fuzzy Decomposition of Sound Into Sines, Transients, and Noise", "abstract": "The decomposition of sounds into sines, transients, and noise is a\nlong-standing research problem in audio processing. The current solutions for\nthis three-way separation detect either horizontal and vertical structures or\nanisotropy and orientations in the spectrogram to identify the properties of\neach spectral bin and classify it as sinusoidal, transient, or noise. This\npaper proposes an enhanced three-way decomposition method based on fuzzy logic,\nenabling soft masking while preserving the perfect reconstruction property. The\nproposed method allows each spectral bin to simultaneously belong to two\nclasses, sine and noise or transient and noise. Results of a subjective\nlistening test against three other techniques are reported, showing that the\nproposed decomposition yields a better or comparable quality. The main\nimprovement appears in transient separation, which enjoys little or no loss of\nenergy or leakage from the other components and performs well for test signals\npresenting strong transients. The audio quality of the separation is shown to\ndepend on the complexity of the input signal for all tested methods. The\nproposed method helps improve the quality of various audio processing\napplications. A successful implementation over a state-of-the-art time-scale\nmodification method is reported as an example.", "published": "2022-10-25 14:17:04", "link": "http://arxiv.org/abs/2210.14041v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EBEN: Extreme bandwidth extension network applied to speech signals\n  captured with noise-resilient body-conduction microphones", "abstract": "In this paper, we present Extreme Bandwidth Extension Network (EBEN), a\nGenerative Adversarial network (GAN) that enhances audio measured with\nbody-conduction microphones. This type of capture equipment suppresses ambient\nnoise at the expense of speech bandwidth, thereby requiring signal enhancement\ntechniques to recover the wideband speech signal. EBEN leverages a multiband\ndecomposition of the raw captured speech to decrease the data time-domain\ndimensions, and give better control over the full-band signal. This multiband\nrepresentation is fed to a U-Net-like model, which adopts a combination of\nfeature and adversarial losses to recover an enhanced audio signal. We also\nbenefit from this original representation in the proposed discriminator\narchitecture. Our approach can achieve state-of-the-art results with a\nlightweight generator and real-time compatible operation.", "published": "2022-10-25 15:19:20", "link": "http://arxiv.org/abs/2210.14090v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Speech Endpoint Detection with Regression Targets", "abstract": "Interactive voice assistants have been widely used as input interfaces in\nvarious scenarios, e.g. on smart homes devices, wearables and on AR devices.\nDetecting the end of a speech query, i.e. speech end-pointing, is an important\ntask for voice assistants to interact with users. Traditionally, speech\nend-pointing is based on pure classification methods along with arbitrary\nbinary targets. In this paper, we propose a novel regression-based speech\nend-pointing model, which enables an end-pointer to adjust its detection\nbehavior based on context of user queries. Specifically, we present a pause\nmodeling method and show its effectiveness for dynamic end-pointing. Based on\nour experiments with vendor-collected smartphone and wearables speech queries,\nour strategy shows a better trade-off between endpointing latency and accuracy,\ncompared to the traditional classification-based method. We further discuss the\nbenefits of this model and generalization of the framework in the paper.", "published": "2022-10-25 18:09:42", "link": "http://arxiv.org/abs/2210.14252v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Highly Efficient Real-Time Streaming and Fully On-Device Speaker\n  Diarization with Multi-Stage Clustering", "abstract": "While recent research advances in speaker diarization mostly focus on\nimproving the quality of diarization results, there is also an increasing\ninterest in improving the efficiency of diarization systems. In this paper, we\ndemonstrate that a multi-stage clustering strategy that uses different\nclustering algorithms for input of different lengths can address multi-faceted\nchallenges of on-device speaker diarization applications. Specifically, a\nfallback clusterer is used to handle short-form inputs; a main clusterer is\nused to handle medium-length inputs; and a pre-clusterer is used to compress\nlong-form inputs before they are processed by the main clusterer. Both the main\nclusterer and the pre-clusterer can be configured with an upper bound of the\ncomputational complexity to adapt to devices with different resource\nconstraints. This multi-stage clustering strategy is critical for streaming\non-device speaker diarization systems, where the budgets of CPU, memory and\nbattery are tight.", "published": "2022-10-25 01:20:24", "link": "http://arxiv.org/abs/2210.13690v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluating and Optimizing Hearing-Aid Self-Fitting Methods using\n  Population Coverage", "abstract": "Adults with mild-to-moderate hearing loss can use over-the-counter hearing\naids to treat their hearing loss at a fraction of traditional hearing care\ncosts. These products incorporate self-fitting methods that allow end-users to\nconfigure their hearing aids without the help of an audiologist. A self-fitting\nmethod helps users configure the gain-frequency responses that control the\namplification for each frequency band of the incoming sound. This paper\nconsiders how to design effective self-fitting methods and whether we may\nevaluate certain aspects of their design without resorting to expensive user\nstudies. Most existing fitting methods provide various user interfaces to allow\nusers to select a configuration from a predetermined set of presets. We propose\na novel metric for evaluating the performance of preset-based approaches by\ncomputing their population coverage. The population coverage estimates the\nfraction of users for which it is possible to find a configuration they prefer.\nA unique aspect of our approach is a probabilistic model that captures how a\nuser's unique preferences differ from other users with similar hearing loss.\nNext, we develop methods for determining presets to maximize population\ncoverage. Exploratory results demonstrate that the proposed algorithms can\neffectively select a small number of presets that provide higher population\ncoverage than clustering-based approaches. Moreover, we may use our algorithms\nto configure the number of increments for slider-based methods.", "published": "2022-10-25 03:02:55", "link": "http://arxiv.org/abs/2210.13732v1", "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "CoLoC: Conditioned Localizer and Classifier for Sound Event Localization\n  and Detection", "abstract": "In this article, we describe Conditioned Localizer and Classifier (CoLoC)\nwhich is a novel solution for Sound Event Localization and Detection (SELD).\nThe solution constitutes of two stages: the localization is done first and is\nfollowed by classification conditioned by the output of the localizer. In order\nto resolve the problem of the unknown number of sources we incorporate the idea\nborrowed from Sequential Set Generation (SSG). Models from both stages are\nSELDnet-like CRNNs, but with single outputs. Conducted reasoning shows that\nsuch two single-output models are fit for SELD task. We show that our solution\nimproves on the baseline system in most metrics on the STARSS22 Dataset.", "published": "2022-10-25 11:37:43", "link": "http://arxiv.org/abs/2210.13932v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Survey on Artificial Intelligence for Music Generation: Agents,\n  Domains and Perspectives", "abstract": "Music is one of the Gardner's intelligences in his theory of multiple\nintelligences. How humans perceive and understand music is still being studied\nand is crucial to develop artificial intelligence models that imitate such\nprocesses. Music generation with Artificial Intelligence is an emerging field\nthat is gaining much attention in the recent years. In this paper, we describe\nhow humans compose music and how new AI systems could imitate such process by\ncomparing past and recent advances in the field with music composition\ntechniques. To understand how AI models and algorithms generate music and the\npotential applications that might appear in the future, we explore, analyze and\ndescribe the agents that take part of the music generation process: the\ndatasets, models, interfaces, the users and the generated music. We mention\npossible applications that might benefit from this field and we also propose\nnew trends and future research directions that could be explored in the future.", "published": "2022-10-25 11:54:30", "link": "http://arxiv.org/abs/2210.13944v2", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Audio MFCC-gram Transformers for respiratory insufficiency detection in\n  COVID-19", "abstract": "This work explores speech as a biomarker and investigates the detection of\nrespiratory insufficiency (RI) by analyzing speech samples. Previous work\n\\cite{spira2021} constructed a dataset of respiratory insufficiency COVID-19\npatient utterances and analyzed it by means of a convolutional neural network\nachieving an accuracy of $87.04\\%$, validating the hypothesis that one can\ndetect RI through speech. Here, we study how Transformer neural network\narchitectures can improve the performance on RI detection. This approach\nenables construction of an acoustic model. By choosing the correct pretraining\ntechnique, we generate a self-supervised acoustic model, leading to improved\nperformance ($96.53\\%$) of Transformers for RI detection.", "published": "2022-10-25 15:11:40", "link": "http://arxiv.org/abs/2210.14085v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Semi-Supervised Learning Based on Reference Model for Low-resource TTS", "abstract": "Most previous neural text-to-speech (TTS) methods are mainly based on\nsupervised learning methods, which means they depend on a large training\ndataset and hard to achieve comparable performance under low-resource\nconditions. To address this issue, we propose a semi-supervised learning method\nfor neural TTS in which labeled target data is limited, which can also resolve\nthe problem of exposure bias in the previous auto-regressive models.\nSpecifically, we pre-train the reference model based on Fastspeech2 with much\nsource data, fine-tuned on a limited target dataset. Meanwhile, pseudo labels\ngenerated by the original reference model are used to guide the fine-tuned\nmodel's training further, achieve a regularization effect, and reduce the\noverfitting of the fine-tuned model during training on the limited target data.\nExperimental results show that our proposed semi-supervised learning scheme\nwith limited target data significantly improves the voice quality for test data\nto achieve naturalness and robustness in speech synthesis.", "published": "2022-10-25 07:48:07", "link": "http://arxiv.org/abs/2210.14723v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Artificial ASMR: A Cyber-Psychological Approach", "abstract": "The popularity of Autonomous Sensory Meridian Response (ASMR) has skyrockted\nover the past decade, but scientific studies on what exactly triggered ASMR\neffect remain few and immature, one most commonly acknowledged trigger is that\nASMR clips typically provide rich semantic information. With our attention\ncaught by the common acoustic patterns in ASMR audios, we investigate the\ncorrelation between the cyclic features of audio signals and their\neffectiveness in triggering ASMR effects. A cyber-psychological approach that\ncombines signal processing, artificial intelligence, and experimental\npsychology is taken, with which we are able to quantize ASMR-related acoustic\nfeatures, and therewith synthesize ASMR clips with random cyclic patterns but\nnot delivering identifiably scenarios to the audience, which were proven to be\neffective in triggering ASMR effects.", "published": "2022-10-25 20:25:49", "link": "http://arxiv.org/abs/2210.14321v3", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
