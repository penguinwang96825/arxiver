{"title": "A Random Forest approach to detect and identify Unlawful Insider Trading", "abstract": "According to The Exchange Act, 1934 unlawful insider trading is the abuse of\naccess to privileged corporate information. While a blurred line between\n\"routine\" the \"opportunistic\" insider trading exists, detection of strategies\nthat insiders mold to maneuver fair market prices to their advantage is an\nuphill battle for hand-engineered approaches. In the context of detailed\nhigh-dimensional financial and trade data that are structurally built by\nmultiple covariates, in this study, we explore, implement and provide detailed\ncomparison to the existing study (Deng et al. (2019)) and independently\nimplement automated end-to-end state-of-art methods by integrating principal\ncomponent analysis to the random forest (PCA-RF) followed by a standalone\nrandom forest (RF) with 320 and 3984 randomly selected, semi-manually labeled\nand normalized transactions from multiple industry. The settings successfully\nuncover latent structures and detect unlawful insider trading. Among the\nmultiple scenarios, our best-performing model accurately classified 96.43\npercent of transactions. Among all transactions the models find 95.47 lawful as\nlawful and $98.00$ unlawful as unlawful percent. Besides, the model makes very\nfew mistakes in classifying lawful as unlawful by missing only 2.00 percent. In\naddition to the classification task, model generated Gini Impurity based\nfeatures ranking, our analysis show ownership and governance related features\nbased on permutation values play important roles. In summary, a simple yet\npowerful automated end-to-end method relieves labor-intensive activities to\nredirect resources to enhance rule-making and tracking the uncaptured unlawful\ninsider trading transactions. We emphasize that developed financial and trading\nfeatures are capable of uncovering fraudulent behaviors.", "published": "2024-11-09 18:01:19", "link": "http://arxiv.org/abs/2411.13564v1", "categories": ["q-fin.ST", "cs.LG", "q-fin.RM", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "The lexical ratio: A new perspective on portfolio diversification", "abstract": "Portfolio diversification, traditionally measured through asset correlations\nand volatilitybased metrics, is fundamental to managing financial risk.\nHowever, existing diversification metrics often overlook non-numerical\nrelationships between assets that can impact portfolio stability, particularly\nduring market stresses. This paper introduces the lexical ratio (LR), a novel\nmetric that leverages textual data to capture diversification dimensions absent\nin standard approaches. By treating each asset as a unique document composed of\nsectorspecific and financial keywords, the LR evaluates portfolio\ndiversification by distributing these terms across assets, incorporating\nentropy-based insights from information theory. We thoroughly analyze LR's\nproperties, including scale invariance, concavity, and maximality,\ndemonstrating its theoretical robustness and ability to enhance risk-adjusted\nportfolio returns. Using empirical tests on S&P 500 portfolios, we compare LR's\nperformance to established metrics such as Markowitz's volatility-based\nmeasures and diversification ratios. Our tests reveal LR's superiority in\noptimizing portfolio returns, especially under varied market conditions. Our\nfindings show that LR aligns with conventional metrics and captures unique\ndiversification aspects, suggesting it is a viable tool for portfolio managers.", "published": "2024-11-09 06:26:11", "link": "http://arxiv.org/abs/2411.06080v1", "categories": ["q-fin.PM", "q-fin.RM", "q-fin.ST", "91G10, 62H20, 94A17, 91G70", "I.2.7; G.3"], "primary_category": "q-fin.PM"}
{"title": "BreakGPT: Leveraging Large Language Models for Predicting Asset Price Surges", "abstract": "This paper introduces BreakGPT, a novel large language model (LLM)\narchitecture adapted specifically for time series forecasting and the\nprediction of sharp upward movements in asset prices. By leveraging both the\ncapabilities of LLMs and Transformer-based models, this study evaluates\nBreakGPT and other Transformer-based models for their ability to address the\nunique challenges posed by highly volatile financial markets. The primary\ncontribution of this work lies in demonstrating the effectiveness of combining\ntime series representation learning with LLM prediction frameworks. We showcase\nBreakGPT as a promising solution for financial forecasting with minimal\ntraining and as a strong competitor for capturing both local and global\ntemporal dependencies.", "published": "2024-11-09 05:40:32", "link": "http://arxiv.org/abs/2411.06076v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Improved intent classification based on context information using a\n  windows-based approach", "abstract": "Conversational systems have a Natural Language Understanding (NLU) module. In\nthis module, there is a task known as an intent classification that aims at\nidentifying what a user is attempting to achieve from an utterance. Previous\nworks use only the current utterance to predict the intent of a given query and\nthey do not consider the role of the context (one or a few previous utterances)\nin the dialog flow for this task. In this work, we propose several approaches\nto investigate the role of contextual information for the intent classification\ntask. Each approach is used to carry out a concatenation between the dialogue\nhistory and the current utterance. Our intent classification method is based on\na convolutional neural network that obtains effective vector representations\nfrom BERT to perform accurate intent classification using an approach\nwindow-based. Our experiments were carried out on a real-world Brazilian\nPortuguese corpus with dialog flows provided by Wavy global company. Our\nresults achieved substantial improvements over the baseline, isolated\nutterances (without context), in three approaches using the user's utterance\nand system's response from previous messages as dialogue context.", "published": "2024-11-09 00:56:02", "link": "http://arxiv.org/abs/2411.06022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM\n  Output", "abstract": "Immense effort has been dedicated to minimizing the presence of harmful or\nbiased generative content and better aligning AI output to human intention;\nhowever, research investigating the cultural values of LLMs is still in very\nearly stages. Cultural values underpin how societies operate, providing\nprofound insights into the norms, priorities, and decision making of their\nmembers. In recognition of this need for further research, we draw upon\ncultural psychology theory and the empirically-validated GLOBE framework to\npropose the LLM-GLOBE benchmark for evaluating the cultural value systems of\nLLMs, and we then leverage the benchmark to compare the values of Chinese and\nUS LLMs. Our methodology includes a novel \"LLMs-as-a-Jury\" pipeline which\nautomates the evaluation of open-ended content to enable large-scale analysis\nat a conceptual level. Results clarify similarities and differences that exist\nbetween Eastern and Western cultural value systems and suggest that\nopen-generation tasks represent a more promising direction for evaluation of\ncultural values. We interpret the implications of this research for subsequent\nmodel development, evaluation, and deployment efforts as they relate to LLMs,\nAI cultural alignment more broadly, and the influence of AI cultural value\nsystems on human-AI collaboration outcomes.", "published": "2024-11-09 01:38:55", "link": "http://arxiv.org/abs/2411.06032v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "abstract": "Augmenting LLMs with context leads to improved performance across many\napplications. Despite much research on Retrieval Augmented Generation (RAG)\nsystems, an open question is whether errors arise because LLMs fail to utilize\nthe context from retrieval or the context itself is insufficient to answer the\nquery. To shed light on this, we develop a new notion of sufficient context,\nalong with a way to classify instances that have enough information to answer\nthe query. We then use sufficient context to analyze several models and\ndatasets. By stratifying errors based on context sufficiency, we find that\nproprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the\ncontext is sufficient, but often output incorrect answers instead of abstaining\nwhen the context is not. On the other hand, open-source LLMs (Llama, Mistral,\nGemma) hallucinate or abstain often, even with sufficient context. We further\ncategorize cases when the context is useful, and improves accuracy, even though\nit does not fully answer the query and the model errs without the context.\nBuilding on our findings, we explore ways to reduce hallucinations in RAG\nsystems, including a new selective generation method that leverages sufficient\ncontext information for guided abstention. Our method improves the fraction of\ncorrect answers among times where the model responds by 2-10% for Gemini, GPT,\nand Gemma.", "published": "2024-11-09 02:13:14", "link": "http://arxiv.org/abs/2411.06037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZhoBLiMP: a Systematic Assessment of Language Models with Linguistic\n  Minimal Pairs in Chinese", "abstract": "Whether and how language models (LMs) acquire the syntax of natural languages\nhas been widely evaluated under the minimal pair paradigm. However, a lack of\nwide-coverage benchmarks in languages other than English has constrained\nsystematic investigations into the issue. Addressing it, we first introduce\nZhoBLiMP, the most comprehensive benchmark of linguistic minimal pairs for\nChinese to date, with 118 paradigms, covering 15 linguistic phenomena. We then\ntrain 20 LMs of different sizes (14M to 1.4B) on Chinese corpora of various\nvolumes (100M to 3B tokens) and evaluate them along with 14 off-the-shelf LLMs\non ZhoBLiMP. The overall results indicate that Chinese grammar can be mostly\nlearned by models with around 500M parameters, trained on 1B tokens with one\nepoch, showing limited benefits for further scaling. Most (N=95) linguistic\nparadigms are of easy or medium difficulty for LMs, while there are still 13\nparadigms that remain challenging even for models with up to 32B parameters. In\nregard to how LMs acquire Chinese grammar, we observe a U-shaped learning\npattern in several phenomena, similar to those observed in child language\nacquisition.", "published": "2024-11-09 07:16:08", "link": "http://arxiv.org/abs/2411.06096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Reference Errors in Scientific Literature with Large Language\n  Models", "abstract": "Reference errors, such as citation and quotation errors, are common in\nscientific papers. Such errors can result in the propagation of inaccurate\ninformation, but are difficult and time-consuming to detect, posing a\nsignificant challenge to scientific publishing. To support automatic detection\nof reference errors, this work evaluated the ability of large language models\nin OpenAI's GPT family to detect quotation errors. Specifically, we prepared an\nexpert-annotated, general-domain dataset of statement-reference pairs from\njournal articles. Large language models were evaluated in different settings\nwith varying amounts of reference information provided by retrieval\naugmentation. Our results showed that large language models are able to detect\nerroneous citations with limited context and without fine-tuning. This study\ncontributes to the growing literature that seeks to utilize artificial\nintelligence to assist in the writing, reviewing, and publishing of scientific\npapers. Potential avenues for further improvements in this task are also\ndiscussed.", "published": "2024-11-09 07:30:38", "link": "http://arxiv.org/abs/2411.06101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building an Efficient Multilingual Non-Profit IR System for the Islamic\n  Domain Leveraging Multiprocessing Design in Rust", "abstract": "The widespread use of large language models (LLMs) has dramatically improved\nmany applications of Natural Language Processing (NLP), including Information\nRetrieval (IR). However, domains that are not driven by commercial interest\noften lag behind in benefiting from AI-powered solutions. One such area is\nreligious and heritage corpora. Alongside similar domains, Islamic literature\nholds significant cultural value and is regularly utilized by scholars and the\ngeneral public. Navigating this extensive amount of text is challenging, and\nthere is currently no unified resource that allows for easy searching of this\ndata using advanced AI tools. This work focuses on the development of a\nmultilingual non-profit IR system for the Islamic domain. This process brings a\nfew major challenges, such as preparing multilingual domain-specific corpora\nwhen data is limited in certain languages, deploying a model on\nresource-constrained devices, and enabling fast search on a limited budget. By\nemploying methods like continued pre-training for domain adaptation and\nlanguage reduction to decrease model size, a lightweight multilingual retrieval\nmodel was prepared, demonstrating superior performance compared to larger\nmodels pre-trained on general domain data. Furthermore, evaluating the proposed\narchitecture that utilizes Rust Language capabilities shows the possibility of\nimplementing efficient semantic search in a low-resource setting.", "published": "2024-11-09 11:37:18", "link": "http://arxiv.org/abs/2411.06151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding\n  And A Retrieval-Aware Tuning Framework", "abstract": "The ability to understand and answer questions over documents can be useful\nin many business and practical applications. However, documents often contain\nlengthy and diverse multimodal contents such as texts, figures, and tables,\nwhich are very time-consuming for humans to read thoroughly. Hence, there is an\nurgent need to develop effective and automated methods to aid humans in this\ntask. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an\nautomated framework to evaluate the performance of large multimodal models. We\nfurther propose a retrieval-aware tuning approach for efficient and effective\nmultimodal document reading. Compared to existing works, our benchmark consists\nof more recent and lengthy documents with hundreds of pages, while also\nrequiring open-ended solutions and not just extractive answers. To our\nknowledge, our training framework is the first to directly address the\nretrieval setting for multimodal long documents. To enable tuning open-source\nmodels, we construct a training corpus in a fully automatic manner for the\nquestion-answering task over such documents. Experiments show that our tuning\napproach achieves a relative improvement of 4.6% for the correctness of model\nresponses, compared to the baseline open-source models. Our data, code, and\nmodels are available at https://multimodal-documents.github.io.", "published": "2024-11-09 13:30:38", "link": "http://arxiv.org/abs/2411.06176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WMT24 Test Suite: Gender Resolution in Speaker-Listener Dialogue Roles", "abstract": "We assess the difficulty of gender resolution in literary-style dialogue\nsettings and the influence of gender stereotypes. Instances of the test suite\ncontain spoken dialogue interleaved with external meta-context about the\ncharacters and the manner of speaking. We find that character and manner\nstereotypes outside of the dialogue significantly impact the gender agreement\nof referents within the dialogue.", "published": "2024-11-09 14:30:58", "link": "http://arxiv.org/abs/2411.06194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Knowledge Boundaries in Large Language Models for Retrieval\n  Judgment", "abstract": "Large Language Models (LLMs) are increasingly recognized for their practical\napplications. However, these models often encounter challenges in dynamically\nchanging knowledge, as well as in managing unknown static knowledge.\nRetrieval-Augmented Generation (RAG) tackles this challenge and has shown a\nsignificant impact on LLMs. Actually, we find that the impact of RAG on the\nquestion answering capabilities of LLMs can be categorized into three groups:\nbeneficial, neutral, and harmful. By minimizing retrieval requests that yield\nneutral or harmful results, we can effectively reduce both time and\ncomputational costs, while also improving the overall performance of LLMs. This\ninsight motivates us to differentiate between types of questions using certain\nmetrics as indicators, to decrease the retrieval ratio without compromising\nperformance. In our work, we propose a method that is able to identify\ndifferent types of questions from this view by training a Knowledge Boundary\nModel (KBM). Experiments conducted on 11 English and Chinese datasets\nillustrate that the KBM effectively delineates the knowledge boundary,\nsignificantly decreasing the proportion of retrievals required for optimal\nend-to-end performance. Specifically, we evaluate the effectiveness of KBM in\nthree complex scenarios: dynamic knowledge, long-tail static knowledge, and\nmulti-hop problems, as well as its functionality as an external LLM plug-in.", "published": "2024-11-09 15:12:28", "link": "http://arxiv.org/abs/2411.06207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Human Explanations for Robust Hate Speech Detection", "abstract": "Given the black-box nature and complexity of large transformer language\nmodels (LM), concerns about generalizability and robustness present ethical\nimplications for domains such as hate speech (HS) detection. Using the content\nrich Social Bias Frames dataset, containing human-annotated stereotypes,\nintent, and targeted groups, we develop a three stage analysis to evaluate if\nLMs faithfully assess hate speech. First, we observe the need for modeling\ncontextually grounded stereotype intents to capture implicit semantic meaning.\nNext, we design a new task, Stereotype Intent Entailment (SIE), which\nencourages a model to contextually understand stereotype presence. Finally,\nthrough ablation tests and user studies, we find a SIE objective improves\ncontent understanding, but challenges remain in modeling implicit intent.", "published": "2024-11-09 15:29:04", "link": "http://arxiv.org/abs/2411.06213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An $\\mathbf{L^*}$ Algorithm for Deterministic Weighted Regular Languages", "abstract": "Extracting finite state automata (FSAs) from black-box models offers a\npowerful approach to gaining interpretable insights into complex model\nbehaviors. To support this pursuit, we present a weighted variant of Angluin's\n(1987) $\\mathbf{L^*}$ algorithm for learning FSAs. We stay faithful to the\noriginal algorithm, devising a way to exactly learn deterministic weighted FSAs\nwhose weights support division. Furthermore, we formulate the learning process\nin a manner that highlights the connection with FSA minimization, showing how\n$\\mathbf{L^*}$ directly learns a minimal automaton for the target language.", "published": "2024-11-09 16:17:14", "link": "http://arxiv.org/abs/2411.06228v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Detection of LLM-Generated Text: A Comparative Analysis", "abstract": "The ability of large language models to generate complex texts allows them to\nbe widely integrated into many aspects of life, and their output can quickly\nfill all network resources. As the impact of LLMs grows, it becomes\nincreasingly important to develop powerful detectors for the generated text.\nThis detector is essential to prevent the potential misuse of these\ntechnologies and to protect areas such as social media from the negative\neffects of false content generated by LLMS. The main goal of LLM-generated text\ndetection is to determine whether text is generated by an LLM, which is a basic\nbinary classification task. In our work, we mainly use three different\nclassification methods based on open source datasets: traditional machine\nlearning techniques such as logistic regression, k-means clustering, Gaussian\nNaive Bayes, support vector machines, and methods based on converters such as\nBERT, and finally algorithms that use LLMs to detect LLM-generated text. We\nfocus on model generalization, potential adversarial attacks, and accuracy of\nmodel evaluation. Finally, the possible research direction in the future is\nproposed, and the current experimental results are summarized.", "published": "2024-11-09 18:27:15", "link": "http://arxiv.org/abs/2411.06248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zyda-2: a 5 Trillion Token High-Quality Dataset", "abstract": "In this technical report, we present Zyda-2: a five trillion token dataset\nfor language model pretraining. Zyda-2 was used to train our Zamba2 series of\nmodels which are state-of-the-art for their weight class. We build Zyda-2 by\ncollating high-quality open-source tokens such as FineWeb and DCLM, then\ndistilling them to the highest-quality subset via cross-deduplication and\nmodel-based quality filtering. Zyda-2 is released under a permissive open\nlicense, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2", "published": "2024-11-09 04:57:41", "link": "http://arxiv.org/abs/2411.06068v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StopHC: A Harmful Content Detection and Mitigation Architecture for\n  Social Media Platforms", "abstract": "The mental health of social media users has started more and more to be put\nat risk by harmful, hateful, and offensive content. In this paper, we propose\n\\textsc{StopHC}, a harmful content detection and mitigation architecture for\nsocial media platforms. Our aim with \\textsc{StopHC} is to create more secure\nonline environments. Our solution contains two modules, one that employs deep\nneural network architecture for harmful content detection, and one that uses a\nnetwork immunization algorithm to block toxic nodes and stop the spread of\nharmful content. The efficacy of our solution is demonstrated by experiments\nconducted on two real-world datasets.", "published": "2024-11-09 10:23:22", "link": "http://arxiv.org/abs/2411.06138v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation", "abstract": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research.", "published": "2024-11-09 12:06:40", "link": "http://arxiv.org/abs/2411.06159v3", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "SEEKR: Selective Attention-Guided Knowledge Retention for Continual\n  Learning of Large Language Models", "abstract": "Continual learning (CL) is crucial for language models to dynamically adapt\nto the evolving real-world demands. To mitigate the catastrophic forgetting\nproblem in CL, data replay has been proven a simple and effective strategy, and\nthe subsequent data-replay-based distillation can further enhance the\nperformance. However, existing methods fail to fully exploit the knowledge\nembedded in models from previous tasks, resulting in the need for a relatively\nlarge number of replay samples to achieve good results. In this work, we first\nexplore and emphasize the importance of attention weights in knowledge\nretention, and then propose a SElective attEntion-guided Knowledge Retention\nmethod (SEEKR) for data-efficient replay-based continual learning of large\nlanguage models (LLMs). Specifically, SEEKR performs attention distillation on\nthe selected attention heads for finer-grained knowledge retention, where the\nproposed forgettability-based and task-sensitivity-based measures are used to\nidentify the most valuable attention heads. Experimental results on two\ncontinual learning benchmarks for LLMs demonstrate the superiority of SEEKR\nover the existing methods on both performance and efficiency. Explicitly, SEEKR\nachieves comparable or even better performance with only 1/10 of the replayed\ndata used by other methods, and reduces the proportion of replayed data to 1%.", "published": "2024-11-09 13:02:36", "link": "http://arxiv.org/abs/2411.06171v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs", "abstract": "This paper proposes a Clustering, Labeling, then Augmenting framework that\nsignificantly enhances performance in Semi-Supervised Text Classification\n(SSTC) tasks, effectively addressing the challenge of vast datasets with\nlimited labeled examples. Unlike traditional SSTC approaches that rely on a\npredefined small set of labeled data to generate pseudo-labels for the\nunlabeled data, this framework innovatively employs clustering to select\nrepresentative \"landmarks\" for labeling. These landmarks subsequently act as\nintermediaries in an ensemble of augmentation techniques, including\nRetrieval-Augmented Generation (RAG), Large Language Model (LLMs)-based\nrewriting, and synonym substitution, to generate synthetic labeled data without\nmaking pseudo-labels for the unlabeled data. Empirical results show that even\nin complex text document classification scenarios involving over 100\ncategories, our method achieves state-of-the-art accuracies of 95.41% on the\nReuters dataset and 82.43% on the Web of Science dataset. Our approach\nsignificantly reduces the reliance on human labeling efforts and the associated\nexpenses, while simultaneously ensuring high data quality and minimizing\nprivacy risks. The finetuning results further show the efficiency of\nfine-tuning LLMs for text classification tasks, highlighting a robust solution\nfor leveraging limited labeled data.", "published": "2024-11-09 13:17:39", "link": "http://arxiv.org/abs/2411.06175v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization", "abstract": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.", "published": "2024-11-09 15:12:43", "link": "http://arxiv.org/abs/2411.06208v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating\n  Financial Large Language Models", "abstract": "As large language models become increasingly prevalent in the financial\nsector, there is a pressing need for a standardized method to comprehensively\nassess their performance. However, existing finance benchmarks often suffer\nfrom limited language and task coverage, as well as challenges such as\nlow-quality datasets and inadequate adaptability for LLM evaluation. To address\nthese limitations, we propose \"Golden Touchstone\", the first comprehensive\nbilingual benchmark for financial LLMs, which incorporates representative\ndatasets from both Chinese and English across eight core financial NLP tasks.\nDeveloped from extensive open source data collection and industry-specific\ndemands, this benchmark includes a variety of financial tasks aimed at\nthoroughly assessing models' language understanding and generation\ncapabilities. Through comparative analysis of major models on the benchmark,\nsuch as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and\nlimitations in processing complex financial information. Additionally, we\nopen-sourced Touchstone-GPT, a financial LLM trained through continual\npre-training and financial instruction tuning, which demonstrates strong\nperformance on the bilingual benchmark but still has limitations in specific\ntasks.This research not only provides the financial large language models with\na practical evaluation tool but also guides the development and optimization of\nfuture research. The source code for Golden Touchstone and model weight of\nTouchstone-GPT have been made publicly available at\n\\url{https://github.com/IDEA-FinAI/Golden-Touchstone}, contributing to the\nongoing evolution of FinLLMs and fostering further research in this critical\narea.", "published": "2024-11-09 20:09:11", "link": "http://arxiv.org/abs/2411.06272v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Target-driven Attack for Large Language Models", "abstract": "Current large language models (LLM) provide a strong foundation for\nlarge-scale user-oriented natural language tasks. Many users can easily inject\nadversarial text or instructions through the user interface, thus causing LLM\nmodel security challenges like the language model not giving the correct\nanswer. Although there is currently a large amount of research on black-box\nattacks, most of these black-box attacks use random and heuristic strategies.\nIt is unclear how these strategies relate to the success rate of attacks and\nthus effectively improve model robustness. To solve this problem, we propose\nour target-driven black-box attack method to maximize the KL divergence between\nthe conditional probabilities of the clean text and the attack text to redefine\nthe attack's goal. We transform the distance maximization problem into two\nconvex optimization problems based on the attack goal to solve the attack text\nand estimate the covariance. Furthermore, the projected gradient descent\nalgorithm solves the vector corresponding to the attack text. Our target-driven\nblack-box attack approach includes two attack strategies: token manipulation\nand misinformation attack. Experimental results on multiple Large Language\nModels and datasets demonstrate the effectiveness of our attack method.", "published": "2024-11-09 15:59:59", "link": "http://arxiv.org/abs/2411.07268v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimizing Large Language Models through Quantization: A Comparative\n  Analysis of PTQ and QAT Techniques", "abstract": "This paper presents a comprehensive analysis of quantization techniques for\noptimizing Large Language Models (LLMs), specifically focusing on Post-Training\nQuantization (PTQ) and Quantization-Aware Training (QAT). Through empirical\nevaluation across models ranging from 10M to 1B parameters, we demonstrate that\nquantization can achieve up to 68% reduction in model size while maintaining\nperformance within 6% of full-precision baselines when utilizing our proposed\nscaling factor {\\gamma}. Our experiments show that INT8 quantization delivers a\n40% reduction in computational cost and power consumption, while INT4\nquantization further improves these metrics by 60%. We introduce a novel\ntheoretical framework for mixed-precision quantization, deriving optimal bit\nallocation strategies based on layer sensitivity and weight variance. Hardware\nefficiency evaluations on edge devices reveal that our quantization approach\nenables up to 2.4x throughput improvement for INT8 and 3x for INT4, with 60%\npower reduction compared to full-precision models.", "published": "2024-11-09 06:30:13", "link": "http://arxiv.org/abs/2411.06084v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Expansion Quantization Network: An Efficient Micro-emotion Annotation\n  and Detection Framework", "abstract": "Text emotion detection constitutes a crucial foundation for advancing\nartificial intelligence from basic comprehension to the exploration of\nemotional reasoning. Most existing emotion detection datasets rely on manual\nannotations, which are associated with high costs, substantial subjectivity,\nand severe label imbalances. This is particularly evident in the inadequate\nannotation of micro-emotions and the absence of emotional intensity\nrepresentation, which fail to capture the rich emotions embedded in sentences\nand adversely affect the quality of downstream task completion. By proposing an\nall-labels and training-set label regression method, we map label values to\nenergy intensity levels, thereby fully leveraging the learning capabilities of\nmachine models and the interdependencies among labels to uncover multiple\nemotions within samples. This led to the establishment of the Emotion\nQuantization Network (EQN) framework for micro-emotion detection and\nannotation. Using five commonly employed sentiment datasets, we conducted\ncomparative experiments with various models, validating the broad applicability\nof our framework within NLP machine learning models. Based on the EQN\nframework, emotion detection and annotation are conducted on the GoEmotions\ndataset. A comprehensive comparison with the results from Google literature\ndemonstrates that the EQN framework possesses a high capability for automatic\ndetection and annotation of micro-emotions. The EQN framework is the first to\nachieve automatic micro-emotion annotation with energy-level scores, providing\nstrong support for further emotion detection analysis and the quantitative\nresearch of emotion computing.", "published": "2024-11-09 12:09:26", "link": "http://arxiv.org/abs/2411.06160v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech-Based Estimation of Schizophrenia Severity Using Feature Fusion", "abstract": "Speech-based assessment of the schizophrenia spectrum has been widely\nresearched over in the recent past. In this study, we develop a deep learning\nframework to estimate schizophrenia severity scores from speech using a feature\nfusion approach that fuses articulatory features with different self-supervised\nspeech features extracted from pre-trained audio models. We also propose an\nauto-encoder-based self-supervised representation learning framework to extract\ncompact articulatory embeddings from speech. Our top-performing speech-based\nfusion model with Multi-Head Attention (MHA) reduces Mean Absolute Error (MAE)\nby 9.18% and Root Mean Squared Error (RMSE) by 9.36% for schizophrenia severity\nestimation when compared with the previous models that combined speech and\nvideo inputs.", "published": "2024-11-09 02:06:00", "link": "http://arxiv.org/abs/2411.06033v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Selective State Space Model for Monaural Speech Enhancement", "abstract": "Voice user interfaces (VUIs) have facilitated the efficient interactions\nbetween humans and machines through spoken commands. Since real-word acoustic\nscenes are complex, speech enhancement plays a critical role for robust VUI.\nTransformer and its variants, such as Conformer, have demonstrated cutting-edge\nresults in speech enhancement. However, both of them suffers from the quadratic\ncomputational complexity with respect to the sequence length, which hampers\ntheir ability to handle long sequences. Recently a novel State Space Model\ncalled Mamba, which shows strong capability to handle long sequences with\nlinear complexity, offers a solution to address this challenge. In this paper,\nwe propose a novel hybrid convolution-Mamba backbone, denoted as MambaDC, for\nspeech enhancement. Our MambaDC marries the benefits of convolutional networks\nto model the local interactions and Mamba's ability for modeling long-range\nglobal dependencies. We conduct comprehensive experiments within both basic and\nstate-of-the-art (SoTA) speech enhancement frameworks, on two commonly used\ntraining targets. The results demonstrate that MambaDC outperforms Transformer,\nConformer, and the standard Mamba across all training targets. Built upon the\ncurrent advanced framework, the use of MambaDC backbone showcases superior\nresults compared to existing \\textcolor{black}{SoTA} systems. This sets the\nstage for efficient long-range global modeling in speech enhancement.", "published": "2024-11-09 15:42:44", "link": "http://arxiv.org/abs/2411.06217v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Acoustic Volume Rendering for Neural Impulse Response Fields", "abstract": "Realistic audio synthesis that captures accurate acoustic phenomena is\nessential for creating immersive experiences in virtual and augmented reality.\nSynthesizing the sound received at any position relies on the estimation of\nimpulse response (IR), which characterizes how sound propagates in one scene\nalong different paths before arriving at the listener's position. In this\npaper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts\nvolume rendering techniques to model acoustic impulse responses. While volume\nrendering has been successful in modeling radiance fields for images and neural\nscene representations, IRs present unique challenges as time-series signals. To\naddress these challenges, we introduce frequency-domain volume rendering and\nuse spherical integration to fit the IR measurements. Our method constructs an\nimpulse response field that inherently encodes wave propagation principles and\nachieves state-of-the-art performance in synthesizing impulse responses for\nnovel poses. Experiments show that AVR surpasses current leading methods by a\nsubstantial margin. Additionally, we develop an acoustic simulation platform,\nAcoustiX, which provides more accurate and realistic IR simulations than\nexisting simulators. Code for AVR and AcoustiX are available at\nhttps://zitonglan.github.io/avr.", "published": "2024-11-09 23:15:51", "link": "http://arxiv.org/abs/2411.06307v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Intelligent Fault Diagnosis of Type and Severity in Low-Frequency, Low\n  Bit-Depth Signals", "abstract": "This study focuses on Intelligent Fault Diagnosis (IFD) in rotating machinery\nutilizing a single microphone and a data-driven methodology, effectively\ndiagnosing 42 classes of fault types and severities. The research leverages\nsound data from the imbalanced MaFaulDa dataset, aiming to strike a balance\nbetween high performance and low resource consumption. The testing phase\nencompassed a variety of configurations, including sampling, quantization,\nsignal normalization, silence removal, Wiener filtering, data scaling,\nwindowing, augmentation, and classifier tuning using XGBoost. Through the\nanalysis of time, frequency, mel-frequency, and statistical features, we\nachieved an impressive accuracy of 99.54% and an F-Beta score of 99.52% with\njust 6 boosting trees at an 8 kHz, 8-bit configuration. Moreover, when\nutilizing only MFCCs along with their first- and second-order deltas, we\nrecorded an accuracy of 97.83% and an F-Beta score of 97.67%. Lastly, by\nimplementing a greedy wrapper approach, we obtained a remarkable accuracy of\n96.82% and an F-Beta score of 98.86% using 50 selected features, nearly all of\nwhich were first- and second-order deltas of the MFCCs.", "published": "2024-11-09 22:01:11", "link": "http://arxiv.org/abs/2411.06299v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
