{"title": "Connecting Distant Entities with Induction through Conditional Random\n  Fields for Named Entity Recognition: Precursor-Induced CRF", "abstract": "This paper presents a method of designing specific high-order dependency\nfactor on the linear chain conditional random fields (CRFs) for named entity\nrecognition (NER). Named entities tend to be separated from each other by\nmultiple outside tokens in a text, and thus the first-order CRF, as well as the\nsecond-order CRF, may innately lose transition information between distant\nnamed entities. The proposed design uses outside label in NER as a transmission\nmedium of precedent entity information on the CRF. Then, empirical results\napparently demonstrate that it is possible to exploit long-distance label\ndependency in the original first-order linear chain CRF structure upon NER\nwhile reducing computational loss rather than in the second-order CRF.", "published": "2018-05-26 02:39:10", "link": "http://arxiv.org/abs/1805.10414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term\n  Embeddings", "abstract": "This paper describes a hypernym discovery system for our participation in the\nSemEval-2018 Task 9, which aims to discover the best (set of) candidate\nhypernyms for input concepts or entities, given the search space of a\npre-defined vocabulary. We introduce a neural network architecture for the\nconcerned task and empirically study various neural network models to build the\nrepresentations in latent space for words and phrases. The evaluated models\ninclude convolutional neural network, long-short term memory network, gated\nrecurrent unit and recurrent convolutional neural network. We also explore\ndifferent embedding methods, including word embedding and sense embedding for\nbetter performance.", "published": "2018-05-26 11:55:59", "link": "http://arxiv.org/abs/1805.10465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependent Gated Reading for Cloze-Style Question Answering", "abstract": "We present a novel deep learning architecture to address the cloze-style\nquestion answering task. Existing approaches employ reading mechanisms that do\nnot fully exploit the interdependency between the document and the query. In\nthis paper, we propose a novel \\emph{dependent gated reading} bidirectional GRU\nnetwork (DGR) to efficiently model the relationship between the document and\nthe query during encoding and decision making. Our evaluation shows that DGR\nobtains highly competitive performance on well-known machine comprehension\nbenchmarks such as the Children's Book Test (CBT-NE and CBT-CN) and Who DiD\nWhat (WDW, Strict and Relaxed). Finally, we extensively analyze and validate\nour model by ablation and attention studies.", "published": "2018-05-26 19:26:35", "link": "http://arxiv.org/abs/1805.10528v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Splitting source code identifiers using Bidirectional LSTM Recurrent\n  Neural Network", "abstract": "Programmers make rich use of natural language in the source code they write\nthrough identifiers and comments. Source code identifiers are selected from a\npool of tokens which are strongly related to the meaning, naming conventions,\nand context. These tokens are often combined to produce more precise and\nobvious designations. Such multi-part identifiers count for 97% of all naming\ntokens in the Public Git Archive - the largest dataset of Git repositories to\ndate. We introduce a bidirectional LSTM recurrent neural network to detect\nsubtokens in source code identifiers. We trained that network on 41.7 million\ndistinct splittable identifiers collected from 182,014 open source projects in\nPublic Git Archive, and show that it outperforms several other machine learning\nmodels. The proposed network can be used to improve the upstream models which\nare based on source code identifiers, as well as improving developer experience\nallowing writing code without switching the keyboard case.", "published": "2018-05-26 06:46:55", "link": "http://arxiv.org/abs/1805.11651v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Using Syntax to Ground Referring Expressions in Natural Images", "abstract": "We introduce GroundNet, a neural network for referring expression recognition\n-- the task of localizing (or grounding) in an image the object referred to by\na natural language expression. Our approach to this task is the first to rely\non a syntactic analysis of the input referring expression in order to inform\nthe structure of the computation graph. Given a parse tree for an input\nexpression, we explicitly map the syntactic constituents and relationships\npresent in the tree to a composed graph of neural modules that defines our\narchitecture for performing localization. This syntax-based approach aids\nlocalization of \\textit{both} the target object and auxiliary supporting\nobjects mentioned in the expression. As a result, GroundNet is more\ninterpretable than previous methods: we can (1) determine which phrase of the\nreferring expression points to which object in the image and (2) track how the\nlocalization of the target object is determined by the network. We study this\nproperty empirically by introducing a new set of annotations on the GoogleRef\ndataset to evaluate localization of supporting objects. Our experiments show\nthat GroundNet achieves state-of-the-art accuracy in identifying supporting\nobjects, while maintaining comparable performance in the localization of target\nobjects.", "published": "2018-05-26 22:02:05", "link": "http://arxiv.org/abs/1805.10547v1", "categories": ["cs.CV", "cs.CL", "cs.NE"], "primary_category": "cs.CV"}
{"title": "Automatic context window composition for distant speech recognition", "abstract": "Distant speech recognition is being revolutionized by deep learning, that has\ncontributed to significantly outperform previous HMM-GMM systems. A key aspect\nbehind the rapid rise and success of DNNs is their ability to better manage\nlarge time contexts. With this regard, asymmetric context windows that embed\nmore past than future frames have been recently used with feed-forward neural\nnetworks. This context configuration turns out to be useful not only to address\nlow-latency speech recognition, but also to boost the recognition performance\nunder reverberant conditions. This paper investigates on the mechanisms\noccurring inside DNNs, which lead to an effective application of asymmetric\ncontexts.In particular, we propose a novel method for automatic context window\ncomposition based on a gradient analysis. The experiments, performed with\ndifferent acoustic environments, features, DNN architectures, microphone\nsettings, and recognition tasks show that our simple and efficient strategy\nleads to a less redundant frame configuration, which makes DNN training more\neffective in reverberant scenarios.", "published": "2018-05-26 15:36:44", "link": "http://arxiv.org/abs/1805.10498v1", "categories": ["eess.AS", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dodecatonic Cycles and Parsimonious Voice-Leading in the Mystic-Wozzeck\n  Genus", "abstract": "This paper develops a unified voice-leading model for the genus of mystic and\nWozzeck chords. These voice-leading regions are constructed by perturbing\nsymmetric partitions of the octave, and new Neo-Riemannian transformations\nbetween nearly symmetric hexachords are defined. The behaviors of these\ntransformations are shown within visual representations of the voice-leading\nregions for the mystic-Wozzeck genus.", "published": "2018-05-26 05:05:48", "link": "http://arxiv.org/abs/1805.11087v1", "categories": ["math.HO", "cs.SD", "eess.AS", "00A65"], "primary_category": "math.HO"}
