{"title": "Sentiment trading with large language models", "abstract": "We investigate the efficacy of large language models (LLMs) in sentiment\nanalysis of U.S. financial news and their potential in predicting stock market\nreturns. We analyze a dataset comprising 965,375 news articles that span from\nJanuary 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,\nincluding BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary\nmodel, which has been a dominant methodology in the finance literature. The\nstudy documents a significant association between LLM scores and subsequent\ndaily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the\nhighest accuracy in sentiment prediction with an accuracy of 74.4%, slightly\nahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald\ndictionary model demonstrates considerably lower effectiveness with only 50.1%\naccuracy. Regression analyses highlight a robust positive impact of OPT model\nscores on next-day stock returns, with coefficients of 0.274 and 0.254 in\ndifferent model specifications. BERT and FINBERT also exhibit predictive\nrelevance, though to a lesser extent. Notably, we do not observe a significant\nrelationship between the Loughran-McDonald dictionary model scores and stock\nreturns, challenging the efficacy of this traditional method in the current\nfinancial context. In portfolio performance, the long-short OPT strategy excels\nwith a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT\nlong-short strategies. Strategies based on the Loughran-McDonald dictionary\nyield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior\nperformance of advanced LLMs, especially OPT, in financial market prediction\nand portfolio management, marking a significant shift in the landscape of\nfinancial analysis tools with implications to financial regulation and policy\nanalysis.", "published": "2024-12-26 15:01:24", "link": "http://arxiv.org/abs/2412.19245v1", "categories": ["q-fin.CP", "cs.LG", "econ.EM", "q-fin.PM", "q-fin.TR"], "primary_category": "q-fin.CP"}
{"title": "A Malliavin Calculus Approach to Backward Stochastic Volterra Integral Equations", "abstract": "In this paper, we establish existence, uniqueness, and regularity properties\nof the solutions to multi-dimensional backward stochastic Volterra integral\nequations (BSVIEs), whose (possibly random) generator reflects nonlinear\ndependence on both the solution process and the martingale integrand component\nof the adapted solutions, as well as their diagonal processes. The\nwell-posedness results are developed with the use of Malliavin calculus, which\nrenders a novel perspective in tackling with the challenging diagonal processes\nwhile contrasts with the existing methods. We also provide a probabilistic\ninterpretation of the classical solutions to the counterpart semi-linear\npartial differential equations through the explicit adapted solutions of\nBSVIEs. Moreover, we formulate with BSVIEs to explicitly characterize\ndynamically optimal mean-variance portfolios for various stochastic investment\nopportunities, with the myopic investment and intertemporal hedging demands\nbeing identified as two diagonal processes of BSVIE solutions.", "published": "2024-12-26 14:34:26", "link": "http://arxiv.org/abs/2412.19236v2", "categories": ["math.PR", "math.AP", "q-fin.MF", "60H10, 60H20, 35K55"], "primary_category": "math.PR"}
{"title": "A System of BSDEs with Singular Terminal Values Arising in Optimal Liquidation with Regime Switching", "abstract": "We study a stochastic control problem with regime switching arising in an\noptimal liquidation problem with dark pools and multiple regimes. The new\nfeature of this model is that it introduces a system of BSDEs with jumps and\nwith singular terminal values, which appears in literature for the first time.\nThe existence result for this system is obtained. As a result, we solve the\nstochastic control problem with regime switching. More importantly, the\nuniqueness result of this system is also obtained, in contrast to merely\nminimal solutions established in most related literature.", "published": "2024-12-26 05:03:16", "link": "http://arxiv.org/abs/2412.19058v2", "categories": ["q-fin.MF", "math.OC", "math.PR"], "primary_category": "q-fin.MF"}
{"title": "Travelling wave solutions of an equation of Harry Dym type arising in the Black-Scholes framework", "abstract": "The Black-Scholes framework is crucial in pricing a vast number of financial\ninstruments that permeate the complex dynamics of world markets. Associated\nwith this framework, we consider a second-order differential operator $L(x,\n{\\partial_x}) := v^2(x,t) (\\partial_x^2 -\\partial_x)$ that carries a variable\nvolatility term $v(x,t)$ and which is dependent on the underlying log-price $x$\nand a time parameter $t$ motivated by the celebrated Dupire local volatility\nmodel. In this context, we ask and answer the question of whether one can find\na non-linear evolution equation derived from a zero-curvature condition for a\ntime-dependent deformation of the operator $L$. The result is a variant of the\nHarry Dym equation for which we can then find a family of travelling wave\nsolutions. This brings in extensive machinery from soliton theory and\nintegrable systems. As a by-product, it opens up the way to the use of coherent\nstructures in financial-market volatility studies.", "published": "2024-12-26 02:09:47", "link": "http://arxiv.org/abs/2412.19020v1", "categories": ["math.NA", "cs.NA", "q-fin.MF", "35Q51, 35C08, 91G20, 35Q91"], "primary_category": "math.NA"}
{"title": "Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price Forecasting in High-Frequency Trading", "abstract": "High-frequency trading (HFT) has transformed modern financial markets, making\nreliable short-term price forecasting models essential. In this study, we\npresent a novel approach to mid-price forecasting using Level 1 limit order\nbook (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&P 500 index\nduring the period from September to November 2022. Expanding on our previous\nwork with Radial Basis Function Neural Networks (RBFNN), which leveraged\nautomated feature importance techniques based on mean decrease impurity (MDI)\nand gradient descent (GD), we introduce the Adaptive Learning Policy Engine\n(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,\nimmediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to\ndynamically balance exploration and exploitation, outperforming a diverse range\nof highly effective machine learning (ML) and deep learning (DL) models in\nforecasting performance.", "published": "2024-12-26 22:49:53", "link": "http://arxiv.org/abs/2412.19372v2", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability", "abstract": "Large language models (LLMs) often struggle with balanced class accuracy in\ntext classification tasks using in-context learning (ICL), hindering some\npractical uses due to user dissatisfaction or safety risks caused by\nmisclassifications. Retraining LLMs to address root causes in data or model\npriors is neither easy nor cost-effective. This paper delves deeper into the\nclass accuracy imbalance issue, identifying that it arises because certain\nclasses consistently receive disproportionately high ICL probabilities, causing\nunder-prediction and lower accuracy for others. More importantly, probability\nranges affect the imbalance differently, allowing for precise, range-specific\ncorrections. We introduce FuRud (Fuzzy Rule Optimization-based Debiasing), a\nmethod for sample-level class probability correction. FuRud tackles\ninterpretability challenges by determining why certain classes need corrections\nand tailoring adjustments for each instance's class probabilities which is\npowered by fuzzy sets with triangular membership functions, transforming a\nclass probability based on the range it belongs to. By solving a nonlinear\ninteger programming problem with a labeled set of ICL class probabilities to\nminimize class accuracy bias (COBias) and maximize overall accuracy, each class\nselects an optimal correction function from 19 triangular membership functions\nwithout updating an LLM, and the selected functions correct test instances at\ninference. Across seven benchmark datasets, FuRud reduces COBias by over half\n(56%) and improves overall accuracy by 21% relatively, outperforming\nstate-of-the-art debiasing methods.", "published": "2024-12-26 01:56:42", "link": "http://arxiv.org/abs/2412.19018v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Demographic Portability of Deep NLP-Based Depression Models", "abstract": "Deep learning models are rapidly gaining interest for real-world applications\nin behavioral health. An important gap in current literature is how well such\nmodels generalize over different populations. We study Natural Language\nProcessing (NLP) based models to explore portability over two different corpora\nhighly mismatched in age. The first and larger corpus contains younger\nspeakers. It is used to train an NLP model to predict depression. When testing\non unseen speakers from the same age distribution, this model performs at\nAUC=0.82. We then test this model on the second corpus, which comprises seniors\nfrom a retirement community. Despite the large demographic differences in the\ntwo corpora, we saw only modest degradation in performance for the\nsenior-corpus data, achieving AUC=0.76. Interestingly, in the senior\npopulation, we find AUC=0.81 for the subset of patients whose health state is\nconsistent over time. Implications for demographic portability of speech-based\napplications are discussed.", "published": "2024-12-26 05:54:24", "link": "http://arxiv.org/abs/2412.19070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing LLM detection in the ALTA 2024 Shared Task: Techniques and\n  Analysis", "abstract": "The recent proliferation of AI-generated content has prompted significant\ninterest in developing reliable detection methods. This study explores\ntechniques for identifying AI-generated text through sentence-level evaluation\nwithin hybrid articles. Our findings indicate that ChatGPT-3.5 Turbo exhibits\ndistinct, repetitive probability patterns that enable consistent in-domain\ndetection. Empirical tests show that minor textual modifications, such as\nrewording, have minimal impact on detection accuracy. These results provide\nvaluable insights for advancing AI detection methodologies, offering a pathway\ntoward robust solutions to address the complexities of synthetic text\nidentification.", "published": "2024-12-26 06:23:53", "link": "http://arxiv.org/abs/2412.19076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"I've Heard of You!\": Generate Spoken Named Entity Recognition Data for\n  Unseen Entities", "abstract": "Spoken named entity recognition (NER) aims to identify named entities from\nspeech, playing an important role in speech processing. New named entities\nappear every day, however, annotating their Spoken NER data is costly. In this\npaper, we demonstrate that existing Spoken NER systems perform poorly when\ndealing with previously unseen named entities. To tackle this challenge, we\npropose a method for generating Spoken NER data based on a named entity\ndictionary (NED) to reduce costs. Specifically, we first use a large language\nmodel (LLM) to generate sentences from the sampled named entities and then use\na text-to-speech (TTS) system to generate the speech. Furthermore, we introduce\na noise metric to filter out noisy data. To evaluate our approach, we release a\nnovel Spoken NER benchmark along with a corresponding NED containing 8,853\nentities. Experiment results show that our method achieves state-of-the-art\n(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully\nzero-shot settings. Our data will be available at\nhttps://github.com/DeepLearnXMU/HeardU.", "published": "2024-12-26 07:43:18", "link": "http://arxiv.org/abs/2412.19102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GFG -- Gender-Fair Generation: A CALAMITA Challenge", "abstract": "Gender-fair language aims at promoting gender equality by using terms and\nexpressions that include all identities and avoid reinforcing gender\nstereotypes. Implementing gender-fair strategies is particularly challenging in\nheavily gender-marked languages, such as Italian. To address this, the\nGender-Fair Generation challenge intends to help shift toward gender-fair\nlanguage in written communication. The challenge, designed to assess and\nmonitor the recognition and generation of gender-fair language in both mono-\nand cross-lingual scenarios, includes three tasks: (1) the detection of\ngendered expressions in Italian sentences, (2) the reformulation of gendered\nexpressions into gender-fair alternatives, and (3) the generation of\ngender-fair language in automatic translation from English to Italian. The\nchallenge relies on three different annotated datasets: the GFL-it corpus,\nwhich contains Italian texts extracted from administrative documents provided\nby the University of Brescia; GeNTE, a bilingual test set for gender-neutral\nrewriting and translation built upon a subset of the Europarl dataset; and\nNeo-GATE, a bilingual test set designed to assess the use of non-binary\nneomorphemes in Italian for both fair formulation and translation tasks.\nFinally, each task is evaluated with specific metrics: average of F1-score\nobtained by means of BERTScore computed on each entry of the datasets for task\n1, an accuracy measured with a gender-neutral classifier, and a\ncoverage-weighted accuracy for tasks 2 and 3.", "published": "2024-12-26 10:58:40", "link": "http://arxiv.org/abs/2412.19168v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Skill Adaptation for Large Language Models", "abstract": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework\nto adapt novel and complex skills to Large Language Models (LLMs). Compared\nwith previous work which learns from human-curated and static data in random\norders, we propose to first automatically generate and organize the training\ndata by mimicking the learning pathways of human and then dynamically tailor\nthe training data based on the training dynamics. Specifically, inspired by the\nlearning structures and teaching strategies in the human education system, we\nfirst construct a skill graph by decomposing complex skills into sub-skills and\narranging them based on their dependencies in human syllables. For every skill,\nwe utilize LLMs to generate both textbook-like data which contains detailed\ndescriptions of skills for pre-training and exercise-like data which targets at\nexplicitly utilizing the skills to solve problems for instruction-tuning.\nFurthermore, during the instruction-tuning, we dynamically update the training\ndata which down-weight easy-to-learn examples, generate more complex examples,\nand filter out data with errors. Experiments on large language models such as\nLLAMA and Mistral demonstrate the effectiveness of our proposed methods in\nadapting math reasoning skills and social study skills.", "published": "2024-12-26 22:04:23", "link": "http://arxiv.org/abs/2412.19361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Speech and Natural Language Processing Models for Depression\n  Screening", "abstract": "Depression is a global health concern with a critical need for increased\npatient screening. Speech technology offers advantages for remote screening but\nmust perform robustly across patients. We have described two deep learning\nmodels developed for this purpose. One model is based on acoustics; the other\nis based on natural language processing. Both models employ transfer learning.\nData from a depression-labeled corpus in which 11,000 unique users interacted\nwith a human-machine application using conversational speech is used. Results\non binary depression classification have shown that both models perform at or\nabove AUC=0.80 on unseen data with no speaker overlap. Performance is further\nanalyzed as a function of test subset characteristics, finding that the models\nare generally robust over speaker and session variables. We conclude that\nmodels based on these approaches offer promise for generalized automated\ndepression screening.", "published": "2024-12-26 06:05:52", "link": "http://arxiv.org/abs/2412.19072v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Referencing Where to Focus: Improving VisualGrounding with Referential\n  Query", "abstract": "Visual Grounding aims to localize the referring object in an image given a\nnatural language expression. Recent advancements in DETR-based visual grounding\nmethods have attracted considerable attention, as they directly predict the\ncoordinates of the target object without relying on additional efforts, such as\npre-generated proposal candidates or pre-defined anchor boxes. However,\nexisting research primarily focuses on designing stronger multi-modal decoder,\nwhich typically generates learnable queries by random initialization or by\nusing linguistic embeddings. This vanilla query generation approach inevitably\nincreases the learning difficulty for the model, as it does not involve any\ntarget-related information at the beginning of decoding. Furthermore, they only\nuse the deepest image feature during the query learning process, overlooking\nthe importance of features from other levels. To address these issues, we\npropose a novel approach, called RefFormer. It consists of the query adaption\nmodule that can be seamlessly integrated into CLIP and generate the referential\nquery to provide the prior context for decoder, along with a task-specific\ndecoder. By incorporating the referential query into the decoder, we can\neffectively mitigate the learning difficulty of the decoder, and accurately\nconcentrate on the target object. Additionally, our proposed query adaption\nmodule can also act as an adapter, preserving the rich knowledge within CLIP\nwithout the need to tune the parameters of the backbone network. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmethod, outperforming state-of-the-art approaches on five visual grounding\nbenchmarks.", "published": "2024-12-26 10:19:20", "link": "http://arxiv.org/abs/2412.19155v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for\n  Enhanced Image-Text Matching", "abstract": "With the rapid development of multimodal learning, the image-text matching\ntask, as a bridge connecting vision and language, has become increasingly\nimportant. Based on existing research, this study proposes an innovative visual\nsemantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic\nEmbedding (MH-CVSE). This model introduces a multi-head self-attention\nmechanism based on the consensus-aware visual semantic embedding model (CVSE)\nto capture information in multiple subspaces in parallel, significantly\nenhancing the model's ability to understand and represent the complex\nrelationship between images and texts. In addition, we adopt a parameterized\nfeature fusion strategy to flexibly integrate feature information at different\nlevels, further improving the model's expressive power. In terms of loss\nfunction design, the MH-CVSE model adopts a dynamic weight adjustment strategy\nto dynamically adjust the weight according to the loss value itself, so that\nthe model can better balance the contribution of different loss terms during\ntraining. At the same time, we introduce a cosine annealing learning rate\nstrategy to help the model converge more stably in the later stages of\ntraining. Extensive experimental verification on the Flickr30k dataset shows\nthat the MH-CVSE model achieves better performance than previous methods in\nboth bidirectional image and text retrieval tasks, fully demonstrating its\neffectiveness and superiority.", "published": "2024-12-26 11:46:22", "link": "http://arxiv.org/abs/2412.19184v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multi-matrix Factorization Attention", "abstract": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.", "published": "2024-12-26 15:45:45", "link": "http://arxiv.org/abs/2412.19255v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Semi-Supervised Learning from Small Annotated Data and Large Unlabeled\n  Data for Fine-grained PICO Entity Recognition", "abstract": "Objective: Extracting PICO elements -- Participants, Intervention,\nComparison, and Outcomes -- from clinical trial literature is essential for\nclinical evidence retrieval, appraisal, and synthesis. Existing approaches do\nnot distinguish the attributes of PICO entities. This study aims to develop a\nnamed entity recognition (NER) model to extract PICO entities with fine\ngranularities.\n  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions\nfrom 4 public datasets, we developed a semi-supervised method to facilitate the\ntraining of a NER model, FinePICO, by combining limited annotated data of PICO\nentities and abundant unlabeled data. For evaluation, we divided the entire\ndataset into two subsets: a smaller group with annotations and a larger group\nwithout annotations. We then established the theoretical lower and upper\nperformance bounds based on the performance of supervised learning models\ntrained solely on the small, annotated subset and on the entire set with\ncomplete annotations, respectively. Finally, we evaluated FinePICO on both the\nsmaller annotated subset and the larger, initially unannotated subset. We\nmeasured the performance of FinePICO using precision, recall, and F1.\n  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,\nrespectively, using a small set of annotated samples, outperforming the\nbaseline model (F1: 0.437) by more than 16\\%. The model demonstrates\ngeneralizability to a different PICO framework and to another corpus, which\nconsistently outperforms the benchmark in diverse experimental settings\n(p-value \\textless0.001).\n  Conclusion: This study contributes a generalizable and effective\nsemi-supervised approach to named entity recognition leveraging large unlabeled\ndata together with small, annotated data. It also initially supports\nfine-grained PICO extraction.", "published": "2024-12-26 20:24:35", "link": "http://arxiv.org/abs/2412.19346v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Indonesian-English Code-Switching Speech Synthesizer Utilizing\n  Multilingual STEN-TTS and Bert LID", "abstract": "Multilingual text-to-speech systems convert text into speech across multiple\nlanguages. In many cases, text sentences may contain segments in different\nlanguages, a phenomenon known as code-switching. This is particularly common in\nIndonesia, especially between Indonesian and English. Despite its significance,\nno research has yet developed a multilingual TTS system capable of handling\ncode-switching between these two languages. This study addresses\nIndonesian-English code-switching in STEN-TTS. Key modifications include adding\na language identification component to the text-to-phoneme conversion using\nfinetuned BERT for per-word language identification, as well as removing\nlanguage embedding from the base model. Experimental results demonstrate that\nthe code-switching model achieves superior naturalness and improved speech\nintelligibility compared to the Indonesian and English baseline STEN-TTS\nmodels.", "published": "2024-12-26 03:37:40", "link": "http://arxiv.org/abs/2412.19043v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MoPD: Mixture-of-Prompts Distillation for Vision-Language Models", "abstract": "Soft prompt learning methods are effective for adapting vision-language\nmodels (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a\ntendency of existing methods that they overfit seen classes and exhibit\ndegraded performance on unseen classes. This limitation is due to the inherent\nbias in the training data towards the seen classes. To address this issue, we\npropose a novel soft prompt learning method, named Mixture-of-Prompts\nDistillation (MoPD), which can effectively transfer useful knowledge from hard\nprompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft\nprompt (a.k.a. student prompt), thereby enhancing the generalization ability of\nsoft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a\ngating network that learns to select hard prompts used for prompt distillation.\nExtensive experiments demonstrate that the proposed MoPD method outperforms\nstate-of-the-art baselines especially on on unseen classes.", "published": "2024-12-26 06:57:04", "link": "http://arxiv.org/abs/2412.19087v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing\n  Values", "abstract": "Missing value is a critical issue in data science, significantly impacting\nthe reliability of analyses and predictions. Missing value imputation (MVI) is\na longstanding problem because it highly relies on domain knowledge. Large\nlanguage models (LLMs) have emerged as a promising tool for data cleaning,\nincluding MVI for tabular data, offering advanced capabilities for\nunderstanding and generating content. However, despite their promise, existing\nLLM techniques such as in-context learning and Chain-of-Thought (CoT) often\nfall short in guiding LLMs to perform complex reasoning for MVI, particularly\nwhen imputing derived missing values, which require mathematical formulas and\ndata relationships across rows and columns. This gap underscores the need for\nfurther advancements in LLM methodologies to enhance their reasoning\ncapabilities for more reliable imputation outcomes. To fill this gap, we\npropose SketchFill, a novel sketch-based method to guide LLMs in generating\naccurate formulas to impute missing numerical values. Our experimental results\ndemonstrate that SketchFill significantly outperforms state-of-the-art methods,\nachieving 56.2% higher accuracy than CoT-based methods and 78.8% higher\naccuracy than MetaGPT. This sets a new standard for automated data cleaning and\nadvances the field of MVI for numerical values.", "published": "2024-12-26 08:13:34", "link": "http://arxiv.org/abs/2412.19113v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SILC-EFSA: Self-aware In-context Learning Correction for Entity-level\n  Financial Sentiment Analysis", "abstract": "In recent years, fine-grained sentiment analysis in finance has gained\nsignificant attention, but the scarcity of entity-level datasets remains a key\nchallenge. To address this, we have constructed the largest English and Chinese\nfinancial entity-level sentiment analysis datasets to date. Building on this\nfoundation, we propose a novel two-stage sentiment analysis approach called\nSelf-aware In-context Learning Correction (SILC). The first stage involves\nfine-tuning a base large language model to generate pseudo-labeled data\nspecific to our task. In the second stage, we train a correction model using a\nGNN-based example retriever, which is informed by the pseudo-labeled data. This\ntwo-stage strategy has allowed us to achieve state-of-the-art performance on\nthe newly constructed datasets, advancing the field of financial sentiment\nanalysis. In a case study, we demonstrate the enhanced practical utility of our\ndata and methods in monitoring the cryptocurrency market. Our datasets and code\nare available at https://github.com/NLP-Bin/SILC-EFSA.", "published": "2024-12-26 09:53:01", "link": "http://arxiv.org/abs/2412.19140v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes", "abstract": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.", "published": "2024-12-26 15:54:10", "link": "http://arxiv.org/abs/2412.19260v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing Multi-Stage Language Models for Effective Text Retrieval", "abstract": "Efficient text retrieval is critical for applications such as legal document\nanalysis, particularly in specialized contexts like Japanese legal systems.\nExisting retrieval methods often underperform in such domain-specific\nscenarios, necessitating tailored approaches. In this paper, we introduce a\nnovel two-phase text retrieval pipeline optimized for Japanese legal datasets.\nOur method leverages advanced language models to achieve state-of-the-art\nperformance, significantly improving retrieval efficiency and accuracy. To\nfurther enhance robustness and adaptability, we incorporate an ensemble model\nthat integrates multiple retrieval strategies, resulting in superior outcomes\nacross diverse tasks. Extensive experiments validate the effectiveness of our\napproach, demonstrating strong performance on both Japanese legal datasets and\nwidely recognized benchmarks like MS-MARCO. Our work establishes new standards\nfor text retrieval in domain-specific and general contexts, providing a\ncomprehensive solution for addressing complex queries in legal and multilingual\nenvironments.", "published": "2024-12-26 16:05:19", "link": "http://arxiv.org/abs/2412.19265v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image\n  Captioning", "abstract": "Recent lightweight image captioning models using retrieved data mainly focus\non text prompts. However, previous works only utilize the retrieved text as\ntext prompts, and the visual information relies only on the CLIP visual\nembedding. Because of this issue, there is a limitation that the image\ndescriptions inherent in the prompt are not sufficiently reflected in the\nvisual embedding space. To tackle this issue, we propose ViPCap, a novel\nretrieval text-based visual prompt for lightweight image captioning. ViPCap\nleverages the retrieved text with image information as visual prompts to\nenhance the ability of the model to capture relevant visual information. By\nmapping text prompts into the CLIP space and generating multiple randomized\nGaussian distributions, our method leverages sampling to explore randomly\naugmented distributions and effectively retrieves the semantic features that\ncontain image information. These retrieved features are integrated into the\nimage and designated as the visual prompt, leading to performance improvements\non the datasets such as COCO, Flickr30k, and NoCaps. Experimental results\ndemonstrate that ViPCap significantly outperforms prior lightweight captioning\nmodels in efficiency and effectiveness, demonstrating the potential for a\nplug-and-play solution. The source code is available at\nhttps://github.com/taewhankim/VIPCAP.", "published": "2024-12-26 17:29:38", "link": "http://arxiv.org/abs/2412.19289v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "On the Expressiveness and Length Generalization of Selective State-Space\n  Models on Regular Languages", "abstract": "Selective state-space models (SSMs) are an emerging alternative to the\nTransformer, offering the unique advantage of parallel training and sequential\ninference. Although these models have shown promising performance on a variety\nof tasks, their formal expressiveness and length generalization properties\nremain underexplored. In this work, we provide insight into the workings of\nselective SSMs by analyzing their expressiveness and length generalization\nperformance on regular language tasks, i.e., finite-state automaton (FSA)\nemulation. We address certain limitations of modern SSM-based architectures by\nintroducing the Selective Dense State-Space Model (SD-SSM), the first selective\nSSM that exhibits perfect length generalization on a set of various regular\nlanguage tasks using a single layer. It utilizes a dictionary of dense\ntransition matrices, a softmax selection mechanism that creates a convex\ncombination of dictionary matrices at each time step, and a readout consisting\nof layer normalization followed by a linear map. We then proceed to evaluate\nvariants of diagonal selective SSMs by considering their empirical performance\non commutative and non-commutative automata. We explain the experimental\nresults with theoretical considerations. Our code is available at\nhttps://github.com/IBM/selective-dense-state-space-model.", "published": "2024-12-26 20:53:04", "link": "http://arxiv.org/abs/2412.19350v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ETTA: Elucidating the Design Space of Text-to-Audio Models", "abstract": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,\nenabling users to enrich their creative workflows with synthetic audio\ngenerated from natural language prompts. Despite this progress, the effects of\ndata, model architecture, training objective functions, and sampling strategies\non target benchmarks are not well understood. With the purpose of providing a\nholistic understanding of the design space of TTA models, we set up a\nlarge-scale empirical experiment focused on diffusion and flow matching models.\nOur contributions include: 1) AF-Synthetic, a large dataset of high quality\nsynthetic captions obtained from an audio understanding model; 2) a systematic\ncomparison of different architectural, training, and inference design choices\nfor TTA models; 3) an analysis of sampling methods and their Pareto curves with\nrespect to generation quality and inference speed. We leverage the knowledge\nobtained from this extensive analysis to propose our best model dubbed\nElucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,\nETTA provides improvements over the baselines trained on publicly available\ndata, while being competitive with models trained on proprietary data. Finally,\nwe show ETTA's improved ability to generate creative audio following complex\nand imaginative captions -- a task that is more challenging than current\nbenchmarks.", "published": "2024-12-26 21:13:12", "link": "http://arxiv.org/abs/2412.19351v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal\n  Video-Text Retrieval", "abstract": "Cross-modal (e.g. image-text, video-text) retrieval is an important task in\ninformation retrieval and multimodal vision-language understanding field.\nTemporal understanding makes video-text retrieval more challenging than\nimage-text retrieval. However, we find that the widely used video-text\nbenchmarks have shortcomings in comprehensively assessing abilities of models,\nespecially in temporal understanding, causing large-scale image-text\npre-trained models can already achieve comparable zero-shot performance with\nvideo-text pre-trained models. In this paper, we introduce RTime, a novel\ntemporal-emphasized video-text retrieval dataset. We first obtain videos of\nactions or events with significant temporality, and then reverse these videos\nto create harder negative samples. We then recruit annotators to judge the\nsignificance and reversibility of candidate videos, and write captions for\nqualified videos. We further adopt GPT-4 to extend more captions based on\nhuman-written captions. Our RTime dataset currently consists of 21k videos with\n10 captions per video, totalling about 122 hours. Based on RTime, we propose\nthree retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We\nfurther enhance the use of harder-negatives in model training, and benchmark a\nvariety of video-text models on RTime. Extensive experiment analysis proves\nthat RTime indeed poses new and higher challenges to video-text retrieval. We\nrelease our RTime\ndataset\\footnote{\\url{https://github.com/qyr0403/Reversed-in-Time}} to further\nadvance video-text retrieval and multimodal understanding research.", "published": "2024-12-26 11:32:00", "link": "http://arxiv.org/abs/2412.19178v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards a Single ASR Model That Generalizes to Disordered Speech", "abstract": "This study investigates the impact of integrating a dataset of disordered\nspeech recordings ($\\sim$1,000 hours) into the fine-tuning of a near\nstate-of-the-art ASR baseline system. Contrary to what one might expect,\ndespite the data being less than 1% of the training data of the ASR system, we\nfind a considerable improvement in disordered speech recognition accuracy.\nSpecifically, we observe a 33% improvement on prompted speech, and a 26%\nimprovement on a newly gathered spontaneous, conversational dataset of\ndisordered speech. Importantly, there is no significant performance decline on\nstandard speech recognition benchmarks. Further, we observe that the proposed\ntuning strategy helps close the gap between the baseline system and\npersonalized models by 64% highlighting the significant progress as well as the\nroom for improvement. Given the substantial benefits of our findings, this\nexperiment suggests that from a fairness perspective, incorporating a small\nfraction of high quality disordered speech data in a training recipe is an easy\nstep that could be done to make speech technology more accessible for users\nwith speech disabilities.", "published": "2024-12-26 18:39:15", "link": "http://arxiv.org/abs/2412.19315v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Enhancing Audiovisual Speech Recognition through Bifocal Preference\n  Optimization", "abstract": "Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech\nrecognition accuracy by leveraging visual signals. It is particularly\nchallenging in unconstrained real-world scenarios across various domains due to\nnoisy acoustic environments, spontaneous speech, and the uncertain use of\nvisual information. Most previous works fine-tune audio-only ASR models on\naudiovisual datasets, optimizing them for conventional ASR objectives. However,\nthey often neglect visual features and common errors in unconstrained video\nscenarios. In this paper, we propose using a preference optimization strategy\nto improve speech recognition accuracy for real-world videos. First, we create\npreference data via simulating common errors that occurred in AV-ASR from two\nfocals: manipulating the audio or vision input and rewriting the output\ntranscript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization\nmethod to improve AV-ASR models by leveraging both input-side and output-side\npreference. Extensive experiments demonstrate that our approach significantly\nimproves speech recognition accuracy across various domains, outperforming\nprevious state-of-the-art models on real-world video speech recognition.", "published": "2024-12-26 00:26:45", "link": "http://arxiv.org/abs/2412.19005v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Attacking Voice Anonymization Systems with Augmented Feature and Speaker\n  Identity Difference", "abstract": "This study focuses on the First VoicePrivacy Attacker Challenge within the\nICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker\nverification systems capable of determining whether two anonymized speech\nsignals are from the same speaker. However, differences between feature\ndistributions of original and anonymized speech complicate this task. To\naddress this challenge, we propose an attacker system that combines Data\nAugmentation enhanced feature representation and Speaker Identity Difference\nenhanced classifier to improve verification performance, termed DA-SID.\nSpecifically, data augmentation strategies (i.e., data fusion and SpecAugment)\nare utilized to mitigate feature distribution gaps, while probabilistic linear\ndiscriminant analysis (PLDA) is employed to further enhance speaker identity\ndifference. Our system significantly outperforms the baseline, demonstrating\nexceptional effectiveness and robustness against various voice anonymization\nsystems, ultimately securing a top-5 ranking in the challenge.", "published": "2024-12-26 05:52:44", "link": "http://arxiv.org/abs/2412.19068v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for\n  Acoustic Traffic Monitoring", "abstract": "Microphone array techniques are widely used in sound source localization and\nsmart city acoustic-based traffic monitoring, but these applications face\nsignificant challenges due to the scarcity of labeled real-world traffic audio\ndata and the complexity and diversity of application scenarios. The DCASE\nChallenge's Task 10 focuses on using multi-channel audio signals to count\nvehicles (cars or commercial vehicles) and identify their directions\n(left-to-right or vice versa). In this paper, we propose a graph-enhanced\ndual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,\nwhich simultaneously considers vehicle type and direction to improve detection.\nWe propose a graph-enhanced dual-stream feature fusion strategy which consists\nof a vehicle type feature extraction (VTFE) branch, a vehicle direction feature\nextraction (VDFE) branch, and a frame-level feature fusion module to combine\nthe type and direction feature for enhanced performance. A pre-trained model\n(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the\ntype features, followed by a graph attention mechanism to exploit temporal\nrelationships and highlight important audio events within these features. The\nframe-level fusion of direction and type features enables fine-grained feature\nrepresentation, resulting in better detection performance. Experiments\ndemonstrate the effectiveness of our proposed method. GEDF-Net is our\nsubmission that achieved 1st place in the DCASE 2024 Challenge Task 10.", "published": "2024-12-26 06:28:42", "link": "http://arxiv.org/abs/2412.19078v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces\n  Mechanism for Monaural Speech Enhancement", "abstract": "Although the complex spectrum-based speech enhancement(SE) methods have\nachieved significant performance, coupling amplitude and phase can lead to a\ncompensation effect, where amplitude information is sacrificed to compensate\nfor the phase that is harmful to SE. In addition, to further improve the\nperformance of SE, many modules are stacked onto SE, resulting in increased\nmodel complexity that limits the application of SE. To address these problems,\nwe proposed a dual-path network based on compressed frequency using Mamba.\nFirst, we extract amplitude and phase information through parallel dual\nbranches. This approach leverages structured complex spectra to implicitly\ncapture phase information and solves the compensation effect by decoupling\namplitude and phase, and the network incorporates an interaction module to\nsuppress unnecessary parts and recover missing components from the other\nbranch. Second, to reduce network complexity, the network introduces a\nband-split strategy to compress the frequency dimension. To further reduce\ncomplexity while maintaining good performance, we designed a Mamba-based module\nthat models the time and frequency dimensions under linear complexity. Finally,\ncompared to baselines, our model achieves an average 8.3 times reduction in\ncomputational complexity while maintaining superior performance. Furthermore,\nit achieves a 25 times reduction in complexity compared to transformer-based\nmodels.", "published": "2024-12-26 07:42:07", "link": "http://arxiv.org/abs/2412.19099v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Causal Speech Enhancement with Predicting Semantics based on Quantized\n  Self-supervised Learning Features", "abstract": "Real-time speech enhancement (SE) is essential to online speech\ncommunication. Causal SE models use only the previous context while predicting\nfuture information, such as phoneme continuation, may help performing causal\nSE. The phonetic information is often represented by quantizing latent features\nof self-supervised learning (SSL) models. This work is the first to incorporate\nSSL features with causality into an SE model. The causal SSL features are\nencoded and combined with spectrogram features using feature-wise linear\nmodulation to estimate a mask for enhancing the noisy input speech.\nSimultaneously, we quantize the causal SSL features using vector quantization\nto represent phonetic characteristics as semantic tokens. The model not only\nencodes SSL features but also predicts the future semantic tokens in multi-task\nlearning (MTL). The experimental results using VoiceBank + DEMAND dataset show\nthat our proposed method achieves 2.88 in PESQ, especially with semantic\nprediction MTL, in which we confirm that the semantic prediction played an\nimportant role in causal SE.", "published": "2024-12-26 15:08:36", "link": "http://arxiv.org/abs/2412.19248v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware\n  Speech Synthesis", "abstract": "We present VoiceDiT, a multi-modal generative model for producing\nenvironment-aware speech and audio from text and visual prompts. While aligning\nspeech with text is crucial for intelligible speech, achieving this alignment\nin noisy conditions remains a significant and underexplored challenge in the\nfield. To address this, we present a novel audio generation pipeline named\nVoiceDiT. This pipeline includes three key components: (1) the creation of a\nlarge-scale synthetic speech dataset for pre-training and a refined real-world\nspeech dataset for fine-tuning, (2) the Dual-DiT, a model designed to\nefficiently preserve aligned speech information while accurately reflecting\nenvironmental conditions, and (3) a diffusion-based Image-to-Audio Translator\nthat allows the model to bridge the gap between audio and image, facilitating\nthe generation of environmental sound that aligns with the multi-modal prompts.\nExtensive experimental results demonstrate that VoiceDiT outperforms previous\nmodels on real-world datasets, showcasing significant improvements in both\naudio quality and modality integration.", "published": "2024-12-26 15:52:58", "link": "http://arxiv.org/abs/2412.19259v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CoheDancers: Enhancing Interactive Group Dance Generation through\n  Music-Driven Coherence Decomposition", "abstract": "Dance generation is crucial and challenging, particularly in domains like\ndance performance and virtual gaming. In the current body of literature, most\nmethodologies focus on Solo Music2Dance. While there are efforts directed\ntowards Group Music2Dance, these often suffer from a lack of coherence,\nresulting in aesthetically poor dance performances. Thus, we introduce\nCoheDancers, a novel framework for Music-Driven Interactive Group Dance\nGeneration. CoheDancers aims to enhance group dance generation coherence by\ndecomposing it into three key aspects: synchronization, naturalness, and\nfluidity. Correspondingly, we develop a Cycle Consistency based Dance\nSynchronization strategy to foster music-dance correspondences, an\nAuto-Regressive-based Exposure Bias Correction strategy to enhance the fluidity\nof the generated dances, and an Adversarial Training Strategy to augment the\nnaturalness of the group dance output. Collectively, these strategies enable\nCohdeDancers to produce highly coherent group dances with superior quality.\nFurthermore, to establish better benchmarks for Group Music2Dance, we construct\nthe most diverse and comprehensive open-source dataset to date, I-Dancers,\nfeaturing rich dancer interactions, and create comprehensive evaluation\nmetrics. Experimental evaluations on I-Dancers and other extant datasets\nsubstantiate that CoheDancers achieves unprecedented state-of-the-art\nperformance. Code will be released.", "published": "2024-12-26 08:47:13", "link": "http://arxiv.org/abs/2412.19123v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Personalized Dynamic Music Emotion Recognition with Dual-Scale\n  Attention-Based Meta-Learning", "abstract": "Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of\ndifferent moments in music, playing a crucial role in music information\nretrieval. The existing DMER methods struggle to capture long-term dependencies\nwhen dealing with sequence data, which limits their performance. Furthermore,\nthese methods often overlook the influence of individual differences on emotion\nperception, even though everyone has their own personalized emotional\nperception in the real world. Motivated by these issues, we explore more\neffective sequence processing methods and introduce the Personalized DMER\n(PDMER) problem, which requires models to predict emotions that align with\npersonalized perception. Specifically, we propose a Dual-Scale Attention-Based\nMeta-Learning (DSAML) method. This method fuses features from a dual-scale\nfeature extractor and captures both short and long-term dependencies using a\ndual-scale attention transformer, improving the performance in traditional\nDMER. To achieve PDMER, we design a novel task construction strategy that\ndivides tasks by annotators. Samples in a task are annotated by the same\nannotator, ensuring consistent perception. Leveraging this strategy alongside\nmeta-learning, DSAML can predict personalized perception of emotions with just\none personalized annotation sample. Our objective and subjective experiments\ndemonstrate that our method can achieve state-of-the-art performance in both\ntraditional DMER and PDMER.", "published": "2024-12-26 12:47:35", "link": "http://arxiv.org/abs/2412.19200v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Generalization for AI-Synthesized Voice Detection", "abstract": "AI-synthesized voice technology has the potential to create realistic human\nvoices for beneficial applications, but it can also be misused for malicious\npurposes. While existing AI-synthesized voice detection models excel in\nintra-domain evaluation, they face challenges in generalizing across different\ndomains, potentially becoming obsolete as new voice generators emerge. Current\nsolutions use diverse data and advanced machine learning techniques (e.g.,\ndomain-invariant representation, self-supervised learning), but are limited by\npredefined vocoders and sensitivity to factors like background noise and\nspeaker identity. In this work, we introduce an innovative disentanglement\nframework aimed at extracting domain-agnostic artifact features related to\nvocoders. Utilizing these features, we enhance model learning in a flat loss\nlandscape, enabling escape from suboptimal solutions and improving\ngeneralization. Extensive experiments on benchmarks show our approach\noutperforms state-of-the-art methods, achieving up to 5.12% improvement in the\nequal error rate metric in intra-domain and 7.59% in cross-domain evaluations.", "published": "2024-12-26 16:45:20", "link": "http://arxiv.org/abs/2412.19279v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
