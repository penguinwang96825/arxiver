{"title": "Some Stylometric Remarks on Ovid's Heroides and the Epistula Sapphus", "abstract": "This article aims to contribute to two well-worn areas of debate in classical\nLatin philology, relating to Ovid's Heroides. The first is the question of the\nauthenticity (and, to a lesser extent the correct position) of the letter\nplaced fifteenth by almost every editor -- the so-called Epistula Sapphus\n(henceforth ES). The secondary question, although perhaps now less fervently\ndebated, is the authenticity of the 'Double Heroides', placed by those who\naccept them as letters 16-21. I employ a variety of methods drawn from the\ndomain of computational stylometry to consider the poetics and the\nlexico-grammatical features of these elegiac poems in the broader context of a\ncorpus of 'shorter' (from 20 to 546 lines) elegiac works from five authors (266\npoems in all) comprising more or less all of the non-fragmentary classical\ncorpus. Based on a variety of techniques, every measure gives clear indication\nthat the poetic style of the Heroides is Ovidian, but distinctive; they can be\naccurately isolated from Ovid more broadly. The Single and Double Heroides\nsplit into two clear groups, with the ES grouped consistently with the single\nletters. Furthermore, by comparing the style of the letters with the 'early'\n(although there are complications in this label) works of the Amores and the\nlate works of the Ex Ponto, the evidence supports sequential composition --\nmeaning that the ES is correctly placed -- and, further, supports the growing\nconsensus that the double letters were composed significantly later, in exile.", "published": "2022-02-24 02:03:51", "link": "http://arxiv.org/abs/2202.11864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using calibrator to improve robustness in Machine Reading Comprehension", "abstract": "Machine Reading Comprehension(MRC) has achieved a remarkable result since\nsome powerful models, such as BERT, are proposed. However, these models are not\nrobust enough and vulnerable to adversarial input perturbation and\ngeneralization examples. Some works tried to improve the performance on\nspecific types of data by adding some related examples into training data while\nit leads to degradation on the original dataset, because the shift of data\ndistribution makes the answer ranking based on the softmax probability of model\nunreliable. In this paper, we propose a method to improve the robustness by\nusing a calibrator as the post-hoc reranker, which is implemented based on\nXGBoost model. The calibrator combines both manual features and representation\nlearning features to rerank candidate results. Experimental results on\nadversarial datasets show that our model can achieve performance improvement by\nmore than 10\\% and also make improvement on the original and generalization\ndatasets.", "published": "2022-02-24 02:16:42", "link": "http://arxiv.org/abs/2202.11865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Welcome to the Modern World of Pronouns: Identity-Inclusive Natural\n  Language Processing beyond Gender", "abstract": "The world of pronouns is changing. From a closed class of words with few\nmembers to a much more open set of terms to reflect identities. However,\nNatural Language Processing (NLP) is barely reflecting this linguistic shift,\neven though recent work outlined the harms of gender-exclusive language\ntechnology. Particularly problematic is the current modeling 3rd person\npronouns, as it largely ignores various phenomena like neopronouns, i.e.,\npronoun sets that are novel and not (yet) widely established. This omission\ncontributes to the discrimination of marginalized and underrepresented groups,\ne.g., non-binary individuals. However, other identity-expression phenomena\nbeyond gender are also ignored by current NLP technology. In this paper, we\nprovide an overview of 3rd person pronoun issues for NLP. Based on our\nobservations and ethical considerations, we define a series of desiderata for\nmodeling pronouns in language technology. We evaluate existing and novel\nmodeling approaches w.r.t. these desiderata qualitatively, and quantify the\nimpact of a more discrimination-free approach on established benchmark data.", "published": "2022-02-24 06:42:11", "link": "http://arxiv.org/abs/2202.11923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization Requires Compositional Parsers", "abstract": "A rapidly growing body of research on compositional generalization\ninvestigates the ability of a semantic parser to dynamically recombine\nlinguistic elements seen in training into unseen sequences. We present a\nsystematic comparison of sequence-to-sequence models and models guided by\ncompositional principles on the recent COGS corpus (Kim and Linzen, 2020).\nThough seq2seq models can perform well on lexical tasks, they perform with\nnear-zero accuracy on structural generalization tasks that require novel\nsyntactic structures; this holds true even when they are trained to predict\nsyntax instead of semantics. In contrast, compositional models achieve\nnear-perfect accuracy on structural generalization; we present new results\nconfirming this from the AM parser (Groschwitz et al., 2021). Our findings show\nstructural generalization is a key measure of compositional generalization and\nrequires models that are aware of complex structure.", "published": "2022-02-24 07:36:35", "link": "http://arxiv.org/abs/2202.11937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NoisyTune: A Little Noise Can Help You Finetune Pretrained Language\n  Models Better", "abstract": "Effectively finetuning pretrained language models (PLMs) is critical for\ntheir success in downstream tasks. However, PLMs may have risks in overfitting\nthe pretraining tasks and data, which usually have gap with the target\ndownstream tasks. Such gap may be difficult for existing PLM finetuning methods\nto overcome and lead to suboptimal performance. In this paper, we propose a\nvery simple yet effective method named NoisyTune to help better finetune PLMs\non downstream tasks by adding some noise to the parameters of PLMs before\nfine-tuning. More specifically, we propose a matrix-wise perturbing method\nwhich adds different uniform noises to different parameter matrices based on\ntheir standard deviations. In this way, the varied characteristics of different\ntypes of parameters in PLMs can be considered. Extensive experiments on both\nGLUE English benchmark and XTREME multilingual benchmark show NoisyTune can\nconsistently empower the finetuning of different PLMs on different downstream\ntasks.", "published": "2022-02-24 11:08:02", "link": "http://arxiv.org/abs/2202.12024v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"splink\" is happy and \"phrouth\" is scary: Emotion Intensity Analysis for\n  Nonsense Words", "abstract": "People associate affective meanings to words - \"death\" is scary and sad while\n\"party\" is connotated with surprise and joy. This raises the question if the\nassociation is purely a product of the learned affective imports inherent to\nsemantic meanings, or is also an effect of other features of words, e.g.,\nmorphological and phonological patterns. We approach this question with an\nannotation-based analysis leveraging nonsense words. Specifically, we conduct a\nbest-worst scaling crowdsourcing study in which participants assign intensity\nscores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense\nwords and, for comparison of the results to previous work, to 68 real words.\nBased on this resource, we develop character-level and phonology-based\nintensity regressors. We evaluate them on both nonsense words and real words\n(making use of the NRC emotion intensity lexicon of 7493 words), across six\nemotion categories. The analysis of our data reveals that some phonetic\npatterns show clear differences between emotion intensities. For instance, s as\na first phoneme contributes to joy, sh to surprise, p as last phoneme more to\ndisgust than to anger and fear. In the modelling experiments, a regressor\ntrained on real words from the NRC emotion intensity lexicon shows a higher\nperformance (r = 0.17) than regressors that aim at learning the emotion\nconnotation purely from nonsense words. We conclude that humans do associate\naffective meaning to words based on surface patterns, but also based on\nsimilarities to existing words (\"juy\" to \"joy\", or \"flike\" to \"like\").", "published": "2022-02-24 14:48:43", "link": "http://arxiv.org/abs/2202.12132v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretraining without Wordpieces: Learning Over a Vocabulary of Millions\n  of Words", "abstract": "The standard BERT adopts subword-based tokenization, which may break a word\ninto two or more wordpieces (e.g., converting \"lossless\" to \"loss\" and \"less\").\nThis will bring inconvenience in following situations: (1) what is the best way\nto obtain the contextual vector of a word that is divided into multiple\nwordpieces? (2) how to predict a word via cloze test without knowing the number\nof wordpieces in advance? In this work, we explore the possibility of\ndeveloping BERT-style pretrained model over a vocabulary of words instead of\nwordpieces. We call such word-level BERT model as WordBERT. We train models\nwith different vocabulary sizes, initialization configurations and languages.\nResults show that, compared to standard wordpiece-based BERT, WordBERT makes\nsignificant improvements on cloze test and machine reading comprehension. On\nmany other natural language understanding tasks, including POS tagging,\nchunking and NER, WordBERT consistently performs better than BERT. Model\nanalysis indicates that the major advantage of WordBERT over BERT lies in the\nunderstanding for low-frequency words and rare words. Furthermore, since the\npipeline is language-independent, we train WordBERT for Chinese language and\nobtain significant gains on five natural language understanding datasets.\nLastly, the analyse on inference speed illustrates WordBERT has comparable time\ncost to BERT in natural language understanding tasks.", "published": "2022-02-24 15:15:48", "link": "http://arxiv.org/abs/2202.12142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Attention for Incomplete Utterance Rewriting", "abstract": "Incomplete utterance rewriting (IUR) has recently become an essential task in\nNLP, aiming to complement the incomplete utterance with sufficient context\ninformation for comprehension. In this paper, we propose a novel method by\ndirectly extracting the coreference and omission relationship from the\nself-attention weight matrix of the transformer instead of word embeddings and\nedit the original text accordingly to generate the complete utterance.\nBenefiting from the rich information in the self-attention weight matrix, our\nmethod achieved competitive results on public IUR datasets.", "published": "2022-02-24 15:55:16", "link": "http://arxiv.org/abs/2202.12160v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing BERT's priors with serial reproduction chains", "abstract": "Sampling is a promising bottom-up method for exposing what generative models\nhave learned about language, but it remains unclear how to generate\nrepresentative samples from popular masked language models (MLMs) like BERT.\nThe MLM objective yields a dependency network with no guarantee of consistent\nconditional distributions, posing a problem for naive approaches. Drawing from\ntheories of iterated learning in cognitive science, we explore the use of\nserial reproduction chains to sample from BERT's priors. In particular, we\nobserve that a unique and consistent estimator of the ground-truth joint\ndistribution is given by a Generative Stochastic Network (GSN) sampler, which\nrandomly selects which token to mask and reconstruct on each step. We show that\nthe lexical and syntactic statistics of sentences from GSN chains closely match\nthe ground-truth corpus distribution and perform better than other methods in a\nlarge corpus of naturalness judgments. Our findings establish a firmer\ntheoretical foundation for bottom-up probing and highlight richer deviations\nfrom human priors.", "published": "2022-02-24 17:42:28", "link": "http://arxiv.org/abs/2202.12226v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural reality of argument structure constructions", "abstract": "In lexicalist linguistic theories, argument structure is assumed to be\npredictable from the meaning of verbs. As a result, the verb is the primary\ndeterminant of the meaning of a clause. In contrast, construction grammarians\npropose that argument structure is encoded in constructions (or form-meaning\npairs) that are distinct from verbs. Decades of psycholinguistic research have\nproduced substantial empirical evidence in favor of the construction view. Here\nwe adapt several psycholinguistic studies to probe for the existence of\nargument structure constructions (ASCs) in Transformer-based language models\n(LMs). First, using a sentence sorting experiment, we find that sentences\nsharing the same construction are closer in embedding space than sentences\nsharing the same verb. Furthermore, LMs increasingly prefer grouping by\nconstruction with more input data, mirroring the behaviour of non-native\nlanguage learners. Second, in a \"Jabberwocky\" priming-based experiment, we find\nthat LMs associate ASCs with meaning, even in semantically nonsensical\nsentences. Our work offers the first evidence for ASCs in LMs and highlights\nthe potential to devise novel probing methods grounded in psycholinguistic\nresearch.", "published": "2022-02-24 18:02:50", "link": "http://arxiv.org/abs/2202.12246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled\n  Studies", "abstract": "When we transfer a pretrained language model to a new language, there are\nmany axes of variation that change at once. To disentangle the impact of\ndifferent factors like syntactic similarity and vocabulary similarity, we\npropose a set of controlled transfer studies: we systematically transform the\nlanguage of the GLUE benchmark, altering one axis of crosslingual variation at\na time, and then measure the resulting drops in a pretrained model's downstream\nperformance. We find that models can largely recover from syntactic-style\nshifts, but cannot recover from vocabulary misalignment and embedding matrix\nre-initialization, even with continued pretraining on 15 million tokens. %On\nthe other hand, transferring to a dataset with an unaligned vocabulary is\nextremely hard to recover from in the low-data regime. Moreover, good-quality\ntokenizers in the transfer language do not make vocabulary alignment easier.\nOur experiments provide insights into the factors of cross-lingual transfer\nthat researchers should most focus on when designing language transfer\nscenarios.", "published": "2022-02-24 19:00:39", "link": "http://arxiv.org/abs/2202.12312v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TrimBERT: Tailoring BERT for Trade-offs", "abstract": "Models based on BERT have been extremely successful in solving a variety of\nnatural language processing (NLP) tasks. Unfortunately, many of these large\nmodels require a great deal of computational resources and/or time for\npre-training and fine-tuning which limits wider adoptability. While\nself-attention layers have been well-studied, a strong justification for\ninclusion of the intermediate layers which follow them remains missing in the\nliterature. In this work, we show that reducing the number of intermediate\nlayers in BERT-Base results in minimal fine-tuning accuracy loss of downstream\ntasks while significantly decreasing model size and training time. We further\nmitigate two key bottlenecks, by replacing all softmax operations in the\nself-attention layers with a computationally simpler alternative and removing\nhalf of all layernorm operations. This further decreases the training time\nwhile maintaining a high level of fine-tuning accuracy.", "published": "2022-02-24 23:06:29", "link": "http://arxiv.org/abs/2202.12411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Unfairness of DP-SGD Across Settings", "abstract": "End users and regulators require private and fair artificial intelligence\nmodels, but previous work suggests these objectives may be at odds. We use the\nCivilComments to evaluate the impact of applying the {\\em de facto} standard\napproach to privacy, DP-SGD, across several fairness metrics. We evaluate three\nimplementations of DP-SGD: for dimensionality reduction (PCA), linear\nclassification (logistic regression), and robust deep learning (Group-DRO). We\nestablish a negative, logarithmic correlation between privacy and fairness in\nthe case of linear classification and robust deep learning. DP-SGD had no\nsignificant impact on fairness for PCA, but upon inspection, also did not seem\nto lead to private representations.", "published": "2022-02-24 12:26:08", "link": "http://arxiv.org/abs/2202.12058v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KESA: A Knowledge Enhanced Approach For Sentiment Analysis", "abstract": "Though some recent works focus on injecting sentiment knowledge into\npre-trained language models, they usually design mask and reconstruction tasks\nin the post-training phase. In this paper, we aim to benefit from sentiment\nknowledge in a lighter way. To achieve this goal, we study sentence-level\nsentiment analysis and, correspondingly, propose two sentiment-aware auxiliary\ntasks named sentiment word cloze and conditional sentiment prediction. The\nfirst task learns to select the correct sentiment words within the input, given\nthe overall sentiment polarity as prior knowledge. On the contrary, the second\ntask predicts the overall sentiment polarity given the sentiment polarity of\nthe word as prior knowledge. In addition, two kinds of label combination\nmethods are investigated to unify multiple types of labels in each task. We\nargue that more information can promote the models to learn more profound\nsemantic representation. We implement it in a straightforward way to verify\nthis hypothesis. The experimental results demonstrate that our approach\nconsistently outperforms pre-trained models and is additive to existing\nknowledge-enhanced post-trained models. The code and data are released at\nhttps://github.com/lshowway/KESA.", "published": "2022-02-24 13:21:27", "link": "http://arxiv.org/abs/2202.12093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Natural Language to Simulations: Applying GPT-3 Codex to Automate\n  Simulation Modeling of Logistics Systems", "abstract": "Our work is the first attempt to apply Natural Language Processing to\nautomate the development of simulation models of systems vitally important for\nlogistics. We demonstrated that the framework built on top of the fine-tuned\nGPT-3 Codex, a Transformer-based language model, could produce functionally\nvalid simulations of queuing and inventory control systems given the verbal\ndescription. In conducted experiments, GPT-3 Codex demonstrated convincing\nexpertise in Python as well as an understanding of the domain-specific\nvocabulary. As a result, the language model could produce simulations of a\nsingle-product inventory-control system and single-server queuing system given\nthe domain-specific context, a detailed description of the process, and a list\nof variables with the corresponding values. The demonstrated results, along\nwith the rapid improvement of language models, open the door for significant\nsimplification of the workflow behind the simulation model development, which\nwill allow experts to focus on the high-level consideration of the problem and\nholistic thinking.", "published": "2022-02-24 14:01:50", "link": "http://arxiv.org/abs/2202.12107v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event\n  Argument Extraction", "abstract": "In this paper, we propose an effective yet efficient model PAIE for both\nsentence-level and document-level Event Argument Extraction (EAE), which also\ngeneralizes well when there is a lack of training data. On the one hand, PAIE\nutilizes prompt tuning for extractive objectives to take the best advantages of\nPre-trained Language Models (PLMs). It introduces two span selectors based on\nthe prompt to select start/end tokens among input texts for each role. On the\nother hand, it captures argument interactions via multi-role prompts and\nconducts joint optimization with optimal span assignments via a bipartite\nmatching loss. Also, with a flexible prompt design, PAIE can extract multiple\narguments with the same role instead of conventional heuristic threshold\ntuning. We have conducted extensive experiments on three benchmarks, including\nboth sentence- and document-level EAE. The results present promising\nimprovements from PAIE (3.5\\% and 2.3\\% F1 gains in average on three\nbenchmarks, for PAIE-base and PAIE-large respectively). Further analysis\ndemonstrates the efficiency, generalization to few-shot settings, and\neffectiveness of different extractive prompt tuning strategies. Our code is\navailable at https://github.com/mayubo2333/PAIE.", "published": "2022-02-24 14:07:25", "link": "http://arxiv.org/abs/2202.12109v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How reparametrization trick broke differentially-private text\n  representation learning", "abstract": "As privacy gains traction in the NLP community, researchers have started\nadopting various approaches to privacy-preserving methods. One of the favorite\nprivacy frameworks, differential privacy (DP), is perhaps the most compelling\nthanks to its fundamental theoretical guarantees. Despite the apparent\nsimplicity of the general concept of differential privacy, it seems non-trivial\nto get it right when applying it to NLP. In this short paper, we formally\nanalyze several recent NLP papers proposing text representation learning using\nDPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and\nreveal their false claims of being differentially private. Furthermore, we also\nshow a simple yet general empirical sanity check to determine whether a given\nimplementation of a DP mechanism almost certainly violates the privacy loss\nguarantees. Our main goal is to raise awareness and help the community\nunderstand potential pitfalls of applying differential privacy to text\nrepresentation learning.", "published": "2022-02-24 15:02:42", "link": "http://arxiv.org/abs/2202.12138v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "An NLP Solution to Foster the Use of Information in Electronic Health\n  Records for Efficiency in Decision-Making in Hospital Care", "abstract": "The project aimed to define the rules and develop a technological solution to\nautomatically identify a set of attributes within free-text clinical records\nwritten in Portuguese. The first application developed and implemented on this\nbasis was a structured summary of a patient's clinical history, including\nprevious diagnoses and procedures, usual medication, and relevant\ncharacteristics or conditions for clinical decisions, such as allergies, being\nunder anticoagulant therapy, etc. The project's goal was achieved by a\nmultidisciplinary team that included clinicians, epidemiologists, computational\nlinguists, machine learning researchers and software engineers, bringing\ntogether the expertise and perspectives of a public hospital, the university\nand the private sector. Relevant benefits to users and patients are related\nwith facilitated access to the patient's history, which translates into\nexhaustiveness in apprehending the patient's clinical past and efficiency due\nto time saving.", "published": "2022-02-24 15:52:59", "link": "http://arxiv.org/abs/2202.12159v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Overcoming a Theoretical Limitation of Self-Attention", "abstract": "Although transformers are remarkably effective for many tasks, there are some\nsurprisingly easy-looking regular languages that they struggle with. Hahn shows\nthat for languages where acceptance depends on a single input symbol, a\ntransformer's classification decisions become less and less confident (that is,\nwith cross-entropy approaching 1 bit per string) as input strings get longer\nand longer. We examine this limitation using two languages: PARITY, the\nlanguage of bit strings with an odd number of 1s, and FIRST, the language of\nbit strings starting with a 1. We demonstrate three ways of overcoming the\nlimitation suggested by Hahn's lemma. First, we settle an open question by\nconstructing a transformer that recognizes PARITY with perfect accuracy, and\nsimilarly for FIRST. Second, we use layer normalization to bring the\ncross-entropy of both models arbitrarily close to zero. Third, when\ntransformers need to focus on a single position, as for FIRST, we find that\nthey can fail to generalize to longer strings; we offer a simple remedy to this\nproblem that also improves length generalization in machine translation.", "published": "2022-02-24 16:14:29", "link": "http://arxiv.org/abs/2202.12172v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BERTVision -- A Parameter-Efficient Approach for Question Answering", "abstract": "We present a highly parameter efficient approach for Question Answering that\nsignificantly reduces the need for extended BERT fine-tuning. Our method uses\ninformation from the hidden state activations of each BERT transformer layer,\nwhich is discarded during typical BERT inference. Our best model achieves\nmaximal BERT performance at a fraction of the training time and GPU or TPU\nexpense. Performance is further improved by ensembling our model with BERTs\npredictions. Furthermore, we find that near optimal performance can be achieved\nfor QA span annotation using less training data. Our experiments show that this\napproach works well not only for span annotation, but also for classification,\nsuggesting that it may be extensible to a wider range of tasks.", "published": "2022-02-24 17:16:25", "link": "http://arxiv.org/abs/2202.12210v1", "categories": ["cs.CL", "cs.LG", "68T07", "I.2"], "primary_category": "cs.CL"}
{"title": "Toward More Meaningful Resources for Lower-resourced Languages", "abstract": "In this position paper, we describe our perspective on how meaningful\nresources for lower-resourced languages should be developed in connection with\nthe speakers of those languages. We first examine two massively multilingual\nresources in detail. We explore the contents of the names stored in Wikidata\nfor a few lower-resourced languages and find that many of them are not in fact\nin the languages they claim to be and require non-trivial effort to correct. We\ndiscuss quality issues present in WikiAnn and evaluate whether it is a useful\nsupplement to hand annotated data. We then discuss the importance of creating\nannotation for lower-resourced languages in a thoughtful and ethical way that\nincludes the languages' speakers as part of the development process. We\nconclude with recommended guidelines for resource development.", "published": "2022-02-24 18:39:57", "link": "http://arxiv.org/abs/2202.12288v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DoCoGen: Domain Counterfactual Generation for Low Resource Domain\n  Adaptation", "abstract": "Natural language processing (NLP) algorithms have become very successful, but\nthey still struggle when applied to out-of-distribution examples. In this paper\nwe propose a controllable generation approach in order to deal with this domain\nadaptation (DA) challenge. Given an input text example, our DoCoGen algorithm\ngenerates a domain-counterfactual textual example (D-con) - that is similar to\nthe original in all aspects, including the task label, but its domain is\nchanged to a desired one. Importantly, DoCoGen is trained using only unlabeled\nexamples from multiple domains - no NLP task labels or parallel pairs of\ntextual examples and their domain-counterfactuals are required. We show that\nDoCoGen can generate coherent counterfactuals consisting of multiple sentences.\nWe use the D-cons generated by DoCoGen to augment a sentiment classifier and a\nmulti-label intent classifier in 20 and 78 DA setups, respectively, where\nsource-domain labeled data is scarce. Our model outperforms strong baselines\nand improves the accuracy of a state-of-the-art unsupervised DA algorithm.", "published": "2022-02-24 20:25:46", "link": "http://arxiv.org/abs/2202.12350v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "First is Better Than Last for Language Data Influence", "abstract": "The ability to identify influential training examples enables us to debug\ntraining data and explain model behavior. Existing techniques to do so are\nbased on the flow of training data influence through the model parameters. For\nlarge models in NLP applications, it is often computationally infeasible to\nstudy this flow through all model parameters, therefore techniques usually pick\nthe last layer of weights. However, we observe that since the activation\nconnected to the last layer of weights contains \"shared logic\", the data\ninfluenced calculated via the last layer weights prone to a ``cancellation\neffect'', where the data influence of different examples have large magnitude\nthat contradicts each other. The cancellation effect lowers the discriminative\npower of the influence score, and deleting influential examples according to\nthis measure often does not change the model's behavior by much. To mitigate\nthis, we propose a technique called TracIn-WE that modifies a method called\nTracIn to operate on the word embedding layer instead of the last layer, where\nthe cancellation effect is less severe. One potential concern is that influence\nbased on the word embedding layer may not encode sufficient high level\ninformation. However, we find that gradients (unlike embeddings) do not suffer\nfrom this, possibly because they chain through higher layers. We show that\nTracIn-WE significantly outperforms other data influence methods applied on the\nlast layer significantly on the case deletion evaluation on three language\nclassification tasks for different models. In addition, TracIn-WE can produce\nscores not just at the level of the overall training input, but also at the\nlevel of words within the training input, a further aid in debugging.", "published": "2022-02-24 00:48:29", "link": "http://arxiv.org/abs/2202.11844v3", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "CAISE: Conversational Agent for Image Search and Editing", "abstract": "Demand for image editing has been increasing as users' desire for expression\nis also increasing. However, for most users, image editing tools are not easy\nto use since the tools require certain expertise in photo effects and have\ncomplex interfaces. Hence, users might need someone to help edit their images,\nbut having a personal dedicated human assistant for every user is impossible to\nscale. For that reason, an automated assistant system for image editing is\ndesirable. Additionally, users want more image sources for diverse image\nediting works, and integrating an image search functionality into the editing\ntool is a potential remedy for this demand. Thus, we propose a dataset of an\nautomated Conversational Agent for Image Search and Editing (CAISE). To our\nknowledge, this is the first dataset that provides conversational image search\nand editing annotations, where the agent holds a grounded conversation with\nusers and helps them to search and edit images according to their requests. To\nbuild such a system, we first collect image search and editing conversations\nbetween pairs of annotators. The assistant-annotators are equipped with a\ncustomized image search and editing tool to address the requests from the\nuser-annotators. The functions that the assistant-annotators conduct with the\ntool are recorded as executable commands, allowing the trained system to be\nuseful for real-world application execution. We also introduce a\ngenerator-extractor baseline model for this task, which can adaptively select\nthe source of the next token (i.e., from the vocabulary or from textual/visual\ncontexts) for the executable command. This serves as a strong starting point\nwhile still leaving a large human-machine performance gap for useful future\nwork. Our code and dataset are publicly available at:\nhttps://github.com/hyounghk/CAISE", "published": "2022-02-24 00:55:52", "link": "http://arxiv.org/abs/2202.11847v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Phase Continuity: Learning Derivatives of Phase Spectrum for Speech\n  Enhancement", "abstract": "Modern neural speech enhancement models usually include various forms of\nphase information in their training loss terms, either explicitly or\nimplicitly. However, these loss terms are typically designed to reduce the\ndistortion of phase spectrum values at specific frequencies, which ensures they\ndo not significantly affect the quality of the enhanced speech. In this paper,\nwe propose an effective phase reconstruction strategy for neural speech\nenhancement that can operate in noisy environments. Specifically, we introduce\na phase continuity loss that considers relative phase variations across the\ntime and frequency axes. By including this phase continuity loss in a\nstate-of-the-art neural speech enhancement system trained with reconstruction\nloss and a number of magnitude spectral losses, we show that our proposed\nmethod further improves the quality of enhanced speech signals over the\nbaseline, especially when training is done jointly with a magnitude spectrum\nloss.", "published": "2022-02-24 06:15:16", "link": "http://arxiv.org/abs/2202.11918v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Word Segmentation on Discovered Phone Units with Dynamic Programming and\n  Self-Supervised Scoring", "abstract": "Recent work on unsupervised speech segmentation has used self-supervised\nmodels with phone and word segmentation modules that are trained jointly. This\npaper instead revisits an older approach to word segmentation: bottom-up\nphone-like unit discovery is performed first, and symbolic word segmentation is\nthen performed on top of the discovered units (without influencing the lower\nlevel). To do this, I propose a new unit discovery model, a new symbolic word\nsegmentation model, and then chain the two models to segment speech. Both\nmodels use dynamic programming to minimize segment costs from a self-supervised\nnetwork with an additional duration penalty that encourages longer units.\nConcretely, for acoustic unit discovery, duration-penalized dynamic programming\n(DPDP) is used with a contrastive predictive coding model as the scoring\nnetwork. For word segmentation, DPDP is applied with an autoencoding recurrent\nneural as the scoring network. The two models are chained in order to segment\nspeech. This approach gives comparable word segmentation results to\nstate-of-the-art joint self-supervised segmentation models on an English\nbenchmark. On French, Mandarin, German and Wolof data, it outperforms previous\nsystems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP\nsystem segments shorter filler words well, but longer words might require some\nexternal top-down signal.", "published": "2022-02-24 07:02:56", "link": "http://arxiv.org/abs/2202.11929v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Attentive Temporal Pooling for Conformer-based Streaming Language\n  Identification in Long-form Speech", "abstract": "In this paper, we introduce a novel language identification system based on\nconformer layers. We propose an attentive temporal pooling mechanism to allow\nthe model to carry information in long-form audio via a recurrent form, such\nthat the inference can be performed in a streaming fashion. Additionally, we\ninvestigate two domain adaptation approaches to allow adapting an existing\nlanguage identification model without retraining the model parameters for a new\ndomain. We perform a comparative study of different model topologies under\ndifferent constraints of model size, and find that conformer-based models\nsignificantly outperform LSTM and transformer based models. Our experiments\nalso show that attentive temporal pooling and domain adaptation improve model\naccuracy.", "published": "2022-02-24 16:01:07", "link": "http://arxiv.org/abs/2202.12163v4", "categories": ["eess.AS", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing?\n  A Structured Review", "abstract": "Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that\ncombining deep learning with symbolic reasoning will lead to stronger AI than\neither paradigm on its own. As successful as deep learning has been, it is\ngenerally accepted that even our best deep learning systems are not very good\nat abstract reasoning. And since reasoning is inextricably linked to language,\nit makes intuitive sense that Natural Language Processing (NLP), would be a\nparticularly well-suited candidate for NeSy. We conduct a structured review of\nstudies implementing NeSy for NLP, with the aim of answering the question of\nwhether NeSy is indeed meeting its promises: reasoning, out-of-distribution\ngeneralization, interpretability, learning and reasoning from small data, and\ntransferability to new domains. We examine the impact of knowledge\nrepresentation, such as rules and semantic networks, language structure and\nrelational structure, and whether implicit or explicit reasoning contributes to\nhigher promise scores. We find that systems where logic is compiled into the\nneural network lead to the most NeSy goals being satisfied, while other factors\nsuch as knowledge representation, or type of neural architecture do not exhibit\na clear correlation with goals being met. We find many discrepancies in how\nreasoning is defined, specifically in relation to human level reasoning, which\nimpact decisions about model architectures and drive conclusions which are not\nalways consistent across studies. Hence we advocate for a more methodical\napproach to the application of theories of human reasoning as well as the\ndevelopment of appropriate benchmarks, which we hope can lead to a better\nunderstanding of progress in the field. We make our data and code available on\ngithub for further analysis.", "published": "2022-02-24 17:13:33", "link": "http://arxiv.org/abs/2202.12205v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Capturing Failures of Large Language Models via Human Cognitive Biases", "abstract": "Large language models generate complex, open-ended outputs: instead of\noutputting a class label they write summaries, generate dialogue, or produce\nworking code. In order to asses the reliability of these open-ended generation\nsystems, we aim to identify qualitative categories of erroneous behavior,\nbeyond identifying individual errors. To hypothesize and test for such\nqualitative errors, we draw inspiration from human cognitive biases --\nsystematic patterns of deviation from rational judgement. Specifically, we use\ncognitive biases as motivation to (i) generate hypotheses for problems that\nmodels may have, and (ii) develop experiments that elicit these problems. Using\ncode generation as a case study, we find that OpenAI's Codex errs predictably\nbased on how the input prompt is framed, adjusts outputs towards anchors, and\nis biased towards outputs that mimic frequent training examples. We then use\nour framework to elicit high-impact errors such as incorrectly deleting files.\nOur results indicate that experimental methodology from cognitive science can\nhelp characterize how machine learning systems behave.", "published": "2022-02-24 18:58:52", "link": "http://arxiv.org/abs/2202.12299v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Better Meta-Initialization with Task Augmentation for\n  Kindergarten-aged Speech Recognition", "abstract": "Children's automatic speech recognition (ASR) is always difficult due to, in\npart, the data scarcity problem, especially for kindergarten-aged kids. When\ndata are scarce, the model might overfit to the training data, and hence good\nstarting points for training are essential. Recently, meta-learning was\nproposed to learn model initialization (MI) for ASR tasks of different\nlanguages. This method leads to good performance when the model is adapted to\nan unseen language. However, MI is vulnerable to overfitting on training tasks\n(learner overfitting). It is also unknown whether MI generalizes to other\nlow-resource tasks. In this paper, we validate the effectiveness of MI in\nchildren's ASR and attempt to alleviate the problem of learner overfitting. To\nachieve model-agnostic meta-learning (MAML), we regard children's speech at\neach age as a different task. In terms of learner overfitting, we propose a\ntask-level augmentation method by simulating new ages using frequency warping\ntechniques. Detailed experiments are conducted to show the impact of task\naugmentation on each age for kindergarten-aged speech. As a result, our\napproach achieves a relative word error rate (WER) improvement of 51% over the\nbaseline system with no augmentation or initialization.", "published": "2022-02-24 19:20:37", "link": "http://arxiv.org/abs/2202.12326v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Construction of Large-Scale Misinformation Labeled Datasets from Social\n  Media Discourse using Label Refinement", "abstract": "Malicious accounts spreading misinformation has led to widespread false and\nmisleading narratives in recent times, especially during the COVID-19 pandemic,\nand social media platforms struggle to eliminate these contents rapidly. This\nis because adapting to new domains requires human intensive fact-checking that\nis slow and difficult to scale. To address this challenge, we propose to\nleverage news-source credibility labels as weak labels for social media posts\nand propose model-guided refinement of labels to construct large-scale, diverse\nmisinformation labeled datasets in new domains. The weak labels can be\ninaccurate at the article or social media post level where the stance of the\nuser does not align with the news source or article credibility. We propose a\nframework to use a detection model self-trained on the initial weak labels with\nuncertainty sampling based on entropy in predictions of the model to identify\npotentially inaccurate labels and correct for them using self-supervision or\nrelabeling. The framework will incorporate social context of the post in terms\nof the community of its associated user for surfacing inaccurate labels towards\nbuilding a large-scale dataset with minimum human effort. To provide labeled\ndatasets with distinction of misleading narratives where information might be\nmissing significant context or has inaccurate ancillary details, the proposed\nframework will use the few labeled samples as class prototypes to separate high\nconfidence samples into false, unproven, mixture, mostly false, mostly true,\ntrue, and debunk information. The approach is demonstrated for providing a\nlarge-scale misinformation dataset on COVID-19 vaccines.", "published": "2022-02-24 23:10:29", "link": "http://arxiv.org/abs/2202.12413v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Ask2Mask: Guided Data Selection for Masked Speech Modeling", "abstract": "Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn\nrepresentations over speech frames which are randomly masked within an\nutterance. While these methods improve performance of Automatic Speech\nRecognition (ASR) systems, they have one major limitation. They treat all\nunsupervised speech samples with equal weight, which hinders learning as not\nall samples have relevant information to learn meaningful representations. In\nthis work, we address this limitation. We propose ask2mask (ATM), a novel\napproach to focus on specific samples during MSM pre-training. ATM employs an\nexternal ASR model or \\textit{scorer} to weight unsupervised input samples in\ntwo different ways: 1) A fine-grained data selection is performed by masking\nover the highly confident input frames as chosen by the scorer. This allows the\nmodel to learn meaningful representations. 2) ATM is further extended to focus\nat utterance-level by weighting the final MSM loss with the utterance-level\nconfidence score. We conduct fine-tuning experiments on two well-benchmarked\ncorpora: LibriSpeech (matching the pre-training data) and Commonvoice,\nTED-LIUM, AMI and CHiME-6 (not matching the pre-training data). The results\nsubstantiate the efficacy of ATM on significantly improving the recognition\nperformance under mismatched conditions (up to 11.6\\% relative over published\nresults and upto 4.46\\% relative over our internal baseline) while still\nyielding modest improvements under matched conditions.", "published": "2022-02-24 17:34:54", "link": "http://arxiv.org/abs/2202.12719v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual\n  Speech Recognition", "abstract": "Training Transformer-based models demands a large amount of data, while\nobtaining aligned and labelled data in multimodality is rather cost-demanding,\nespecially for audio-visual speech recognition (AVSR). Thus it makes a lot of\nsense to make use of unlabelled unimodal data. On the other side, although the\neffectiveness of large-scale self-supervised learning is well established in\nboth audio and visual modalities, how to integrate those pre-trained models\ninto a multimodal scenario remains underexplored. In this work, we successfully\nleverage unimodal self-supervised learning to promote the multimodal AVSR. In\nparticular, audio and visual front-ends are trained on large-scale unimodal\ndatasets, then we integrate components of both front-ends into a larger\nmultimodal framework which learns to recognize parallel audio-visual data into\ncharacters through a combination of CTC and seq2seq decoding. We show that both\ncomponents inherited from unimodal self-supervised learning cooperate well,\nresulting in that the multimodal framework yields competitive results through\nfine-tuning. Our model is experimentally validated on both word-level and\nsentence-level tasks. Especially, even without an external language model, our\nproposed model raises the state-of-the-art performances on the widely accepted\nLip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative\nimprovement of 30%.", "published": "2022-02-24 15:12:17", "link": "http://arxiv.org/abs/2203.07996v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "openFEAT: Improving Speaker Identification by Open-set Few-shot\n  Embedding Adaptation with Transformer", "abstract": "Household speaker identification with few enrollment utterances is an\nimportant yet challenging problem, especially when household members share\nsimilar voice characteristics and room acoustics. A common embedding space\nlearned from a large number of speakers is not universally applicable for the\noptimal identification of every speaker in a household. In this work, we first\nformulate household speaker identification as a few-shot open-set recognition\ntask and then propose a novel embedding adaptation framework to adapt speaker\nrepresentations from the given universal embedding space to a\nhousehold-specific embedding space using a set-to-set function, yielding better\nhousehold speaker identification performance. With our algorithm, Open-set\nFew-shot Embedding Adaptation with Transformer (openFEAT), we observe that the\nspeaker identification equal error rate (IEER) on simulated households with 2\nto 7 hard-to-discriminate speakers is reduced by 23% to 31% relative.", "published": "2022-02-24 20:23:34", "link": "http://arxiv.org/abs/2202.12349v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Automatic speaker verification spoofing and deepfake detection using\n  wav2vec 2.0 and data augmentation", "abstract": "The performance of spoofing countermeasure systems depends fundamentally upon\nthe use of sufficiently representative training data. With this usually being\nlimited, current solutions typically lack generalisation to attacks encountered\nin the wild. Strategies to improve reliability in the face of uncontrolled,\nunpredictable attacks are hence needed. We report in this paper our efforts to\nuse self-supervised learning in the form of a wav2vec 2.0 front-end with fine\ntuning. Despite initial base representations being learned using only bona fide\ndata and no spoofed data, we obtain the lowest equal error rates reported in\nthe literature for both the ASVspoof 2021 Logical Access and Deepfake\ndatabases. When combined with data augmentation,these results correspond to an\nimprovement of almost 90% relative to our baseline system.", "published": "2022-02-24 17:55:00", "link": "http://arxiv.org/abs/2202.12233v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Low-distortion Multi-channel Speech Enhancement: The ESPNet-SE\n  Submission to The L3DAS22 Challenge", "abstract": "This paper describes our submission to the L3DAS22 Challenge Task 1, which\nconsists of speech enhancement with 3D Ambisonic microphones. The core of our\napproach combines Deep Neural Network (DNN) driven complex spectral mapping\nwith linear beamformers such as the multi-frame multi-channel Wiener filter.\nOur proposed system has two DNNs and a linear beamformer in between. Both DNNs\nare trained to perform complex spectral mapping, using a combination of\nwaveform and magnitude spectrum losses. The estimated signal from the first DNN\nis used to drive a linear beamformer, and the beamforming result, together with\nthis enhanced signal, are used as extra inputs for the second DNN which refines\nthe estimation. Then, from this new estimated signal, the linear beamformer and\nsecond DNN are run iteratively. The proposed method was ranked first in the\nchallenge, achieving, on the evaluation set, a ranking metric of 0.984, versus\n0.833 of the challenge baseline.", "published": "2022-02-24 18:58:08", "link": "http://arxiv.org/abs/2202.12298v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ADPCM with nonlinear prediction", "abstract": "Many speech coders are based on linear prediction coding (LPC), nevertheless\nwith LPC is not possible to model the nonlinearities present in the speech\nsignal. Because of this there is a growing interest for nonlinear techniques.\nIn this paper we discuss ADPCM schemes with a nonlinear predictor based on\nneural nets, which yields an increase of 1-2.5dB in the SEGSNR over classical\nmethods. This paper will discuss the block-adaptive and sample-adaptive\npredictions.", "published": "2022-02-24 00:17:59", "link": "http://arxiv.org/abs/2203.01818v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Closing the Gap between Single-User and Multi-User VoiceFilter-Lite", "abstract": "VoiceFilter-Lite is a speaker-conditioned voice separation model that plays a\ncrucial role in improving speech recognition and speaker verification by\nsuppressing overlapping speech from non-target speakers. However, one\nlimitation of VoiceFilter-Lite, and other speaker-conditioned speech models in\ngeneral, is that these models are usually limited to a single target speaker.\nThis is undesirable as most smart home devices now support multiple enrolled\nusers. In order to extend the benefits of personalization to multiple users, we\npreviously developed an attention-based speaker selection mechanism and applied\nit to VoiceFilter-Lite. However, the original multi-user VoiceFilter-Lite model\nsuffers from significant performance degradation compared with single-user\nmodels. In this paper, we devised a series of experiments to improve the\nmulti-user VoiceFilter-Lite model. By incorporating a dual learning rate\nschedule and by using feature-wise linear modulation (FiLM) to condition the\nmodel with the attended speaker embedding, we successfully closed the\nperformance gap between multi-user and single-user VoiceFilter-Lite models on\nsingle-speaker evaluations. At the same time, the new model can also be easily\nextended to support any number of users, and significantly outperforms our\npreviously published model on multi-speaker evaluations.", "published": "2022-02-24 16:10:16", "link": "http://arxiv.org/abs/2202.12169v2", "categories": ["eess.AS", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
{"title": "SonOpt: Sonifying Bi-objective Population-Based Optimization Algorithms", "abstract": "We propose SonOpt, the first (open source) data sonification application for\nmonitoring the progress of bi-objective population-based optimization\nalgorithms during search, to facilitate algorithm understanding. SonOpt\nprovides insights into convergence/stagnation of search, the evolution of the\napproximation set shape, location of recurring points in the approximation set,\nand population diversity. The benefits of data sonification have been shown for\nvarious non-optimization related monitoring tasks. However, very few attempts\nhave been made in the context of optimization and their focus has been\nexclusively on single-objective problems. In comparison, SonOpt is designed for\nbi-objective optimization problems, relies on objective function values of\nnon-dominated solutions only, and is designed with the user (listener) in mind;\navoiding convolution of multiple sounds and prioritising ease of familiarizing\nwith the system. This is achieved using two sonification paths relying on the\nconcepts of wavetable and additive synthesis. This paper motivates and\ndescribes the architecture of SonOpt, and then validates SonOpt for two popular\nmulti-objective optimization algorithms (NSGA-II and MOEA/D). Experience SonOpt\nyourself via https://github.com/tasos-a/SonOpt-1.0 .", "published": "2022-02-24 16:39:58", "link": "http://arxiv.org/abs/2202.12187v1", "categories": ["cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
{"title": "A Perceptual Measure for Evaluating the Resynthesis of Automatic Music\n  Transcriptions", "abstract": "This study focuses on the perception of music performances when contextual\nfactors, such as room acoustics and instrument, change. We propose to\ndistinguish the concept of \"performance\" from the one of \"interpretation\",\nwhich expresses the \"artistic intention\". Towards assessing this distinction,\nwe carried out an experimental evaluation where 91 subjects were invited to\nlisten to various audio recordings created by resynthesizing MIDI data obtained\nthrough Automatic Music Transcription (AMT) systems and a sensorized acoustic\npiano. During the resynthesis, we simulated different contexts and asked\nlisteners to evaluate how much the interpretation changes when the context\nchanges. Results show that: (1) MIDI format alone is not able to completely\ngrasp the artistic intention of a music performance; (2) usual objective\nevaluation measures based on MIDI data present low correlations with the\naverage subjective evaluation. To bridge this gap, we propose a novel measure\nwhich is meaningfully correlated with the outcome of the tests. In addition, we\ninvestigate multimodal machine learning by providing a new score-informed AMT\nmethod and propose an approximation algorithm for the $p$-dispersion problem.", "published": "2022-02-24 18:09:22", "link": "http://arxiv.org/abs/2202.12257v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the relevance of bandwidth extension for speaker identification", "abstract": "In this paper we discuss the relevance of bandwidth extension for speaker\nidentification tasks. Mainly we want to study if it is possible to recognize\nvoices that have been bandwith extended. For this purpose, we created two\ndifferent databases (microphonic and ISDN) of speech signals that were\nbandwidth extended from telephone bandwidth ([300, 3400] Hz) to full bandwidth\n([100, 8000] Hz). We have evaluated different parameterizations, and we have\nfound that the MELCEPST parameterization can take advantage of the bandwidth\nextension algorithms in several situations.", "published": "2022-02-24 09:14:25", "link": "http://arxiv.org/abs/2202.13865v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A comparative study of several parameterizations for speaker recognition", "abstract": "This paper presents an exhaustive study about the robustness of several\nparameterizations, in speaker verification and identification tasks. We have\nstudied several mismatch conditions: different recording sessions, microphones,\nand different languages (it has been obtained from a bilingual set of\nspeakers). This study reveals that the combination of several parameterizations\ncan improve the robustness in all the scenarios for both tasks, identification\nand verification. In addition, two different methods have been evaluated:\nvector quantization, and covariance matrices with an arithmetic-harmonic\nsphericity measure.", "published": "2022-02-24 00:31:27", "link": "http://arxiv.org/abs/2203.00513v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech segmentation using multilevel hybrid filters", "abstract": "A novel approach for speech segmentation is proposed, based on Multilevel\nHybrid (mean/min) Filters (MHF) with the following features: An accurate\ntransition location. Good performance in noisy environments (gaussian and\nimpulsive noise). The proposed method is based on spectral changes, with the\ngoal of segmenting the voice into homogeneous acoustic segments. This algorithm\nis being used for phoneticallysegmented speech coder, with successful results.", "published": "2022-02-24 00:03:02", "link": "http://arxiv.org/abs/2203.01819v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Retriever: Learning Content-Style Representation as a Token-Level\n  Bipartite Graph", "abstract": "This paper addresses the unsupervised learning of content-style decomposed\nrepresentation. We first give a definition of style and then model the\ncontent-style representation as a token-level bipartite graph. An unsupervised\nframework, named Retriever, is proposed to learn such representations. First, a\ncross-attention module is employed to retrieve permutation invariant (P.I.)\ninformation, defined as style, from the input data. Second, a vector\nquantization (VQ) module is used, together with man-induced constraints, to\nproduce interpretable content tokens. Last, an innovative link attention module\nserves as the decoder to reconstruct data from the decomposed content and\nstyle, with the help of the linking keys. Being modal-agnostic, the proposed\nRetriever is evaluated in both speech and image domains. The state-of-the-art\nzero-shot voice conversion performance confirms the disentangling ability of\nour framework. Top performance is also achieved in the part discovery task for\nimages, verifying the interpretability of our representation. In addition, the\nvivid part-based style transfer quality demonstrates the potential of Retriever\nto support various fascinating generative tasks. Project page at\nhttps://ydcustc.github.io/retriever-demo/.", "published": "2022-02-24 19:00:03", "link": "http://arxiv.org/abs/2202.12307v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
