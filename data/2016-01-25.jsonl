{"title": "A Kernel Independence Test for Geographical Language Variation", "abstract": "Quantifying the degree of spatial dependence for linguistic variables is a\nkey task for analyzing dialectal variation. However, existing approaches have\nimportant drawbacks. First, they are based on parametric models of dependence,\nwhich limits their power in cases where the underlying parametric assumptions\nare violated. Second, they are not applicable to all types of linguistic data:\nsome approaches apply only to frequencies, others to boolean indicators of\nwhether a linguistic variable is present. We present a new method for measuring\ngeographical language variation, which solves both of these problems. Our\napproach builds on Reproducing Kernel Hilbert space (RKHS) representations for\nnonparametric statistics, and takes the form of a test statistic that is\ncomputed from pairs of individual geotagged observations without aggregation\ninto predefined geographical bins. We compare this test with prior work using\nsynthetic data as well as a diverse set of real datasets: a corpus of Dutch\ntweets, a Dutch syntactic atlas, and a dataset of letters to the editor in\nNorth American newspapers. Our proposed test is shown to support robust\ninferences across a broad range of scenarios and types of data.", "published": "2016-01-25 12:45:59", "link": "http://arxiv.org/abs/1601.06579v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Short-Term Memory-Networks for Machine Reading", "abstract": "In this paper we address the question of how to render sequence-level\nnetworks better at handling structured input. We propose a machine reading\nsimulator which processes text incrementally from left to right and performs\nshallow reasoning with memory and attention. The reader extends the Long\nShort-Term Memory architecture with a memory network in place of a single\nmemory cell. This enables adaptive memory usage during recurrence with neural\nattention, offering a way to weakly induce relations among tokens. The system\nis initially designed to process a single sequence but we also demonstrate how\nto integrate it with an encoder-decoder architecture. Experiments on language\nmodeling, sentiment analysis, and natural language inference show that our\nmodel matches or outperforms the state of the art.", "published": "2016-01-25 19:25:48", "link": "http://arxiv.org/abs/1601.06733v7", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Label Semantics Approach to Linguistic Hedges", "abstract": "We introduce a model for the linguistic hedges `very' and `quite' within the\nlabel semantics framework, and combined with the prototype and conceptual\nspaces theories of concepts. The proposed model emerges naturally from the\nrepresentational framework we use and as such, has a clear semantic grounding.\nWe give generalisations of these hedge models and show that they can be\ncomposed with themselves and with other functions, going on to examine their\nbehaviour in the limit of composition.", "published": "2016-01-25 19:38:37", "link": "http://arxiv.org/abs/1601.06738v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Character-Level Incremental Speech Recognition with Recurrent Neural\n  Networks", "abstract": "In real-time speech recognition applications, the latency is an important\nissue. We have developed a character-level incremental speech recognition (ISR)\nsystem that responds quickly even during the speech, where the hypotheses are\ngradually improved while the speaking proceeds. The algorithm employs a\nspeech-to-character unidirectional recurrent neural network (RNN), which is\nend-to-end trained with connectionist temporal classification (CTC), and an\nRNN-based character-level language model (LM). The output values of the\nCTC-trained RNN are character-level probabilities, which are processed by beam\nsearch decoding. The RNN LM augments the decoding by providing long-term\ndependency information. We propose tree-based online beam search with\nadditional depth-pruning, which enables the system to process infinitely long\ninput speech with low latency. This system not only responds quickly on speech\nbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.\nThe proposed model achieves the word error rate (WER) of 8.90% on the Wall\nStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284\ntraining set.", "published": "2016-01-25 12:51:46", "link": "http://arxiv.org/abs/1601.06581v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Concept Generation in Language Evolution", "abstract": "This thesis investigates the generation of new concepts from combinations of\nexisting concepts as a language evolves. We give a method for combining\nconcepts, and will be investigating the utility of composite concepts in\nlanguage evolution and thence the utility of concept generation.", "published": "2016-01-25 19:23:44", "link": "http://arxiv.org/abs/1601.06732v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "The Utility of Hedged Assertions in the Emergence of Shared Categorical\n  Labels", "abstract": "We investigate the emergence of shared concepts in a community of language\nusers using a multi-agent simulation. We extend results showing that negated\nassertions are of use in developing shared categories, to include assertions\nmodified by linguistic hedges. Results show that using hedged assertions\npositively affects the emergence of shared categories in two distinct ways.\nFirstly, using contraction hedges like `very' gives better convergence over\ntime. Secondly, using expansion hedges such as `quite' reduces concept overlap.\nHowever, both these improvements come at a cost of slower speed of development.", "published": "2016-01-25 20:24:50", "link": "http://arxiv.org/abs/1601.06755v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Emerging Dimension Weights in a Conceptual Spaces Model of Concept\n  Combination", "abstract": "We investigate the generation of new concepts from combinations of properties\nas an artificial language develops. To do so, we have developed a new framework\nfor conjunctive concept combination. This framework gives a semantic grounding\nto the weighted sum approach to concept combination seen in the literature. We\nimplement the framework in a multi-agent simulation of language evolution and\nshow that shared combination weights emerge. The expected value and the\nvariance of these weights across agents may be predicted from the distribution\nof elements in the conceptual space, as determined by the underlying\nenvironment, together with the rate at which agents adopt others' concepts.\nWhen this rate is smaller, the agents are able to converge to weights with\nlower variance. However, the time taken to converge to a steady state\ndistribution of weights is longer.", "published": "2016-01-25 20:40:55", "link": "http://arxiv.org/abs/1601.06763v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
