{"title": "Neural Network Models for Implicit Discourse Relation Classification in\n  English and Chinese without Surface Features", "abstract": "Inferring implicit discourse relations in natural language text is the most\ndifficult subtask in discourse parsing. Surface features achieve good\nperformance, but they are not readily applicable to other languages without\nsemantic lexicons. Previous neural models require parses, surface features, or\na small label set to work well. Here, we propose neural network models that are\nbased on feedforward and long-short term memory architecture without any\nsurface features. To our surprise, our best configured feedforward architecture\noutperforms LSTM-based model in most cases despite thorough tuning. Under\nvarious fine-grained label sets and a cross-linguistic setting, our feedforward\nmodels perform consistently better or at least just as well as systems that\nrequire hand-crafted surface features. Our models present the first neural\nChinese discourse parser in the style of Chinese Discourse Treebank, showing\nthat our results hold cross-linguistically.", "published": "2016-06-07 01:17:00", "link": "http://arxiv.org/abs/1606.01990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CFO: Conditional Focused Neural Question Answering with Large-scale\n  Knowledge Bases", "abstract": "How can we enable computers to automatically answer questions like \"Who\ncreated the character Harry Potter\"? Carefully built knowledge bases provide\nrich sources of facts. However, it remains a challenge to answer factoid\nquestions raised in natural language due to numerous expressions of one\nquestion. In particular, we focus on the most common questions --- ones that\ncan be answered with a single fact in the knowledge base. We propose CFO, a\nConditional Focused neural-network-based approach to answering factoid\nquestions with knowledge bases. Our approach first zooms in a question to find\nmore probable candidate subject mentions, and infers the final answers with a\nunified conditional probabilistic framework. Powered by deep recurrent neural\nnetworks and neural embeddings, our proposed CFO achieves an accuracy of 75.7%\non a dataset of 108k questions - the largest public one to date. It outperforms\nthe current state of the art by an absolute margin of 11.8%.", "published": "2016-06-07 01:36:07", "link": "http://arxiv.org/abs/1606.01994v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory-enhanced Decoder for Neural Machine Translation", "abstract": "We propose to enhance the RNN decoder in a neural machine translator (NMT)\nwith external memory, as a natural but powerful extension to the state in the\ndecoding RNN. This memory-enhanced RNN decoder is called \\textsc{MemDec}. At\neach time during decoding, \\textsc{MemDec} will read from this memory and write\nto this memory once, both with content-based addressing. Unlike the unbounded\nmemory in previous work\\cite{RNNsearch} to store the representation of source\nsentence, the memory in \\textsc{MemDec} is a matrix with pre-determined size\ndesigned to better capture the information important for the decoding process\nat each time step. Our empirical study on Chinese-English translation shows\nthat it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses,\nyielding the best performance achieved with the same training set.", "published": "2016-06-07 02:28:19", "link": "http://arxiv.org/abs/1606.02003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Discrete Translation Lexicons into Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) often makes mistakes in translating\nlow-frequency content words that are essential to understanding the meaning of\nthe sentence. We propose a method to alleviate this problem by augmenting NMT\nsystems with discrete translation lexicons that efficiently encode translations\nof these low-frequency words. We describe a method to calculate the lexicon\nprobability of the next word in the translation candidate by using the\nattention vector of the NMT model to select which source word lexical\nprobabilities the model should focus on. We test two methods to combine this\nprobability with the standard NMT probability: (1) using it as a bias, and (2)\nlinear interpolation. Experiments on two corpora show an improvement of 2.0-2.3\nBLEU and 0.13-0.44 NIST score, and faster convergence time.", "published": "2016-06-07 02:40:42", "link": "http://arxiv.org/abs/1606.02006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can neural machine translation do simultaneous translation?", "abstract": "We investigate the potential of attention-based neural machine translation in\nsimultaneous translation. We introduce a novel decoding algorithm, called\nsimultaneous greedy decoding, that allows an existing neural machine\ntranslation model to begin translating before a full source sentence is\nreceived. This approach is unique from previous works on simultaneous\ntranslation in that segmentation and translation are done jointly to maximize\nthe translation quality and that translating each segment is strongly\nconditioned on all the previous segments. This paper presents a first step\ntoward building a full simultaneous translation system based on neural machine\ntranslation.", "published": "2016-06-07 03:38:46", "link": "http://arxiv.org/abs/1606.02012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supervised Syntax-based Alignment between English Sentences and Abstract\n  Meaning Representation Graphs", "abstract": "As alignment links are not given between English sentences and Abstract\nMeaning Representation (AMR) graphs in the AMR annotation, automatic alignment\nbecomes indispensable for training an AMR parser. Previous studies formalize it\nas a string-to-string problem and solve it in an unsupervised way, which\nsuffers from data sparseness due to the small size of training data for\nEnglish-AMR alignment. In this paper, we formalize it as a syntax-based\nalignment problem and solve it in a supervised manner based on syntax trees,\nwhich can address the data sparseness problem by generalizing English-AMR\ntokens to syntax tags. Experiments verify the effectiveness of the proposed\nmethod not only for English-AMR alignment, but also for AMR parsing.", "published": "2016-06-07 13:00:48", "link": "http://arxiv.org/abs/1606.02126v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Comprehension with the EpiReader", "abstract": "We present the EpiReader, a novel model for machine comprehension of text.\nMachine comprehension of unstructured, real-world text is a major research goal\nfor natural language processing. Current tests of machine comprehension pose\nquestions whose answers can be inferred from some supporting text, and evaluate\na model's response to the questions. The EpiReader is an end-to-end neural\nmodel comprising two components: the first component proposes a small set of\ncandidate answers after comparing a question to its supporting text, and the\nsecond component formulates hypotheses using the proposed candidates and the\nquestion, then reranks the hypotheses based on their estimated concordance with\nthe supporting text. We present experiments demonstrating that the EpiReader\nsets a new state-of-the-art on the CNN and Children's Book Test machine\ncomprehension benchmarks, outperforming previous neural models by a significant\nmargin.", "published": "2016-06-07 19:27:04", "link": "http://arxiv.org/abs/1606.02270v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Spectral Learning for Parsing", "abstract": "We describe a search algorithm for optimizing the number of latent states\nwhen estimating latent-variable PCFGs with spectral methods. Our results show\nthat contrary to the common belief that the number of latent states for each\nnonterminal in an L-PCFG can be decided in isolation with spectral methods,\nparsing results significantly improve if the number of latent states for each\nnonterminal is globally optimized, while taking into account interactions\nbetween the different nonterminals. In addition, we contribute an empirical\nanalysis of spectral algorithms on eight morphologically rich languages:\nBasque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our\nresults show that our estimation consistently performs better or close to\ncoarse-to-fine expectation-maximization techniques for these languages.", "published": "2016-06-07 21:58:41", "link": "http://arxiv.org/abs/1606.02342v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Alternating Neural Attention for Machine Reading", "abstract": "We propose a novel neural attention architecture to tackle machine\ncomprehension tasks, such as answering Cloze-style queries with respect to a\ndocument. Unlike previous models, we do not collapse the query into a single\nvector, instead we deploy an iterative alternating attention mechanism that\nallows a fine-grained exploration of both the query and the document. Our model\noutperforms state-of-the-art baselines in standard machine comprehension\nbenchmarks such as CNN news articles and the Children's Book Test (CBT)\ndataset.", "published": "2016-06-07 18:25:48", "link": "http://arxiv.org/abs/1606.02245v4", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Multilingual Visual Sentiment Concept Matching", "abstract": "The impact of culture in visual emotion perception has recently captured the\nattention of multimedia research. In this study, we pro- vide powerful\ncomputational linguistics tools to explore, retrieve and browse a dataset of\n16K multilingual affective visual concepts and 7.3M Flickr images. First, we\ndesign an effective crowdsourc- ing experiment to collect human judgements of\nsentiment connected to the visual concepts. We then use word embeddings to\nrepre- sent these concepts in a low dimensional vector space, allowing us to\nexpand the meaning around concepts, and thus enabling insight about\ncommonalities and differences among different languages. We compare a variety\nof concept representations through a novel evaluation task based on the notion\nof visual semantic relatedness. Based on these representations, we design\nclustering schemes to group multilingual visual concepts, and evaluate them\nwith novel metrics based on the crowdsourced sentiment annotations as well as\nvisual semantic relatedness. The proposed clustering framework enables us to\nanalyze the full multilingual dataset in-depth and also show an application on\na facial data subset, exploring cultural in- sights of portrait-related\naffective visual concepts.", "published": "2016-06-07 19:40:00", "link": "http://arxiv.org/abs/1606.02276v1", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
