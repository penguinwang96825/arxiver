{"title": "Adversarial Domain Adaptation for Duplicate Question Detection", "abstract": "We address the problem of detecting duplicate questions in forums, which is\nan important step towards automating the process of answering new questions. As\nfinding and annotating such potential duplicates manually is very tedious and\ncostly, automatic methods based on machine learning are a viable alternative.\nHowever, many forums do not have annotated data, i.e., questions labeled by\nexperts as duplicates, and thus a promising solution is to use domain\nadaptation from another forum that has such annotations. Here we focus on\nadversarial domain adaptation, deriving important findings about when it\nperforms well and what properties of the domains are important in this regard.\nOur experiments with StackExchange data show an average improvement of 5.6%\nover the best baseline across multiple pairs of domains.", "published": "2018-09-07 00:00:39", "link": "http://arxiv.org/abs/1809.02255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Source Domain Adaptation with Mixture of Experts", "abstract": "We propose a mixture-of-experts approach for unsupervised domain adaptation\nfrom multiple sources. The key idea is to explicitly capture the relationship\nbetween a target example and different source domains. This relationship,\nexpressed by a point-to-set metric, determines how to combine predictors\ntrained on various domains. The metric is learned in an unsupervised fashion\nusing meta-training. Experimental results on sentiment analysis and\npart-of-speech tagging demonstrate that our approach consistently outperforms\nmultiple baselines and can robustly handle negative transfer.", "published": "2018-09-07 00:01:42", "link": "http://arxiv.org/abs/1809.02256v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cell-aware Stacked LSTMs for Modeling Sentences", "abstract": "We propose a method of stacking multiple long short-term memory (LSTM) layers\nfor modeling sentences. In contrast to the conventional stacked LSTMs where\nonly hidden states are fed as input to the next layer, the suggested\narchitecture accepts both hidden and memory cell states of the preceding layer\nand fuses information from the left and the lower context using the soft gating\nmechanism of LSTMs. Thus the architecture modulates the amount of information\nto be delivered not only in horizontal recurrence but also in vertical\nconnections, from which useful features extracted from lower layers are\neffectively conveyed to upper layers. We dub this architecture Cell-aware\nStacked LSTM (CAS-LSTM) and show from experiments that our models bring\nsignificant performance gain over the standard LSTMs on benchmark datasets for\nnatural language inference, paraphrase detection, sentiment classification, and\nmachine translation. We also conduct extensive qualitative analysis to\nunderstand the internal behavior of the suggested approach.", "published": "2018-09-07 02:17:23", "link": "http://arxiv.org/abs/1809.02279v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Compositionality in Recursive Neural Networks with\n  Structure-aware Tag Representations", "abstract": "Most existing recursive neural network (RvNN) architectures utilize only the\nstructure of parse trees, ignoring syntactic tags which are provided as\nby-products of parsing. We present a novel RvNN architecture that can provide\ndynamic compositionality by considering comprehensive syntactic information\nderived from both the structure and linguistic tags. Specifically, we introduce\na structure-aware tag representation constructed by a separate tag-level\ntree-LSTM. With this, we can control the composition function of the existing\nword-level tree-LSTM by augmenting the representation as a supplementary input\nto the gate functions of the tree-LSTM. In extensive experiments, we show that\nmodels built upon the proposed architecture obtain superior or competitive\nperformance on several sentence-level tasks such as sentiment analysis and\nnatural language inference when compared against previous tree-structured\nmodels and other sophisticated neural models.", "published": "2018-09-07 02:59:42", "link": "http://arxiv.org/abs/1809.02286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Spoken Language Understanding via Joint\n  Variational Generation", "abstract": "Data scarcity is one of the main obstacles of domain adaptation in spoken\nlanguage understanding (SLU) due to the high cost of creating manually tagged\nSLU datasets. Recent works in neural text generative models, particularly\nlatent variable models such as variational autoencoder (VAE), have shown\npromising results in regards to generating plausible and natural sentences. In\nthis paper, we propose a novel generative architecture which leverages the\ngenerative power of latent variable models to jointly synthesize fully\nannotated utterances. Our experiments show that existing SLU models trained on\nthe additional synthetic examples achieve performance gains. Our approach not\nonly helps alleviate the data scarcity issue in the SLU task for many datasets\nbut also indiscriminately improves language understanding performances for\nvarious SLU models, supported by extensive experiments and rigorous statistical\ntesting.", "published": "2018-09-07 04:17:06", "link": "http://arxiv.org/abs/1809.02305v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask and Multilingual Modelling for Lexical Analysis", "abstract": "In Natural Language Processing (NLP), one traditionally considers a single\ntask (e.g. part-of-speech tagging) for a single language (e.g. English) at a\ntime. However, recent work has shown that it can be beneficial to take\nadvantage of relatedness between tasks, as well as between languages. In this\nwork I examine the concept of relatedness and explore how it can be utilised to\nbuild NLP models that require less manually annotated data. A large selection\nof NLP tasks is investigated for a substantial language sample comprising 60\nlanguages. The results show potential for joint multitask and multilingual\nmodelling, and hints at linguistic insights which can be gained from such\nmodels.", "published": "2018-09-07 12:07:59", "link": "http://arxiv.org/abs/1809.02428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Sparse Semantic Embeddings Learned from Multimodal Text and Image\n  Data to Model Human Conceptual Knowledge", "abstract": "Distributional models provide a convenient way to model semantics using dense\nembedding spaces derived from unsupervised learning algorithms. However, the\ndimensions of dense embedding spaces are not designed to resemble human\nsemantic knowledge. Moreover, embeddings are often built from a single source\nof information (typically text data), even though neurocognitive research\nsuggests that semantics is deeply linked to both language and perception. In\nthis paper, we combine multimodal information from both text and image-based\nrepresentations derived from state-of-the-art distributional models to produce\nsparse, interpretable vectors using Joint Non-Negative Sparse Embedding.\nThrough in-depth analyses comparing these sparse models to human-derived\nbehavioural and neuroimaging data, we demonstrate their ability to predict\ninterpretable linguistic descriptions of human ground-truth semantic knowledge.", "published": "2018-09-07 15:22:04", "link": "http://arxiv.org/abs/1809.02534v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sentence Compression using Denoising Auto-Encoders", "abstract": "In sentence compression, the task of shortening sentences while retaining the\noriginal meaning, models tend to be trained on large corpora containing pairs\nof verbose and compressed sentences. To remove the need for paired corpora, we\nemulate a summarization task and add noise to extend sentences and train a\ndenoising auto-encoder to recover the original, constructing an end-to-end\ntraining regime without the need for any examples of compressed sentences. We\nconduct a human evaluation of our model on a standard text summarization\ndataset and show that it performs comparably to a supervised baseline based on\ngrammatical correctness and retention of meaning. Despite being exposed to no\ntarget data, our unsupervised models learn to generate imperfect but reasonably\nreadable sentence summaries. Although we underperform supervised models based\non ROUGE scores, our models are competitive with a supervised baseline based on\nhuman evaluation for grammatical correctness and retention of meaning.", "published": "2018-09-07 20:56:33", "link": "http://arxiv.org/abs/1809.02669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation of Logographic Languages Using Sub-character\n  Level Information", "abstract": "Recent neural machine translation (NMT) systems have been greatly improved by\nencoder-decoder models with attention mechanisms and sub-word units. However,\nimportant differences between languages with logographic and alphabetic writing\nsystems have long been overlooked. This study focuses on these differences and\nuses a simple approach to improve the performance of NMT systems utilizing\ndecomposed sub-character level information for logographic languages. Our\nresults indicate that our approach not only improves the translation\ncapabilities of NMT systems between Chinese and English, but also further\nimproves NMT systems between Chinese and Japanese, because it utilizes the\nshared information brought by similar sub-character units.", "published": "2018-09-07 22:02:43", "link": "http://arxiv.org/abs/1809.02694v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textual Analogy Parsing: What's Shared and What's Compared among\n  Analogous Facts", "abstract": "To understand a sentence like \"whereas only 10% of White Americans live at or\nbelow the poverty line, 28% of African Americans do\" it is important not only\nto identify individual facts, e.g., poverty rates of distinct demographic\ngroups, but also the higher-order relations between them, e.g., the disparity\nbetween them. In this paper, we propose the task of Textual Analogy Parsing\n(TAP) to model this higher-order meaning. The output of TAP is a frame-style\nmeaning representation which explicitly specifies what is shared (e.g., poverty\nrates) and what is compared (e.g., White Americans vs. African Americans, 10%\nvs. 28%) between its component facts. Such a meaning representation can enable\nnew applications that rely on discourse understanding such as automated chart\ngeneration from quantitative text. We present a new dataset for TAP, baselines,\nand a model that successfully uses an ILP to enforce the structural constraints\nof the problem.", "published": "2018-09-07 22:22:26", "link": "http://arxiv.org/abs/1809.02700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trick Me If You Can: Human-in-the-loop Generation of Adversarial\n  Examples for Question Answering", "abstract": "Adversarial evaluation stress tests a model's understanding of natural\nlanguage. While past approaches expose superficial patterns, the resulting\nadversarial examples are limited in complexity and diversity. We propose\nhuman-in-the-loop adversarial generation, where human authors are guided to\nbreak models. We aid the authors with interpretations of model predictions\nthrough an interactive user interface. We apply this generation framework to a\nquestion answering task called Quizbowl, where trivia enthusiasts craft\nadversarial questions. The resulting questions are validated via live\nhuman--computer matches: although the questions appear ordinary to humans, they\nsystematically stump neural and information retrieval models. The adversarial\nquestions cover diverse phenomena from multi-hop reasoning to entity type\ndistractors, exposing open challenges in robust question answering.", "published": "2018-09-07 22:39:33", "link": "http://arxiv.org/abs/1809.02701v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meteorologists and Students: A resource for language grounding of\n  geographical descriptors", "abstract": "We present a data resource which can be useful for research purposes on\nlanguage grounding tasks in the context of geographical referring expression\ngeneration. The resource is composed of two data sets that encompass 25\ndifferent geographical descriptors and a set of associated graphical\nrepresentations, drawn as polygons on a map by two groups of human subjects:\nteenage students and expert meteorologists.", "published": "2018-09-07 14:20:32", "link": "http://arxiv.org/abs/1809.02494v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Logographic Subword Model for Neural Machine Translation", "abstract": "A novel logographic subword model is proposed to reinterpret logograms as\nabstract subwords for neural machine translation. Our approach drastically\nreduces the size of an artificial neural network, while maintaining comparable\nBLEU scores as those attained with the baseline RNN and CNN seq2seq models. The\nsmaller model size also leads to shorter training and inference time.\nExperiments demonstrate that in the tasks of English-Chinese/Chinese-English\ntranslation, the reduction of those aspects can be from $11\\%$ to as high as\n$77\\%$. Compared to previous subword models, abstract subwords can be applied\nto various logographic languages. Considering most of the logographic languages\nare ancient and very low resource languages, these advantages are very\ndesirable for archaeological computational linguistic applications such as a\nresource-limited offline hand-held Demotic-English translator.", "published": "2018-09-07 17:34:34", "link": "http://arxiv.org/abs/1809.02592v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Transfer-Learnable Natural Language Interface for Databases", "abstract": "Relational database management systems (RDBMSs) are powerful because they are\nable to optimize and answer queries against any relational database. A natural\nlanguage interface (NLI) for a database, on the other hand, is tailored to\nsupport that specific database. In this work, we introduce a general purpose\ntransfer-learnable NLI with the goal of learning one model that can be used as\nNLI for any relational database. We adopt the data management principle of\nseparating data and its schema, but with the additional support for the\nidiosyncrasy and complexity of natural languages. Specifically, we introduce an\nautomatic annotation mechanism that separates the schema and the data, where\nthe schema also covers knowledge about natural language. Furthermore, we\npropose a customized sequence model that translates annotated natural language\nqueries to SQL statements. We show in experiments that our approach outperforms\nprevious NLI methods on the WikiSQL dataset and the model we learned can be\napplied to another benchmark dataset OVERNIGHT without retraining.", "published": "2018-09-07 19:38:51", "link": "http://arxiv.org/abs/1809.02649v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Coherence-Aware Neural Topic Modeling", "abstract": "Topic models are evaluated based on their ability to describe documents well\n(i.e. low perplexity) and to produce topics that carry coherent semantic\nmeaning. In topic modeling so far, perplexity is a direct optimization target.\nHowever, topic coherence, owing to its challenging computation, is not\noptimized for and is only evaluated after training. In this work, under a\nneural variational inference framework, we propose methods to incorporate a\ntopic coherence objective into the training process. We demonstrate that such a\ncoherence-aware topic model exhibits a similar level of perplexity as baseline\nmodels but achieves substantially higher topic coherence.", "published": "2018-09-07 21:43:30", "link": "http://arxiv.org/abs/1809.02687v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What If We Simply Swap the Two Text Fragments? A Straightforward yet\n  Effective Way to Test the Robustness of Methods to Confounding Signals in\n  Nature Language Inference Tasks", "abstract": "Nature language inference (NLI) task is a predictive task of determining the\ninference relationship of a pair of natural language sentences. With the\nincreasing popularity of NLI, many state-of-the-art predictive models have been\nproposed with impressive performances. However, several works have noticed the\nstatistical irregularities in the collected NLI data set that may result in an\nover-estimated performance of these models and proposed remedies. In this\npaper, we further investigate the statistical irregularities, what we refer as\nconfounding factors, of the NLI data sets. With the belief that some NLI labels\nshould preserve under swapping operations, we propose a simple yet effective\nway (swapping the two text fragments) of evaluating the NLI predictive models\nthat naturally mitigate the observed problems. Further, we continue to train\nthe predictive models with our swapping manner and propose to use the deviation\nof the model's evaluation performances under different percentages of training\ntext fragments to be swapped to describe the robustness of a predictive model.\nOur evaluation metrics leads to some interesting understandings of recent\npublished NLI methods. Finally, we also apply the swapping operation on NLI\nmodels to see the effectiveness of this straightforward method in mitigating\nthe confounding factor problems in training generic sentence embeddings for\nother NLP transfer tasks.", "published": "2018-09-07 23:59:22", "link": "http://arxiv.org/abs/1809.02719v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Cross-lingual Word Embedding by Multilingual Neural\n  Language Models", "abstract": "We propose an unsupervised method to obtain cross-lingual embeddings without\nany parallel data or pre-trained word embeddings. The proposed model, which we\ncall multilingual neural language models, takes sentences of multiple languages\nas an input. The proposed model contains bidirectional LSTMs that perform as\nforward and backward language models, and these networks are shared among all\nthe languages. The other parameters, i.e. word embeddings and linear\ntransformation between hidden states and outputs, are specific to each\nlanguage. The shared LSTMs can capture the common sentence structure among all\nlanguages. Accordingly, word embeddings of each language are mapped into a\ncommon latent space, making it possible to measure the similarity of words\nacross multiple languages. We evaluate the quality of the cross-lingual word\nembeddings on a word alignment task. Our experiments demonstrate that our model\ncan obtain cross-lingual embeddings of much higher quality than existing\nunsupervised models when only a small amount of monolingual data (i.e. 50k\nsentences) are available, or the domains of monolingual data are different\nacross languages.", "published": "2018-09-07 04:17:40", "link": "http://arxiv.org/abs/1809.02306v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting local and global performance of candidate systems for\n  aggregation of summarization techniques", "abstract": "With an ever growing number of extractive summarization techniques being\nproposed, there is less clarity then ever about how good each system is\ncompared to the rest. Several studies highlight the variance in performance of\nthese systems with change in datasets or even across documents within the same\ncorpus. An effective way to counter this variance and to make the systems more\nrobust could be to use inputs from multiple systems when generating a summary.\nIn the present work, we define a novel way of creating such ensemble by\nexploiting similarity between the content of candidate summaries to estimate\ntheir reliability. We define GlobalRank which captures the performance of a\ncandidate system on an overall corpus and LocalRank which estimates its\nperformance on a given document cluster. We then use these two scores to assign\na weight to each individual systems, which is then used to generate the new\naggregate ranking. Experiments on DUC2003 and DUC 2004 datasets show a\nsignificant improvement in terms of ROUGE score, over existing sate-of-art\ntechniques.", "published": "2018-09-07 08:18:01", "link": "http://arxiv.org/abs/1809.02343v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Improving Neural Question Generation using Answer Separation", "abstract": "Neural question generation (NQG) is the task of generating a question from a\ngiven passage with deep neural networks. Previous NQG models suffer from a\nproblem that a significant proportion of the generated questions include words\nin the question target, resulting in the generation of unintended questions. In\nthis paper, we propose answer-separated seq2seq, which better utilizes the\ninformation from both the passage and the target answer. By replacing the\ntarget answer in the original passage with a special token, our model learns to\nidentify which interrogative word should be used. We also propose a new module\ntermed keyword-net, which helps the model better capture the key information in\nthe target answer and generate an appropriate question. Experimental results\ndemonstrate that our answer separation method significantly reduces the number\nof improper questions which include answers. Consequently, our model\nsignificantly outperforms previous state-of-the-art NQG models.", "published": "2018-09-07 10:35:42", "link": "http://arxiv.org/abs/1809.02393v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Neural Generation of Diverse Questions using Answer Focus, Contextual\n  and Linguistic Features", "abstract": "Question Generation is the task of automatically creating questions from\ntextual input. In this work we present a new Attentional Encoder--Decoder\nRecurrent Neural Network model for automatic question generation. Our model\nincorporates linguistic features and an additional sentence embedding to\ncapture meaning at both sentence and word levels. The linguistic features are\ndesigned to capture information related to named entity recognition, word case,\nand entity coreference resolution. In addition our model uses a copying\nmechanism and a special answer signal that enables generation of numerous\ndiverse questions on a given sentence. Our model achieves state of the art\nresults of 19.98 Bleu_4 on a benchmark Question Generation dataset,\noutperforming all previously published results by a significant margin. A human\nevaluation also shows that these added features improve the quality of the\ngenerated questions.", "published": "2018-09-07 18:47:20", "link": "http://arxiv.org/abs/1809.02637v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Generation of Spatial Audio for 360 Video", "abstract": "We introduce an approach to convert mono audio recorded by a 360 video camera\ninto spatial audio, a representation of the distribution of sound over the full\nviewing sphere. Spatial audio is an important component of immersive 360 video\nviewing, but spatial audio microphones are still rare in current 360 video\nproduction. Our system consists of end-to-end trainable neural networks that\nseparate individual sound sources and localize them on the viewing sphere,\nconditioned on multi-modal analysis of audio and 360 video frames. We introduce\nseveral datasets, including one filmed ourselves, and one collected in-the-wild\nfrom YouTube, consisting of 360 videos uploaded with spatial audio. During\ntraining, ground-truth spatial audio serves as self-supervision and a mixed\ndown mono track forms the input to our network. Using our approach, we show\nthat it is possible to infer the spatial location of sound sources based only\non 360 video and a mono audio track.", "published": "2018-09-07 17:25:59", "link": "http://arxiv.org/abs/1809.02587v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
