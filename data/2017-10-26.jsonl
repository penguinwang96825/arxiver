{"title": "ALL-IN-1: Short Text Classification with One Model for All Languages", "abstract": "We present ALL-IN-1, a simple model for multilingual text classification that\ndoes not require any parallel data. It is based on a traditional Support Vector\nMachine classifier exploiting multilingual word embeddings and character\nn-grams. Our model is simple, easily extendable yet very effective, overall\nranking 1st (out of 12 teams) in the IJCNLP 2017 shared task on customer\nfeedback analysis in four languages: English, French, Japanese and Spanish.", "published": "2017-10-26 08:41:50", "link": "http://arxiv.org/abs/1710.09589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Streaming Small-Footprint Keyword Spotting using Sequence-to-Sequence\n  Models", "abstract": "We develop streaming keyword spotting systems using a recurrent neural\nnetwork transducer (RNN-T) model: an all-neural, end-to-end trained,\nsequence-to-sequence model which jointly learns acoustic and language model\ncomponents. Our models are trained to predict either phonemes or graphemes as\nsubword units, thus allowing us to detect arbitrary keyword phrases, without\nany out-of-vocabulary words. In order to adapt the models to the requirements\nof keyword spotting, we propose a novel technique which biases the RNN-T system\ntowards a specific keyword of interest.\n  Our systems are compared against a strong sequence-trained, connectionist\ntemporal classification (CTC) based \"keyword-filler\" baseline, which is\naugmented with a separate phoneme language model. Overall, our RNN-T system\nwith the proposed biasing technique significantly improves performance over the\nbaseline system.", "published": "2017-10-26 09:50:16", "link": "http://arxiv.org/abs/1710.09617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of Coreference Resolution on Slot Filling", "abstract": "In this paper, we demonstrate the importance of coreference resolution for\nnatural language processing on the example of the TAC Slot Filling shared task.\nWe illustrate the strengths and weaknesses of automatic coreference resolution\nsystems and provide experimental results to show that they improve performance\nin the slot filling end-to-end setting. Finally, we publish KBPchains, a\nresource containing automatically extracted coreference chains from the TAC\nsource corpus in order to support other researchers working on this topic.", "published": "2017-10-26 15:25:37", "link": "http://arxiv.org/abs/1710.09753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CANDiS: Coupled & Attention-Driven Neural Distant Supervision", "abstract": "Distant Supervision for Relation Extraction uses heuristically aligned text\ndata with an existing knowledge base as training data. The unsupervised nature\nof this technique allows it to scale to web-scale relation extraction tasks, at\nthe expense of noise in the training data. Previous work has explored\nrelationships among instances of the same entity-pair to reduce this noise, but\nrelationships among instances across entity-pairs have not been fully\nexploited. We explore the use of inter-instance couplings based on verb-phrase\nand entity type similarities. We propose a novel technique, CANDiS, which casts\ndistant supervision using inter-instance coupling into an end-to-end neural\nnetwork model. CANDiS incorporates an attention module at the instance-level to\nmodel the multi-instance nature of this problem. CANDiS outperforms existing\nstate-of-the-art techniques on a standard benchmark dataset.", "published": "2017-10-26 23:16:31", "link": "http://arxiv.org/abs/1710.09942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Negative Sampling for Word Representation using Self-embedded\n  Features", "abstract": "Although the word-popularity based negative sampler has shown superb\nperformance in the skip-gram model, the theoretical motivation behind\noversampling popular (non-observed) words as negative samples is still not well\nunderstood. In this paper, we start from an investigation of the gradient\nvanishing issue in the skipgram model without a proper negative sampler. By\nperforming an insightful analysis from the stochastic gradient descent (SGD)\nlearning perspective, we demonstrate that, both theoretically and intuitively,\nnegative samples with larger inner product scores are more informative than\nthose with lower scores for the SGD learner in terms of both convergence rate\nand accuracy. Understanding this, we propose an alternative sampling algorithm\nthat dynamically selects informative negative samples during each SGD update.\nMore importantly, the proposed sampler accounts for multi-dimensional\nself-embedded features during the sampling process, which essentially makes it\nmore effective than the original popularity-based (one-dimensional) sampler.\nEmpirical experiments further verify our observations, and show that our\nfine-grained samplers gain significant improvement over the existing ones\nwithout increasing computational complexity.", "published": "2017-10-26 16:54:13", "link": "http://arxiv.org/abs/1710.09805v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Understanding Early Word Learning in Situated Artificial Agents", "abstract": "Neural network-based systems can now learn to locate the referents of words\nand phrases in images, answer questions about visual scenes, and execute\nsymbolic instructions as first-person actors in partially-observable worlds. To\nachieve this so-called grounded language learning, models must overcome\nchallenges that infants face when learning their first words. While it is\nnotable that models with no meaningful prior knowledge overcome these\nobstacles, researchers currently lack a clear understanding of how they do so,\na problem that we attempt to address in this paper. For maximum control and\ngenerality, we focus on a simple neural network-based language learning agent,\ntrained via policy-gradient methods, which can interpret single-word\ninstructions in a simulated 3D world. Whilst the goal is not to explicitly\nmodel infant word learning, we take inspiration from experimental paradigms in\ndevelopmental psychology and apply some of these to the artificial agent,\nexploring the conditions under which established human biases and learning\neffects emerge. We further propose a novel method for visualising semantic\nrepresentations in the agent.", "published": "2017-10-26 18:48:20", "link": "http://arxiv.org/abs/1710.09867v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Lip2AudSpec: Speech reconstruction from silent lip movements video", "abstract": "In this study, we propose a deep neural network for reconstructing\nintelligible speech from silent lip movement videos. We use auditory\nspectrogram as spectral representation of speech and its corresponding sound\ngeneration method resulting in a more natural sounding reconstructed speech.\nOur proposed network consists of an autoencoder to extract bottleneck features\nfrom the auditory spectrogram which is then used as target to our main lip\nreading network comprising of CNN, LSTM and fully connected layers. Our\nexperiments show that the autoencoder is able to reconstruct the original\nauditory spectrogram with a 98% correlation and also improves the quality of\nreconstructed speech from the main lip reading network. Our model, trained\njointly on different speakers is able to extract individual speaker\ncharacteristics and gives promising results of reconstructing intelligible\nspeech with superior word recognition accuracy.", "published": "2017-10-26 16:39:05", "link": "http://arxiv.org/abs/1710.09798v1", "categories": ["cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
