{"title": "Enhance Long Text Understanding via Distilled Gist Detector from\n  Abstractive Summarization", "abstract": "Long text understanding is important yet challenging in natural language\nprocessing. A long article or essay usually contains many redundant words that\nare not pertinent to its gist and sometimes can be regarded as noise. In this\npaper, we consider the problem of how to disentangle the gist-relevant and\nirrelevant information for long text understanding. With distillation\nmechanism, we transfer the knowledge about how to focus the salient parts from\nthe abstractive summarization model and further integrate the distilled model,\nnamed \\emph{Gist Detector}, into existing models as a supplementary component\nto augment the long text understanding. Experiments on document classification,\ndistantly supervised open-domain question answering (DS-QA) and non-parallel\ntext style transfer show that our method can significantly improve the\nperformance of the baseline models, and achieves state-of-the-art overall\nresults for document classification.", "published": "2021-10-10 09:21:24", "link": "http://arxiv.org/abs/2110.04741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PASTE: A Tagging-Free Decoding Framework Using Pointer Networks for\n  Aspect Sentiment Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) deals with extracting opinion\ntriplets, consisting of an opinion target or aspect, its associated sentiment,\nand the corresponding opinion term/span explaining the rationale behind the\nsentiment. Existing research efforts are majorly tagging-based. Among the\nmethods taking a sequence tagging approach, some fail to capture the strong\ninterdependence between the three opinion factors, whereas others fall short of\nidentifying triplets with overlapping aspect/opinion spans. A recent grid\ntagging approach on the other hand fails to capture the span-level semantics\nwhile predicting the sentiment between an aspect-opinion pair. Different from\nthese, we present a tagging-free solution for the task, while addressing the\nlimitations of the existing works. We adapt an encoder-decoder architecture\nwith a Pointer Network-based decoding framework that generates an entire\nopinion triplet at each time step thereby making our solution end-to-end.\nInteractions between the aspects and opinions are effectively captured by the\ndecoder by considering their entire detected spans while predicting their\nconnecting sentiment. Extensive experiments on several benchmark datasets\nestablish the better efficacy of our proposed approach, especially in the\nrecall, and in predicting multiple and aspect/opinion-overlapped triplets from\nthe same review sentence. We report our results both with and without BERT and\nalso demonstrate the utility of domain-specific BERT post-training for the\ntask.", "published": "2021-10-10 13:39:39", "link": "http://arxiv.org/abs/2110.04794v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DCT: Dynamic Compressive Transformer for Modeling Unbounded Sequence", "abstract": "In this paper, we propose Dynamic Compressive Transformer (DCT), a\ntransformer-based framework for modeling the unbounded sequence. In contrast to\nthe previous baselines which append every sentence representation to memory,\nconditionally selecting and appending them is a more reasonable solution to\ndeal with unlimited long sequences. Our model uses a policy that determines\nwhether the sequence should be kept in memory with a compressed state or\ndiscarded during the training process. With the benefits of retaining\nsemantically meaningful sentence information in the memory system, our\nexperiment results on Enwik8 benchmark show that DCT outperforms the previous\nstate-of-the-art (SOTA) model.", "published": "2021-10-10 15:21:19", "link": "http://arxiv.org/abs/2110.04821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes Sentences Semantically Related: A Textual Relatedness Dataset\n  and Empirical Study", "abstract": "The degree of semantic relatedness of two units of language has long been\nconsidered fundamental to understanding meaning. Additionally, automatically\ndetermining relatedness has many applications such as question answering and\nsummarization. However, prior NLP work has largely focused on semantic\nsimilarity, a subset of relatedness, because of a lack of relatedness datasets.\nIn this paper, we introduce a dataset for Semantic Textual Relatedness,\nSTR-2022, that has 5,500 English sentence pairs manually annotated using a\ncomparative annotation framework, resulting in fine-grained scores. We show\nthat human intuition regarding relatedness of sentence pairs is highly\nreliable, with a repeat annotation correlation of 0.84. We use the dataset to\nexplore questions on what makes sentences semantically related. We also show\nthe utility of STR-2022 for evaluating automatic methods of sentence\nrepresentation and for various downstream NLP tasks.\n  Our dataset, data statement, and annotation questionnaire can be found at:\nhttps://doi.org/10.5281/zenodo.7599667", "published": "2021-10-10 16:23:54", "link": "http://arxiv.org/abs/2110.04845v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Text Extractive Summarization Based on Graph and Pre-trained\n  Language Model Attention", "abstract": "Representing a text as a graph for obtaining automatic text summarization has\nbeen investigated for over ten years. With the development of attention or\nTransformer on natural language processing (NLP), it is possible to make a\nconnection between the graph and attention structure for a text. In this paper,\nan attention matrix between the sentences of the whole text is adopted as a\nweighted adjacent matrix of a fully connected graph of the text, which can be\nproduced through the pre-training language model. The GCN is further applied to\nthe text graph model for classifying each node and finding out the salient\nsentences from the text. It is demonstrated by the experimental results on two\ntypical datasets that our proposed model can achieve a competitive result in\ncomparison with sate-of-the-art models.", "published": "2021-10-10 18:49:19", "link": "http://arxiv.org/abs/2110.04878v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distantly-Supervised Evidence Retrieval Enables Question Answering\n  without Evidence Annotation", "abstract": "Open-domain question answering answers a question based on evidence retrieved\nfrom a large corpus. State-of-the-art neural approaches require intermediate\nevidence annotations for training. However, such intermediate annotations are\nexpensive, and methods that rely on them cannot transfer to the more common\nsetting, where only question-answer pairs are available. This paper\ninvestigates whether models can learn to find evidence from a large corpus,\nwith only distant supervision from answer labels for model training, thereby\ngenerating no additional annotation cost. We introduce a novel approach\n(DistDR) that iteratively improves over a weak retriever by alternately finding\nevidence from the up-to-date model and encouraging the model to learn the most\nlikely evidence. Without using any evidence labels, DistDR is on par with\nfully-supervised state-of-the-art methods on both multi-hop and single-hop QA\nbenchmarks. Our analysis confirms that DistDR finds more accurate evidence over\niterations, which leads to model improvements.", "published": "2021-10-10 20:01:27", "link": "http://arxiv.org/abs/2110.04889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SuperShaper: Task-Agnostic Super Pre-training of BERT Models with\n  Variable Hidden Dimensions", "abstract": "Task-agnostic pre-training followed by task-specific fine-tuning is a default\napproach to train NLU models. Such models need to be deployed on devices across\nthe cloud and the edge with varying resource and accuracy constraints. For a\ngiven task, repeating pre-training and fine-tuning across tens of devices is\nprohibitively expensive. We propose SuperShaper, a task agnostic pre-training\napproach which simultaneously pre-trains a large number of Transformer models\nby varying shapes, i.e., by varying the hidden dimensions across layers. This\nis enabled by a backbone network with linear bottleneck matrices around each\nTransformer layer which are sliced to generate differently shaped sub-networks.\nIn spite of its simple design space and efficient implementation, SuperShaper\ndiscovers networks that effectively trade-off accuracy and model size:\nDiscovered networks are more accurate than a range of hand-crafted and\nautomatically searched networks on GLUE benchmarks. Further, we find two\ncritical advantages of shape as a design variable for Neural Architecture\nSearch (NAS): (a) heuristics of good shapes can be derived and networks found\nwith these heuristics match and even improve on carefully searched networks\nacross a range of parameter counts, and (b) the latency of networks across\nmultiple CPUs and GPUs are insensitive to the shape and thus enable\ndevice-agnostic search. In summary, SuperShaper radically simplifies NAS for\nlanguage models and discovers networks that generalize across tasks, parameter\nconstraints, and devices.", "published": "2021-10-10 05:44:02", "link": "http://arxiv.org/abs/2110.04711v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and\n  Few-Shot Learning", "abstract": "Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot\nand Few-Shot learning on many natural language processing (NLP) tasks by\nscaling up model size, dataset size and the amount of computation. However,\ntraining a model like GPT-3 requires huge amount of computational resources\nwhich makes it challengeable to researchers. In this work, we propose a method\nthat incorporates large-scale distributed training performance into model\narchitecture design. With this method, Yuan 1.0, the current largest singleton\nlanguage model with 245B parameters, achieves excellent performance on\nthousands GPUs during training, and the state-of-the-art results on NLP tasks.\nA data processing method is designed to efficiently filter massive amount of\nraw data. The current largest high-quality Chinese corpus with 5TB high quality\ntexts is built based on this method. In addition, a calibration and label\nexpansion method is proposed to improve the Zero-Shot and Few-Shot performance,\nand steady improvement is observed on the accuracy of various tasks. Yuan 1.0\npresents strong capacity of natural language generation, and the generated\narticles are difficult to distinguish from the human-written ones.", "published": "2021-10-10 07:40:22", "link": "http://arxiv.org/abs/2110.04725v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Injecting Text and Cross-lingual Supervision in Few-shot Learning from\n  Self-Supervised Models", "abstract": "Self-supervised model pre-training has recently garnered significant\ninterest, but relatively few efforts have explored using additional resources\nin fine-tuning these models. We demonstrate how universal phoneset acoustic\nmodels can leverage cross-lingual supervision to improve transfer of pretrained\nself-supervised representations to new languages. We also show how\ntarget-language text can be used to enable and improve fine-tuning with the\nlattice-free maximum mutual information (LF-MMI) objective. In three\nlow-resource languages these techniques greatly improved few-shot learning\nperformance.", "published": "2021-10-10 17:33:44", "link": "http://arxiv.org/abs/2110.04863v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "An Analysis of COVID-19 Knowledge Graph Construction and Applications", "abstract": "The construction and application of knowledge graphs have seen a rapid\nincrease across many disciplines in recent years. Additionally, the problem of\nuncovering relationships between developments in the COVID-19 pandemic and\nsocial media behavior is of great interest to researchers hoping to curb the\nspread of the disease. In this paper we present a knowledge graph constructed\nfrom COVID-19 related tweets in the Los Angeles area, supplemented with federal\nand state policy announcements and disease spread statistics. By incorporating\ndates, topics, and events as entities, we construct a knowledge graph that\ndescribes the connections between these useful information. We use natural\nlanguage processing and change point analysis to extract tweet-topic,\ntweet-date, and event-date relations. Further analysis on the constructed\nknowledge graph provides insight into how tweets reflect public sentiments\ntowards COVID-19 related topics and how changes in these sentiments correlate\nwith real-world events.", "published": "2021-10-10 23:58:57", "link": "http://arxiv.org/abs/2110.04932v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Learning to Learn End-to-End Goal-Oriented Dialog From Related Dialog\n  Tasks", "abstract": "For each goal-oriented dialog task of interest, large amounts of data need to\nbe collected for end-to-end learning of a neural dialog system. Collecting that\ndata is a costly and time-consuming process. Instead, we show that we can use\nonly a small amount of data, supplemented with data from a related dialog task.\nNaively learning from related data fails to improve performance as the related\ndata can be inconsistent with the target task. We describe a meta-learning\nbased method that selectively learns from the related dialog task data. Our\napproach leads to significant accuracy improvements in an example dialog task.", "published": "2021-10-10 15:27:45", "link": "http://arxiv.org/abs/2110.15724v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Audio Captions Be Evaluated with Image Caption Metrics?", "abstract": "Automated audio captioning aims at generating textual descriptions for an\naudio clip. To evaluate the quality of generated audio captions, previous works\ndirectly adopt image captioning metrics like SPICE and CIDEr, without\njustifying their suitability in this new domain, which may mislead the\ndevelopment of advanced models. This problem is still unstudied due to the lack\nof human judgment datasets on caption quality. Therefore, we firstly construct\ntwo evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established\nwith pairwise comparison instead of absolute rating to achieve better\ninter-annotator agreement. Current metrics are found in poor correlation with\nhuman annotations on these datasets. To overcome their limitations, we propose\na metric named FENSE, where we combine the strength of Sentence-BERT in\ncapturing similarity, and a novel Error Detector to penalize erroneous\nsentences for robustness. On the newly established benchmarks, FENSE\noutperforms current metrics by 14-25% accuracy. Code, data and web demo\navailable at: https://github.com/blmoistawinde/fense", "published": "2021-10-10 02:34:40", "link": "http://arxiv.org/abs/2110.04684v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Channel End-to-End Neural Diarization with Distributed Microphones", "abstract": "Recent progress on end-to-end neural diarization (EEND) has enabled\noverlap-aware speaker diarization with a single neural network. This paper\nproposes to enhance EEND by using multi-channel signals from distributed\nmicrophones. We replace Transformer encoders in EEND with two types of encoders\nthat process a multi-channel input: spatio-temporal and co-attention encoders.\nBoth are independent of the number and geometry of microphones and suitable for\ndistributed microphone settings. We also propose a model adaptation method\nusing only single-channel recordings. With simulated and real-recorded\ndatasets, we demonstrated that the proposed method outperformed conventional\nEEND when a multi-channel input was given while maintaining comparable\nperformance with a single-channel input. We also showed that the proposed\nmethod performed well even when spatial information is inoperative given\nmulti-channel inputs, such as in hybrid meetings in which the utterances of\nmultiple remote participants are played back from the same loudspeaker.", "published": "2021-10-10 03:24:03", "link": "http://arxiv.org/abs/2110.04694v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards High-fidelity Singing Voice Conversion with Acoustic Reference\n  and Contrastive Predictive Coding", "abstract": "Recently, phonetic posteriorgrams (PPGs) based methods have been quite\npopular in non-parallel singing voice conversion systems. However, due to the\nlack of acoustic information in PPGs, style and naturalness of the converted\nsinging voices are still limited. To solve these problems, in this paper, we\nutilize an acoustic reference encoder to implicitly model singing\ncharacteristics. We experiment with different auxiliary features, including mel\nspectrograms, HuBERT, and the middle hidden feature (PPG-Mid) of pretrained\nautomatic speech recognition (ASR) model, as the input of the reference\nencoder, and finally find the HuBERT feature is the best choice. In addition,\nwe use contrastive predictive coding (CPC) module to further smooth the voices\nby predicting future observations in latent space. Experiments show that,\ncompared with the baseline models, our proposed model can significantly improve\nthe naturalness of converted singing voices and the similarity with the target\nsinger. Moreover, our proposed model can also make the speakers with just\nspeech data sing.", "published": "2021-10-10 10:27:20", "link": "http://arxiv.org/abs/2110.04754v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Language Models As or For Knowledge Bases", "abstract": "Pre-trained language models (LMs) have recently gained attention for their\npotential as an alternative to (or proxy for) explicit knowledge bases (KBs).\nIn this position paper, we examine this hypothesis, identify strengths and\nlimitations of both LMs and KBs, and discuss the complementary nature of the\ntwo paradigms. In particular, we offer qualitative arguments that latent LMs\nare not suitable as a substitute for explicit KBs, but could play a major role\nfor augmenting and curating KBs.", "published": "2021-10-10 20:00:09", "link": "http://arxiv.org/abs/2110.04888v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Have best of both worlds: two-pass hybrid and E2E cascading framework\n  for speech recognition", "abstract": "Hybrid and end-to-end (E2E) systems have their individual advantages, with\ndifferent error patterns in the speech recognition results. By jointly modeling\naudio and text, the E2E model performs better in matched scenarios and scales\nwell with a large amount of paired audio-text training data. The modularized\nhybrid model is easier for customization, and better to make use of a massive\namount of unpaired text data. This paper proposes a two-pass hybrid and E2E\ncascading (HEC) framework to combine the hybrid and E2E model in order to take\nadvantage of both sides, with hybrid in the first pass and E2E in the second\npass. We show that the proposed system achieves 8-10% relative word error rate\nreduction with respect to each individual system. More importantly, compared\nwith the pure E2E system, we show the proposed system has the potential to keep\nthe advantages of hybrid system, e.g., customization and segmentation\ncapabilities. We also show the second pass E2E model in HEC is robust with\nrespect to the change in the first pass hybrid model.", "published": "2021-10-10 20:11:38", "link": "http://arxiv.org/abs/2110.04891v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Are Words the Quanta of Human Language? Extending the Domain of Quantum\n  Cognition", "abstract": "In previous research, we showed that 'texts that tell a story' exhibit a\nstatistical structure that is not Maxwell-Boltzmann but Bose-Einstein. Our\nexplanation is that this is due to the presence of 'indistinguishability' in\nhuman language as a result of the same words in different parts of the story\nbeing indistinguishable from one another. In the current article, we set out to\nprovide an explanation for this Bose-Einstein statistics. We show that it is\nthe presence of 'meaning' in 'stories' that gives rise to the lack of\nindependence characteristic of Bose-Einstein, and provides conclusive evidence\nthat 'words can be considered the quanta of human language', structurally\nsimilar to how 'photons are the quanta of light'. Using several studies on\nentanglement from our Brussels research group, we also show that it is also the\npresence of 'meaning' in texts that makes the von Neumann entropy of a total\ntext smaller relative to the entropy of the words composing it. We explain how\nthe new insights in this article fit in with the research domain called\n'quantum cognition', where quantum probability models and quantum vector spaces\nare used in human cognition, and are also relevant to the use of quantum\nstructures in information retrieval and natural language processing, and how\nthey introduce 'quantization' and 'Bose-Einstein statistics' as relevant\nquantum effects there. Inspired by the conceptuality interpretation of quantum\nmechanics, and relying on the new insights, we put forward hypotheses about the\nnature of physical reality. In doing so, we note how this new type of decrease\nin entropy, and its explanation, may be important for the development of\nquantum thermodynamics. We likewise note how it can also give rise to an\noriginal explanatory picture of the nature of physical reality on the surface\nof planet Earth, in which human culture emerges as a reinforcing continuation\nof life.", "published": "2021-10-10 22:02:06", "link": "http://arxiv.org/abs/2110.04913v2", "categories": ["q-bio.NC", "cs.CL", "quant-ph"], "primary_category": "q-bio.NC"}
{"title": "Deep convolutional forest: a dynamic deep ensemble approach for spam\n  detection in text", "abstract": "The increase in people's use of mobile messaging services has led to the\nspread of social engineering attacks like phishing, considering that spam text\nis one of the main factors in the dissemination of phishing attacks to steal\nsensitive data such as credit cards and passwords. In addition, rumors and\nincorrect medical information regarding the COVID-19 pandemic are widely shared\non social media leading to people's fear and confusion. Thus, filtering spam\ncontent is vital to reduce risks and threats. Previous studies relied on\nmachine learning and deep learning approaches for spam classification, but\nthese approaches have two limitations. Machine learning models require manual\nfeature engineering, whereas deep neural networks require a high computational\ncost. This paper introduces a dynamic deep ensemble model for spam detection\nthat adjusts its complexity and extracts features automatically. The proposed\nmodel utilizes convolutional and pooling layers for feature extraction along\nwith base classifiers such as random forests and extremely randomized trees for\nclassifying texts into spam or legitimate ones. Moreover, the model employs\nensemble learning procedures like boosting and bagging. As a result, the model\nachieved high precision, recall, f1-score and accuracy of 98.38\\%.", "published": "2021-10-10 17:19:37", "link": "http://arxiv.org/abs/2110.15718v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SP-GPT2: Semantics Improvement in Vietnamese Poetry Generation", "abstract": "Automatic text generation has garnered growing attention in recent years as\nan essential step towards computer creativity. Generative Pretraining\nTransformer 2 (GPT2) is one of the state of the art approaches that have\nexcellent successes. In this paper, we took the first step to investigate the\npower of GPT2 in traditional Vietnamese poetry generation. In the earlier time,\nour experiment with base GPT2 was quite good at generating the poem in the\nproper template. Though it can learn the patterns, including rhyme and tone\nrules, from the training data, like almost all other text generation\napproaches, the poems generated still has a topic drift and semantic\ninconsistency. To improve the cohesion within the poems, we proposed a new\nmodel SP-GPT2 (semantic poem GPT2) which was built on the top GPT2 model and an\nadditional loss to constrain context throughout the entire poem. For better\nevaluation, we examined the methods by both automatic quantitative evaluation\nand human evaluation. Both automatic and human evaluation demonstrated that our\napproach can generate poems that have better cohesion without losing the\nquality due to additional loss. At the same time, we are the pioneers of this\ntopic. We released the first computational scoring module for poems generated\nin the template containing the style rule dictionary. Additionally, we are the\nfirst to publish a Luc-Bat dataset, including 87609 Luc Bat poems, which is\nequivalent to about 2.6 million sentences, combined with about 83579 poems in\nother styles was also published for further exploration. The code is available\nat https://github.com/fsoft-ailab/Poem-Generator", "published": "2021-10-10 14:31:08", "link": "http://arxiv.org/abs/2110.15723v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks", "abstract": "The use of contrastive loss for representation learning has become prominent\nin computer vision, and it is now getting attention in Natural Language\nProcessing (NLP). Here, we explore the idea of using a batch-softmax\ncontrastive loss when fine-tuning large-scale pre-trained transformer models to\nlearn better task-specific sentence embeddings for pairwise sentence scoring\ntasks. We introduce and study a number of variations in the calculation of the\nloss as well as in the overall training procedure; in particular, we find that\ndata shuffling can be quite important. Our experimental results show sizable\nimprovements on a number of datasets and pairwise sentence scoring tasks\nincluding classification, ranking, and regression. Finally, we offer detailed\nanalysis and discussion, which should be useful for researchers aiming to\nexplore the utility of contrastive loss in NLP.", "published": "2021-10-10 16:43:44", "link": "http://arxiv.org/abs/2110.15725v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.NE", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Poformer: A simple pooling transformer for speaker verification", "abstract": "Most recent speaker verification systems are based on extracting speaker\nembeddings using a deep neural network. The pooling layer in the network aims\nto aggregate frame-level features extracted by the backbone. In this paper, we\npropose a new transformer based pooling structure called PoFormer to enhance\nthe ability of the pooling layer to capture information along the whole time\naxis. Different from previous works that apply attention mechanism in a simple\nway or implement the multi-head mechanism in serial instead of in parallel,\nPoFormer follows the initial transformer structure with some minor\nmodifications like a positional encoding generator, drop path and LayerScale to\nmake the training procedure more stable and to prevent overfitting. Evaluated\non various datasets, PoFormer outperforms the existing pooling system with at\nleast a 13.00% improvement in EER and a 9.12% improvement in minDCF.", "published": "2021-10-10 03:12:07", "link": "http://arxiv.org/abs/2110.04692v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DITTO: Data-efficient and Fair Targeted Subset Selection for ASR Accent\n  Adaptation", "abstract": "State-of-the-art Automatic Speech Recognition (ASR) systems are known to\nexhibit disparate performance on varying speech accents. To improve performance\non a specific target accent, a commonly adopted solution is to finetune the ASR\nmodel using accent-specific labeled speech. However, acquiring large amounts of\nlabeled speech for specific target accents is challenging. Choosing an\ninformative subset of speech samples that are most representative of the target\naccents becomes important for effective ASR finetuning. To address this\nproblem, we propose DITTO (Data-efficient and faIr Targeted subseT selectiOn)\nthat uses Submodular Mutual Information (SMI) functions as acquisition\nfunctions to find the most informative set of utterances matching a target\naccent within a fixed budget. An important feature of DITTO is that it supports\nfair targeting for multiple accents, i.e. it can automatically select\nrepresentative data points from multiple accents when the ASR model needs to\nperform well on more than one accent. We show that DITTO is 3-5 times more\nlabel-efficient than other speech selection methods on the IndicTTS and L2\ndatasets.", "published": "2021-10-10 21:37:46", "link": "http://arxiv.org/abs/2110.04908v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Overview of Techniques for Biomarker Discovery in Voice Signal", "abstract": "This paper reflects on the effect of several categories of medical conditions\non human voice, focusing on those that may be hypothesized to have effects on\nvoice, but for which the changes themselves may be subtle enough to have eluded\nobservation in standard analytical examinations of the voice signal. It\npresents three categories of techniques that can potentially uncover such\nelusive biomarkers and allow them to be measured and used for predictive and\ndiagnostic purposes. These approaches include proxy techniques, model-based\nanalytical techniques and data-driven AI techniques.", "published": "2021-10-10 01:39:28", "link": "http://arxiv.org/abs/2110.04678v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-task Learning with Metadata for Music Mood Classification", "abstract": "Mood recognition is an important problem in music informatics and has key\napplications in music discovery and recommendation. These applications have\nbecome even more relevant with the rise of music streaming. Our work\ninvestigates the research question of whether we can leverage audio metadata\nsuch as artist and year, which is readily available, to improve the performance\nof mood classification models. To this end, we propose a multi-task learning\napproach in which a shared model is simultaneously trained for mood and\nmetadata prediction tasks with the goal to learn richer representations.\nExperimentally, we demonstrate that applying our technique on the existing\nstate-of-the-art convolutional neural networks for mood classification improves\ntheir performances consistently. We conduct experiments on multiple datasets\nand report that our approach can lead to improvements in the average precision\nmetric by up to 8.7 points.", "published": "2021-10-10 11:36:34", "link": "http://arxiv.org/abs/2110.04765v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Estimating the confidence of speech spoofing countermeasure", "abstract": "Conventional speech spoofing countermeasures (CMs) are designed to make a\nbinary decision on an input trial. However, a CM trained on a closed-set\ndatabase is theoretically not guaranteed to perform well on unknown spoofing\nattacks. In some scenarios, an alternative strategy is to let the CM defer a\ndecision when it is not confident. The question is then how to estimate a CM's\nconfidence regarding an input trial. We investigated a few confidence\nestimators that can be easily plugged into a CM. On the ASVspoof2019 logical\naccess database, the results demonstrate that an energy-based estimator and a\nneural-network-based one achieved acceptable performance in identifying unknown\nattacks in the test set. On a test set with additional unknown attacks and bona\nfide trials from other databases, the confidence estimators performed\nmoderately well, and the CMs better discriminated bona fide and spoofed trials\nthat had a high confidence score. Additional results also revealed the\ndifficulty in enhancing a confidence estimator by adding unknown attacks to the\ntraining set.", "published": "2021-10-10 12:35:05", "link": "http://arxiv.org/abs/2110.04775v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Stepwise-Refining Speech Separation Network via Fine-Grained Encoding in\n  High-order Latent Domain", "abstract": "The crux of single-channel speech separation is how to encode the mixture of\nsignals into such a latent embedding space that the signals from different\nspeakers can be precisely separated. Existing methods for speech separation\neither transform the speech signals into frequency domain to perform separation\nor seek to learn a separable embedding space by constructing a latent domain\nbased on convolutional filters. While the latter type of methods learning an\nembedding space achieves substantial improvement for speech separation, we\nargue that the embedding space defined by only one latent domain does not\nsuffice to provide a thoroughly separable encoding space for speech separation.\nIn this paper, we propose the Stepwise-Refining Speech Separation Network\n(SRSSN), which follows a coarse-to-fine separation framework. It first learns a\n1-order latent domain to define an encoding space and thereby performs a rough\nseparation in the coarse phase. Then the proposed SRSSN learns a new latent\ndomain along each basis function of the existing latent domain to obtain a\nhigh-order latent domain in the refining phase, which enables our model to\nperform a refining separation to achieve a more precise speech separation. We\ndemonstrate the effectiveness of our SRSSN by conducting extensive experiments,\nincluding speech separation in a clean (noise-free) setting on WSJ0-2/3mix\ndatasets as well as in noisy/reverberant settings on WHAM!/WHAMR! datasets.\nFurthermore, we also perform experiments of speech recognition on separated\nspeech signals by our model to evaluate the performance of speech separation\nindirectly.", "published": "2021-10-10 13:21:16", "link": "http://arxiv.org/abs/2110.04791v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Direct source and early reflections localization using deep\n  deconvolution network under reverberant environment", "abstract": "This paper proposes a deconvolution-based network (DCNN) model for DOA\nestimation of direct source and early reflections under reverberant scenarios.\nConsidering that the first-order reflections of the sound source also contain\nspatial directivity like the direct source, we treat both of them as the\nsources in the learning process. We use the covariance matrix of high order\nAmbisonics (HOA) signals in the time domain as the input feature of the\nnetwork, which is concise while containing precise spatial information under\nreverberant scenarios. Besides, we use the deconvolution-based network for the\nspatial pseudo-spectrum (SPS) reconstruction in the 2D polar space, based on\nwhich the spatial relationship between elevation and azimuth can be depicted.\nWe have carried out a series of experiments based on simulated and measured\ndata under different reverberant scenarios, which prove the robustness and\naccuracy of the proposed DCNN model.", "published": "2021-10-10 16:56:07", "link": "http://arxiv.org/abs/2110.04850v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Crack detection using tap-testing and machine learning techniques to\n  prevent potential rockfall incidents", "abstract": "Rockfalls are a hazard for the safety of infrastructure as well as people.\nIdentifying loose rocks by inspection of slopes adjacent to roadways and other\ninfrastructure and removing them in advance can be an effective way to prevent\nunexpected rockfall incidents. This paper proposes a system towards an\nautomated inspection for potential rockfalls. A robot is used to repeatedly\nstrike or tap on the rock surface. The sound from the tapping is collected by\nthe robot and subsequently classified with the intent of identifying rocks that\nare broken and prone to fall. Principal Component Analysis (PCA) of the\ncollected acoustic data is used to recognize patterns associated with rocks of\nvarious conditions, including intact as well as rock with different types and\nlocations of cracks. The PCA classification was first demonstrated simulating\nsounds of different characteristics that were automatically trained and tested.\nSecondly, a laboratory test was conducted tapping rock specimens with three\ndifferent levels of discontinuity in depth and shape. A real microphone mounted\non the robot recorded the sound and the data were classified in three clusters\nwithin 2D space. A model was created using the training data to classify the\nreminder of the data (the test data). The performance of the method is\nevaluated with a confusion matrix.", "published": "2021-10-10 22:53:36", "link": "http://arxiv.org/abs/2110.04923v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Denoising Diffusion Gamma Models", "abstract": "Generative diffusion processes are an emerging and effective tool for image\nand speech generation. In the existing methods, the underlying noise\ndistribution of the diffusion process is Gaussian noise. However, fitting\ndistributions with more degrees of freedom could improve the performance of\nsuch generative models. In this work, we investigate other types of noise\ndistribution for the diffusion process. Specifically, we introduce the\nDenoising Diffusion Gamma Model (DDGM) and show that noise from Gamma\ndistribution provides improved results for image and speech generation. Our\napproach preserves the ability to efficiently sample state in the training\ndiffusion process while using Gamma noise.", "published": "2021-10-10 10:46:31", "link": "http://arxiv.org/abs/2110.05948v1", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.GR", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "eess.SP"}
