{"title": "A Sub-Character Architecture for Korean Language Processing", "abstract": "We introduce a novel sub-character architecture that exploits a unique\ncompositional structure of the Korean language. Our method decomposes each\ncharacter into a small set of primitive phonetic units called jamo letters from\nwhich character- and word-level representations are induced. The jamo letters\ndivulge syntactic and semantic information that is difficult to access with\nconventional character-level units. They greatly alleviate the data sparsity\nproblem, reducing the observation space to 1.6% of the original while\nincreasing accuracy in our experiments. We apply our architecture to dependency\nparsing and achieve dramatic improvement over strong lexical baselines.", "published": "2017-07-20 02:09:23", "link": "http://arxiv.org/abs/1707.06341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Discourse Relation Projection to Build Discourse Annotated\n  Corpora", "abstract": "The naive approach to annotation projection is not effective to project\ndiscourse annotations from one language to another because implicit discourse\nrelations are often changed to explicit ones and vice-versa in the translation.\nIn this paper, we propose a novel approach based on the intersection between\nstatistical word-alignment models to identify unsupported discourse\nannotations. This approach identified 65% of the unsupported annotations in the\nEnglish-French parallel sentences from Europarl. By filtering out these\nunsupported annotations, we induced the first PDTB-style discourse annotated\ncorpus for French from Europarl. We then used this corpus to train a classifier\nto identify the discourse-usage of French discourse connectives and show a 15%\nimprovement of F1-score compared to the classifier trained on the non-filtered\nannotations.", "published": "2017-07-20 03:17:19", "link": "http://arxiv.org/abs/1707.06357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-Scale Goodness Polarity Lexicons for Community Question Answering", "abstract": "We transfer a key idea from the field of sentiment analysis to a new domain:\ncommunity question answering (cQA). The cQA task we are interested in is the\nfollowing: given a question and a thread of comments, we want to re-rank the\ncomments so that the ones that are good answers to the question would be ranked\nhigher than the bad ones. We notice that good vs. bad comments use specific\nvocabulary and that one can often predict the goodness/badness of a comment\neven ignoring the question, based on the comment contents only. This leads us\nto the idea to build a good/bad polarity lexicon as an analogy to the\npositive/negative sentiment polarity lexicons, commonly used in sentiment\nanalysis. In particular, we use pointwise mutual information in order to build\nlarge-scale goodness polarity lexicons in a semi-supervised manner starting\nwith a small number of initial seeds. The evaluation results show an\nimprovement of 0.7 MAP points absolute over a very strong baseline and\nstate-of-the art performance on SemEval-2016 Task 3.", "published": "2017-07-20 05:40:37", "link": "http://arxiv.org/abs/1707.06378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Selectional Preferences for Coreference Resolution", "abstract": "Selectional preferences have long been claimed to be essential for\ncoreference resolution. However, they are mainly modeled only implicitly by\ncurrent coreference resolvers. We propose a dependency-based embedding model of\nselectional preferences which allows fine-grained compatibility judgments with\nhigh coverage. We show that the incorporation of our model improves coreference\nresolution performance on the CoNLL dataset, matching the state-of-the-art\nresults of a more complex system. However, it comes with a cost that makes it\ndebatable how worthwhile such improvements are.", "published": "2017-07-20 11:54:37", "link": "http://arxiv.org/abs/1707.06456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "High-risk learning: acquiring new word vectors from tiny data", "abstract": "Distributional semantics models are known to struggle with small data. It is\ngenerally accepted that in order to learn 'a good vector' for a word, a model\nmust have sufficient examples of its usage. This contradicts the fact that\nhumans can guess the meaning of a word from a few occurrences only. In this\npaper, we show that a neural language model such as Word2Vec only necessitates\nminor modifications to its standard architecture to learn new terms from tiny\ndata, using background knowledge from a previously learnt semantic space. We\ntest our model on word definitions and on a nonce task involving 2-6 sentences'\nworth of context, showing a large increase in performance over state-of-the-art\nmodels on the definitional task.", "published": "2017-07-20 15:02:14", "link": "http://arxiv.org/abs/1707.06556v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Task Classification Towards Similarity Measures for Recommendation\n  in Crowdsourcing Systems", "abstract": "Task selection in micro-task markets can be supported by recommender systems\nto help individuals to find appropriate tasks. Previous work showed that for\nthe selection process of a micro-task the semantic aspects, such as the\nrequired action and the comprehensibility, are rated more important than\nfactual aspects, such as the payment or the required completion time. This work\ngives a foundation to create such similarity measures. Therefore, we show that\nan automatic classification based on task descriptions is possible.\nAdditionally, we propose similarity measures to cluster micro-tasks according\nto semantic aspects.", "published": "2017-07-20 15:06:43", "link": "http://arxiv.org/abs/1707.06562v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Toward Incorporation of Relevant Documents in word2vec", "abstract": "Recent advances in neural word embedding provide significant benefit to\nvarious information retrieval tasks. However as shown by recent studies,\nadapting the embedding models for the needs of IR tasks can bring considerable\nfurther improvements. The embedding models in general define the term\nrelatedness by exploiting the terms' co-occurrences in short-window contexts.\nAn alternative (and well-studied) approach in IR for related terms to a query\nis using local information i.e. a set of top-retrieved documents. In view of\nthese two methods of term relatedness, in this work, we report our study on\nincorporating the local information of the query in the word embeddings. One\nmain challenge in this direction is that the dense vectors of word embeddings\nand their estimation of term-to-term relatedness remain difficult to interpret\nand hard to analyze. As an alternative, explicit word representations propose\nvectors whose dimensions are easily interpretable, and recent methods show\ncompetitive performance to the dense vectors. We introduce a neural-based\nexplicit representation, rooted in the conceptual ideas of the word2vec\nSkip-Gram model. The method provides interpretable explicit vectors while\nkeeping the effectiveness of the Skip-Gram model. The evaluation of various\nexplicit representations on word association collections shows that the newly\nproposed method out- performs the state-of-the-art explicit representations\nwhen tasked with ranking highly similar terms. Based on the introduced ex-\nplicit representation, we discuss our approaches on integrating local documents\nin globally-trained embedding models and discuss the preliminary results.", "published": "2017-07-20 16:33:48", "link": "http://arxiv.org/abs/1707.06598v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "abstract": "We study the problem of learning to reason in large scale knowledge graphs\n(KGs). More specifically, we describe a novel reinforcement learning framework\nfor learning multi-hop relational paths: we use a policy-based agent with\ncontinuous states based on knowledge graph embeddings, which reasons in a KG\nvector space by sampling the most promising relation to extend its path. In\ncontrast to prior work, our approach includes a reward function that takes the\naccuracy, diversity, and efficiency into consideration. Experimentally, we show\nthat our proposed method outperforms a path-ranking based algorithm and\nknowledge graph embedding methods on Freebase and Never-Ending Language\nLearning datasets.", "published": "2017-07-20 19:39:23", "link": "http://arxiv.org/abs/1707.06690v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Video Question Answering via Attribute-Augmented Attention Network\n  Learning", "abstract": "Video Question Answering is a challenging problem in visual information\nretrieval, which provides the answer to the referenced video content according\nto the question. However, the existing visual question answering approaches\nmainly tackle the problem of static image question, which may be ineffectively\nfor video question answering due to the insufficiency of modeling the temporal\ndynamics of video contents. In this paper, we study the problem of video\nquestion answering by modeling its temporal dynamics with frame-level attention\nmechanism. We propose the attribute-augmented attention network learning\nframework that enables the joint frame-level attribute detection and unified\nvideo representation learning for video question answering. We then incorporate\nthe multi-step reasoning process for our proposed attention network to further\nimprove the performance. We construct a large-scale video question answering\ndataset. We conduct the experiments on both multiple-choice and open-ended\nvideo question answering tasks to show the effectiveness of the proposed\nmethod.", "published": "2017-07-20 03:12:29", "link": "http://arxiv.org/abs/1707.06355v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware\n  Ones", "abstract": "Syllabification does not seem to improve word-level RNN language modeling\nquality when compared to character-based segmentation. However, our best\nsyllable-aware language model, achieving performance comparable to the\ncompetitive character-aware model, has 18%-33% fewer parameters and is trained\n1.2-2.2 times faster.", "published": "2017-07-20 12:46:09", "link": "http://arxiv.org/abs/1707.06480v1", "categories": ["cs.CL", "cs.NE", "stat.ML", "68T50", "I.2.7"], "primary_category": "cs.CL"}
