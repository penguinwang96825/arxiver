{"title": "Search and Learn: Improving Semantic Coverage for Data-to-Text\n  Generation", "abstract": "Data-to-text generation systems aim to generate text descriptions based on\ninput data (often represented in the tabular form). A typical system uses huge\ntraining samples for learning the correspondence between tables and texts.\nHowever, large training sets are expensive to obtain, limiting the\napplicability of these approaches in real-world scenarios. In this work, we\nfocus on few-shot data-to-text generation. We observe that, while fine-tuned\npretrained language models may generate plausible sentences, they suffer from\nthe low semantic coverage problem in the few-shot setting. In other words,\nimportant input slots tend to be missing in the generated text. To this end, we\npropose a search-and-learning approach that leverages pretrained language\nmodels but inserts the missing slots to improve the semantic coverage. We\nfurther fine-tune our system based on the search results to smooth out the\nsearch noise, yielding better-quality text and improving inference efficiency\nto a large extent. Experiments show that our model achieves high performance on\nE2E and WikiBio datasets. Especially, we cover 98.35% of input slots on E2E,\nlargely alleviating the low coverage problem.", "published": "2021-12-06 03:51:56", "link": "http://arxiv.org/abs/2112.02770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast and Accurate End-to-End Span-based Semantic Role Labeling as\n  Word-based Graph Parsing", "abstract": "This paper proposes to cast end-to-end span-based SRL as a word-based graph\nparsing task. The major challenge is how to represent spans at the word level.\nBorrowing ideas from research on Chinese word segmentation and named entity\nrecognition, we propose and compare four different schemata of graph\nrepresentation, i.e., BES, BE, BIES, and BII, among which we find that the BES\nschema performs the best. We further gain interesting insights through detailed\nanalysis. Moreover, we propose a simple constrained Viterbi procedure to ensure\nthe legality of the output graph according to the constraints of the SRL\nstructure. We conduct experiments on two widely used benchmark datasets, i.e.,\nCoNLL05 and CoNLL12. Results show that our word-based graph parsing approach\nachieves consistently better performance than previous results, under all\nsettings of end-to-end and predicate-given, without and with pre-trained\nlanguage models (PLMs). More importantly, our model can parse 669/252 sentences\nper second, without and with PLMs respectively.", "published": "2021-12-06 12:30:58", "link": "http://arxiv.org/abs/2112.02970v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VAE based Text Style Transfer with Pivot Words Enhancement Learning", "abstract": "Text Style Transfer (TST) aims to alter the underlying style of the source\ntext to another specific style while keeping the same content. Due to the\nscarcity of high-quality parallel training data, unsupervised learning has\nbecome a trending direction for TST tasks. In this paper, we propose a novel\nVAE based Text Style Transfer with pivOt Words Enhancement leaRning (VT-STOWER)\nmethod which utilizes Variational AutoEncoder (VAE) and external style\nembeddings to learn semantics and style distribution jointly. Additionally, we\nintroduce pivot words learning, which is applied to learn decisive words for a\nspecific style and thereby further improve the overall performance of the style\ntransfer. The proposed VT-STOWER can be scaled to different TST scenarios given\nvery limited and non-parallel training data with a novel and flexible style\nstrength control mechanism. Experiments demonstrate that the VT-STOWER\noutperforms the state-of-the-art on sentiment, formality, and code-switching\nTST tasks.", "published": "2021-12-06 16:41:26", "link": "http://arxiv.org/abs/2112.03154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot hashtag segmentation for multilingual sentiment analysis", "abstract": "Hashtag segmentation, also known as hashtag decomposition, is a common step\nin preprocessing pipelines for social media datasets. It usually precedes tasks\nsuch as sentiment analysis and hate speech detection. For sentiment analysis in\nmedium to low-resourced languages, previous research has demonstrated that a\nmultilingual approach that resorts to machine translation can be competitive or\nsuperior to previous approaches to the task. We develop a zero-shot hashtag\nsegmentation framework and demonstrate how it can be used to improve the\naccuracy of multilingual sentiment analysis pipelines. Our zero-shot framework\nestablishes a new state-of-the-art for hashtag segmentation datasets,\nsurpassing even previous approaches that relied on feature engineering and\nlanguage models trained on in-domain data.", "published": "2021-12-06 18:13:46", "link": "http://arxiv.org/abs/2112.03213v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Impact of Target Word and Context on End-to-End Metonymy Detection", "abstract": "Metonymy is a figure of speech in which an entity is referred to by another\nrelated entity. The task of metonymy detection aims to distinguish metonymic\ntokens from literal ones. Until now, metonymy detection methods attempt to\ndisambiguate only a single noun phrase in a sentence, typically location names\nor organization names. In this paper, we disambiguate every word in a sentence\nby reformulating metonymy detection as a sequence labeling task. We also\ninvestigate the impact of target word and context on metonymy detection. We\nshow that the target word is less useful for detecting metonymy in our dataset.\nOn the other hand, the entity types that are associated with domain-specific\nwords in their context are easier to solve. This shows that the context words\nare much more relevant for detecting metonymy.", "published": "2021-12-06 18:59:27", "link": "http://arxiv.org/abs/2112.03256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JointLK: Joint Reasoning with Language Models and Knowledge Graphs for\n  Commonsense Question Answering", "abstract": "Existing KG-augmented models for commonsense question answering primarily\nfocus on designing elaborate Graph Neural Networks (GNNs) to model knowledge\ngraphs (KGs). However, they ignore (i) the effectively fusing and reasoning\nover question context representations and the KG representations, and (ii)\nautomatically selecting relevant nodes from the noisy KGs during reasoning. In\nthis paper, we propose a novel model, JointLK, which solves the above\nlimitations through the joint reasoning of LM and GNN and the dynamic KGs\npruning mechanism. Specifically, JointLK performs joint reasoning between LM\nand GNN through a novel dense bidirectional attention module, in which each\nquestion token attends on KG nodes and each KG node attends on question tokens,\nand the two modal representations fuse and update mutually by multi-step\ninteractions. Then, the dynamic pruning module uses the attention weights\ngenerated by joint reasoning to prune irrelevant KG nodes recursively. We\nevaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate\nits improvements to the existing LM and LM+KG models, as well as its capability\nto perform interpretable reasoning.", "published": "2021-12-06 01:46:46", "link": "http://arxiv.org/abs/2112.02732v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Make It Move: Controllable Image-to-Video Generation with Text\n  Descriptions", "abstract": "Generating controllable videos conforming to user intentions is an appealing\nyet challenging topic in computer vision. To enable maneuverable control in\nline with user intentions, a novel video generation task, named\nText-Image-to-Video generation (TI2V), is proposed. With both controllable\nappearance and motion, TI2V aims at generating videos from a static image and a\ntext description. The key challenges of TI2V task lie both in aligning\nappearance and motion from different modalities, and in handling uncertainty in\ntext descriptions. To address these challenges, we propose a Motion\nAnchor-based video GEnerator (MAGE) with an innovative motion anchor (MA)\nstructure to store appearance-motion aligned representation. To model the\nuncertainty and increase the diversity, it further allows the injection of\nexplicit condition and implicit randomness. Through three-dimensional axial\ntransformers, MA is interacted with given image to generate next frames\nrecursively with satisfying controllability and diversity. Accompanying the new\ntask, we build two new video-text paired datasets based on MNIST and CATER for\nevaluation. Experiments conducted on these datasets verify the effectiveness of\nMAGE and show appealing potentials of TI2V task. Source code for model and\ndatasets will be available soon.", "published": "2021-12-06 07:00:36", "link": "http://arxiv.org/abs/2112.02815v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "General Facial Representation Learning in a Visual-Linguistic Manner", "abstract": "How to learn a universal facial representation that boosts all face analysis\ntasks? This paper takes one step toward this goal. In this paper, we study the\ntransfer performance of pre-trained models on face analysis tasks and introduce\na framework, called FaRL, for general Facial Representation Learning in a\nvisual-linguistic manner. On one hand, the framework involves a contrastive\nloss to learn high-level semantic meaning from image-text pairs. On the other\nhand, we propose exploring low-level information simultaneously to further\nenhance the face representation, by adding a masked image modeling. We perform\npre-training on LAION-FACE, a dataset containing large amount of face\nimage-text pairs, and evaluate the representation capability on multiple\ndownstream tasks. We show that FaRL achieves better transfer performance\ncompared with previous pre-trained models. We also verify its superiority in\nthe low-data regime. More importantly, our model surpasses the state-of-the-art\nmethods on face analysis tasks including face parsing and face alignment.", "published": "2021-12-06 15:22:05", "link": "http://arxiv.org/abs/2112.03109v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Embedding Arithmetic of Multimodal Queries for Image Retrieval", "abstract": "Latent text representations exhibit geometric regularities, such as the\nfamous analogy: queen is to king what woman is to man. Such structured semantic\nrelations were not demonstrated on image representations. Recent works aiming\nat bridging this semantic gap embed images and text into a multimodal space,\nenabling the transfer of text-defined transformations to the image modality. We\nintroduce the SIMAT dataset to evaluate the task of Image Retrieval with\nMultimodal queries. SIMAT contains 6k images and 18k textual transformation\nqueries that aim at either replacing scene elements or changing pairwise\nrelationships between scene elements. The goal is to retrieve an image\nconsistent with the (source image, text transformation) query. We use an\nimage/text matching oracle (OSCAR) to assess whether the image transformation\nis successful. The SIMAT dataset will be publicly available. We use SIMAT to\nevaluate the geometric properties of multimodal embedding spaces trained with\nan image/text matching objective, like CLIP. We show that vanilla CLIP\nembeddings are not very well suited to transform images with delta vectors, but\nthat a simple finetuning on the COCO dataset can bring dramatic improvements.\nWe also study whether it is beneficial to leverage pretrained universal\nsentence encoders (FastText, LASER and LaBSE).", "published": "2021-12-06 16:51:50", "link": "http://arxiv.org/abs/2112.03162v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A New Sentence Extraction Strategy for Unsupervised Extractive\n  Summarization Methods", "abstract": "In recent years, text summarization methods have attracted much attention\nagain thanks to the researches on neural network models. Most of the current\ntext summarization methods based on neural network models are supervised\nmethods which need large-scale datasets. However, large-scale datasets are\ndifficult to obtain in practical applications. In this paper, we model the task\nof extractive text summarization methods from the perspective of Information\nTheory, and then describe the unsupervised extractive methods with a uniform\nframework. To improve the feature distribution and to decrease the mutual\ninformation of summarization sentences, we propose a new sentence extraction\nstrategy which can be applied to existing unsupervised extractive methods.\nExperiments are carried out on different datasets, and results show that our\nstrategy is indeed effective and in line with expectations.", "published": "2021-12-06 18:00:02", "link": "http://arxiv.org/abs/2112.03203v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantifying Adaptability in Pre-trained Language Models with 500 Tasks", "abstract": "When a neural language model (LM) is adapted to perform a new task, what\naspects of the task predict the eventual performance of the model? In NLP,\nsystematic features of LM generalization to individual examples are well\ncharacterized, but systematic aspects of LM adaptability to new tasks are not\nnearly as well understood. We present a large-scale empirical study of the\nfeatures and limits of LM adaptability using a new benchmark, TaskBench500,\nbuilt from 500 procedurally generated sequence modeling tasks. These tasks\ncombine core aspects of language processing, including lexical semantics,\nsequence processing, memorization, logical reasoning, and world knowledge.\nUsing TaskBench500, we evaluate three facets of adaptability, finding that: (1)\nadaptation procedures differ dramatically in their ability to memorize small\ndatasets; (2) within a subset of task types, adaptation procedures exhibit\ncompositional adaptability to complex tasks; and (3) failure to match training\nlabel distributions is explained by mismatches in the intrinsic difficulty of\npredicting individual labels. Our experiments show that adaptability to new\ntasks, like generalization to new examples, can be systematically described and\nunderstood, and we conclude with a discussion of additional aspects of\nadaptability that could be studied using the new benchmark.", "published": "2021-12-06 18:00:25", "link": "http://arxiv.org/abs/2112.03204v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JUSTICE: A Benchmark Dataset for Supreme Court's Judgment Prediction", "abstract": "Artificial intelligence is being utilized in many domains as of late, and the\nlegal system is no exception. However, as it stands now, the number of\nwell-annotated datasets pertaining to legal documents from the Supreme Court of\nthe United States (SCOTUS) is very limited for public use. Even though the\nSupreme Court rulings are public domain knowledge, trying to do meaningful work\nwith them becomes a much greater task due to the need to manually gather and\nprocess that data from scratch each time. Hence, our goal is to create a\nhigh-quality dataset of SCOTUS court cases so that they may be readily used in\nnatural language processing (NLP) research and other data-driven applications.\nAdditionally, recent advances in NLP provide us with the tools to build\npredictive models that can be used to reveal patterns that influence court\ndecisions. By using advanced NLP algorithms to analyze previous court cases,\nthe trained models are able to predict and classify a court's judgment given\nthe case's facts from the plaintiff and the defendant in textual format; in\nother words, the model is emulating a human jury by generating a final verdict.", "published": "2021-12-06 23:19:08", "link": "http://arxiv.org/abs/2112.03414v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Skeletal Graph Self-Attention: Embedding a Skeleton Inductive Bias into\n  Sign Language Production", "abstract": "Recent approaches to Sign Language Production (SLP) have adopted spoken\nlanguage Neural Machine Translation (NMT) architectures, applied without\nsign-specific modifications. In addition, these works represent sign language\nas a sequence of skeleton pose vectors, projected to an abstract representation\nwith no inherent skeletal structure. In this paper, we represent sign language\nsequences as a skeletal graph structure, with joints as nodes and both spatial\nand temporal connections as edges. To operate on this graphical structure, we\npropose Skeletal Graph Self-Attention (SGSA), a novel graphical attention layer\nthat embeds a skeleton inductive bias into the SLP model. Retaining the\nskeletal feature representation throughout, we directly apply a spatio-temporal\nadjacency matrix into the self-attention formulation. This provides structure\nand context to each skeletal joint that is not possible when using a\nnon-graphical abstract representation, enabling fluid and expressive sign\nlanguage production. We evaluate our Skeletal Graph Self-Attention architecture\non the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset, achieving\nstate-of-the-art back translation performance with an 8% and 7% improvement\nover competing methods for the dev and test sets.", "published": "2021-12-06 10:12:11", "link": "http://arxiv.org/abs/2112.05277v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NL-Augmenter: A Framework for Task-Sensitive Natural Language\n  Augmentation", "abstract": "Data augmentation is an important component in the robustness evaluation of\nmodels in natural language processing (NLP) and in enhancing the diversity of\nthe data they are trained on. In this paper, we present NL-Augmenter, a new\nparticipatory Python-based natural language augmentation framework which\nsupports the creation of both transformations (modifications to the data) and\nfilters (data splits according to specific features). We describe the framework\nand an initial set of 117 transformations and 23 filters for a variety of\nnatural language tasks. We demonstrate the efficacy of NL-Augmenter by using\nseveral of its transformations to analyze the robustness of popular natural\nlanguage models. The infrastructure, datacards and robustness analysis results\nare available publicly on the NL-Augmenter repository\n(https://github.com/GEM-benchmark/NL-Augmenter).", "published": "2021-12-06 00:37:59", "link": "http://arxiv.org/abs/2112.02721v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Team Hitachi @ AutoMin 2021: Reference-free Automatic Minuting Pipeline\n  with Argument Structure Construction over Topic-based Summarization", "abstract": "This paper introduces the proposed automatic minuting system of the Hitachi\nteam for the First Shared Task on Automatic Minuting (AutoMin-2021). We utilize\na reference-free approach (i.e., without using training minutes) for automatic\nminuting (Task A), which first splits a transcript into blocks on the basis of\ntopics and subsequently summarizes those blocks with a pre-trained BART model\nfine-tuned on a summarization corpus of chat dialogue. In addition, we apply a\ntechnique of argument mining to the generated minutes, reorganizing them in a\nwell-structured and coherent way. We utilize multiple relevance scores to\ndetermine whether or not a minute is derived from the same meeting when either\na transcript or another minute is given (Task B and C). On top of those scores,\nwe train a conventional machine learning model to bind them and to make final\ndecisions. Consequently, our approach for Task A achieve the best adequacy\nscore among all submissions and close performance to the best system in terms\nof grammatical correctness and fluency. For Task B and C, the proposed model\nsuccessfully outperformed a majority vote baseline.", "published": "2021-12-06 02:23:20", "link": "http://arxiv.org/abs/2112.02741v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Localized Representations from Medical Images and\n  Reports", "abstract": "Contrastive learning has proven effective for pre-training image models on\nunlabeled data with promising results for tasks such as medical image\nclassification. Using paired text (like radiological reports) during\npre-training improves the results even further. Still, most existing methods\ntarget image classification downstream tasks and may not be optimal for\nlocalized tasks like semantic segmentation or object detection. We therefore\npropose Localized representation learning from Vision and Text (LoVT), to our\nbest knowledge, the first text-supervised pre-training method that targets\nlocalized medical imaging tasks. Our method combines instance-level\nimage-report contrastive learning with local contrastive learning on image\nregion and report sentence representations. We evaluate LoVT and commonly used\npre-training methods on an evaluation framework of 18 localized tasks on chest\nX-rays from five public datasets. LoVT performs best on 10 of the 18 studied\ntasks making it the preferred method of choice for localized tasks.", "published": "2021-12-06 09:27:24", "link": "http://arxiv.org/abs/2112.02889v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Scaling Up Influence Functions", "abstract": "We address efficient calculation of influence functions for tracking\npredictions back to the training data. We propose and analyze a new approach to\nspeeding up the inverse Hessian calculation based on Arnoldi iteration. With\nthis improvement, we achieve, to the best of our knowledge, the first\nsuccessful implementation of influence functions that scales to full-size\n(language and vision) Transformer models with several hundreds of millions of\nparameters. We evaluate our approach on image classification and\nsequence-to-sequence tasks with tens to a hundred of millions of training\nexamples. Our code will be available at\nhttps://github.com/google-research/jax-influence.", "published": "2021-12-06 13:54:08", "link": "http://arxiv.org/abs/2112.03052v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "VocBench: A Neural Vocoder Benchmark for Speech Synthesis", "abstract": "Neural vocoders, used for converting the spectral representations of an audio\nsignal to the waveforms, are a commonly used component in speech synthesis\npipelines. It focuses on synthesizing waveforms from low-dimensional\nrepresentation, such as Mel-Spectrograms. In recent years, different approaches\nhave been introduced to develop such vocoders. However, it becomes more\nchallenging to assess these new vocoders and compare their performance to\nprevious ones. To address this problem, we present VocBench, a framework that\nbenchmark the performance of state-of-the art neural vocoders. VocBench uses a\nsystematic study to evaluate different neural vocoders in a shared environment\nthat enables a fair comparison between them. In our experiments, we use the\nsame setup for datasets, training pipeline, and evaluation metrics for all\nneural vocoders. We perform a subjective and objective evaluation to compare\nthe performance of each vocoder along a different axis. Our results demonstrate\nthat the framework is capable of showing the competitive efficacy and the\nquality of the synthesized samples for each vocoder. VocBench framework is\navailable at https://github.com/facebookresearch/vocoder-benchmark.", "published": "2021-12-06 15:09:57", "link": "http://arxiv.org/abs/2112.03099v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text2Mesh: Text-Driven Neural Stylization for Meshes", "abstract": "In this work, we develop intuitive controls for editing the style of 3D\nobjects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and\nlocal geometric details which conform to a target text prompt. We consider a\ndisentangled representation of a 3D object using a fixed mesh input (content)\ncoupled with a learned neural network, which we term neural style field\nnetwork. In order to modify style, we obtain a similarity score between a text\nprompt (describing style) and a stylized mesh by harnessing the\nrepresentational power of CLIP. Text2Mesh requires neither a pre-trained\ngenerative model nor a specialized 3D mesh dataset. It can handle low-quality\nmeshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not\nrequire UV parameterization. We demonstrate the ability of our technique to\nsynthesize a myriad of styles over a wide variety of 3D meshes.", "published": "2021-12-06 18:23:29", "link": "http://arxiv.org/abs/2112.03221v1", "categories": ["cs.CV", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External\n  Attention", "abstract": "Most of today's AI systems focus on using self-attention mechanisms and\ntransformer architectures on large amounts of diverse data to achieve\nimpressive performance gains. In this paper, we propose to augment the\ntransformer architecture with an external attention mechanism to bring external\nknowledge and context to bear. By integrating external information into the\nprediction process, we hope to reduce the need for ever-larger models and\nincrease the democratization of AI systems. We find that the proposed external\nattention mechanism can significantly improve the performance of existing AI\nsystems, allowing practitioners to easily customize foundation AI models to\nmany diverse downstream applications. In particular, we focus on the task of\nCommonsense Reasoning, demonstrating that the proposed external attention\nmechanism can augment existing transformer models and significantly improve the\nmodel's reasoning capabilities. The proposed system, Knowledgeable External\nAttention for commonsense Reasoning (KEAR), reaches human parity on the open\nCommonsenseQA research benchmark with an accuracy of 89.4\\% in comparison to\nthe human accuracy of 88.9\\%.", "published": "2021-12-06 18:59:02", "link": "http://arxiv.org/abs/2112.03254v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment\n  Classification Tasks", "abstract": "This paper studies continual learning (CL) of a sequence of aspect sentiment\nclassification (ASC) tasks. Although some CL techniques have been proposed for\ndocument sentiment classification, we are not aware of any CL work on ASC. A CL\nsystem that incrementally learns a sequence of ASC tasks should address the\nfollowing two issues: (1) transfer knowledge learned from previous tasks to the\nnew task to help it learn a better model, and (2) maintain the performance of\nthe models for previous tasks so that they are not forgotten. This paper\nproposes a novel capsule network based model called B-CL to address these\nissues. B-CL markedly improves the ASC performance on both the new task and the\nold tasks via forward and backward knowledge transfer. The effectiveness of\nB-CL is demonstrated through extensive experiments.", "published": "2021-12-06 02:46:06", "link": "http://arxiv.org/abs/2112.03271v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for\n  Long-Horizon Robot Manipulation Tasks", "abstract": "General-purpose robots coexisting with humans in their environment must learn\nto relate human language to their perceptions and actions to be useful in a\nrange of daily tasks. Moreover, they need to acquire a diverse repertoire of\ngeneral-purpose skills that allow composing long-horizon tasks by following\nunconstrained language instructions. In this paper, we present CALVIN\n(Composing Actions from Language and Vision), an open-source simulated\nbenchmark to learn long-horizon language-conditioned tasks. Our aim is to make\nit possible to develop agents that can solve many robotic manipulation tasks\nover a long horizon, from onboard sensors, and specified only via human\nlanguage. CALVIN tasks are more complex in terms of sequence length, action\nspace, and language than existing vision-and-language task datasets and\nsupports flexible specification of sensor suites. We evaluate the agents in\nzero-shot to novel language instructions and to novel environments and objects.\nWe show that a baseline model based on multi-context imitation learning\nperforms poorly on CALVIN, suggesting that there is significant room for\ndeveloping innovative agents that learn to relate human language to their world\nmodels with this benchmark.", "published": "2021-12-06 18:37:33", "link": "http://arxiv.org/abs/2112.03227v4", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Steerable discovery of neural audio effects", "abstract": "Applications of deep learning for audio effects often focus on modeling\nanalog effects or learning to control effects to emulate a trained audio\nengineer. However, deep learning approaches also have the potential to expand\ncreativity through neural audio effects that enable new sound transformations.\nWhile recent work demonstrated that neural networks with random weights produce\ncompelling audio effects, control of these effects is limited and unintuitive.\nTo address this, we introduce a method for the steerable discovery of neural\naudio effects. This method enables the design of effects using example\nrecordings provided by the user. We demonstrate how this method produces an\neffect similar to the target effect, along with interesting inaccuracies, while\nalso providing perceptually relevant controls.", "published": "2021-12-06 10:56:16", "link": "http://arxiv.org/abs/2112.02926v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conditional Deep Hierarchical Variational Autoencoder for Voice\n  Conversion", "abstract": "Variational autoencoder-based voice conversion (VAE-VC) has the advantage of\nrequiring only pairs of speeches and speaker labels for training. Unlike the\nmajority of the research in VAE-VC which focuses on utilizing auxiliary losses\nor discretizing latent variables, this paper investigates how an increasing\nmodel expressiveness has benefits and impacts on the VAE-VC. Specifically, we\nfirst analyze VAE-VC from a rate-distortion perspective, and point out that\nmodel expressiveness is significant for VAE-VC because rate and distortion\nreflect similarity and naturalness of converted speeches. Based on the\nanalysis, we propose a novel VC method using a deep hierarchical VAE, which has\nhigh model expressiveness as well as having fast conversion speed thanks to its\nnon-autoregressive decoder. Also, our analysis reveals another problem that\nsimilarity can be degraded when the latent variable of VAEs has redundant\ninformation. We address the problem by controlling the information contained in\nthe latent variable using $\\beta$-VAE objective. In the experiment using VCTK\ncorpus, the proposed method achieved mean opinion scores higher than 3.5 on\nboth naturalness and similarity in inter-gender settings, which are higher than\nthe scores of existing autoencoder-based VC methods.", "published": "2021-12-06 05:54:11", "link": "http://arxiv.org/abs/2112.02796v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The artificial synesthete: Image-melody translations with variational\n  autoencoders", "abstract": "Abstract This project presents a system of neural networks to translate\nbetween images and melodies. Autoencoders compress the information in samples\nto abstract representation. A translation network learns a set of\ncorrespondences between musical and visual concepts from repeated joint\nexposure. The resulting \"artificial synesthete\" generates simple melodies\ninspired by images, and images from music. These are novel interpretation (not\ntransposed data), expressing the machine' perception and understanding.\nObserving the work, one explores the machine's perception and thus, by\ncontrast, one's own.", "published": "2021-12-06 11:54:13", "link": "http://arxiv.org/abs/2112.02953v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Intelligent Acoustic Module for Autonomous Vehicles using Fast Gated\n  Recurrent approach", "abstract": "This paper elucidates a model for acoustic single and multi-tone\nclassification in resource constrained edge devices. The proposed model is of\nState-of-the-art Fast Accurate Stable Tiny Gated Recurrent Neural Network. This\nmodel has resulted in improved performance metrics and lower size compared to\nprevious hypothesized methods by using lesser parameters with higher efficiency\nand employment of a noise reduction algorithm. The model is implemented as an\nacoustic AI module, focused for the application of sound identification,\nlocalization, and deployment on AI systems like that of an autonomous car.\nFurther, the inclusion of localization techniques carries the potential of\nadding a new dimension to the multi-tone classifiers present in autonomous\nvehicles, as its demand increases in urban cities and developing countries in\nthe future.", "published": "2021-12-06 17:06:48", "link": "http://arxiv.org/abs/2112.03174v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Piano Timbre Development Analysis using Machine Learning", "abstract": "A data set of recorded single played tones of a concert grand piano is\ninvestigated using Machine Learning (ML) on psychoacoustic timbre features. The\nexamined instrument has been recorded at two stages: firstly right after\nmanufacture and secondly after being played in a concert hall for one year. A\nprevious study [Plath2019] revealed that listeners clearly distinguished both\nstages but no clear correlation with acoustics, signal processing tools or\nverbalizations of perceived differences could be found. Using a Self-Organizing\nMap (SOM), training single as well as double feature sets, it can be shown that\nspectral flux is able to perfectly cluster the two stages. Sound Pressure Level\n(SPL), roughness, and fractal correlation dimension (as a measure for initial\ntransient chaoticity) are furthermore able to order the keys with respect to\nhigh and low notes. Combining spectral flux with the three other features in\ndouble-feature training sets maintains stage clustering only for SPL and\nfractal dimension, showing sub-clusters for both stages. These sub-clusters\npoint to a homogenization of SPL for stage 2 with respect to stage 1 and a\npronounced ordering and sub-clustering of key regions with respect to initial\ntransient chaoticity.", "published": "2021-12-06 18:14:52", "link": "http://arxiv.org/abs/2112.03214v2", "categories": ["q-bio.NC", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Modeling synchronization in human musical rhythms using Impulse Pattern\n  Formulation (IPF)", "abstract": "When musicians perform in an ensemble, synchronizing to a mutual pace is the\nfoundation of their musical interaction. Clock generators, e.g., metronomes, or\ndrum machines, might assist such synchronization, but these means, in general,\nwill also distort this natural, self-organized, inter-human synchronization\nprocess. In this work, the synchronization of musicians to an external rhythm\nis modeled using the Impulse Pattern Formulation (IPF), an analytical modeling\napproach for synergetic systems motivated by research on musical instruments.\nNonlinear coupling of system components is described as the interaction of\nindividually propagating and exponentially damped impulse trains. The derived\nmodel is systematically examined by analyzing its behavior when coupled to\nnumerical designed and carefully controlled rhythmical beat sequences. The\nresults are evaluated by comparison in the light of other publications on\ntapping. Finally, the IPF model can be applied to analyze the personal\nrhythmical signature of specific musicians or to replace drum machines and\nclick tracks with more musical and creative solutions.", "published": "2021-12-06 18:20:08", "link": "http://arxiv.org/abs/2112.03218v1", "categories": ["q-bio.NC", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Audio Deepfake Perceptions in College Going Populations", "abstract": "Deepfake is content or material that is generated or manipulated using AI\nmethods, to pass off as real. There are four different deepfake types: audio,\nvideo, image and text. In this research we focus on audio deepfakes and how\npeople perceive it. There are several audio deepfake generation frameworks, but\nwe chose MelGAN which is a non-autoregressive and fast audio deepfake\ngenerating framework, requiring fewer parameters. This study tries to assess\naudio deepfake perceptions among college students from different majors. This\nstudy also answers the question of how their background and major can affect\ntheir perception towards AI generated deepfakes. We also analyzed the results\nbased on different aspects of: grade level, complexity of the grammar used in\nthe audio clips, length of the audio clips, those who knew the term deepfakes\nand those who did not, as well as the political angle. It is interesting that\nthe results show when an audio clip has a political connotation, it can affect\nwhat people think about whether it is real or fake, even if the content is\nfairly similar. This study also explores the question of how background and\nmajor can affect perception towards deepfakes.", "published": "2021-12-06 20:53:41", "link": "http://arxiv.org/abs/2112.03351v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
