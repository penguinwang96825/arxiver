{"title": "ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and\n  Diagnosing Dialogue Systems", "abstract": "We present ConvLab-2, an open-source toolkit that enables researchers to\nbuild task-oriented dialogue systems with state-of-the-art models, perform an\nend-to-end evaluation, and diagnose the weakness of systems. As the successor\nof ConvLab (Lee et al., 2019b), ConvLab-2 inherits ConvLab's framework but\nintegrates more powerful dialogue models and supports more datasets. Besides,\nwe have developed an analysis tool and an interactive tool to assist\nresearchers in diagnosing dialogue systems. The analysis tool presents rich\nstatistics and summarizes common mistakes from simulated dialogues, which\nfacilitates error analysis and system improvement. The interactive tool\nprovides a user interface that allows developers to diagnose an assembled\ndialogue system by interacting with the system and modifying the output of each\nsystem component.", "published": "2020-02-12 04:31:40", "link": "http://arxiv.org/abs/2002.04793v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis\n  and Natural Language Inference", "abstract": "Aspect based sentiment analysis aims to identify the sentimental tendency\ntowards a given aspect in text. Fine-tuning of pretrained BERT performs\nexcellent on this task and achieves state-of-the-art performances. Existing\nBERT-based works only utilize the last output layer of BERT and ignore the\nsemantic knowledge in the intermediate layers. This paper explores the\npotential of utilizing BERT intermediate layers to enhance the performance of\nfine-tuning of BERT. To the best of our knowledge, no existing work has been\ndone on this research. To show the generality, we also apply this approach to a\nnatural language inference task. Experimental results demonstrate the\neffectiveness and generality of the proposed approach.", "published": "2020-02-12 06:11:48", "link": "http://arxiv.org/abs/2002.04815v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Embedding in Named Entity Linking on Sentence Level", "abstract": "Named entity linking is to map an ambiguous mention in documents to an entity\nin a knowledge base. The named entity linking is challenging, given the fact\nthat there are multiple candidate entities for a mention in a document. It is\ndifficult to link a mention when it appears multiple times in a document, since\nthere are conflicts by the contexts around the appearances of the mention. In\naddition, it is difficult since the given training dataset is small due to the\nreason that it is done manually to link a mention to its mapping entity. In the\nliterature, there are many reported studies among which the recent embedding\nmethods learn vectors of entities from the training dataset at document level.\nTo address these issues, we focus on how to link entity for mentions at a\nsentence level, which reduces the noises introduced by different appearances of\nthe same mention in a document at the expense of insufficient information to be\nused. We propose a new unified embedding method by maximizing the relationships\nlearned from knowledge graphs. We confirm the effectiveness of our method in\nour experimental studies.", "published": "2020-02-12 12:06:32", "link": "http://arxiv.org/abs/2002.04936v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Compare for Better Training and Evaluation of Open Domain\n  Natural Language Generation Models", "abstract": "Automated evaluation of open domain natural language generation (NLG) models\nremains a challenge and widely used metrics such as BLEU and Perplexity can be\nmisleading in some cases. In our paper, we propose to evaluate natural language\ngeneration models by learning to compare a pair of generated sentences by\nfine-tuning BERT, which has been shown to have good natural language\nunderstanding ability. We also propose to evaluate the model-level quality of\nNLG models with sample-level comparison results with skill rating system. While\nable to be trained in a fully self-supervised fashion, our model can be further\nfine-tuned with a little amount of human preference annotation to better\nimitate human judgment. In addition to evaluating trained models, we propose to\napply our model as a performance indicator during training for better\nhyperparameter tuning and early-stopping. We evaluate our approach on both\nstory generation and chit-chat dialogue response generation. Experimental\nresults show that our model correlates better with human preference compared\nwith previous automated evaluation approaches. Training with the proposed\nmetric yields better performance in human evaluation, which further\ndemonstrates the effectiveness of the proposed model.", "published": "2020-02-12 15:52:21", "link": "http://arxiv.org/abs/2002.05058v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Combined Stochastic and Physical Framework for Modeling Indoor 5G\n  Millimeter Wave Propagation", "abstract": "Indoor coverage is a major challenge for 5G millimeter waves (mmWaves). In\nthis paper, we address this problem through a novel theoretical framework that\ncombines stochastic indoor environment modeling with advanced physical\npropagation simulation. This approach is particularly adapted to investigate\nindoor-to-indoor 5G mmWave propagation. Its system implementation, so-called\niGeoStat, generates parameterized typical environments that account for the\nindoor spatial variations, then simulates radio propagation based on the\nphysical interaction between electromagnetic waves and material properties.\nThis framework is not dedicated to a particular environment, material,\nfrequency or use case and aims to statistically understand the influence of\nindoor environment parameters on mmWave propagation properties, especially\ncoverage and path loss. Its implementation raises numerous computational\nchallenges that we solve by formulating an adapted link budget and designing\nnew memory optimization algorithms. The first simulation results for two major\n5G applications are validated with measurement data and show the efficiency of\niGeoStat to simulate multiple diffusion in realistic environments, within a\nreasonable amount of time and memory resources. Generated output maps confirm\nthat diffusion has a critical impact on indoor mmWave propagation and that\nproper physical modeling is of the utmost importance to generate relevant\npropagation models.", "published": "2020-02-12 17:28:30", "link": "http://arxiv.org/abs/2002.05162v2", "categories": ["cs.NI", "cs.CL"], "primary_category": "cs.NI"}
{"title": "Deep compositional robotic planners that follow natural language\n  commands", "abstract": "We demonstrate how a sampling-based robotic planner can be augmented to learn\nto understand a sequence of natural language commands in a continuous\nconfiguration space to move and manipulate objects. Our approach combines a\ndeep network structured according to the parse of a complex command that\nincludes objects, verbs, spatial relations, and attributes, with a\nsampling-based planner, RRT. A recurrent hierarchical deep network controls how\nthe planner explores the environment, determines when a planned path is likely\nto achieve a goal, and estimates the confidence of each move to trade off\nexploitation and exploration between the network and the planner. Planners are\ndesigned to have near-optimal behavior when information about the task is\nmissing, while networks learn to exploit observations which are available from\nthe environment, making the two naturally complementary. Combining the two\nenables generalization to new maps, new kinds of obstacles, and more complex\nsentences that do not occur in the training set. Little data is required to\ntrain the model despite it jointly acquiring a CNN that extracts features from\nthe environment as it learns the meanings of words. The model provides a level\nof interpretability through the use of attention maps allowing users to see its\nreasoning steps despite being an end-to-end model. This end-to-end model allows\nrobots to learn to follow natural language commands in challenging continuous\nenvironments.", "published": "2020-02-12 19:56:58", "link": "http://arxiv.org/abs/2002.05201v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Unsupervised Separation of Native and Loanwords for Malayalam and Telugu", "abstract": "Quite often, words from one language are adopted within a different language\nwithout translation; these words appear in transliterated form in text written\nin the latter language. This phenomenon is particularly widespread within\nIndian languages where many words are loaned from English. In this paper, we\naddress the task of identifying loanwords automatically and in an unsupervised\nmanner, from large datasets of words from agglutinative Dravidian languages. We\ntarget two specific languages from the Dravidian family, viz., Malayalam and\nTelugu. Based on familiarity with the languages, we outline an observation that\nnative words in both these languages tend to be characterized by a much more\nversatile stem - stem being a shorthand to denote the subword sequence formed\nby the first few characters of the word - than words that are loaned from other\nlanguages. We harness this observation to build an objective function and an\niterative optimization formulation to optimize for it, yielding a scoring of\neach word's nativeness in the process. Through an extensive empirical analysis\nover real-world datasets from both Malayalam and Telugu, we illustrate the\neffectiveness of our method in quantifying nativeness effectively over\navailable baselines for the task.", "published": "2020-02-12 04:01:57", "link": "http://arxiv.org/abs/2002.05527v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Constructing a Highlight Classifier with an Attention Based LSTM Neural\n  Network", "abstract": "Data is being produced in larger quantities than ever before in human\nhistory. It's only natural to expect a rise in demand for technology that aids\nhumans in sifting through and analyzing this inexhaustible supply of\ninformation. This need exists in the market research industry, where large\namounts of consumer research data is collected through video recordings. At\npresent, the standard method for analyzing video data is human labor. Market\nresearchers manually review the vast majority of consumer research video in\norder to identify relevant portions - highlights. The industry state of the art\nturnaround ratio is 2.2 - for every hour of video content 2.2 hours of manpower\nare required. In this study we present a novel approach for NLP-based highlight\nidentification and extraction based on a supervised learning model that aides\nmarket researchers in sifting through their data. Our approach hinges on a\nmanually curated user-generated highlight clips constructed from long and\nshort-form video data. The problem is best suited for an NLP approach due to\nthe availability of video transcription. We evaluate multiple classes of\nmodels, from gradient boosting to recurrent neural networks, comparing their\nperformance in extraction and identification of highlights. The best performing\nmodels are then evaluated using four sampling methods designed to analyze\ndocuments much larger than the maximum input length of the classifiers. We\nreport very high performances for the standalone classifiers, ROC AUC scores in\nthe range 0.93-0.94, but observe a significant drop in effectiveness when\nevaluated on large documents. Based on our results we suggest combinations of\nmodels/sampling algorithms for various use cases.", "published": "2020-02-12 15:18:31", "link": "http://arxiv.org/abs/2002.04608v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Layer Normalization in the Transformer Architecture", "abstract": "The Transformer is widely used in natural language processing tasks. To train\na Transformer however, one usually needs a carefully designed learning rate\nwarm-up stage, which is shown to be crucial to the final performance but will\nslow down the optimization and bring more hyper-parameter tunings. In this\npaper, we first study theoretically why the learning rate warm-up stage is\nessential and show that the location of layer normalization matters.\nSpecifically, we prove with mean field theory that at initialization, for the\noriginal-designed Post-LN Transformer, which places the layer normalization\nbetween the residual blocks, the expected gradients of the parameters near the\noutput layer are large. Therefore, using a large learning rate on those\ngradients makes the training unstable. The warm-up stage is practically helpful\nfor avoiding this problem. On the other hand, our theory also shows that if the\nlayer normalization is put inside the residual blocks (recently proposed as\nPre-LN Transformer), the gradients are well-behaved at initialization. This\nmotivates us to remove the warm-up stage for the training of Pre-LN\nTransformers. We show in our experiments that Pre-LN Transformers without the\nwarm-up stage can reach comparable results with baselines while requiring\nsignificantly less training time and hyper-parameter tuning on a wide range of\napplications.", "published": "2020-02-12 00:33:03", "link": "http://arxiv.org/abs/2002.04745v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "DeepMutation: A Neural Mutation Tool", "abstract": "Mutation testing can be used to assess the fault-detection capabilities of a\ngiven test suite. To this aim, two characteristics of mutation testing\nframeworks are of paramount importance: (i) they should generate mutants that\nare representative of real faults; and (ii) they should provide a complete tool\nchain able to automatically generate, inject, and test the mutants. To address\nthe first point, we recently proposed an approach using a Recurrent Neural\nNetwork Encoder-Decoder architecture to learn mutants from ~787k faults mined\nfrom real programs. The empirical evaluation of this approach confirmed its\nability to generate mutants representative of real faults. In this paper, we\naddress the second point, presenting DeepMutation, a tool wrapping our deep\nlearning model into a fully automated tool chain able to generate, inject, and\ntest mutants learned from real faults. Video:\nhttps://sites.google.com/view/learning-mutation/deepmutation", "published": "2020-02-12 01:57:41", "link": "http://arxiv.org/abs/2002.04760v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Component Analysis for Visual Question Answering Architectures", "abstract": "Recent research advances in Computer Vision and Natural Language Processing\nhave introduced novel tasks that are paving the way for solving AI-complete\nproblems. One of those tasks is called Visual Question Answering (VQA). A VQA\nsystem must take an image and a free-form, open-ended natural language question\nabout the image, and produce a natural language answer as the output. Such a\ntask has drawn great attention from the scientific community, which generated a\nplethora of approaches that aim to improve the VQA predictive accuracy. Most of\nthem comprise three major components: (i) independent representation learning\nof images and questions; (ii) feature fusion so the model can use information\nfrom both sources to answer visual questions; and (iii) the generation of the\ncorrect answer in natural language. With so many approaches being recently\nintroduced, it became unclear the real contribution of each component for the\nultimate performance of the model. The main goal of this paper is to provide a\ncomprehensive analysis regarding the impact of each component in VQA models.\nOur extensive set of experiments cover both visual and textual elements, as\nwell as the combination of these representations in form of fusion and\nattention mechanisms. Our major contribution is to identify core components for\ntraining VQA models so as to maximize their predictive performance.", "published": "2020-02-12 17:25:50", "link": "http://arxiv.org/abs/2002.05104v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Attentional Speech Recognition Models Misbehave on Out-of-domain\n  Utterances", "abstract": "We discuss the problem of echographic transcription in autoregressive\nsequence-to-sequence attentional architectures for automatic speech\nrecognition, where a model produces very long sequences of repetitive outputs\nwhen presented with out-of-domain utterances. We decode audio from the British\nNational Corpus with an attentional encoder-decoder model trained solely on the\nLibriSpeech corpus. We observe that there are many 5-second recordings that\nproduce more than 500 characters of decoding output (i.e. more than 100\ncharacters per second). A frame-synchronous hybrid (DNN-HMM) model trained on\nthe same data does not produce these unusually long transcripts. These decoding\nissues are reproducible in a speech transformer model from ESPnet, and to a\nlesser extent in a self-attention CTC model, suggesting that these issues are\nintrinsic to the use of the attention mechanism. We create a separate length\nprediction model to predict the correct number of wordpieces in the output,\nwhich allows us to identify and truncate problematic decoding results without\nincreasing word error rates on the LibriSpeech task.", "published": "2020-02-12 18:53:56", "link": "http://arxiv.org/abs/2002.05150v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Image-to-Image Translation with Text Guidance", "abstract": "The goal of this paper is to embed controllable factors, i.e., natural\nlanguage descriptions, into image-to-image translation with generative\nadversarial networks, which allows text descriptions to determine the visual\nattributes of synthetic images. We propose four key components: (1) the\nimplementation of part-of-speech tagging to filter out non-semantic words in\nthe given description, (2) the adoption of an affine combination module to\neffectively fuse different modality text and image features, (3) a novel\nrefined multi-stage architecture to strengthen the differential ability of\ndiscriminators and the rectification ability of generators, and (4) a new\nstructure loss to further improve discriminators to better distinguish real and\nsynthetic images. Extensive experiments on the COCO dataset demonstrate that\nour method has a superior performance on both visual realism and semantic\nconsistency with given descriptions.", "published": "2020-02-12 21:09:15", "link": "http://arxiv.org/abs/2002.05235v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Comparative Study of Sequence Classification Models for Privacy Policy\n  Coverage Analysis", "abstract": "Privacy policies are legal documents that describe how a website will\ncollect, use, and distribute a user's data. Unfortunately, such documents are\noften overly complicated and filled with legal jargon; making it difficult for\nusers to fully grasp what exactly is being collected and why. Our solution to\nthis problem is to provide users with a coverage analysis of a given website's\nprivacy policy using a wide range of classical machine learning and deep\nlearning techniques. Given a website's privacy policy, the classifier\nidentifies the associated data practice for each logical segment. These data\npractices/labels are taken directly from the OPP-115 corpus. For example, the\ndata practice \"Data Retention\" refers to how long a website stores a user's\ninformation. The coverage analysis allows users to determine how many of the\nten possible data practices are covered, along with identifying the sections\nthat correspond to the data practices of particular interest.", "published": "2020-02-12 21:46:22", "link": "http://arxiv.org/abs/2003.04972v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deep Feature Embedding and Hierarchical Classification for Audio Scene\n  Classification", "abstract": "In this work, we propose an approach that features deep feature embedding\nlearning and hierarchical classification with triplet loss function for\nAcoustic Scene Classification (ASC). In the one hand, a deep convolutional\nneural network is firstly trained to learn a feature embedding from scene audio\nsignals. Via the trained convolutional neural network, the learned embedding\nembeds an input into the embedding feature space and transforms it into a\nhigh-level feature vector for representation. In the other hand, in order to\nexploit the structure of the scene categories, the original scene\nclassification problem is structured into a hierarchy where similar categories\nare grouped into meta-categories. Then, hierarchical classification is\naccomplished using deep neural network classifiers associated with triplet loss\nfunction. Our experiments show that the proposed system achieves good\nperformance on both the DCASE 2018 Task 1A and 1B datasets, resulting in\naccuracy gains of 15.6% and 16.6% absolute over the DCASE 2018 baseline on Task\n1A and 1B, respectively.", "published": "2020-02-12 09:12:31", "link": "http://arxiv.org/abs/2002.04857v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Content Based Singing Voice Extraction From a Musical Mixture", "abstract": "We present a deep learning based methodology for extracting the singing voice\nsignal from a musical mixture based on the underlying linguistic content. Our\nmodel follows an encoder decoder architecture and takes as input the magnitude\ncomponent of the spectrogram of a musical mixture with vocals. The encoder part\nof the model is trained via knowledge distillation using a teacher network to\nlearn a content embedding, which is decoded to generate the corresponding\nvocoder features. Using this methodology, we are able to extract the\nunprocessed raw vocal signal from the mixture even for a processed mixture\ndataset with singers not seen during training. While the nature of our system\nmakes it incongruous with traditional objective evaluation metrics, we use\nsubjective evaluation via listening tests to compare the methodology to\nstate-of-the-art deep learning based source separation algorithms. We also\nprovide sound examples and source code for reproducibility.", "published": "2020-02-12 12:03:40", "link": "http://arxiv.org/abs/2002.04933v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Active Learning for Sound Event Detection", "abstract": "This paper proposes an active learning system for sound event detection\n(SED). It aims at maximizing the accuracy of a learned SED model with limited\nannotation effort. The proposed system analyzes an initially unlabeled audio\ndataset, from which it selects sound segments for manual annotation. The\ncandidate segments are generated based on a proposed change point detection\napproach, and the selection is based on the principle of mismatch-first\nfarthest-traversal. During the training of SED models, recordings are used as\ntraining inputs, preserving the long-term context for annotated segments. The\nproposed system clearly outperforms reference methods in the two datasets used\nfor evaluation (TUT Rare Sound 2017 and TAU Spatial Sound 2019). Training with\nrecordings as context outperforms training with only annotated segments.\nMismatch-first farthest-traversal outperforms reference sample selection\nmethods based on random sampling and uncertainty sampling. Remarkably, the\nrequired annotation effort can be greatly reduced on the dataset where target\nsound events are rare: by annotating only 2% of the training data, the achieved\nSED performance is similar to annotating all the training data.", "published": "2020-02-12 14:46:55", "link": "http://arxiv.org/abs/2002.05033v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "x-vectors meet emotions: A study on dependencies between emotion and\n  speaker recognition", "abstract": "In this work, we explore the dependencies between speaker recognition and\nemotion recognition. We first show that knowledge learned for speaker\nrecognition can be reused for emotion recognition through transfer learning.\nThen, we show the effect of emotion on speaker recognition. For emotion\nrecognition, we show that using a simple linear model is enough to obtain good\nperformance on the features extracted from pre-trained models such as the\nx-vector model. Then, we improve emotion recognition performance by fine-tuning\nfor emotion classification. We evaluated our experiments on three different\ntypes of datasets: IEMOCAP, MSP-Podcast, and Crema-D. By fine-tuning, we\nobtained 30.40%, 7.99%, and 8.61% absolute improvement on IEMOCAP, MSP-Podcast,\nand Crema-D respectively over baseline model with no pre-training. Finally, we\npresent results on the effect of emotion on speaker verification. We observed\nthat speaker verification performance is prone to changes in test speaker\nemotions. We found that trials with angry utterances performed worst in all\nthree datasets. We hope our analysis will initiate a new line of research in\nthe speaker recognition community.", "published": "2020-02-12 15:13:07", "link": "http://arxiv.org/abs/2002.05039v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Improving automated segmentation of radio shows with audio embeddings", "abstract": "Audio features have been proven useful for increasing the performance of\nautomated topic segmentation systems. This study explores the novel task of\nusing audio embeddings for automated, topically coherent segmentation of radio\nshows. We created three different audio embedding generators using multi-class\nclassification tasks on three datasets from different domains. We evaluate\ntopic segmentation performance of the audio embeddings and compare it against a\ntext-only baseline. We find that a set-up including audio embeddings generated\nthrough a non-speech sound event classification task significantly outperforms\nour text-only baseline by 32.3% in F1-measure. In addition, we find that\ndifferent classification tasks yield audio embeddings that vary in segmentation\nperformance.", "published": "2020-02-12 19:40:22", "link": "http://arxiv.org/abs/2002.05194v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Autotuner: a Pitch Correcting Network for Singing Performances", "abstract": "We introduce a data-driven approach to automatic pitch correction of solo\nsinging performances. The proposed approach predicts note-wise pitch shifts\nfrom the relationship between the respective spectrograms of the singing and\naccompaniment. This approach differs from commercial systems, where vocal track\nnotes are usually shifted to be centered around pitches in a user-defined\nscore, or mapped to the closest pitch among the twelve equal-tempered scale\ndegrees. The proposed system treats pitch as a continuous value rather than\nrelying on a set of discretized notes found in musical scores, thus allowing\nfor improvisation and harmonization in the singing performance. We train our\nneural network model using a dataset of 4,702 amateur karaoke performances\nselected for good intonation. Our model is trained on both incorrect\nintonation, for which it learns a correction, and intentional pitch variation,\nwhich it learns to preserve. The proposed deep neural network with gated\nrecurrent units on top of convolutional layers shows promising performance on\nthe real-world score-free singing pitch correction task of autotuning.", "published": "2020-02-12 01:33:56", "link": "http://arxiv.org/abs/2002.05511v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "AlignNet: A Unifying Approach to Audio-Visual Alignment", "abstract": "We present AlignNet, a model that synchronizes videos with reference audios\nunder non-uniform and irregular misalignments. AlignNet learns the end-to-end\ndense correspondence between each frame of a video and an audio. Our method is\ndesigned according to simple and well-established principles: attention,\npyramidal processing, warping, and affinity function. Together with the model,\nwe release a dancing dataset Dance50 for training and evaluation. Qualitative,\nquantitative and subjective evaluation results on dance-music alignment and\nspeech-lip alignment demonstrate that our method far outperforms the\nstate-of-the-art methods. Project video and code are available at\nhttps://jianrenw.github.io/AlignNet.", "published": "2020-02-12 16:19:28", "link": "http://arxiv.org/abs/2002.05070v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
