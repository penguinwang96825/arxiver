{"title": "Domain Mismatch Doesn't Always Prevent Cross-Lingual Transfer Learning", "abstract": "Cross-lingual transfer learning without labeled target language data or\nparallel text has been surprisingly effective in zero-shot cross-lingual\nclassification, question answering, unsupervised machine translation, etc.\nHowever, some recent publications have claimed that domain mismatch prevents\ncross-lingual transfer, and their results show that unsupervised bilingual\nlexicon induction (UBLI) and unsupervised neural machine translation (UNMT) do\nnot work well when the underlying monolingual corpora come from different\ndomains (e.g., French text from Wikipedia but English text from UN\nproceedings). In this work, we show that a simple initialization regimen can\novercome much of the effect of domain mismatch in cross-lingual transfer. We\npre-train word and contextual embeddings on the concatenated domain-mismatched\ncorpora, and use these as initializations for three tasks: MUSE UBLI, UN\nParallel UNMT, and the SemEval 2017 cross-lingual word similarity task. In all\ncases, our results challenge the conclusions of prior work by showing that\nproper initialization can recover a large portion of the losses incurred by\ndomain mismatch.", "published": "2022-11-30 01:24:33", "link": "http://arxiv.org/abs/2211.16671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Findings of the WMT 2022 Shared Task on Translation Suggestion", "abstract": "We report the result of the first edition of the WMT shared task on\nTranslation Suggestion (TS). The task aims to provide alternatives for specific\nwords or phrases given the entire documents generated by machine translation\n(MT). It consists two sub-tasks, namely, the naive translation suggestion and\ntranslation suggestion with hints. The main difference is that some hints are\nprovided in sub-task two, therefore, it is easier for the model to generate\nmore accurate suggestions. For sub-task one, we provide the corpus for the\nlanguage pairs English-German and English-Chinese. And only English-Chinese\ncorpus is provided for the sub-task two.\n  We received 92 submissions from 5 participating teams in sub-task one and 6\nsubmissions for the sub-task 2, most of them covering all of the translation\ndirections. We used the automatic metric BLEU for evaluating the performance of\neach submission.", "published": "2022-11-30 03:48:36", "link": "http://arxiv.org/abs/2211.16717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit Knowledge Transfer for Weakly-Supervised Code Generation", "abstract": "Large language models (LLMs) can acquire strong code-generation capabilities\nthrough few-shot learning. In contrast, supervised fine-tuning is still needed\nfor smaller models to achieve good performance. Such fine-tuning demands a\nlarge number of task-specific NL-code pairs, which are expensive to obtain. In\nthis paper, we attempt to transfer the code generation ability of an LLM to a\nsmaller model with the aid of weakly-supervised data. More specifically, we\npropose explicit knowledge transfer (EKT), which uses the few-shot capabilities\nof a teacher LLM to create NL-code pairs that we then filter for correctness\nand fine-tune the student on. We evaluate EKT on the task of generating code\nsolutions to math word problems from the GSM8k dataset. We find that EKT not\nonly yields better performance than training with expert iteration, but also\noutperforms knowledge distillation, another form of knowledge transfer. A\nGPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%\npass@100 on GSM8k, while the same student and teacher trained with knowledge\ndistillation yield only a 3.7% pass@100. We also show that it is possible for a\nstudent model to outperform the teacher using EKT.", "published": "2022-11-30 04:51:26", "link": "http://arxiv.org/abs/2211.16740v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KRLS: Improving End-to-End Response Generation in Task Oriented Dialog\n  with Reinforced Keywords Learning", "abstract": "In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train\na model to directly optimize response for task-related metrics. However, RL\nneeds to perform exploration, which can be time-consuming due to the slow\nauto-regressive sequence generation process. We investigate an approach to\ncreate a more efficient RL-based algorithm to improve TOD performance in an\noffline setting. First, we use a faster generation procedure that samples from\nindependent next-word distributions after training the language model (LM) with\nsupervised learning. We then introduce a fine-grained reward function to help\nthe model focus on learning key information in a dialog, by measuring the\nimportance and semantic closeness of each generated token. Experiments on the\nMultiWoZ dataset show our new training algorithm, Keywords Reinforcement\nLearning with Next-word Sampling (KRLS), achieves state-of-the-art performance\non the end-to-end response generation task, with a 15% training time reduction\ncompared to a standard RL algorithm using auto-regressive generation.", "published": "2022-11-30 06:27:46", "link": "http://arxiv.org/abs/2211.16773v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalised Spherical Text Embedding", "abstract": "This paper aims to provide an unsupervised modelling approach that allows for\na more flexible representation of text embeddings. It jointly encodes the words\nand the paragraphs as individual matrices of arbitrary column dimension with\nunit Frobenius norm. The representation is also linguistically motivated with\nthe introduction of a novel similarity metric. The proposed modelling and the\nnovel similarity metric exploits the matrix structure of embeddings. We then go\non to show that the same matrices can be reshaped into vectors of unit norm and\ntransform our problem into an optimization problem over the spherical manifold.\nWe exploit manifold optimization to efficiently train the matrix embeddings. We\nalso quantitatively verify the quality of our text embeddings by showing that\nthey demonstrate improved results in document classification, document\nclustering, and semantic textual similarity benchmark tests.", "published": "2022-11-30 07:50:07", "link": "http://arxiv.org/abs/2211.16801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Camelira: An Arabic Multi-Dialect Morphological Disambiguator", "abstract": "We present Camelira, a web-based Arabic multi-dialect morphological\ndisambiguation tool that covers four major variants of Arabic: Modern Standard\nArabic, Egyptian, Gulf, and Levantine. Camelira offers a user-friendly web\ninterface that allows researchers and language learners to explore various\nlinguistic information, such as part-of-speech, morphological features, and\nlemmas. Our system also provides an option to automatically choose an\nappropriate dialect-specific disambiguator based on the prediction of a dialect\nidentification component. Camelira is publicly accessible at\nhttp://camelira.camel-lab.com.", "published": "2022-11-30 08:02:11", "link": "http://arxiv.org/abs/2211.16807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Probabilistic-Logic based Commonsense Representation Framework for\n  Modelling Inferences with Multiple Antecedents and Varying Likelihoods", "abstract": "Commonsense knowledge-graphs (CKGs) are important resources towards building\nmachines that can 'reason' on text or environmental inputs and make inferences\nbeyond perception. While current CKGs encode world knowledge for a large number\nof concepts and have been effectively utilized for incorporating commonsense in\nneural models, they primarily encode declarative or single-condition\ninferential knowledge and assume all conceptual beliefs to have the same\nlikelihood. Further, these CKGs utilize a limited set of relations shared\nacross concepts and lack a coherent knowledge organization structure resulting\nin redundancies as well as sparsity across the larger knowledge graph.\nConsequently, today's CKGs, while useful for a first level of reasoning, do not\nadequately capture deeper human-level commonsense inferences which can be more\nnuanced and influenced by multiple contextual or situational factors.\n  Accordingly, in this work, we study how commonsense knowledge can be better\nrepresented by -- (i) utilizing a probabilistic logic representation scheme to\nmodel composite inferential knowledge and represent conceptual beliefs with\nvarying likelihoods and (ii) incorporating a hierarchical conceptual ontology\nto identify salient concept-relevant relations and organize beliefs at\ndifferent conceptual levels. Our resulting knowledge representation framework\ncan encode a wider variety of world knowledge and represent beliefs flexibly\nusing grounded concepts as well as free-text phrases. As a result, the\nframework can be utilized as both a traditional free-text knowledge graph and a\ngrounded logic-based inference system more suitable for neuro-symbolic\napplications. We describe how we extend the PrimeNet knowledge base with our\nframework through crowd-sourcing and expert-annotation, and demonstrate its\napplication for more interpretable passage-based semantic parsing and question\nanswering.", "published": "2022-11-30 08:44:30", "link": "http://arxiv.org/abs/2211.16822v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting text decomposition methods for NLI-based factuality scoring\n  of summaries", "abstract": "Scoring the factuality of a generated summary involves measuring the degree\nto which a target text contains factual information using the input document as\nsupport. Given the similarities in the problem formulation, previous work has\nshown that Natural Language Inference models can be effectively repurposed to\nperform this task. As these models are trained to score entailment at a\nsentence level, several recent studies have shown that decomposing either the\ninput document or the summary into sentences helps with factuality scoring. But\nis fine-grained decomposition always a winning strategy? In this paper we\nsystematically compare different granularities of decomposition -- from\ndocument to sub-sentence level, and we show that the answer is no. Our results\nshow that incorporating additional context can yield improvement, but that this\ndoes not necessarily apply to all datasets. We also show that small changes to\npreviously proposed entailment-based scoring methods can result in better\nperformance, highlighting the need for caution in model and methodology\nselection for downstream tasks.", "published": "2022-11-30 09:54:37", "link": "http://arxiv.org/abs/2211.16853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Major Obstacle for NLP Research: Let's Talk about Time Allocation!", "abstract": "The field of natural language processing (NLP) has grown over the last few\nyears: conferences have become larger, we have published an incredible amount\nof papers, and state-of-the-art research has been implemented in a large\nvariety of customer-facing products. However, this paper argues that we have\nbeen less successful than we should have been and reflects on where and how the\nfield fails to tap its full potential. Specifically, we demonstrate that, in\nrecent years, subpar time allocation has been a major obstacle for NLP\nresearch. We outline multiple concrete problems together with their negative\nconsequences and, importantly, suggest remedies to improve the status quo. We\nhope that this paper will be a starting point for discussions around which\ncommon practices are -- or are not -- beneficial for NLP research.", "published": "2022-11-30 10:00:12", "link": "http://arxiv.org/abs/2211.16858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rephrasing the Reference for Non-Autoregressive Machine Translation", "abstract": "Non-autoregressive neural machine translation (NAT) models suffer from the\nmulti-modality problem that there may exist multiple possible translations of a\nsource sentence, so the reference sentence may be inappropriate for the\ntraining when the NAT output is closer to other translations. In response to\nthis problem, we introduce a rephraser to provide a better training target for\nNAT by rephrasing the reference sentence according to the NAT output. As we\ntrain NAT based on the rephraser output rather than the reference sentence, the\nrephraser output should fit well with the NAT output and not deviate too far\nfrom the reference, which can be quantified as reward functions and optimized\nby reinforcement learning. Experiments on major WMT benchmarks and NAT\nbaselines show that our approach consistently improves the translation quality\nof NAT. Specifically, our best variant achieves comparable performance to the\nautoregressive Transformer, while being 14.7 times more efficient in inference.", "published": "2022-11-30 10:05:03", "link": "http://arxiv.org/abs/2211.16863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformers are Short Text Classifiers: A Study of Inductive Short Text\n  Classifiers on Benchmarks and Real-world Datasets", "abstract": "Short text classification is a crucial and challenging aspect of Natural\nLanguage Processing. For this reason, there are numerous highly specialized\nshort text classifiers. However, in recent short text research, State of the\nArt (SOTA) methods for traditional text classification, particularly the pure\nuse of Transformers, have been unexploited. In this work, we examine the\nperformance of a variety of short text classifiers as well as the top\nperforming traditional text classifier. We further investigate the effects on\ntwo new real-world short text datasets in an effort to address the issue of\nbecoming overly dependent on benchmark datasets with a limited number of\ncharacteristics. Our experiments unambiguously demonstrate that Transformers\nachieve SOTA accuracy on short text classification tasks, raising the question\nof whether specialized short text techniques are necessary.", "published": "2022-11-30 10:25:24", "link": "http://arxiv.org/abs/2211.16878v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-PuDu at SemEval-2022 Task 6: Multilingual Learning for English and\n  Arabic Sarcasm Detection", "abstract": "Detecting sarcasm and verbal irony from people's subjective statements is\ncrucial to understanding their intended meanings and real sentiments and\npositions in social scenarios. This paper describes the X-PuDu system that\nparticipated in SemEval-2022 Task 6, iSarcasmEval - Intended Sarcasm Detection\nin English and Arabic, which aims at detecting intended sarcasm in various\nsettings of natural language understanding. Our solution finetunes pre-trained\nlanguage models, such as ERNIE-M and DeBERTa, under the multilingual settings\nto recognize the irony from Arabic and English texts. Our system ranked second\nout of 43, and ninth out of 32 in Task A: one-sentence detection in English and\nArabic; fifth out of 22 in Task B: binary multi-label classification in\nEnglish; first out of 16, and fifth out of 13 in Task C: sentence-pair\ndetection in English and Arabic.", "published": "2022-11-30 10:34:08", "link": "http://arxiv.org/abs/2211.16883v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-Specific Embeddings for Ante-Hoc Explainable Text Classification", "abstract": "Current state-of-the-art approaches to text classification typically leverage\nBERT-style Transformer models with a softmax classifier, jointly fine-tuned to\npredict class labels of a target task. In this paper, we instead propose an\nalternative training objective in which we learn task-specific embeddings of\ntext: our proposed objective learns embeddings such that all texts that share\nthe same target class label should be close together in the embedding space,\nwhile all others should be far apart. This allows us to replace the softmax\nclassifier with a more interpretable k-nearest-neighbor classification\napproach. In a series of experiments, we show that this yields a number of\ninteresting benefits: (1) The resulting order induced by distances in the\nembedding space can be used to directly explain classification decisions. (2)\nThis facilitates qualitative inspection of the training data, helping us to\nbetter understand the problem space and identify labelling quality issues. (3)\nThe learned distances to some degree generalize to unseen classes, allowing us\nto incrementally add new classes without retraining the model. We present\nextensive experiments which show that the benefits of ante-hoc explainability\nand incremental learning come at no cost in overall classification accuracy,\nthus pointing to practical applicability of our proposed approach.", "published": "2022-11-30 19:56:25", "link": "http://arxiv.org/abs/2212.00086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Alignment in the Era of Deep Learning: A Tutorial", "abstract": "The word alignment task, despite its prominence in the era of statistical\nmachine translation (SMT), is niche and under-explored today. In this two-part\ntutorial, we argue for the continued relevance for word alignment. The first\npart provides a historical background to word alignment as a core component of\nthe traditional SMT pipeline. We zero-in on GIZA++, an unsupervised,\nstatistical word aligner with surprising longevity. Jumping forward to the era\nof neural machine translation (NMT), we show how insights from word alignment\ninspired the attention mechanism fundamental to present-day NMT. The second\npart shifts to a survey approach. We cover neural word aligners, showing the\nslow but steady progress towards surpassing GIZA++ performance. Finally, we\ncover the present-day applications of word alignment, from cross-lingual\nannotation projection, to improving translation.", "published": "2022-11-30 22:07:34", "link": "http://arxiv.org/abs/2212.00138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Relation and Event Type Discovery with Type Abstraction", "abstract": "Conventional closed-world information extraction (IE) approaches rely on\nhuman ontologies to define the scope for extraction. As a result, such\napproaches fall short when applied to new domains. This calls for systems that\ncan automatically infer new types from given corpora, a task which we refer to\nas type discovery. To tackle this problem, we introduce the idea of type\nabstraction, where the model is prompted to generalize and name the type. Then\nwe use the similarity between inferred names to induce clusters. Observing that\nthis abstraction-based representation is often complementary to the\nentity/trigger token representation, we set up these two representations as two\nviews and design our model as a co-training framework. Our experiments on\nmultiple relation extraction and event extraction datasets consistently show\nthe advantage of our type abstraction approach. Code available at\nhttps://github.com/raspberryice/type-discovery-abs.", "published": "2022-11-30 23:47:49", "link": "http://arxiv.org/abs/2212.00178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Identification of Motivation for Code-Switching in Speech\n  Transcripts", "abstract": "Code-switching, or switching between languages, occurs for many reasons and\nhas important linguistic, sociological, and cultural implications. Multilingual\nspeakers code-switch for a variety of purposes, such as expressing emotions,\nborrowing terms, making jokes, introducing a new topic, etc. The reason for\ncode-switching may be quite useful for analysis, but is not readily apparent.\nTo remedy this situation, we annotate a new dataset of motivations for\ncode-switching in Spanish-English. We build the first system (to our knowledge)\nto automatically identify a wide range of motivations that speakers code-switch\nin everyday speech, achieving an accuracy of 75% across all motivations.\nAdditionally, we show that the system can be adapted to new language pairs,\nachieving 66% accuracy on a new language pair (Hindi-English), demonstrating\nthe cross-lingual applicability of our annotation scheme", "published": "2022-11-30 05:45:05", "link": "http://arxiv.org/abs/2212.08565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logic and Commonsense-Guided Temporal Knowledge Graph Completion", "abstract": "A temporal knowledge graph (TKG) stores the events derived from the data\ninvolving time. Predicting events is extremely challenging due to the\ntime-sensitive property of events. Besides, the previous TKG completion (TKGC)\napproaches cannot represent both the timeliness and the causality properties of\nevents, simultaneously. To address these challenges, we propose a Logic and\nCommonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive\nrepresentation involving timeliness and causality of events, together with the\ntime-independent representation of events from the perspective of commonsense.\nSpecifically, we design a temporal rule learning algorithm to construct a\nrule-guided predicate embedding regularization strategy for learning the\ncausality among events. Furthermore, we could accurately evaluate the\nplausibility of events via auxiliary commonsense knowledge. The experimental\nresults of TKGC task illustrate the significant performance improvements of our\nmodel compared with the existing approaches. More interestingly, our model is\nable to provide the explainability of the predicted results in the view of\ncausal inference. The source code and datasets of this paper are available at\nhttps://github.com/ngl567/LCGE.", "published": "2022-11-30 10:06:55", "link": "http://arxiv.org/abs/2211.16865v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AIONER: All-in-one scheme-based biomedical named entity recognition\n  using deep learning", "abstract": "Biomedical named entity recognition (BioNER) seeks to automatically recognize\nbiomedical entities in natural language text, serving as a necessary foundation\nfor downstream text mining tasks and applications such as information\nextraction and question answering. Manually labeling training data for the\nBioNER task is costly, however, due to the significant domain expertise\nrequired for accurate annotation. The resulting data scarcity causes current\nBioNER approaches to be prone to overfitting, to suffer from limited\ngeneralizability, and to address a single entity type at a time (e.g., gene or\ndisease). We therefore propose a novel all-in-one (AIO) scheme that uses\nexternal data from existing annotated resources to enhance the accuracy and\nstability of BioNER models. We further present AIONER, a general-purpose BioNER\ntool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER\non 14 BioNER benchmark tasks and show that AIONER is effective, robust, and\ncompares favorably to other state-of-the-art approaches such as multi-task\nlearning. We further demonstrate the practical utility of AIONER in three\nindependent tasks to recognize entity types not previously seen in training\ndata, as well as the advantages of AIONER over existing methods for processing\nbiomedical text at a large scale (e.g., the entire PubMed data).", "published": "2022-11-30 12:35:00", "link": "http://arxiv.org/abs/2211.16944v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Pipeline for Generating, Annotating and Employing Synthetic Data for\n  Real World Question Answering", "abstract": "Question Answering (QA) is a growing area of research, often used to\nfacilitate the extraction of information from within documents.\nState-of-the-art QA models are usually pre-trained on domain-general corpora\nlike Wikipedia and thus tend to struggle on out-of-domain documents without\nfine-tuning. We demonstrate that synthetic domain-specific datasets can be\ngenerated easily using domain-general models, while still providing significant\nimprovements to QA performance. We present two new tools for this task: A\nflexible pipeline for validating the synthetic QA data and training downstream\nmodels on it, and an online interface to facilitate human annotation of this\ngenerated data. Using this interface, crowdworkers labelled 1117 synthetic QA\npairs, which we then used to fine-tune downstream models and improve\ndomain-specific QA performance by 8.75 F1.", "published": "2022-11-30 13:24:30", "link": "http://arxiv.org/abs/2211.16971v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Rationale-Guided Few-Shot Classification to Detect Abusive Language", "abstract": "Abusive language is a concerning problem in online social media. Past\nresearch on detecting abusive language covers different platforms, languages,\ndemographies, etc. However, models trained using these datasets do not perform\nwell in cross-domain evaluation settings. To overcome this, a common strategy\nis to use a few samples from the target domain to train models to get better\nperformance in that domain (cross-domain few-shot training). However, this\nmight cause the models to overfit the artefacts of those samples. A compelling\nsolution could be to guide the models toward rationales, i.e., spans of text\nthat justify the text's label. This method has been found to improve model\nperformance in the in-domain setting across various NLP tasks. In this paper,\nwe propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language\ndetection. We first build a multitask learning setup to jointly learn\nrationales, targets, and labels, and find a significant improvement of 6% macro\nF1 on the rationale detection task over training solely rationale classifiers.\nWe introduce two rationale-integrated BERT-based architectures (the RGFS\nmodels) and evaluate our systems over five different abusive language datasets,\nfinding that in the few-shot classification setting, RGFS-based models\noutperform baseline models by about 7% in macro F1 scores and perform\ncompetitively to models finetuned on other source domains. Furthermore,\nRGFS-based models outperform LIME/SHAP-based approaches in terms of\nplausibility and are close in performance in terms of faithfulness.", "published": "2022-11-30 14:47:14", "link": "http://arxiv.org/abs/2211.17046v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data\n  Format", "abstract": "Task-oriented dialogue (TOD) systems function as digital assistants, guiding\nusers through various tasks such as booking flights or finding restaurants.\nExisting toolkits for building TOD systems often fall short of in delivering\ncomprehensive arrays of data, models, and experimental environments with a\nuser-friendly experience. We introduce ConvLab-3: a multifaceted dialogue\nsystem toolkit crafted to bridge this gap. Our unified data format simplifies\nthe integration of diverse datasets and models, significantly reducing\ncomplexity and cost for studying generalization and transfer. Enhanced with\nrobust reinforcement learning (RL) tools, featuring a streamlined training\nprocess, in-depth evaluation tools, and a selection of user simulators,\nConvLab-3 supports the rapid development and evaluation of robust dialogue\npolicies. Through an extensive study, we demonstrate the efficacy of transfer\nlearning and RL and showcase that ConvLab-3 is not only a powerful tool for\nseasoned researchers but also an accessible platform for newcomers.", "published": "2022-11-30 16:37:42", "link": "http://arxiv.org/abs/2211.17148v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Misogyny classification of German newspaper forum comments", "abstract": "This paper presents work on detecting misogyny in the comments of a large\nAustrian German language newspaper forum. We describe the creation of a corpus\nof 6600 comments which were annotated with 5 levels of misogyny. The forum\nmoderators were involved as experts in the creation of the annotation\nguidelines and the annotation of the comments. We also describe the results of\ntraining transformer-based classification models for both binarized and\noriginal label classification of that corpus.", "published": "2022-11-30 16:56:09", "link": "http://arxiv.org/abs/2211.17163v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fast Inference from Transformers via Speculative Decoding", "abstract": "Inference from large autoregressive models like Transformers is slow -\ndecoding K tokens takes K serial runs of the model. In this work we introduce\nspeculative decoding - an algorithm to sample from autoregressive models faster\nwithout any changes to the outputs, by computing several tokens in parallel. At\nthe heart of our approach lie the observations that (1) hard language-modeling\ntasks often include easier subtasks that can be approximated well by more\nefficient models, and (2) using speculative execution and a novel sampling\nmethod, we can make exact decoding from the large models faster, by running\nthem in parallel on the outputs of the approximation models, potentially\ngenerating several tokens concurrently, and without changing the distribution.\nOur method can accelerate existing off-the-shelf models without retraining or\narchitecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration\ncompared to the standard T5X implementation, with identical outputs.", "published": "2022-11-30 17:33:28", "link": "http://arxiv.org/abs/2211.17192v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CREPE: Open-Domain Question Answering with False Presuppositions", "abstract": "Information seeking users often pose questions with false presuppositions,\nespecially when asking about unfamiliar topics. Most existing question\nanswering (QA) datasets, in contrast, assume all questions have well defined\nanswers. We introduce CREPE, a QA dataset containing a natural distribution of\npresupposition failures from online information-seeking forums. We find that\n25% of questions contain false presuppositions, and provide annotations for\nthese presuppositions and their corrections. Through extensive baseline\nexperiments, we show that adaptations of existing open-domain QA models can\nfind presuppositions moderately well, but struggle when predicting whether a\npresupposition is factually correct. This is in large part due to difficulty in\nretrieving relevant evidence passages from a large text corpus. CREPE provides\na benchmark to study question answering in the wild, and our analyses provide\navenues for future work in better modeling and further studying the task.", "published": "2022-11-30 18:54:49", "link": "http://arxiv.org/abs/2211.17257v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation", "abstract": "Household environments are visually diverse. Embodied agents performing\nVision-and-Language Navigation (VLN) in the wild must be able to handle this\ndiversity, while also following arbitrary language instructions. Recently,\nVision-Language models like CLIP have shown great performance on the task of\nzero-shot object recognition. In this work, we ask if these models are also\ncapable of zero-shot language grounding. In particular, we utilize CLIP to\ntackle the novel problem of zero-shot VLN using natural language referring\nexpressions that describe target objects, in contrast to past work that used\nsimple language templates describing object classes. We examine CLIP's\ncapability in making sequential navigational decisions without any\ndataset-specific finetuning, and study how it influences the path that an agent\ntakes. Our results on the coarse-grained instruction following task of REVERIE\ndemonstrate the navigational capability of CLIP, surpassing the supervised\nbaseline in terms of both success rate (SR) and success weighted by path length\n(SPL). More importantly, we quantitatively show that our CLIP-based zero-shot\napproach generalizes better to show consistent performance across environments\nwhen compared to SOTA, fully supervised learning approaches when evaluated via\nRelative Change in Success (RCS).", "published": "2022-11-30 00:38:54", "link": "http://arxiv.org/abs/2211.16649v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "A minor extension of the logistic equation for growth of word counts on\n  online media: Parametric description of diversity of growth phenomena in\n  society", "abstract": "To understand the growing phenomena of new vocabulary on nationwide online\nsocial media, we analyzed monthly word count time series extracted from\napproximately 1 billion Japanese blog articles from 2007 to 2019. In\nparticular, we first introduced the extended logistic equation by adding one\nparameter to the original equation and showed that the model can consistently\nreproduce various patterns of actual growth curves, such as the logistic\nfunction, linear growth, and finite-time divergence. Second, by analyzing the\nmodel parameters, we found that the typical growth pattern is not only a\nlogistic function, which often appears in various complex systems, but also a\nnontrivial growth curve that starts with an exponential function and\nasymptotically approaches a power function without a steady state. Furthermore,\nwe observed a connection between the functional form of growth and the\npeak-out. Finally, we showed that the proposed model and statistical properties\nare also valid for Google Trends data (English, French, Spanish, and Japanese),\nwhich is a time series of the nationwide popularity of search queries.", "published": "2022-11-30 04:37:36", "link": "http://arxiv.org/abs/2211.16733v2", "categories": ["physics.soc-ph", "cs.CL", "stat.AP"], "primary_category": "physics.soc-ph"}
{"title": "sEHR-CE: Language modelling of structured EHR data for efficient and\n  generalizable patient cohort expansion", "abstract": "Electronic health records (EHR) offer unprecedented opportunities for\nin-depth clinical phenotyping and prediction of clinical outcomes. Combining\nmultiple data sources is crucial to generate a complete picture of disease\nprevalence, incidence and trajectories. The standard approach to combining\nclinical data involves collating clinical terms across different terminology\nsystems using curated maps, which are often inaccurate and/or incomplete. Here,\nwe propose sEHR-CE, a novel framework based on transformers to enable\nintegrated phenotyping and analyses of heterogeneous clinical datasets without\nrelying on these mappings. We unify clinical terminologies using textual\ndescriptors of concepts, and represent individuals' EHR as sections of text. We\nthen fine-tune pre-trained language models to predict disease phenotypes more\naccurately than non-text and single terminology approaches. We validate our\napproach using primary and secondary care data from the UK Biobank, a\nlarge-scale research study. Finally, we illustrate in a type 2 diabetes use\ncase how sEHR-CE identifies individuals without diagnosis that share clinical\ncharacteristics with patients.", "published": "2022-11-30 16:00:43", "link": "http://arxiv.org/abs/2211.17121v1", "categories": ["cs.CL", "cs.LG", "stat.AP"], "primary_category": "cs.CL"}
{"title": "BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model\n  From Scratch?", "abstract": "Pretrained transformer models have achieved state-of-the-art results in many\ntasks and benchmarks recently. Many state-of-the-art Language Models (LMs),\nhowever, do not scale well above the threshold of 512 input tokens. In\nspecialized domains though (such as legal, scientific or biomedical), models\noften need to process very long text (sometimes well above 10000 tokens). Even\nthough many efficient transformers have been proposed (such as Longformer,\nBigBird or FNet), so far, only very few such efficient models are available for\nspecialized domains. Additionally, since the pretraining process is extremely\ncostly in general - but even more so as the sequence length increases - it is\noften only in reach of large research labs. One way of making pretraining\ncheaper is the Replaced Token Detection (RTD) task, by providing more signal\nduring training, since the loss can be computed over all tokens. In this work,\nwe train Longformer models with the efficient RTD task on legal data to\nshowcase that pretraining efficient LMs is possible using much less compute. We\nevaluate the trained models on challenging summarization tasks requiring the\nmodel to summarize long texts to show to what extent the models can achieve\ngood performance on downstream tasks. We find that both the small and base\nmodels outperform their baselines on the in-domain BillSum and out-of-domain\nPubMed tasks in their respective parameter range. We publish our code and\nmodels for research purposes.", "published": "2022-11-30 16:09:20", "link": "http://arxiv.org/abs/2211.17135v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "EURO: ESPnet Unsupervised ASR Open-source Toolkit", "abstract": "This paper describes the ESPnet Unsupervised ASR Open-source Toolkit (EURO),\nan end-to-end open-source toolkit for unsupervised automatic speech recognition\n(UASR). EURO adopts the state-of-the-art UASR learning method introduced by the\nWav2vec-U, originally implemented at FAIRSEQ, which leverages self-supervised\nspeech representations and adversarial training. In addition to wav2vec2, EURO\nextends the functionality and promotes reproducibility for UASR tasks by\nintegrating S3PRL and k2, resulting in flexible frontends from 27\nself-supervised models and various graph-based decoding strategies. EURO is\nimplemented in ESPnet and follows its unified pipeline to provide UASR recipes\nwith a complete setup. This improves the pipeline's efficiency and allows EURO\nto be easily applied to existing datasets in ESPnet. Extensive experiments on\nthree mainstream self-supervised models demonstrate the toolkit's effectiveness\nand achieve state-of-the-art UASR performance on TIMIT and LibriSpeech\ndatasets. EURO will be publicly available at https://github.com/espnet/espnet,\naiming to promote this exciting and emerging research area based on UASR\nthrough open-source activity.", "published": "2022-11-30 17:40:54", "link": "http://arxiv.org/abs/2211.17196v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT", "abstract": "In this paper, we present ExtremeBERT, a toolkit for accelerating and\ncustomizing BERT pretraining. Our goal is to provide an easy-to-use BERT\npretraining toolkit for the research community and industry. Thus, the\npretraining of popular language models on customized datasets is affordable\nwith limited resources. Experiments show that, to achieve the same or better\nGLUE scores, the time cost of our toolkit is over $6\\times$ times less for BERT\nBase and $9\\times$ times less for BERT Large when compared with the original\nBERT paper. The documentation and code are released at\nhttps://github.com/extreme-bert/extreme-bert under the Apache-2.0 license.", "published": "2022-11-30 17:50:35", "link": "http://arxiv.org/abs/2211.17201v1", "categories": ["cs.CL", "cs.LG", "math.OC"], "primary_category": "cs.CL"}
{"title": "xTrimoABFold: De novo Antibody Structure Prediction without MSA", "abstract": "In the field of antibody engineering, an essential task is to design a novel\nantibody whose paratopes bind to a specific antigen with correct epitopes.\nUnderstanding antibody structure and its paratope can facilitate a mechanistic\nunderstanding of its function. Therefore, antibody structure prediction from\nits sequence alone has always been a highly valuable problem for de novo\nantibody design. AlphaFold2, a breakthrough in the field of structural biology,\nprovides a solution to predict protein structure based on protein sequences and\ncomputationally expensive coevolutionary multiple sequence alignments (MSAs).\nHowever, the computational efficiency and undesirable prediction accuracy of\nantibodies, especially on the complementarity-determining regions (CDRs) of\nantibodies limit their applications in the industrially high-throughput drug\ndesign. To learn an informative representation of antibodies, we employed a\ndeep antibody language model (ALM) on curated sequences from the observed\nantibody space database via a transformer model. We also developed a novel\nmodel named xTrimoABFold to predict antibody structure from antibody sequence\nbased on the pretrained ALM as well as efficient evoformers and structural\nmodules. The model was trained end-to-end on the antibody structures in PDB by\nminimizing the ensemble loss of domain-specific focal loss on CDR and the\nframe-aligned point loss. xTrimoABFold outperforms AlphaFold2 and other protein\nlanguage model based SOTAs, e.g., OmegaFold, HelixFold-Single, and IgFold with\na large significant margin (30+\\% improvement on RMSD) while performing 151\ntimes faster than AlphaFold2. To the best of our knowledge, xTrimoABFold\nachieved state-of-the-art antibody structure prediction. Its improvement in\nboth accuracy and efficiency makes it a valuable tool for de novo antibody\ndesign and could make further improvements in immuno-theory.", "published": "2022-11-30 09:26:08", "link": "http://arxiv.org/abs/2212.00735v3", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.QM"}
{"title": "An Overview of Indian Spoken Language Recognition from Machine Learning\n  Perspective", "abstract": "Automatic spoken language identification (LID) is a very important research\nfield in the era of multilingual voice-command-based human-computer interaction\n(HCI). A front-end LID module helps to improve the performance of many\nspeech-based applications in the multilingual scenario. India is a populous\ncountry with diverse cultures and languages. The majority of the Indian\npopulation needs to use their respective native languages for verbal\ninteraction with machines. Therefore, the development of efficient Indian\nspoken language recognition systems is useful for adapting smart technologies\nin every section of Indian society. The field of Indian LID has started gaining\nmomentum in the last two decades, mainly due to the development of several\nstandard multilingual speech corpora for the Indian languages. Even though\nsignificant research progress has already been made in this field, to the best\nof our knowledge, there are not many attempts to analytically review them\ncollectively. In this work, we have conducted one of the very first attempts to\npresent a comprehensive review of the Indian spoken language recognition\nresearch field. In-depth analysis has been presented to emphasize the unique\nchallenges of low-resource and mutual influences for developing LID systems in\nthe Indian contexts. Several essential aspects of the Indian LID research, such\nas the detailed description of the available speech corpora, the major research\ncontributions, including the earlier attempts based on statistical modeling to\nthe recent approaches based on different neural network architectures, and the\nfuture research trends are discussed. This review work will help assess the\nstate of the present Indian LID research by any active researcher or any\nresearch enthusiasts from related fields.", "published": "2022-11-30 11:03:51", "link": "http://arxiv.org/abs/2212.03812v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VideoDubber: Machine Translation with Speech-Aware Length Control for\n  Video Dubbing", "abstract": "Video dubbing aims to translate the original speech in a film or television\nprogram into the speech in a target language, which can be achieved with a\ncascaded system consisting of speech recognition, machine translation and\nspeech synthesis. To ensure the translated speech to be well aligned with the\ncorresponding video, the length/duration of the translated speech should be as\nclose as possible to that of the original speech, which requires strict length\ncontrol. Previous works usually control the number of words or characters\ngenerated by the machine translation model to be similar to the source\nsentence, without considering the isochronicity of speech as the speech\nduration of words/characters in different languages varies. In this paper, we\npropose a machine translation system tailored for the task of video dubbing,\nwhich directly considers the speech duration of each token in translation, to\nmatch the length of source and target speech. Specifically, we control the\nspeech length of generated sentence by guiding the prediction of each word with\nthe duration information, including the speech duration of itself as well as\nhow much duration is left for the remaining words. We design experiments on\nfour language directions (German -> English, Spanish -> English, Chinese <->\nEnglish), and the results show that the proposed method achieves better length\ncontrol ability on the generated speech than baseline methods. To make up the\nlack of real-world datasets, we also construct a real-world test set collected\nfrom films to provide comprehensive evaluations on the video dubbing task.", "published": "2022-11-30 12:09:40", "link": "http://arxiv.org/abs/2211.16934v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Topological Data Analysis for Speech Processing", "abstract": "We apply topological data analysis (TDA) to speech classification problems\nand to the introspection of a pretrained speech model, HuBERT. To this end, we\nintroduce a number of topological and algebraic features derived from\nTransformer attention maps and embeddings. We show that a simple linear\nclassifier built on top of such features outperforms a fine-tuned\nclassification head. In particular, we achieve an improvement of about $9\\%$\naccuracy and $5\\%$ ERR on four common datasets; on CREMA-D, the proposed\nfeature set reaches a new state of the art performance with accuracy $80.155$.\nWe also show that topological features are able to reveal functional roles of\nspeech Transformer heads; e.g., we find the heads capable to distinguish\nbetween pairs of sample sources (natural/synthetic) or voices without any\ndownstream fine-tuning. Our results demonstrate that TDA is a promising new\napproach for speech analysis, especially for tasks that require structural\nprediction. Appendices, an introduction to TDA, and other additional materials\nare available here - https://topohubert.github.io/speech-topology-webpages/", "published": "2022-11-30 18:22:37", "link": "http://arxiv.org/abs/2211.17223v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "math.AT"], "primary_category": "cs.SD"}
{"title": "MSV Challenge 2022: NPU-HC Speaker Verification System for Low-resource\n  Indian Languages", "abstract": "This report describes the NPU-HC speaker verification system submitted to the\nO-COCOSDA Multi-lingual Speaker Verification (MSV) Challenge 2022, which\nfocuses on developing speaker verification systems for low-resource Asian\nlanguages. We participate in the I-MSV track, which aims to develop speaker\nverification systems for various Indian languages. In this challenge, we first\nexplore different neural network frameworks for low-resource speaker\nverification. Then we leverage vanilla fine-tuning and weight transfer\nfine-tuning to transfer the out-domain pre-trained models to the in-domain\nIndian dataset. Specifically, the weight transfer fine-tuning aims to constrain\nthe distance of the weights between the pre-trained model and the fine-tuned\nmodel, which takes advantage of the previously acquired discriminative ability\nfrom the large-scale out-domain datasets and avoids catastrophic forgetting and\noverfitting at the same time. Finally, score fusion is adopted to further\nimprove performance. Together with the above contributions, we obtain 0.223%\nEER on the public evaluation set, ranking 2nd place on the leaderboard. On the\nprivate evaluation set, the EER of our submitted system is 2.123% and 0.630%\nfor the constrained and unconstrained sub-tasks of the I-MSV track, leading to\nthe 1st and 3rd place in the ranking, respectively.", "published": "2022-11-30 02:27:51", "link": "http://arxiv.org/abs/2211.16694v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A General Unfolding Speech Enhancement Method Motivated by Taylor's\n  Theorem", "abstract": "While deep neural networks have facilitated significant advancements in the\nfield of speech enhancement, most existing methods are developed following\neither empirical or relatively blind criteria, lacking adequate guidelines in\npipeline design. Inspired by Taylor's theorem, we propose a general unfolding\nframework for both single- and multi-channel speech enhancement tasks.\nConcretely, we formulate the complex spectrum recovery into the spectral\nmagnitude mapping in the neighborhood space of the noisy mixture, in which an\nunknown sparse term is introduced and applied for phase modification in\nadvance. Based on that, the mapping function is decomposed into the\nsuperimposition of the 0th-order and high-order polynomials in Taylor's series,\nwhere the former coarsely removes the interference in the magnitude domain and\nthe latter progressively complements the remaining spectral detail in the\ncomplex spectrum domain. In addition, we study the relation between adjacent\norder terms and reveal that each high-order term can be recursively estimated\nwith its lower-order term, and each high-order term is then proposed to\nevaluate using a surrogate function with trainable weights so that the whole\nsystem can be trained in an end-to-end manner. Given that the proposed\nframework is devised based on Taylor's theorem, it possesses improved internal\nflexibility. Extensive experiments are conducted on WSJ0-SI84, DNS-Challenge,\nVoicebank+Demand, spatialized Librispeech, and L3DAS22 multi-channel speech\nenhancement challenge datasets. Quantitative results show that the proposed\napproach yields competitive performance over existing top-performing approaches\nin terms of multiple objective metrics.", "published": "2022-11-30 06:05:59", "link": "http://arxiv.org/abs/2211.16764v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SNAC: Speaker-normalized affine coupling layer in flow-based\n  architecture for zero-shot multi-speaker text-to-speech", "abstract": "Zero-shot multi-speaker text-to-speech (ZSM-TTS) models aim to generate a\nspeech sample with the voice characteristic of an unseen speaker. The main\nchallenge of ZSM-TTS is to increase the overall speaker similarity for unseen\nspeakers. One of the most successful speaker conditioning methods for\nflow-based multi-speaker text-to-speech (TTS) models is to utilize the\nfunctions which predict the scale and bias parameters of the affine coupling\nlayers according to the given speaker embedding vector. In this letter, we\nimprove on the previous speaker conditioning method by introducing a\nspeaker-normalized affine coupling (SNAC) layer which allows for unseen speaker\nspeech synthesis in a zero-shot manner leveraging a normalization-based\nconditioning technique. The newly designed coupling layer explicitly normalizes\nthe input by the parameters predicted from a speaker embedding vector while\ntraining, enabling an inverse process of denormalizing for a new speaker\nembedding at inference. The proposed conditioning scheme yields the\nstate-of-the-art performance in terms of the speech quality and speaker\nsimilarity in a ZSM-TTS setting.", "published": "2022-11-30 10:07:29", "link": "http://arxiv.org/abs/2211.16866v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "How to (virtually) train your speaker localizer", "abstract": "Learning-based methods have become ubiquitous in speaker localization.\nExisting systems rely on simulated training sets for the lack of sufficiently\nlarge, diverse and annotated real datasets. Most room acoustics simulators used\nfor this purpose rely on the image source method (ISM) because of its\ncomputational efficiency. This paper argues that carefully extending the ISM to\nincorporate more realistic surface, source and microphone responses into\ntraining sets can significantly boost the real-world performance of speaker\nlocalization systems. It is shown that increasing the training-set realism of a\nstate-of-the-art direction-of-arrival estimator yields consistent improvements\nacross three different real test sets featuring human speakers in a variety of\nrooms and various microphone arrays. An ablation study further reveals that\nevery added layer of realism contributes positively to these improvements.", "published": "2022-11-30 13:01:11", "link": "http://arxiv.org/abs/2211.16958v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Extreme Audio Time Stretching Using Neural Synthesis", "abstract": "A deep neural network solution for time-scale modification (TSM) focused on\nlarge stretching factors is proposed, targeting environmental sounds.\nTraditional TSM artifacts such as transient smearing, loss of presence, and\nphasiness are heavily accentuated and cause poor audio quality when the TSM\nfactor is four or larger. The weakness of established TSM methods, often based\non a phase vocoder structure, lies in the poor description and scaling of the\ntransient and noise components, or nuances, of a sound. Our novel solution\ncombines a sines-transients-noise decomposition with an independent WaveNet\nsynthesizer to provide a better description of the noise component and an\nimprove sound quality for large stretching factors. Results of a subjective\nlistening test against four other TSM algorithms are reported, showing the\nproposed method to be often superior. The proposed method is stereo compatible\nand has a wide range of applications related to the slow motion of media\ncontent.", "published": "2022-11-30 13:47:05", "link": "http://arxiv.org/abs/2211.16992v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Assisted RTF-Vector-Based Binaural Direction of Arrival Estimation\n  Exploiting a Calibrated External Microphone Array", "abstract": "Recently, a relative transfer function (RTF)-vector-based method has been\nproposed to estimate the direction of arrival (DOA) of a target speaker for a\nbinaural hearing aid setup, assuming the availability of external microphones.\nThis method exploits the external microphones to estimate the RTF vector\ncorresponding to the binaural hearing aid and constructs a one-dimensional\nspatial spectrum by comparing the estimated RTF vector against a database of\nanechoic prototype RTF vectors for several directions. In this paper we assume\nthe availability of a calibrated array of external microphones, which is\ncharacterized by a second database of anechoic prototype RTF vectors. We\npropose a method, where the external microphones are not only exploited to\nestimate the RTF vector corresponding to the binaural hearing aid but also\nassist in estimating the DOA of the target speaker. Based on the estimated RTF\nvector for all microphones and both prototype databases, a two-dimensional\nspatial spectrum is constructed from which the DOA is estimated. Experimental\nresults for a reverberant environment with diffuse-like noise show that\nassisted DOA estimation outperforms DOA estimation where the prototype database\ncharacterizing the array of external microphones is not used.", "published": "2022-11-30 17:51:13", "link": "http://arxiv.org/abs/2211.17202v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Preliminary Study on SSCF-derived Polar Coordinate for ASR", "abstract": "The transition angles are defined to describe the vowel-to-vowel transitions\nin the acoustic space of the Spectral Subband Centroids, and the findings show\nthat they are similar among speakers and speaking rates. In this paper, we\npropose to investigate the usage of polar coordinates in favor of angles to\ndescribe a speech signal by characterizing its acoustic trajectory and using\nthem in Automatic Speech Recognition. According to the experimental results\nevaluated on the BRAF100 dataset, the polar coordinates achieved significantly\nhigher accuracy than the angles in the mixed and cross-gender speech\nrecognitions, demonstrating that these representations are superior at defining\nthe acoustic trajectory of the speech signal. Furthermore, the accuracy was\nsignificantly improved when they were utilized with their first and\nsecond-order derivatives ($\\Delta$, $\\Delta$$\\Delta$), especially in\ncross-female recognition. However, the results showed they were not much more\ngender-independent than the conventional Mel-frequency Cepstral Coefficients\n(MFCCs).", "published": "2022-11-30 14:57:28", "link": "http://arxiv.org/abs/2212.01245v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
