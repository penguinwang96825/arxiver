{"title": "Prompts have evil twins", "abstract": "We discover that many natural-language prompts can be replaced by\ncorresponding prompts that are unintelligible to humans but that provably\nelicit similar behavior in language models. We call these prompts \"evil twins\"\nbecause they are obfuscated and uninterpretable (evil), but at the same time\nmimic the functionality of the original natural-language prompts (twins).\nRemarkably, evil twins transfer between models. We find these prompts by\nsolving a maximum-likelihood problem which has applications of independent\ninterest.", "published": "2023-11-13 04:08:49", "link": "http://arxiv.org/abs/2311.07064v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Consistency between Training and Testing in Simultaneous Machine\n  Translation", "abstract": "Simultaneous Machine Translation (SiMT) aims to yield a real-time partial\ntranslation with a monotonically growing the source-side context. However,\nthere is a counterintuitive phenomenon about the context usage between training\nand testing: e.g., the wait-k testing model consistently trained with wait-k is\nmuch worse than that model inconsistently trained with wait-k' (k' is not equal\nto k) in terms of translation quality. To this end, we first investigate the\nunderlying reasons behind this phenomenon and uncover the following two\nfactors: 1) the limited correlation between translation quality and training\n(cross-entropy) loss; 2) exposure bias between training and testing. Based on\nboth reasons, we then propose an effective training approach called context\nconsistency training accordingly, which makes consistent the context usage\nbetween training and testing by optimizing translation quality and latency as\nbi-objectives and exposing the predictions to the model during the training.\nThe experiments on three language pairs demonstrate our intuition: our system\nencouraging context consistency outperforms that existing systems with context\ninconsistency for the first time, with the help of our context consistency\ntraining approach.", "published": "2023-11-13 04:11:32", "link": "http://arxiv.org/abs/2311.07066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explain-then-Translate: An Analysis on Improving Program Translation\n  with Self-generated Explanations", "abstract": "This work explores the use of self-generated natural language explanations as\nan intermediate step for code-to-code translation with language models. Across\nthree types of explanations and 19 programming languages constructed from the\nMultiPL-E dataset, we find the explanations to be particularly effective in the\nzero-shot case, improving performance by 12% on average. Improvements with\nnatural language explanations are particularly pronounced on difficult\nprograms. We release our dataset, code, and canonical solutions in all 19\nlanguages.", "published": "2023-11-13 04:28:49", "link": "http://arxiv.org/abs/2311.07070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Discussion of Large Language Models: Symmetry of Agents and\n  Interplay with Prompts", "abstract": "Two ways has been discussed to unlock the reasoning capability of a large\nlanguage model. The first one is prompt engineering and the second one is to\ncombine the multiple inferences of large language models, or the multi-agent\ndiscussion. Theoretically, this paper justifies the multi-agent discussion\nmechanisms from the symmetry of agents. Empirically, this paper reports the\nempirical results of the interplay of prompts and discussion mechanisms,\nrevealing the empirical state-of-the-art performance of complex multi-agent\nmechanisms can be approached by carefully developed prompt engineering. This\npaper also proposes a scalable discussion mechanism based on conquer and merge,\nproviding a simple multi-agent discussion solution with simple prompts but\nstate-of-the-art performance.", "published": "2023-11-13 04:56:48", "link": "http://arxiv.org/abs/2311.07076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fovea Transformer: Efficient Long-Context Modeling with Structured\n  Fine-to-Coarse Attention", "abstract": "The quadratic complexity of self-attention in Transformers has hindered the\nprocessing of long text. To alleviate this problem, previous works have\nproposed to sparsify the attention matrix, taking advantage of the observation\nthat crucial information about a token can be derived from its neighbors. These\nmethods typically combine one or another form of local attention and global\nattention. Such combinations introduce abrupt changes in contextual granularity\nwhen going from local to global, which may be undesirable. We believe that a\nsmoother transition could potentially enhance model's ability to capture\nlong-context dependencies. In this study, we introduce Fovea Transformer, a\nlong-context focused transformer that addresses the challenges of capturing\nglobal dependencies while maintaining computational efficiency. To achieve\nthis, we construct a multi-scale tree from the input sequence, and use\nrepresentations of context tokens with a progressively coarser granularity in\nthe tree, as their distance to the query token increases. We evaluate our model\non three long-context summarization tasks\\footnote{Our code is publicly\navailable at: \\textit{https://github.com/ZiweiHe/Fovea-Transformer}}. It\nachieves state-of-the-art performance on two of them, and competitive results\non the third with mixed improvement and setback of the evaluation metrics.", "published": "2023-11-13 06:24:27", "link": "http://arxiv.org/abs/2311.07102v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gen-Z: Generative Zero-Shot Text Classification with Contextualized\n  Label Descriptions", "abstract": "Language model (LM) prompting--a popular paradigm for solving NLP tasks--has\nbeen shown to be susceptible to miscalibration and brittleness to slight prompt\nvariations, caused by its discriminative prompting approach, i.e., predicting\nthe label given the input. To address these issues, we propose Gen-Z--a\ngenerative prompting framework for zero-shot text classification. GEN-Z is\ngenerative, as it measures the LM likelihood of input text, conditioned on\nnatural language descriptions of labels. The framework is multivariate, as\nlabel descriptions allow us to seamlessly integrate additional contextual\ninformation about the labels to improve task performance. On various standard\nclassification benchmarks, with six open-source LM families, we show that\nzero-shot classification with simple contextualization of the data source of\nthe evaluation set consistently outperforms both zero-shot and few-shot\nbaselines while improving robustness to prompt variations. Further, our\napproach enables personalizing classification in a zero-shot manner by\nincorporating author, subject, or reader information in the label descriptions.", "published": "2023-11-13 07:12:57", "link": "http://arxiv.org/abs/2311.07115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing a Named Entity Recognition Dataset for Tagalog", "abstract": "We present the development of a Named Entity Recognition (NER) dataset for\nTagalog. This corpus helps fill the resource gap present in Philippine\nlanguages today, where NER resources are scarce. The texts were obtained from a\npretraining corpora containing news reports, and were labeled by native\nspeakers in an iterative fashion. The resulting dataset contains ~7.8k\ndocuments across three entity types: Person, Organization, and Location. The\ninter-annotator agreement, as measured by Cohen's $\\kappa$, is 0.81. We also\nconducted extensive empirical evaluation of state-of-the-art methods across\nsupervised and transfer learning settings. Finally, we released the data and\nprocessing code publicly to inspire future work on Tagalog NLP.", "published": "2023-11-13 08:56:47", "link": "http://arxiv.org/abs/2311.07161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "calamanCy: A Tagalog Natural Language Processing Toolkit", "abstract": "We introduce calamanCy, an open-source toolkit for constructing natural\nlanguage processing (NLP) pipelines for Tagalog. It is built on top of spaCy,\nenabling easy experimentation and integration with other frameworks. calamanCy\naddresses the development gap by providing a consistent API for building NLP\napplications and offering general-purpose multitask models with out-of-the-box\nsupport for dependency parsing, parts-of-speech (POS) tagging, and named entity\nrecognition (NER). calamanCy aims to accelerate the progress of Tagalog NLP by\nconsolidating disjointed resources in a unified framework. The calamanCy\ntoolkit is available on GitHub: https://github.com/ljvmiranda921/calamanCy.", "published": "2023-11-13 09:06:43", "link": "http://arxiv.org/abs/2311.07171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Factual Consistency in Dialogue Comprehension of Large\n  Language Models", "abstract": "LLMs (Large Language Models) usually interact with users in the form of\ndialogue and generate responses following their instructions, which naturally\nrequire dialogue comprehension abilities. However, dialogue comprehension is a\ngeneral language ability which is hard to be evaluated directly. In this work,\nwe propose to perform the evaluation focusing on the factual consistency issue\nwith the help of the dialogue summarization task. Besides evaluating and\nanalyzing the dialogue summarization performance (DIAC-Sum) of different LLMs,\nwe also derive factual questions from the generated summaries and use them as a\nmore flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation\nshows that, on average, 26.8% of the summaries generated by LLMs contain\nfactual inconsistency. Even ChatGPT, the strongest model evaluated, has such\nerrors in 16% of its summaries. For answering the factual questions, which is\nmore challenging, the average error rate of all evaluated LLMs is 36.1%. Both\nresults indicate serious deficiencies. Detailed analysis shows that the\nunderstanding of subject/object of the conversation is still challenging for\nLLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability\nof LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task\ndata, which achieved a relative error rate reduction of 11% on DIAC-QA.", "published": "2023-11-13 09:32:12", "link": "http://arxiv.org/abs/2311.07194v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Troubles and Failures in Interactional Language. Towards a\n  Linguistically Informed Taxonomy", "abstract": "The goal of this talk is to introduce a systematic research agenda which aims\nto understand the nature of interaction between humans and artificial\nconversational agents (CA) (henceforth humanmachine interaction, HMI).\nSpecifically, we shall take an explicit linguistic perspective focusing on\nlinguistically defined variables that are known to influence the flow of\nconversations among humans (henceforth human-human interaction, HHI).", "published": "2023-11-13 10:24:51", "link": "http://arxiv.org/abs/2311.07217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How are Prompts Different in Terms of Sensitivity?", "abstract": "In-context learning (ICL) has become one of the most popular learning\nparadigms. While there is a growing body of literature focusing on prompt\nengineering, there is a lack of systematic analysis comparing the effects of\nprompts across different models and tasks. To address this gap, we present a\ncomprehensive prompt analysis based on the sensitivity of a function. Our\nanalysis reveals that sensitivity is an unsupervised proxy for model\nperformance, as it exhibits a strong negative correlation with accuracy. We use\ngradient-based saliency scores to empirically demonstrate how different prompts\naffect the relevance of input tokens to the output, resulting in different\nlevels of sensitivity. Furthermore, we introduce sensitivity-aware decoding\nwhich incorporates sensitivity estimation as a penalty term in the standard\ngreedy decoding. We show that this approach is particularly helpful when\ninformation in the input is scarce. Our work provides a fresh perspective on\nthe analysis of prompts, and contributes to a better understanding of the\nmechanism of ICL.", "published": "2023-11-13 10:52:01", "link": "http://arxiv.org/abs/2311.07230v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Danish Foundation Models", "abstract": "Large language models, sometimes referred to as foundation models, have\ntransformed multiple fields of research. However, smaller languages risk\nfalling behind due to high training costs and small incentives for large\ncompanies to train these models. To combat this, the Danish Foundation Models\nproject seeks to provide and maintain open, well-documented, and high-quality\nfoundation models for the Danish language. This is achieved through broad\ncooperation with public and private institutions, to ensure high data quality\nand applicability of the trained models. We present the motivation of the\nproject, the current status, and future perspectives.", "published": "2023-11-13 12:03:52", "link": "http://arxiv.org/abs/2311.07264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Logical Puzzle Solving in Large Language Models: Insights from\n  a Minesweeper Case Study", "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in language\nunderstanding and have been successfully applied to a variety of real-world\ntasks through task-specific fine-tuning or prompt engineering. Despite these\nadvancements, it remains an open question whether LLMs are fundamentally\ncapable of reasoning and planning, or if they primarily rely on recalling and\nsynthesizing information from their training data. In our research, we\nintroduce a novel task -- Minesweeper -- specifically designed in a format\nunfamiliar to LLMs and absent from their training datasets. This task\nchallenges LLMs to identify the locations of mines based on numerical clues\nprovided by adjacent opened cells. Successfully completing this task requires\nan understanding of each cell's state, discerning spatial relationships between\nthe clues and mines, and strategizing actions based on logical deductions drawn\nfrom the arrangement of the cells. Our experiments, including trials with the\nadvanced GPT-4 model, indicate that while LLMs possess the foundational\nabilities required for this task, they struggle to integrate these into a\ncoherent, multi-step logical reasoning process needed to solve Minesweeper.\nThese findings highlight the need for further research to understand the nature\nof reasoning capabilities in LLMs under similar circumstances, and to explore\npathways towards more sophisticated AI reasoning and planning models.", "published": "2023-11-13 15:11:26", "link": "http://arxiv.org/abs/2311.07387v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Text Generation for Black-box Language Models via Score-based\n  Progressive Editor", "abstract": "Controlled text generation is very important for the practical use of\nlanguage models because it ensures that the produced text includes only the\ndesired attributes from a specific domain or dataset. Existing methods,\nhowever, are inapplicable to black-box models or suffer a significant trade-off\nbetween controlling the generated text and maintaining its fluency. This paper\nintroduces the Score-based Progressive Editor (ScoPE), a novel approach\ndesigned to overcome these issues. ScoPE modifies the context at the token\nlevel during the generation process of a backbone language model. This\nmodification guides the subsequent text to naturally include the target\nattributes. To facilitate this process, ScoPE employs a training objective that\nmaximizes a target score, thoroughly considering both the ability to guide the\ntext and its fluency. Experimental results on diverse controlled generation\ntasks demonstrate that ScoPE can effectively regulate the attributes of the\ngenerated text while fully utilizing the capability of the backbone large\nlanguage models. Our codes are available at\n\\url{https://github.com/ysw1021/ScoPE}.", "published": "2023-11-13 16:03:23", "link": "http://arxiv.org/abs/2311.07430v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEGAVERSE: Benchmarking Large Language Models Across Languages,\n  Modalities, Models and Tasks", "abstract": "There has been a surge in LLM evaluation research to understand LLM\ncapabilities and limitations. However, much of this research has been confined\nto English, leaving LLM building and evaluation for non-English languages\nrelatively unexplored. Several new LLMs have been introduced recently,\nnecessitating their evaluation on non-English languages. This study aims to\nperform a thorough evaluation of the non-English capabilities of SoTA LLMs\n(GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by\ncomparing them on the same set of multilingual datasets. Our benchmark\ncomprises 22 datasets covering 83 languages, including low-resource African\nlanguages. We also include two multimodal datasets in the benchmark and compare\nthe performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our\nexperiments show that larger models such as GPT-4, Gemini-Pro and PaLM2\noutperform smaller models on various tasks, notably on low-resource languages,\nwith GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform\na study on data contamination and find that several models are likely to be\ncontaminated with multilingual evaluation benchmarks, necessitating approaches\nto detect and handle contamination while assessing the multilingual performance\nof LLMs.", "published": "2023-11-13 16:45:37", "link": "http://arxiv.org/abs/2311.07463v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers", "abstract": "Understanding the internal mechanisms by which multi-modal large language\nmodels (LLMs) interpret different modalities and integrate cross-modal\nrepresentations is becoming increasingly critical for continuous improvements\nin both academia and industry. In this paper, we propose a novel method to\nidentify key neurons for interpretability -- how multi-modal LLMs bridge visual\nand textual concepts for captioning. Our method improves conventional works\nupon efficiency and applied range by removing needs of costly gradient\ncomputation. Based on those identified neurons, we further design a multi-modal\nknowledge editing method, beneficial to mitigate sensitive words or\nhallucination. For rationale of our design, we provide theoretical assumption.\nFor empirical evaluation, we have conducted extensive quantitative and\nqualitative experiments. The results not only validate the effectiveness of our\nmethods, but also offer insightful findings that highlight three key properties\nof multi-modal neurons: sensitivity, specificity and causal-effect, to shed\nlight for future research.", "published": "2023-11-13 17:03:02", "link": "http://arxiv.org/abs/2311.07470v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question\n  Decomposition with Large Language Models", "abstract": "While large language models exhibit remarkable performance in the Question\nAnswering task, they are susceptible to hallucinations. Challenges arise when\nthese models grapple with understanding multi-hop relations in complex\nquestions or lack the necessary knowledge for a comprehensive response. To\naddress this issue, we introduce the \"Decompose-and-Query\" framework (D&Q).\nThis framework guides the model to think and utilize external knowledge similar\nto ReAct, while also restricting its thinking to reliable information,\neffectively mitigating the risk of hallucinations. Experiments confirm the\neffectiveness of D&Q: On our ChitChatQA dataset, D&Q does not lose to ChatGPT\nin 67% of cases; on the HotPotQA question-only setting, D&Q achieved an F1\nscore of 59.6%. Our code is available at\nhttps://github.com/alkaidpku/DQ-ToolQA.", "published": "2023-11-13 17:28:03", "link": "http://arxiv.org/abs/2311.07491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Nonce Dependency Treebanks: Understanding how Language\n  Models represent and process syntactic structure", "abstract": "We introduce SPUD (Semantically Perturbed Universal Dependencies), a\nframework for creating nonce treebanks for the multilingual Universal\nDependencies (UD) corpora. SPUD data satisfies syntactic argument structure,\nprovides syntactic annotations, and ensures grammaticality via\nlanguage-specific rules. We create nonce data in Arabic, English, French,\nGerman, and Russian, and demonstrate two use cases of SPUD treebanks. First, we\ninvestigate the effect of nonce data on word co-occurrence statistics, as\nmeasured by perplexity scores of autoregressive (ALM) and masked language\nmodels (MLM). We find that ALM scores are significantly more affected by nonce\ndata than MLM scores. Second, we show how nonce data affects the performance of\nsyntactic dependency probes. We replicate the findings of M\\\"uller-Eberstein et\nal. (2022) on nonce test data and show that the performance declines on both\nMLMs and ALMs wrt. original test data. However, a majority of the performance\nis kept, suggesting that the probe indeed learns syntax independently from\nsemantics.", "published": "2023-11-13 17:36:58", "link": "http://arxiv.org/abs/2311.07497v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It's Not Easy Being Wrong: Large Language Models Struggle with Process\n  of Elimination Reasoning", "abstract": "Chain-of-thought (COT) prompting can help large language models (LLMs) reason\ntoward correct answers, but its efficacy in reasoning toward incorrect answers\nis unexplored. This process of elimination (PoE), when used with COT, can\nenhance self-consistency, interpretability, and tasks such as medical diagnoses\nof exclusion. Thus, we propose PoE with COT, where LLMs must reason toward\nincorrect options on multiple-choice questions. We evaluate the ability of\nGPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four\ncommonsense and scientific reasoning datasets. We find that the strategy of PoE\nalways underperforms the strategy of choosing the correct answer. The agreement\nof these strategies is also lower than the self-consistency of each strategy.\nTo study these issues further, we conduct error analyses and give suggestions\nfor future work.", "published": "2023-11-13 18:18:22", "link": "http://arxiv.org/abs/2311.07532v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual\n  Question Answering", "abstract": "The emergence of multimodal large models (MLMs) has significantly advanced\nthe field of visual understanding, offering remarkable capabilities in the\nrealm of visual question answering (VQA). Yet, the true challenge lies in the\ndomain of knowledge-intensive VQA tasks, which necessitate not just recognition\nof visual elements, but also a deep comprehension of the visual information in\nconjunction with a vast repository of learned knowledge. To uncover such\ncapabilities of MLMs, particularly the newly introduced GPT-4V and Gemini, we\nprovide an in-depth evaluation from three perspectives: 1) Commonsense\nKnowledge, which assesses how well models can understand visual cues and\nconnect to general knowledge; 2) Fine-grained World Knowledge, which tests the\nmodel's skill in reasoning out specific knowledge from images, showcasing their\nproficiency across various specialized fields; 3) Comprehensive Knowledge with\nDecision-making Rationales, which examines model's capability to provide\nlogical explanations for its inference, facilitating a deeper analysis from the\ninterpretability perspective. Additionally, we utilize a visual\nknowledge-enhanced training strategy and multimodal retrieval-augmented\ngeneration approach to enhance MLMs, highlighting the future need for\nadvancements in this research direction. Extensive experiments indicate that:\na) GPT-4V demonstrates enhanced explanation generation when using composite\nimages as few-shots; b) GPT-4V and other MLMs produce severe hallucinations\nwhen dealing with world knowledge; c) Visual knowledge enhanced training and\nprompting technicals present potential to improve performance. Codes:\nhttps://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper", "published": "2023-11-13 18:22:32", "link": "http://arxiv.org/abs/2311.07536v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Explanations to Improve Robustness of In-context\n  Learning", "abstract": "Recent studies demonstrated that large language models (LLMs) can excel in\nmany tasks via in-context learning (ICL). However, recent works show that\nICL-prompted models tend to produce inaccurate results when presented with\nadversarial inputs. In this work, we investigate whether augmenting ICL with\nnatural language explanations (NLEs) improves the robustness of LLMs on\nadversarial datasets covering natural language inference and paraphrasing\nidentification. We prompt LLMs with a small set of human-generated NLEs to\nproduce further NLEs, yielding more accurate results than both a zero-shot-ICL\nsetting and using only human-generated NLEs. Our results on five popular LLMs\n(GPT3.5-turbo, Llama2, Vicuna, Zephyr, and Mistral) show that our approach\nyields over 6% improvement over baseline approaches for eight adversarial\ndatasets: HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS. Furthermore,\nprevious studies have demonstrated that prompt selection strategies\nsignificantly enhance ICL on in-distribution test sets. However, our findings\nreveal that these strategies do not match the efficacy of our approach for\nrobustness evaluations, resulting in an accuracy drop of 8% compared to the\nproposed approach.", "published": "2023-11-13 18:49:13", "link": "http://arxiv.org/abs/2311.07556v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming", "abstract": "Red-teaming is a common practice for mitigating unsafe behaviors in Large\nLanguage Models (LLMs), which involves thoroughly assessing LLMs to identify\npotential flaws and addressing them with responsible and accurate responses.\nWhile effective, manual red-teaming is costly, and existing automatic\nred-teaming typically discovers safety risks without addressing them. In this\npaper, we propose a Multi-round Automatic Red-Teaming (MART) method, which\nincorporates both automatic adversarial prompt writing and safe response\ngeneration, significantly increasing red-teaming scalability and the safety of\nthe target LLM. Specifically, an adversarial LLM and a target LLM interplay\nwith each other in an iterative manner, where the adversarial LLM aims to\ngenerate challenging prompts that elicit unsafe responses from the target LLM,\nwhile the target LLM is fine-tuned with safety aligned data on these\nadversarial prompts. In each round, the adversarial LLM crafts better attacks\non the updated target LLM, while the target LLM also improves itself through\nsafety fine-tuning. On adversarial prompt benchmarks, the violation rate of an\nLLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,\nachieving comparable performance to LLMs with extensive adversarial prompt\nwriting. Notably, model helpfulness on non-adversarial prompts remains stable\nthroughout iterations, indicating the target LLM maintains strong performance\non instruction following.", "published": "2023-11-13 19:13:29", "link": "http://arxiv.org/abs/2311.07689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IruMozhi: Automatically classifying diglossia in Tamil", "abstract": "Tamil, a Dravidian language of South Asia, is a highly diglossic language\nwith two very different registers in everyday use: Literary Tamil (preferred in\nwriting and formal communication) and Spoken Tamil (confined to speech and\ninformal media). Spoken Tamil is under-supported in modern NLP systems. In this\npaper, we release IruMozhi, a human-annotated dataset of parallel text in\nLiterary and Spoken Tamil. We train classifiers on the task of identifying\nwhich variety a text belongs to. We use these models to gauge the availability\nof pretraining data in Spoken Tamil, to audit the composition of existing\nlabelled datasets for Tamil, and to encourage future work on the variety.", "published": "2023-11-13 23:36:35", "link": "http://arxiv.org/abs/2311.07804v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "In-context Learning Generalizes, But Not Always Robustly: The Case of\n  Syntax", "abstract": "In-context learning (ICL) is now a common method for teaching large language\nmodels (LLMs) new tasks: given labeled examples in the input context, the LLM\nlearns to perform the task without weight updates. Do models guided via ICL\ninfer the underlying structure of the task defined by the context, or do they\nrely on superficial heuristics that only generalize to identically distributed\nexamples? We address this question using transformations tasks and an NLI task\nthat assess sensitivity to syntax - a requirement for robust language\nunderstanding. We further investigate whether out-of-distribution\ngeneralization can be improved via chain-of-thought prompting, where the model\nis provided with a sequence of intermediate computation steps that illustrate\nhow the task ought to be performed. In experiments with models from the GPT,\nPaLM, and Llama 2 families, we find large variance across LMs. The variance is\nexplained more by the composition of the pre-training corpus and supervision\nmethods than by model size; in particular, models pre-trained on code\ngeneralize better, and benefit more from chain-of-thought prompting.", "published": "2023-11-13 23:52:43", "link": "http://arxiv.org/abs/2311.07811v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-dependent Instruction Tuning for Dialogue Response Generation", "abstract": "Recent language models have achieved impressive performance in natural\nlanguage tasks by incorporating instructions with task input during\nfine-tuning. Since all samples in the same natural language task can be\nexplained with the same task instructions, many instruction datasets only\nprovide a few instructions for the entire task, without considering the input\nof each example in the task. However, this approach becomes ineffective in\ncomplex multi-turn dialogue generation tasks, where the input varies highly\nwith each turn as the dialogue context changes, so that simple task\ninstructions cannot improve the generation performance. To address this\nlimitation, we introduce a context-based instruction fine-tuning framework for\neach multi-turn dialogue which generates both responses and instructions based\non the previous context as input. During the evaluation, the model generates\ninstructions based on the previous context to self-guide the response. The\nproposed framework produces comparable or even outstanding results compared to\nthe baselines by aligning instructions to the input during fine-tuning with the\ninstructions in quantitative evaluations on dialogue benchmark datasets with\nreduced computation budget.", "published": "2023-11-13 01:25:30", "link": "http://arxiv.org/abs/2311.07006v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ExpNote: Black-box Large Language Models are Better Task Solvers with\n  Experience Notebook", "abstract": "Black-box Large Language Models (LLMs) have shown great power in solving\nvarious tasks and are considered general problem solvers. However, LLMs still\nfail in many specific tasks although understand the task instruction. In this\npaper, we focus on the problem of boosting the ability of black-box LLMs to\nsolve downstream tasks. We propose ExpNote, an automated framework to help LLMs\nbetter adapt to unfamiliar tasks through reflecting and noting experiences from\ntraining data and retrieving them from external memory during testing. We\nevaluate ExpNote on multiple tasks and the experimental results demonstrate\nthat the proposed method significantly improves the performance of black-box\nLLMs. The data and code are available at\nhttps://github.com/forangel2014/ExpNote", "published": "2023-11-13 02:31:16", "link": "http://arxiv.org/abs/2311.07032v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "To Tell The Truth: Language of Deception and Language Models", "abstract": "Text-based misinformation permeates online discourses, yet evidence of\npeople's ability to discern truth from such deceptive textual content is\nscarce. We analyze a novel TV game show data where conversations in a\nhigh-stake environment between individuals with conflicting objectives result\nin lies. We investigate the manifestation of potentially verifiable language\ncues of deception in the presence of objective truth, a distinguishing feature\nabsent in previous text-based deception datasets. We show that there exists a\nclass of detectors (algorithms) that have similar truth detection performance\ncompared to human subjects, even when the former accesses only the language\ncues while the latter engages in conversations with complete access to all\npotential sources of cues (language and audio-visual). Our model, built on a\nlarge language model, employs a bottleneck framework to learn discernible cues\nto determine truth, an act of reasoning in which human subjects often perform\npoorly, even with incentives. Our model detects novel but accurate language\ncues in many cases where humans failed to detect deception, opening up the\npossibility of humans collaborating with algorithms and ameliorating their\nability to detect the truth.", "published": "2023-11-13 05:40:11", "link": "http://arxiv.org/abs/2311.07092v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explanation-aware Soft Ensemble Empowers Large Language Model In-context\n  Learning", "abstract": "Large language models (LLMs) have shown remarkable capabilities in various\nnatural language understanding tasks. With only a few demonstration examples,\nthese LLMs can quickly adapt to target tasks without expensive gradient\nupdates. Common strategies to boost such 'in-context' learning ability are to\nensemble multiple model decoded results and require the model to generate an\nexplanation along with the prediction. However, these models often treat\ndifferent class predictions equally and neglect the potential discrepancy\nbetween the explanations and predictions. To fully unleash the power of\nexplanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to\nempower in-context learning with LLMs. We design two techniques,\nexplanation-guided ensemble, and soft probability aggregation, to mitigate the\neffect of unreliable explanations and improve the consistency between\nexplanations and final predictions. Experiments on seven natural language\nunderstanding tasks and four varying-size LLMs demonstrate the effectiveness of\nour proposed framework.", "published": "2023-11-13 06:13:38", "link": "http://arxiv.org/abs/2311.07099v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language\n  Models", "abstract": "To mitigate the potential misuse of large language models (LLMs), recent\nresearch has developed watermarking algorithms, which restrict the generation\nprocess to leave an invisible trace for watermark detection. Due to the\ntwo-stage nature of the task, most studies evaluate the generation and\ndetection separately, thereby presenting a challenge in unbiased, thorough, and\napplicable evaluations. In this paper, we introduce WaterBench, the first\ncomprehensive benchmark for LLM watermarks, in which we design three crucial\nfactors: (1) For benchmarking procedure, to ensure an apples-to-apples\ncomparison, we first adjust each watermarking method's hyper-parameter to reach\nthe same watermarking strength, then jointly evaluate their generation and\ndetection performance. (2) For task selection, we diversify the input and\noutput length to form a five-category taxonomy, covering $9$ tasks. (3) For\nevaluation metric, we adopt the GPT4-Judge for automatically evaluating the\ndecline of instruction-following abilities after watermarking. We evaluate $4$\nopen-source watermarks on $2$ LLMs under $2$ watermarking strengths and observe\nthe common struggles for current methods on maintaining the generation quality.\nThe code and data are available at https://github.com/THU-KEG/WaterBench.", "published": "2023-11-13 08:09:01", "link": "http://arxiv.org/abs/2311.07138v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STEER: Unified Style Transfer with Expert Reinforcement", "abstract": "While text style transfer has many applications across natural language\nprocessing, the core premise of transferring from a single source style is\nunrealistic in a real-world setting. In this work, we focus on arbitrary style\ntransfer: rewriting a text from an arbitrary, unknown style to a target style.\n  We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified\nframe-work developed to overcome the challenge of limited parallel data for\nstyle transfer. STEER involves automatically generating a corpus of\nstyle-transfer pairs using a product of experts during decoding. The generated\noffline data is then used to pre-train an initial policy before switching to\nonline, off-policy reinforcement learning for further improvements via\nfine-grained reward signals. STEER is unified and can transfer to multiple\ntarget styles from an arbitrary, unknown source style, making it particularly\nflexible and efficient.\n  Experimental results on a challenging dataset with text from a diverse set of\nstyles demonstrate state-of-the-art results compared to competitive baselines.\nRemarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on\noverall style transfer quality, despite being 226 times smaller in size. We\nalso show STEER is robust, maintaining its style transfer capabilities on\nout-of-domain data, and surpassing nearly all baselines across various styles.\nThe success of our method highlights the potential of RL algorithms when\naugmented with controllable decoding to overcome the challenge of limited data\nsupervision.", "published": "2023-11-13 09:02:30", "link": "http://arxiv.org/abs/2311.07167v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VerityMath: Advancing Mathematical Reasoning by Self-Verification\n  Through Unit Consistency", "abstract": "Large Language Models (LLMs), combined with program-based solving techniques,\nare increasingly demonstrating proficiency in mathematical reasoning. For\nexample, closed-source models such as OpenAI GPT-4 and Claude show excellent\nresults in solving math word problems. However, progress in math word\nproblem-solving for open-source LLMs is limited, and the challenges these\nmodels face are not well-studied. In this paper, we study the performance of\nstrong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral\n(7B) on math word problems using program-based solving techniques.\nSpecifically, we analyze the outputs of these models when applied to math word\nproblems and identify a category of problems that pose a significant challenge,\nparticularly those involving quantities spanning multiple units. To address\nthis issue, we propose a systematic approach by defining the units for each\nquantity and ensuring the consistency of these units during mathematical\noperations. We developed Unit Consistency Programs (UCPs), an annotated dataset\nof math word problems, each paired with programs containing unit specifications\nand unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B),\nand Mistral (7B) models with UCPs to produce theirVerityMath variants. Our\nfindings indicate that our approach, which incorporates unit consistency,\ncurrently slightly underperforms compared to an approach that does not. To\nunderstand the reasons behind this, we conduct an in-depth error analysis and\nsuggest options for future improvements. Our code and dataset are available at\nhttps://github.com/vernontoh/VerityMath.", "published": "2023-11-13 09:06:58", "link": "http://arxiv.org/abs/2311.07172v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback", "abstract": "Code editing is an essential step towards reliable program synthesis to\nautomatically correct critical errors generated from code LLMs. Recent studies\nhave demonstrated that closed-source LLMs (i.e., ChatGPT and GPT-4) are capable\nof generating corrective feedback to edit erroneous inputs. However, it remains\nchallenging for open-source code LLMs to generate feedback for code editing,\nsince these models tend to adhere to the superficial formats of feedback and\nprovide feedback with misleading information. Hence, the focus of our work is\nto leverage open-source code LLMs to generate helpful feedback with correct\nguidance for code editing. To this end, we present Coffee, a collected dataset\nspecifically designed for code fixing with feedback. Using this dataset, we\nconstruct CoffeePots, a framework for COde Fixing with FEEdback via\nPreference-Optimized Tuning and Selection. The proposed framework aims to\nautomatically generate helpful feedback for code editing while minimizing the\npotential risk of superficial feedback. The combination of Coffee and\nCoffeePots marks a significant advancement, achieving state-of-the-art\nperformance on HumanEvalFix benchmark. Codes and model checkpoints are publicly\navailable at https://github.com/Lune-Blue/COFFEE.", "published": "2023-11-13 10:15:19", "link": "http://arxiv.org/abs/2311.07215v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "In Search of the Long-Tail: Systematic Generation of Long-Tail\n  Inferential Knowledge via Logical Rule Guided Search", "abstract": "To effectively use large language models (LLMs) for real-world queries, it is\nimperative that they generalize to the long-tail distribution, i.e. rare\nexamples where models exhibit low confidence. In this work, we take the first\nstep towards evaluating LLMs in the long-tail distribution of inferential\nknowledge. We exemplify long-tail evaluation on the Natural Language Inference\ntask. First, we introduce Logic-Induced-Knowledge-Search (LINK), a systematic\nlong-tail data generation framework, to obtain factually-correct yet long-tail\ninferential statements. LINK uses variable-wise prompting grounded on symbolic\nrules to seek low-confidence statements while ensuring factual correctness. We\nthen use LINK to curate Logic-Induced-Long-Tail (LINT), a large-scale long-tail\ninferential knowledge dataset that contains 108K statements spanning four\ndomains. We evaluate popular LLMs on LINT; we find that state-of-the-art LLMs\nshow significant performance drop (21% relative drop for GPT4) on long-tail\ndata as compared to on head distribution data, and smaller models show even\nmore generalization weakness. These results further underscore the necessity of\nlong-tail evaluation in developing generalizable LLMs.", "published": "2023-11-13 10:56:59", "link": "http://arxiv.org/abs/2311.07237v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdaCCD: Adaptive Semantic Contrasts Discovery Based Cross Lingual\n  Adaptation for Code Clone Detection", "abstract": "Code Clone Detection, which aims to retrieve functionally similar programs\nfrom large code bases, has been attracting increasing attention. Modern\nsoftware often involves a diverse range of programming languages. However,\ncurrent code clone detection methods are generally limited to only a few\npopular programming languages due to insufficient annotated data as well as\ntheir own model design constraints. To address these issues, we present AdaCCD,\na novel cross-lingual adaptation method that can detect cloned codes in a new\nlanguage without annotations in that language. AdaCCD leverages\nlanguage-agnostic code representations from pre-trained programming language\nmodels and propose an Adaptively Refined Contrastive Learning framework to\ntransfer knowledge from resource-rich languages to resource-poor languages. We\nevaluate the cross-lingual adaptation results of AdaCCD by constructing a\nmultilingual code clone detection benchmark consisting of 5 programming\nlanguages. AdaCCD achieves significant improvements over other baselines, and\nachieve comparable performance to supervised fine-tuning.", "published": "2023-11-13 12:20:48", "link": "http://arxiv.org/abs/2311.07277v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "BIDRN: A Method of Bidirectional Recurrent Neural Network for Sentiment\n  Analysis", "abstract": "Text mining research has grown in importance in recent years due to the\ntremendous increase in the volume of unstructured textual data. This has\nresulted in immense potential as well as obstacles in the sector, which may be\nefficiently addressed with adequate analytical and study methods. Deep\nBidirectional Recurrent Neural Networks are used in this study to analyze\nsentiment. The method is categorized as sentiment polarity analysis because it\nmay generate a dataset with sentiment labels. This dataset can be used to train\nand evaluate sentiment analysis models capable of extracting impartial\nopinions. This paper describes the Sentiment Analysis-Deep Bidirectional\nRecurrent Neural Networks (SA-BDRNN) Scheme, which seeks to overcome the\nchallenges and maximize the potential of text mining in the context of Big\nData. The current study proposes a SA-DBRNN Scheme that attempts to give a\nsystematic framework for sentiment analysis in the context of student input on\ninstitution choice. The purpose of this study is to compare the effectiveness\nof the proposed SA- DBRNN Scheme to existing frameworks to establish a robust\ndeep neural network that might serve as an adequate classification model in the\nfield of sentiment analysis.", "published": "2023-11-13 12:36:53", "link": "http://arxiv.org/abs/2311.07296v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Do large language models and humans have similar behaviors in causal\n  inference with script knowledge?", "abstract": "Recently, large pre-trained language models (LLMs) have demonstrated superior\nlanguage understanding abilities, including zero-shot causal reasoning.\nHowever, it is unclear to what extent their capabilities are similar to human\nones. We here study the processing of an event $B$ in a script-based story,\nwhich causally depends on a previous event $A$. In our manipulation, event $A$\nis stated, negated, or omitted in an earlier section of the text. We first\nconducted a self-paced reading experiment, which showed that humans exhibit\nsignificantly longer reading times when causal conflicts exist ($\\neg A\n\\rightarrow B$) than under logical conditions ($A \\rightarrow B$). However,\nreading times remain similar when cause A is not explicitly mentioned,\nindicating that humans can easily infer event B from their script knowledge. We\nthen tested a variety of LLMs on the same data to check to what extent the\nmodels replicate human behavior. Our experiments show that 1) only recent LLMs,\nlike GPT-3 or Vicuna, correlate with human behavior in the $\\neg A \\rightarrow\nB$ condition. 2) Despite this correlation, all models still fail to predict\nthat $nil \\rightarrow B$ is less surprising than $\\neg A \\rightarrow B$,\nindicating that LLMs still have difficulties integrating script knowledge. Our\ncode and collected data set are available at\nhttps://github.com/tony-hong/causal-script.", "published": "2023-11-13 13:05:15", "link": "http://arxiv.org/abs/2311.07311v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.0"], "primary_category": "cs.CL"}
{"title": "Semi-automatic Data Enhancement for Document-Level Relation Extraction\n  with Distant Supervision from Large Language Models", "abstract": "Document-level Relation Extraction (DocRE), which aims to extract relations\nfrom a long context, is a critical challenge in achieving fine-grained\nstructural comprehension and generating interpretable document representations.\nInspired by recent advances in in-context learning capabilities emergent from\nlarge language models (LLMs), such as ChatGPT, we aim to design an automated\nannotation method for DocRE with minimum human effort. Unfortunately, vanilla\nin-context learning is infeasible for document-level relation extraction due to\nthe plenty of predefined fine-grained relation types and the uncontrolled\ngenerations of LLMs. To tackle this issue, we propose a method integrating a\nlarge language model (LLM) and a natural language inference (NLI) module to\ngenerate relation triples, thereby augmenting document-level relation datasets.\nWe demonstrate the effectiveness of our approach by introducing an enhanced\ndataset known as DocGNRE, which excels in re-annotating numerous long-tail\nrelation types. We are confident that our method holds the potential for\nbroader applications in domain-specific relation type definitions and offers\ntangible benefits in advancing generalized language semantic comprehension.", "published": "2023-11-13 13:10:44", "link": "http://arxiv.org/abs/2311.07314v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Impact of Large Language Models on Scientific Discovery: a\n  Preliminary Study using GPT-4", "abstract": "In recent years, groundbreaking advancements in natural language processing\nhave culminated in the emergence of powerful large language models (LLMs),\nwhich have showcased remarkable capabilities across a vast array of domains,\nincluding the understanding, generation, and translation of natural language,\nand even tasks that extend beyond language processing. In this report, we delve\ninto the performance of LLMs within the context of scientific discovery,\nfocusing on GPT-4, the state-of-the-art language model. Our investigation spans\na diverse range of scientific areas encompassing drug discovery, biology,\ncomputational chemistry (density functional theory (DFT) and molecular dynamics\n(MD)), materials design, and partial differential equations (PDE). Evaluating\nGPT-4 on scientific tasks is crucial for uncovering its potential across\nvarious research domains, validating its domain-specific expertise,\naccelerating scientific progress, optimizing resource allocation, guiding\nfuture model development, and fostering interdisciplinary research. Our\nexploration methodology primarily consists of expert-driven case assessments,\nwhich offer qualitative insights into the model's comprehension of intricate\nscientific concepts and relationships, and occasionally benchmark testing,\nwhich quantitatively evaluates the model's capacity to solve well-defined\ndomain-specific problems. Our preliminary exploration indicates that GPT-4\nexhibits promising potential for a variety of scientific applications,\ndemonstrating its aptitude for handling complex problem-solving and knowledge\nintegration tasks. Broadly speaking, we evaluate GPT-4's knowledge base,\nscientific understanding, scientific numerical calculation abilities, and\nvarious scientific prediction capabilities.", "published": "2023-11-13 14:26:12", "link": "http://arxiv.org/abs/2311.07361v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Volcano: Mitigating Multimodal Hallucination through Self-Feedback\n  Guided Revision", "abstract": "Large multimodal models suffer from multimodal hallucination, where they\nprovide incorrect responses misaligned with the given visual information.\nRecent works have conjectured that one of the reasons behind multimodal\nhallucination is due to the vision encoder failing to ground on the image\nproperly. To mitigate this issue, we propose a novel approach that leverages\nself-feedback as visual cues. Building on this approach, we introduce Volcano,\na multimodal self-feedback guided revision model. Volcano generates natural\nlanguage feedback to its initial response based on the provided visual\ninformation and utilizes this feedback to self-revise its initial response.\nVolcano effectively reduces multimodal hallucination and achieves\nstate-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general\nmultimodal abilities and outperforms previous models on MM-Vet and MMBench.\nThrough qualitative analysis, we show that Volcano's feedback is properly\ngrounded on the image than the initial response. This indicates that Volcano\ncan provide itself with richer visual information through feedback generation,\nleading to self-correct hallucinations. We publicly release our model, data,\nand code at https://github.com/kaistAI/Volcano}{github.com/kaistAI/Volcano", "published": "2023-11-13 14:26:24", "link": "http://arxiv.org/abs/2311.07362v4", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LM-Polygraph: Uncertainty Estimation for Language Models", "abstract": "Recent advancements in the capabilities of large language models (LLMs) have\npaved the way for a myriad of groundbreaking applications in various fields.\nHowever, a significant challenge arises as these models often \"hallucinate\",\ni.e., fabricate facts without providing users an apparent means to discern the\nveracity of their statements. Uncertainty estimation (UE) methods are one path\nto safer, more responsible, and more effective use of LLMs. However, to date,\nresearch on UE methods for LLMs has been focused primarily on theoretical\nrather than engineering contributions. In this work, we tackle this issue by\nintroducing LM-Polygraph, a framework with implementations of a battery of\nstate-of-the-art UE methods for LLMs in text generation tasks, with unified\nprogram interfaces in Python. Additionally, it introduces an extendable\nbenchmark for consistent evaluation of UE techniques by researchers, and a demo\nweb application that enriches the standard chat dialog with confidence scores,\nempowering end-users to discern unreliable responses. LM-Polygraph is\ncompatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and\nGPT-4, and is designed to support future releases of similarly-styled LMs.", "published": "2023-11-13 15:08:59", "link": "http://arxiv.org/abs/2311.07383v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination\n  Evaluation", "abstract": "Despite making significant progress in multi-modal tasks, current Multi-modal\nLarge Language Models (MLLMs) encounter the significant challenge of\nhallucinations, which may lead to harmful consequences. Therefore, evaluating\nMLLMs' hallucinations is becoming increasingly important in model improvement\nand practical application deployment. Previous works are limited in high\nevaluation costs (e.g., relying on humans or advanced LLMs) and insufficient\nevaluation dimensions (e.g., types of tasks and hallucinations). In this paper,\nwe propose an LLM-free multi-dimensional benchmark AMBER, which can be used to\nevaluate both generative task and discriminative task including existence,\nattribute and relation hallucination. Based on AMBER, we design a low-cost and\nefficient evaluation pipeline. Additionally, we conduct a comprehensive\nevaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision),\nand also give guideline suggestions for mitigating hallucinations. The data and\ncode of AMBER are available at https://github.com/junyangwang0410/AMBER.", "published": "2023-11-13 15:25:42", "link": "http://arxiv.org/abs/2311.07397v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Hallucination Augmented Recitations for Language Models", "abstract": "Attribution is a key concept in large language models (LLMs) as it enables\ncontrol over information sources and enhances the factuality of LLMs. While\nexisting approaches utilize open book question answering to improve\nattribution, factual datasets may reward language models to recall facts that\nthey already know from their pretraining data, not attribution. In contrast,\ncounterfactual open book QA datasets would further improve attribution because\nthe answer could only be grounded in the given text. We propose Hallucination\nAugmented Recitations (HAR) for creating counterfactual datasets by utilizing\nhallucination in LLMs to improve attribution. For open book QA as a case study,\nwe demonstrate that models finetuned with our counterfactual datasets improve\ntext grounding, leading to better open book QA performance, with up to an 8.0%\nincrease in F1 score. Our counterfactual dataset leads to significantly better\nperformance than using humanannotated factual datasets, even with 4x smaller\ndatasets and 4x smaller models. We observe that improvements are consistent\nacross various model sizes and datasets, including multi-hop, biomedical, and\nadversarial QA datasets.", "published": "2023-11-13 15:58:18", "link": "http://arxiv.org/abs/2311.07424v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Think Before You Speak: Cultivating Communication Skills of Large\n  Language Models via Inner Monologue", "abstract": "The emergence of large language models (LLMs) further improves the\ncapabilities of open-domain dialogue systems and can generate fluent, coherent,\nand diverse responses. However, LLMs still lack a crucial ability:\ncommunication skills. This limitation renders them more like information\nseeking tools rather than anthropomorphic chatbots. Communication skills, such\nas topic transition, proactively asking questions, concept guidance, empathy,\nand summarising often should be taken into consideration, to make LLMs more\nanthropomorphic and proactive during the conversation, thereby increasing the\ninterest of users and attracting them to chat for longer. However, enabling\nthese communication skills in black-box LLMs remains a key challenge because\nthey do not have the same utterance formation mode as real people: think before\nspeaking. Inspired by linguistics and cognitive science, we empower LLMs with\ncommunication skills through inner monologues. To evaluate various\ncommunication skills, we construct a benchmark named Cskills, which can also\nmore comprehensively evaluate the dialogue generation ability of the model.\nExperimental results show that the proposed CSIM strategy improves the backbone\nmodels and outperforms the baselines.", "published": "2023-11-13 16:19:42", "link": "http://arxiv.org/abs/2311.07445v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChartCheck: Explainable Fact-Checking over Real-World Chart Images", "abstract": "Whilst fact verification has attracted substantial interest in the natural\nlanguage processing community, verifying misinforming statements against data\nvisualizations such as charts has so far been overlooked. Charts are commonly\nused in the real-world to summarize and communicate key information, but they\ncan also be easily misused to spread misinformation and promote certain\nagendas. In this paper, we introduce ChartCheck, a novel, large-scale dataset\nfor explainable fact-checking against real-world charts, consisting of 1.7k\ncharts and 10.5k human-written claims and explanations. We systematically\nevaluate ChartCheck using vision-language and chart-to-table models, and\npropose a baseline to the community. Finally, we study chart reasoning types\nand visual attributes that pose a challenge to these models", "published": "2023-11-13 16:35:29", "link": "http://arxiv.org/abs/2311.07453v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "InCA: Rethinking In-Car Conversational System Assessment Leveraging\n  Large Language Models", "abstract": "The assessment of advanced generative large language models (LLMs) poses a\nsignificant challenge, given their heightened complexity in recent\ndevelopments. Furthermore, evaluating the performance of LLM-based applications\nin various industries, as indicated by Key Performance Indicators (KPIs), is a\ncomplex undertaking. This task necessitates a profound understanding of\nindustry use cases and the anticipated system behavior. Within the context of\nthe automotive industry, existing evaluation metrics prove inadequate for\nassessing in-car conversational question answering (ConvQA) systems. The unique\ndemands of these systems, where answers may relate to driver or car safety and\nare confined within the car domain, highlight the limitations of current\nmetrics. To address these challenges, this paper introduces a set of KPIs\ntailored for evaluating the performance of in-car ConvQA systems, along with\ndatasets specifically designed for these KPIs. A preliminary and comprehensive\nempirical evaluation substantiates the efficacy of our proposed approach.\nFurthermore, we investigate the impact of employing varied personas in prompts\nand found that it enhances the model's capacity to simulate diverse viewpoints\nin assessments, mirroring how individuals with different backgrounds perceive a\ntopic.", "published": "2023-11-13 17:02:06", "link": "http://arxiv.org/abs/2311.07469v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Psychometric Predictive Power of Large Language Models", "abstract": "Instruction tuning aligns the response of large language models (LLMs) with\nhuman preferences. Despite such efforts in human--LLM alignment, we find that\ninstruction tuning does not always make LLMs human-like from a cognitive\nmodeling perspective. More specifically, next-word probabilities estimated by\ninstruction-tuned LLMs are often worse at simulating human reading behavior\nthan those estimated by base LLMs. In addition, we explore prompting\nmethodologies for simulating human reading behavior with LLMs. Our results show\nthat prompts reflecting a particular linguistic hypothesis improve psychometric\npredictive power, but are still inferior to small base models. These findings\nhighlight that recent advancements in LLMs, i.e., instruction tuning and\nprompting, do not offer better estimates than direct probability measurements\nfrom base LLMs in cognitive modeling. In other words, pure next-word\nprobability remains a strong predictor for human reading behavior, even in the\nage of LLMs.", "published": "2023-11-13 17:19:14", "link": "http://arxiv.org/abs/2311.07484v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Multiple Teachers for Test-Time Adaptation of Language-Guided\n  Classifiers", "abstract": "Recent approaches have explored language-guided classifiers capable of\nclassifying examples from novel tasks when provided with task-specific natural\nlanguage explanations, instructions or prompts (Sanh et al., 2022; R. Menon et\nal., 2022). While these classifiers can generalize in zero-shot settings, their\ntask performance often varies substantially between different language\nexplanations in unpredictable ways (Lu et al., 2022; Gonen et al., 2022). Also,\ncurrent approaches fail to leverage unlabeled examples that may be available in\nmany scenarios. Here, we introduce TALC, a framework that uses data programming\nto adapt a language-guided classifier for a new task during inference when\nprovided with explanations from multiple teachers and unlabeled test examples.\nOur results show that TALC consistently outperforms a competitive baseline from\nprior work by an impressive 9.3% (relative improvement). Further, we\ndemonstrate the robustness of TALC to variations in the quality and quantity of\nprovided explanations, highlighting its potential in scenarios where learning\nfrom multiple teachers or a crowd is involved. Our code is available at:\nhttps://github.com/WeiKangda/TALC.git.", "published": "2023-11-13 18:28:25", "link": "http://arxiv.org/abs/2311.07538v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Authorship Attribution Models Distinguish Speakers in Speech\n  Transcripts?", "abstract": "Authorship verification is the task of determining if two distinct writing\nsamples share the same author and is typically concerned with the attribution\nof written text. In this paper, we explore the attribution of transcribed\nspeech, which poses novel challenges. The main challenge is that many stylistic\nfeatures, such as punctuation and capitalization, are not informative in this\nsetting. On the other hand, transcribed speech exhibits other patterns, such as\nfiller words and backchannels (e.g., 'um', 'uh-huh'), which may be\ncharacteristic of different speakers. We propose a new benchmark for speaker\nattribution focused on human-transcribed conversational speech transcripts. To\nlimit spurious associations of speakers with topic, we employ both conversation\nprompts and speakers participating in the same conversation to construct\nverification trials of varying difficulties. We establish the state of the art\non this new benchmark by comparing a suite of neural and non-neural baselines,\nfinding that although written text attribution models achieve surprisingly good\nperformance in certain settings, they perform markedly worse as conversational\ntopic is increasingly controlled. We present analyses of the impact of\ntranscription style on performance as well as the ability of fine-tuning on\nspeech transcripts to improve performance.", "published": "2023-11-13 18:54:17", "link": "http://arxiv.org/abs/2311.07564v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GreekT5: A Series of Greek Sequence-to-Sequence Models for News\n  Summarization", "abstract": "Text summarization (TS) is a natural language processing (NLP) subtask\npertaining to the automatic formulation of a concise and coherent summary that\ncovers the major concepts and topics from one or multiple documents. Recent\nadvancements in deep learning have led to the development of abstractive\nsummarization transformer-based models, which outperform classical approaches.\nIn any case, research in this field focuses on high resource languages such as\nEnglish, while the corresponding work for low resource languages is still\nunderdeveloped. Taking the above into account, this paper proposes a series of\nnovel TS models for Greek news articles. The proposed models were thoroughly\nevaluated on the same dataset against GreekBART, which is the state-of-the-art\nmodel in Greek abstractive news summarization. Our evaluation results reveal\nthat most of the proposed models significantly outperform GreekBART on various\nevaluation metrics. We make our evaluation code public, aiming to increase the\nreproducibility of this work and facilitate future research in the field.", "published": "2023-11-13 21:33:12", "link": "http://arxiv.org/abs/2311.07767v1", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "In-context Learning and Gradient Descent Revisited", "abstract": "In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. A recent\nline of work suggests that ICL performs gradient descent (GD)-based\noptimization implicitly. While appealing, much of the research focuses on\nsimplified settings, where the parameters of a shallow model are optimized. In\nthis work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks\nand models. We find gaps in evaluation, both in terms of problematic metrics\nand insufficient baselines. We show that surprisingly, even untrained models\nachieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,\nwe explore a major discrepancy in the flow of information throughout the model\nbetween ICL and GD, which we term Layer Causality. We propose a simple GD-based\noptimization procedure that respects layer causality, and show it improves\nsimilarity scores significantly.", "published": "2023-11-13 21:42:38", "link": "http://arxiv.org/abs/2311.07772v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Contentious Terms About People and Cultures are Used in Linked Open\n  Data", "abstract": "Web resources in linked open data (LOD) are comprehensible to humans through\nliteral textual values attached to them, such as labels, notes, or comments.\nWord choices in literals may not always be neutral. When outdated and\nculturally stereotyping terminology is used in literals, they may appear as\noffensive to users in interfaces and propagate stereotypes to algorithms\ntrained on them. We study how frequently and in which literals contentious\nterms about people and cultures occur in LOD and whether there are attempts to\nmark the usage of such terms. For our analysis, we reuse English and Dutch\nterms from a knowledge graph that provides opinions of experts from the\ncultural heritage domain about terms' contentiousness. We inspect occurrences\nof these terms in four widely used datasets: Wikidata, The Getty Art &\nArchitecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms\nare ambiguous and contentious only in particular senses. Applying word sense\ndisambiguation, we generate a set of literals relevant to our analysis. We\nfound that outdated, derogatory, stereotyping terms frequently appear in\ndescriptive and labelling literals, such as preferred labels that are usually\ndisplayed in interfaces and used for indexing. In some cases, LOD contributors\nmark contentious terms with words and phrases in literals (implicit markers) or\nproperties linked to resources (explicit markers). However, such marking is\nrare and non-consistent in all datasets. Our quantitative and qualitative\ninsights could be helpful in developing more systematic approaches to address\nthe propagation of stereotypes via LOD.", "published": "2023-11-13 18:25:20", "link": "http://arxiv.org/abs/2311.10757v1", "categories": ["cs.CL", "cs.AI", "I.2.1"], "primary_category": "cs.CL"}
{"title": "Teach me with a Whisper: Enhancing Large Language Models for Analyzing\n  Spoken Transcripts using Speech Embeddings", "abstract": "Speech data has rich acoustic and paralinguistic information with important\ncues for understanding a speaker's tone, emotion, and intent, yet traditional\nlarge language models such as BERT do not incorporate this information. There\nhas been an increased interest in multi-modal language models leveraging audio\nand/or visual information and text. However, current multi-modal language\nmodels require both text and audio/visual data streams during inference/test\ntime. In this work, we propose a methodology for training language models\nleveraging spoken language audio data but without requiring the audio stream\nduring prediction time. This leads to an improved language model for analyzing\nspoken transcripts while avoiding an audio processing overhead at test time. We\nachieve this via an audio-language knowledge distillation framework, where we\ntransfer acoustic and paralinguistic information from a pre-trained speech\nembedding (OpenAI Whisper) teacher model to help train a student language model\non an audio-text dataset. In our experiments, the student model achieves\nconsistent improvement over traditional language models on tasks analyzing\nspoken transcripts.", "published": "2023-11-13 01:53:12", "link": "http://arxiv.org/abs/2311.07014v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in\n  Video-Language Models", "abstract": "With the ever-increasing popularity of pretrained Video-Language Models\n(VidLMs), there is a pressing need to develop robust evaluation methodologies\nthat delve deeper into their visio-linguistic capabilities. To address this\nchallenge, we present ViLMA (Video Language Model Assessment), a task-agnostic\nbenchmark that places the assessment of fine-grained capabilities of these\nmodels on a firm footing. Task-based evaluations, while valuable, fail to\ncapture the complexities and specific temporal aspects of moving images that\nVidLMs need to process. Through carefully curated counterfactuals, ViLMA offers\na controlled evaluation suite that sheds light on the true potential of these\nmodels, as well as their performance gaps compared to human-level\nunderstanding. ViLMA also includes proficiency tests, which assess basic\ncapabilities deemed essential to solving the main counterfactual tests. We show\nthat current VidLMs' grounding abilities are no better than those of\nvision-language models which use static images. This is especially striking\nonce the performance on proficiency tests is factored in. Our benchmark serves\nas a catalyst for future research on VidLMs, helping to highlight areas that\nstill need to be explored.", "published": "2023-11-13 02:13:13", "link": "http://arxiv.org/abs/2311.07022v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Phonological Level wav2vec2-based Mispronunciation Detection and\n  Diagnosis Method", "abstract": "The automatic identification and analysis of pronunciation errors, known as\nMispronunciation Detection and Diagnosis (MDD) plays a crucial role in Computer\nAided Pronunciation Learning (CAPL) tools such as Second-Language (L2) learning\nor speech therapy applications. Existing MDD methods relying on analysing\nphonemes can only detect categorical errors of phonemes that have an adequate\namount of training data to be modelled. With the unpredictable nature of the\npronunciation errors of non-native or disordered speakers and the scarcity of\ntraining datasets, it is unfeasible to model all types of mispronunciations.\nMoreover, phoneme-level MDD approaches have a limited ability to provide\ndetailed diagnostic information about the error made. In this paper, we propose\na low-level MDD approach based on the detection of speech attribute features.\nSpeech attribute features break down phoneme production into elementary\ncomponents that are directly related to the articulatory system leading to more\nformative feedback to the learner. We further propose a multi-label variant of\nthe Connectionist Temporal Classification (CTC) approach to jointly model the\nnon-mutually exclusive speech attributes using a single model. The pre-trained\nwav2vec2 model was employed as a core model for the speech attribute detector.\nThe proposed method was applied to L2 speech corpora collected from English\nlearners from different native languages. The proposed speech attribute MDD\nmethod was further compared to the traditional phoneme-level MDD and achieved a\nsignificantly lower False Acceptance Rate (FAR), False Rejection Rate (FRR),\nand Diagnostic Error Rate (DER) over all speech attributes compared to the\nphoneme-level equivalent.", "published": "2023-11-13 02:41:41", "link": "http://arxiv.org/abs/2311.07037v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Towards the Law of Capacity Gap in Distilling Language Models", "abstract": "Language model (LM) distillation is a trending area that aims to distil the\nknowledge residing in a large teacher LM to a small student one. While various\nmethods have been proposed to maximize the effectiveness of the distillation,\nsignificant challenges persist, particularly when there is a substantial\ncapacity gap between the teacher and student LMs. This issue, often referred to\nas the \\textit{curse} of capacity gap, suggests that a larger teacher does not\nnecessarily result in a superior student compared to one distilled from a\nsmaller teacher. In other words, there is likely an optimal teacher yielding\nthe best student along the scaling course of the teacher. However, the curse of\ncapacity gap can not be tackled without notable compute overhead, as indicated\nin previous studies. In the context of large LMs (LLMs), previously viable\napproaches become much less meaningful, as it is an impossible triangle to\ndistill an expected student from an optimal teacher student with small compute\noverhead. Fortunately, the impossible triangle can fortunately be possible\nprovided an inducted \\textit{law} of capacity gap. In this paper, we take the\nspirits of scaling law and reveal that the optimal teacher scale almost\nconsistently follows a linear scaling with the student scale across different\nmodel architectures and data scales. The law later guides us to distil a 3B\nstudent LM (termed \\textsc{MiniMA}) from LLaMA2-7B. \\textsc{MiniMA} is\ndemonstrated to outperform a wide range of 3B competitors and could even\ncompete with several 7B models.", "published": "2023-11-13 03:36:18", "link": "http://arxiv.org/abs/2311.07052v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of ASR Representations in Real-world Noisy Speech\n  Emotion Recognition", "abstract": "This paper proposes an efficient attempt to noisy speech emotion recognition\n(NSER). Conventional NSER approaches have proven effective in mitigating the\nimpact of artificial noise sources, such as white Gaussian noise, but are\nlimited to non-stationary noises in real-world environments due to their\ncomplexity and uncertainty. To overcome this limitation, we introduce a new\nmethod for NSER by adopting the automatic speech recognition (ASR) model as a\nnoise-robust feature extractor to eliminate non-vocal information in noisy\nspeech. We first obtain intermediate layer information from the ASR model as a\nfeature representation for emotional speech and then apply this representation\nfor the downstream NSER task. Our experimental results show that 1) the\nproposed method achieves better NSER performance compared with the conventional\nnoise reduction method, 2) outperforms self-supervised learning approaches, and\n3) even outperforms text-based approaches using ASR transcription or the ground\ntruth transcription of noisy speech.", "published": "2023-11-13 05:45:55", "link": "http://arxiv.org/abs/2311.07093v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interaction is all You Need? A Study of Robots Ability to Understand and\n  Execute", "abstract": "This paper aims to address a critical challenge in robotics, which is\nenabling them to operate seamlessly in human environments through natural\nlanguage interactions. Our primary focus is to equip robots with the ability to\nunderstand and execute complex instructions in coherent dialogs to facilitate\nintricate task-solving scenarios. To explore this, we build upon the Execution\nfrom Dialog History (EDH) task from the Teach benchmark. We employ a\nmulti-transformer model with BART LM. We observe that our best configuration\noutperforms the baseline with a success rate score of 8.85 and a\ngoal-conditioned success rate score of 14.02. In addition, we suggest an\nalternative methodology for completing this task. Moreover, we introduce a new\ntask by expanding the EDH task and making predictions about game plans instead\nof individual actions. We have evaluated multiple BART models and an LLaMA2\nLLM, which has achieved a ROGUE-L score of 46.77 for this task.", "published": "2023-11-13 08:39:06", "link": "http://arxiv.org/abs/2311.07150v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Speech-based Slot Filling using Large Language Models", "abstract": "Recently, advancements in large language models (LLMs) have shown an\nunprecedented ability across various language tasks. This paper investigates\nthe potential application of LLMs to slot filling with noisy ASR\ntranscriptions, via both in-context learning and task-specific fine-tuning.\nDedicated prompt designs and fine-tuning approaches are proposed to improve the\nrobustness of LLMs for slot filling with noisy ASR transcriptions. Moreover, a\nlinearised knowledge injection (LKI) scheme is also proposed to integrate\ndynamic external knowledge into LLMs. Experiments were performed on SLURP to\nquantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B and\nVicuna-13B (v1.1 and v1.5) with different ASR error rates. The use of the\nproposed fine-tuning together with the LKI scheme for LLaMA-13B achieved an\n8.3% absolute SLU-F1 improvement compared to the strong Flan-T5-base baseline\nsystem on a limited data setup.", "published": "2023-11-13 15:54:30", "link": "http://arxiv.org/abs/2311.07418v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Investigating Multi-Pivot Ensembling with Massively Multilingual Machine\n  Translation Models", "abstract": "Massively multilingual machine translation models allow for the translation\nof a large number of languages with a single model, but have limited\nperformance on low- and very-low-resource translation directions. Pivoting via\nhigh-resource languages remains a strong strategy for low-resource directions,\nand in this paper we revisit ways of pivoting through multiple languages.\nPrevious work has used a simple averaging of probability distributions from\nmultiple paths, but we find that this performs worse than using a single pivot,\nand exacerbates the hallucination problem because the same hallucinations can\nbe probable across different paths. We also propose MaxEns, a novel combination\nstrategy that makes the output biased towards the most confident predictions,\nhypothesising that confident predictions are less prone to be hallucinations.\nWe evaluate different strategies on the FLORES benchmark for 20 low-resource\nlanguage directions, demonstrating that MaxEns improves translation quality for\nlow-resource languages while reducing hallucination in translations, compared\nto both direct translation and an averaging approach. On average, multi-pivot\nstrategies still lag behind using English as a single pivot language, raising\nthe question of how to identify the best pivoting strategy for a given\ntranslation direction.", "published": "2023-11-13 16:15:20", "link": "http://arxiv.org/abs/2311.07439v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Measuring Faithfulness or Self-consistency of Natural Language\n  Explanations", "abstract": "Large language models (LLMs) can explain their predictions through post-hoc\nor Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably\nsounding explanations that are unfaithful to its underlying reasoning. Recent\nwork has designed tests that aim to judge the faithfulness of post-hoc or CoT\nexplanations. In this work we argue that these faithfulness tests do not\nmeasure faithfulness to the models' inner workings -- but rather their\nself-consistency at output level. Our contributions are three-fold: i) We\nclarify the status of faithfulness tests in view of model explainability,\ncharacterising them as self-consistency tests instead. This assessment we\nunderline by ii) constructing a Comparative Consistency Bank for\nself-consistency tests that for the first time compares existing tests on a\ncommon suite of 11 open LLMs and 5 tasks -- including iii) our new\nself-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a\ntest) of LLM self-consistency. It compares how a model's input contributes to\nthe predicted answer and to generating the explanation. Our fine-grained\nCC-SHAP metric allows us iii) to compare LLM behaviour when making predictions\nand to analyse the effect of other consistency tests at a deeper level, which\ntakes us one step further towards measuring faithfulness by bringing us closer\nto the internals of the model than strictly surface output-oriented tests. Our\ncode is available at \\url{https://github.com/Heidelberg-NLP/CC-SHAP}", "published": "2023-11-13 16:53:51", "link": "http://arxiv.org/abs/2311.07466v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "68Txx", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "An Analysis and Mitigation of the Reversal Curse", "abstract": "Recent research observed a noteworthy phenomenon in large language models\n(LLMs), referred to as the ``reversal curse.'' The reversal curse is that when\ndealing with two entities, denoted as $a$ and $b$, connected by their relation\n$R$ and its inverse $R^{-1}$, LLMs excel in handling sequences in the form of\n``$aRb$,'' but encounter challenges when processing ``$bR^{-1}a$,'' whether in\ngeneration or comprehension. For instance, GPT-4 can accurately respond to the\nquery ``Tom Cruise's mother is?'' with ``Mary Lee Pfeiffer,'' but it struggles\nto provide a satisfactory answer when asked ``Mary Lee Pfeiffer's son is?'' In\nthis paper, we undertake the first-ever study of how the reversal curse happens\nin LLMs. Our investigations reveal that the reversal curse can stem from the\nspecific training objectives, which become particularly evident in the\nwidespread use of next-token prediction within most causal language models. We\nhope this initial investigation can draw more attention to the reversal curse,\nas well as other underlying limitations in current LLMs.", "published": "2023-11-13 17:01:12", "link": "http://arxiv.org/abs/2311.07468v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Benchmark to Understand the Role of Knowledge Graphs on Large Language\n  Model's Accuracy for Question Answering on Enterprise SQL Databases", "abstract": "Enterprise applications of Large Language Models (LLMs) hold promise for\nquestion answering on enterprise SQL databases. However, the extent to which\nLLMs can accurately respond to enterprise questions in such databases remains\nunclear, given the absence of suitable Text-to-SQL benchmarks tailored to\nenterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to\nenhance LLM-based question answering by providing business context is not well\nunderstood. This study aims to evaluate the accuracy of LLM-powered question\nanswering systems in the context of enterprise questions and SQL databases,\nwhile also exploring the role of knowledge graphs in improving accuracy. To\nachieve this, we introduce a benchmark comprising an enterprise SQL schema in\nthe insurance domain, a range of enterprise queries encompassing reporting to\nmetrics, and a contextual layer incorporating an ontology and mappings that\ndefine a knowledge graph. Our primary finding reveals that question answering\nusing GPT-4, with zero-shot prompts directly on SQL databases, achieves an\naccuracy of 16%. Notably, this accuracy increases to 54% when questions are\nposed over a Knowledge Graph representation of the enterprise SQL database.\nTherefore, investing in Knowledge Graph provides higher accuracy for LLM\npowered question answering systems.", "published": "2023-11-13 17:54:50", "link": "http://arxiv.org/abs/2311.07509v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "GPT-4V(ision) as A Social Media Analysis Engine", "abstract": "Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.", "published": "2023-11-13 18:36:50", "link": "http://arxiv.org/abs/2311.07547v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for\n  Multi-modal Large Language Models", "abstract": "We present SPHINX, a versatile multi-modal large language model (MLLM) with a\njoint mixing of model weights, tuning tasks, and visual embeddings. First, for\nstronger vision-language alignment, we unfreeze the large language model (LLM)\nduring pre-training, and introduce a weight mix strategy between LLMs trained\nby real-world and synthetic data. By directly integrating the weights from two\ndomains, the mixed LLM can efficiently incorporate diverse semantics with\nfavorable robustness. Then, to enable multi-purpose capabilities, we mix a\nvariety of tasks for joint visual instruction tuning, and design task-specific\ninstructions to avoid inter-task conflict. In addition to the basic visual\nquestion answering, we include more challenging tasks such as region-level\nunderstanding, caption grounding, document layout detection, and human pose\nestimation, contributing to mutual enhancement over different scenarios.\nAdditionally, we propose to extract comprehensive visual embeddings from\nvarious network architectures, pre-training paradigms, and information\ngranularity, providing language models with more robust image representations.\nBased on our proposed joint mixing, SPHINX exhibits superior multi-modal\nunderstanding capabilities on a wide range of applications. On top of this, we\nfurther propose an efficient strategy aiming to better capture fine-grained\nappearances of high-resolution images. With a mixing of different scales and\nhigh-resolution sub-images, SPHINX attains exceptional visual parsing and\nreasoning performance on existing evaluation benchmarks. We hope our work may\ncast a light on the exploration of joint mixing in future MLLM research. Code\nis released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.", "published": "2023-11-13 18:59:47", "link": "http://arxiv.org/abs/2311.07575v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Past as a Guide: Leveraging Retrospective Learning for Python Code\n  Completion", "abstract": "This work presents Past as a Guide (PaG), a simple approach for Large\nLanguage Models (LLMs) to improve the coding capabilities by integrating the\npast history with interactive and iterative code refinements. To be specific,\ninspired by human cognitive processes, the proposed method enables LLMs to\nutilize previous programming and debugging experiences to enhance the Python\ncode completion tasks. The framework facilitates LLMs to iteratively refine the\nPython code based on previous execution and debugging results and optimize\nlearning and reasoning capabilities. The proposed methodology achieved a 92\\%\npass@1 on HumanEval, demonstrating the potential to advance the field by\nleveraging retrospection from past experiences and interactive and iterative\nrefinement processes without external correctness indicators.", "published": "2023-11-13 14:40:33", "link": "http://arxiv.org/abs/2311.07635v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Fuse to Forget: Bias Reduction and Selective Memorization through Model\n  Fusion", "abstract": "Model fusion research aims to aggregate the knowledge of multiple individual\nmodels to enhance performance by combining their weights. In this work, we\nstudy the inverse problem: investigating whether model fusion can be used to\nreduce unwanted knowledge. We investigate the effects of model fusion in three\nscenarios: the learning of shortcuts, social biases, and memorization of\ntraining data in fine-tuned language models. Through experiments covering\nclassification and generation tasks, our analysis highlights that shared\nknowledge among models is enhanced during model fusion, while unshared\nknowledge is usually forgotten. Based on this observation, we demonstrate the\npotential of model fusion as a debiasing tool and showcase its efficacy in\naddressing privacy concerns associated with language models.", "published": "2023-11-13 19:02:56", "link": "http://arxiv.org/abs/2311.07682v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend\n  Actions in Text Games", "abstract": "Large Language Models (LLMs) have demonstrated superior performance in\nlanguage understanding benchmarks. CALM, a popular approach, leverages\nlinguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to\nimprove the performance in text games in Jericho without environment-provided\nactions. However, CALM adapts GPT-2 with annotated human gameplays and keeps\nthe LLM fixed during the learning of the text based games. In this work, we\nexplore and evaluate updating LLM used for candidate recommendation during the\nlearning of the text based game as well to mitigate the reliance on the human\nannotated gameplays, which are costly to acquire. We observe that by updating\nthe LLM during learning using carefully selected in-game transitions, we can\nreduce the dependency on using human annotated game plays for fine-tuning the\nLLMs. We conducted further analysis to study the transferability of the updated\nLLMs and observed that transferring in-game trained models to other games did\nnot result in a consistent transfer.", "published": "2023-11-13 19:12:49", "link": "http://arxiv.org/abs/2311.07687v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On The Truthfulness of 'Surprisingly Likely' Responses of Large Language\n  Models", "abstract": "The principle of rewarding a crowd for surprisingly common answers has been\nused in the literature for designing a number of truthful information\nelicitation mechanisms. A related method has also been proposed in the\nliterature for better aggregation of crowd wisdom. Drawing a comparison between\ncrowd based collective intelligence systems and large language models, we\ndefine the notion of 'surprisingly likely' textual response of a large language\nmodel. This notion is inspired by the surprisingly common principle, but\ntailored for text in a language model. Using benchmarks such as TruthfulQA and\nopenly available LLMs: GPT-2 and LLaMA-2, we show that the surprisingly likely\ntextual responses of large language models are more accurate in many cases\ncompared to standard baselines. For example, we observe up to 24 percentage\npoints aggregate improvement on TruthfulQA and up to 70 percentage points\nimprovement on individual categories of questions in this benchmark. We also\nprovide further analysis of the results, including the cases when surprisingly\nlikely responses are less or not more accurate.", "published": "2023-11-13 19:21:25", "link": "http://arxiv.org/abs/2311.07692v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.LG"}
{"title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language\n  Models Denoising", "abstract": "Large language models (LLMs) have opened up enormous opportunities while\nsimultaneously posing ethical dilemmas. One of the major concerns is their\nability to create text that closely mimics human writing, which can lead to\npotential misuse, such as academic misconduct, disinformation, and fraud. To\naddress this problem, we present AuthentiGPT, an efficient classifier that\ndistinguishes between machine-generated and human-written texts. Under the\nassumption that human-written text resides outside the distribution of\nmachine-generated text, AuthentiGPT leverages a black-box LLM to denoise input\ntext with artificially added noise, and then semantically compares the denoised\ntext with the original to determine if the content is machine-generated. With\nonly one trainable parameter, AuthentiGPT eliminates the need for a large\ntraining dataset, watermarking the LLM's output, or computing the\nlog-likelihood. Importantly, the detection capability of AuthentiGPT can be\neasily adapted to any generative language model. With a 0.918 AUROC score on a\ndomain-specific dataset, AuthentiGPT demonstrates its effectiveness over other\ncommercial algorithms, highlighting its potential for detecting\nmachine-generated text in academic settings.", "published": "2023-11-13 19:36:54", "link": "http://arxiv.org/abs/2311.07700v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Entrainment in Spontaneous Code-switched Speech", "abstract": "It is well-known that speakers who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans, finding that (1)\npatterns of written and spoken entrainment in monolingual settings largely\ngeneralize to code-switched settings, and (2) some patterns of entrainment on\ncode-switching in dialogue agent-generated text generalize to spontaneous\ncode-switched speech. Our findings give rise to important implications for the\npotentially \"universal\" nature of entrainment as a communication phenomenon,\nand potential applications in inclusive and interactive speech technology.", "published": "2023-11-13 19:41:34", "link": "http://arxiv.org/abs/2311.07703v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PolyIE: A Dataset of Information Extraction from Polymer Material\n  Scientific Literature", "abstract": "Scientific information extraction (SciIE), which aims to automatically\nextract information from scientific literature, is becoming more important than\never. However, there are no existing SciIE datasets for polymer materials,\nwhich is an important class of materials used ubiquitously in our daily lives.\nTo bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer\nmaterials. POLYIE is curated from 146 full-length polymer scholarly articles,\nwhich are annotated with different named entities (i.e., materials, properties,\nvalues, conditions) as well as their N-ary relations by domain experts. POLYIE\npresents several unique challenges due to diverse lexical formats of entities,\nambiguity between entities, and variable-length relations. We evaluate\nstate-of-the-art named entity extraction and relation extraction models on\nPOLYIE, analyze their strengths and weaknesses, and highlight some difficult\ncases for these models. To the best of our knowledge, POLYIE is the first SciIE\nbenchmark for polymer materials, and we hope it will lead to more research\nefforts from the community on this challenging task. Our code and data are\navailable on: https://github.com/jerry3027/PolyIE.", "published": "2023-11-13 19:56:18", "link": "http://arxiv.org/abs/2311.07715v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalization Analogies: A Testbed for Generalizing AI Oversight to\n  Hard-To-Measure Domains", "abstract": "As AI systems become more intelligent and their behavior becomes more\nchallenging to assess, they may learn to game the flaws of human feedback\ninstead of genuinely striving to follow instructions; however, this risk can be\nmitigated by controlling how LLMs generalize human feedback to situations where\nit is unreliable. To better understand how reward models generalize, we craft\n69 distribution shifts spanning 8 categories. We find that reward models do not\nlearn to evaluate `instruction-following' by default and instead favor personas\nthat resemble internet text. Techniques for interpreting reward models'\ninternal representations achieve better generalization than standard\nfine-tuning, but still frequently fail to distinguish instruction-following\nfrom conflated behaviors. We consolidate the 15 most challenging distribution\nshifts into the GENeralization analogIES (GENIES) benchmark, which we hope will\nenable progress toward controlling reward model generalization.", "published": "2023-11-13 20:07:36", "link": "http://arxiv.org/abs/2311.07723v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Vision-Language Integration in Multimodal Video Transformers (Partially)\n  Aligns with the Brain", "abstract": "Integrating information from multiple modalities is arguably one of the\nessential prerequisites for grounding artificial intelligence systems with an\nunderstanding of the real world. Recent advances in video transformers that\njointly learn from vision, text, and sound over time have made some progress\ntoward this goal, but the degree to which these models integrate information\nfrom modalities still remains unclear. In this work, we present a promising\napproach for probing a pre-trained multimodal video transformer model by\nleveraging neuroscientific evidence of multimodal information processing in the\nbrain. Using brain recordings of participants watching a popular TV show, we\nanalyze the effects of multi-modal connections and interactions in a\npre-trained multi-modal video transformer on the alignment with uni- and\nmulti-modal brain regions. We find evidence that vision enhances masked\nprediction performance during language processing, providing support that\ncross-modal representations in models can benefit individual modalities.\nHowever, we don't find evidence of brain-relevant information captured by the\njoint multi-modal transformer representations beyond that captured by all of\nthe individual modalities. We finally show that the brain alignment of the\npre-trained joint representation can be improved by fine-tuning using a task\nthat requires vision-language inferences. Overall, our results paint an\noptimistic picture of the ability of multi-modal transformers to integrate\nvision and language in partially brain-relevant ways but also show that\nimproving the brain alignment of these models may require new approaches.", "published": "2023-11-13 21:32:37", "link": "http://arxiv.org/abs/2311.07766v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Decoupling and Interacting Multi-Task Learning Network for Joint Speech\n  and Accent Recognition", "abstract": "Accents, as variations from standard pronunciation, pose significant\nchallenges for speech recognition systems. Although joint automatic speech\nrecognition (ASR) and accent recognition (AR) training has been proven\neffective in handling multi-accent scenarios, current multi-task ASR-AR\napproaches overlook the granularity differences between tasks. Fine-grained\nunits capture pronunciation-related accent characteristics, while\ncoarse-grained units are better for learning linguistic information. Moreover,\nan explicit interaction of two tasks can also provide complementary information\nand improve the performance of each other, but it is rarely used by existing\napproaches. In this paper, we propose a novel Decoupling and Interacting\nMulti-task Network (DIMNet) for joint speech and accent recognition, which is\ncomprised of a connectionist temporal classification (CTC) branch, an AR\nbranch, an ASR branch, and a bottom feature encoder. Specifically, AR and ASR\nare first decoupled by separated branches and two-granular modeling units to\nlearn task-specific representations. The AR branch is from our previously\nproposed linguistic-acoustic bimodal AR model and the ASR branch is an\nencoder-decoder based Conformer model. Then, for the task interaction, the CTC\nbranch provides aligned text for the AR task, while accent embeddings extracted\nfrom our AR model are incorporated into the ASR branch's encoder and decoder.\nFinally, during ASR inference, a cross-granular rescoring method is introduced\nto fuse the complementary information from the CTC and attention decoder after\nthe decoupling. Our experiments on English and Chinese datasets demonstrate the\neffectiveness of the proposed model, which achieves 21.45%/28.53% AR accuracy\nrelative improvement and 32.33%/14.55% ASR error rate relative reduction over a\npublished standard baseline, respectively.", "published": "2023-11-13 04:03:22", "link": "http://arxiv.org/abs/2311.07062v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music ControlNet: Multiple Time-varying Controls for Music Generation", "abstract": "Text-to-music generation models are now capable of generating high-quality\nmusic audio in broad styles. However, text control is primarily suitable for\nthe manipulation of global musical attributes like genre, mood, and tempo, and\nis less suitable for precise control over time-varying attributes such as the\npositions of beats in time or the changing dynamics of the music. We propose\nMusic ControlNet, a diffusion-based music generation model that offers multiple\nprecise, time-varying controls over generated audio. To imbue text-to-music\nmodels with time-varying control, we propose an approach analogous to\npixel-wise control of the image-domain ControlNet method. Specifically, we\nextract controls from training audio yielding paired data, and fine-tune a\ndiffusion-based conditional generative model over audio spectrograms given\nmelody, dynamics, and rhythm controls. While the image-domain Uni-ControlNet\nmethod already allows generation with any subset of controls, we devise a new\nstrategy to allow creators to input controls that are only partially specified\nin time. We evaluate both on controls extracted from audio and controls we\nexpect creators to provide, demonstrating that we can generate realistic music\nthat corresponds to control inputs in both settings. While few comparable music\ngeneration models exist, we benchmark against MusicGen, a recent model that\naccepts text and melody input, and show that our model generates music that is\n49% more faithful to input melodies despite having 35x fewer parameters,\ntraining on 11x less data, and enabling two additional forms of time-varying\ncontrol. Sound examples can be found at https://MusicControlNet.github.io/web/.", "published": "2023-11-13 04:24:14", "link": "http://arxiv.org/abs/2311.07069v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SponTTS: modeling and transferring spontaneous style for TTS", "abstract": "Spontaneous speaking style exhibits notable differences from other speaking\nstyles due to various spontaneous phenomena (e.g., filled pauses, prolongation)\nand substantial prosody variation (e.g., diverse pitch and duration variation,\noccasional non-verbal speech like a smile), posing challenges to modeling and\nprediction of spontaneous style. Moreover, the limitation of high-quality\nspontaneous data constrains spontaneous speech generation for speakers without\nspontaneous data. To address these problems, we propose SponTTS, a two-stage\napproach based on neural bottleneck (BN) features to model and transfer\nspontaneous style for TTS. In the first stage, we adopt a Conditional\nVariational Autoencoder (CVAE) to capture spontaneous prosody from a BN feature\nand involve the spontaneous phenomena by the constraint of spontaneous\nphenomena embedding prediction loss. Besides, we introduce a flow-based\npredictor to predict a latent spontaneous style representation from the text,\nwhich enriches the prosody and context-specific spontaneous phenomena during\ninference. In the second stage, we adopt a VITS-like module to transfer the\nspontaneous style learned in the first stage to the target speakers.\nExperiments demonstrate that SponTTS is effective in modeling spontaneous style\nand transferring the style to the target speakers, generating spontaneous\nspeech with high naturalness, expressiveness, and speaker similarity. The\nzero-shot spontaneous style TTS test further verifies the generalization and\nrobustness of SponTTS in generating spontaneous speech for unseen speakers.", "published": "2023-11-13 09:11:27", "link": "http://arxiv.org/abs/2311.07179v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Duet Singing Voices Separation with Diffusion Models", "abstract": "In recent studies, diffusion models have shown promise as priors for solving\naudio inverse problems. These models allow us to sample from the posterior\ndistribution of a target signal given an observed signal by manipulating the\ndiffusion process. However, when separating audio sources of the same type,\nsuch as duet singing voices, the prior learned by the diffusion process may not\nbe sufficient to maintain the consistency of the source identity in the\nseparated audio. For example, the singer may change from one to another\noccasionally. Tackling this problem will be useful for separating sources in a\nchoir, or a mixture of multiple instruments with similar timbre, without\nacquiring large amounts of paired data. In this paper, we examine this problem\nin the context of duet singing voices separation, and propose a method to\nenforce the coherency of singer identity by splitting the mixture into\noverlapping segments and performing posterior sampling in an auto-regressive\nmanner, conditioning on the previous segment. We evaluate the proposed method\non the MedleyVox dataset and show that the proposed method outperforms the\nnaive posterior sampling baseline. Our source code and the pre-trained model\nare publicly available at https://github.com/iamycy/duet-svs-diffusion.", "published": "2023-11-13 14:01:21", "link": "http://arxiv.org/abs/2311.07345v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient bandwidth extension of musical signals using a differentiable\n  harmonic plus noise model", "abstract": "The task of bandwidth extension addresses the generation of missing high\nfrequencies of audio signals based on knowledge of the low-frequency part of\nthe sound. This task applies to various problems, such as audio coding or audio\nrestoration. In this article, we focus on efficient bandwidth extension of\nmonophonic and polyphonic musical signals using a differentiable digital signal\nprocessing (DDSP) model. Such a model is composed of a neural network part with\nrelatively few parameters trained to infer the parameters of a differentiable\ndigital signal processing model, which efficiently generates the output\nfull-band audio signal.\n  We first address bandwidth extension of monophonic signals, and then propose\ntwo methods to explicitely handle polyphonic signals. The benefits of the\nproposed models are first demonstrated on monophonic and polyphonic synthetic\ndata against a baseline and a deep-learning-based resnet model. The models are\nnext evaluated on recorded monophonic and polyphonic data, for a wide variety\nof instruments and musical genres. We show that all proposed models surpass a\nhigher complexity deep learning model for an objective metric computed in the\nfrequency domain. A MUSHRA listening test confirms the superiority of the\nproposed approach in terms of perceptual quality.", "published": "2023-11-13 14:26:32", "link": "http://arxiv.org/abs/2311.07363v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Musical Object Discovery from Audio", "abstract": "Current object-centric learning models such as the popular SlotAttention\narchitecture allow for unsupervised visual scene decomposition. Our novel\nMusicSlots method adapts SlotAttention to the audio domain, to achieve\nunsupervised music decomposition. Since concepts of opacity and occlusion in\nvision have no auditory analogues, the softmax normalization of alpha masks in\nthe decoders of visual object-centric models is not well-suited for decomposing\naudio objects. MusicSlots overcomes this problem. We introduce a\nspectrogram-based multi-object music dataset tailored to evaluate\nobject-centric learning on western tonal music. MusicSlots achieves good\nperformance on unsupervised note discovery and outperforms several established\nbaselines on supervised note property prediction tasks.", "published": "2023-11-13 18:21:33", "link": "http://arxiv.org/abs/2311.07534v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-modal Generative Model for Visual-Guided Binaural Stereo\n  Generation", "abstract": "Binaural stereo audio is recorded by imitating the way the human ear receives\nsound, which provides people with an immersive listening experience. Existing\napproaches leverage autoencoders and directly exploit visual spatial\ninformation to synthesize binaural stereo, resulting in a limited\nrepresentation of visual guidance. For the first time, we propose a visually\nguided generative adversarial approach for generating binaural stereo audio\nfrom mono audio. Specifically, we develop a Stereo Audio Generation Model\n(SAGM), which utilizes shared spatio-temporal visual information to guide the\ngenerator and the discriminator to work separately. The shared visual\ninformation is updated alternately in the generative adversarial stage,\nallowing the generator and discriminator to deliver their respective guided\nknowledge while visually sharing. The proposed method learns bidirectional\ncomplementary visual information, which facilitates the expression of visual\nguidance in generation. In addition, spatial perception is a crucial attribute\nof binaural stereo audio, and thus the evaluation of stereo spatial perception\nis essential. However, previous metrics failed to measure the spatial\nperception of audio. To this end, a metric to measure the spatial perception of\naudio is proposed for the first time. The proposed metric is capable of\nmeasuring the magnitude and direction of spatial perception in the temporal\ndimension. Further, considering its function, it is feasible to utilize it\ninstead of demanding user studies to some extent. The proposed method achieves\nstate-of-the-art performance on 2 datasets and 5 evaluation metrics.\nQualitative experiments and user studies demonstrate that the method generates\nspace-realistic stereo audio.", "published": "2023-11-13 09:53:14", "link": "http://arxiv.org/abs/2311.07630v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distributed pressure matching strategy using diffusion adaptation", "abstract": "Personal sound zone (PSZ) systems, which aim to create listening (bright) and\nsilent (dark) zones in neighboring regions of space, are often based on\ntime-varying acoustics. Conventional adaptive-based methods for handling PSZ\ntasks suffer from the collection and processing of acoustic transfer\nfunctions~(ATFs) between all the matching microphones and all the loudspeakers\nin a centralized manner, resulting in high calculation complexity and costly\naccuracy requirements. This paper presents a distributed pressure-matching (PM)\nmethod relying on diffusion adaptation (DPM-D) to spread the computational load\namongst nodes in order to overcome these issues. The global PM problem is\ndefined as a sum of local costs, and the diffusion adaption approach is then\nused to create a distributed solution that just needs local information\nexchanges. Simulations over multi-frequency bins and a computational complexity\nanalysis are conducted to evaluate the properties of the algorithm and to\ncompare it with centralized counterparts.", "published": "2023-11-13 20:24:13", "link": "http://arxiv.org/abs/2311.07729v1", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "cs.SD"}
{"title": "Parrot-Trained Adversarial Examples: Pushing the Practicality of\n  Black-Box Audio Attacks against Speaker Recognition Models", "abstract": "Audio adversarial examples (AEs) have posed significant security challenges\nto real-world speaker recognition systems. Most black-box attacks still require\ncertain information from the speaker recognition model to be effective (e.g.,\nkeeping probing and requiring the knowledge of similarity scores). This work\naims to push the practicality of the black-box attacks by minimizing the\nattacker's knowledge about a target speaker recognition model. Although it is\nnot feasible for an attacker to succeed with completely zero knowledge, we\nassume that the attacker only knows a short (or a few seconds) speech sample of\na target speaker. Without any probing to gain further knowledge about the\ntarget model, we propose a new mechanism, called parrot training, to generate\nAEs against the target model. Motivated by recent advancements in voice\nconversion (VC), we propose to use the one short sentence knowledge to generate\nmore synthetic speech samples that sound like the target speaker, called parrot\nspeech. Then, we use these parrot speech samples to train a parrot-trained(PT)\nsurrogate model for the attacker. Under a joint transferability and perception\nframework, we investigate different ways to generate AEs on the PT model\n(called PT-AEs) to ensure the PT-AEs can be generated with high transferability\nto a black-box target model with good human perceptual quality. Real-world\nexperiments show that the resultant PT-AEs achieve the attack success rates of\n45.8% - 80.8% against the open-source models in the digital-line scenario and\n47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon\nEcho, and Google Home, in the over-the-air scenario.", "published": "2023-11-13 22:12:19", "link": "http://arxiv.org/abs/2311.07780v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Research and experimental verification on low-frequency long-range\n  underwater sound propagation dispersion characteristics under dual-channel\n  sound speed profiles in the Chukchi Plateau", "abstract": "The dual-channel sound speed profiles of the Chukchi Plateau and the Canadian\nBasin have become current research hotspots due to their excellent\nlow-frequency sound signal propagation ability. Previous research has mainly\nfocused on using sound propagation theory to explain the changes in sound\nsignal energy. This article is mainly based on the theory of normal modes to\nstudy the fine structure of low-frequency wide-band sound propagation\ndispersion under dual-channel sound speed profiles. In this paper, the problem\nof the intersection of normal mode dispersion curves caused by the dual-channel\nsound speed profile (SSP) has been explained, the blocking effect of seabed\nterrain changes on dispersion structures has been analyzed, and the normal\nmodes has been separated by using modified warping operator. The above research\nresults have been verified through a long-range seismic exploration experiment\nat the Chukchi Plateau. At the same time, based on the acoustic signal\ncharacteristics in this environment, two methods for estimating the distance of\nsound sources have been proposed, and the experiment data at sea has also\nverified these two methods.", "published": "2023-11-13 09:21:34", "link": "http://arxiv.org/abs/2311.08425v1", "categories": ["cs.SD", "cs.NA", "eess.AS", "math.NA", "physics.ao-ph", "physics.app-ph"], "primary_category": "cs.SD"}
