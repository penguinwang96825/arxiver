{"title": "From Bilingual to Multilingual Neural Machine Translation by Incremental\n  Training", "abstract": "Multilingual Neural Machine Translation approaches are based on the use of\ntask-specific models and the addition of one more language can only be done by\nretraining the whole system. In this work, we propose a new training schedule\nthat allows the system to scale to more languages without modification of the\nprevious components based on joint training and language-independent\nencoder/decoder modules allowing for zero-shot translation. This work in\nprogress shows close results to the state-of-the-art in the WMT task.", "published": "2019-06-28 09:28:00", "link": "http://arxiv.org/abs/1907.00735v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Concise Model for Multi-Criteria Chinese Word Segmentation with\n  Transformer Encoder", "abstract": "Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the\nrelations among the multiple heterogeneous segmentation criteria and further\nimprove the performance of each single criterion. Previous work usually regards\nMCCWS as different tasks, which are learned together under the multi-task\nlearning framework. In this paper, we propose a concise but effective unified\nmodel for MCCWS, which is fully-shared for all the criteria. By leveraging the\npowerful ability of the Transformer encoder, the proposed unified model can\nsegment Chinese text according to a unique criterion-token indicating the\noutput criterion. Besides, the proposed unified model can segment both\nsimplified and traditional Chinese and has an excellent transfer capability.\nExperiments on eight datasets with different criteria show that our model\noutperforms our single-criterion baseline model and other multi-criteria\nmodels. Source codes of this paper are available on Github\nhttps://github.com/acphile/MCCWS.", "published": "2019-06-28 04:08:15", "link": "http://arxiv.org/abs/1906.12035v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Supervised Contextual Embeddings for Transfer Learning in Natural\n  Language Processing Tasks", "abstract": "Pre-trained word embeddings are the primary method for transfer learning in\nseveral Natural Language Processing (NLP) tasks. Recent works have focused on\nusing unsupervised techniques such as language modeling to obtain these\nembeddings. In contrast, this work focuses on extracting representations from\nmultiple pre-trained supervised models, which enriches word embeddings with\ntask and domain specific knowledge. Experiments performed in cross-task,\ncross-domain and cross-lingual settings indicate that such supervised\nembeddings are helpful, especially in the low-resource setting, but the extent\nof gains is dependent on the nature of the task and domain. We make our code\npublicly available.", "published": "2019-06-28 04:41:06", "link": "http://arxiv.org/abs/1906.12039v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lost in Translation: Loss and Decay of Linguistic Richness in Machine\n  Translation", "abstract": "This work presents an empirical approach to quantifying the loss of lexical\nrichness in Machine Translation (MT) systems compared to Human Translation\n(HT). Our experiments show how current MT systems indeed fail to render the\nlexical diversity of human generated or translated text. The inability of MT\nsystems to generate diverse outputs and its tendency to exacerbate already\nfrequent patterns while ignoring less frequent ones, might be the underlying\ncause for, among others, the currently heavily debated issues related to gender\nbiased output. Can we indeed, aside from biased data, talk about an algorithm\nthat exacerbates seen biases?", "published": "2019-06-28 07:31:33", "link": "http://arxiv.org/abs/1906.12068v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Widening the Representation Bottleneck in Neural Machine Translation\n  with Lexical Shortcuts", "abstract": "The transformer is a state-of-the-art neural translation model that uses\nattention to iteratively refine lexical representations with information drawn\nfrom the surrounding context. Lexical features are fed into the first layer and\npropagated through a deep network of hidden layers. We argue that the need to\nrepresent and propagate lexical features in each layer limits the model's\ncapacity for learning and representing other information relevant to the task.\nTo alleviate this bottleneck, we introduce gated shortcut connections between\nthe embedding layer and each subsequent layer within the encoder and decoder.\nThis enables the model to access relevant lexical content dynamically, without\nexpending limited resources on storing it within intermediate states. We show\nthat the proposed modification yields consistent improvements over a baseline\ntransformer on standard WMT translation tasks in 5 translation directions (0.9\nBLEU on average) and reduces the amount of lexical information passed along the\nhidden layers. We furthermore evaluate different ways to integrate lexical\nconnections into the transformer architecture and present ablation experiments\nexploring the effect of proposed shortcuts on model behavior.", "published": "2019-06-28 16:14:06", "link": "http://arxiv.org/abs/1906.12284v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Acoustic Cues and Paralinguistic Embeddings to Detect\n  Expression from Voice", "abstract": "Millions of people reach out to digital assistants such as Siri every day,\nasking for information, making phone calls, seeking assistance, and much more.\nThe expectation is that such assistants should understand the intent of the\nusers query. Detecting the intent of a query from a short, isolated utterance\nis a difficult task. Intent cannot always be obtained from speech-recognized\ntranscriptions. A transcription driven approach can interpret what has been\nsaid but fails to acknowledge how it has been said, and as a consequence, may\nignore the expression present in the voice. Our work investigates whether a\nsystem can reliably detect vocal expression in queries using acoustic and\nparalinguistic embedding. Results show that the proposed method offers a\nrelative equal error rate (EER) decrease of 60% compared to a bag-of-word based\nsystem, corroborating that expression is significantly represented by vocal\nattributes, rather than being purely lexical. Addition of emotion embedding\nhelped to reduce the EER by 30% relative to the acoustic embedding,\ndemonstrating the relevance of emotion in expressive voice.", "published": "2019-06-28 22:57:36", "link": "http://arxiv.org/abs/1907.00112v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Introducing MathQA -- A Math-Aware Question Answering System", "abstract": "We present an open source math-aware Question Answering System based on Ask\nPlatypus. Our system returns as a single mathematical formula for a natural\nlanguage question in English or Hindi. This formulae originate from the\nknowledge-base Wikidata. We translate these formulae to computable data by\nintegrating the calculation engine sympy into our system. This way, users can\nenter numeric values for the variables occurring in the formula. Moreover, the\nsystem loads numeric values for constants occurring in the formula from\nWikidata. In a user study, our system outperformed a commercial computational\nmathematical knowledge engine by 13%. However, the performance of our system\nheavily depends on the size and quality of the formula data available in\nWikidata. Since only a few items in Wikidata contained formulae when we started\nthe project, we facilitated the import process by suggesting formula edits to\nWikidata editors. With the simple heuristic that the first formula is\nsignificant for the article, 80% of the suggestions were correct.", "published": "2019-06-28 08:27:53", "link": "http://arxiv.org/abs/1907.01642v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "FIESTA: Fast IdEntification of State-of-The-Art models using adaptive\n  bandit algorithms", "abstract": "We present FIESTA, a model selection approach that significantly reduces the\ncomputational resources required to reliably identify state-of-the-art\nperformance from large collections of candidate models. Despite being known to\nproduce unreliable comparisons, it is still common practice to compare model\nevaluations based on single choices of random seeds. We show that reliable\nmodel selection also requires evaluations based on multiple train-test splits\n(contrary to common practice in many shared tasks). Using bandit theory from\nthe statistics literature, we are able to adaptively determine appropriate\nnumbers of data splits and random seeds used to evaluate each model, focusing\ncomputational resources on the evaluation of promising models whilst avoiding\nwasting evaluations on models with lower performance. Furthermore, our\nuser-friendly Python implementation produces confidence guarantees of correctly\nselecting the optimal model. We evaluate our algorithms by selecting between 8\ntarget-dependent sentiment analysis methods using dramatically fewer model\nevaluations than current model selection approaches.", "published": "2019-06-28 14:11:13", "link": "http://arxiv.org/abs/1906.12230v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "An Image Source Method Framework for Arbitrary Reflecting Boundaries", "abstract": "We propose a theoretical framework for the image source method that\ngeneralizes to arbitrary reflecting boundaries, e.g. boundaries that are curved\nor even with certain openings. Furthermore, it can seamlessly incorporate\nboundary absorption, source directivity, and nonspecular reflections. This\nframework is based on the notion of reflection paths that allows the\nintroduction of the concepts of validity and visibility of virtual sources.\nThese definitions facilitate the determination, for a given source and receiver\nlocation, of the distribution of virtual sources that explain the boundary\neffects of a wide range of reflecting surfaces. The structure of the set of\nvirtual sources is then more general than just punctual virtual sources. Due to\nthis more diverse configuration of image sources, we represent the room impulse\nresponse as an integral involving the temporal excitation signal against a\nmeasure determined by the source and receiver locations, and the original\nboundary. The latter smoothly enables, in an analytically tractable manner, the\nincorporation of more general boundary shapes as well as directivity of sources\nand boundary absorption while, at the same time, maintaining the conceptual\nbenefits of the image source method.", "published": "2019-06-28 13:58:02", "link": "http://arxiv.org/abs/1906.12227v1", "categories": ["eess.SP", "eess.AS", "28A78"], "primary_category": "eess.SP"}
{"title": "Lipper: Synthesizing Thy Speech using Multi-View Lipreading", "abstract": "Lipreading has a lot of potential applications such as in the domain of\nsurveillance and video conferencing. Despite this, most of the work in building\nlipreading systems has been limited to classifying silent videos into classes\nrepresenting text phrases. However, there are multiple problems associated with\nmaking lipreading a text-based classification task like its dependence on a\nparticular language and vocabulary mapping. Thus, in this paper we propose a\nmulti-view lipreading to audio system, namely Lipper, which models it as a\nregression task. The model takes silent videos as input and produces speech as\nthe output. With multi-view silent videos, we observe an improvement over\nsingle-view speech reconstruction results. We show this by presenting an\nexhaustive set of experiments for speaker-dependent, out-of-vocabulary and\nspeaker-independent settings. Further, we compare the delay values of Lipper\nwith other speechreading systems in order to show the real-time nature of audio\nproduced. We also perform a user study for the audios produced in order to\nunderstand the level of comprehensibility of audios produced using Lipper.", "published": "2019-06-28 10:26:23", "link": "http://arxiv.org/abs/1907.01367v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "RecurSIA-RRT: Recursive translatable point-set pattern discovery with\n  removal of redundant translators", "abstract": "We introduce two algorithms, RECURSIA and RRT, designed to increase the\ncompression factor achievable using point-set cover algorithms based on the SIA\nand SIATEC pattern discovery algorithms. SIA computes the maximal translatable\npatterns (MTPs) in a point set, while SIATEC computes the translational\nequivalence class (TEC) of every MTP in a point set, where the TEC of an MTP is\nthe set of translationally invariant occurrences of that MTP in the point set.\nIn its output, SIATEC encodes each MTP TEC as a pair, <P,V>, where P is the\nfirst occurrence of the MTP and V is the set of non-zero vectors that map P\nonto its other occurrences. RECURSIA recursively applies a TEC cover algorithm\nto the pattern P, in each TEC, <P,V>, that it discovers. RRT attempts to remove\ntranslators from V in each TEC without reducing the total set of points covered\nby the TEC. When evaluated with COSIATEC, SIATECCompress and Forth's algorithm\non the JKU Patterns Development Database, using RECURSIA with or without RRT\nincreased compression factor and recall but reduced precision. Using RRT alone\nincreased compression factor and reduced recall and precision, but had a\nsmaller effect than RECURSIA.", "published": "2019-06-28 16:18:34", "link": "http://arxiv.org/abs/1906.12286v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
