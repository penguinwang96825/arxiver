{"title": "Hedging in Jump Diffusion Model with Transaction Costs", "abstract": "We consider the jump-diffusion risky asset model and study its conditional\nprediction laws. Next, we explain the conditional least square hedging strategy\nand calculate its closed form for the jump-diffusion model, considering the\nBlack-Scholes framework with interpretations related to investor priorities and\ntransaction costs. We investigate the explicit form of this result for the\nparticular case of the European call option under transaction costs and\nformulate recursive hedging strategies. Finally, we present a decision tree,\ntable of values, and figures to support our results.", "published": "2024-08-20 12:23:04", "link": "http://arxiv.org/abs/2408.10785v1", "categories": ["q-fin.MF", "math.PR", "q-fin.PM", "91G10, 91G20, 91G80, 60G51, 60J60, 60J65, 60J70, 60J76"], "primary_category": "q-fin.MF"}
{"title": "NoMatterXAI: Generating \"No Matter What\" Alterfactual Examples for\n  Explaining Black-Box Text Classification Models", "abstract": "In Explainable AI (XAI), counterfactual explanations (CEs) are a well-studied\nmethod to communicate feature relevance through contrastive reasoning of \"what\nif\" to explain AI models' predictions. However, they only focus on important\n(i.e., relevant) features and largely disregard less important (i.e.,\nirrelevant) ones. Such irrelevant features can be crucial in many applications,\nespecially when users need to ensure that an AI model's decisions are not\naffected or biased against specific attributes such as gender, race, religion,\nor political affiliation. To address this gap, the concept of alterfactual\nexplanations (AEs) has been proposed. AEs explore an alternative reality of \"no\nmatter what\", where irrelevant features are substituted with alternative\nfeatures (e.g., \"republicans\" -> \"democrats\") within the same attribute (e.g.,\n\"politics\") while maintaining a similar prediction output. This serves to\nvalidate whether AI model predictions are influenced by the specified\nattributes. Despite the promise of AEs, there is a lack of computational\napproaches to systematically generate them, particularly in the text domain,\nwhere creating AEs for AI text classifiers presents unique challenges. This\npaper addresses this challenge by formulating AE generation as an optimization\nproblem and introducing MoMatterXAI, a novel algorithm that generates AEs for\ntext classification tasks. Our approach achieves high fidelity of up to 95%\nwhile preserving context similarity of over 90% across multiple models and\ndatasets. A human study further validates the effectiveness of AEs in\nexplaining AI text classifiers to end users. All codes will be publicly\navailable.", "published": "2024-08-20 04:06:21", "link": "http://arxiv.org/abs/2408.10528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modeling on Tabular Data: A Survey of Foundations, Techniques\n  and Evolution", "abstract": "Tabular data, a prevalent data type across various domains, presents unique\nchallenges due to its heterogeneous nature and complex structural\nrelationships. Achieving high predictive performance and robustness in tabular\ndata analysis holds significant promise for numerous applications. Influenced\nby recent advancements in natural language processing, particularly transformer\narchitectures, new methods for tabular data modeling have emerged. Early\ntechniques concentrated on pre-training transformers from scratch, often\nencountering scalability issues. Subsequently, methods leveraging pre-trained\nlanguage models like BERT have been developed, which require less data and\nyield enhanced performance. The recent advent of large language models, such as\nGPT and LLaMA, has further revolutionized the field, facilitating more advanced\nand diverse applications with minimal fine-tuning. Despite the growing\ninterest, a comprehensive survey of language modeling techniques for tabular\ndata remains absent. This paper fills this gap by providing a systematic review\nof the development of language modeling for tabular data, encompassing: (1) a\ncategorization of different tabular data structures and data types; (2) a\nreview of key datasets used in model training and tasks used for evaluation;\n(3) a summary of modeling techniques including widely-adopted data processing\nmethods, popular architectures, and training objectives; (4) the evolution from\nadapting traditional Pre-training/Pre-trained language models to the\nutilization of large language models; (5) an identification of persistent\nchallenges and potential future research directions in language modeling for\ntabular data analysis. GitHub page associated with this survey is available at:\nhttps://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.", "published": "2024-08-20 04:59:19", "link": "http://arxiv.org/abs/2408.10548v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Representation Learning Revisited: The Necessity of Separate\n  Learnable Parameters and Robust Data Augmentation", "abstract": "Speech modeling methods learn one embedding for a fixed segment of speech,\ntypically in between 10-25 ms. The information present in speech can be divided\ninto two categories: \"what is being said\" (content) and \"how it is expressed\"\n(other) and these two are orthogonal in nature causing the optimization\nalgorithm to find a sub-optimal solution if forced to optimize together. This\nleads to sub-optimal performance in one or all downstream tasks as shown by\nprevious studies. Current self-supervised learning (SSL) methods such as HuBERT\nare very good at modeling the content information present in speech. Data\naugmentation improves the performance on tasks which require effective modeling\nof other information but this leads to a divided capacity of the model. In this\nwork, we conduct a preliminary study to understand the importance of modeling\nother information using separate learnable parameters. We propose a modified\nversion of HuBERT, termed Other HuBERT (O-HuBERT), to test our hypothesis. Our\nfindings are twofold: first, the O-HuBERT method is able to utilize all layers\nto build complex features to encode other information; second, a robust data\naugmentation strategy is essential for learning the information required by\ntasks that depend on other information and to achieve state-of-the-art (SOTA)\nperformance on the SUPERB benchmark with a similarly sized model (100 million\nparameters) and pre-training data (960 hours).", "published": "2024-08-20 05:45:04", "link": "http://arxiv.org/abs/2408.10557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Robustness in Large Language Models: Prompting for Mitigating\n  the Impact of Irrelevant Information", "abstract": "In recent years, Large language models (LLMs) have garnered significant\nattention due to their superior performance in complex reasoning tasks.\nHowever, recent studies may diminish their reasoning capabilities markedly when\nproblem descriptions contain irrelevant information, even with the use of\nadvanced prompting techniques. To further investigate this issue, a dataset of\nprimary school mathematics problems containing irrelevant information, named\nGSMIR, was constructed. Testing prominent LLMs and prompting techniques on this\ndataset revealed that while LLMs can identify irrelevant information, they do\nnot effectively mitigate the interference it causes once identified. A novel\nautomatic construction method, ATF, which enhances the ability of LLMs to\nidentify and self-mitigate the influence of irrelevant information, is proposed\nto address this shortcoming. This method operates in two steps: first, analysis\nof irrelevant information, followed by its filtering. The ATF method, as\ndemonstrated by experimental results, significantly improves the reasoning\nperformance of LLMs and prompting techniques, even in the presence of\nirrelevant information on the GSMIR dataset.", "published": "2024-08-20 07:49:38", "link": "http://arxiv.org/abs/2408.10615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REInstruct: Building Instruction Data from Unlabeled Corpus", "abstract": "Manually annotating instruction data for large language models is difficult,\ncostly, and hard to scale. Meanwhile, current automatic annotation methods\ntypically rely on distilling synthetic data from proprietary LLMs, which not\nonly limits the upper bound of the quality of the instruction data but also\nraises potential copyright issues. In this paper, we propose REInstruct, a\nsimple and scalable method to automatically build instruction data from an\nunlabeled corpus without heavy reliance on proprietary LLMs and human\nannotation. Specifically, REInstruct first selects a subset of unlabeled texts\nthat potentially contain well-structured helpful and insightful content and\nthen generates instructions for these texts. To generate accurate and relevant\nresponses for effective and robust training, REInstruct further proposes a\nrewriting-based approach to improve the quality of the generated instruction\ndata. By training Llama-7b on a combination of 3k seed data and 32k synthetic\ndata from REInstruct, fine-tuned model achieves a 65.41\\% win rate on\nAlpacaEval leaderboard against text-davinci-003, outperforming other\nopen-source, non-distilled instruction data construction methods. The code is\npublicly available at \\url{https://github.com/cs32963/REInstruct}.", "published": "2024-08-20 09:05:03", "link": "http://arxiv.org/abs/2408.10663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unconditional Truthfulness: Learning Conditional Dependency for\n  Uncertainty Quantification of Large Language Models", "abstract": "Uncertainty quantification (UQ) is a perspective approach to detecting Large\nLanguage Model (LLM) hallucinations and low quality output. In this work, we\naddress one of the challenges of UQ in generation tasks that arises from the\nconditional dependency between the generation steps of an LLM. We propose to\nlearn this dependency from data. We train a regression model, which target\nvariable is the gap between the conditional and the unconditional generation\nconfidence. During LLM inference, we use this learned conditional dependency\nmodel to modulate the uncertainty of the current generation step based on the\nuncertainty of the previous step. Our experimental evaluation on nine datasets\nand three LLMs shows that the proposed method is highly effective for\nuncertainty quantification, achieving substantial improvements over rivaling\napproaches.", "published": "2024-08-20 09:42:26", "link": "http://arxiv.org/abs/2408.10692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ferret: Faster and Effective Automated Red Teaming with Reward-Based\n  Scoring Technique", "abstract": "In today's era, where large language models (LLMs) are integrated into\nnumerous real-world applications, ensuring their safety and robustness is\ncrucial for responsible AI usage. Automated red-teaming methods play a key role\nin this process by generating adversarial attacks to identify and mitigate\npotential vulnerabilities in these models. However, existing methods often\nstruggle with slow performance, limited categorical diversity, and high\nresource demands. While Rainbow Teaming, a recent approach, addresses the\ndiversity challenge by framing adversarial prompt generation as a\nquality-diversity search, it remains slow and requires a large fine-tuned\nmutator for optimal performance. To overcome these limitations, we propose\nFerret, a novel approach that builds upon Rainbow Teaming by generating\nmultiple adversarial prompt mutations per iteration and using a scoring\nfunction to rank and select the most effective adversarial prompt. We explore\nvarious scoring functions, including reward models, Llama Guard, and\nLLM-as-a-judge, to rank adversarial mutations based on their potential harm to\nimprove the efficiency of the search for harmful mutations. Our results\ndemonstrate that Ferret, utilizing a reward model as a scoring function,\nimproves the overall attack success rate (ASR) to 95%, which is 46% higher than\nRainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90%\nASR by 15.2% compared to the baseline and generates adversarial prompts that\nare transferable i.e. effective on other LLMs of larger size. Our codes are\navailable at https://github.com/declare-lab/ferret.", "published": "2024-08-20 09:58:01", "link": "http://arxiv.org/abs/2408.10701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crafting Tomorrow's Headlines: Neural News Generation and Detection in\n  English, Turkish, Hungarian, and Persian", "abstract": "In the era dominated by information overload and its facilitation with Large\nLanguage Models (LLMs), the prevalence of misinformation poses a significant\nthreat to public discourse and societal well-being. A critical concern at\npresent involves the identification of machine-generated news. In this work, we\ntake a significant step by introducing a benchmark dataset designed for neural\nnews detection in four languages: English, Turkish, Hungarian, and Persian. The\ndataset incorporates outputs from multiple multilingual generators (in both,\nzero-shot and fine-tuned setups) such as BloomZ, LLaMa-2, Mistral, Mixtral, and\nGPT-4. Next, we experiment with a variety of classifiers, ranging from those\nbased on linguistic features to advanced Transformer-based models and LLMs\nprompting. We present the detection results aiming to delve into the\ninterpretablity and robustness of machine-generated texts detectors across all\ntarget languages.", "published": "2024-08-20 10:45:36", "link": "http://arxiv.org/abs/2408.10724v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion\n  for Efficient Inference Intervention in Large Language Model", "abstract": "Transformer-based large language models (LLMs) exhibit limitations such as\ngenerating unsafe responses, unreliable reasoning, etc. Existing inference\nintervention approaches attempt to mitigate these issues by finetuning\nadditional models to produce calibration signals (such as rewards) that guide\nthe LLM's decoding process. However, this solution introduces substantial time\nand space overhead due to the separate models required. This work proposes\nNon-disruptive parameters insertion (Otter), inserting extra parameters into\nthe transformer architecture to predict calibration signals along with the\noriginal LLM output. Otter offers state-of-the-art performance on multiple\ndemanding tasks while saving up to 86.5\\% extra space and 98.5\\% extra time.\nFurthermore, Otter seamlessly integrates with existing inference engines,\nrequiring only a one-line code change, and the original model response remains\naccessible after the parameter insertion. Our code is publicly available at\n\\url{https://github.com/chenhan97/Otter}", "published": "2024-08-20 12:00:35", "link": "http://arxiv.org/abs/2408.10764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Attack for Explanation Robustness of Rationalization Models", "abstract": "Rationalization models, which select a subset of input text as\nrationale-crucial for humans to understand and trust predictions-have recently\nemerged as a prominent research area in eXplainable Artificial Intelligence.\nHowever, most of previous studies mainly focus on improving the quality of the\nrationale, ignoring its robustness to malicious attack. Specifically, whether\nthe rationalization models can still generate high-quality rationale under the\nadversarial attack remains unknown. To explore this, this paper proposes UAT2E,\nwhich aims to undermine the explainability of rationalization models without\naltering their predictions, thereby eliciting distrust in these models from\nhuman users. UAT2E employs the gradient-based search on triggers and then\ninserts them into the original input to conduct both the non-target and target\nattack. Experimental results on five datasets reveal the vulnerability of\nrationalization models in terms of explanation, where they tend to select more\nmeaningless tokens under attacks. Based on this, we make a series of\nrecommendations for improving rationalization models in terms of explanation.", "published": "2024-08-20 12:43:58", "link": "http://arxiv.org/abs/2408.10795v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs", "abstract": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.", "published": "2024-08-20 14:45:23", "link": "http://arxiv.org/abs/2408.10902v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Code, or Not To Code? Exploring Impact of Code in Pre-training", "abstract": "Including code in the pre-training data mixture, even for models not\nspecifically designed for code, has become a common practice in LLMs\npre-training. While there has been anecdotal consensus among practitioners that\ncode data plays a vital role in general LLMs' performance, there is only\nlimited work analyzing the precise impact of code on non-code tasks. In this\nwork, we systematically investigate the impact of code data on general\nperformance. We ask \"what is the impact of code data used in pre-training on a\nlarge variety of downstream tasks beyond code generation\". We conduct extensive\nablations and evaluate across a broad range of natural language reasoning\ntasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for\nmodels with sizes ranging from 470M to 2.8B parameters. Across settings, we\nfind a consistent results that code is a critical building block for\ngeneralization far beyond coding tasks and improvements to code quality have an\noutsized impact across all tasks. In particular, compared to text-only\npre-training, the addition of code results in up to relative increase of 8.2%\nin natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement\nin generative win-rates, and a 12x boost in code performance respectively. Our\nwork suggests investments in code quality and preserving code during\npre-training have positive impacts.", "published": "2024-08-20 14:58:13", "link": "http://arxiv.org/abs/2408.10914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHECKWHY: Causal Fact Verification via Argument Structure", "abstract": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements.", "published": "2024-08-20 15:03:35", "link": "http://arxiv.org/abs/2408.10918v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SysBench: Can Large Language Models Follow System Messages?", "abstract": "Large Language Models (LLMs) have become instrumental across various\napplications, with the customization of these models to specific scenarios\nbecoming increasingly critical. System message, a fundamental component of\nLLMs, is consist of carefully crafted instructions that guide the behavior of\nmodel to meet intended goals. Despite the recognized potential of system\nmessages to optimize AI-driven solutions, there is a notable absence of a\ncomprehensive benchmark for evaluating how well LLMs follow system messages. To\nfill this gap, we introduce SysBench, a benchmark that systematically analyzes\nsystem message following ability in terms of three limitations of existing\nLLMs: constraint violation, instruction misjudgement and multi-turn\ninstability. Specifically, we manually construct evaluation dataset based on\nsix prevalent types of constraints, including 500 tailor-designed system\nmessages and multi-turn user conversations covering various interaction\nrelationships. Additionally, we develop a comprehensive evaluation protocol to\nmeasure model performance. Finally, we conduct extensive evaluation across\nvarious existing LLMs, measuring their ability to follow specified constraints\ngiven in system messages. The results highlight both the strengths and\nweaknesses of existing models, offering key insights and directions for future\nresearch. The open source library SysBench is available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/SysBench.", "published": "2024-08-20 15:33:16", "link": "http://arxiv.org/abs/2408.10943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP for The Greek Language: A Longer Survey", "abstract": "English language is in the spotlight of the Natural Language Processing (NLP)\ncommunity with other languages, like Greek, lagging behind in terms of offered\nmethods, tools and resources. Due to the increasing interest in NLP, in this\npaper we try to condense research efforts for the automatic processing of Greek\nlanguage covering the last three decades. In particular, we list and briefly\ndiscuss related works, resources and tools, categorized according to various\nprocessing layers and contexts. We are not restricted to the modern form of\nGreek language but also cover Ancient Greek and various Greek dialects. This\nsurvey can be useful for researchers and students interested in NLP tasks,\nInformation Retrieval and Knowledge Management for the Greek language.", "published": "2024-08-20 15:57:18", "link": "http://arxiv.org/abs/2408.10962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The fusion of phonography and ideographic characters into virtual\n  Chinese characters -- Based on Chinese and English", "abstract": "The characters used in modern countries are mainly divided into ideographic\ncharacters and phonetic characters, both of which have their advantages and\ndisadvantages. Chinese is difficult to learn and easy to master, while English\nis easy to learn but has a large vocabulary. There is still no language that\ncombines the advantages of both languages and has less memory capacity, can\nform words, and is easy to learn. Therefore, inventing new characters that can\nbe combined and the popularization of deep knowledge, and reduce disputes\nthrough communication. Firstly, observe the advantages and disadvantages of\nChinese and English, such as their vocabulary, information content, and ease of\nlearning in deep scientific knowledge, and create a new writing system. Then,\nuse comparative analysis to observe the total score of the new language.\nThrough this article, it can be concluded that the new text combines the\nadvantages of both pictographic and alphabetical writing: new characters that\ncan be combined into words reduces the vocabulary that needs to be learned;\nSpecial prefixes allow beginners to quickly guess the approximate category and\nmeaning of unseen words; New characters can enable humans to quickly learn more\nadvanced knowledge.", "published": "2024-08-20 16:13:28", "link": "http://arxiv.org/abs/2408.10979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language\n  Models", "abstract": "New medical treatment development requires multiple phases of clinical\ntrials. Despite the significant human and financial costs of bringing a drug to\nmarket, less than 20% of drugs in testing will make it from the first phase to\nfinal approval. Recent literature indicates that the design of the trial\nprotocols significantly contributes to trial performance. We investigated\nClinical Trial Outcome Prediction (CTOP) using trial design documents to\npredict phase transitions automatically. We propose CTP-LLM, the first Large\nLanguage Model (LLM) based model for CTOP. We also introduce the\nPhaseTransition (PT) Dataset; which labels trials based on their progression\nthrough the regulatory process and serves as a benchmark for CTOP evaluation.\nOur fine-tuned GPT-3.5-based model (CTP-LLM) predicts clinical trial phase\ntransition by analyzing the trial's original protocol texts without requiring\nhuman-selected features. CTP-LLM achieves a 67% accuracy rate in predicting\ntrial phase transitions across all phases and a 75% accuracy rate specifically\nin predicting the transition from Phase~III to final approval. Our experimental\nperformance highlights the potential of LLM-powered applications in forecasting\nclinical trial outcomes and assessing trial design.", "published": "2024-08-20 16:43:05", "link": "http://arxiv.org/abs/2408.10995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inside the Black Box: Detecting Data Leakage in Pre-trained Language\n  Encoders", "abstract": "Despite being prevalent in the general field of Natural Language Processing\n(NLP), pre-trained language models inherently carry privacy and copyright\nconcerns due to their nature of training on large-scale web-scraped data. In\nthis paper, we pioneer a systematic exploration of such risks associated with\npre-trained language encoders, specifically focusing on the membership leakage\nof pre-training data exposed through downstream models adapted from pre-trained\nlanguage encoders-an aspect largely overlooked in existing literature. Our\nstudy encompasses comprehensive experiments across four types of pre-trained\nencoder architectures, three representative downstream tasks, and five\nbenchmark datasets. Intriguingly, our evaluations reveal, for the first time,\nthe existence of membership leakage even when only the black-box output of the\ndownstream model is exposed, highlighting a privacy risk far greater than\npreviously assumed. Alongside, we present in-depth analysis and insights toward\nguiding future researchers and practitioners in addressing the privacy\nconsiderations in developing pre-trained language models.", "published": "2024-08-20 17:55:15", "link": "http://arxiv.org/abs/2408.11046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding", "abstract": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.", "published": "2024-08-20 17:57:31", "link": "http://arxiv.org/abs/2408.11049v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unboxing Occupational Bias: Grounded Debiasing of LLMs with U.S. Labor\n  Data", "abstract": "Large Language Models (LLMs) are prone to inheriting and amplifying societal\nbiases embedded within their training data, potentially reinforcing harmful\nstereotypes related to gender, occupation, and other sensitive categories. This\nissue becomes particularly problematic as biased LLMs can have far-reaching\nconsequences, leading to unfair practices and exacerbating social inequalities\nacross various domains, such as recruitment, online content moderation, or even\nthe criminal justice system. Although prior research has focused on detecting\nbias in LLMs using specialized datasets designed to highlight intrinsic biases,\nthere has been a notable lack of investigation into how these findings\ncorrelate with authoritative datasets, such as those from the U.S. National\nBureau of Labor Statistics (NBLS). To address this gap, we conduct empirical\nresearch that evaluates LLMs in a ``bias-out-of-the-box\" setting, analyzing how\nthe generated outputs compare with the distributions found in NBLS data.\nFurthermore, we propose a straightforward yet effective debiasing mechanism\nthat directly incorporates NBLS instances to mitigate bias within LLMs. Our\nstudy spans seven different LLMs, including instructable, base, and\nmixture-of-expert models, and reveals significant levels of bias that are often\noverlooked by existing bias detection techniques. Importantly, our debiasing\nmethod, which does not rely on external datasets, demonstrates a substantial\nreduction in bias scores, highlighting the efficacy of our approach in creating\nfairer and more reliable LLMs.", "published": "2024-08-20 23:54:26", "link": "http://arxiv.org/abs/2408.11247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing One-shot Pruned Pre-trained Language Models through\n  Sparse-Dense-Sparse Mechanism", "abstract": "Pre-trained language models (PLMs) are engineered to be robust in contextual\nunderstanding and exhibit outstanding performance in various natural language\nprocessing tasks. However, their considerable size incurs significant\ncomputational and storage costs. Modern pruning strategies employ one-shot\ntechniques to compress PLMs without the need for retraining on task-specific or\notherwise general data; however, these approaches often lead to an\nindispensable reduction in performance. In this paper, we propose SDS, a\nSparse-Dense-Sparse pruning framework to enhance the performance of the pruned\nPLMs from a weight distribution optimization perspective. We outline the\npruning process in three steps. Initially, we prune less critical connections\nin the model using conventional one-shot pruning methods. Next, we reconstruct\na dense model featuring a pruning-friendly weight distribution by reactivating\npruned connections with sparse regularization. Finally, we perform a second\npruning round, yielding a superior pruned model compared to the initial\npruning. Experimental results demonstrate that SDS outperforms the\nstate-of-the-art pruning techniques SparseGPT and Wanda under an identical\nsparsity configuration. For instance, SDS reduces perplexity by 9.13 on\nRaw-Wikitext2 and improves accuracy by an average of 2.05% across multiple\nzero-shot benchmarks for OPT-125M with 2:4 sparsity.", "published": "2024-08-20 01:05:45", "link": "http://arxiv.org/abs/2408.10473v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analysis of Plan-based Retrieval for Grounded Text Generation", "abstract": "In text generation, hallucinations refer to the generation of seemingly\ncoherent text that contradicts established knowledge. One compelling hypothesis\nis that hallucinations occur when a language model is given a generation task\noutside its parametric knowledge (due to rarity, recency, domain, etc.). A\ncommon strategy to address this limitation is to infuse the language models\nwith retrieval mechanisms, providing the model with relevant knowledge for the\ntask. In this paper, we leverage the planning capabilities of instruction-tuned\nLLMs and analyze how planning can be used to guide retrieval to further reduce\nthe frequency of hallucinations. We empirically evaluate several variations of\nour proposed approach on long-form text generation tasks. By improving the\ncoverage of relevant facts, plan-guided retrieval and generation can produce\nmore informative responses while providing a higher rate of attribution to\nsource documents.", "published": "2024-08-20 02:19:35", "link": "http://arxiv.org/abs/2408.10490v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "QUITO-X: A New Perspective on Context Compression from the Information\n  Bottleneck Theory", "abstract": "Generative LLM have achieved remarkable success in various industrial\napplications, owing to their promising In-Context Learning capabilities.\nHowever, the issue of long context in complex tasks poses a significant barrier\nto their wider adoption, manifested in two main aspects: (i) The excessively\nlong context leads to high costs and inference delays. (ii) A substantial\namount of task-irrelevant information introduced by long contexts exacerbates\nthe \"lost in the middle\" problem. Existing methods compress context by removing\nredundant tokens using metrics such as self-information or PPL, which is\ninconsistent with the objective of retaining the most important tokens when\nconditioning on a given query. In this study, we introduce information\nbottleneck theory (IB) to model the problem, offering a novel perspective that\nthoroughly addresses the essential properties required for context compression.\nAdditionally, we propose a cross-attention-based approach to approximate mutual\ninformation in IB, which can be flexibly replaced with suitable alternatives in\ndifferent scenarios. Extensive experiments on four datasets demonstrate that\nour method achieves a 25% increase in compression rate compared to the\nstate-of-the-art, while maintaining question answering performance. In\nparticular, the context compressed by our method even outperform the full\ncontext in some cases.", "published": "2024-08-20 02:44:45", "link": "http://arxiv.org/abs/2408.10497v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation Integrating Dialogue Flow and Style to Adapt Spoken\n  Dialogue Systems to Low-Resource User Groups", "abstract": "This study addresses the interaction challenges encountered by spoken\ndialogue systems (SDSs) when engaging with users who exhibit distinct\nconversational behaviors, particularly minors, in scenarios where data are\nscarce. We propose a novel data augmentation framework to enhance SDS\nperformance for user groups with limited resources. Our approach leverages a\nlarge language model (LLM) to extract speaker styles and a pre-trained language\nmodel (PLM) to simulate dialogue act history. This method generates enriched\nand personalized dialogue data, facilitating improved interactions with unique\nuser demographics. Extensive experiments validate the efficacy of our\nmethodology, highlighting its potential to foster the development of more\nadaptive and inclusive dialogue systems.", "published": "2024-08-20 03:33:04", "link": "http://arxiv.org/abs/2408.10516v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synergistic Approach for Simultaneous Optimization of Monolingual,\n  Cross-lingual, and Multilingual Information Retrieval", "abstract": "Information retrieval across different languages is an increasingly important\nchallenge in natural language processing. Recent approaches based on\nmultilingual pre-trained language models have achieved remarkable success, yet\nthey often optimize for either monolingual, cross-lingual, or multilingual\nretrieval performance at the expense of others. This paper proposes a novel\nhybrid batch training strategy to simultaneously improve zero-shot retrieval\nperformance across monolingual, cross-lingual, and multilingual settings while\nmitigating language bias. The approach fine-tunes multilingual language models\nusing a mix of monolingual and cross-lingual question-answer pair batches\nsampled based on dataset size. Experiments on XQuAD-R, MLQA-R, and MIRACL\nbenchmark datasets show that the proposed method consistently achieves\ncomparable or superior results in zero-shot retrieval across various languages\nand retrieval tasks compared to monolingual-only or cross-lingual-only\ntraining. Hybrid batch training also substantially reduces language bias in\nmultilingual retrieval compared to monolingual training. These results\ndemonstrate the effectiveness of the proposed approach for learning\nlanguage-agnostic representations that enable strong zero-shot retrieval\nperformance across diverse languages.", "published": "2024-08-20 04:30:26", "link": "http://arxiv.org/abs/2408.10536v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Putting People in LLMs' Shoes: Generating Better Answers via Question\n  Rewriter", "abstract": "Large Language Models (LLMs) have demonstrated significant capabilities,\nparticularly in the domain of question answering (QA). However, their\neffectiveness in QA is often undermined by the vagueness of user questions. To\naddress this issue, we introduce single-round instance-level prompt\noptimization, referred to as question rewriter. By enhancing the\nintelligibility of human questions for black-box LLMs, our question rewriter\nimproves the quality of generated answers. The rewriter is optimized using\ndirect preference optimization based on feedback collected from automatic\ncriteria for evaluating generated answers; therefore, its training does not\nrequire costly human annotations. The experiments across multiple black-box\nLLMs and long-form question answering (LFQA) datasets demonstrate the efficacy\nof our method. This paper provides a practical framework for training question\nrewriters and sets a precedent for future explorations in prompt optimization\nwithin LFQA tasks. Code is available at\nhttps://github.com/3244we/Question-Rewriter.", "published": "2024-08-20 06:24:47", "link": "http://arxiv.org/abs/2408.10573v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Efficient Sign Language Translation Using Spatial Configuration and\n  Motion Dynamics with LLMs", "abstract": "Gloss-free Sign Language Translation (SLT) converts sign videos directly into\nspoken language sentences without relying on glosses. Recently, Large Language\nModels (LLMs) have shown remarkable translation performance in gloss-free\nmethods by harnessing their powerful natural language generation capabilities.\nHowever, these methods often rely on domain-specific fine-tuning of visual\nencoders to achieve optimal results. By contrast, this paper emphasizes the\nimportance of capturing the spatial configurations and motion dynamics inherent\nin sign language. With this in mind, we introduce Spatial and Motion-based Sign\nLanguage Translation (SpaMo), a novel LLM-based SLT framework. The core idea of\nSpaMo is simple yet effective. We first extract spatial and motion features\nusing off-the-shelf visual encoders and then input these features into an LLM\nwith a language prompt. Additionally, we employ a visual-text alignment process\nas a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo\nachieves state-of-the-art performance on two popular datasets, PHOENIX14T and\nHow2Sign.", "published": "2024-08-20 07:10:40", "link": "http://arxiv.org/abs/2408.10593v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Promoting Equality in Large Language Models: Identifying and Mitigating\n  the Implicit Bias based on Bayesian Theory", "abstract": "Large language models (LLMs) are trained on extensive text corpora, which\ninevitably include biased information. Although techniques such as Affective\nAlignment can mitigate some negative impacts of these biases, existing\nprompt-based attack methods can still extract these biases from the model's\nweights. Moreover, these biases frequently appear subtly when LLMs are prompted\nto perform identical tasks across different demographic groups, thereby\ncamouflaging their presence. To address this issue, we have formally defined\nthe implicit bias problem and developed an innovative framework for bias\nremoval based on Bayesian theory, Bayesian-Theory based Bias Removal (BTBR).\nBTBR employs likelihood ratio screening to pinpoint data entries within\npublicly accessible biased datasets that represent biases inadvertently\nincorporated during the LLM training phase. It then automatically constructs\nrelevant knowledge triples and expunges bias information from LLMs using model\nediting techniques. Through extensive experimentation, we have confirmed the\npresence of the implicit bias problem in LLMs and demonstrated the\neffectiveness of our BTBR approach.", "published": "2024-08-20 07:40:12", "link": "http://arxiv.org/abs/2408.10608v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search", "abstract": "In this paper, we propose a new method STRATEGIST that utilizes LLMs to\nacquire new skills for playing multi-agent games through a self-improvement\nprocess. Our method gathers quality feedback through self-play simulations with\nMonte Carlo tree search and LLM-based reflection, which can then be used to\nlearn high-level strategic skills such as how to evaluate states that guide the\nlow-level execution. We showcase how our method can be used in both action\nplanning and dialogue generation in the context of games, achieving good\nperformance on both tasks. Specifically, we demonstrate that our method can\nhelp train agents with better performance than both traditional reinforcement\nlearning-based approaches and other LLM-based skill learning approaches in\ngames including the Game of Pure Strategy (GOPS) and The Resistance: Avalon.\nSTRATEGIST helps bridge the gap between foundation models and symbolic\ndecision-making methods through its bi-level approach, leading to more robust\ndecision-making.", "published": "2024-08-20 08:22:04", "link": "http://arxiv.org/abs/2408.10635v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Minor SFT loss for LLM fine-tune to increase performance and reduce\n  model deviation", "abstract": "Instruct LLM provide a paradigm used in large scale language model to align\nLLM to human preference. The paradigm contains supervised fine tuning and\nreinforce learning from human feedback. This paradigm is also used in\ndownstream scenarios to adapt LLM to specific corpora and applications.\nComparing to SFT, there are many efforts focused on RLHF and several algorithms\nbeing proposed, such as PPO, DPO, IPO, KTO, MinorDPO and etc. Meanwhile most\nefforts for SFT are focused on how to collect, filter and mix high quality\ndata. In this article with insight from DPO and MinorDPO, we propose a training\nmetric for SFT to measure the discrepancy between the optimized model and the\noriginal model, and a loss function MinorSFT that can increase the training\neffectiveness, and reduce the discrepancy between the optimized LLM and\noriginal LLM.", "published": "2024-08-20 08:32:44", "link": "http://arxiv.org/abs/2408.10642v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beneath the Surface of Consistency: Exploring Cross-lingual Knowledge\n  Representation Sharing in LLMs", "abstract": "The veracity of a factoid is largely independent of the language it is\nwritten in. However, language models are inconsistent in their ability to\nanswer the same factual question across languages. This raises questions about\nhow LLMs represent a given fact across languages. We explore multilingual\nfactual knowledge through two aspects: the model's ability to answer a query\nconsistently across languages, and the ability to ''store'' answers in a shared\nrepresentation for several languages. We propose a methodology to measure the\nextent of representation sharing across languages by repurposing knowledge\nediting methods. We examine LLMs with various multilingual configurations using\na new multilingual dataset. We reveal that high consistency does not\nnecessarily imply shared representation, particularly for languages with\ndifferent scripts. Moreover, we find that script similarity is a dominant\nfactor in representation sharing. Finally, we observe that if LLMs could fully\nshare knowledge across languages, their accuracy in their best-performing\nlanguage could benefit an increase of up to 150\\% on average. These findings\nhighlight the need for improved multilingual knowledge representation in LLMs\nand suggest a path for the development of more robust and consistent\nmultilingual LLMs.", "published": "2024-08-20 08:38:30", "link": "http://arxiv.org/abs/2408.10646v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HMoE: Heterogeneous Mixture of Experts for Language Modeling", "abstract": "Mixture of Experts (MoE) offers remarkable performance and computational\nefficiency by selectively activating subsets of model parameters.\nTraditionally, MoE models use homogeneous experts, each with identical\ncapacity. However, varying complexity in input data necessitates experts with\ndiverse capabilities, while homogeneous MoE hinders effective expert\nspecialization and efficient parameter utilization. In this study, we propose a\nnovel Heterogeneous Mixture of Experts (HMoE), where experts differ in size and\nthus possess diverse capacities. This heterogeneity allows for more specialized\nexperts to handle varying token complexities more effectively. To address the\nimbalance in expert activation, we propose a novel training objective that\nencourages the frequent activation of smaller experts, enhancing computational\nefficiency and parameter utilization. Extensive experiments demonstrate that\nHMoE achieves lower loss with fewer activated parameters and outperforms\nconventional homogeneous MoE models on various pre-training evaluation\nbenchmarks. Codes will be released upon acceptance.", "published": "2024-08-20 09:35:24", "link": "http://arxiv.org/abs/2408.10681v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code\n  Understanding?", "abstract": "Recent advancements in large language models (LLMs) have showcased impressive\ncode generation capabilities, primarily evaluated through language-to-code\nbenchmarks. However, these benchmarks may not fully capture a model's code\nunderstanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel\nbenchmark designed to assess LLMs' code understanding abilities from the\nperspective of code judging rather than code generation. CJ-Eval challenges\nmodels to determine the correctness of provided code solutions, encompassing\nvarious error types and compilation issues. By leveraging a diverse set of\nproblems and a fine-grained judging system, CJ-Eval addresses the limitations\nof traditional benchmarks, including the potential memorization of solutions.\nEvaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art\nmodels struggle, highlighting the benchmark's ability to probe deeper into\nmodels' code understanding abilities. Our codes and benchmark are available at\n\\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}.", "published": "2024-08-20 10:40:35", "link": "http://arxiv.org/abs/2408.10718v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "MEGen: Generative Backdoor in Large Language Models via Model Editing", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities. Their\npowerful generative abilities enable flexible responses based on various\nqueries or instructions. Emerging as widely adopted generalists for diverse\ntasks, LLMs are still vulnerable to backdoors. This paper proposes an\nediting-based generative backdoor, named MEGen, aiming to create a customized\nbackdoor for NLP tasks with the least side effects. In our approach, we first\nleverage a language model to insert a trigger selected on fixed metrics into\nthe input, then design a pipeline of model editing to directly embed a backdoor\ninto an LLM. By adjusting a small set of local parameters with a mini-batch of\nsamples, MEGen significantly enhances time efficiency and achieves high\nrobustness. Experimental results indicate that our backdoor attack strategy\nachieves a high attack success rate on poison data while maintaining the\nmodel's performance on clean data. Notably, the backdoored model, when\ntriggered, can freely output pre-set dangerous information while successfully\ncompleting downstream tasks. This suggests that future LLM applications could\nbe guided to deliver certain dangerous information, thus altering the LLM's\ngenerative style. We believe this approach provides insights for future LLM\napplications and the execution of backdoor attacks on conversational AI\nsystems.", "published": "2024-08-20 10:44:29", "link": "http://arxiv.org/abs/2408.10722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Efficient Large Language Models for Scientific Text: A Review", "abstract": "Large language models (LLMs) have ushered in a new era for processing complex\ninformation in various fields, including science. The increasing amount of\nscientific literature allows these models to acquire and understand scientific\nknowledge effectively, thus improving their performance in a wide range of\ntasks. Due to the power of LLMs, they require extremely expensive computational\nresources, intense amounts of data, and training time. Therefore, in recent\nyears, researchers have proposed various methodologies to make scientific LLMs\nmore affordable. The most well-known approaches align in two directions. It can\nbe either focusing on the size of the models or enhancing the quality of data.\nTo date, a comprehensive review of these two families of methods has not yet\nbeen undertaken. In this paper, we (I) summarize the current advances in the\nemerging abilities of LLMs into more accessible AI solutions for science, and\n(II) investigate the challenges and opportunities of developing affordable\nsolutions for scientific domains using LLMs.", "published": "2024-08-20 10:57:34", "link": "http://arxiv.org/abs/2408.10729v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Flexora: Flexible Low Rank Adaptation for Large Language Models", "abstract": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.", "published": "2024-08-20 12:13:04", "link": "http://arxiv.org/abs/2408.10774v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ColBERT Retrieval and Ensemble Response Scoring for Language Model\n  Question Answering", "abstract": "Domain-specific question answering remains challenging for language models,\ngiven the deep technical knowledge required to answer questions correctly. This\ndifficulty is amplified for smaller language models that cannot encode as much\ninformation in their parameters as larger models. The \"Specializing Large\nLanguage Models for Telecom Networks\" challenge aimed to enhance the\nperformance of two small language models, Phi-2 and Falcon-7B in\ntelecommunication question answering. In this paper, we present our question\nanswering systems for this challenge. Our solutions achieved leading marks of\n81.9% accuracy for Phi-2 and 57.3% for Falcon-7B. We have publicly released our\ncode and fine-tuned models.", "published": "2024-08-20 12:58:16", "link": "http://arxiv.org/abs/2408.10808v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Beyond English-Centric LLMs: What Language Do Multilingual Language\n  Models Think in?", "abstract": "In this study, we investigate whether non-English-centric LLMs, despite their\nstrong performance, `think' in their respective dominant language: more\nprecisely, `think' refers to how the representations of intermediate layers,\nwhen un-embedded into the vocabulary space, exhibit higher probabilities for\ncertain dominant languages during generation. We term such languages as\ninternal $\\textbf{latent languages}$.\n  We examine the latent language of three typical categories of models for\nJapanese processing: Llama2, an English-centric model; Swallow, an\nEnglish-centric model with continued pre-training in Japanese; and LLM-jp, a\nmodel pre-trained on balanced English and Japanese corpora. Our empirical\nfindings reveal that, unlike Llama2 which relies exclusively on English as the\ninternal latent language, Japanese-specific Swallow and LLM-jp employ both\nJapanese and English, exhibiting dual internal latent languages. For any given\ntarget language, the model preferentially activates the latent language most\nclosely related to it. In addition, we explore how intermediate layers respond\nto questions involving cultural conflicts between latent internal and target\noutput languages. We further explore how the language identity shifts across\nlayers while keeping consistent semantic meaning reflected in the intermediate\nlayer representations.\n  This study deepens the understanding of non-English-centric large language\nmodels, highlighting the intricate dynamics of language representation within\ntheir intermediate layers.", "published": "2024-08-20 13:05:41", "link": "http://arxiv.org/abs/2408.10811v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GS-KGC: A Generative Subgraph-based Framework for Knowledge Graph\n  Completion with Large Language Models", "abstract": "Knowledge graph completion (KGC) focuses on identifying missing triples in a\nknowledge graph (KG) , which is crucial for many downstream applications. Given\nthe rapid development of large language models (LLMs), some LLM-based methods\nare proposed for KGC task. However, most of them focus on prompt engineering\nwhile overlooking the fact that finer-grained subgraph information can aid LLMs\nin generating more accurate answers. In this paper, we propose a novel\ncompletion framework called \\textbf{G}enerative \\textbf{S}ubgraph-based KGC\n(GS-KGC), which utilizes subgraph information as contextual reasoning and\nemploys a QA approach to achieve the KGC task. This framework primarily\nincludes a subgraph partitioning algorithm designed to generate negatives and\nneighbors. Specifically, negatives can encourage LLMs to generate a broader\nrange of answers, while neighbors provide additional contextual insights for\nLLM reasoning. Furthermore, we found that GS-KGC can discover potential triples\nwithin the KGs and new facts beyond the KGs. Experiments conducted on four\ncommon KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it\nshows a 5.6\\% increase in Hits@3 compared to the LLM-based model CP-KGC on the\nFB15k-237N, and a 9.3\\% increase over the LLM-based model TECHS on the ICEWS14.", "published": "2024-08-20 13:13:41", "link": "http://arxiv.org/abs/2408.10819v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models for Math Reasoning Tasks", "abstract": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.", "published": "2024-08-20 13:34:17", "link": "http://arxiv.org/abs/2408.10839v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General\n  Role-Playing Language Model", "abstract": "The rapid advancement of large language models (LLMs) has revolutionized\nrole-playing, enabling the development of general role-playing models. However,\ncurrent role-playing training has two significant issues: (I) Using a\npredefined role profile to prompt dialogue training for specific scenarios\nusually leads to inconsistencies and even conflicts between the dialogue and\nthe profile, resulting in training biases. (II) The model learns to imitate the\nrole based solely on the profile, neglecting profile-dialogue alignment at the\nsentence level. In this work, we propose a simple yet effective framework\ncalled BEYOND DIALOGUE, designed to overcome these hurdles. This framework\ninnovatively introduces \"beyond dialogue\" tasks to align dialogue with profile\ntraits based on each specific scenario, thereby eliminating biases during\ntraining. Furthermore, by adopting an innovative prompting mechanism that\ngenerates reasoning outcomes for training, the framework allows the model to\nachieve fine-grained alignment between profile and dialogue at the sentence\nlevel. The aforementioned methods are fully automated and low-cost.\nAdditionally, the integration of automated dialogue and objective evaluation\nmethods forms a comprehensive framework, paving the way for general\nrole-playing. Experimental results demonstrate that our model excels in\nadhering to and reflecting various dimensions of role profiles, outperforming\nmost proprietary general and specialized role-playing baselines. All code and\ndatasets are available at https://github.com/yuyouyu32/BeyondDialogue.", "published": "2024-08-20 14:47:38", "link": "http://arxiv.org/abs/2408.10903v5", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization", "abstract": "Large Language Models (LLMs) have great success in natural language\nprocessing tasks such as response generation. However, their use in tabular\ndata has been limited due to their inferior performance compared to traditional\nmachine learning models (TMLs) such as XGBoost. We find that the pre-trained\nknowledge of LLMs enables them to interpret new variables that appear in a test\nwithout additional training, a capability central to the concept of\nOut-of-Variable (OOV). From the findings, we propose a\nLanguage-Based-Classifier (LBC), a classifier that maximizes the benefits of\nLLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's\nunderstanding, 2) Advanced order and indicator to enhance data representation\nto the model, and 3) Using verbalizer to map logit scores to classes during\ninference to generate model predictions. These strategies, combined with the\npre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of\nLBC. LBC is the first study to apply an LLM-based model to OOV tasks. The\nsource code is at https://github.com/sksmssh/LBCforOOVGen", "published": "2024-08-20 15:05:02", "link": "http://arxiv.org/abs/2408.10923v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Disentangling segmental and prosodic factors to non-native speech\n  comprehensibility", "abstract": "Current accent conversion (AC) systems do not disentangle the two main\nsources of non-native accent: segmental and prosodic characteristics. Being\nable to manipulate a non-native speaker's segmental and/or prosodic channels\nindependently is critical to quantify how these two channels contribute to\nspeech comprehensibility and social attitudes. We present an AC system that not\nonly decouples voice quality from accent, but also disentangles the latter into\nits segmental and prosodic characteristics. The system is able to generate\naccent conversions that combine (1) the segmental characteristics from a source\nutterance, (2) the voice characteristics from a target utterance, and (3) the\nprosody of a reference utterance. We show that vector quantization of acoustic\nembeddings and removal of consecutive duplicated codewords allows the system to\ntransfer prosody and improve voice similarity. We conduct perceptual listening\ntests to quantify the individual contributions of segmental features and\nprosody on the perceived comprehensibility of non-native speech. Our results\nindicate that, contrary to prior research in non-native speech, segmental\nfeatures have a larger impact on comprehensibility than prosody. The proposed\nAC system may also be used to study how segmental and prosody cues affect\nsocial attitudes towards non-native speech.", "published": "2024-08-20 16:43:55", "link": "http://arxiv.org/abs/2408.10997v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Security Attacks on LLM-based Code Completion Tools", "abstract": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.", "published": "2024-08-20 17:00:04", "link": "http://arxiv.org/abs/2408.11006v4", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Mistral-SPLADE: LLMs for better Learned Sparse Retrieval", "abstract": "Learned Sparse Retrievers (LSR) have evolved into an effective retrieval\nstrategy that can bridge the gap between traditional keyword-based sparse\nretrievers and embedding-based dense retrievers. At its core, learned sparse\nretrievers try to learn the most important semantic keyword expansions from a\nquery and/or document which can facilitate better retrieval with overlapping\nkeyword expansions. LSR like SPLADE has typically been using encoder only\nmodels with MLM (masked language modeling) style objective in conjunction with\nknown ways of retrieval performance improvement such as hard negative mining,\ndistillation, etc. In this work, we propose to use decoder-only model for\nlearning semantic keyword expansion. We posit, decoder only models that have\nseen much higher magnitudes of data are better equipped to learn keyword\nexpansions needed for improved retrieval. We use Mistral as the backbone to\ndevelop our Learned Sparse Retriever similar to SPLADE and train it on a subset\nof sentence-transformer data which is often used for training text embedding\nmodels. Our experiments support the hypothesis that a sparse retrieval model\nbased on decoder only large language model (LLM) surpasses the performance of\nexisting LSR systems, including SPLADE and all its variants. The LLM based\nmodel (Echo-Mistral-SPLADE) now stands as a state-of-the-art learned sparse\nretrieval model on the BEIR text retrieval benchmark.", "published": "2024-08-20 18:21:54", "link": "http://arxiv.org/abs/2408.11119v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Public Health in Disaster: Emotional Health and Life Incidents\n  Extraction during Hurricane Harvey", "abstract": "Countless disasters have resulted from climate change, causing severe damage\nto infrastructure and the economy. These disasters have significant societal\nimpacts, necessitating mental health services for the millions affected. To\nprepare for and respond effectively to such events, it is important to\nunderstand people's emotions and the life incidents they experience before and\nafter a disaster strikes. In this case study, we collected a dataset of\napproximately 400,000 public tweets related to the storm. Using a BERT-based\nmodel, we predicted the emotions associated with each tweet. To efficiently\nidentify these topics, we utilized the Latent Dirichlet Allocation (LDA)\ntechnique for topic modeling, which allowed us to bypass manual content\nanalysis and extract meaningful patterns from the data. However, rather than\nstopping at topic identification like previous methods \\cite{math11244910}, we\nfurther refined our analysis by integrating Graph Neural Networks (GNN) and\nLarge Language Models (LLM). The GNN was employed to generate embeddings and\nconstruct a similarity graph of the tweets, which was then used to optimize\nclustering. Subsequently, we used an LLM to automatically generate descriptive\nnames for each event cluster, offering critical insights for disaster\npreparedness and response strategies.", "published": "2024-08-20 18:31:20", "link": "http://arxiv.org/abs/2408.11133v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Combining Objective and Subjective Perspectives for Political News\n  Understanding", "abstract": "Researchers and practitioners interested in computational politics rely on\nautomatic content analysis tools to make sense of the large amount of political\ntexts available on the Web. Such tools should provide objective and subjective\naspects at different granularity levels to make the analyses useful in\npractice. Existing methods produce interesting insights for objective aspects,\nbut are limited for subjective ones, are often limited to national contexts,\nand have limited explainability. We introduce a text analysis framework which\nintegrates both perspectives and provides a fine-grained processing of\nsubjective aspects. Information retrieval techniques and knowledge bases\ncomplement powerful natural language processing components to allow a flexible\naggregation of results at different granularity levels. Importantly, the\nproposed bottom-up approach facilitates the explainability of the obtained\nresults. We illustrate its functioning with insights on news outlets, political\norientations, topics, individual entities, and demographic segments. The\napproach is instantiated on a large corpus of French news, but is designed to\nwork seamlessly for other languages and countries.", "published": "2024-08-20 20:13:19", "link": "http://arxiv.org/abs/2408.11174v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "DSP-MLIR: A MLIR Dialect for Digital Signal Processing", "abstract": "Traditional Digital Signal Processing ( DSP ) compilers work at low level (\nC-level / assembly level ) and hence lose much of the optimization\nopportunities present at high-level ( domain-level ). The emerging multi-level\ncompiler infrastructure MLIR ( Multi-level Intermediate Representation ) allows\nto specify optimizations at higher level. In this paper, we utilize MLIR\nframework to introduce a DSP Dialect and perform domain-specific optimizations\nat dialect -level ( high-level ) and show the usefulness of these optimizations\non sample DSP apps. In particular, we develop a compiler for DSP and a DSL\n(Domain Specific Language) to ease the development of apps. We show the\nperformance improvement in execution time for these sample apps by upto 10x\nwhich would have been difficult if the IR were at C/ affine level.", "published": "2024-08-20 21:33:17", "link": "http://arxiv.org/abs/2408.11205v1", "categories": ["eess.SP", "cs.CL"], "primary_category": "eess.SP"}
{"title": "CoDi: Conversational Distillation for Grounded Question Answering", "abstract": "Distilling conversational skills into Small Language Models (SLMs) with\napproximately 1 billion parameters presents significant challenges. Firstly,\nSLMs have limited capacity in their model parameters to learn extensive\nknowledge compared to larger models. Secondly, high-quality conversational\ndatasets are often scarce, small, and domain-specific. Addressing these\nchallenges, we introduce a novel data distillation framework named CoDi (short\nfor Conversational Distillation, pronounced \"Cody\"), allowing us to synthesize\nlarge-scale, assistant-style datasets in a steerable and diverse manner.\nSpecifically, while our framework is task agnostic at its core, we explore and\nevaluate the potential of CoDi on the task of conversational grounded reasoning\nfor question answering. This is a typical on-device scenario for specialist\nSLMs, allowing for open-domain model responses, without requiring the model to\n\"memorize\" world knowledge in its limited weights. Our evaluations show that\nSLMs trained with CoDi-synthesized data achieve performance comparable to\nmodels trained on human-annotated data in standard metrics. Additionally, when\nusing our framework to generate larger datasets from web data, our models\nsurpass larger, instruction-tuned models in zero-shot conversational grounded\nreasoning tasks.", "published": "2024-08-20 22:35:47", "link": "http://arxiv.org/abs/2408.11219v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automating Knowledge Discovery from Scientific Literature via LLMs: A\n  Dual-Agent Approach with Progressive Ontology Prompting", "abstract": "To address the challenge of automating knowledge discovery from a vast volume\nof literature, in this paper, we introduce a novel framework based on large\nlanguage models (LLMs) that combines a progressive ontology prompting (POP)\nalgorithm with a dual-agent system, named LLM-Duo, designed to enhance the\nautomation of knowledge extraction from scientific articles. The POP algorithm\nutilizes a prioritized breadth-first search (BFS) across a predefined ontology\nto generate structured prompt templates and action orders, thereby guiding LLMs\nto discover knowledge in an automatic manner. Additionally, our LLM-Duo employs\ntwo specialized LLM agents: an explorer and an evaluator. These two agents work\ncollaboratively and adversarially to enhance the reliability of the discovery\nand annotation processes. Experiments demonstrate that our method outperforms\nadvanced baselines, enabling more accurate and complete annotations. To\nvalidate the effectiveness of our method in real-world scenarios, we employ our\nmethod in a case study of speech-language intervention discovery. Our method\nidentifies 2,421 interventions from 64,177 research articles in the\nspeech-language therapy domain. We curate these findings into a publicly\naccessible intervention knowledge base that holds significant potential to\nbenefit the speech-language therapy community.", "published": "2024-08-20 16:42:23", "link": "http://arxiv.org/abs/2409.00054v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions", "abstract": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.", "published": "2024-08-20 00:40:49", "link": "http://arxiv.org/abs/2408.10468v4", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Event Stream based Sign Language Translation: A High-Definition\n  Benchmark Dataset and A New Algorithm", "abstract": "Sign Language Translation (SLT) is a core task in the field of AI-assisted\ndisability. Unlike traditional SLT based on visible light videos, which is\neasily affected by factors such as lighting, rapid hand movements, and privacy\nbreaches, this paper proposes the use of high-definition Event streams for SLT,\neffectively mitigating the aforementioned issues. This is primarily because\nEvent streams have a high dynamic range and dense temporal signals, which can\nwithstand low illumination and motion blur well. Additionally, due to their\nsparsity in space, they effectively protect the privacy of the target person.\nMore specifically, we propose a new high-resolution Event stream sign language\ndataset, termed Event-CSL, which effectively fills the data gap in this area of\nresearch. It contains 14,827 videos, 14,821 glosses, and 2,544 Chinese words in\nthe text vocabulary. These samples are collected in a variety of indoor and\noutdoor scenes, encompassing multiple angles, light intensities, and camera\nmovements. We have benchmarked existing mainstream SLT works to enable fair\ncomparison for future efforts. Based on this dataset and several other\nlarge-scale datasets, we propose a novel baseline method that fully leverages\nthe Mamba model's ability to integrate temporal information of CNN features,\nresulting in improved sign language translation outcomes. Both the benchmark\ndataset and source code will be released on\nhttps://github.com/Event-AHU/OpenESL", "published": "2024-08-20 02:01:30", "link": "http://arxiv.org/abs/2408.10488v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.CV"}
{"title": "XCB: an effective contextual biasing approach to bias cross-lingual\n  phrases in speech recognition", "abstract": "Contextualized ASR models have been demonstrated to effectively improve the\nrecognition accuracy of uncommon phrases when a predefined phrase list is\navailable. However, these models often struggle with bilingual settings, which\nare prevalent in code-switching speech recognition. In this study, we make the\ninitial attempt to address this challenge by introducing a Cross-lingual\nContextual Biasing(XCB) module. Specifically, we augment a pre-trained ASR\nmodel for the dominant language by integrating an auxiliary language biasing\nmodule and a supplementary language-specific loss, aimed at enhancing the\nrecognition of phrases in the secondary language. Experimental results\nconducted on our in-house code-switching dataset have validated the efficacy of\nour approach, demonstrating significant improvements in the recognition of\nbiasing phrases in the secondary language, even without any additional\ninference overhead. Additionally, our proposed system exhibits both efficiency\nand generalization when is applied by the unseen ASRU-2019 test set.", "published": "2024-08-20 04:00:19", "link": "http://arxiv.org/abs/2408.10524v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multilingual Non-Factoid Question Answering with Answer Paragraph\n  Selection", "abstract": "Most existing Question Answering Datasets (QuADs) primarily focus on\nfactoid-based short-context Question Answering (QA) in high-resource languages.\nHowever, the scope of such datasets for low-resource languages remains limited,\nwith only a few works centered on factoid-based QuADs and none on non-factoid\nQuADs. Therefore, this work presents MuNfQuAD, a multilingual QuAD with\nnon-factoid questions. It utilizes interrogative sub-headings from BBC news\narticles as questions and the corresponding paragraphs as silver answers. The\ndataset comprises over 578K QA pairs across 38 languages, encompassing several\nlow-resource languages, and stands as the largest multilingual QA dataset to\ndate. Based on the manual annotations of 790 QA-pairs from MuNfQuAD (golden\nset), we observe that 98\\% of questions can be answered using their\ncorresponding silver answer. Our fine-tuned Answer Paragraph Selection (APS)\nmodel outperforms the baselines. The APS model attained an accuracy of 80\\% and\n72\\%, as well as a macro F1 of 72\\% and 66\\%, on the MuNfQuAD testset and the\ngolden set, respectively. Furthermore, the APS model effectively generalizes a\ncertain language within the golden set, even after being fine-tuned on silver\nlabels. We also observe that the fine-tuned APS model is beneficial for\nreducing the context of a question. These findings suggest that this resource\nwould be a valuable contribution to the QA research community.", "published": "2024-08-20 07:37:06", "link": "http://arxiv.org/abs/2408.10604v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for\n  Large Language Models", "abstract": "Large language models (LLMs) have grown significantly in scale, leading to a\ncritical need for efficient model pruning techniques. Existing post-training\npruning techniques primarily focus on measuring weight importance on converged\ndense models to determine salient weights to retain. However, they often\noverlook the changes in weight importance during the pruning process, which can\nlead to performance degradation in the pruned models. To address this issue, we\npresent LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a\nnovel one-shot pruning framework that rebuilds the sparsity mask of pruned\nmodels without any retraining or weight reconstruction. LLM-Barber incorporates\nblock-aware error optimization across Self-Attention and MLP blocks, ensuring\nglobal performance optimization. Inspired by the recent discovery of prominent\noutliers in LLMs, LLM-Barber introduces an innovative pruning metric that\nidentifies weight importance using weights multiplied by gradients. Our\nexperiments show that LLM-Barber can efficiently prune models like LLaMA and\nOPT families with 7B to 13B parameters on a single A100 GPU in just 30 minutes,\nachieving state-of-the-art results in both perplexity and zero-shot performance\nacross various language benchmarks. Code is available at\nhttps://github.com/YupengSu/LLM-Barber.", "published": "2024-08-20 08:13:52", "link": "http://arxiv.org/abs/2408.10631v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on\n  Whisper", "abstract": "Pre-trained multilingual speech foundation models, like Whisper, have shown\nimpressive performance across different languages. However, adapting these\nmodels to new or specific languages is computationally extensive and faces\ncatastrophic forgetting problems. Addressing these issues, our study\ninvestigates strategies to enhance the model on new languages in the absence of\noriginal training data, while also preserving the established performance on\nthe original languages. Specifically, we first compare various LoRA-based\nmethods to find out their vulnerability to forgetting. To mitigate this issue,\nwe propose to leverage the LoRA parameters from the original model for\napproximate orthogonal gradient descent on the new samples. Additionally, we\nalso introduce a learnable rank coefficient to allocate trainable parameters\nfor more efficient training. Our experiments with a Chinese Whisper model (for\nUyghur and Tibetan) yield better results with a more compact parameter set.", "published": "2024-08-20 09:31:59", "link": "http://arxiv.org/abs/2408.10680v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for\n  Assessing and Improving Unlearning Robustness in Large Language Models", "abstract": "LLM have achieved success in many fields but still troubled by problematic\ncontent in the training corpora. LLM unlearning aims at reducing their\ninfluence and avoid undesirable behaviours. However, existing unlearning\nmethods remain vulnerable to adversarial queries and the unlearned knowledge\nresurfaces after the manually designed attack queries. As part of a red-team\neffort to proactively assess the vulnerabilities of unlearned models, we design\nDynamic Unlearning Attack (DUA), a dynamic and automated framework to attack\nthese models and evaluate their robustness. It optimizes adversarial suffixes\nto reintroduce the unlearned knowledge in various scenarios. We find that\nunlearned knowledge can be recovered in $55.2\\%$ of the questions, even without\nrevealing the unlearned model's parameters. In response to this vulnerability,\nwe propose Latent Adversarial Unlearning (LAU), a universal framework that\neffectively enhances the robustness of the unlearned process. It formulates the\nunlearning process as a min-max optimization problem and resolves it through\ntwo stages: an attack stage, where perturbation vectors are trained and added\nto the latent space of LLMs to recover the unlearned knowledge, and a defense\nstage, where previously trained perturbation vectors are used to enhance\nunlearned model's robustness. With our LAU framework, we obtain two robust\nunlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across\nmultiple unlearning benchmarks and various models, and demonstrate that they\nimprove the unlearning effectiveness by over $53.5\\%$, cause only less than a\n$11.6\\%$ reduction in neighboring knowledge, and have almost no impact on the\nmodel's general capabilities.", "published": "2024-08-20 09:36:04", "link": "http://arxiv.org/abs/2408.10682v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in\n  Education for Large Language Models", "abstract": "Teachers are important to imparting knowledge and guiding learners, and the\nrole of large language models (LLMs) as potential educators is emerging as an\nimportant area of study. Recognizing LLMs' capability to generate educational\ncontent can lead to advances in automated and personalized learning. While LLMs\nhave been tested for their comprehension and problem-solving skills, their\ncapability in teaching remains largely unexplored. In teaching, questioning is\na key skill that guides students to analyze, evaluate, and synthesize core\nconcepts and principles. Therefore, our research introduces a benchmark to\nevaluate the questioning capability in education as a teacher of LLMs through\nevaluating their generated educational questions, utilizing Anderson and\nKrathwohl's taxonomy across general, monodisciplinary, and interdisciplinary\ndomains. We shift the focus from LLMs as learners to LLMs as educators,\nassessing their teaching capability through guiding them to generate questions.\nWe apply four metrics, including relevance, coverage, representativeness, and\nconsistency, to evaluate the educational quality of LLMs' outputs. Our results\nindicate that GPT-4 demonstrates significant potential in teaching general,\nhumanities, and science courses; Claude2 appears more apt as an\ninterdisciplinary teacher. Furthermore, the automatic scores align with human\nperspectives.", "published": "2024-08-20 15:36:30", "link": "http://arxiv.org/abs/2408.10947v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Athena: Safe Autonomous Agents with Verbal Contrastive Learning", "abstract": "Due to emergent capabilities, large language models (LLMs) have been utilized\nas language-based agents to perform a variety of tasks and make decisions with\nan increasing degree of autonomy. These autonomous agents can understand\nhigh-level instructions, interact with their environments, and execute complex\ntasks using a selection of tools available to them. As the capabilities of the\nagents expand, ensuring their safety and trustworthiness becomes more\nimperative. In this study, we introduce the Athena framework which leverages\nthe concept of verbal contrastive learning where past safe and unsafe\ntrajectories are used as in-context (contrastive) examples to guide the agent\ntowards safety while fulfilling a given task. The framework also incorporates a\ncritiquing mechanism to guide the agent to prevent risky actions at every step.\nFurthermore, due to the lack of existing benchmarks on the safety reasoning\nability of LLM-based agents, we curate a set of 80 toolkits across 8 categories\nwith 180 scenarios to provide a safety evaluation benchmark. Our experimental\nevaluation, with both closed- and open-source LLMs, indicates verbal\ncontrastive learning and interaction-level critiquing improve the safety rate\nsignificantly.", "published": "2024-08-20 17:21:10", "link": "http://arxiv.org/abs/2408.11021v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Scaling Law with Learning Rate Annealing", "abstract": "We find that the cross-entropy loss curves of neural language models\nempirically adhere to a scaling law with learning rate (LR) annealing over\ntraining steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where\n$L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR\ncurve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are\nconstant parameters. This formulation takes into account two factors: (1)\npower-law scaling over data size, and (2) the additional loss reduction during\nLR annealing. Therefore, this formulation can describe the full loss curve at\neach step, rather than the single loss point at the end of training. Applying\nthe scaling law with LR annealing and fitting only one or two training curves,\nwe can accurately predict the loss at any given step across any learning rate\nscheduler (LRS). This approach significantly reduces computational cost in\nformulating scaling laws while providing more accuracy and expressiveness for\ntraining dynamics. Extensive experiments demonstrate that our findings hold\nacross a range of hyper-parameters and model architectures, and our equation\ncan extend to scaling effect of model sizes. Moreover, our formulation provides\naccurate theoretical verification and explanation for empirical results\nobserved in numerous previous studies, particularly those focusing on LR\nschedule and annealing. We believe that this work is promising to enhance the\nunderstanding of LLM training dynamics while greatly democratizing scaling\nlaws, and it can guide researchers in refining training strategies (e.g.\ncritical LRS) for further LLMs.", "published": "2024-08-20 17:30:48", "link": "http://arxiv.org/abs/2408.11029v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments", "abstract": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for route summarization, and end-to-end\ntraining on VLN datasets. The augmented datasets are synthesized automatically.\nExperimental results demonstrate FLAME's superiority over existing methods,\nsurpassing state-of-the-art methods by a 7.3% increase in task completion on\nTouchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs)\nin complex navigation tasks, representing an advancement towards applications\nof MLLMs in the field of embodied intelligence.", "published": "2024-08-20 17:57:46", "link": "http://arxiv.org/abs/2408.11051v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "What can Large Language Models Capture about Code Functional\n  Equivalence?", "abstract": "Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress\nin learning rich representations of the structure and syntax of code,\nsuccessfully using it to generate or classify code fragments. At the same time,\nunderstanding if they are able to do so because they capture code semantics,\nand how well, is still an open question. In this paper, we tackle this problem\nby introducing SeqCoBench, a benchmark for systematically assessing how\nCode-LLMs can capture code functional equivalence. SeqCoBench contains over 20\ncode transformations that either preserve or alter the semantics of Python\nprograms. We conduct extensive evaluations in different settings, including\nzero-shot and parameter-efficient finetuning methods on state-of-the-art\n(Code)-LLMs to see if they can discern semantically equivalent or different\npairs of programs in SeqCoBench. We find that the performance gap between these\nLLMs and classical match-based retrieval scores is minimal, with both\napproaches showing a concerning lack of depth in understanding code semantics.", "published": "2024-08-20 11:19:06", "link": "http://arxiv.org/abs/2408.11081v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "DOMBA: Double Model Balancing for Access-Controlled Language Models via\n  Minimum-Bounded Aggregation", "abstract": "The utility of large language models (LLMs) depends heavily on the quality\nand quantity of their training data. Many organizations possess large data\ncorpora that could be leveraged to train or fine-tune LLMs tailored to their\nspecific needs. However, these datasets often come with access restrictions\nthat are based on user privileges and enforced by access control mechanisms.\nTraining LLMs on such datasets could result in exposure of sensitive\ninformation to unauthorized users. A straightforward approach for preventing\nsuch exposure is to train a separate model for each access level. This,\nhowever, may result in low utility models due to the limited amount of training\ndata per model compared to the amount in the entire organizational corpus.\nAnother approach is to train a single LLM on all the data while limiting the\nexposure of unauthorized information. However, current exposure-limiting\nmethods for LLMs are ineffective for access-controlled data, where sensitive\ninformation appears frequently across many training examples. We propose DOMBA\n- double model balancing - a simple approach for training and deploying LLMs\nthat provides high utility and access-control functionality with security\nguarantees. DOMBA aggregates the probability distributions of two models, each\ntrained on documents with (potentially many) different access levels, using a\n\"min-bounded\" average function (a function that is bounded by the smaller\nvalue, e.g., harmonic mean). A detailed mathematical analysis and extensive\nevaluation show that DOMBA safeguards restricted information while offering\nutility comparable to non-secure models.", "published": "2024-08-20 18:23:38", "link": "http://arxiv.org/abs/2408.11121v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "SubgoalXL: Subgoal-based Expert Learning for Theorem Proving", "abstract": "Formal theorem proving, a field at the intersection of mathematics and\ncomputer science, has seen renewed interest with advancements in large language\nmodels (LLMs). This paper introduces SubgoalXL, a novel approach that\nsynergizes subgoal-based proofs with expert learning to enhance LLMs'\ncapabilities in formal theorem proving within the Isabelle environment.\nSubgoalXL addresses two critical challenges: the scarcity of specialized\nmathematics and theorem-proving data, and the need for improved multi-step\nreasoning abilities in LLMs. By optimizing data efficiency and employing\nsubgoal-level supervision, SubgoalXL extracts richer information from limited\nhuman-generated proofs. The framework integrates subgoal-oriented proof\nstrategies with an expert learning system, iteratively refining formal\nstatement, proof, and subgoal generators. Leveraging the Isabelle environment's\nadvantages in subgoal-based proofs, SubgoalXL achieves a new state-of-the-art\nperformance of 56.1\\% in Isabelle on the standard miniF2F dataset, marking an\nabsolute improvement of 4.9\\%. Notably, SubgoalXL successfully solves 41 AMC12,\n9 AIME, and 3 IMO problems from miniF2F. These results underscore the\neffectiveness of maximizing limited data utility and employing targeted\nguidance for complex reasoning in formal theorem proving, contributing to the\nongoing advancement of AI reasoning capabilities. The implementation is\navailable at \\url{https://github.com/zhaoxlpku/SubgoalXL}.", "published": "2024-08-20 20:10:53", "link": "http://arxiv.org/abs/2408.11172v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.LG"}
{"title": "Reading with Intent", "abstract": "Retrieval augmented generation (RAG) systems augment how knowledge language\nmodels are by integrating external information sources such as Wikipedia,\ninternal documents, scientific papers, or the open internet. RAG systems that\nrely on the open internet as their knowledge source have to contend with the\ncomplexities of human-generated content. Human communication extends much\ndeeper than just the words rendered as text. Intent, tonality, and connotation\ncan all change the meaning of what is being conveyed. Recent real-world\ndeployments of RAG systems have shown some difficulty in understanding these\nnuances of human communication. One significant challenge for these systems\nlies in processing sarcasm. Though the Large Language Models (LLMs) that make\nup the backbone of these RAG systems are able to detect sarcasm, they currently\ndo not always use these detections for the subsequent processing of text. To\naddress these issues, in this paper, we synthetically generate sarcastic\npassages from Natural Question's Wikipedia retrieval corpus. We then test the\nimpact of these passages on the performance of both the retriever and reader\nportion of the RAG pipeline. We introduce a prompting system designed to\nenhance the model's ability to interpret and generate responses in the presence\nof sarcasm, thus improving overall system performance. Finally, we conduct\nablation studies to validate the effectiveness of our approach, demonstrating\nimprovements in handling sarcastic content within RAG systems.", "published": "2024-08-20 20:47:27", "link": "http://arxiv.org/abs/2408.11189v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Out-of-Distribution Detection with Attention Head Masking for Multimodal\n  Document Classification", "abstract": "Detecting out-of-distribution (OOD) data is crucial in machine learning\napplications to mitigate the risk of model overconfidence, thereby enhancing\nthe reliability and safety of deployed systems. The majority of existing OOD\ndetection methods predominantly address uni-modal inputs, such as images or\ntexts. In the context of multi-modal documents, there is a notable lack of\nextensive research on the performance of these methods, which have primarily\nbeen developed with a focus on computer vision tasks. We propose a novel\nmethodology termed as attention head masking (AHM) for multi-modal OOD tasks in\ndocument classification systems. Our empirical results demonstrate that the\nproposed AHM method outperforms all state-of-the-art approaches and\nsignificantly decreases the false positive rate (FPR) compared to existing\nsolutions up to 7.5\\%. This methodology generalizes well to multi-modal data,\nsuch as documents, where visual and textual information are modeled under the\nsame Transformer architecture. To address the scarcity of high-quality publicly\navailable document datasets and encourage further research on OOD detection for\ndocuments, we introduce FinanceDocs, a new document AI dataset. Our code and\ndataset are publicly available.", "published": "2024-08-20 23:30:00", "link": "http://arxiv.org/abs/2408.11237v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Hierarchical Retrieval-Augmented Generation Model with Rethink for\n  Multi-hop Question Answering", "abstract": "Multi-hop Question Answering (QA) necessitates complex reasoning by\nintegrating multiple pieces of information to resolve intricate questions.\nHowever, existing QA systems encounter challenges such as outdated information,\ncontext window length limitations, and an accuracy-quantity trade-off. To\naddress these issues, we propose a novel framework, the Hierarchical\nRetrieval-Augmented Generation Model with Rethink (HiRAG), comprising\nDecomposer, Definer, Retriever, Filter, and Summarizer five key modules. We\nintroduce a new hierarchical retrieval strategy that incorporates both sparse\nretrieval at the document level and dense retrieval at the chunk level,\neffectively integrating their strengths. Additionally, we propose a\nsingle-candidate retrieval method to mitigate the limitations of\nmulti-candidate retrieval. We also construct two new corpora, Indexed\nWikicorpus and Profile Wikicorpus, to address the issues of outdated and\ninsufficient knowledge.\n  Our experimental results on four datasets demonstrate that HiRAG outperforms\nstate-of-the-art models across most metrics, and our Indexed Wikicorpus is\neffective. The code for HiRAG is available at\nhttps://github.com/2282588541a/HiRAG", "published": "2024-08-20 09:29:31", "link": "http://arxiv.org/abs/2408.11875v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Open-FinLLMs: Open Multimodal Large Language Models for Financial\n  Applications", "abstract": "Financial LLMs hold promise for advancing financial tasks and domain-specific\napplications. However, they are limited by scarce corpora, weak multimodal\ncapabilities, and narrow evaluations, making them less suited for real-world\napplication. To address this, we introduce \\textit{Open-FinLLMs}, the first\nopen-source multimodal financial LLMs designed to handle diverse tasks across\ntext, tabular, time-series, and chart data, excelling in zero-shot, few-shot,\nand fine-tuning settings. The suite includes FinLLaMA, pre-trained on a\ncomprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K\nfinancial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning\npairs for strong cross-modal reasoning. We comprehensively evaluate\nOpen-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in\nzero-shot, few-shot, and supervised fine-tuning settings, introducing two new\nmultimodal evaluation datasets. Our results show that Open-FinLLMs outperforms\nafvanced financial and general LLMs such as GPT-4, across financial NLP,\ndecision-making, and multi-modal tasks, highlighting their potential to tackle\nreal-world challenges. To foster innovation and collaboration across academia\nand industry, we release all codes\n(https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under\nOSI-approved licenses.", "published": "2024-08-20 16:15:28", "link": "http://arxiv.org/abs/2408.11878v2", "categories": ["cs.CL", "cs.CE", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Beyond Labels: Aligning Large Language Models with Human-like Reasoning", "abstract": "Aligning large language models (LLMs) with a human reasoning approach ensures\nthat LLMs produce morally correct and human-like decisions. Ethical concerns\nare raised because current models are prone to generating false positives and\nproviding malicious responses. To contribute to this issue, we have curated an\nethics dataset named Dataset for Aligning Reasons (DFAR), designed to aid in\naligning language models to generate human-like reasons. The dataset comprises\nstatements with ethical-unethical labels and their corresponding reasons. In\nthis study, we employed a unique and novel fine-tuning approach that utilizes\nethics labels and their corresponding reasons (L+R), in contrast to the\nexisting fine-tuning approach that only uses labels (L). The original\npre-trained versions, the existing fine-tuned versions, and our proposed\nfine-tuned versions of LLMs were then evaluated on an ethical-unethical\nclassification task and a reason-generation task. Our proposed fine-tuning\nstrategy notably outperforms the others in both tasks, achieving significantly\nhigher accuracy scores in the classification task and lower misalignment rates\nin the reason-generation task. The increase in classification accuracies and\ndecrease in misalignment rates indicate that the L+R fine-tuned models align\nmore with human ethics. Hence, this study illustrates that injecting reasons\nhas substantially improved the alignment of LLMs, resulting in more human-like\nresponses. We have made the DFAR dataset and corresponding codes publicly\navailable at https://github.com/apurba-nsu-rnd-lab/DFAR.", "published": "2024-08-20 17:44:51", "link": "http://arxiv.org/abs/2408.11879v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LeCov: Multi-level Testing Criteria for Large Language Models", "abstract": "Large Language Models (LLMs) are widely used in many different domains, but\nbecause of their limited interpretability, there are questions about how\ntrustworthy they are in various perspectives, e.g., truthfulness and toxicity.\nRecent research has started developing testing methods for LLMs, aiming to\nuncover untrustworthy issues, i.e., defects, before deployment. However,\nsystematic and formalized testing criteria are lacking, which hinders a\ncomprehensive assessment of the extent and adequacy of testing exploration. To\nmitigate this threat, we propose a set of multi-level testing criteria, LeCov,\nfor LLMs. The criteria consider three crucial LLM internal components, i.e.,\nthe attention mechanism, feed-forward neurons, and uncertainty, and contain\nnine types of testing criteria in total. We apply the criteria in two\nscenarios: test prioritization and coverage-guided testing. The experiment\nevaluation, on three models and four datasets, demonstrates the usefulness and\neffectiveness of LeCov.", "published": "2024-08-20 01:17:54", "link": "http://arxiv.org/abs/2408.10474v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SE"}
{"title": "A Little Confidence Goes a Long Way", "abstract": "We introduce a group of related methods for binary classification tasks using\nprobes of the hidden state activations in large language models (LLMs).\nPerformance is on par with the largest and most advanced LLMs currently\navailable, but requiring orders of magnitude fewer computational resources and\nnot requiring labeled data. This approach involves translating class labels\ninto a semantically rich description, spontaneous symmetry breaking of\nmultilayer perceptron probes for unsupervised learning and inference, training\nprobes to generate confidence scores (prior probabilities) from hidden state\nactivations subject to known constraints via entropy maximization, and\nselecting the most confident probe model from an ensemble for prediction. These\ntechniques are evaluated on four datasets using five base LLMs.", "published": "2024-08-20 23:36:00", "link": "http://arxiv.org/abs/2408.11239v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "cs.NE", "math.IT"], "primary_category": "cs.LG"}
{"title": "ICSD: An Open-source Dataset for Infant Cry and Snoring Detection", "abstract": "The detection and analysis of infant cry and snoring events are crucial tasks\nwithin the field of audio signal processing. While existing datasets for\ngeneral sound event detection are plentiful, they often fall short in providing\nsufficient, strongly labeled data specific to infant cries and snoring. To\nprovide a benchmark dataset and thus foster the research of infant cry and\nsnoring detection, this paper introduces the Infant Cry and Snoring Detection\n(ICSD) dataset, a novel, publicly available dataset specially designed for ICSD\ntasks. The ICSD comprises three types of subsets: a real strongly labeled\nsubset with event-based labels annotated manually, a weakly labeled subset with\nonly clip-level event annotations, and a synthetic subset generated and labeled\nwith strong annotations. This paper provides a detailed description of the ICSD\ncreation process, including the challenges encountered and the solutions\nadopted. We offer a comprehensive characterization of the dataset, discussing\nits limitations and key factors for ICSD usage. Additionally, we conduct\nextensive experiments on the ICSD dataset to establish baseline systems and\noffer insights into the main factors when using this dataset for ICSD research.\nOur goal is to develop a dataset that will be widely adopted by the community\nas a new open benchmark for future ICSD research.", "published": "2024-08-20 06:01:50", "link": "http://arxiv.org/abs/2408.10561v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Noval Feature via Color Quantisation for Fake Audio Detection", "abstract": "In the field of deepfake detection, previous studies focus on using\nreconstruction or mask and prediction methods to train pre-trained models,\nwhich are then transferred to fake audio detection training where the encoder\nis used to extract features, such as wav2vec2.0 and Masked Auto Encoder. These\nmethods have proven that using real audio for reconstruction pre-training can\nbetter help the model distinguish fake audio. However, the disadvantage lies in\npoor interpretability, meaning it is hard to intuitively present the\ndifferences between deepfake and real audio. This paper proposes a noval\nfeature extraction method via color quantisation which constrains the\nreconstruction to use a limited number of colors for the spectral image-like\ninput. The proposed method ensures reconstructed input differs from the\noriginal, which allows for intuitive observation of the focus areas in the\nspectral reconstruction. Experiments conducted on the ASVspoof2019 dataset\ndemonstrate that the proposed method achieves better classification performance\ncompared to using the original spectral as input and pretraining the recolor\nnetwork can also benefit the fake audio detection.", "published": "2024-08-20 13:43:20", "link": "http://arxiv.org/abs/2408.10849v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EELE: Exploring Efficient and Extensible LoRA Integration in Emotional\n  Text-to-Speech", "abstract": "In the current era of Artificial Intelligence Generated Content (AIGC), a\nLow-Rank Adaptation (LoRA) method has emerged. It uses a plugin-based approach\nto learn new knowledge with lower parameter quantities and computational costs,\nand it can be plugged in and out based on the specific sub-tasks, offering high\nflexibility. However, the current application schemes primarily incorporate\nLoRA into the pre-introduced conditional parts of the speech models. This fixes\nthe position of LoRA, limiting the flexibility and scalability of its\napplication. Therefore, we propose the Exploring Efficient and Extensible LoRA\nIntegration in Emotional Text-to-Speech (EELE) method. Starting from a general\nneutral speech model, we do not pre-introduce emotional information but instead\nuse the LoRA plugin to design a flexible adaptive scheme that endows the model\nwith emotional generation capabilities. Specifically, we initially train the\nmodel using only neutral speech data. After training is complete, we insert\nLoRA into different modules and fine-tune the model with emotional speech data\nto find the optimal insertion scheme. Through experiments, we compare and test\nthe effects of inserting LoRA at different positions within the model and\nassess LoRA's ability to learn various emotions, effectively proving the\nvalidity of our method. Additionally, we explore the impact of the rank size of\nLoRA and the difference compared to directly fine-tuning the entire model.", "published": "2024-08-20 13:45:28", "link": "http://arxiv.org/abs/2408.10852v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rage Music Classification and Analysis using K-Nearest Neighbour, Random\n  Forest, Support Vector Machine, Convolutional Neural Networks, and Gradient\n  Boosting", "abstract": "We classify rage music (a subgenre of rap well-known for disagreements on\nwhether a particular song is part of the genre) with an extensive feature set\nthrough algorithms including Random Forest, Support Vector Machine, K-nearest\nNeighbour, Gradient Boosting, and Convolutional Neural Networks. We compare\nmethods of classification in the application of audio analysis with machine\nlearning and identify optimal models. We then analyze the significant audio\nfeatures present in and most effective in categorizing rage music, while also\nidentifying key audio features as well as broader separating sonic variations\nand trends.", "published": "2024-08-20 13:55:49", "link": "http://arxiv.org/abs/2408.10864v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BUT Systems and Analyses for the ASVspoof 5 Challenge", "abstract": "This paper describes the BUT submitted systems for the ASVspoof 5 challenge,\nalong with analyses. For the conventional deepfake detection task, we use\nResNet18 and self-supervised models for the closed and open conditions,\nrespectively. In addition, we analyze and visualize different combinations of\nspeaker information and spoofing information as label schemes for training. For\nspoofing-robust automatic speaker verification (SASV), we introduce effective\npriors and propose using logistic regression to jointly train affine\ntransformations of the countermeasure scores and the automatic speaker\nverification scores in such a way that the SASV LLR is optimized.", "published": "2024-08-20 19:18:20", "link": "http://arxiv.org/abs/2408.11152v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial training of Keyword Spotting to Minimize TTS Data\n  Overfitting", "abstract": "The keyword spotting (KWS) problem requires large amounts of real speech\ntraining data to achieve high accuracy across diverse populations. Utilizing\nlarge amounts of text-to-speech (TTS) synthesized data can reduce the cost and\ntime associated with KWS development. However, TTS data may contain artifacts\nnot present in real speech, which the KWS model can exploit (overfit), leading\nto degraded accuracy on real speech. To address this issue, we propose applying\nan adversarial training method to prevent the KWS model from learning\nTTS-specific features when trained on large amounts of TTS data. Experimental\nresults demonstrate that KWS model accuracy on real speech data can be improved\nby up to 12% when adversarial loss is used in addition to the original KWS\nloss. Surprisingly, we also observed that the adversarial setup improves\naccuracy by up to 8%, even when trained solely on TTS and real negative speech\ndata, without any real positive examples.", "published": "2024-08-20 00:16:12", "link": "http://arxiv.org/abs/2408.10463v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for\n  Multimodal Emotion Recognition", "abstract": "This paper presents our winning approach for the MER-NOISE and MER-OV tracks\nof the MER2024 Challenge on multimodal emotion recognition. Our system\nleverages the advanced emotional understanding capabilities of Emotion-LLaMA to\ngenerate high-quality annotations for unlabeled samples, addressing the\nchallenge of limited labeled data. To enhance multimodal fusion while\nmitigating modality-specific noise, we introduce Conv-Attention, a lightweight\nand efficient hybrid framework. Extensive experimentation vali-dates the\neffectiveness of our approach. In the MER-NOISE track, our system achieves a\nstate-of-the-art weighted average F-score of 85.30%, surpassing the second and\nthird-place teams by 1.47% and 1.65%, respectively. For the MER-OV track, our\nutilization of Emotion-LLaMA for open-vocabulary annotation yields an 8.52%\nimprovement in average accuracy and recall compared to GPT-4V, securing the\nhighest score among all participating large multimodal models. The code and\nmodel for Emotion-LLaMA are available at\nhttps://github.com/ZebangCheng/Emotion-LLaMA.", "published": "2024-08-20 02:46:03", "link": "http://arxiv.org/abs/2408.10500v2", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "kNN Retrieval for Simple and Effective Zero-Shot Multi-speaker\n  Text-to-Speech", "abstract": "While recent zero-shot multi-speaker text-to-speech (TTS) models achieve\nimpressive results, they typically rely on extensive transcribed speech\ndatasets from numerous speakers and intricate training pipelines. Meanwhile,\nself-supervised learning (SSL) speech features have emerged as effective\nintermediate representations for TTS. Further, SSL features from different\nspeakers that are linearly close share phonetic information while maintaining\nindividual speaker identity. In this study, we introduce kNN-TTS, a simple and\neffective framework for zero-shot multi-speaker TTS using retrieval methods\nwhich leverage the linear relationships between SSL features. Objective and\nsubjective evaluations show that our models, trained on transcribed speech from\na single speaker only, achieve performance comparable to state-of-the-art\nmodels that are trained on significantly larger training datasets. The low\ntraining data requirements mean that kNN-TTS is well suited for the development\nof multi-speaker TTS systems for low-resource domains and languages. We also\nintroduce an interpolation parameter which enables fine-grained voice morphing.\nDemo samples are available at https://idiap.github.io/knn-tts", "published": "2024-08-20 12:09:58", "link": "http://arxiv.org/abs/2408.10771v3", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DisMix: Disentangling Mixtures of Musical Instruments for Source-level\n  Pitch and Timbre Manipulation", "abstract": "Existing work on pitch and timbre disentanglement has been mostly focused on\nsingle-instrument music audio, excluding the cases where multiple instruments\nare presented. To fill the gap, we propose DisMix, a generative framework in\nwhich the pitch and timbre representations act as modular building blocks for\nconstructing the melody and instrument of a source, and the collection of which\nforms a set of per-instrument latent representations underlying the observed\nmixture. By manipulating the representations, our model samples mixtures with\nnovel combinations of pitch and timbre of the constituent instruments. We can\njointly learn the disentangled pitch-timbre representations and a latent\ndiffusion transformer that reconstructs the mixture conditioned on the set of\nsource-level representations. We evaluate the model using both a simple dataset\nof isolated chords and a realistic four-part chorales in the style of J.S.\nBach, identify the key components for the success of disentanglement, and\ndemonstrate the application of mixture transformation based on source-level\nattribute manipulation.", "published": "2024-08-20 12:56:49", "link": "http://arxiv.org/abs/2408.10807v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Does Current Deepfake Audio Detection Model Effectively Detect ALM-based\n  Deepfake Audio?", "abstract": "Currently, Audio Language Models (ALMs) are rapidly advancing due to the\ndevelopments in large language models and audio neural codecs. These ALMs have\nsignificantly lowered the barrier to creating deepfake audio, generating highly\nrealistic and diverse types of deepfake audio, which pose severe threats to\nsociety. Consequently, effective audio deepfake detection technologies to\ndetect ALM-based audio have become increasingly critical. This paper\ninvestigate the effectiveness of current countermeasure (CM) against ALM-based\naudio. Specifically, we collect 12 types of the latest ALM-based deepfake audio\nand utilizing the latest CMs to evaluate. Our findings reveal that the latest\ncodec-trained CM can effectively detect ALM-based audio, achieving 0% equal\nerror rate under most ALM test conditions, which exceeded our expectations.\nThis indicates promising directions for future research in ALM-based deepfake\naudio detection.", "published": "2024-08-20 13:45:34", "link": "http://arxiv.org/abs/2408.10853v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Match Cutting: Finding and Creating Matching Audio Transitions in\n  Movies and Videos", "abstract": "A \"match cut\" is a common video editing technique where a pair of shots that\nhave a similar composition transition fluidly from one to another. Although\nmatch cuts are often visual, certain match cuts involve the fluid transition of\naudio, where sounds from different sources merge into one indistinguishable\ntransition between two shots. In this paper, we explore the ability to\nautomatically find and create \"audio match cuts\" within videos and movies. We\ncreate a self-supervised audio representation for audio match cutting and\ndevelop a coarse-to-fine audio match pipeline that recommends matching shots\nand creates the blended audio. We further annotate a dataset for the proposed\naudio match cut task and compare the ability of multiple audio representations\nto find audio match cut candidates. Finally, we evaluate multiple methods to\nblend two matching audio candidates with the goal of creating a smooth\ntransition. Project page and examples are available at:\nhttps://denfed.github.io/audiomatchcut/", "published": "2024-08-20 16:46:54", "link": "http://arxiv.org/abs/2408.10998v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
