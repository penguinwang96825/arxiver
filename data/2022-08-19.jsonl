{"title": "Discourse Cohesion Evaluation for Document-Level Neural Machine\n  Translation", "abstract": "It is well known that translations generated by an excellent document-level\nneural machine translation (NMT) model are consistent and coherent. However,\nexisting sentence-level evaluation metrics like BLEU can hardly reflect the\nmodel's performance at the document level. To tackle this issue, we propose a\nDiscourse Cohesion Evaluation Method (DCoEM) in this paper and contribute a new\ntest suite that considers four cohesive manners (reference, conjunction,\nsubstitution, and lexical cohesion) to measure the cohesiveness of document\ntranslations. The evaluation results on recent document-level NMT systems show\nthat our method is practical and essential in estimating translations at the\ndocument level.", "published": "2022-08-19 01:56:00", "link": "http://arxiv.org/abs/2208.09118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coarse-to-Fine: Hierarchical Multi-task Learning for Natural Language\n  Understanding", "abstract": "Generalized text representations are the foundation of many natural language\nunderstanding tasks. To fully utilize the different corpus, it is inevitable\nthat models need to understand the relevance among them. However, many methods\nignore the relevance and adopt a single-channel model (a coarse paradigm)\ndirectly for all tasks, which lacks enough rationality and interpretation. In\naddition, some existing works learn downstream tasks by stitches skill block(a\nfine paradigm), which might cause irrationalresults due to its redundancy and\nnoise. Inthis work, we first analyze the task correlation through three\ndifferent perspectives, i.e., data property, manual design, and model-based\nrelevance, based on which the similar tasks are grouped together. Then, we\npropose a hierarchical framework with a coarse-to-fine paradigm, with the\nbottom level shared to all the tasks, the mid-level divided to different\ngroups, and the top-level assigned to each of the tasks. This allows our model\nto learn basic language properties from all tasks, boost performance on\nrelevant tasks, and reduce the negative impact from irrelevant tasks. Our\nexperiments on 13 benchmark datasets across five natural language understanding\ntasks demonstrate the superiority of our method.", "published": "2022-08-19 02:46:20", "link": "http://arxiv.org/abs/2208.09129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniCausal: Unified Benchmark and Repository for Causal Text Mining", "abstract": "Current causal text mining datasets vary in objectives, data coverage, and\nannotation schemes. These inconsistent efforts prevent modeling capabilities\nand fair comparisons of model performance. Furthermore, few datasets include\ncause-effect span annotations, which are needed for end-to-end causal relation\nextraction. To address these issues, we propose UniCausal, a unified benchmark\nfor causal text mining across three tasks: (I) Causal Sequence Classification,\n(II) Cause-Effect Span Detection and (III) Causal Pair Classification. We\nconsolidated and aligned annotations of six high quality, mainly\nhuman-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165\nexamples for each task respectively. Since the definition of causality can be\nsubjective, our framework was designed to allow researchers to work on some or\nall datasets and tasks. To create an initial benchmark, we fine-tuned BERT\npre-trained language models to each task, achieving 70.10% Binary F1, 52.42%\nMacro F1, and 84.68% Binary F1 scores respectively.", "published": "2022-08-19 06:14:05", "link": "http://arxiv.org/abs/2208.09163v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pseudo-Labels Are All You Need", "abstract": "Automatically estimating the complexity of texts for readers has a variety of\napplications, such as recommending texts with an appropriate complexity level\nto language learners or supporting the evaluation of text simplification\napproaches. In this paper, we present our submission to the Text Complexity DE\nChallenge 2022, a regression task where the goal is to predict the complexity\nof a German sentence for German learners at level B. Our approach relies on\nmore than 220,000 pseudo-labels created from the German Wikipedia and other\ncorpora to train Transformer-based models, and refrains from any feature\nengineering or any additional, labeled data. We find that the\npseudo-label-based approach gives impressive results yet requires little to no\nadjustment to the specific task and therefore could be easily adapted to other\ndomains and tasks.", "published": "2022-08-19 09:52:41", "link": "http://arxiv.org/abs/2208.09243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UKP-SQuARE v2: Explainability and Adversarial Attacks for Trustworthy QA", "abstract": "Question Answering (QA) systems are increasingly deployed in applications\nwhere they support real-world decisions. However, state-of-the-art models rely\non deep neural networks, which are difficult to interpret by humans. Inherently\ninterpretable models or post hoc explainability methods can help users to\ncomprehend how a model arrives at its prediction and, if successful, increase\ntheir trust in the system. Furthermore, researchers can leverage these insights\nto develop new methods that are more accurate and less biased. In this paper,\nwe introduce SQuARE v2, the new version of SQuARE, to provide an explainability\ninfrastructure for comparing models based on methods such as saliency maps and\ngraph-based explanations. While saliency maps are useful to inspect the\nimportance of each input token for the model's prediction, graph-based\nexplanations from external Knowledge Graphs enable the users to verify the\nreasoning behind the model prediction. In addition, we provide multiple\nadversarial attacks to compare the robustness of QA models. With these\nexplainability methods and adversarial attacks, we aim to ease the research on\ntrustworthy QA models. SQuARE is available on https://square.ukp-lab.de.", "published": "2022-08-19 13:01:01", "link": "http://arxiv.org/abs/2208.09316v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Task-Oriented Dialogue Models for Email Conversations", "abstract": "Intent detection is a key part of any Natural Language Understanding (NLU)\nsystem of a conversational assistant. Detecting the correct intent is essential\nyet difficult for email conversations where multiple directives and intents are\npresent. In such settings, conversation context can become a key disambiguating\nfactor for detecting the user's request from the assistant. One prominent way\nof incorporating context is modeling past conversation history like\ntask-oriented dialogue models. However, the nature of email conversations (long\nform) restricts direct usage of the latest advances in task-oriented dialogue\nmodels. So in this paper, we provide an effective transfer learning framework\n(EMToD) that allows the latest development in dialogue models to be adapted for\nlong-form conversations. We show that the proposed EMToD framework improves\nintent detection performance over pre-trained language models by 45% and over\npre-trained dialogue models by 30% for task-oriented email conversations.\nAdditionally, the modular nature of the proposed framework allows plug-and-play\nfor any future developments in both pre-trained language and task-oriented\ndialogue models.", "published": "2022-08-19 16:41:34", "link": "http://arxiv.org/abs/2208.09439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Optimization for Unsupervised Extractive Summarization of Long\n  Documents with the Frank-Wolfe Algorithm", "abstract": "We address the problem of unsupervised extractive document summarization,\nespecially for long documents. We model the unsupervised problem as a sparse\nauto-regression one and approximate the resulting combinatorial problem via a\nconvex, norm-constrained problem. We solve it using a dedicated Frank-Wolfe\nalgorithm. To generate a summary with $k$ sentences, the algorithm only needs\nto execute $\\approx k$ iterations, making it very efficient. We explain how to\navoid explicit calculation of the full gradient and how to include sentence\nembedding information. We evaluate our approach against two other unsupervised\nmethods using both lexical (standard) ROUGE scores, as well as semantic\n(embedding-based) ones. Our method achieves better results with both datasets\nand works especially well when combined with embeddings for highly paraphrased\nsummaries.", "published": "2022-08-19 17:17:43", "link": "http://arxiv.org/abs/2208.09454v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Transfer Learning for Low-Resource Natural Language\n  Understanding", "abstract": "Natural language understanding (NLU) is the task of semantic decoding of\nhuman languages by machines. NLU models rely heavily on large training data to\nensure good performance. However, substantial languages and domains have very\nfew data resources and domain experts. It is necessary to overcome the data\nscarcity challenge, when very few or even zero training samples are available.\nIn this thesis, we focus on developing cross-lingual and cross-domain methods\nto tackle the low-resource issues. First, we propose to improve the model's\ncross-lingual ability by focusing on the task-related keywords, enhancing the\nmodel's robustness and regularizing the representations. We find that the\nrepresentations for low-resource languages can be easily and greatly improved\nby focusing on just the keywords. Second, we present Order-Reduced Modeling\nmethods for the cross-lingual adaptation, and find that modeling partial word\norders instead of the whole sequence can improve the robustness of the model\nagainst word order differences between languages and task knowledge transfer to\nlow-resource languages. Third, we propose to leverage different levels of\ndomain-related corpora and additional masking of data in the pre-training for\nthe cross-domain adaptation, and discover that more challenging pre-training\ncan better address the domain discrepancy issue in the task knowledge transfer.\nFinally, we introduce a coarse-to-fine framework, Coach, and a cross-lingual\nand cross-domain parsing framework, X2Parser. Coach decomposes the\nrepresentation learning process into a coarse-grained and a fine-grained\nfeature learning, and X2Parser simplifies the hierarchical task structures into\nflattened ones. We observe that simplifying task structures makes the\nrepresentation learning more effective for low-resource languages and domains.", "published": "2022-08-19 06:59:00", "link": "http://arxiv.org/abs/2208.09180v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Text Generation: Supporting Writers with Continuous Automatic\n  Text Summaries", "abstract": "We propose a text editor to help users plan, structure and reflect on their\nwriting process. It provides continuously updated paragraph-wise summaries as\nmargin annotations, using automatic text summarization. Summary levels range\nfrom full text, to selected (central) sentences, down to a collection of\nkeywords. To understand how users interact with this system during writing, we\nconducted two user studies (N=4 and N=8) in which people wrote analytic essays\nabout a given topic and article. As a key finding, the summaries gave users an\nexternal perspective on their writing and helped them to revise the content and\nscope of their drafted paragraphs. People further used the tool to quickly gain\nan overview of the text and developed strategies to integrate insights from the\nautomated summaries. More broadly, this work explores and highlights the value\nof designing AI tools for writers, with Natural Language Processing (NLP)\ncapabilities that go beyond direct text generation and correction.", "published": "2022-08-19 13:09:56", "link": "http://arxiv.org/abs/2208.09323v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Causal Intervention Improves Implicit Sentiment Analysis", "abstract": "Despite having achieved great success for sentiment analysis, existing neural\nmodels struggle with implicit sentiment analysis. This may be due to the fact\nthat they may latch onto spurious correlations (\"shortcuts\", e.g., focusing\nonly on explicit sentiment words), resulting in undermining the effectiveness\nand robustness of the learned model. In this work, we propose a causal\nintervention model for Implicit Sentiment Analysis using Instrumental Variable\n(ISAIV). We first review sentiment analysis from a causal perspective and\nanalyze the confounders existing in this task. Then, we introduce an\ninstrumental variable to eliminate the confounding causal effects, thus\nextracting the pure causal effect between sentence and sentiment. We compare\nthe proposed ISAIV model with several strong baselines on both the general\nimplicit sentiment analysis and aspect-based implicit sentiment analysis tasks.\nThe results indicate the great advantages of our model and the efficacy of\nimplicit sentiment reasoning.", "published": "2022-08-19 13:17:57", "link": "http://arxiv.org/abs/2208.09329v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-end Clinical Event Extraction from Chinese Electronic Health\n  Record", "abstract": "Event extraction is an important work of medical text processing. According\nto the complex characteristics of medical text annotation, we use the\nend-to-end event extraction model to enhance the output formatting information\nof events. Through pre training and fine-tuning, we can extract the attributes\nof the four dimensions of medical text: anatomical position, subject word,\ndescription word and occurrence state. On the test set, the accuracy rate was\n0.4511, the recall rate was 0.3928, and the F1 value was 0.42. The method of\nthis model is simple, and it has won the second place in the task of mining\nclinical discovery events (task2) in the Chinese electronic medical record of\nthe seventh China health information processing Conference (chip2021).", "published": "2022-08-19 14:06:20", "link": "http://arxiv.org/abs/2208.09354v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialogue Policies for Confusion Mitigation in Situated HRI", "abstract": "Confusion is a mental state triggered by cognitive disequilibrium that can\noccur in many types of task-oriented interaction, including Human-Robot\nInteraction (HRI). People may become confused while interacting with robots due\nto communicative or even task-centred challenges. To build a smooth and\nengaging HRI, it is insufficient for an agent to simply detect confusion;\ninstead, the system should aim to mitigate the situation. In light of this, in\nthis paper, we present our approach to a linguistic design of dialogue policies\nto build a dialogue framework to alleviate interlocutor confusion. We also\noutline our sketch and discuss challenges with respect to its\noperationalisation.", "published": "2022-08-19 14:28:13", "link": "http://arxiv.org/abs/2208.09367v1", "categories": ["cs.CL", "cs.HC", "C.2; F.4"], "primary_category": "cs.CL"}
{"title": "Graph-Augmented Cyclic Learning Framework for Similarity Estimation of\n  Medical Clinical Notes", "abstract": "Semantic textual similarity (STS) in the clinical domain helps improve\ndiagnostic efficiency and produce concise texts for downstream data mining\ntasks. However, given the high degree of domain knowledge involved in clinic\ntext, it remains challenging for general language models to infer implicit\nmedical relationships behind clinical sentences and output similarities\ncorrectly. In this paper, we present a graph-augmented cyclic learning\nframework for similarity estimation in the clinical domain. The framework can\nbe conveniently implemented on a state-of-art backbone language model, and\nimprove its performance by leveraging domain knowledge through co-training with\nan auxiliary graph convolution network (GCN) based network. We report the\nsuccess of introducing domain knowledge in GCN and the co-training framework by\nimproving the Bio-clinical BERT baseline by 16.3% and 27.9%, respectively.", "published": "2022-08-19 16:34:41", "link": "http://arxiv.org/abs/2208.09437v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SimLDA: A tool for topic model evaluation", "abstract": "Variational Bayes (VB) applied to latent Dirichlet allocation (LDA) has\nbecome the most popular algorithm for aspect modeling. While sufficiently\nsuccessful in text topic extraction from large corpora, VB is less successful\nin identifying aspects in the presence of limited data. We present a novel\nvariational message passing algorithm as applied to Latent Dirichlet Allocation\n(LDA) and compare it with the gold standard VB and collapsed Gibbs sampling. In\nsituations where marginalisation leads to non-conjugate messages, we use ideas\nfrom sampling to derive approximate update equations. In cases where conjugacy\nholds, Loopy Belief update (LBU) (also known as Lauritzen-Spiegelhalter) is\nused. Our algorithm, ALBU (approximate LBU), has strong similarities with\nVariational Message Passing (VMP) (which is the message passing variant of VB).\nTo compare the performance of the algorithms in the presence of limited data,\nwe use data sets consisting of tweets and news groups. Using coherence measures\nwe show that ALBU learns latent distributions more accurately than does VB,\nespecially for smaller data sets.", "published": "2022-08-19 12:25:53", "link": "http://arxiv.org/abs/2208.09299v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Text to Image Generation: Leaving no Language Behind", "abstract": "One of the latest applications of Artificial Intelligence (AI) is to generate\nimages from natural language descriptions. These generators are now becoming\navailable and achieve impressive results that have been used for example in the\nfront cover of magazines. As the input to the generators is in the form of a\nnatural language text, a question that arises immediately is how these models\nbehave when the input is written in different languages. In this paper we\nperform an initial exploration of how the performance of three popular\ntext-to-image generators depends on the language. The results show that there\nis a significant performance degradation when using languages other than\nEnglish, especially for languages that are not widely used. This observation\nleads us to discuss different alternatives on how text-to-image generators can\nbe improved so that performance is consistent across different languages. This\nis fundamental to ensure that this new technology can be used by non-native\nEnglish speakers and to preserve linguistic diversity.", "published": "2022-08-19 13:24:56", "link": "http://arxiv.org/abs/2208.09333v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Gender Bias and Universal Substitution Adversarial Attacks on\n  Grammatical Error Correction Systems for Automated Assessment", "abstract": "Grammatical Error Correction (GEC) systems perform a sequence-to-sequence\ntask, where an input word sequence containing grammatical errors, is corrected\nfor these errors by the GEC system to output a grammatically correct word\nsequence. With the advent of deep learning methods, automated GEC systems have\nbecome increasingly popular. For example, GEC systems are often used on speech\ntranscriptions of English learners as a form of assessment and feedback - these\npowerful GEC systems can be used to automatically measure an aspect of a\ncandidate's fluency. The count of \\textit{edits} from a candidate's input\nsentence (or essay) to a GEC system's grammatically corrected output sentence\nis indicative of a candidate's language ability, where fewer edits suggest\nbetter fluency. The count of edits can thus be viewed as a \\textit{fluency\nscore} with zero implying perfect fluency. However, although deep learning\nbased GEC systems are extremely powerful and accurate, they are susceptible to\nadversarial attacks: an adversary can introduce a small, specific change at the\ninput of a system that causes a large, undesired change at the output. When\nconsidering the application of GEC systems to automated language assessment,\nthe aim of an adversary could be to cheat by making a small change to a\ngrammatically incorrect input sentence that conceals the errors from a GEC\nsystem, such that no edits are found and the candidate is unjustly awarded a\nperfect fluency score. This work examines a simple universal substitution\nadversarial attack that non-native speakers of English could realistically\nemploy to deceive GEC systems used for assessment.", "published": "2022-08-19 17:44:13", "link": "http://arxiv.org/abs/2208.09466v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A decomposition of book structure through ousiometric fluctuations in\n  cumulative word-time", "abstract": "While quantitative methods have been used to examine changes in word usage in\nbooks, studies have focused on overall trends, such as the shapes of\nnarratives, which are independent of book length. We instead look at how words\nchange over the course of a book as a function of the number of words, rather\nthan the fraction of the book, completed at any given point; we define this\nmeasure as \"cumulative word-time\". Using ousiometrics, a reinterpretation of\nthe valence-arousal-dominance framework of meaning obtained from semantic\ndifferentials, we convert text into time series of power and danger scores in\ncumulative word-time. Each time series is then decomposed using empirical mode\ndecomposition into a sum of constituent oscillatory modes and a non-oscillatory\ntrend. By comparing the decomposition of the original power and danger time\nseries with those derived from shuffled text, we find that shorter books\nexhibit only a general trend, while longer books have fluctuations in addition\nto the general trend. These fluctuations typically have a period of a few\nthousand words regardless of the book length or library classification code,\nbut vary depending on the content and structure of the book. Our findings\nsuggest that, in the ousiometric sense, longer books are not expanded versions\nof shorter books, but are more similar in structure to a concatenation of\nshorter texts. Further, they are consistent with editorial practices that\nrequire longer texts to be broken down into sections, such as chapters. Our\nmethod also provides a data-driven denoising approach that works for texts of\nvarious lengths, in contrast to the more traditional approach of using large\nwindow sizes that may inadvertently smooth out relevant information, especially\nfor shorter texts. These results open up avenues for future work in\ncomputational literary analysis, particularly the measurement of a basic unit\nof narrative.", "published": "2022-08-19 18:17:27", "link": "http://arxiv.org/abs/2208.09496v4", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Searching for Structure in Unfalsifiable Claims", "abstract": "Social media platforms give rise to an abundance of posts and comments on\nevery topic imaginable. Many of these posts express opinions on various aspects\nof society, but their unfalsifiable nature makes them ill-suited to\nfact-checking pipelines. In this work, we aim to distill such posts into a\nsmall set of narratives that capture the essential claims related to a given\ntopic. Understanding and visualizing these narratives can facilitate more\ninformed debates on social media. As a first step towards systematically\nidentifying the underlying narratives on social media, we introduce PAPYER, a\nfine-grained dataset of online comments related to hygiene in public restrooms,\nwhich contains a multitude of unfalsifiable claims. We present a\nhuman-in-the-loop pipeline that uses a combination of machine and human kernels\nto discover the prevailing narratives and show that this pipeline outperforms\nrecent large transformer models and state-of-the-art unsupervised topic models.", "published": "2022-08-19 13:32:15", "link": "http://arxiv.org/abs/2209.00495v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "3M: An Effective Multi-view, Multi-granularity, and Multi-aspect\n  Modeling Approach to English Pronunciation Assessment", "abstract": "As an indispensable ingredient of computer-assisted pronunciation training\n(CAPT), automatic pronunciation assessment (APA) plays a pivotal role in aiding\nself-directed language learners by providing multi-aspect and timely feedback.\nHowever, there are at least two potential obstacles that might hinder its\nperformance for practical use. On one hand, most of the studies focus\nexclusively on leveraging segmental (phonetic)-level features such as goodness\nof pronunciation (GOP); this, however, may cause a discrepancy of feature\ngranularity when performing suprasegmental (prosodic)-level pronunciation\nassessment. On the other hand, automatic pronunciation assessments still suffer\nfrom the lack of large-scale labeled speech data of non-native speakers, which\ninevitably limits the performance of pronunciation assessment. In this paper,\nwe tackle these problems by integrating multiple prosodic and phonological\nfeatures to provide a multi-view, multi-granularity, and multi-aspect (3M)\npronunciation modeling. Specifically, we augment GOP with prosodic and\nself-supervised learning (SSL) features, and meanwhile develop a\nvowel/consonant positional embedding for a more phonology-aware automatic\npronunciation assessment. A series of experiments conducted on the\npublicly-available speechocean762 dataset show that our approach can obtain\nsignificant improvements on several assessment granularities in comparison with\nprevious work, especially on the assessment of speaking fluency and speech\nprosody.", "published": "2022-08-19 01:24:00", "link": "http://arxiv.org/abs/2208.09110v3", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Improving Post-Processing of Audio Event Detectors Using Reinforcement\n  Learning", "abstract": "We apply post-processing to the class probability distribution outputs of\naudio event classification models and employ reinforcement learning to jointly\ndiscover the optimal parameters for various stages of a post-processing stack,\nsuch as the classification thresholds and the kernel sizes of median filtering\nalgorithms used to smooth out model predictions. To achieve this we define a\nreinforcement learning environment where: 1) a state is the class probability\ndistribution provided by the model for a given audio sample, 2) an action is\nthe choice of a candidate optimal value for each parameter of the\npost-processing stack, 3) the reward is based on the classification accuracy\nmetric we aim to optimize, which is the audio event-based macro F1-score in our\ncase. We apply our post-processing to the class probability distribution\noutputs of two audio event classification models submitted to the DCASE Task4\n2020 challenge. We find that by using reinforcement learning to discover the\noptimal per-class parameters for the post-processing stack that is applied to\nthe outputs of audio event classification models, we can improve the audio\nevent-based macro F1-score (the main metric used in the DCASE challenge to\ncompare audio event classification accuracy) by 4-5% compared to using the same\npost-processing stack with manually tuned parameters.", "published": "2022-08-19 08:00:26", "link": "http://arxiv.org/abs/2208.09201v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Feature Selection Enhancement and Feature Space Visualization for\n  Speech-Based Emotion Recognition", "abstract": "Robust speech emotion recognition relies on the quality of the speech\nfeatures. We present speech features enhancement strategy that improves speech\nemotion recognition. We used the INTERSPEECH 2010 challenge feature-set. We\nidentified subsets from the features set and applied Principle Component\nAnalysis to the subsets. Finally, the features are fused horizontally. The\nresulting feature set is analyzed using t-distributed neighbour embeddings\n(t-SNE) before the application of features for emotion recognition. The method\nis compared with the state-of-the-art methods used in the literature. The\nempirical evidence is drawn using two well-known datasets: Emotional Speech\nDataset (EMO-DB) and Ryerson Audio-Visual Database of Emotional Speech and Song\n(RAVDESS) for two languages, German and English, respectively. Our method\nachieved an average recognition gain of 11.5\\% for six out of seven emotions\nfor the EMO-DB dataset, and 13.8\\% for seven out of eight emotions for the\nRAVDESS dataset as compared to the baseline study.", "published": "2022-08-19 11:29:03", "link": "http://arxiv.org/abs/2208.09269v1", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
