{"title": "A Self-Training Method for Machine Reading Comprehension with Soft\n  Evidence Extraction", "abstract": "Neural models have achieved great success on machine reading comprehension\n(MRC), many of which typically consist of two components: an evidence extractor\nand an answer predictor. The former seeks the most relevant information from a\nreference text, while the latter is to locate or generate answers from the\nextracted evidence. Despite the importance of evidence labels for training the\nevidence extractor, they are not cheaply accessible, particularly in many\nnon-extractive MRC tasks such as YES/NO question answering and multi-choice\nMRC.\n  To address this problem, we present a Self-Training method (STM), which\nsupervises the evidence extractor with auto-generated evidence labels in an\niterative process. At each iteration, a base MRC model is trained with golden\nanswers and noisy evidence labels. The trained model will predict pseudo\nevidence labels as extra supervision in the next iteration. We evaluate STM on\nseven datasets over three MRC tasks. Experimental results demonstrate the\nimprovement on existing MRC models, and we also analyze how and why such a\nself-training method works in MRC. The source code can be obtained from\nhttps://github.com/SparkJiao/Self-Training-MRC", "published": "2020-05-11 15:26:07", "link": "http://arxiv.org/abs/2005.05189v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Better Storylines with Sentence-Level Language Models", "abstract": "We propose a sentence-level language model which selects the next sentence in\na story from a finite set of fluent alternatives. Since it does not need to\nmodel fluency, the sentence-level language model can focus on longer range\ndependencies, which are crucial for multi-sentence coherence. Rather than\ndealing with individual words, our method treats the story so far as a list of\npre-trained sentence embeddings and predicts an embedding for the next\nsentence, which is more efficient than predicting word embeddings. Notably this\nallows us to consider a large number of candidates for the next sentence during\ntraining. We demonstrate the effectiveness of our approach with\nstate-of-the-art accuracy on the unsupervised Story Cloze task and with\npromising results on larger-scale next sentence prediction tasks.", "published": "2020-05-11 16:54:19", "link": "http://arxiv.org/abs/2005.05255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset for Statutory Reasoning in Tax Law Entailment and Question\n  Answering", "abstract": "Legislation can be viewed as a body of prescriptive rules expressed in\nnatural language. The application of legislation to facts of a case we refer to\nas statutory reasoning, where those facts are also expressed in natural\nlanguage. Computational statutory reasoning is distinct from most existing work\nin machine reading, in that much of the information needed for deciding a case\nis declared exactly once (a law), while the information needed in much of\nmachine reading tends to be learned through distributional language statistics.\nTo investigate the performance of natural language understanding approaches on\nstatutory reasoning, we introduce a dataset, together with a legal-domain text\ncorpus. Straightforward application of machine reading models exhibits low\nout-of-the-box performance on our questions, whether or not they have been\nfine-tuned to the legal domain. We contrast this with a hand-constructed\nProlog-based system, designed to fully solve the task. These experiments\nsupport a discussion of the challenges facing statutory reasoning moving\nforward, which we argue is an interesting real-world task that can motivate the\ndevelopment of models able to utilize prescriptive rules specified in natural\nlanguage.", "published": "2020-05-11 16:54:42", "link": "http://arxiv.org/abs/2005.05257v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multidirectional Associative Optimization of Function-Specific Word\n  Representations", "abstract": "We present a neural framework for learning associations between interrelated\ngroups of words such as the ones found in Subject-Verb-Object (SVO) structures.\nOur model induces a joint function-specific word vector space, where vectors of\ne.g. plausible SVO compositions lie close together. The model retains\ninformation about word group membership even in the joint space, and can\nthereby effectively be applied to a number of tasks reasoning over the SVO\nstructure. We show the robustness and versatility of the proposed framework by\nreporting state-of-the-art results on the tasks of estimating selectional\npreference and event similarity. The results indicate that the combinations of\nrepresentations learned with our task-independent model outperform\ntask-specific architectures from prior work, while reducing the number of\nparameters by up to 95%.", "published": "2020-05-11 17:07:20", "link": "http://arxiv.org/abs/2005.05264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Segmenting Scientific Abstracts into Discourse Categories: A Deep\n  Learning-Based Approach for Sparse Labeled Data", "abstract": "The abstract of a scientific paper distills the contents of the paper into a\nshort paragraph. In the biomedical literature, it is customary to structure an\nabstract into discourse categories like BACKGROUND, OBJECTIVE, METHOD, RESULT,\nand CONCLUSION, but this segmentation is uncommon in other fields like computer\nscience. Explicit categories could be helpful for more granular, that is,\ndiscourse-level search and recommendation. The sparsity of labeled data makes\nit challenging to construct supervised machine learning solutions for automatic\ndiscourse-level segmentation of abstracts in non-bio domains. In this paper, we\naddress this problem using transfer learning. In particular, we define three\ndiscourse categories BACKGROUND, TECHNIQUE, OBSERVATION-for an abstract because\nthese three categories are the most common. We train a deep neural network on\nstructured abstracts from PubMed, then fine-tune it on a small hand-labeled\ncorpus of computer science papers. We observe an accuracy of 75% on the test\ncorpus. We perform an ablation study to highlight the roles of the different\nparts of the model. Our method appears to be a promising solution to the\nautomatic segmentation of abstracts, where the labeled data is sparse.", "published": "2020-05-11 20:21:25", "link": "http://arxiv.org/abs/2005.05414v2", "categories": ["cs.CL", "I.5.1; H.3.7"], "primary_category": "cs.CL"}
{"title": "Neural Polysynthetic Language Modelling", "abstract": "Research in natural language processing commonly assumes that approaches that\nwork well for English and and other widely-used languages are \"language\nagnostic\". In high-resource languages, especially those that are analytic, a\ncommon approach is to treat morphologically-distinct variants of a common root\nas completely independent word types. This assumes, that there are limited\nmorphological inflections per root, and that the majority will appear in a\nlarge enough corpus, so that the model can adequately learn statistics about\neach form. Approaches like stemming, lemmatization, or subword segmentation are\noften used when either of those assumptions do not hold, particularly in the\ncase of synthetic languages like Spanish or Russian that have more inflection\nthan English.\n  In the literature, languages like Finnish or Turkish are held up as extreme\nexamples of complexity that challenge common modelling assumptions. Yet, when\nconsidering all of the world's languages, Finnish and Turkish are closer to the\naverage case. When we consider polysynthetic languages (those at the extreme of\nmorphological complexity), approaches like stemming, lemmatization, or subword\nmodelling may not suffice. These languages have very high numbers of hapax\nlegomena, showing the need for appropriate morphological handling of words,\nwithout which it is not possible for a model to capture enough word statistics.\n  We examine the current state-of-the-art in language modelling, machine\ntranslation, and text prediction for four polysynthetic languages: Guaran\\'i,\nSt. Lawrence Island Yupik, Central Alaskan Yupik, and Inuktitut. We then\npropose a novel framework for language modelling that combines knowledge\nrepresentations from finite-state morphological analyzers with Tensor Product\nRepresentations in order to enable neural language models capable of handling\nthe full range of typologically variant languages.", "published": "2020-05-11 22:57:04", "link": "http://arxiv.org/abs/2005.05477v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schema-Guided Natural Language Generation", "abstract": "Neural network based approaches to data-to-text natural language generation\n(NLG) have gained popularity in recent years, with the goal of generating a\nnatural language prompt that accurately realizes an input meaning\nrepresentation. To facilitate the training of neural network models,\nresearchers created large datasets of paired utterances and their meaning\nrepresentations. However, the creation of such datasets is an arduous task and\nthey mostly consist of simple meaning representations composed of slot and\nvalue tokens to be realized. These representations do not include any\ncontextual information that an NLG system can use when trying to generalize,\nsuch as domain information and descriptions of slots and values. In this paper,\nwe present the novel task of Schema-Guided Natural Language Generation\n(SG-NLG). Here, the goal is still to generate a natural language prompt, but in\nSG-NLG, the input MRs are paired with rich schemata providing contextual\ninformation. To generate a dataset for SG-NLG we re-purpose an existing dataset\nfor another task: dialog state tracking, which includes a large and rich schema\nspanning multiple different attributes, including information about the domain,\nuser intent, and slot descriptions. We train different state-of-the-art models\nfor neural natural language generation on this dataset and show that in many\ncases, including rich schema information allows our models to produce higher\nquality outputs both in terms of semantics and diversity. We also conduct\nexperiments comparing model performance on seen versus unseen domains, and\npresent a human evaluation demonstrating high ratings for overall output\nquality.", "published": "2020-05-11 23:01:22", "link": "http://arxiv.org/abs/2005.05480v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Monolingual Data with Self-Supervision for Multilingual\n  Neural Machine Translation", "abstract": "Over the last few years two promising research directions in low-resource\nneural machine translation (NMT) have emerged. The first focuses on utilizing\nhigh-resource languages to improve the quality of low-resource languages via\nmultilingual NMT. The second direction employs monolingual data with\nself-supervision to pre-train translation models, followed by fine-tuning on\nsmall amounts of supervised data. In this work, we join these two lines of\nresearch and demonstrate the efficacy of monolingual data with self-supervision\nin multilingual NMT. We offer three major results: (i) Using monolingual data\nsignificantly boosts the translation quality of low-resource languages in\nmultilingual models. (ii) Self-supervision improves zero-shot translation\nquality in multilingual models. (iii) Leveraging monolingual data with\nself-supervision provides a viable path towards adding new languages to\nmultilingual models, getting up to 33 BLEU on ro-en translation without any\nparallel data or back-translation.", "published": "2020-05-11 00:20:33", "link": "http://arxiv.org/abs/2005.04816v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Listen Attentively, and Spell Once: Whole Sentence Generation via a\n  Non-Autoregressive Architecture for Low-Latency Speech Recognition", "abstract": "Although attention based end-to-end models have achieved promising\nperformance in speech recognition, the multi-pass forward computation in\nbeam-search increases inference time cost, which limits their practical\napplications. To address this issue, we propose a non-autoregressive end-to-end\nspeech recognition system called LASO (listen attentively, and spell once).\nBecause of the non-autoregressive property, LASO predicts a textual token in\nthe sequence without the dependence on other tokens. Without beam-search, the\none-pass propagation much reduces inference time cost of LASO. And because the\nmodel is based on the attention based feedforward structure, the computation\ncan be implemented in parallel efficiently. We conduct experiments on publicly\navailable Chinese dataset AISHELL-1. LASO achieves a character error rate of\n6.4%, which outperforms the state-of-the-art autoregressive transformer model\n(6.7%). The average inference latency is 21 ms, which is 1/50 of the\nautoregressive transformer model.", "published": "2020-05-11 04:45:02", "link": "http://arxiv.org/abs/2005.04862v4", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Towards logical negation for compositional distributional semantics", "abstract": "The categorical compositional distributional model of meaning gives the\ncomposition of words into phrases and sentences pride of place. However, it has\nso far lacked a model of logical negation. This paper gives some steps towards\nproviding this operator, modelling it as a version of projection onto the\nsubspace orthogonal to a word. We give a small demonstration of the operators\nperformance in a sentence entailment task.", "published": "2020-05-11 08:51:30", "link": "http://arxiv.org/abs/2005.04929v1", "categories": ["cs.CL", "math.CT"], "primary_category": "cs.CL"}
{"title": "A Deep Learning Approach for Automatic Detection of Fake News", "abstract": "Fake news detection is a very prominent and essential task in the field of\njournalism. This challenging problem is seen so far in the field of politics,\nbut it could be even more challenging when it is to be determined in the\nmulti-domain platform. In this paper, we propose two effective models based on\ndeep learning for solving fake news detection problem in online news contents\nof multiple domains. We evaluate our techniques on the two recently released\ndatasets, namely FakeNews AMT and Celebrity for fake news detection. The\nproposed systems yield encouraging performance, outperforming the current\nhandcrafted feature engineering based state-of-the-art system with a\nsignificant margin of 3.08% and 9.3% by the two models, respectively. In order\nto exploit the datasets, available for the related tasks, we perform\ncross-domain analysis (i.e. model trained on FakeNews AMT and tested on\nCelebrity and vice versa) to explore the applicability of our systems across\nthe domains.", "published": "2020-05-11 09:07:46", "link": "http://arxiv.org/abs/2005.04938v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Sparse Interpretable Word Embeddings for Biomedical Domain", "abstract": "Word embeddings have found their way into a wide range of natural language\nprocessing tasks including those in the biomedical domain. While these vector\nrepresentations successfully capture semantic and syntactic word relations,\nhidden patterns and trends in the data, they fail to offer interpretability.\nInterpretability is a key means to justification which is an integral part when\nit comes to biomedical applications. We present an inclusive study on\ninterpretability of word embeddings in the medical domain, focusing on the role\nof sparse methods. Qualitative and quantitative measurements and metrics for\ninterpretability of word vector representations are provided. For the\nquantitative evaluation, we introduce an extensive categorized dataset that can\nbe used to quantify interpretability based on category theory. Intrinsic and\nextrinsic evaluation of the studied methods are also presented. As for the\nlatter, we propose datasets which can be utilized for effective extrinsic\nevaluation of word vectors in the biomedical domain. Based on our experiments,\nit is seen that sparse word vectors show far more interpretability while\npreserving the performance of their original vectors in downstream tasks.", "published": "2020-05-11 13:56:58", "link": "http://arxiv.org/abs/2005.05114v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Commonsense Evidence Generation and Injection in Reading Comprehension", "abstract": "Human tackle reading comprehension not only based on the given context itself\nbut often rely on the commonsense beyond. To empower the machine with\ncommonsense reasoning, in this paper, we propose a Commonsense Evidence\nGeneration and Injection framework in reading comprehension, named CEGI. The\nframework injects two kinds of auxiliary commonsense evidence into\ncomprehensive reading to equip the machine with the ability of rational\nthinking. Specifically, we build two evidence generators: the first generator\naims to generate textual evidence via a language model; the other generator\naims to extract factual evidence (automatically aligned text-triples) from a\ncommonsense knowledge graph after graph completion. Those evidences incorporate\ncontextual commonsense and serve as the additional inputs to the model.\nThereafter, we propose a deep contextual encoder to extract semantic\nrelationships among the paragraph, question, option, and evidence. Finally, we\nemploy a capsule network to extract different linguistic units (word and\nphrase) from the relations, and dynamically predict the optimal option based on\nthe extracted units. Experiments on the CosmosQA dataset demonstrate that the\nproposed CEGI model outperforms the current state-of-the-art approaches and\nachieves the accuracy (83.6%) on the leaderboard.", "published": "2020-05-11 16:31:08", "link": "http://arxiv.org/abs/2005.05240v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "On the Generation of Medical Dialogues for COVID-19", "abstract": "Under the pandemic of COVID-19, people experiencing COVID19-related symptoms\nor exposed to risk factors have a pressing need to consult doctors. Due to\nhospital closure, a lot of consulting services have been moved online. Because\nof the shortage of medical professionals, many people cannot receive online\nconsultations timely. To address this problem, we aim to develop a medical\ndialogue system that can provide COVID19-related consultations. We collected\ntwo dialogue datasets -- CovidDialog -- (in English and Chinese respectively)\ncontaining conversations between doctors and patients about COVID-19. On these\ntwo datasets, we train several dialogue generation models based on Transformer,\nGPT, and BERT-GPT. Since the two COVID-19 dialogue datasets are small in size,\nwhich bear high risk of overfitting, we leverage transfer learning to mitigate\ndata deficiency. Specifically, we take the pretrained models of Transformer,\nGPT, and BERT-GPT on dialog datasets and other large-scale texts, then finetune\nthem on our CovidDialog tasks. We perform both automatic and human evaluation\nof responses generated by these models. The results show that the generated\nresponses are promising in being doctor-like, relevant to the conversation\nhistory, and clinically informative. The data and code are available at\nhttps://github.com/UCSD-AI4H/COVID-Dialogue.", "published": "2020-05-11 21:23:43", "link": "http://arxiv.org/abs/2005.05442v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Luganda Text-to-Speech Machine", "abstract": "In Uganda, Luganda is the most spoken native language. It is used for\ncommunication in informal as well as formal business transactions. The\ndevelopment of technology startups globally related to TTS has mainly been with\nlanguages like English, French, etc. These are added in TTS engines by Google,\nMicrosoft among others, allowing developers in these regions to innovate TTS\nproducts. Luganda is not supported because the language is not built and\ntrained on these engines. In this study, we analyzed the Luganda language\nstructure and constructions and then proposed and developed a Luganda TTS. The\nsystem was built and trained using locally sourced Luganda language text and\naudio. The engine is now able to capture text and reads it aloud. We tested the\naccuracy using MRT and MOS. MRT and MOS tests results are quite good with MRT\nhaving better results. The results general score was 71%. This study will\nenhance previous solutions to NLP gaps in Uganda, as well as provide raw data\nsuch that other research in this area can take place.", "published": "2020-05-11 21:33:33", "link": "http://arxiv.org/abs/2005.05447v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SCAT: Second Chance Autoencoder for Textual Data", "abstract": "We present a k-competitive learning approach for textual autoencoders named\nSecond Chance Autoencoder (SCAT). SCAT selects the $k$ largest and smallest\npositive activations as the winner neurons, which gain the activation values of\nthe loser neurons during the learning process, and thus focus on retrieving\nwell-representative features for topics. Our experiments show that SCAT\nachieves outstanding performance in classification, topic modeling, and\ndocument visualization compared to LDA, K-Sparse, NVCTM, and KATE.", "published": "2020-05-11 19:04:31", "link": "http://arxiv.org/abs/2005.06632v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian\n  Portuguese", "abstract": "Speech provides a natural way for human-computer interaction. In particular,\nspeech synthesis systems are popular in different applications, such as\npersonal assistants, GPS applications, screen readers and accessibility tools.\nHowever, not all languages are on the same level when in terms of resources and\nsystems for speech synthesis. This work consists of creating publicly available\nresources for Brazilian Portuguese in the form of a novel dataset along with\ndeep learning models for end-to-end speech synthesis. Such dataset has 10.5\nhours from a single speaker, from which a Tacotron 2 model with the RTISI-LA\nvocoder presented the best performance, achieving a 4.03 MOS value. The\nobtained results are comparable to related works covering English language and\nthe state-of-the-art in Portuguese.", "published": "2020-05-11 14:36:44", "link": "http://arxiv.org/abs/2005.05144v4", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Reinforced Rewards Framework for Text Style Transfer", "abstract": "Style transfer deals with the algorithms to transfer the stylistic properties\nof a piece of text into that of another while ensuring that the core content is\npreserved. There has been a lot of interest in the field of text style transfer\ndue to its wide application to tailored text generation. Existing works\nevaluate the style transfer models based on content preservation and transfer\nstrength. In this work, we propose a reinforcement learning based framework\nthat directly rewards the framework on these target metrics yielding a better\ntransfer of the target style. We show the improved performance of our proposed\nframework based on automatic and human evaluation on three independent tasks:\nwherein we transfer the style of text from formal to informal, high excitement\nto low excitement, modern English to Shakespearean English, and vice-versa in\nall the three cases. Improved performance of the proposed framework over\nexisting state-of-the-art frameworks indicates the viability of the approach.", "published": "2020-05-11 16:54:28", "link": "http://arxiv.org/abs/2005.05256v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Safari of Update Structures: Visiting the Lens and Quantum\n  Enclosures", "abstract": "We build upon our recently introduced concept of an update structure to show\nthat it is a generalisation of very-well-behaved lenses, that is, there is a\nbijection between a strict subset of update structures and vwb lenses in\ncartesian categories. We show that update structures are also sufficiently\ngeneral to capture quantum observables, pinpointing the additional assumptions\nrequired to make the two coincide. In doing so, we shift the focus from special\ncommutative dagger-Frobenius algebras to interacting (co)magma (co)module\npairs, showing that the algebraic properties of the (co)multiplication arise\nfrom the module-comodule interaction, rather than direct assumptions about the\nmagma-comagma pair. We then begin to investigate the zoo of possible update\nstructures, introducing the notions of classical security-flagged databases,\nand databases of quantum systems. This work is of foundational interest as\nupdate structures place previously distinct areas of research in a general\nclass of operationally motivated structures, we expect the taming of this class\nto illuminate novel relationships between separately studied topics in computer\nscience, physics and mathematics.", "published": "2020-05-11 17:51:38", "link": "http://arxiv.org/abs/2005.05293v3", "categories": ["quant-ph", "cs.CL", "math.CT"], "primary_category": "quant-ph"}
{"title": "SOLOIST: Building Task Bots at Scale with Transfer Learning and Machine\n  Teaching", "abstract": "We present a new method SOLOIST that uses transfer learning and machine\nteaching to build task bots at scale. We parameterize classical modular\ntask-oriented dialog systems using a Transformer-based auto-regressive language\nmodel, which subsumes different dialog modules into a single neural model. We\npre-train, on heterogeneous dialog corpora, a task-grounded response generation\nmodel, which can generate dialog responses grounded in user goals and\nreal-world knowledge for task completion. The pre-trained model can be\nefficiently adapted to accomplish new tasks with a handful of task-specific\ndialogs via machine teaching, where training samples are generated by human\nteachers interacting with the system. Experiments show that (i) SOLOIST creates\nnew state-of-the-art on well-studied task-oriented dialog benchmarks, including\nCamRest676 and MultiWOZ; (ii) in the few-shot fine-tuning settings, SOLOIST\nsignificantly outperforms existing methods, and (iii) the use of machine\nteaching substantially reduces the labeling cost of fine-tuning. The\npre-trained models and codes are available at https://aka.ms/soloist.", "published": "2020-05-11 17:58:34", "link": "http://arxiv.org/abs/2005.05298v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enabling Language Models to Fill in the Blanks", "abstract": "We present a simple approach for text infilling, the task of predicting\nmissing spans of text at any position in a document. While infilling could\nenable rich functionality especially for writing assistance tools, more\nattention has been devoted to language modeling---a special case of infilling\nwhere text is predicted at the end of a document. In this paper, we aim to\nextend the capabilities of language models (LMs) to the more general task of\ninfilling. To this end, we train (or fine-tune) off-the-shelf LMs on sequences\ncontaining the concatenation of artificially-masked text and the text which was\nmasked. We show that this approach, which we call infilling by language\nmodeling, can enable LMs to infill entire sentences effectively on three\ndifferent domains: short stories, scientific abstracts, and lyrics.\nFurthermore, we show that humans have difficulty identifying sentences infilled\nby our approach as machine-generated in the domain of short stories.", "published": "2020-05-11 18:00:03", "link": "http://arxiv.org/abs/2005.05339v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MART: Memory-Augmented Recurrent Transformer for Coherent Video\n  Paragraph Captioning", "abstract": "Generating multi-sentence descriptions for videos is one of the most\nchallenging captioning tasks due to its high requirements for not only visual\nrelevance but also discourse-based coherence across the sentences in the\nparagraph. Towards this goal, we propose a new approach called Memory-Augmented\nRecurrent Transformer (MART), which uses a memory module to augment the\ntransformer architecture. The memory module generates a highly summarized\nmemory state from the video segments and the sentence history so as to help\nbetter prediction of the next sentence (w.r.t. coreference and repetition\naspects), thus encouraging coherent paragraph generation. Extensive\nexperiments, human evaluations, and qualitative analyses on two popular\ndatasets ActivityNet Captions and YouCookII show that MART generates more\ncoherent and less repetitive paragraph captions than baseline methods, while\nmaintaining relevance to the input video events. All code is available\nopen-source at: https://github.com/jayleicn/recurrent-transformer", "published": "2020-05-11 20:01:41", "link": "http://arxiv.org/abs/2005.05402v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring TTS without T Using Biologically/Psychologically Motivated\n  Neural Network Modules (ZeroSpeech 2020)", "abstract": "In this study, we reported our exploration of Text-To-Speech without Text\n(TTS without T) in the Zero Resource Speech Challenge 2020, in which\nparticipants proposed an end-to-end, unsupervised system that learned speech\nrecognition and TTS together. We addressed the challenge using\nbiologically/psychologically motivated modules of Artificial Neural Networks\n(ANN), with a particular interest in unsupervised learning of human language as\na biological/psychological problem. The system first processes Mel Frequency\nCepstral Coefficient (MFCC) frames with an Echo-State Network (ESN), and\nsimulates computations in cortical microcircuits. The outcome is discretized by\nour original Variational Autoencoder (VAE) that implements the Dirichlet-based\nBayesian clustering widely accepted in computational linguistics and cognitive\nscience. The discretized signal is then reverted into sound waveform via a\nneural-network implementation of the source-filter model for speech production.", "published": "2020-05-11 23:44:37", "link": "http://arxiv.org/abs/2005.05487v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CrisisBERT: a Robust Transformer for Crisis Classification and\n  Contextual Crisis Embedding", "abstract": "Classification of crisis events, such as natural disasters, terrorist attacks\nand pandemics, is a crucial task to create early signals and inform relevant\nparties for spontaneous actions to reduce overall damage. Despite crisis such\nas natural disasters can be predicted by professional institutions, certain\nevents are first signaled by civilians, such as the recent COVID-19 pandemics.\nSocial media platforms such as Twitter often exposes firsthand signals on such\ncrises through high volume information exchange over half a billion tweets\nposted daily. Prior works proposed various crisis embeddings and classification\nusing conventional Machine Learning and Neural Network models. However, none of\nthe works perform crisis embedding and classification using state of the art\nattention-based deep neural networks models, such as Transformers and\ndocument-level contextual embeddings. This work proposes CrisisBERT, an\nend-to-end transformer-based model for two crisis classification tasks, namely\ncrisis detection and crisis recognition, which shows promising results across\naccuracy and f1 scores. The proposed model also demonstrates superior\nrobustness over benchmark, as it shows marginal performance compromise while\nextending from 6 to 36 events with only 51.4% additional data points. We also\nproposed Crisis2Vec, an attention-based, document-level contextual embedding\narchitecture for crisis embedding, which achieve better performance than\nconventional crisis embedding methods such as Word2Vec and GloVe. To the best\nof our knowledge, our works are first to propose using transformer-based crisis\nclassification and document-level contextual crisis embedding in the\nliterature.", "published": "2020-05-11 09:57:24", "link": "http://arxiv.org/abs/2005.06627v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Detecting Adverse Drug Reactions from Twitter through Domain-Specific\n  Preprocessing and BERT Ensembling", "abstract": "The automation of adverse drug reaction (ADR) detection in social media would\nrevolutionize the practice of pharmacovigilance, supporting drug regulators,\nthe pharmaceutical industry and the general public in ensuring the safety of\nthe drugs prescribed in daily practice. Following from the published\nproceedings of the Social Media Mining for Health (SMM4H) Applications Workshop\n& Shared Task in August 2019, we aimed to develop a deep learning model to\nclassify ADRs within Twitter tweets that contain drug mentions. Our approach\ninvolved fine-tuning $BERT_{LARGE}$ and two domain-specific BERT\nimplementations, $BioBERT$ and $Bio + clinicalBERT$, applying a domain-specific\npreprocessor, and developing a max-prediction ensembling approach. Our final\nmodel resulted in state-of-the-art performance on both $F_1$-score (0.6681) and\nrecall (0.7700) outperforming all models submitted in SMM4H 2019 and during\npost-evaluation to date.", "published": "2020-05-11 20:49:24", "link": "http://arxiv.org/abs/2005.06634v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Incremental Learning for End-to-End Automatic Speech Recognition", "abstract": "In this paper, we propose an incremental learning method for end-to-end\nAutomatic Speech Recognition (ASR) which enables an ASR system to perform well\non new tasks while maintaining the performance on its originally learned ones.\nTo mitigate catastrophic forgetting during incremental learning, we design a\nnovel explainability-based knowledge distillation for ASR models, which is\ncombined with a response-based knowledge distillation to maintain the original\nmodel's predictions and the \"reason\" for the predictions. Our method works\nwithout access to the training data of original tasks, which addresses the\ncases where the previous data is no longer available or joint training is\ncostly. Results on a multi-stage sequential training task show that our method\noutperforms existing ones in mitigating forgetting. Furthermore, in two\npractical scenarios, compared to the target-reference joint training method,\nthe performance drop of our method is 0.02% Character Error Rate (CER), which\nis 97% smaller than the drops of the baseline methods.", "published": "2020-05-11 08:18:08", "link": "http://arxiv.org/abs/2005.04288v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Online Monaural Speech Enhancement Using Delayed Subband LSTM", "abstract": "This paper proposes a delayed subband LSTM network for online monaural\n(single-channel) speech enhancement. The proposed method is developed in the\nshort time Fourier transform (STFT) domain. Online processing requires\nframe-by-frame signal reception and processing. A paramount feature of the\nproposed method is that the same LSTM is used across frequencies, which\ndrastically reduces the number of network parameters, the amount of training\ndata and the computational burden. Training is performed in a subband manner:\nthe input consists of one frequency, together with a few context frequencies.\nThe network learns a speech-to-noise discriminative function relying on the\nsignal stationarity and on the local spectral pattern, based on which it\npredicts a clean-speech mask at each frequency. To exploit future information,\ni.e. look-ahead, we propose an output-delayed subband architecture, which\nallows the unidirectional forward network to process a few future frames in\naddition to the current frame. We leverage the proposed method to participate\nto the DNS real-time speech enhancement challenge. Experiments with the DNS\ndataset show that the proposed method achieves better performance-measuring\nscores than the DNS baseline method, which learns the full-band spectra using a\ngated recurrent unit network.", "published": "2020-05-11 12:33:43", "link": "http://arxiv.org/abs/2005.05037v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-band MelGAN: Faster Waveform Generation for High-Quality\n  Text-to-Speech", "abstract": "In this paper, we propose multi-band MelGAN, a much faster waveform\ngeneration model targeting to high-quality text-to-speech. Specifically, we\nimprove the original MelGAN by the following aspects. First, we increase the\nreceptive field of the generator, which is proven to be beneficial to speech\ngeneration. Second, we substitute the feature matching loss with the\nmulti-resolution STFT loss to better measure the difference between fake and\nreal speech. Together with pre-training, this improvement leads to both better\nquality and better training stability. More importantly, we extend MelGAN with\nmulti-band processing: the generator takes mel-spectrograms as input and\nproduces sub-band signals which are subsequently summed back to full-band\nsignals as discriminator input. The proposed multi-band MelGAN has achieved\nhigh MOS of 4.34 and 4.22 in waveform generation and TTS, respectively. With\nonly 1.91M parameters, our model effectively reduces the total computational\ncomplexity of the original MelGAN from 5.85 to 0.95 GFLOPS. Our Pytorch\nimplementation, which will be open-resourced shortly, can achieve a real-time\nfactor of 0.03 on CPU without hardware specific optimization.", "published": "2020-05-11 13:48:41", "link": "http://arxiv.org/abs/2005.05106v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GACELA -- A generative adversarial context encoder for long audio\n  inpainting", "abstract": "We introduce GACELA, a generative adversarial network (GAN) designed to\nrestore missing musical audio data with a duration ranging between hundreds of\nmilliseconds to a few seconds, i.e., to perform long-gap audio inpainting.\nWhile previous work either addressed shorter gaps or relied on exemplars by\ncopying available information from other signal parts, GACELA addresses the\ninpainting of long gaps in two aspects. First, it considers various time scales\nof audio information by relying on five parallel discriminators with increasing\nresolution of receptive fields. Second, it is conditioned not only on the\navailable information surrounding the gap, i.e., the context, but also on the\nlatent variable of the conditional GAN. This addresses the inherent\nmulti-modality of audio inpainting at such long gaps and provides the option of\nuser-defined inpainting. GACELA was tested in listening tests on music signals\nof varying complexity and gap durations ranging from 375~ms to 1500~ms. While\nour subjects were often able to detect the inpaintings, the severity of the\nartifacts decreased from unacceptable to mildly disturbing. GACELA represents a\nframework capable to integrate future improvements such as processing of more\nauditory-related features or more explicit musical features.", "published": "2020-05-11 12:17:26", "link": "http://arxiv.org/abs/2005.05032v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "deepSELF: An Open Source Deep Self End-to-End Learning Framework", "abstract": "We introduce an open-source toolkit, i.e., the deep Self End-to-end Learning\nFramework (deepSELF), as a toolkit of deep self end-to-end learning framework\nfor multi-modal signals. To the best of our knowledge, it is the first public\ntoolkit assembling a series of state-of-the-art deep learning technologies.\nHighlights of the proposed deepSELF toolkit include: First, it can be used to\nanalyse a variety of multi-modal signals, including images, audio, and single\nor multi-channel sensor data. Second, we provide multiple options for\npre-processing, e.g., filtering, or spectrum image generation by Fourier or\nwavelet transformation. Third, plenty of topologies in terms of NN, 1D/2D/3D\nCNN, and RNN/LSTM/GRU can be customised and a series of pretrained 2D CNN\nmodels, e.g., AlexNet, VGGNet, ResNet can be used easily. Last but not least,\nabove these features, deepSELF can be flexibly used not only as a single model\nbut also as a fusion of such.", "published": "2020-05-11 13:50:01", "link": "http://arxiv.org/abs/2005.06993v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Foreground-Background Ambient Sound Scene Separation", "abstract": "Ambient sound scenes typically comprise multiple short events occurring on\ntop of a somewhat stationary background. We consider the task of separating\nthese events from the background, which we call foreground-background ambient\nsound scene separation. We propose a deep learning-based separation framework\nwith a suitable feature normaliza-tion scheme and an optional auxiliary network\ncapturing the background statistics, and we investigate its ability to handle\nthe great variety of sound classes encountered in ambient sound scenes, which\nhave often not been seen in training. To do so, we create single-channel\nforeground-background mixtures using isolated sounds from the DESED and\nAudioset datasets, and we conduct extensive experiments with mixtures of seen\nor unseen sound classes at various signal-to-noise ratios. Our experimental\nfindings demonstrate the generalization ability of the proposed approach.", "published": "2020-05-11 06:59:46", "link": "http://arxiv.org/abs/2005.07006v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
