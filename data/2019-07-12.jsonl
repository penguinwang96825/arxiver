{"title": "GRN: Gated Relation Network to Enhance Convolutional Neural Network for\n  Named Entity Recognition", "abstract": "The dominant approaches for named entity recognition (NER) mostly adopt\ncomplex recurrent neural networks (RNN), e.g., long-short-term-memory (LSTM).\nHowever, RNNs are limited by their recurrent nature in terms of computational\nefficiency. In contrast, convolutional neural networks (CNN) can fully exploit\nthe GPU parallelism with their feedforward architectures. However, little\nattention has been paid to performing NER with CNNs, mainly owing to their\ndifficulties in capturing the long-term context information in a sequence. In\nthis paper, we propose a simple but effective CNN-based network for NER, i.e.,\ngated relation network (GRN), which is more capable than common CNNs in\ncapturing long-term context. Specifically, in GRN we firstly employ CNNs to\nexplore the local context features of each word. Then we model the relations\nbetween words and use them as gates to fuse local context features into global\nones for predicting labels. Without using recurrent layers that process a\nsentence in a sequential manner, our GRN allows computations to be performed in\nparallel across the entire sentence. Experiments on two benchmark NER datasets\n(i.e., CoNLL2003 and Ontonotes 5.0) show that, our proposed GRN can achieve\nstate-of-the-art performance with or without external knowledge. It also enjoys\nlower time costs to train and test.We have made the code publicly available at\nhttps://github.com/HuiChen24/NER-GRN.", "published": "2019-07-12 08:16:26", "link": "http://arxiv.org/abs/1907.05611v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Word Stress Detection in Russian", "abstract": "In this study we address the problem of automated word stress detection in\nRussian using character level models and no part-speech-taggers. We use a\nsimple bidirectional RNN with LSTM nodes and achieve the accuracy of 90% or\nhigher. We experiment with two training datasets and show that using the data\nfrom an annotated corpus is much more efficient than using a dictionary, since\nit allows us to take into account word frequencies and the morphological\ncontext of the word.", "published": "2019-07-12 14:13:53", "link": "http://arxiv.org/abs/1907.05757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hello, It's GPT-2 -- How Can I Help You? Towards the Use of Pretrained\n  Language Models for Task-Oriented Dialogue Systems", "abstract": "Data scarcity is a long-standing and crucial challenge that hinders quick\ndevelopment of task-oriented dialogue systems across multiple domains:\ntask-oriented dialogue models are expected to learn grammar, syntax, dialogue\nreasoning, decision making, and language generation from absurdly small amounts\nof task-specific data. In this paper, we demonstrate that recent progress in\nlanguage modeling pre-training and transfer learning shows promise to overcome\nthis problem. We propose a task-oriented dialogue model that operates solely on\ntext input: it effectively bypasses explicit policy and language generation\nmodules. Building on top of the TransferTransfo framework (Wolf et al., 2019)\nand generative model pre-training (Radford et al., 2019), we validate the\napproach on complex multi-domain task-oriented dialogues from the MultiWOZ\ndataset. Our automatic and human evaluations show that the proposed model is on\npar with a strong task-specific neural baseline. In the long run, our approach\nholds promise to mitigate the data scarcity problem, and to support the\nconstruction of more engaging and more eloquent task-oriented conversational\nagents.", "published": "2019-07-12 14:55:06", "link": "http://arxiv.org/abs/1907.05774v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The University of Edinburgh's Submissions to the WMT19 News Translation\n  Task", "abstract": "The University of Edinburgh participated in the WMT19 Shared Task on News\nTranslation in six language directions: English-to-Gujarati,\nGujarati-to-English, English-to-Chinese, Chinese-to-English, German-to-English,\nand English-to-Czech. For all translation directions, we created or used\nback-translations of monolingual data in the target language as additional\nsynthetic training data. For English-Gujarati, we also explored semi-supervised\nMT with cross-lingual language model pre-training, and translation pivoting\nthrough Hindi. For translation to and from Chinese, we investigated\ncharacter-based tokenisation vs. sub-word segmentation of Chinese text. For\nGerman-to-English, we studied the impact of vast amounts of back-translated\ntraining data on translation quality, gaining a few additional insights over\nEdunov et al. (2018). For English-to-Czech, we compared different\npre-processing and tokenisation regimes.", "published": "2019-07-12 17:23:30", "link": "http://arxiv.org/abs/1907.05854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Dynamic Embedded Topic Model", "abstract": "Topic modeling analyzes documents to learn meaningful patterns of words. For\ndocuments collected in sequence, dynamic topic models capture how these\npatterns vary over time. We develop the dynamic embedded topic model (D-ETM), a\ngenerative model of documents that combines dynamic latent Dirichlet allocation\n(D-LDA) and word embeddings. The D-ETM models each word with a categorical\ndistribution parameterized by the inner product between the word embedding and\na per-time-step embedding representation of its assigned topic. The D-ETM\nlearns smooth topic trajectories by defining a random walk prior over the\nembedding representations of the topics. We fit the D-ETM using structured\namortized variational inference with a recurrent neural network. On three\ndifferent corpora---a collection of United Nations debates, a set of ACL\nabstracts, and a dataset of Science Magazine articles---we found that the D-ETM\noutperforms D-LDA on a document completion task. We further found that the\nD-ETM learns more diverse and coherent topics than D-LDA while requiring\nsignificantly less time to fit.", "published": "2019-07-12 01:55:36", "link": "http://arxiv.org/abs/1907.05545v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "NPA: Neural News Recommendation with Personalized Attention", "abstract": "News recommendation is very important to help users find interested news and\nalleviate information overload. Different users usually have different\ninterests and the same user may have various interests. Thus, different users\nmay click the same news article with attention on different aspects. In this\npaper, we propose a neural news recommendation model with personalized\nattention (NPA). The core of our approach is a news representation model and a\nuser representation model. In the news representation model we use a CNN\nnetwork to learn hidden representations of news articles based on their titles.\nIn the user representation model we learn the representations of users based on\nthe representations of their clicked news articles. Since different words and\ndifferent news articles may have different informativeness for representing\nnews and users, we propose to apply both word- and news-level attention\nmechanism to help our model attend to important words and news articles. In\naddition, the same news article and the same word may have different\ninformativeness for different users. Thus, we propose a personalized attention\nnetwork which exploits the embedding of user ID to generate the query vector\nfor the word- and news-level attentions. Extensive experiments are conducted on\na real-world news recommendation dataset collected from MSN news, and the\nresults validate the effectiveness of our approach on news recommendation.", "published": "2019-07-12 03:11:14", "link": "http://arxiv.org/abs/1907.05559v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ScenarioSA: A Large Scale Conversational Database for Interactive\n  Sentiment Analysis", "abstract": "Interactive sentiment analysis is an emerging, yet challenging, subtask of\nthe sentiment analysis problem. It aims to discover the affective state and\nsentimental change of each person in a conversation. Existing sentiment\nanalysis approaches are insufficient in modelling the interactions among\npeople. However, the development of new approaches are critically limited by\nthe lack of labelled interactive sentiment datasets. In this paper, we present\na new conversational emotion database that we have created and made publically\navailable, namely ScenarioSA. We manually label 2,214 multi-turn English\nconversations collected from natural contexts. In comparison with existing\nsentiment datasets, ScenarioSA (1) covers a wide range of scenarios; (2)\ndescribes the interactions between two speakers; and (3) reflects the\nsentimental evolution of each speaker over the course of a conversation.\nFinally, we evaluate various state-of-the-art algorithms on ScenarioSA,\ndemonstrating the need of novel interactive sentiment analysis models and the\npotential of ScenarioSA to facilitate the development of such models.", "published": "2019-07-12 03:34:06", "link": "http://arxiv.org/abs/1907.05562v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Saliency Maps Generation for Automatic Text Summarization", "abstract": "Saliency map generation techniques are at the forefront of explainable AI\nliterature for a broad range of machine learning applications. Our goal is to\nquestion the limits of these approaches on more complex tasks. In this paper we\napply Layer-Wise Relevance Propagation (LRP) to a sequence-to-sequence\nattention model trained on a text summarization dataset. We obtain unexpected\nsaliency maps and discuss the rightfulness of these \"explanations\". We argue\nthat we need a quantitative way of testing the counterfactual case to judge the\ntruthfulness of the saliency maps. We suggest a protocol to check the validity\nof the importance attributed to the input and show that the saliency maps\nobtained sometimes capture the real use of the input features by the network,\nand sometimes do not. We use this example to discuss how careful we need to be\nwhen accepting them as explanation.", "published": "2019-07-12 10:28:00", "link": "http://arxiv.org/abs/1907.05664v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Equiprobable mappings in weighted constraint grammars", "abstract": "We show that MaxEnt is so rich that it can distinguish between any two\ndifferent mappings: there always exists a nonnegative weight vector which\nassigns them different MaxEnt probabilities. Stochastic HG instead does admit\nequiprobable mappings and we give a complete formal characterization of them.\nWe compare these different predictions of the two frameworks on a test case of\nFinnish stress.", "published": "2019-07-12 17:02:14", "link": "http://arxiv.org/abs/1907.05839v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PyKaldi2: Yet another speech toolkit based on Kaldi and PyTorch", "abstract": "We introduce PyKaldi2 speech recognition toolkit implemented based on Kaldi\nand PyTorch. While similar toolkits are available built on top of the two, a\nkey feature of PyKaldi2 is sequence training with criteria such as MMI, sMBR\nand MPE. In particular, we implemented the sequence training module with\non-the-fly lattice generation during model training in order to simplify the\ntraining pipeline. To address the challenging acoustic environments in real\napplications, PyKaldi2 also supports on-the-fly noise and reverberation\nsimulation to improve the model robustness. With this feature, it is possible\nto backpropogate the gradients from the sequence-level loss to the front-end\nfeature extraction module, which, hopefully, can foster more research in the\ndirection of joint front-end and backend learning. We performed benchmark\nexperiments on Librispeech, and show that PyKaldi2 can achieve reasonable\nrecognition accuracy. The toolkit is released under the MIT license.", "published": "2019-07-12 20:54:59", "link": "http://arxiv.org/abs/1907.05955v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "R-Transformer: Recurrent Neural Network Enhanced Transformer", "abstract": "Recurrent Neural Networks have long been the dominating choice for sequence\nmodeling. However, it severely suffers from two issues: impotent in capturing\nvery long-term dependencies and unable to parallelize the sequential\ncomputation procedure. Therefore, many non-recurrent sequence models that are\nbuilt on convolution and attention operations have been proposed recently.\nNotably, models with multi-head attention such as Transformer have demonstrated\nextreme effectiveness in capturing long-term dependencies in a variety of\nsequence modeling tasks. Despite their success, however, these models lack\nnecessary components to model local structures in sequences and heavily rely on\nposition embeddings that have limited effects and require a considerable amount\nof design efforts. In this paper, we propose the R-Transformer which enjoys the\nadvantages of both RNNs and the multi-head attention mechanism while avoids\ntheir respective drawbacks. The proposed model can effectively capture both\nlocal structures and global long-term dependencies in sequences without any use\nof position embeddings. We evaluate R-Transformer through extensive experiments\nwith data from a wide range of domains and the empirical results show that\nR-Transformer outperforms the state-of-the-art methods by a large margin in\nmost of the tasks. We have made the code publicly available at\n\\url{https://github.com/DSE-MSU/R-transformer}.", "published": "2019-07-12 04:01:57", "link": "http://arxiv.org/abs/1907.05572v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Neural News Recommendation with Attentive Multi-View Learning", "abstract": "Personalized news recommendation is very important for online news platforms\nto help users find interested news and improve user experience. News and user\nrepresentation learning is critical for news recommendation. Existing news\nrecommendation methods usually learn these representations based on single news\ninformation, e.g., title, which may be insufficient. In this paper we propose a\nneural news recommendation approach which can learn informative representations\nof users and news by exploiting different kinds of news information. The core\nof our approach is a news encoder and a user encoder. In the news encoder we\npropose an attentive multi-view learning model to learn unified news\nrepresentations from titles, bodies and topic categories by regarding them as\ndifferent views of news. In addition, we apply both word-level and view-level\nattention mechanism to news encoder to select important words and views for\nlearning informative news representations. In the user encoder we learn the\nrepresentations of users based on their browsed news and apply attention\nmechanism to select informative news for user representation learning.\nExtensive experiments on a real-world dataset show our approach can effectively\nimprove the performance of news recommendation.", "published": "2019-07-12 04:50:33", "link": "http://arxiv.org/abs/1907.05576v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effective Incorporation of Speaker Information in Utterance Encoding in\n  Dialog", "abstract": "In dialog studies, we often encode a dialog using a hierarchical encoder\nwhere each utterance is converted into an utterance vector, and then a sequence\nof utterance vectors is converted into a dialog vector. Since knowing who\nproduced which utterance is essential to understanding a dialog, conventional\nmethods tried integrating speaker labels into utterance vectors. We found the\nmethod problematic in some cases where speaker annotations are inconsistent\namong different dialogs. A relative speaker modeling method is proposed to\naddress the problem. Experimental evaluations on dialog act recognition and\nresponse generation show that the proposed method yields superior and more\nconsistent performances.", "published": "2019-07-12 07:37:00", "link": "http://arxiv.org/abs/1907.05599v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Justifying Diagnosis Decisions by Deep Neural Networks", "abstract": "An integrated approach is proposed across visual and textual data to both\ndetermine and justify a medical diagnosis by a neural network. As deep learning\ntechniques improve, interest grows to apply them in medical applications. To\nenable a transition to workflows in a medical context that are aided by machine\nlearning, the need exists for such algorithms to help justify the obtained\noutcome so human clinicians can judge their validity. In this work, deep\nlearning methods are used to map a frontal X-Ray image to a continuous textual\nrepresentation. This textual representation is decoded into a diagnosis and the\nassociated textual justification that will help a clinician evaluate the\noutcome. Additionally, more explanatory data is provided for the diagnosis by\ngenerating a realistic X-Ray that belongs to the nearest alternative diagnosis.\nWith a clinical expert opinion study on a subset of the X-Ray data set from the\nIndiana University hospital network, we demonstrate that our justification\nmechanism significantly outperforms existing methods that use saliency maps.\nWhile performing multi-task training with multiple loss functions, our method\nachieves excellent diagnosis accuracy and captioning quality when compared to\ncurrent state-of-the-art single-task methods.", "published": "2019-07-12 10:51:48", "link": "http://arxiv.org/abs/1907.05671v1", "categories": ["cs.LG", "cs.CL", "cs.HC", "eess.IV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Toeplitz Inverse Covariance based Robust Speaker Clustering for\n  Naturalistic Audio Streams", "abstract": "Speaker diarization determines who spoke and when? in an audio stream. In\nthis study, we propose a model-based approach for robust speaker clustering\nusing i-vectors. The ivectors extracted from different segments of same speaker\nare correlated. We model this correlation with a Markov Random Field (MRF)\nnetwork. Leveraging the advancements in MRF modeling, we used Toeplitz Inverse\nCovariance (TIC) matrix to represent the MRF correlation network for each\nspeaker. This approaches captures the sequential structure of i-vectors (or\nequivalent speaker turns) belonging to same speaker in an audio stream. A\nvariant of standard Expectation Maximization (EM) algorithm is adopted for\nderiving closed-form solution using dynamic programming (DP) and the\nalternating direction method of multiplier (ADMM). Our diarization system has\nfour steps: (1) ground-truth segmentation; (2) i-vector extraction; (3)\npost-processing (mean subtraction, principal component analysis, and\nlength-normalization) ; and (4) proposed speaker clustering. We employ cosine\nK-means and movMF speaker clustering as baseline approaches. Our evaluation\ndata is derived from: (i) CRSS-PLTL corpus, and (ii) two meetings subset of the\nAMI corpus. Relative reduction in diarization error rate (DER) for CRSS-PLTL\ncorpus is 43.22% using the proposed advancements as compared to baseline. For\nAMI meetings IS1000a and IS1003b, relative DER reduction is 29.37% and 9.21%,\nrespectively.", "published": "2019-07-12 05:54:33", "link": "http://arxiv.org/abs/1907.05584v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Pathology Detection Using Deep Learning: a Preliminary Study", "abstract": "This paper describes a preliminary investigation of Voice Pathology Detection\nusing Deep Neural Networks (DNN). We used voice recordings of sustained vowel\n/a/ produced at normal pitch from German corpus Saarbruecken Voice Database\n(SVD). This corpus contains voice recordings and electroglottograph signals of\nmore than 2 000 speakers. The idea behind this experiment is the use of\nconvolutional layers in combination with recurrent Long-Short-Term-Memory\n(LSTM) layers on raw audio signal. Each recording was split into 64 ms Hamming\nwindowed segments with 30 ms overlap. Our trained model achieved 71.36%\naccuracy with 65.04% sensitivity and 77.67% specificity on 206 validation files\nand 68.08% accuracy with 66.75% sensitivity and 77.89% specificity on 874\ntesting files. This is a promising result in favor of this approach because it\nis comparable to similar previously published experiment that used different\nmethodology. Further investigation is needed to achieve the state-of-the-art\nresults.", "published": "2019-07-12 18:06:02", "link": "http://arxiv.org/abs/1907.05905v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
