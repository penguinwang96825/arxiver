{"title": "Answer is All You Need: Instruction-following Text Embedding via\n  Answering the Question", "abstract": "This work aims to build a text embedder that can capture characteristics of\ntexts specified by user instructions. Despite its tremendous potential to\ndeploy user-oriented embeddings, none of previous approaches provides a\nconcrete solution for it. This paper offers a new viewpoint, which treats the\ninstruction as a question about the input text and encodes the expected answers\nto obtain the representation accordingly. Intuitively, texts with the same\n(implicit) semantics would share similar answers following the instruction,\nthus leading to more similar embeddings. Specifically, we propose InBedder that\ninstantiates this embed-via-answering idea by only fine-tuning language models\non abstractive question answering tasks. InBedder demonstrates significantly\nimproved instruction-following capabilities according to our proposed\ninstruction awareness tests and instruction robustness tests, when applied to\nboth large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based\nLMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering\noutcomes, achieved by applying different instructions to the same corpus,\ndemonstrates a high degree of interpretability.", "published": "2024-02-15 01:02:41", "link": "http://arxiv.org/abs/2402.09642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph\n  Completion", "abstract": "Commonsense knowledge graph completion is a new challenge for commonsense\nknowledge graph construction and application. In contrast to factual knowledge\ngraphs such as Freebase and YAGO, commonsense knowledge graphs (CSKGs; e.g.,\nConceptNet) utilize free-form text to represent named entities, short phrases,\nand events as their nodes. Such a loose structure results in large and sparse\nCSKGs, which makes the semantic understanding of these nodes more critical for\nlearning rich commonsense knowledge graph embedding. While current methods\nleverage semantic similarities to increase the graph density, the semantic\nplausibility of the nodes and their relations are under-explored. Previous\nworks adopt conceptual abstraction to improve the consistency of modeling\n(event) plausibility, but they are not scalable enough and still suffer from\ndata sparsity. In this paper, we propose to adopt textual entailment to find\nimplicit entailment relations between CSKG nodes, to effectively densify the\nsubgraph connecting nodes within the same conceptual class, which indicates a\nsimilar level of plausibility. Each node in CSKG finds its top entailed nodes\nusing a finetuned transformer over natural language inference (NLI) tasks,\nwhich sufficiently capture textual entailment signals. The entailment relation\nbetween these nodes are further utilized to: 1) build new connections between\nsource triplets and entailed nodes to densify the sparse CSKGs; 2) enrich the\ngeneralization ability of node representations by comparing the node embeddings\nwith a contrastive loss. Experiments on two standard CSKGs demonstrate that our\nproposed framework EntailE can improve the performance of CSKG completion tasks\nunder both transductive and inductive settings.", "published": "2024-02-15 02:27:23", "link": "http://arxiv.org/abs/2402.09666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of Language Frequency and Error Correction for Esperanto", "abstract": "Current Grammar Error Correction (GEC) initiatives tend to focus on major\nlanguages, with less attention given to low-resource languages like Esperanto.\nIn this article, we begin to bridge this gap by first conducting a\ncomprehensive frequency analysis using the Eo-GP dataset, created explicitly\nfor this purpose. We then introduce the Eo-GEC dataset, derived from authentic\nuser cases and annotated with fine-grained linguistic details for error\nidentification. Leveraging GPT-3.5 and GPT-4, our experiments show that GPT-4\noutperforms GPT-3.5 in both automated and human evaluations, highlighting its\nefficacy in addressing Esperanto's grammatical peculiarities and illustrating\nthe potential of advanced language models to enhance GEC strategies for less\ncommonly studied languages.", "published": "2024-02-15 04:10:25", "link": "http://arxiv.org/abs/2402.09696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs Know about Hallucination? An Empirical Investigation of LLM's\n  Hidden States", "abstract": "Large Language Models (LLMs) can make up answers that are not real, and this\nis known as hallucination. This research aims to see if, how, and to what\nextent LLMs are aware of hallucination. More specifically, we check whether and\nhow an LLM reacts differently in its hidden states when it answers a question\nright versus when it hallucinates. To do this, we introduce an experimental\nframework which allows examining LLM's hidden states in different hallucination\nsituations. Building upon this framework, we conduct a series of experiments\nwith language models in the LLaMA family (Touvron et al., 2023). Our empirical\nfindings suggest that LLMs react differently when processing a genuine response\nversus a fabricated one. We then apply various model interpretation techniques\nto help understand and explain the findings better. Moreover, informed by the\nempirical observations, we show great potential of using the guidance derived\nfrom LLM's hidden representation space to mitigate hallucination. We believe\nthis work provides insights into how LLMs produce hallucinated answers and how\nto make them occur less often.", "published": "2024-02-15 06:14:55", "link": "http://arxiv.org/abs/2402.09733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Align before Attend: Aligning Visual and Textual Features for Multimodal\n  Hateful Content Detection", "abstract": "Multimodal hateful content detection is a challenging task that requires\ncomplex reasoning across visual and textual modalities. Therefore, creating a\nmeaningful multimodal representation that effectively captures the interplay\nbetween visual and textual features through intermediate fusion is critical.\nConventional fusion techniques are unable to attend to the modality-specific\nfeatures effectively. Moreover, most studies exclusively concentrated on\nEnglish and overlooked other low-resource languages. This paper proposes a\ncontext-aware attention framework for multimodal hateful content detection and\nassesses it for both English and non-English languages. The proposed approach\nincorporates an attention layer to meaningfully align the visual and textual\nfeatures. This alignment enables selective focus on modality-specific features\nbefore fusing them. We evaluate the proposed approach on two benchmark hateful\nmeme datasets, viz. MUTE (Bengali code-mixed) and MultiOFF (English).\nEvaluation results demonstrate our proposed approach's effectiveness with\nF1-scores of $69.7$% and $70.3$% for the MUTE and MultiOFF datasets. The scores\nshow approximately $2.5$% and $3.2$% performance improvement over the\nstate-of-the-art systems on these datasets. Our implementation is available at\nhttps://github.com/eftekhar-hossain/Bengali-Hateful-Memes.", "published": "2024-02-15 06:34:15", "link": "http://arxiv.org/abs/2402.09738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical\n  Interaction Simulator", "abstract": "Artificial intelligence has significantly advanced healthcare, particularly\nthrough large language models (LLMs) that excel in medical question answering\nbenchmarks. However, their real-world clinical application remains limited due\nto the complexities of doctor-patient interactions. To address this, we\nintroduce \\textbf{AI Hospital}, a multi-agent framework simulating dynamic\nmedical interactions between \\emph{Doctor} as player and NPCs including\n\\emph{Patient}, \\emph{Examiner}, \\emph{Chief Physician}. This setup allows for\nrealistic assessments of LLMs in clinical scenarios. We develop the Multi-View\nMedical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical\nrecords and NPCs to evaluate LLMs' performance in symptom collection,\nexamination recommendations, and diagnoses. Additionally, a dispute resolution\ncollaborative mechanism is proposed to enhance diagnostic accuracy through\niterative discussions. Despite improvements, current LLMs exhibit significant\nperformance gaps in multi-turn interactions compared to one-step approaches.\nOur findings highlight the need for further research to bridge these gaps and\nimprove LLMs' clinical diagnostic capabilities. Our data, code, and\nexperimental results are all open-sourced at\n\\url{https://github.com/LibertFan/AI_Hospital}.", "published": "2024-02-15 06:46:48", "link": "http://arxiv.org/abs/2402.09742v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for\n  Large Language Models", "abstract": "The considerable size of Large Language Models (LLMs) presents notable\ndeployment challenges, particularly on resource-constrained hardware.\nStructured pruning, offers an effective means to compress LLMs, thereby\nreducing storage costs and enhancing inference speed for more efficient\nutilization. In this work, we study data-efficient and resource-efficient\nstructure pruning methods to obtain smaller yet still powerful models.\nKnowledge Distillation is well-suited for pruning, as the intact model can\nserve as an excellent teacher for pruned students. However, it becomes\nchallenging in the context of LLMs due to memory constraints. To address this,\nwe propose an efficient progressive Numerous-teacher pruning method\n(NutePrune). NutePrune mitigates excessive memory costs by loading only one\nintact model and integrating it with various masks and LoRA modules, enabling\nit to seamlessly switch between teacher and student roles. This approach allows\nus to leverage numerous teachers with varying capacities to progressively guide\nthe pruned model, enhancing overall performance. Extensive experiments across\nvarious tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot\nexperiments, NutePrune retains 97.17% of the performance of the original model\nat 20% sparsity and 95.07% at 25% sparsity. Our code is available at\nhttps://github.com/Lucius-lsr/NutePrune.", "published": "2024-02-15 08:03:12", "link": "http://arxiv.org/abs/2402.09773v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge of Pretrained Language Models on Surface Information of Tokens", "abstract": "Do pretrained language models have knowledge regarding the surface\ninformation of tokens? We examined the surface information stored in word or\nsubword embeddings acquired by pretrained language models from the perspectives\nof token length, substrings, and token constitution. Additionally, we evaluated\nthe ability of models to generate knowledge regarding token surfaces. We\nfocused on 12 pretrained language models that were mainly trained on English\nand Japanese corpora. Experimental results demonstrate that pretrained language\nmodels have knowledge regarding token length and substrings but not token\nconstitution. Additionally, the results imply that there is a bottleneck on the\ndecoder side in terms of effectively utilizing acquired knowledge.", "published": "2024-02-15 09:14:53", "link": "http://arxiv.org/abs/2402.09808v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Camouflage is all you need: Evaluating and Enhancing Language Model\n  Robustness Against Camouflage Adversarial Attacks", "abstract": "Adversarial attacks represent a substantial challenge in Natural Language\nProcessing (NLP). This study undertakes a systematic exploration of this\nchallenge in two distinct phases: vulnerability evaluation and resilience\nenhancement of Transformer-based models under adversarial attacks.\n  In the evaluation phase, we assess the susceptibility of three Transformer\nconfigurations, encoder-decoder, encoder-only, and decoder-only setups, to\nadversarial attacks of escalating complexity across datasets containing\noffensive language and misinformation. Encoder-only models manifest a 14% and\n21% performance drop in offensive language detection and misinformation\ndetection tasks, respectively. Decoder-only models register a 16% decrease in\nboth tasks, while encoder-decoder models exhibit a maximum performance drop of\n14% and 26% in the respective tasks.\n  The resilience-enhancement phase employs adversarial training, integrating\npre-camouflaged and dynamically altered data. This approach effectively reduces\nthe performance drop in encoder-only models to an average of 5% in offensive\nlanguage detection and 2% in misinformation detection tasks. Decoder-only\nmodels, occasionally exceeding original performance, limit the performance drop\nto 7% and 2% in the respective tasks. Although not surpassing the original\nperformance, Encoder-decoder models can reduce the drop to an average of 6% and\n2% respectively.\n  Results suggest a trade-off between performance and robustness, with some\nmodels maintaining similar performance while gaining robustness. Our study and\nadversarial training techniques have been incorporated into an open-source tool\nfor generating camouflaged datasets. However, methodology effectiveness depends\non the specific camouflage technique and data encountered, emphasizing the need\nfor continued exploration.", "published": "2024-02-15 10:58:22", "link": "http://arxiv.org/abs/2402.09874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Case Study: Testing Model Capabilities in Some Reasoning Tasks", "abstract": "Large Language Models (LLMs) excel in generating personalized content and\nfacilitating interactive dialogues, showcasing their remarkable aptitude for a\nmyriad of applications. However, their capabilities in reasoning and providing\nexplainable outputs, especially within the context of reasoning abilities,\nremain areas for improvement. In this study, we delve into the reasoning\nabilities of LLMs, highlighting the current challenges and limitations that\nhinder their effectiveness in complex reasoning scenarios.", "published": "2024-02-15 14:21:30", "link": "http://arxiv.org/abs/2402.09967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Safer Large Language Models through Machine Unlearning", "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated their\nvast potential across various domains, attributed to their extensive\npretraining knowledge and exceptional generalizability. However, LLMs often\nencounter challenges in generating harmful content when faced with problematic\nprompts. To address this problem, existing work attempted to implement a\ngradient ascent based approach to prevent LLMs from producing harmful output.\nWhile these methods can be effective, they frequently impact the model utility\nin responding to normal prompts. To address this gap, we introduce Selective\nKnowledge negation Unlearning (SKU), a novel unlearning framework for LLMs,\ndesigned to eliminate harmful knowledge while preserving utility on normal\nprompts. Specifically, SKU is consisted of two stages: harmful knowledge\nacquisition stage and knowledge negation stage. The first stage aims to\nidentify and acquire harmful knowledge within the model, whereas the second is\ndedicated to remove this knowledge. SKU selectively isolates and removes\nharmful knowledge in model parameters, ensuring the model's performance remains\nrobust on normal prompts. Our experiments conducted across various LLM\narchitectures demonstrate that SKU identifies a good balance point between\nremoving harmful information and preserving utility.", "published": "2024-02-15 16:28:34", "link": "http://arxiv.org/abs/2402.10058v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Both Matter: Enhancing the Emotional Intelligence of Large Language\n  Models without Compromising the General Intelligence", "abstract": "Emotional Intelligence (EI), consisting of emotion perception, emotion\ncognition and emotion expression, plays the critical roles in improving user\ninteraction experience for the current large language model (LLM) based\nconversational general AI assistants. Previous works mainly focus on raising\nthe emotion perception ability of them via naive fine-tuning on EI-related\nclassification or regression tasks. However, this leads to the incomplete\nenhancement of EI and catastrophic forgetting of the general intelligence (GI).\nTo this end, we first introduce \\textsc{EiBench}, a large-scale collection of\nEI-related tasks in the text-to-text formation with task instructions that\ncovers all three aspects of EI, which lays a solid foundation for the\ncomprehensive EI enhancement of LLMs. Then a novel \\underline{\\textbf{Mo}}dular\n\\underline{\\textbf{E}}motional \\underline{\\textbf{I}}ntelligence enhancement\nmethod (\\textbf{MoEI}), consisting of Modular Parameter Expansion and\nintra-inter modulation, is proposed to comprehensively enhance the EI of LLMs\nwithout compromise their GI. Extensive experiments on two representative\nLLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness\nof MoEI to improving EI while maintain GI.", "published": "2024-02-15 16:36:04", "link": "http://arxiv.org/abs/2402.10073v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles", "abstract": "In light of recent advances in large language models (LLMs), the expectations\nfor the next generation of virtual assistants include enhanced naturalness and\nadaptability across diverse usage scenarios. However, the creation of\nhigh-quality annotated data for Task-Oriented Dialog (TOD) is recognized to be\nslow and costly. To address these challenges, we introduce Task-Oriented\nAutomatic Dialogs (TOAD), a novel and scalable TOD dataset along with its\nautomatic generation pipeline. The TOAD dataset simulates realistic app context\ninteraction and provide a variety of system response style options. Two aspects\nof system response styles are considered, verbosity level and users' expression\nmirroring. We benchmark TOAD on two response generation tasks, and the results\nshow that modeling more verbose responses or responses without user expression\nmirroring is more challenging.", "published": "2024-02-15 17:40:02", "link": "http://arxiv.org/abs/2402.10137v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ControlLM: Crafting Diverse Personalities for Language Models", "abstract": "As language models continue to scale in size and capability, they display an\narray of emerging behaviors, both beneficial and concerning. This heightens the\nneed to control model behaviors. We hope to be able to control the personality\ntraits of language models at the inference-time so as to have various character\nfeatures, on top of which the requirements of different types of tasks can be\nmet. Personality is a higher-level and more abstract behavioral representation\nfor language models. We introduce ControlLM, which leverages differential\nactivation patterns, derived from contrasting behavioral prompts in the model's\nlatent space, to influence the model's personality traits at inference. This\napproach allows for the precise, real-time adjustment of model behavior. First,\nwe demonstrate ControlLM's capacity to elicit diverse persona behaviors without\nany training, while precision control allows personality traits to closely\nmatch average human values. Subsequently, we showcase improved reasoning and\nquestion answering through selective amplification of beneficial attributes\nlike conscientiousness and friendliness. We hope that this work will inspire\nresearch on controlling human-like behaviors of language models and provide\ninsights for future research. Our code is publicly available at:\nhttps://github.com/wengsyx/ControlLM.", "published": "2024-02-15 17:58:29", "link": "http://arxiv.org/abs/2402.10151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study\n  for Diabetes Patients", "abstract": "Effective diabetes management is crucial for maintaining health in diabetic\npatients. Large Language Models (LLMs) have opened new avenues for diabetes\nmanagement, facilitating their efficacy. However, current LLM-based approaches\nare limited by their dependence on general sources and lack of integration with\ndomain-specific knowledge, leading to inaccurate responses. In this paper, we\npropose a knowledge-infused LLM-powered conversational health agent (CHA) for\ndiabetic patients. We customize and leverage the open-source openCHA framework,\nenhancing our CHA with external knowledge and analytical capabilities. This\nintegration involves two key components: 1) incorporating the American Diabetes\nAssociation dietary guidelines and the Nutritionix information and 2) deploying\nanalytical tools that enable nutritional intake calculation and comparison with\nthe guidelines. We compare the proposed CHA with GPT4. Our evaluation includes\n100 diabetes-related questions on daily meal choices and assessing the\npotential risks associated with the suggested diet. Our findings show that the\nproposed agent demonstrates superior performance in generating responses to\nmanage essential nutrients.", "published": "2024-02-15 18:00:02", "link": "http://arxiv.org/abs/2402.10153v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for\n  Positional Discourse Coherence", "abstract": "Recent large language models (LLMs) have shown remarkable performance in\naligning generated text with user intentions across various tasks. When it\ncomes to long-form text generation, there has been a growing interest in\ngeneration from a discourse coherence perspective. However, existing lexical or\nsemantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the\ndiscourse coherence. The development of discourse-specific automatic evaluation\nmethods for assessing the output of LLMs warrants greater focus and\nexploration. In this paper, we present a novel automatic metric designed to\nquantify the discourse divergence between two long-form articles. Extensive\nexperiments on three datasets from representative domains demonstrate that our\nmetric aligns more closely with human preferences and GPT-4 coherence\nevaluation, outperforming existing evaluation methods.", "published": "2024-02-15 18:23:39", "link": "http://arxiv.org/abs/2402.10175v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and\n  Agent Generation", "abstract": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the\ndevelopment of LLM-based agents capable of addressing complex, real-world\ntasks. However, these agents often struggle during task execution due to\nmethodological constraints, such as error propagation and limited adaptability.\nTo address this issue, we propose a multi-agent framework based on dynamic Task\nDecomposition and Agent Generation (TDAG). This framework dynamically\ndecomposes complex tasks into smaller subtasks and assigns each to a\nspecifically generated subagent, thereby enhancing adaptability in diverse and\nunpredictable real-world tasks. Simultaneously, existing benchmarks often lack\nthe granularity needed to evaluate incremental progress in complex, multi-step\ntasks. In response, we introduce ItineraryBench in the context of travel\nplanning, featuring interconnected, progressively complex tasks with a\nfine-grained evaluation system. ItineraryBench is designed to assess agents'\nabilities in memory, planning, and tool usage across tasks of varying\ncomplexity. Our experimental results reveal that TDAG significantly outperforms\nestablished baselines, showcasing its superior adaptability and context\nawareness in complex task scenarios.", "published": "2024-02-15 18:27:37", "link": "http://arxiv.org/abs/2402.10178v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-Thought Reasoning Without Prompting", "abstract": "In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding effectively\nelicits reasoning capabilities from language models, which were previously\nobscured by standard greedy decoding.", "published": "2024-02-15 18:55:41", "link": "http://arxiv.org/abs/2402.10200v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Discern Important Urgent News?", "abstract": "We found that a simple property of clusters in a clustered dataset of news\ncorrelate strongly with importance and urgency of news (IUN) as assessed by\nLLM. We verified our finding across different news datasets, dataset sizes,\nclustering algorithms and embeddings. The found correlation should allow using\nclustering (as an alternative to LLM) for identifying the most important urgent\nnews, or for filtering out unimportant articles.", "published": "2024-02-15 20:08:07", "link": "http://arxiv.org/abs/2402.10302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Non-autoregressive Machine Translation with Error Exposure and\n  Consistency Regularization", "abstract": "Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the\nConditional Masked Language Model (CMLM) adopts the mask-predict paradigm to\nre-predict the masked low-confidence tokens. However, CMLM suffers from the\ndata distribution discrepancy between training and inference, where the\nobserved tokens are generated differently in the two cases. In this paper, we\naddress this problem with the training approaches of error exposure and\nconsistency regularization (EECR). We construct the mixed sequences based on\nmodel prediction during training, and propose to optimize over the masked\ntokens under imperfect observation conditions. We also design a consistency\nlearning method to constrain the data distribution for the masked tokens under\ndifferent observing situations to narrow down the gap between training and\ninference. The experiments on five translation benchmarks obtains an average\nimprovement of 0.68 and 0.40 BLEU scores compared to the base models,\nrespectively, and our CMLMC-EECR achieves the best performance with a\ncomparable translation quality with the Transformer. The experiments results\ndemonstrate the effectiveness of our method.", "published": "2024-02-15 05:35:04", "link": "http://arxiv.org/abs/2402.09725v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QuRating: Selecting High-Quality Data for Training Language Models", "abstract": "Selecting high-quality pre-training data is important for creating capable\nlanguage models, but existing methods rely on simple heuristics. We introduce\nQuRating, a method for selecting pre-training data that can capture human\nintuitions about data quality. In this paper, we investigate four qualities -\nwriting style, required expertise, facts & trivia, and educational value - and\nfind that LLMs are able to discern these qualities, especially when making\npairwise judgments of texts. We train a QuRater model to learn scalar ratings\nfrom pairwise judgments, and use it to annotate a 260B training corpus with\nquality ratings for each of the four criteria. In our experiments, we select\n30B tokens according to the different quality ratings and train 1.3B-parameter\nlanguage models on the selected data. We find that it is important to balance\nquality and diversity. When we sample using quality ratings as logits over\ndocuments, our models obtain lower perplexity and stronger in-context learning\nperformance than baselines. Our best model is based on educational value and\nperforms similarly to a model trained with uniform sampling for 50% more steps.\nBeyond data selection, we use the quality ratings to construct a training\ncurriculum which improves performance without changing the training dataset. We\nextensively analyze the quality ratings and discuss their characteristics,\nbiases, and wider implications.", "published": "2024-02-15 06:36:07", "link": "http://arxiv.org/abs/2402.09739v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Language Adaptive Pre-training: Extending State-of-the-Art\n  Large Language Models for Polish", "abstract": "This study explores the potential of fine-tuning foundational English Large\nLanguage Models (LLMs) for generating Polish text. The first step involves\nLanguage Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB,\nconsisting of 276 million Polish tokens. The LAPT is followed by additional\nfine-tuning aimed at solving nine KLEJ challenges. Our trained model\nCurie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02\namong decoder-based Polish models but also closely rivals the performance of\nthe best Polish encoder-decoder models with a less than 2% gap on 8 out of 9\ntasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn\nPolish. The LAPT was completed in less than five days using a consumer GPU,\nhighlighting the method's efficiency. The proficiency of the model in Polish\nwas significantly enhanced, demonstrating the viability of this approach for\nadding new languages to existing LLMs by training just 1.2% of its parameters.\nTo contribute to the community's collaborative progress, the model has been\nreleased as open-source.", "published": "2024-02-15 07:17:10", "link": "http://arxiv.org/abs/2402.09759v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EFUF: Efficient Fine-grained Unlearning Framework for Mitigating\n  Hallucinations in Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention\nin the past few years, but they may still generate descriptions that include\nobjects not present in the corresponding images, a phenomenon known as object\nhallucination. To eliminate hallucinations, existing methods manually annotate\npaired responses with and without hallucinations, and then employ various\nalignment algorithms to improve the alignment capability between images and\ntext. However, they not only demand considerable computation resources during\nthe finetuning stage but also require expensive human annotation to construct\npaired data needed by the alignment algorithms. To address these issues, we\nborrow the idea of unlearning and propose an efficient fine-grained unlearning\nframework (EFUF), which can eliminate hallucinations without the need for\npaired data. Extensive experiments show that our method consistently reduces\nhallucinations while preserving the generation quality with modest\ncomputational overhead. Our code and datasets will be publicly available.", "published": "2024-02-15 08:58:03", "link": "http://arxiv.org/abs/2402.09801v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data", "abstract": "How can we detect if copyrighted content was used in the training process of\na language model, considering that the training data is typically undisclosed?\nWe are motivated by the premise that a language model is likely to identify\nverbatim excerpts from its training text. We propose DE-COP, a method to\ndetermine whether a piece of copyrighted content was included in training.\nDE-COP's core approach is to probe an LLM with multiple-choice questions, whose\noptions include both verbatim text and their paraphrases. We construct\nBookTection, a benchmark with excerpts from 165 books published prior and\nsubsequent to a model's training cutoff, along with their paraphrases. Our\nexperiments show that DE-COP surpasses the prior best method by 9.6% in\ndetection performance (AUC) on models with logits available. Moreover, DE-COP\nalso achieves an average accuracy of 72% for detecting suspect books on fully\nblack-box models where prior methods give approximately 4% accuracy. The code\nand datasets are available at https://github.com/LeiLiLab/DE-COP.", "published": "2024-02-15 12:17:15", "link": "http://arxiv.org/abs/2402.09910v2", "categories": ["cs.CL", "cs.LG", "I.2"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge\n  Graphs for Open-ended Question Answering", "abstract": "Mitigating the hallucinations of Large Language Models is a crucial task.\nAlthough some existing methods employ self-enhancement techniques, they fall\nshort of effectively addressing unknown factual hallucinations. Meanwhile,\nKnowledge Graph (KG) enhancement approaches fail to address the generalization\nacross different KG sources and the enhancement of open-ended answer questions\nsimultaneously. To tackle these limitations, we propose a framework that\ncombines Pseudo-Graph Generation and Atomic Knowledge Verification (PG\\&AKV).\nEnhancement of open-ended question-answering begins with leveraging the\nPseudo-Graph Generation to provide the related knowledge framework.\nSubsequently, Atomic Knowledge Verification utilizes atomic-level knowledge\nquerying and verification to achieve generalizability under different KG\nsources. Compared to the baseline, this approach yields a minimum improvement\nof 11.5 in the ROUGE-L score for open-ended questions. For precise-answered\nquestions, we observe a minimum accuracy improvement of 7.5%. Moreover, PG\\&AKV\nalso exhibits generalizability across different KG sources. Utilizing KG\ndifferent from the question sources, PG\\&AKV can even achieve at least a 3.5 %\nperformance improvement. In summary, our results pave the way for enhancing\nLLMs by incorporating Pseudo- and Multisource-KGs, particularly in the filed of\nopen-ended questions.", "published": "2024-02-15 12:20:02", "link": "http://arxiv.org/abs/2402.09911v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BUSTER: a \"BUSiness Transaction Entity Recognition\" dataset", "abstract": "Albeit Natural Language Processing has seen major breakthroughs in the last\nfew years, transferring such advances into real-world business cases can be\nchallenging. One of the reasons resides in the displacement between popular\nbenchmarks and actual data. Lack of supervision, unbalanced classes, noisy data\nand long documents often affect real problems in vertical domains such as\nfinance, law and health. To support industry-oriented research, we present\nBUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists\nof 3779 manually annotated documents on financial transactions. We establish\nseveral baselines exploiting both general-purpose and domain-specific language\nmodels. The best performing model is also used to automatically annotate 6196\ndocuments, which we release as an additional silver corpus to BUSTER.", "published": "2024-02-15 12:39:57", "link": "http://arxiv.org/abs/2402.09916v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Dataset of Open-Domain Question Answering with Multiple-Span Answers", "abstract": "Multi-span answer extraction, also known as the task of multi-span question\nanswering (MSQA), is critical for real-world applications, as it requires\nextracting multiple pieces of information from a text to answer complex\nquestions. Despite the active studies and rapid progress in English MSQA\nresearch, there is a notable lack of publicly available MSQA benchmark in\nChinese. Previous efforts for constructing MSQA datasets predominantly\nemphasized entity-centric contextualization, resulting in a bias towards\ncollecting factoid questions and potentially overlooking questions requiring\nmore detailed descriptive responses. To overcome these limitations, we present\nCLEAN, a comprehensive Chinese multi-span question answering dataset that\ninvolves a wide range of open-domain subjects with a substantial number of\ninstances requiring descriptive answers. Additionally, we provide established\nmodels from relevant literature as baselines for CLEAN. Experimental results\nand analysis show the characteristics and challenge of the newly proposed CLEAN\ndataset for the community. Our dataset, CLEAN, will be publicly released at\nzhiyiluo.site/misc/clean_v1.0_ sample.json.", "published": "2024-02-15 13:03:57", "link": "http://arxiv.org/abs/2402.09923v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paying Attention to Deflections: Mining Pragmatic Nuances for\n  Whataboutism Detection in Online Discourse", "abstract": "Whataboutism, a potent tool for disrupting narratives and sowing distrust,\nremains under-explored in quantitative NLP research. Moreover, past work has\nnot distinguished its use as a strategy for misinformation and propaganda from\nits use as a tool for pragmatic and semantic framing. We introduce new datasets\nfrom Twitter and YouTube, revealing overlaps as well as distinctions between\nwhataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on\nrecent work in linguistic semantics, we differentiate the `what about' lexical\nconstruct from whataboutism. Our experiments bring to light unique challenges\nin its accurate detection, prompting the introduction of a novel method using\nattention weights for negative sample mining. We report significant\nimprovements of 4% and 10% over previous state-of-the-art methods in our\nTwitter and YouTube collections, respectively.", "published": "2024-02-15 13:34:19", "link": "http://arxiv.org/abs/2402.09934v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-word Tokenization for Sequence Compression", "abstract": "Large Language Models have proven highly successful at modelling a variety of\ntasks. However, this comes at a steep computational cost that hinders wider\nindustrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that\ngoes beyond word boundaries by representing frequent multi-word expressions as\nsingle tokens. MWTs produce a more compact and efficient tokenization that\nyields two benefits: (1) Increase in performance due to a greater coverage of\ninput data given a fixed sequence length budget; (2) Faster and lighter\ninference due to the ability to reduce the sequence length with negligible\ndrops in performance. Our results show that MWT is more robust across shorter\nsequence lengths, thus allowing for major speedups via early sequence\ntruncation.", "published": "2024-02-15 13:52:23", "link": "http://arxiv.org/abs/2402.09949v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of\n  In-Context Learning for Persona-based Dialogue Generation", "abstract": "Previous in-context learning (ICL) research has focused on tasks such as\nclassification, machine translation, text2table, etc., while studies on whether\nICL can improve human-like dialogue generation are scarce. Our work fills this\ngap by systematically investigating the ICL capabilities of large language\nmodels (LLMs) in persona-based dialogue generation, conducting extensive\nexperiments on high-quality real human Chinese dialogue datasets. From\nexperimental results, we draw three conclusions: 1) adjusting prompt\ninstructions is the most direct, effective, and economical way to improve\ngeneration quality; 2) randomly retrieving demonstrations (demos) achieves the\nbest results, possibly due to the greater diversity and the amount of effective\ninformation; counter-intuitively, retrieving demos with a context identical to\nthe query performs the worst; 3) even when we destroy the multi-turn\nassociations and single-turn semantics in the demos, increasing the number of\ndemos still improves dialogue performance, proving that LLMs can learn from\ncorrupted dialogue demos. Previous explanations of the ICL mechanism, such as\n$n$-gram induction head, cannot fully account for this phenomenon.", "published": "2024-02-15 14:03:33", "link": "http://arxiv.org/abs/2402.09954v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMs as Bridges: Reformulating Grounded Multimodal Named Entity\n  Recognition", "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal\ntask that aims to identify named entities, entity types and their corresponding\nvisual regions. GMNER task exhibits two challenging properties: 1) The weak\ncorrelation between image-text pairs in social media results in a significant\nportion of named entities being ungroundable. 2) There exists a distinction\nbetween coarse-grained referring expressions commonly used in similar tasks\n(e.g., phrase localization, referring expression comprehension) and\nfine-grained named entities. In this paper, we propose RiVEG, a unified\nframework that reformulates GMNER into a joint MNER-VE-VG task by leveraging\nlarge language models (LLMs) as a connecting bridge. This reformulation brings\ntwo benefits: 1) It maintains the optimal MNER performance and eliminates the\nneed for employing object detection methods to pre-extract regional features,\nthereby naturally addressing two major limitations of existing GMNER methods.\n2) The introduction of entity expansion expression and Visual Entailment (VE)\nmodule unifies Visual Grounding (VG) and Entity Grounding (EG). It enables\nRiVEG to effortlessly inherit the Visual Entailment and Visual Grounding\ncapabilities of any current or prospective multimodal pretraining models.\nExtensive experiments demonstrate that RiVEG outperforms state-of-the-art\nmethods on the existing GMNER dataset and achieves absolute leads of 10.65%,\n6.21%, and 8.83% in all three subtasks.", "published": "2024-02-15 14:54:33", "link": "http://arxiv.org/abs/2402.09989v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Bridging the Empirical-Theoretical Gap in Neural Network Formal Language\n  Learning Using Minimum Description Length", "abstract": "Neural networks offer good approximation to many tasks but consistently fail\nto reach perfect generalization, even when theoretical work shows that such\nperfect solutions can be expressed by certain architectures. Using the task of\nformal language learning, we focus on one simple formal language and show that\nthe theoretically correct solution is in fact not an optimum of commonly used\nobjectives -- even with regularization techniques that according to common\nwisdom should lead to simple weights and good generalization (L1, L2) or other\nmeta-heuristics (early-stopping, dropout). On the other hand, replacing\nstandard targets with the Minimum Description Length objective (MDL) results in\nthe correct solution being an optimum.", "published": "2024-02-15 15:25:30", "link": "http://arxiv.org/abs/2402.10013v2", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "SwissNYF: Tool Grounded LLM Agents for Black Box Setting", "abstract": "While Large Language Models (LLMs) have demonstrated enhanced capabilities in\nfunction-calling, these advancements primarily rely on accessing the functions'\nresponses. This methodology is practical for simpler APIs but faces scalability\nissues with irreversible APIs that significantly impact the system, such as a\ndatabase deletion API. Similarly, processes requiring extensive time for each\nAPI call and those necessitating forward planning, like automated action\npipelines, present complex challenges. Furthermore, scenarios often arise where\na generalized approach is needed because algorithms lack direct access to the\nspecific implementations of these functions or secrets to use them. Traditional\ntool planning methods are inadequate in these cases, compelling the need to\noperate within black-box environments. Unlike their performance in tool\nmanipulation, LLMs excel in black-box tasks, such as program synthesis.\nTherefore, we harness the program synthesis capabilities of LLMs to strategize\ntool usage in black-box settings, ensuring solutions are verified prior to\nimplementation. We introduce TOPGUN, an ingeniously crafted approach leveraging\nprogram synthesis for black box tool planning. Accompanied by SwissNYF, a\ncomprehensive suite that integrates black-box algorithms for planning and\nverification tasks, addressing the aforementioned challenges and enhancing the\nversatility and effectiveness of LLMs in complex API interactions. The public\ncode for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.", "published": "2024-02-15 16:15:38", "link": "http://arxiv.org/abs/2402.10051v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in\n  Large Language Models", "abstract": "Mitigating the retention of sensitive or private information in large\nlanguage models is essential for enhancing privacy and safety. Existing\nunlearning methods, like Gradient Ascent and Negative Preference Optimization,\ndirectly tune models to remove unwanted information. However, these methods\noften become unstable because they fine-tune by maximizing cross-entropy loss,\nwhich is the opposite of traditional loss minimization in learning. This\nreversal creates instability, especially on larger datasets, as the model\nstruggles to balance unlearning with maintaining language capacity, leading to\nover-unlearning. In this paper, we introduce UnDIAL (Unlearning via\nSelf-Distillation on Adjusted Logits), a novel and robust unlearning method.\nOur approach leverages self-distillation to adjust logits and selectively\nreduce the influence of targeted tokens. This technique ensures smooth\nconvergence and avoids catastrophic forgetting, even in challenging unlearning\ntasks with large datasets and sequential unlearning requests. Extensive\nexperiments show that UnDIAL can achieve both robustness in unlearning and\nscalability while maintaining stable training dynamics and resilience to\nhyperparameter tuning.", "published": "2024-02-15 16:21:14", "link": "http://arxiv.org/abs/2402.10052v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on\n  Geometry Problem-Solving", "abstract": "Recent advancements in large language models (LLMs) and multi-modal models\n(MMs) have demonstrated their remarkable capabilities in problem-solving. Yet,\ntheir proficiency in tackling geometry math problems, which necessitates an\nintegrated understanding of both textual and visual information, has not been\nthoroughly evaluated. To address this gap, we introduce the GeoEval benchmark,\na comprehensive collection that includes a main subset of 2,000 problems, a 750\nproblems subset focusing on backward reasoning, an augmented subset of 2,000\nproblems, and a hard subset of 300 problems. This benchmark facilitates a\ndeeper investigation into the performance of LLMs and MMs in solving geometry\nmath problems. Our evaluation of ten LLMs and MMs across these varied subsets\nreveals that the WizardMath model excels, achieving a 55.67\\% accuracy rate on\nthe main subset but only a 6.00\\% accuracy on the hard subset. This highlights\nthe critical need for testing models against datasets on which they have not\nbeen pre-trained. Additionally, our findings indicate that GPT-series models\nperform more effectively on problems they have rephrased, suggesting a\npromising method for enhancing model capabilities.", "published": "2024-02-15 16:59:41", "link": "http://arxiv.org/abs/2402.10104v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Quantized Embedding Vectors for Controllable Diffusion Language Models", "abstract": "Improving the controllability, portability, and inference speed of diffusion\nlanguage models (DLMs) is a key challenge in natural language generation. While\nrecent research has shown significant success in complex text generation with\nlanguage models, the memory and computational power are still very demanding\nand fall short of expectations, which naturally results in low portability and\ninstability for the models. To mitigate these issues, numerous well-established\nmethods were proposed for neural network quantization. To further enhance their\nportability of independent deployment as well as improve their stability\nevaluated by language perplexity, we propose a novel approach called the\nQuantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM\nbuilds upon the recent successful controllable DLMs by remodeling the\ntask-specific embedding space via quantization. This leads to a gradient-based\ncontroller for the generation tasks, and more stable intermediate latent\nvariables are obtained, which naturally brings in an accelerated convergence as\nwell as better controllability. Additionally, the adaption fine-tuning method\nis employed to reduce tunable weights. Experimental results on five challenging\nfine-grained control tasks demonstrate that QE-CDLM compares favorably to\nexisting methods in terms of quality and feasibility, achieving better\nperplexity and lightweight fine-tuning.", "published": "2024-02-15 17:02:48", "link": "http://arxiv.org/abs/2402.10107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Engineering for Scaling Language Models to 128K Context", "abstract": "We study the continual pretraining recipe for scaling language models'\ncontext lengths to 128K, with a focus on data engineering. We hypothesize that\nlong context modeling, in particular \\textit{the ability to utilize information\nat arbitrary input locations}, is a capability that is mostly already acquired\nthrough large-scale pretraining, and that this capability can be readily\nextended to contexts substantially longer than seen during training~(e.g., 4K\nto 128K) through lightweight continual pretraining on appropriate data mixture.\nWe investigate the \\textit{quantity} and \\textit{quality} of the data for\ncontinual pretraining: (1) for quantity, we show that 500 million to 5 billion\ntokens are enough to enable the model to retrieve information anywhere within\nthe 128K context; (2) for quality, our results equally emphasize \\textit{domain\nbalance} and \\textit{length upsampling}. Concretely, we find that naively\nupsampling longer data on certain domains like books, a common practice of\nexisting work, gives suboptimal performance, and that a balanced domain mixture\nis important. We demonstrate that continual pretraining of the full model on\n1B-5B tokens of such data is an effective and affordable strategy for scaling\nthe context length of language models to 128K. Our recipe outperforms strong\nopen-source long-context models and closes the gap to frontier models like\nGPT-4 128K.", "published": "2024-02-15 18:19:16", "link": "http://arxiv.org/abs/2402.10171v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty Quantification for In-Context Learning of Large Language\n  Models", "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language\nModels (LLMs) and revolutionized various fields by providing a few\ntask-relevant demonstrations in the prompt. However, trustworthy issues with\nLLM's response, such as hallucination, have also been actively discussed.\nExisting works have been devoted to quantifying the uncertainty in LLM's\nresponse, but they often overlook the complex nature of LLMs and the uniqueness\nof in-context learning. In this work, we delve into the predictive uncertainty\nof LLMs associated with in-context learning, highlighting that such\nuncertainties may stem from both the provided demonstrations (aleatoric\nuncertainty) and ambiguities tied to the model's configurations (epistemic\nuncertainty). We propose a novel formulation and corresponding estimation\nmethod to quantify both types of uncertainties. The proposed method offers an\nunsupervised way to understand the prediction of in-context learning in a\nplug-and-play fashion. Extensive experiments are conducted to demonstrate the\neffectiveness of the decomposition. The code and data are available at:\nhttps://github.com/lingchen0331/UQ_ICL.", "published": "2024-02-15 18:46:24", "link": "http://arxiv.org/abs/2402.10189v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit", "abstract": "Large Language Models (LLMs) are typically trained in two phases:\npre-training on large internet-scale datasets, and fine-tuning for downstream\ntasks. Given the higher computational demand of pre-training, it's intuitive to\nassume that fine-tuning adds less new information to the model, and is thus\nmore compressible. We explore this assumption by decomposing the weights of\nfine-tuned models into their pre-trained components and an additional delta. We\nintroduce a simple method, BitDelta, which successfully quantizes this delta\ndown to 1 bit without compromising performance. This interesting finding not\nonly highlights the potential redundancy of information added during\nfine-tuning, but also has significant implications for the multi-tenant serving\nand multi-tenant storage of fine-tuned models. By enabling the use of a single\nhigh-precision base model accompanied by multiple 1-bit deltas, BitDelta\ndramatically reduces GPU memory requirements by more than 10x, which can also\nbe translated to enhanced generation latency in multi-tenant settings. We\nvalidate BitDelta through experiments across Llama-2 and Mistral model\nfamilies, and on models up to 70B parameters, showcasing minimal performance\ndegradation over all tested settings.", "published": "2024-02-15 18:50:06", "link": "http://arxiv.org/abs/2402.10193v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language\n  Agents", "abstract": "Language agents powered by large language models (LLMs) have seen exploding\ndevelopment. Their capability of using language as a vehicle for thought and\ncommunication lends an incredible level of flexibility and versatility. People\nhave quickly capitalized on this capability to connect LLMs to a wide range of\nexternal components and environments: databases, tools, the Internet, robotic\nembodiment, etc. Many believe an unprecedentedly powerful automation technology\nis emerging. However, new automation technologies come with new safety risks,\nespecially for intricate systems like language agents. There is a surprisingly\nlarge gap between the speed and scale of their development and deployment and\nour understanding of their safety risks. Are we building a house of cards? In\nthis position paper, we present the first systematic effort in mapping\nadversarial attacks against language agents. We first present a unified\nconceptual framework for agents with three major components: Perception, Brain,\nand Action. Under this framework, we present a comprehensive discussion and\npropose 12 potential attack scenarios against different components of an agent,\ncovering different attack strategies (e.g., input manipulation, adversarial\ndemonstrations, jailbreaking, backdoors). We also draw connections to\nsuccessful attack strategies previously applied to LLMs. We emphasize the\nurgency to gain a thorough understanding of language agent risks before their\nwidespread deployment.", "published": "2024-02-15 18:51:32", "link": "http://arxiv.org/abs/2402.10196v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The optimal placement of the head in the noun phrase. The case of\n  demonstrative, numeral, adjective and noun", "abstract": "The word order of a sentence is shaped by multiple principles. The principle\nof syntactic dependency distance minimization is in conflict with the principle\nof surprisal minimization (or predictability maximization) in single head\nsyntactic dependency structures: while the former predicts that the head should\nbe placed at the center of the linear arrangement, the latter predicts that the\nhead should be placed at one of the ends (either first or last). A critical\nquestion is when surprisal minimization (or predictability maximization) should\nsurpass syntactic dependency distance minimization. In the context of single\nhead structures, it has been predicted that this is more likely to happen when\ntwo conditions are met, i.e. (a) fewer words are involved and (b) words are\nshorter. Here we test the prediction on the noun phrase when it is composed of\na demonstrative, a numeral, an adjective and a noun. We find that, across\npreferred orders in languages, the noun tends to be placed at one of the ends,\nconfirming the theoretical prediction. We also show evidence of anti locality\neffects: syntactic dependency distances in preferred orders are longer than\nexpected by chance.", "published": "2024-02-15 20:24:39", "link": "http://arxiv.org/abs/2402.10311v8", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of\n  Language Models", "abstract": "Prompt-based learning is susceptible to intrinsic bias present in pre-trained\nlanguage models (LMs), leading to sub-optimal performance in prompt-based\nzero/few-shot settings. In this work, we propose a null-input prompting method\nto calibrate intrinsic bias encoded in pre-trained LMs. Different from prior\nefforts that address intrinsic bias primarily for social fairness and often\ninvolve excessive computational cost, our objective is to explore enhancing\nLMs' performance in downstream zero/few-shot learning while emphasizing the\nefficiency of intrinsic bias calibration. Specifically, we leverage a diverse\nset of auto-selected null-meaning inputs generated from GPT-4 to probe\nintrinsic bias of pre-trained LMs. Utilizing the bias-reflected probability\ndistribution, we formulate a distribution disparity loss for bias calibration,\nwhere we exclusively update bias parameters ($0.1\\%$ of total parameters) of\nLMs towards equal probability distribution. Experimental results show that the\ncalibration promotes an equitable starting point for LMs while preserving\nlanguage modeling abilities. Across a wide range of datasets, including\nsentiment analysis and topic classification, our method significantly improves\nzero/few-shot learning performance of LMs for both in-context learning and\nprompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively).", "published": "2024-02-15 22:54:24", "link": "http://arxiv.org/abs/2402.10353v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can we Soft Prompt LLMs for Graph Learning Tasks?", "abstract": "Graph plays an important role in representing complex relationships in\nreal-world applications such as social networks, biological data and citation\nnetworks. In recent years, Large Language Models (LLMs) have achieved\ntremendous success in various domains, which makes applying LLMs to graphs\nparticularly appealing. However, directly applying LLMs to graph modalities\npresents unique challenges due to the discrepancy and mismatch between the\ngraph and text modalities. Hence, to further investigate LLMs' potential for\ncomprehending graph information, we introduce GraphPrompter, a novel framework\ndesigned to align graph information with LLMs via soft prompts. Specifically,\nGraphPrompter consists of two main components: a graph neural network to encode\ncomplex graph information and an LLM that effectively processes textual\ninformation. Comprehensive experiments on various benchmark datasets under node\nclassification and link prediction tasks demonstrate the effectiveness of our\nproposed method. The GraphPrompter framework unveils the substantial\ncapabilities of LLMs as predictors in graph-related tasks, enabling researchers\nto utilize LLMs across a spectrum of real-world graph scenarios more\neffectively.", "published": "2024-02-15 23:09:42", "link": "http://arxiv.org/abs/2402.10359v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SportsMetrics: Blending Text and Numerical Data to Understand\n  Information Fusion in LLMs", "abstract": "Large language models hold significant potential for integrating various data\ntypes, such as text documents and database records, for advanced analytics.\nHowever, blending text and numerical data presents substantial challenges. LLMs\nneed to process and cross-reference entities and numbers, handle data\ninconsistencies and redundancies, and develop planning capabilities such as\nbuilding a working memory for managing complex data queries. In this paper, we\nintroduce four novel tasks centered around sports data analytics to evaluate\nthe numerical reasoning and information fusion capabilities of LLMs. These\ntasks involve providing LLMs with detailed, play-by-play sports game\ndescriptions, then challenging them with adversarial scenarios such as new game\nrules, longer durations, scrambled narratives, and analyzing key statistics in\ngame summaries. We conduct extensive experiments on NBA and NFL games to assess\nthe performance of LLMs on these tasks. Our benchmark, SportsMetrics,\nintroduces a new mechanism for assessing LLMs' numerical reasoning and fusion\nskills.", "published": "2024-02-15 20:26:07", "link": "http://arxiv.org/abs/2402.10979v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Representation Surgery: Theory and Practice of Affine Steering", "abstract": "Language models often exhibit undesirable behavior, e.g., generating toxic or\ngender-biased text. In the case of neural language models, an encoding of the\nundesirable behavior is often present in the model's representations. Thus, one\nnatural (and common) approach to prevent the model from exhibiting undesirable\nbehavior is to steer the model's representations in a manner that reduces the\nprobability of it generating undesirable text. This paper investigates the\nformal and empirical properties of steering functions, i.e., transformation of\nthe neural language model's representations that alter its behavior. First, we\nderive two optimal, in the least-squares sense, affine steering functions under\ndifferent constraints. Our theory provides justification for existing\napproaches and offers a novel, improved steering approach. Second, we offer a\nseries of experiments that demonstrate the empirical effectiveness of the\nmethods in mitigating bias and reducing toxic generation.", "published": "2024-02-15 00:20:30", "link": "http://arxiv.org/abs/2402.09631v6", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "CodeMind: A Framework to Challenge Large Language Models for Code\n  Reasoning", "abstract": "Solely relying on test passing to evaluate Large Language Models (LLMs) for\ncode synthesis may result in unfair assessment or promoting models with data\nleakage. As an alternative, we introduce CodeMind, a framework designed to\ngauge the code reasoning abilities of LLMs. CodeMind currently supports three\ncode reasoning tasks: Independent Execution Reasoning (IER), Dependent\nExecution Reasoning (DER), and Specification Reasoning (SR). The first two\nevaluate models to predict the execution output of an arbitrary code or code\nthe model could correctly synthesize. The third one evaluates the extent to\nwhich LLMs implement the specified expected behavior.\n  Our extensive evaluation of nine LLMs across five benchmarks in two different\nprogramming languages using CodeMind shows that LLMs fairly follow control flow\nconstructs and, in general, explain how inputs evolve to output, specifically\nfor simple programs and the ones they can correctly synthesize. However, their\nperformance drops for code with higher complexity, non-trivial logical and\narithmetic operators, non-primitive types, and API calls. Furthermore, we\nobserve that, while correlated, specification reasoning (essential for code\nsynthesis) does not imply execution reasoning (essential for broader\nprogramming tasks such as testing and debugging): ranking LLMs based on test\npassing can be different compared to code reasoning.", "published": "2024-02-15 02:24:46", "link": "http://arxiv.org/abs/2402.09664v4", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "How to Train Data-Efficient LLMs", "abstract": "The training of large language models (LLMs) is expensive. In this paper, we\nstudy data-efficient approaches for pre-training LLMs, i.e., techniques that\naim to optimize the Pareto frontier of model quality and training resource/data\nconsumption. We seek to understand the tradeoffs associated with data selection\nroutines based on (i) expensive-to-compute data-quality estimates, and (ii)\nmaximization of coverage and diversity-based measures in the feature space. Our\nfirst technique, Ask-LLM, leverages the zero-shot reasoning capabilities of\ninstruction-tuned LLMs to directly assess the quality of a training example. To\ntarget coverage, we propose Density sampling, which models the data\ndistribution to select a diverse sample. In our comparison of 19 samplers,\ninvolving hundreds of evaluation tasks and pre-training runs, we find that\nAsk-LLM and Density are the best methods in their respective categories.\nCoverage sampling can recover the performance of the full data, while models\ntrained on Ask-LLM data consistently outperform full-data training -- even when\nwe reject 90% of the original dataset, while converging up to 70% faster.", "published": "2024-02-15 02:27:57", "link": "http://arxiv.org/abs/2402.09668v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PAL: Proxy-Guided Black-Box Attack on Large Language Models", "abstract": "Large Language Models (LLMs) have surged in popularity in recent months, but\nthey have demonstrated concerning capabilities to generate harmful content when\nmanipulated. While techniques like safety fine-tuning aim to minimize harmful\nuse, recent works have shown that LLMs remain vulnerable to attacks that elicit\ntoxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs\n(PAL), the first optimization-based attack on LLMs in a black-box query-only\nsetting. In particular, it relies on a surrogate model to guide the\noptimization and a sophisticated loss designed for real-world LLM APIs. Our\nattack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on\nLlama-2-7B, compared to 4% for the current state of the art. We also propose\nGCG++, an improvement to the GCG attack that reaches 94% ASR on white-box\nLlama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple\nbaseline for query-based attacks. We believe the techniques proposed in this\nwork will enable more comprehensive safety testing of LLMs and, in the long\nterm, the development of better security guardrails. The code can be found at\nhttps://github.com/chawins/pal.", "published": "2024-02-15 02:54:49", "link": "http://arxiv.org/abs/2402.09674v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Prompt Optimization Through the Lens of Best Arm\n  Identification", "abstract": "The remarkable instruction-following capability of large language models\n(LLMs) has sparked a growing interest in automatically finding good prompts,\ni.e., prompt optimization. Most existing works follow the scheme of selecting\nfrom a pre-generated pool of candidate prompts. However, these designs mainly\nfocus on the generation strategy, while limited attention has been paid to the\nselection method. Especially, the cost incurred during the selection (e.g.,\naccessing LLM and evaluating the responses) is rarely explicitly considered. To\novercome this limitation, this work provides a principled framework, TRIPLE, to\nefficiently perform prompt selection under an explicit budget constraint.\nTRIPLE is built on a novel connection established between prompt optimization\nand fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB);\nthus, it is capable of leveraging the rich toolbox from BAI-FB systematically\nand also incorporating unique characteristics of prompt optimization. Extensive\nexperiments on multiple well-adopted tasks using various LLMs demonstrate the\nremarkable performance improvement of TRIPLE over baselines while satisfying\nthe limited budget constraints. As an extension, variants of TRIPLE are\nproposed to efficiently select examples for few-shot prompts, also achieving\nsuperior empirical performance.", "published": "2024-02-15 05:31:13", "link": "http://arxiv.org/abs/2402.09723v3", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts", "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3.5-20x.", "published": "2024-02-15 05:40:21", "link": "http://arxiv.org/abs/2402.09727v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Model Compression and Efficient Inference for Large Language Models: A\n  Survey", "abstract": "Transformer based large language models have achieved tremendous success.\nHowever, the significant memory and computational costs incurred during the\ninference process make it challenging to deploy large models on\nresource-constrained devices. In this paper, we investigate compression and\nefficient inference methods for large language models from an algorithmic\nperspective. Regarding taxonomy, similar to smaller models, compression and\nacceleration algorithms for large language models can still be categorized into\nquantization, pruning, distillation, compact architecture design, dynamic\nnetworks. However, Large language models have two prominent characteristics\ncompared to smaller models: (1) Most of compression algorithms require\nfinetuning or even retraining the model after compression. The most notable\naspect of large models is the very high cost associated with model finetuning\nor training. Therefore, many algorithms for large models, such as quantization\nand pruning, start to explore tuning-free algorithms. (2) Large models\nemphasize versatility and generalization rather than performance on a single\ntask. Hence, many algorithms, such as knowledge distillation, focus on how to\npreserving their versatility and generalization after compression. Since these\ntwo characteristics were not very pronounced in early large models, we further\ndistinguish large language models into medium models and ``real'' large models.\nAdditionally, we also provide an introduction to some mature frameworks for\nefficient inference of large models, which can support basic compression or\nacceleration algorithms, greatly facilitating model deployment for users.", "published": "2024-02-15 06:58:30", "link": "http://arxiv.org/abs/2402.09748v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Grounding Language Model with Chunking-Free In-Context Retrieval", "abstract": "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval\napproach, specifically tailored for Retrieval-Augmented Generation (RAG)\nsystems. Traditional RAG systems often struggle with grounding responses using\nprecise evidence text due to the challenges of processing lengthy documents and\nfiltering out irrelevant content. Commonly employed solutions, such as document\nchunking and adapting language models to handle longer contexts, have their\nlimitations. These methods either disrupt the semantic coherence of the text or\nfail to effectively address the issues of noise and inaccuracy in evidence\nretrieval.\n  CFIC addresses these challenges by circumventing the conventional chunking\nprocess. It utilizes the encoded hidden states of documents for in-context\nretrieval, employing auto-aggressive decoding to accurately identify the\nspecific evidence text required for user queries, eliminating the need for\nchunking. CFIC is further enhanced by incorporating two decoding strategies,\nnamely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies\nnot only improve the efficiency of the retrieval process but also ensure that\nthe fidelity of the generated grounding text evidence is maintained. Our\nevaluations of CFIC on a range of open QA datasets demonstrate its superiority\nin retrieving relevant and accurate evidence, offering a significant\nimprovement over traditional methods. By doing away with the need for document\nchunking, CFIC presents a more streamlined, effective, and efficient retrieval\nsolution, making it a valuable advancement in the field of RAG systems.", "published": "2024-02-15 07:22:04", "link": "http://arxiv.org/abs/2402.09760v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LAPDoc: Layout-Aware Prompting for Documents", "abstract": "Recent advances in training large language models (LLMs) using massive\namounts of solely textual data lead to strong generalization across many\ndomains and tasks, including document-specific tasks. Opposed to that there is\na trend to train multi-modal transformer architectures tailored for document\nunderstanding that are designed specifically to fuse textual inputs with the\ncorresponding document layout. This involves a separate fine-tuning step for\nwhich additional training data is required. At present, no document\ntransformers with comparable generalization to LLMs are available That raises\nthe question which type of model is to be preferred for document understanding\ntasks. In this paper we investigate the possibility to use purely text-based\nLLMs for document-specific tasks by using layout enrichment. We explore drop-in\nmodifications and rule-based methods to enrich purely textual LLM prompts with\nlayout information. In our experiments we investigate the effects on the\ncommercial ChatGPT model and the open-source LLM Solar. We demonstrate that\nusing our approach both LLMs show improved performance on various standard\ndocument benchmarks. In addition, we study the impact of noisy OCR and layout\nerrors, as well as the limitations of LLMs when it comes to utilizing document\nlayout. Our results indicate that layout enrichment can improve the performance\nof purely text-based LLMs for document understanding by up to 15% compared to\njust using plain document text. In conclusion, this approach should be\nconsidered for the best model choice between text-based LLM or multi-modal\ndocument transformers.", "published": "2024-02-15 10:00:49", "link": "http://arxiv.org/abs/2402.09841v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inadequacies of Large Language Model Benchmarks in the Era of Generative\n  Artificial Intelligence", "abstract": "The rapid rise in popularity of Large Language Models (LLMs) with emerging\ncapabilities has spurred public curiosity to evaluate and compare different\nLLMs, leading many researchers to propose their own LLM benchmarks. Noticing\npreliminary inadequacies in those benchmarks, we embarked on a study to\ncritically assess 23 state-of-the-art LLM benchmarks, using our novel unified\nevaluation framework through the lenses of people, process, and technology,\nunder the pillars of benchmark functionality and integrity. Our research\nuncovered significant limitations, including biases, difficulties in measuring\ngenuine reasoning, adaptability, implementation inconsistencies, prompt\nengineering complexity, evaluator diversity, and the overlooking of cultural\nand ideological norms in one comprehensive assessment. Our discussions\nemphasized the urgent need for standardized methodologies, regulatory\ncertainties, and ethical guidelines in light of Artificial Intelligence (AI)\nadvancements, including advocating for an evolution from static benchmarks to\ndynamic behavioral profiling to accurately capture LLMs' complex behaviors and\npotential risks. Our study highlighted the necessity for a paradigm shift in\nLLM evaluation methodologies, underlining the importance of collaborative\nefforts for the development of universally accepted benchmarks and the\nenhancement of AI systems' integration into society.", "published": "2024-02-15 11:08:10", "link": "http://arxiv.org/abs/2402.09880v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Not Just Novelty: A Longitudinal Study on Utility and Customization of\n  an AI Workflow", "abstract": "Generative AI brings novel and impressive abilities to help people in\neveryday tasks. There are many AI workflows that solve real and complex\nproblems by chaining AI outputs together with human interaction. Although there\nis an undeniable lure of AI, it is uncertain how useful generative AI workflows\nare after the novelty wears off. Additionally, workflows built with generative\nAI have the potential to be easily customized to fit users' individual needs,\nbut do users take advantage of this? We conducted a three-week longitudinal\nstudy with 12 users to understand the familiarization and customization of\ngenerative AI tools for science communication. Our study revealed that there\nexists a familiarization phase, during which users were exploring the novel\ncapabilities of the workflow and discovering which aspects they found useful.\nAfter this phase, users understood the workflow and were able to anticipate the\noutputs. Surprisingly, after familiarization the perceived utility of the\nsystem was rated higher than before, indicating that the perceived utility of\nAI is not just a novelty effect. The increase in benefits mainly comes from\nend-users' ability to customize prompts, and thus potentially appropriate the\nsystem to their own needs. This points to a future where generative AI systems\ncan allow us to design for appropriation.", "published": "2024-02-15 11:39:11", "link": "http://arxiv.org/abs/2402.09894v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Generative Representational Instruction Tuning", "abstract": "All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.", "published": "2024-02-15 12:12:19", "link": "http://arxiv.org/abs/2402.09906v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast Vocabulary Transfer for Language Model Compression", "abstract": "Real-world business applications require a trade-off between language model\nperformance and size. We propose a new method for model compression that relies\non vocabulary transfer. We evaluate the method on various vertical domains and\ndownstream tasks. Our results indicate that vocabulary transfer can be\neffectively used in combination with other compression techniques, yielding a\nsignificant reduction in model size and inference time while marginally\ncompromising on performance.", "published": "2024-02-15 14:37:07", "link": "http://arxiv.org/abs/2402.09977v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed\n  Tasks in the Wild", "abstract": "Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for\nfine-tuning large language models (LLM). The modular and plug-and-play nature\nof LoRA enables the integration of diverse domain-specific LoRAs to enhance the\ncapabilities of LLMs. Previous research on exploiting multiple LoRAs either\nfocuses on specific isolated downstream tasks or fixes the selection of LoRAs\nduring training. However, in real-world scenarios, LLMs receive diverse prompts\ncovering different tasks, and the pool of candidate LoRAs is often dynamically\nupdated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose\nframework that adaptively retrieves and composes multiple LoRAs according to\nthe input prompts. LoraRetriever contains three main components: firstly,\nidentifying and retrieving LoRAs relevant to the given input; secondly,\nformulating strategies for effectively integrating the retrieved LoRAs; and\nthirdly, developing efficient batch inference to accommodate heterogeneous\nrequests. Experimental results indicate that LoraRetriever consistently\noutperforms the baselines, highlighting its practical effectiveness and\nversatility.", "published": "2024-02-15 15:02:46", "link": "http://arxiv.org/abs/2402.09997v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Self-Augmented In-Context Learning for Unsupervised Word Translation", "abstract": "Recent work has shown that, while large language models (LLMs) demonstrate\nstrong word translation or bilingual lexicon induction (BLI) capabilities in\nfew-shot setups, they still cannot match the performance of 'traditional'\nmapping-based approaches in the unsupervised scenario where no seed translation\npairs are available, especially for lower-resource languages. To address this\nchallenge with LLMs, we propose self-augmented in-context learning (SAIL) for\nunsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a\nset of high-confidence word translation pairs for in-context learning (ICL)\nfrom an LLM, which it then reapplies to the same LLM in the ICL fashion. Our\nmethod shows substantial gains over zero-shot prompting of LLMs on two\nestablished BLI benchmarks spanning a wide range of language pairs, also\noutperforming mapping-based baselines across the board. In addition to\nachieving state-of-the-art unsupervised BLI performance, we also conduct\ncomprehensive analyses on SAIL and discuss its limitations.", "published": "2024-02-15 15:43:05", "link": "http://arxiv.org/abs/2402.10024v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization\n  Method for Alignment of Large Language Models", "abstract": "Reinforcement learning from human feedback (RLHF) has been extensively\nemployed to align large language models with user intent. However, proximal\npolicy optimization (PPO) based RLHF is occasionally unstable requiring\nsignificant hyperparameter finetuning, and computationally expensive to\nmaximize the estimated reward during alignment. Recently, direct preference\noptimization (DPO) is proposed to address those challenges. However, DPO relies\non contrastive responses generated from human annotator and alternative LLM,\ninstead of the policy model, limiting the effectiveness of the RLHF. In this\npaper, we addresses both challenges by systematically combining rejection\nsampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the\ndevelopment of a supervised fine-tuned policy model (SFT). A varied set of k\nresponses per prompt are sampled directly from the SFT model. RS-DPO identifies\npairs of contrastive samples based on their reward distribution. Finally, we\napply DPO with the contrastive samples to align the model to human preference.\nOur experiments indicate that our proposed method effectively fine-tunes LLMs\nwith limited resource environments, leading to improved alignment with user\nintent. Furthermore, it outperforms existing methods, including RS, PPO, and\nDPO.", "published": "2024-02-15 16:00:58", "link": "http://arxiv.org/abs/2402.10038v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QUICK: Quantization-aware Interleaving and Conflict-free Kernel for\n  efficient LLM inference", "abstract": "We introduce QUICK, a group of novel optimized CUDA kernels for the efficient\ninference of quantized Large Language Models (LLMs). QUICK addresses the shared\nmemory bank-conflict problem of state-of-the-art mixed precision matrix\nmultiplication kernels. Our method interleaves the quantized weight matrices of\nLLMs offline to skip the shared memory write-back after the dequantization. We\ndemonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger\nbatches and up to 1.94x throughput gain on representative LLM models on various\nNVIDIA GPU devices.", "published": "2024-02-15 16:38:41", "link": "http://arxiv.org/abs/2402.10076v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Reducing Diagnostic Errors with Interpretable Risk Prediction", "abstract": "Many diagnostic errors occur because clinicians cannot easily access relevant\ninformation in patient Electronic Health Records (EHRs). In this work we\npropose a method to use LLMs to identify pieces of evidence in patient EHR data\nthat indicate increased or decreased risk of specific diagnoses; our ultimate\naim is to increase access to evidence and reduce diagnostic errors. In\nparticular, we propose a Neural Additive Model to make predictions backed by\nevidence with individualized risk estimates at time-points where clinicians are\nstill uncertain, aiming to specifically mitigate delays in diagnosis and errors\nstemming from an incomplete differential. To train such a model, it is\nnecessary to infer temporally fine-grained retrospective labels of eventual\n\"true\" diagnoses. We do so with LLMs, to ensure that the input text is from\nbefore a confident diagnosis can be made. We use an LLM to retrieve an initial\npool of evidence, but then refine this set of evidence according to\ncorrelations learned by the model. We conduct an in-depth evaluation of the\nusefulness of our approach by simulating how it might be used by a clinician to\ndecide between a pre-defined list of differential diagnoses.", "published": "2024-02-15 17:05:48", "link": "http://arxiv.org/abs/2402.10109v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM\n  Instruction-Tuning", "abstract": "Instruction tuning is critical to large language models (LLMs) for achieving\nbetter instruction following and task adaptation capabilities but its success\nheavily relies on the training data quality. Many recent methods focus on\nimproving the data quality but often overlook the compatibility of the data\nwith the student model being finetuned. This paper introduces Selective\nReflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection\nand introspection for improving existing data quality with the data selection\ncapability of the student LLM, to automatically refine existing\ninstruction-tuning data. This teacher-student collaboration produces\nhigh-quality and student-compatible instruction-response pairs, resulting in\nsample-efficient instruction tuning and LLMs of superior performance. Selective\nReflection-Tuning is a data augmentation and synthesis that generally improves\nLLM finetuning and self-improvement without collecting brand-new data. We apply\nour method to Alpaca and WizardLM data and achieve much stronger and top-tier\n7B and 13B LLMs.", "published": "2024-02-15 17:06:21", "link": "http://arxiv.org/abs/2402.10110v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset", "abstract": "Recent work has shown the immense potential of synthetically generated\ndatasets for training large language models (LLMs), especially for acquiring\ntargeted skills. Current large-scale math instruction tuning datasets such as\nMetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed\nusing outputs from closed-source LLMs with commercially restrictive licenses. A\nkey reason limiting the use of open-source LLMs in these data generation\npipelines has been the wide gap between the mathematical skills of the best\nclosed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on\nthe recent progress in open-source LLMs, our proposed prompting novelty, and\nsome brute-force scaling, we construct OpenMathInstruct-1, a math instruction\ntuning dataset with 1.8M problem-solution pairs. The dataset is constructed by\nsynthesizing code-interpreter solutions for GSM8K and MATH, two popular math\nreasoning benchmarks, using the recently released and permissively licensed\nMixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of\nOpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which\nis competitive with the best gpt-distilled models. We release our code, models,\nand the OpenMathInstruct-1 dataset under a commercially permissive license.", "published": "2024-02-15 18:26:11", "link": "http://arxiv.org/abs/2402.10176v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reward Generalization in RLHF: A Topological Perspective", "abstract": "Existing alignment methods share a common topology of information flow, where\nreward information is collected from humans, modeled with preference learning,\nand used to tune language models. However, this shared topology has not been\nsystematically characterized, nor have its alternatives been thoroughly\nexplored, leaving the problems of low data efficiency and unreliable\ngeneralization unaddressed. As a solution, we introduce a theoretical framework\nfor investigating reward generalization in reinforcement learning from human\nfeedback (RLHF), focusing on the topology of information flow at both macro and\nmicro levels. At the macro level, we portray the RLHF information flow as an\nautoencoding process over behavior distributions, formalizing the RLHF\nobjective of distributional consistency between human preference and model\nbehavior. At the micro level, we present induced Bayesian networks as a theory\nof reward generalization in RLHF, introducing fine-grained dataset topologies\ninto generalization bounds. Combining analysis on both levels, we propose\nreward modeling from tree-structured preference information. It is shown to\nreduce reward uncertainty by up to $\\Theta(\\log n/\\log\\log n)$ times compared\nto baselines, where $n$ is the dataset size. Validation on three NLP tasks\nshows that our tree-based reward model achieves an average win rate of 65%\nagainst baseline methods, thus improving reward generalization for free via\ntopology design.", "published": "2024-02-15 18:39:24", "link": "http://arxiv.org/abs/2402.10184v6", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DM"], "primary_category": "cs.LG"}
{"title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with\n  Dynamic Preference Adjustment", "abstract": "We consider the problem of multi-objective alignment of foundation models\nwith human preferences, which is a critical step towards helpful and harmless\nAI systems. However, it is generally costly and unstable to fine-tune large\nfoundation models using reinforcement learning (RL), and the\nmulti-dimensionality, heterogeneity, and conflicting nature of human\npreferences further complicate the alignment process. In this paper, we\nintroduce Rewards-in-Context (RiC), which conditions the response of a\nfoundation model on multiple rewards in its prompt context and applies\nsupervised fine-tuning for alignment. The salient features of RiC are\nsimplicity and adaptivity, as it only requires supervised fine-tuning of a\nsingle foundation model and supports dynamic adjustment for user preferences\nduring inference time. Inspired by the analytical solution of an abstracted\nconvex optimization problem, our dynamic inference-time adjustment method\napproaches the Pareto-optimal solution for multiple objectives. Empirical\nevidence demonstrates the efficacy of our method in aligning both Large\nLanguage Models (LLMs) and diffusion models to accommodate diverse rewards with\nonly around 10% GPU hours compared with multi-objective RL baseline.", "published": "2024-02-15 18:58:31", "link": "http://arxiv.org/abs/2402.10207v6", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Recovering the Pre-Fine-Tuning Weights of Generative Models", "abstract": "The dominant paradigm in generative modeling consists of two steps: i)\npre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained\nmodel with human values via fine-tuning. This practice is considered safe, as\nno current method can recover the unsafe, pre-fine-tuning model weights. In\nthis paper, we demonstrate that this assumption is often false. Concretely, we\npresent Spectral DeTuning, a method that can recover the weights of the\npre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In\ncontrast to previous attacks that attempt to recover pre-fine-tuning\ncapabilities, our method aims to recover the exact pre-fine-tuning weights. Our\napproach exploits this new vulnerability against large-scale models such as a\npersonalized Stable Diffusion and an aligned Mistral.", "published": "2024-02-15 18:59:02", "link": "http://arxiv.org/abs/2402.10208v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A StrongREJECT for Empty Jailbreaks", "abstract": "Most jailbreak papers claim the jailbreaks they propose are highly effective,\noften boasting near-100% attack success rates. However, it is perhaps more\ncommon than not for jailbreak developers to substantially exaggerate the\neffectiveness of their jailbreaks. We suggest this problem arises because\njailbreak researchers lack a standard, high-quality benchmark for evaluating\njailbreak performance, leaving researchers to create their own. To create a\nbenchmark, researchers must choose a dataset of forbidden prompts to which a\nvictim model will respond, along with an evaluation method that scores the\nharmfulness of the victim model's responses. We show that existing benchmarks\nsuffer from significant shortcomings and introduce the StrongREJECT benchmark\nto address these issues. StrongREJECT's dataset contains prompts that victim\nmodels must answer with specific, harmful information, while its automated\nevaluator measures the extent to which a response gives useful information to\nforbidden prompts. In doing so, the StrongREJECT evaluator achieves\nstate-of-the-art agreement with human judgments of jailbreak effectiveness.\nNotably, we find that existing evaluation methods significantly overstate\njailbreak effectiveness compared to human judgments and the StrongREJECT\nevaluator. We describe a surprising and novel phenomenon that explains this\ndiscrepancy: jailbreaks bypassing a victim model's safety fine-tuning tend to\nreduce its capabilities. Together, our findings underscore the need for\nresearchers to use a high-quality benchmark, such as StrongREJECT, when\ndeveloping new jailbreak attacks. We release the StrongREJECT code and data at\nhttps://strong-reject.readthedocs.io/en/latest/.", "published": "2024-02-15 18:58:09", "link": "http://arxiv.org/abs/2402.10260v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video\n  Editing", "abstract": "Video creation has become increasingly popular, yet the expertise and effort\nrequired for editing often pose barriers to beginners. In this paper, we\nexplore the integration of large language models (LLMs) into the video editing\nworkflow to reduce these barriers. Our design vision is embodied in LAVE, a\nnovel system that provides LLM-powered agent assistance and language-augmented\nediting features. LAVE automatically generates language descriptions for the\nuser's footage, serving as the foundation for enabling the LLM to process\nvideos and assist in editing tasks. When the user provides editing objectives,\nthe agent plans and executes relevant actions to fulfill them. Moreover, LAVE\nallows users to edit videos through either the agent or direct UI manipulation,\nproviding flexibility and enabling manual refinement of agent actions. Our user\nstudy, which included eight participants ranging from novices to proficient\neditors, demonstrated LAVE's effectiveness. The results also shed light on user\nperceptions of the proposed LLM-assisted editing paradigm and its impact on\nusers' creativity and sense of co-creation. Based on these findings, we propose\ndesign implications to inform the future development of agent-assisted content\nediting.", "published": "2024-02-15 19:53:11", "link": "http://arxiv.org/abs/2402.10294v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.HC"}
{"title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on\n  Efficient Data Utilization", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has achieved impressive\nempirical successes while relying on a small amount of human feedback. However,\nthere is limited theoretical justification for this phenomenon. Additionally,\nmost recent studies focus on value-based algorithms despite the recent\nempirical successes of policy-based algorithms. In this work, we consider an\nRLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based\non the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes\nknowledge of the reward function. In PO-RLHF, knowledge of the reward function\nis not assumed, and the algorithm uses trajectory-based comparison feedback to\ninfer the reward function. We provide performance bounds for PO-RLHF with low\nquery complexity, which provides insight into why a small amount of human\nfeedback may be sufficient to achieve good performance with RLHF. A key novelty\nis a trajectory-level elliptical potential analysis, which bounds the reward\nestimation error when comparison feedback (rather than numerical reward\nobservation) is given. We provide and analyze algorithms PG-RLHF and NN-PG-RLHF\nfor two settings: linear and neural function approximation, respectively.", "published": "2024-02-15 22:11:18", "link": "http://arxiv.org/abs/2402.10342v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models\n  for Medical Domains", "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in\nrecent years, offering potential applications across specialized domains such\nas healthcare and medicine. Despite the availability of various open-source\nLLMs tailored for health contexts, adapting general-purpose LLMs to the medical\ndomain presents significant challenges. In this paper, we introduce BioMistral,\nan open-source LLM tailored for the biomedical domain, utilizing Mistral as its\nfoundation model and further pre-trained on PubMed Central. We conduct a\ncomprehensive evaluation of BioMistral on a benchmark comprising 10 established\nmedical question-answering (QA) tasks in English. We also explore lightweight\nmodels obtained through quantization and model merging approaches. Our results\ndemonstrate BioMistral's superior performance compared to existing open-source\nmedical models and its competitive edge against proprietary counterparts.\nFinally, to address the limited availability of data beyond English and to\nassess the multilingual generalization of medical LLMs, we automatically\ntranslated and evaluated this benchmark into 7 other languages. This marks the\nfirst large-scale multilingual evaluation of LLMs in the medical domain.\nDatasets, multilingual evaluation benchmarks, scripts, and all the models\nobtained during our experiments are freely released.", "published": "2024-02-15 23:39:04", "link": "http://arxiv.org/abs/2402.10373v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models with Conformal Factuality Guarantees", "abstract": "Guaranteeing the correctness and factuality of language model (LM) outputs is\na major open problem. In this work, we propose conformal factuality, a\nframework that can ensure high probability correctness guarantees for LMs by\nconnecting language modeling and conformal prediction. We observe that the\ncorrectness of an LM output is equivalent to an uncertainty quantification\nproblem, where the uncertainty sets are defined as the entailment set of an\nLM's output. Using this connection, we show that conformal prediction in\nlanguage models corresponds to a back-off algorithm that provides high\nprobability correctness guarantees by progressively making LM outputs less\nspecific (and expanding the associated uncertainty sets). This approach applies\nto any black-box LM and requires very few human-annotated samples. Evaluations\nof our approach on closed book QA (FActScore, NaturalQuestions) and reasoning\ntasks (MATH) show that our approach can provide 80-90% correctness guarantees\nwhile retaining the majority of the LM's original output.", "published": "2024-02-15 18:31:53", "link": "http://arxiv.org/abs/2402.10978v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Analyzing the Roles of Language and Vision in Learning from Limited Data", "abstract": "Does language help make sense of the visual world? How important is it to\nactually see the world rather than having it described with words? These basic\nquestions about the nature of intelligence have been difficult to answer\nbecause we only had one example of an intelligent system -- humans -- and\nlimited access to cases that isolated language or vision. However, the\ndevelopment of sophisticated Vision-Language Models (VLMs) by artificial\nintelligence researchers offers us new opportunities to explore the\ncontributions that language and vision make to learning about the world. We\nablate components from the cognitive architecture of these models to identify\ntheir contributions to learning new tasks from limited data. We find that a\nlanguage model leveraging all components recovers a majority of a VLM's\nperformance, despite its lack of visual input, and that language seems to allow\nthis by providing access to prior knowledge and reasoning.", "published": "2024-02-15 22:19:41", "link": "http://arxiv.org/abs/2403.19669v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Neural Information Organizing and Processing -- Neural Machines", "abstract": "The informational synthesis of neural structures, processes, parameters and\ncharacteristics that allow a unified description and modeling as neural\nmachines of natural and artificial neural systems is presented. The general\ninformational parameters as the global quantitative measure of the neural\nsystems computing potential as absolute and relative neural power were\nproposed. Neural information organizing and processing follows the way in which\nnature manages neural information by developing functions, functionalities and\ncircuits related to different internal or peripheral components and also to the\nwhole system through a non-deterministic memorization, fragmentation and\naggregation of afferent and efferent information, deep neural information\nprocessing representing multiple alternations of fragmentation and aggregation\nstages. The relevant neural characteristics were integrated into a neural\nmachine type model that incorporates unitary also peripheral or interface\ncomponents as the central ones. The proposed approach allows overcoming the\ntechnical constraints in artificial computational implementations of neural\ninformation processes and also provides a more relevant description of natural\nones.", "published": "2024-02-15 15:15:11", "link": "http://arxiv.org/abs/2404.03676v1", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "GPT-4's assessment of its performance in a USMLE-based case study", "abstract": "This study investigates GPT-4's assessment of its performance in healthcare\napplications. A simple prompting technique was used to prompt the LLM with\nquestions taken from the United States Medical Licensing Examination (USMLE)\nquestionnaire and it was tasked to evaluate its confidence score before posing\nthe question and after asking the question. The questionnaire was categorized\ninto two groups-questions with feedback (WF) and questions with no feedback(NF)\npost-question. The model was asked to provide absolute and relative confidence\nscores before and after each question. The experimental findings were analyzed\nusing statistical tools to study the variability of confidence in WF and NF\ngroups. Additionally, a sequential analysis was conducted to observe the\nperformance variation for the WF and NF groups. Results indicate that feedback\ninfluences relative confidence but doesn't consistently increase or decrease\nit. Understanding the performance of LLM is paramount in exploring its utility\nin sensitive areas like healthcare. This study contributes to the ongoing\ndiscourse on the reliability of AI, particularly of LLMs like GPT-4, within\nhealthcare, offering insights into how feedback mechanisms might be optimized\nto enhance AI-assisted medical education and decision support.", "published": "2024-02-15 01:38:50", "link": "http://arxiv.org/abs/2402.09654v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Generative AI in the Construction Industry: A State-of-the-art Analysis", "abstract": "The construction industry is a vital sector of the global economy, but it\nfaces many productivity challenges in various processes, such as design,\nplanning, procurement, inspection, and maintenance. Generative artificial\nintelligence (AI), which can create novel and realistic data or content, such\nas text, image, video, or code, based on some input or prior knowledge, offers\ninnovative and disruptive solutions to address these challenges. However, there\nis a gap in the literature on the current state, opportunities, and challenges\nof generative AI in the construction industry. This study aims to fill this gap\nby providing a state-of-the-art analysis of generative AI in construction, with\nthree objectives: (1) to review and categorize the existing and emerging\ngenerative AI opportunities and challenges in the construction industry; (2) to\npropose a framework for construction firms to build customized generative AI\nsolutions using their own data, comprising steps such as data collection,\ndataset curation, training custom large language model (LLM), model evaluation,\nand deployment; and (3) to demonstrate the framework via a case study of\ndeveloping a generative model for querying contract documents. The results show\nthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,\n9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study\nprovides academics and construction professionals with a comprehensive analysis\nand practical framework to guide the adoption of generative AI techniques to\nenhance productivity, quality, safety, and sustainability across the\nconstruction industry.", "published": "2024-02-15 13:39:55", "link": "http://arxiv.org/abs/2402.09939v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation", "abstract": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.", "published": "2024-02-15 18:59:18", "link": "http://arxiv.org/abs/2402.10210v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A cross-talk robust multichannel VAD model for multiparty agent\n  interactions trained using synthetic re-recordings", "abstract": "In this work, we propose a novel cross-talk rejection framework for a\nmulti-channel multi-talker setup for a live multiparty interactive show. Our\nfar-field audio setup is required to be hands-free during live interaction and\ncomprises four adjacent talkers with directional microphones in the same space.\nSuch setups often introduce heavy cross-talk between channels, resulting in\nreduced automatic speech recognition (ASR) and natural language understanding\n(NLU) performance. To address this problem, we propose voice activity detection\n(VAD) model for all talkers using multichannel information, which is then used\nto filter audio for downstream tasks. We adopt a synthetic training data\ngeneration approach through playback and re-recording for such scenarios,\nsimulating challenging speech overlap conditions. We train our models on this\nsynthetic data and demonstrate that our approach outperforms single-channel VAD\nmodels and energy-based multi-channel VAD algorithm in various acoustic\nenvironments. In addition to VAD results, we also present multiparty ASR\nevaluation results to highlight the impact of using our VAD model for filtering\naudio in downstream tasks by significantly reducing the insertion error.", "published": "2024-02-15 08:52:31", "link": "http://arxiv.org/abs/2402.09797v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion Models for Audio Restoration", "abstract": "With the development of audio playback devices and fast data transmission,\nthe demand for high sound quality is rising for both entertainment and\ncommunications. In this quest for better sound quality, challenges emerge from\ndistortions and interferences originating at the recording side or caused by an\nimperfect transmission pipeline. To address this problem, audio restoration\nmethods aim to recover clean sound signals from the corrupted input data. We\npresent here audio restoration algorithms based on diffusion models, with a\nfocus on speech enhancement and music restoration tasks. Traditional\napproaches, often grounded in handcrafted rules and statistical heuristics,\nhave shaped our understanding of audio signals. In the past decades, there has\nbeen a notable shift towards data-driven methods that exploit the modeling\ncapabilities of DNNs. Deep generative models, and among them diffusion models,\nhave emerged as powerful techniques for learning complex data distributions.\nHowever, relying solely on DNN-based learning approaches carries the risk of\nreducing interpretability, particularly when employing end-to-end models.\nNonetheless, data-driven approaches allow more flexibility in comparison to\nstatistical model-based frameworks, whose performance depends on distributional\nand statistical assumptions that can be difficult to guarantee. Here, we aim to\nshow that diffusion models can combine the best of both worlds and offer the\nopportunity to design audio restoration algorithms with a good degree of\ninterpretability and a remarkable performance in terms of sound quality. We\nexplain the diffusion formalism and its application to the conditional\ngeneration of clean audio signals. We believe that diffusion models open an\nexciting field of research with the potential to spawn new audio restoration\nalgorithms that are natural-sounding and remain robust in difficult acoustic\nsituations.", "published": "2024-02-15 09:36:36", "link": "http://arxiv.org/abs/2402.09821v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MuChin: A Chinese Colloquial Description Benchmark for Evaluating\n  Language Models in the Field of Music", "abstract": "The rapidly evolving multimodal Large Language Models (LLMs) urgently require\nnew benchmarks to uniformly evaluate their performance on understanding and\ntextually describing music. However, due to semantic gaps between Music\nInformation Retrieval (MIR) algorithms and human understanding, discrepancies\nbetween professionals and the public, and low precision of annotations,\nexisting music description datasets cannot serve as benchmarks. To this end, we\npresent MuChin, the first open-source music description benchmark in Chinese\ncolloquial language, designed to evaluate the performance of multimodal LLMs in\nunderstanding and describing music. We established the Caichong Music\nAnnotation Platform (CaiMAP) that employs an innovative multi-person,\nmulti-stage assurance method, and recruited both amateurs and professionals to\nensure the precision of annotations and alignment with popular semantics.\nUtilizing this method, we built a dataset with multi-dimensional,\nhigh-precision music annotations, the Caichong Music Dataset (CaiMD), and\ncarefully selected 1,000 high-quality entries to serve as the test set for\nMuChin. Based on MuChin, we analyzed the discrepancies between professionals\nand amateurs in terms of music description, and empirically demonstrated the\neffectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed\nMuChin to evaluate existing music understanding models on their ability to\nprovide colloquial descriptions of music. All data related to the benchmark,\nalong with the scoring code and detailed appendices, have been open-sourced\n(https://github.com/CarlWangChina/MuChin/).", "published": "2024-02-15 10:55:01", "link": "http://arxiv.org/abs/2402.09871v4", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68Txx(Primary)14F05, 91Fxx(Secondary)", "I.2.7; J.5"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion", "abstract": "Editing signals using large pre-trained models, in a zero-shot manner, has\nrecently seen rapid advancements in the image domain. However, this wave has\nyet to reach the audio domain. In this paper, we explore two zero-shot editing\ntechniques for audio signals, which use DDPM inversion with pre-trained\ndiffusion models. The first, which we coin ZEro-shot Text-based Audio (ZETA)\nediting, is adopted from the image domain. The second, named ZEro-shot\nUnSupervized (ZEUS) editing, is a novel approach for discovering semantically\nmeaningful editing directions without supervision. When applied to music\nsignals, this method exposes a range of musically interesting modifications,\nfrom controlling the participation of specific instruments to improvisations on\nthe melody. Samples and code can be found in\nhttps://hilamanor.github.io/AudioEditing/ .", "published": "2024-02-15 15:17:26", "link": "http://arxiv.org/abs/2402.10009v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Engraving Oriented Joint Estimation of Pitch Spelling and Local and\n  Global Keys", "abstract": "We revisit the problems of pitch spelling and tonality guessing with a new\nalgorithm for their joint estimation from a MIDI file including information\nabout the measure boundaries. Our algorithm does not only identify a global key\nbut also local ones all along the analyzed piece. It uses Dynamic Programming\ntechniques to search for an optimal spelling in term, roughly, of the number of\naccidental symbols that would be displayed in the engraved score. The\nevaluation of this number is coupled with an estimation of the global key and\nsome local keys, one for each measure. Each of the three informations is used\nfor the estimation of the other, in a multi-steps procedure. An evaluation\nconducted on a monophonic and a piano dataset, comprising 216 464 notes in\ntotal, shows a high degree of accuracy, both for pitch spelling (99.5% on\naverage on the Bach corpus and 98.2% on the whole dataset) and global key\nsignature estimation (93.0% on average, 95.58% on the piano dataset). Designed\noriginally as a backend tool in a music transcription framework, this method\nshould also be useful in other tasks related to music notation processing.", "published": "2024-02-15 10:28:59", "link": "http://arxiv.org/abs/2402.10247v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DeepSRGM -- Sequence Classification and Ranking in Indian Classical\n  Music with Deep Learning", "abstract": "A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a\nmelodic framework for compositions and improvisations alike. Raga Recognition\nis an important music information retrieval task in ICM as it can aid numerous\ndownstream applications ranging from music recommendations to organizing huge\nmusic collections. In this work, we propose a deep learning based approach to\nRaga recognition. Our approach employs efficient pre possessing and learns\ntemporal sequences in music data using Long Short Term Memory based Recurrent\nNeural Networks (LSTM-RNN). We train and test the network on smaller sequences\nsampled from the original audio while the final inference is performed on the\naudio as a whole. Our method achieves an accuracy of 88.1% and 97 % during\ninference on the Comp Music Carnatic dataset and its 10 Raga subset\nrespectively making it the state-of-the-art for the Raga recognition task. Our\napproach also enables sequence ranking which aids us in retrieving melodic\npatterns from a given music data base that are closely related to the presented\nquery sequence.", "published": "2024-02-15 18:11:02", "link": "http://arxiv.org/abs/2402.10168v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
