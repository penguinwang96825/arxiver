{"title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization", "abstract": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences.", "published": "2025-07-19 21:23:23", "link": "http://arxiv.org/abs/2507.14758v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the robustness of modeling grounded word learning through a child's egocentric input", "abstract": "What insights can machine learning bring to understanding human language\nacquisition? Large language and multimodal models have achieved remarkable\ncapabilities, but their reliance on massive training datasets creates a\nfundamental mismatch with children, who succeed in acquiring language from\ncomparatively limited input. To help bridge this gap, researchers have\nincreasingly trained neural networks using data similar in quantity and quality\nto children's input. Taking this approach to the limit, Vong et al. (2024)\nshowed that a multimodal neural network trained on 61 hours of visual and\nlinguistic input extracted from just one child's developmental experience could\nacquire word-referent mappings. However, whether this approach's success\nreflects the idiosyncrasies of a single child's experience, or whether it would\nshow consistent and robust learning patterns across multiple children's\nexperiences was not explored. In this article, we applied automated speech\ntranscription methods to the entirety of the SAYCam dataset, consisting of over\n500 hours of video data spread across all three children. Using these automated\ntranscriptions, we generated multi-modal vision-and-language datasets for both\ntraining and evaluation, and explored a range of neural network configurations\nto examine the robustness of simulated word learning. Our findings demonstrate\nthat networks trained on automatically transcribed data from each child can\nacquire and generalize word-referent mappings across multiple network\narchitectures. These results validate the robustness of multimodal neural\nnetworks for grounded word learning, while highlighting the individual\ndifferences that emerge in how models learn when trained on each child's\ndevelopmental experiences.", "published": "2025-07-19 20:55:37", "link": "http://arxiv.org/abs/2507.14749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disparities in Peer Review Tone and the Role of Reviewer Anonymity", "abstract": "The peer review process is often regarded as the gatekeeper of scientific\nintegrity, yet increasing evidence suggests that it is not immune to bias.\nAlthough structural inequities in peer review have been widely debated, much\nless attention has been paid to the subtle ways in which language itself may\nreinforce disparities. This study undertakes one of the most comprehensive\nlinguistic analyses of peer review to date, examining more than 80,000 reviews\nin two major journals. Using natural language processing and large-scale\nstatistical modeling, it uncovers how review tone, sentiment, and supportive\nlanguage vary across author demographics, including gender, race, and\ninstitutional affiliation. Using a data set that includes both anonymous and\nsigned reviews, this research also reveals how the disclosure of reviewer\nidentity shapes the language of evaluation. The findings not only expose hidden\nbiases in peer feedback, but also challenge conventional assumptions about\nanonymity's role in fairness. As academic publishing grapples with reform,\nthese insights raise critical questions about how review policies shape career\ntrajectories and scientific progress.", "published": "2025-07-19 20:19:21", "link": "http://arxiv.org/abs/2507.14741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "abstract": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability.", "published": "2025-07-19 16:54:36", "link": "http://arxiv.org/abs/2507.14693v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations", "abstract": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment.", "published": "2025-07-19 16:30:45", "link": "http://arxiv.org/abs/2507.14688v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization", "abstract": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.", "published": "2025-07-19 16:21:23", "link": "http://arxiv.org/abs/2507.14683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care", "abstract": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation.", "published": "2025-07-19 16:11:10", "link": "http://arxiv.org/abs/2507.14681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks", "abstract": "The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.", "published": "2025-07-19 16:09:48", "link": "http://arxiv.org/abs/2507.14679v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Docopilot: Improving Multimodal Models for Document-Level Understanding", "abstract": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot", "published": "2025-07-19 16:03:34", "link": "http://arxiv.org/abs/2507.14675v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "abstract": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research.", "published": "2025-07-19 15:28:58", "link": "http://arxiv.org/abs/2507.14664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "abstract": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "published": "2025-07-19 15:17:30", "link": "http://arxiv.org/abs/2507.14660v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs", "abstract": "Despite the outstanding performance of large language models (LLMs) across\nvarious NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate\nresponses--remains as a critical problem as it can be directly connected to a\ncrisis of building safe and reliable LLMs. Uncertainty estimation is primarily\nused to measure hallucination levels in LLM responses so that correct and\nincorrect answers can be distinguished clearly. This study proposes an\neffective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based\nsem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse\nquantifies the uncertainty with the proportion of the intra-cluster consistency\nin the total consistency between LLM hidden embeddings which contain adequate\nsemantic information of generations, by employing clustering. The effectiveness\nof Cleanse for detecting hallucination is validated using four off-the-shelf\nmodels, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two\nquestion-answering benchmarks, SQuAD and CoQA.", "published": "2025-07-19 14:48:24", "link": "http://arxiv.org/abs/2507.14649v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Linear Relational Decoding of Morphology in Language Models", "abstract": "A two-part affine approximation has been found to be a good approximation for\ntransformer computations over certain subject object relations. Adapting the\nBigger Analogy Test Set, we show that the linear transformation Ws, where s is\na middle layer representation of a subject token and W is derived from model\nderivatives, is also able to accurately reproduce final object states for many\nrelations. This linear technique is able to achieve 90% faithfulness on\nmorphological relations, and we show similar findings multi-lingually and\nacross models. Our findings indicate that some conceptual relationships in\nlanguage models, such as morphology, are readily interpretable from latent\nspace, and are sparsely encoded by cross-layer linear transformations.", "published": "2025-07-19 14:35:15", "link": "http://arxiv.org/abs/2507.14640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining", "abstract": "Large Language Models (LLMs) face significant challenges in specialized\ndomains like law, where precision and domain-specific knowledge are critical.\nThis paper presents a streamlined two-stage framework consisting of Retrieval\nand Re-ranking to enhance legal document retrieval efficiency and accuracy. Our\napproach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,\nfollowed by a Cross-Encoder for precise re-ranking, both optimized through\nstrategic negative example mining. Key innovations include the introduction of\nthe Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard\nnegatives to mitigate training bias, which significantly improved re-ranking\nperformance. Evaluated on the SoICT Hackathon 2024 for Legal Document\nRetrieval, our team, 4Huiter, achieved a top-three position. While\ntop-performing teams employed ensemble models and iterative self-training on\nlarge bge-m3 architectures, our lightweight, single-pass approach offered a\ncompetitive alternative with far fewer parameters. The framework demonstrates\nthat optimized data processing, tailored loss functions, and balanced negative\nsampling are pivotal for building robust retrieval-augmented systems in legal\ncontexts.", "published": "2025-07-19 13:30:14", "link": "http://arxiv.org/abs/2507.14619v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper", "abstract": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems.", "published": "2025-07-19 13:25:26", "link": "http://arxiv.org/abs/2507.14615v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "abstract": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.", "published": "2025-07-19 12:23:20", "link": "http://arxiv.org/abs/2507.14590v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What do Large Language Models know about materials?", "abstract": "Large Language Models (LLMs) are increasingly applied in the fields of\nmechanical engineering and materials science. As models that establish\nconnections through the interface of language, LLMs can be applied for\nstep-wise reasoning through the Processing-Structure-Property-Performance chain\nof material science and engineering. Current LLMs are built for adequately\nrepresenting a dataset, which is the most part of the accessible internet.\nHowever, the internet mostly contains non-scientific content. If LLMs should be\napplied for engineering purposes, it is valuable to investigate models for\ntheir intrinsic knowledge -- here: the capacity to generate correct information\nabout materials. In the current work, for the example of the Periodic Table of\nElements, we highlight the role of vocabulary and tokenization for the\nuniqueness of material fingerprints, and the LLMs' capabilities of generating\nfactually correct output of different state-of-the-art open models. This leads\nto a material knowledge benchmark for an informed choice, for which steps in\nthe PSPP chain LLMs are applicable, and where specialized models are required.", "published": "2025-07-19 12:02:08", "link": "http://arxiv.org/abs/2507.14586v1", "categories": ["physics.app-ph", "cs.CE", "cs.CL"], "primary_category": "physics.app-ph"}
{"title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "abstract": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills.", "published": "2025-07-19 11:57:24", "link": "http://arxiv.org/abs/2507.14584v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "abstract": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.", "published": "2025-07-19 11:47:08", "link": "http://arxiv.org/abs/2507.14579v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "abstract": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.", "published": "2025-07-19 11:40:37", "link": "http://arxiv.org/abs/2507.14578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "abstract": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.", "published": "2025-07-19 08:32:07", "link": "http://arxiv.org/abs/2507.14534v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Whole Slide Pathology VQA via Token Compression", "abstract": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "published": "2025-07-19 06:04:25", "link": "http://arxiv.org/abs/2507.14497v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "abstract": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "published": "2025-07-19 02:46:19", "link": "http://arxiv.org/abs/2507.14447v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "abstract": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "published": "2025-07-19 01:20:39", "link": "http://arxiv.org/abs/2507.14430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It's Not That Simple. An Analysis of Simple Test-Time Scaling", "abstract": "Prior work proposed simple test-time scaling, a method for replicating this\nscaling behavior with models distilled from o1-like models by manually\ncontrolling test-time compute: either scaling down by enforcing a maximum\nlength or scaling up by iteratively appending \"Wait\" when the model is about to\nterminate its generation. This paper presents an analysis of simple test-time\nscaling and finds that the scaling behavior is largely attributed to scaling\ndown by enforcing a maximum length. In contrast, fine-tuning on long CoT data\ndistilled from o1-like models has no significant impact on scaling behavior,\nand scaling up by appending \"Wait\" leads to inconsistencies, as the model may\noscillate between solutions. A key distinction exists between scaling down by\nenforcing a maximum length and scaling up test-time compute in o1-like models,\nsuch as DeepSeek-R1\\@. These models are typically allowed to utilize as much\ncompute as needed, with the only constraint being the model's maximum supported\nlength. By learning to naturally scale up test-time compute during\nreinforcement learning, o1-like models surpass their peak performance when\nscaling up. In contrast, simple test-time scaling progressively imposes a lower\nupper limit on model performance as it scales down. While replicating the\ntest-time scaling behavior of o1 models can be straightforward by scaling down,\nit is crucial to recognize that the goal of scaling test-time compute is to\nunlock higher performance -- beyond what the model could originally achieve --\nrather than merely reproducing the appearance of scaling behavior.", "published": "2025-07-19 00:28:10", "link": "http://arxiv.org/abs/2507.14419v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Inverse Scaling in Test-Time Compute", "abstract": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.", "published": "2025-07-19 00:06:13", "link": "http://arxiv.org/abs/2507.14417v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Dvorak-Dell-Grohe-Rattan theorem via an asymptotic argument", "abstract": "Two graphs $G_1,G_2$ are distinguished by the Weisfeiler--Leman isomorphism\ntest if and only if there is a tree $T$ that has a different number of\nhomomorphisms to $G_1$ and to $G_2$. There are two known proofs of this fact --\na logical proof by Dvorak and a linear-algebraic proof by Dell, Grohe, and\nRattan. We give another simple proof, based on ordering WL-labels and\nasymptotic arguments.", "published": "2025-07-19 15:40:28", "link": "http://arxiv.org/abs/2507.14669v1", "categories": ["math.CO", "cs.DM", "cs.DS"], "primary_category": "math.CO"}
{"title": "Graphs With the Same Edge Count in Each Neighborhood", "abstract": "In a recent paper, Caro, Lauri, Mifsud, Yuster, and Zarb ask which parameters\n$r$ and $c$ admit the existence of an $r$-regular graph such that the\nneighborhood of each vertex induces exactly $c$ edges. They show that every $r$\nwith $c$ satisfying $0\\leq c\\leq {r\\choose 2}-5r^{3/2}$ is achievable, but no\n$r$ with $c$ satisfying ${r\\choose 2}-\\lfloor\\frac{r}{3}\\rfloor\\leq c\\leq\n{r\\choose 2}-1$ is. We strengthen the bound in their nonexistence result from\n${r\\choose 2}-\\lfloor\\frac{r}{3}\\rfloor$ to ${r\\choose\n2}-\\lfloor\\frac{r-2}{2}\\rfloor$. Additionally, when the graph is the Cayley\ngraph of an abelian group, we obtain a much more fine-grained characterization\nof the achievable values of $c$ between $\\binom{r}{2} - 5r^{3/2}$ and\n$\\binom{r}{2} - \\lfloor\\frac{r-2}{2}\\rfloor$, which we conjecture to be the\ncorrect answer for general graphs as well. That result relies on a lemma about\napproximate subgroups in the \"99% regime,\" quantifying the extent to which\nnearly-additively-closed subsets of an abelian group must be close to actual\nsubgroups. Finally, we consider a generalization to graphs with multiple types\nof edges and partially resolve several open questions of Caro et al. about\n$\\textit{flip}$ colorings of graphs.", "published": "2025-07-19 04:08:04", "link": "http://arxiv.org/abs/2507.14473v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Pseudorandomness of Expander Walks via Fourier Analysis on Groups", "abstract": "One approach to study the pseudorandomness properties of walks on expander\ngraphs is to label the vertices of an expander with elements from an alphabet\n$\\Sigma$, and study the mean of functions over $\\Sigma^n$. We say expander\nwalks $\\varepsilon$-fool a function if, for any unbiased labeling of the\nvertices, the expander walk mean is $\\varepsilon$-close to the true mean. We\nshow that:\n  - The class of symmetric functions is $O(|\\Sigma|\\cdot\\lambda)$-fooled by\nexpander walks over any generic $\\lambda$-expander, and any alphabet $\\Sigma$ .\nThis generalizes the result of Cohen, Peri, Ta-Shma [STOC'21] which analyzes it\nfor $|\\Sigma| =2$, and exponentially improves the previous bound of\n$O(|\\Sigma|^{O(|\\Sigma|)}\\cdot \\lambda)$, by Golowich and Vadhan [CCC'22].\nAdditionally, if the expander is a Cayley graph over $\\mathbb{Z}_{|\\Sigma|}$,\nwe get a further improved bound of $O(\\sqrt{|\\Sigma|}\\cdot\\lambda)$.\n  Morever, when $\\Sigma$ is a finite group $G$, we show the following for\nfunctions over $G^n$:\n  - The class of symmetric class functions is\n$O\\Big({\\frac{\\sqrt{|G|}}{D}\\cdot\\lambda}\\Big)$-fooled by expander walks over\n\"structured\" $\\lambda$-expanders, if $G$ is $D$-quasirandom.\n  - We show a lower bound of $\\Omega(\\lambda)$ for symmetric functions for any\nfinite group $G$ (even for \"structured\" $\\lambda$-expanders).\n  - We study the Fourier spectrum of a class of non-symmetric functions arising\nfrom word maps, and show that they are exponentially fooled by expander walks.\n  Our proof employs Fourier analysis over general groups, which contrasts with\nearlier works that have studied either the case of $\\mathbb{Z}_2$ or\n$\\mathbb{Z}$. This enables us to get quantitatively better bounds even for\nunstructured sets.", "published": "2025-07-19 02:44:34", "link": "http://arxiv.org/abs/2507.14445v1", "categories": ["cs.CC", "cs.DM", "math.CO"], "primary_category": "cs.CC"}
{"title": "Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module", "abstract": "Next point of interest (POI) recommendation primarily predicts future\nactivities based on users' past check-in data and current status, providing\nsignificant value to users and service providers. We observed that the popular\ncheck-in times for different POI categories vary. For example, coffee shops are\ncrowded in the afternoon because people like to have coffee to refresh after\nmeals, while bars are busy late at night. However, existing methods rarely\nexplore the relationship between POI categories and time, which may result in\nthe model being unable to fully learn users' tendencies to visit certain POI\ncategories at different times. Additionally, existing methods for modeling time\ninformation often convert it into time embeddings or calculate the time\ninterval and incorporate it into the model, making it difficult to capture the\ncontinuity of time. Finally, during POI prediction, various weighting\ninformation is often ignored, such as the popularity of each POI, the\ntransition relationships between POIs, and the distances between POIs, leading\nto suboptimal performance. To address these issues, this paper proposes a novel\nnext POI recommendation framework called Graph Disentangler with POI Weighted\nModule (GDPW). This framework aims to jointly consider POI category information\nand multiple POI weighting factors. Specifically, the proposed GDPW learns\ncategory and time representations through the Global Category Graph and the\nGlobal Category-Time Graph. Then, we disentangle category and time information\nthrough contrastive learning. After prediction, the final POI recommendation\nfor users is obtained by weighting the prediction results based on the\ntransition weights and distance relationships between POIs. We conducted\nexperiments on two real-world datasets, and the results demonstrate that the\nproposed GDPW outperforms other existing models, improving performance by 3% to\n11%.", "published": "2025-07-19 13:16:44", "link": "http://arxiv.org/abs/2507.14612v1", "categories": ["cs.IR", "cs.AI", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Understanding Matching Mechanisms in Cross-Encoders", "abstract": "Neural IR architectures, particularly cross-encoders, are highly effective\nmodels whose internal mechanisms are mostly unknown. Most works trying to\nexplain their behavior focused on high-level processes (e.g., what in the input\ninfluences the prediction, does the model adhere to known IR axioms) but fall\nshort of describing the matching process. Instead of Mechanistic\nInterpretability approaches which specifically aim at explaining the hidden\nmechanisms of neural models, we demonstrate that more straightforward methods\ncan already provide valuable insights. In this paper, we first focus on the\nattention process and extract causal insights highlighting the crucial roles of\nsome attention heads in this process. Second, we provide an interpretation of\nthe mechanism underlying matching detection.", "published": "2025-07-19 13:05:27", "link": "http://arxiv.org/abs/2507.14604v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints", "abstract": "Motivated by federated learning (FL), secure aggregation (SA) aims to\nsecurely compute, as efficiently as possible, the sum of a set of inputs\ndistributed across many users. To understand the impact of network topology,\nhierarchical secure aggregation (HSA) investigated the communication and secret\nkey generation efficiency in a 3-layer relay network, where clusters of users\nare connected to the aggregation server through an intermediate layer of\nrelays. Due to the pre-aggregation of the messages at the relays, HSA reduces\nthe communication burden on the relay-to-server links and is able to support a\nlarge number of users. However, as the number of users increases, a practical\nchallenge arises from heterogeneous security requirements--for example, users\nin different clusters may require varying levels of input protection. Motivated\nby this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where\ninstead of protecting all the inputs from any set of colluding users, only the\ninputs belonging to a predefined collection of user groups (referred to as\nsecurity input sets) need to be protected against another predefined collection\nof user groups (referred to as collusion sets). Since the security input sets\nand collusion sets can be arbitrarily defined, our formulation offers a\nflexible framework for addressing heterogeneous security requirements in HSA.\nWe characterize the optimal total key rate, i.e., the total number of\nindependent key symbols required to ensure both server and relay security, for\na broad range of parameter configurations. For the remaining cases, we\nestablish lower and upper bounds on the optimal key rate, providing\nconstant-factor gap optimality guarantees.", "published": "2025-07-19 23:09:57", "link": "http://arxiv.org/abs/2507.14768v1", "categories": ["cs.IT", "cs.CR", "cs.DC", "cs.LG", "math.IT"], "primary_category": "cs.IT"}
{"title": "An Information-Theoretic Intersectional Data Valuation Theory", "abstract": "In contemporary digital markets, personal data often reveals not just\nisolated traits, but complex, intersectional identities based on combinations\nof race, gender, disability, and other protected characteristics. This exposure\ngenerates a privacy externality: firms benefit economically from profiling,\nprediction, and personalization, while users face hidden costs in the form of\nsocial risk and discrimination. We introduce a formal pricing rule that\nquantifies and internalizes this intersectional privacy loss using mutual\ninformation, assigning monetary value to the entropy reduction induced by each\ndatum. The result is a Pigouvian-style surcharge that discourages harmful data\ntrades and rewards transparency. Our formulation has the advantage that it\noperates independently of the underlying statistical model of the\nintersectional variables, be it parametric, nonparametric, or learned, and can\nbe approximated in practice by discretizing the intersectional joint\nprobability distributions. We illustrate how regulators can calibrate this\nsurcharge to reflect different societal values, and argue that it provides not\njust a technical fix to market failures, but also a redistributive shield that\nempowers vulnerable groups in the face of asymmetric digital power.", "published": "2025-07-19 20:29:41", "link": "http://arxiv.org/abs/2507.14742v1", "categories": ["cs.IT", "math.IT", "stat.AP"], "primary_category": "cs.IT"}
{"title": "Study of Delay-Calibrated Joint User Activity Detection, Channel Estimation and Data Detection for Asynchronous mMTC Systems", "abstract": "This work considers uplink asynchronous massive machine-type communications,\nwhere a large number of low-power and low-cost devices asynchronously transmit\nshort packets to an access point equipped with multiple receive antennas. If\northogonal preambles are employed, massive collisions will occur due to the\nlimited number of orthogonal preambles given the preamble sequence length. To\naddress this problem, we propose a delay-calibrated joint user activity\ndetection, channel estimation, and data detection algorithm, and investigate\nthe benefits of oversampling in estimating continuous-valued time delays at the\nreceiver. The proposed algorithm is based on the expectation-maximization\nmethod, which alternately estimates the delays and detects active users and\ntheir channels and data by noting that the collided users have different\ndelays. Under the Bayesian inference framework, we develop a computationally\nefficient iterative algorithm using the approximate message passing principle\nto resolve the joint user activity detection, channel estimation, and data\ndetection problem. Numerical results demonstrate the effectiveness of the\nproposed algorithm in terms of the normalized mean-squared errors of channel\nand data symbols, and the probability of misdetection.", "published": "2025-07-19 19:48:41", "link": "http://arxiv.org/abs/2507.14733v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Information Theoretic Analysis of a Dual-Band MIMO Cellphone Antenna with ANSYS HFSS SBR+", "abstract": "Historically, the design of antenna arrays has evolved separately from\nShannon theory. Shannon theory adopts a probabilistic approach in the design of\ncommunication systems, while antenna design approaches have relied on the\ndeterministic Maxwell theory alone. In this paper, we investigate an\ninformation-theoretic analysis approach which we apply to evaluate the design\nof a dual-band, dual-polarized multiple-input multiple-output (MIMO) array on a\ncellphone. To this end, we use ANSYS HFSS, a commercial electromagnetic (EM)\nsimulation software suitable for the numerical optimization of antenna systems.\nHFSS is used to obtain an accurate model of the cellphone MIMO antenna array\nand HFSS SBR+ is utilized to obtain channel matrices for a large number of\nusers. Taking advantage of linear and optimal processing at the cellphone, we\nestimate the outage probability curves. The curves are then used to determine\nthe diversity gain in a moderate signal-to-noise ratio (SNR) regime and the\nmultiplexing gain at a high SNR regime. This approach is then compared with the\nmethod of estimating the diversity gain from the envelope correlation\ncoefficients or the beam-coupling matrix showing substantial differences in the\ntwo methodologies.", "published": "2025-07-19 17:50:56", "link": "http://arxiv.org/abs/2507.14704v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "FORTA: Byzantine-Resilient FL Aggregation via DFT-Guided Krum", "abstract": "Secure federated learning enables collaborative model training across\ndecentralized users while preserving data privacy. A key component is secure\naggregation, which keeps individual updates hidden from both the server and\nusers, while also defending against Byzantine users who corrupt the\naggregation. To this end, Jinhyun So et al. recently developed a\nByzantine-resilient secure aggregation scheme using a secret-sharing strategy\nover finite-field arithmetic. However, such an approach can suffer from\nnumerical errors and overflows when applied to real-valued model updates,\nmotivating the need for secure aggregation methods that operate directly over\nthe real domain. We propose FORTA, a Byzantine-resilient secure aggregation\nframework that operates entirely in the real domain. FORTA leverages Discrete\nFourier Transform (DFT) codes for privacy and employs Krum-based outlier\ndetection for robustness. While DFT decoder is error-free under infinite\nprecision, finite precision introduces numerical perturbations that can distort\ndistance estimates and allow malicious updates to evade detection. To address\nthis, FORTA refines Krum using feedback from DFT decoder, improving the\nselection of trustworthy updates. Theoretical analysis and experiments show\nthat our modification of Krum offers improved robustness and more accurate\naggregation than standard Krum.", "published": "2025-07-19 12:17:24", "link": "http://arxiv.org/abs/2507.14588v1", "categories": ["cs.CR", "cs.IT", "math.IT"], "primary_category": "cs.CR"}
{"title": "Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence", "abstract": "Popular methods in cooperative Multi-Agent Reinforcement Learning with\npartially observable environments typically allow agents to act independently\nduring execution, which may limit the coordinated effect of the trained\npolicies. However, by sharing information such as known or suspected ongoing\nthreats, effective communication can lead to improved decision-making in the\ncyber battle space. We propose a game design where defender agents learn to\ncommunicate and defend against imminent cyber threats by playing training games\nin the Cyber Operations Research Gym, using the Differentiable Inter Agent\nLearning algorithm adapted to the cyber operational environment. The tactical\npolicies learned by these autonomous agents are akin to those of human experts\nduring incident responses to avert cyber threats. In addition, the agents\nsimultaneously learn minimal cost communication messages while learning their\ndefence tactical policies.", "published": "2025-07-19 15:16:24", "link": "http://arxiv.org/abs/2507.14658v1", "categories": ["cs.MA", "cs.CR", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Strategyproofness and Monotone Allocation of Auction in Social Networks", "abstract": "Strategyproofness in network auctions requires that bidders not only report\ntheir valuations truthfully, but also do their best to invite neighbours from\nthe social network. In contrast to canonical auctions, where the value-monotone\nallocation in Myerson's Lemma is a cornerstone, a general principle of\nallocation rules for strategyproof network auctions is still missing. We show\nthat, due to the absence of such a principle, even extensions to multi-unit\nnetwork auctions with single-unit demand present unexpected difficulties, and\nall pioneering researches fail to be strategyproof. For the first time in this\nfield, we identify two categories of monotone allocation rules on networks:\nInvitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity\n(IP-MON). They encompass all existing allocation rules of network auctions as\nspecific instances. For any given ID-MON or IP-MON allocation rule, we\ncharacterize the existence and sufficient conditions for the strategyproof\npayment rules, and show that among all such payment rules, the\nrevenue-maximizing one exists and is computationally feasible. With these\nresults, the obstacle of combinatorial network auction with single-minded\nbidders is now resolved.", "published": "2025-07-19 04:05:35", "link": "http://arxiv.org/abs/2507.14472v1", "categories": ["cs.GT", "cs.AI", "cs.MA", "econ.TH"], "primary_category": "cs.GT"}
{"title": "Approximate Revenue Maximization for Diffusion Auctions", "abstract": "Reserve prices are widely used in practice. The problem of designing\nrevenue-optimal auctions based on reserve price has drawn much attention in the\nauction design community. Although they have been extensively studied, most\ndevelopments rely on the significant assumption that the target audience of the\nsale is directly reachable by the auctioneer, while a large portion of bidders\nin the economic network unaware of the sale are omitted. This work follows the\ndiffusion auction design, which aims to extend the target audience of optimal\nauction theory to all entities in economic networks. We investigate the design\nof simple and provably near-optimal network auctions via reserve price. Using\nBayesian approximation analysis, we provide a simple and explicit form of the\nreserve price function tailored to the most representative network auction. We\naim to balance setting a sufficiently high reserve price to induce high revenue\nin a successful sale, and attracting more buyers from the network to increase\nthe probability of a successful sale. This reserve price function preserves\nincentive compatibility for network auctions, allowing the seller to extract\nadditional revenue beyond that achieved by the Myerson optimal auction.\nSpecifically, if the seller has $\\rho$ direct neighbours in a network of size\n$n$, this reserve price guarantees a $1-{1 \\over \\rho}$ approximation to the\ntheoretical upper bound, i.e., the maximum possible revenue from any network of\nsize $n$. This result holds for any size and any structure of the networked\nmarket.", "published": "2025-07-19 04:04:09", "link": "http://arxiv.org/abs/2507.14470v1", "categories": ["econ.TH", "cs.AI", "cs.GT", "cs.MA"], "primary_category": "econ.TH"}
{"title": "Spectral Analysis of Node- and Cell-Centered Higher-Order Compact Schemes for Fully Discrete One and Two-Dimensional Convection-Dispersion Equation", "abstract": "In this study, we present a comprehensive global spectral analysis of the\nconvection dispersion equation, which is also referred to in specific contexts\nas the Korteweg de Vries (KdV) equation, to investigate the behaviour of high\norder numerical schemes across a wide range of nondimensional parameters. The\nmotivation for this analysis stems from the equation's importance in modeling\nwave propagation and transport phenomena, where accurate resolution of\ndispersive effects is critical, and traditional numerical schemes often suffer\nfrom spurious artifacts. We analyze one sixth order and two eighth order\ncompact spatial discretization schemes, encompassing both node centered and\ncell centered formulations, combined with a third order strong stability\npreserving Runge Kutta (SSPRK3) time integrator. The analysis is performed in\nterms of key nondimensional parameters such as the wavenumber, Courant\nFriedrichs Lewy number $N_c$, and dispersion number $D_{\\alpha}$ over the full\nspectral plane for both one and two dimensional cases. Key numerical\nindicators, including the amplification factor, normalized phase speed, and\nnormalized group velocity, are evaluated to characterize stability, dispersion\nerror, errors in energy transport, and directional anisotropy. Critical\ndispersion thresholds and Courant numbers are identified, beyond which\nnumerical instability and nonphysical phenomena such as spurious q waves and\nreversed phase or energy transport arise. Theoretical predictions are validated\nthrough numerical experiments involving linear and nonlinear one and two\ndimensional test problems, including cases with exact solutions and established\nbenchmark results. This comprehensive analysis uncovers subtle numerical errors\nand offers practical guidance for selecting reliable discretization parameters,\nensuring accurate and stable simulations of convection dispersion systems.", "published": "2025-07-19 16:53:22", "link": "http://arxiv.org/abs/2507.14692v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "1/2 order convergence rate of Euler-type methods for time-changed stochastic differential equations with super-linearly growing drift and diffusion coefficients", "abstract": "This paper investigates the convergence rates of two Euler-type methods for a\nclass of time-changed stochastic differential equations with super-linearly\ngrowing drift and diffusion coefficients. Building upon existing research, we\nadapt the backward Euler method to time-changed stochastic differential\nequations where both coefficients exhibit super-linear growth and introduce an\nexplicit counterpart, the projected Euler method. It is shown that both methods\nachieve the optimal strong convergence rate of order 1/2 in the mean-square\nsense for this class of equations. Numerical simulations confirm the\ntheoretical findings", "published": "2025-07-19 09:56:04", "link": "http://arxiv.org/abs/2507.14562v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "On the convergence analysis of MsFEM with oversampling: Interpolation error", "abstract": "In this paper, we investigate the approximation properties of two types of\nmultiscale finite element methods with oversampling as proposed in [Hou \\& Wu,\n{\\textit{J. Comput. Phys.}}, 1997] and [Efendiev, Hou \\& Wu, \\textit{SIAM J.\nNumer. Anal.}, 2000] without scale separation. We develop a general\ninterpolation error analysis for elliptic problems with highly oscillatory\nrough coefficients, under the assumption of the existence of a macroscopic\nproblem with suitable $L^2$-accuracy. The distinct features of the analysis, in\nthe setting of highly oscillatory periodic coefficients, include: (i) The\nanalysis is independent of the first-order corrector or the solutions to the\ncell problems, and thus independent of their regularity properties; (ii) The\nanalysis only involves the homogenized solution and its minimal regularity. We\nderive an interpolation error $\\mathcal{O}\\left(H+\\frac{\\epsilon}{H}\\right)$\nwith $\\epsilon$ and $H$ being the period size and the coarse mesh size,\nrespectively, when the oversampling domain includes one layer of elements from\nthe target coarse element.", "published": "2025-07-19 09:11:33", "link": "http://arxiv.org/abs/2507.14548v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "On the vector potential formulation with an energy-based hysteresis model and its numerical solution", "abstract": "The accurate modelling and simulation of electric devices involving\nferromagnetic materials requires the appropriate consideration of magnetic\nhysteresis. We discuss the systematic incorporation of the energy-based vector\nhysteresis model of Henrotte et al. into vector potential formulations for the\ngoverning magnetic field equations. The field model describing a single step in\na load cycle is phrased as a convex minimization problem which allows us to\nestablish existence and uniqueness of solutions and to obtain accurate\napproximations by finite element discretization. Consistency of the model with\nthe governing field equations is deduced from the first order optimality\nconditions. In addition, two globally convergent iterative methods are\npresented for the solution of the underlying minimization problems. The\nefficiency of the approach is illustrated by numerical tests for a typical\nbenchmark problem.", "published": "2025-07-19 07:50:27", "link": "http://arxiv.org/abs/2507.14521v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Mathematical modeling and simulation of two-phase magnetohydrodynamic flows at low magnetic Reynolds numbers", "abstract": "We propose a novel mathematical framework for simulating the two-phase\nincompressible magnetohydrodynamic (MHD) problems. Focusing on low magnetic\nReynolds number regimes, where induced magnetic fields are negligible compared\nto applied fields, an intrinsic sharp-interface system is first formulated.\nSubsequently, we utilize the phase-field approach to characterize the interface\nand derive a thermodynamically consistent phase-field model through the\nOnsager's variational principle. The resulting system couples the\nAbels--Garcke--Gr\\\"un (AGG) model of two-phase flows with a quasi-static\nformulation modeling the electromagnetic phenomena. Theoretically, the\nsharp-interface limit is investigated via asymptotic arguments, deducing that\nthe sharp-interface system can be recovered in the limit of vanishing interface\nthickness. Consequently, this justifies the reliability of the phase-field\napproach as an approximated method. In addition, we present some\nthree-dimensional numerical experiments of magnetic damping effects on bubble\ndynamics, where the observed results demonstrate the validity of the proposed\nframework in capturing complex MHD phenomena.", "published": "2025-07-19 07:35:20", "link": "http://arxiv.org/abs/2507.14518v1", "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "primary_category": "math.NA"}
{"title": "Numerical Artifacts in Learning Dynamical Systems", "abstract": "In many applications, one needs to learn a dynamical system from its\nsolutions sampled at a finite number of time points. The learning problem is\noften formulated\n  as an optimization problem over a chosen function class. However, in the\noptimization procedure, it is necessary to employ a numerical scheme to\nintegrate candidate dynamical systems and assess how their solutions fit the\ndata.\n  This paper reveals potentially serious effects of a chosen numerical scheme\non the learning outcome. In particular, our analysis demonstrates that a damped\noscillatory system may be incorrectly identified as having \"anti-damping\" and\nexhibiting a reversed oscillation direction, despite adequately fitting the\ngiven data points.", "published": "2025-07-19 05:23:39", "link": "http://arxiv.org/abs/2507.14491v1", "categories": ["math.NA", "cs.LG", "cs.NA"], "primary_category": "math.NA"}
{"title": "Entropy Stable Nodal Discontinuous Galerkin Methods via Quadratic Knapsack Limiting", "abstract": "Lin, Chan (High order entropy stable discontinuous Galerkin spectral element\nmethods through subcell limiting, 2024) enforces a cell entropy inequality for\nnodal discontinuous Galerkin methods by combining flux corrected transport\n(FCT)-type limiting and a knapsack solver, which determines optimal limiting\ncoefficients that result in a semi-discrete cell entropy inequality while\npreserving nodal bounds. In this work, we provide a slight modification of this\napproach, where we utilize a quadratic knapsack problem instead of a standard\nlinear knapsack problem. We prove that this quadratic knapsack problem can be\nreduced to efficient scalar root-finding. Numerical results demonstrate that\nthe proposed quadratic knapsack limiting strategy is efficient and results in a\nsemi-discretization with improved regularity in time compared with linear\nknapsack limiting, while resulting in fewer adaptive timesteps in shock-type\nproblems.", "published": "2025-07-19 05:12:59", "link": "http://arxiv.org/abs/2507.14488v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Explicit Runge-Kutta Methods with MQ and IMQ-Radial Basis Functions", "abstract": "This article presents a class of explicit Runge-Kutta methods with\nmultiquadric (MQ) and inverse multiquadric (IMQ) radial basis functions (RBFs)\nto improve the accuracy of time integration for ordinary differential\nequations. By introducing RBF-based corrections derived from Taylor series\nexpansions and optimally selecting the shape parameter, the method achieves a\none-order increase in accuracy without additional stages. Convergence and\nstability analyses support the theoretical claims, and numerical experiments in\nMATLAB confirm the predicted performance.", "published": "2025-07-19 04:10:48", "link": "http://arxiv.org/abs/2507.14474v1", "categories": ["math.NA", "cs.NA", "65L06"], "primary_category": "math.NA"}
{"title": "An inverse moving point source problem in electromagnetics", "abstract": "This paper is concerned with an inverse moving point source problem in\nelectromagnetics. The aim is to reconstruct the moving orbit from the\ntangential components of magnetic fields taken at a finite number of\nobservation points. The distance function between each observation point and\nthe moving point source is computed by solving a nonlinear ordinary\ndifferential equation with an initial value. This ODE system only involves the\nmeasurement data from the tangential trace of the magnetic field at observation\npoints. As a consequence, the dynamical measurement data recorded at four\nnon-coplanar points are sufficient to reconstruct the orbit function. A\nLipschitz stability is established for the inverse problem, and numerical\nexperiments are reported to demonstrate the effectiveness of the proposed\nmethod. Numerical examples have shown that the reconstructed error depends\nlinearly on the noise level and that the wave speed is a critical factor\naffecting the relative error.", "published": "2025-07-19 02:33:17", "link": "http://arxiv.org/abs/2507.14440v1", "categories": ["math.NA", "cs.NA", "35R30, 78A46"], "primary_category": "math.NA"}
{"title": "Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning", "abstract": "Self-supervised feature learning and pretraining methods in reinforcement\nlearning (RL) often rely on information-theoretic principles, termed mutual\ninformation skill learning (MISL). These methods aim to learn a representation\nof the environment while also incentivizing exploration thereof. However, the\nrole of the representation and mutual information parametrization in MISL is\nnot yet well understood theoretically. Our work investigates MISL through the\nlens of identifiable representation learning by focusing on the Contrastive\nSuccessor Features (CSF) method. We prove that CSF can provably recover the\nenvironment's ground-truth features up to a linear transformation due to the\ninner product parametrization of the features and skill diversity in a\ndiscriminative sense. This first identifiability guarantee for representation\nlearning in RL also helps explain the implications of different mutual\ninformation objectives and the downsides of entropy regularizers. We\nempirically validate our claims in MuJoCo and DeepMind Control and show how CSF\nprovably recovers the ground-truth features both from states and pixels.", "published": "2025-07-19 20:48:46", "link": "http://arxiv.org/abs/2507.14748v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization", "abstract": "High-fidelity simulations and physical experiments are essential for\nengineering analysis and design. However, their high cost often limits their\napplications in two critical tasks: global sensitivity analysis (GSA) and\noptimization. This limitation motivates the common use of Gaussian processes\n(GPs) as proxy regression models to provide uncertainty-aware predictions based\non a limited number of high-quality observations. GPs naturally enable\nefficient sampling strategies that support informed decision-making under\nuncertainty by extracting information from a subset of possible functions for\nthe model of interest. Despite their popularity in machine learning and\nstatistics communities, sampling from GPs has received little attention in the\ncommunity of engineering optimization. In this paper, we present the\nformulation and detailed implementation of two notable sampling methods --\nrandom Fourier features and pathwise conditioning -- for generating posterior\nsamples from GPs. Alternative approaches are briefly described. Importantly, we\ndetail how the generated samples can be applied in GSA, single-objective\noptimization, and multi-objective optimization. We show successful applications\nof these sampling methods through a series of numerical examples.", "published": "2025-07-19 20:36:38", "link": "http://arxiv.org/abs/2507.14746v1", "categories": ["cs.LG", "math.OC", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Better Training Data Attribution via Better Inverse Hessian-Vector Products", "abstract": "Training data attribution (TDA) provides insights into which training data is\nresponsible for a learned model behavior. Gradient-based TDA methods such as\ninfluence functions and unrolled differentiation both involve a computation\nthat resembles an inverse Hessian-vector product (iHVP), which is difficult to\napproximate efficiently. We introduce an algorithm (ASTRA) which uses the\nEKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP\napproximation for TDA. ASTRA is easy to tune, requires fewer iterations than\nNeumann series iterations, and is more accurate than EKFAC-based\napproximations. Using ASTRA, we show that improving the accuracy of the iHVP\napproximation can significantly improve TDA performance.", "published": "2025-07-19 20:18:51", "link": "http://arxiv.org/abs/2507.14740v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts", "abstract": "Semi-supervised domain adaptation (SSDA) aims to achieve high predictive\nperformance in the target domain with limited labeled target data by exploiting\nabundant source and unlabeled target data. Despite its significance in numerous\napplications, theory on the effectiveness of SSDA remains largely unexplored,\nparticularly in scenarios involving various types of source-target\ndistributional shifts. In this work, we develop a theoretical framework based\non structural causal models (SCMs) which allows us to analyze and quantify the\nperformance of SSDA methods when labeled target data is limited. Within this\nframework, we introduce three SSDA methods, each having a fine-tuning strategy\ntailored to a distinct assumption about the source and target relationship.\nUnder each assumption, we demonstrate how extending an unsupervised domain\nadaptation (UDA) method to SSDA can achieve minimax-optimal target performance\nwith limited target labels. When the relationship between source and target\ndata is only vaguely known -- a common practical concern -- we propose the\nMulti Adaptive-Start Fine-Tuning (MASFT) algorithm, which fine-tunes UDA models\nfrom multiple starting points and selects the best-performing one based on a\nsmall hold-out target validation dataset. Combined with model selection\nguarantees, MASFT achieves near-optimal target predictive performance across a\nbroad range of types of distributional shifts while significantly reducing the\nneed for labeled target data. We empirically validate the effectiveness of our\nproposed methods through simulations.", "published": "2025-07-19 15:18:28", "link": "http://arxiv.org/abs/2507.14661v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators", "abstract": "Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample\nfrom the posterior distribution in Bayesian inference. However, HMC techniques\nare computationally demanding for Bayesian neural networks due to the high\ndimensionality of the network's parameter space and the non-convexity of their\nposterior distributions. Therefore, various approximation techniques, such as\nvariational inference (VI) or stochastic gradient MCMC, are often employed to\ninfer the posterior distribution of the network parameters. Such approximations\nintroduce inaccuracies in the inferred distributions, resulting in unreliable\nuncertainty estimates. In this work, we propose a hybrid approach that combines\ninexpensive VI and accurate HMC methods to efficiently and accurately quantify\nuncertainties in neural networks and neural operators. The proposed approach\nleverages an initial VI training on the full network. We examine the influence\nof individual parameters on the prediction uncertainty, which shows that a\nlarge proportion of the parameters do not contribute substantially to\nuncertainty in the network predictions. This information is then used to\nsignificantly reduce the dimension of the parameter space, and HMC is performed\nonly for the subset of network parameters that strongly influence prediction\nuncertainties. This yields a framework for accelerating the full batch HMC for\nposterior inference in neural networks. We demonstrate the efficiency and\naccuracy of the proposed framework on deep neural networks and operator\nnetworks, showing that inference can be performed for large networks with tens\nto hundreds of thousands of parameters. We show that this method can\neffectively learn surrogates for complex physical systems by modeling the\noperator that maps from upstream conditions to wall-pressure data on a cone in\nhypersonic flow.", "published": "2025-07-19 14:57:54", "link": "http://arxiv.org/abs/2507.14652v1", "categories": ["stat.ML", "cs.CE", "cs.LG", "physics.data-an"], "primary_category": "stat.ML"}
{"title": "Deep Learning-Based Survival Analysis with Copula-Based Activation Functions for Multivariate Response Prediction", "abstract": "This research integrates deep learning, copula functions, and survival\nanalysis to effectively handle highly correlated and right-censored\nmultivariate survival data. It introduces copula-based activation functions\n(Clayton, Gumbel, and their combinations) to model the nonlinear dependencies\ninherent in such data. Through simulation studies and analysis of real breast\ncancer data, our proposed CNN-LSTM with copula-based activation functions for\nmultivariate multi-types of survival responses enhances prediction accuracy by\nexplicitly addressing right-censored data and capturing complex patterns. The\nmodel's performance is evaluated using Shewhart control charts, focusing on the\naverage run length (ARL).", "published": "2025-07-19 14:35:51", "link": "http://arxiv.org/abs/2507.14641v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Diffusion Models for Time Series Forecasting: A Survey", "abstract": "Diffusion models, initially developed for image synthesis, demonstrate\nremarkable generative capabilities. Recently, their application has expanded to\ntime series forecasting (TSF), yielding promising results. In this survey, we\nfirstly introduce the standard diffusion models and their prevalent variants,\nexplaining their adaptation to TSF tasks. We then provide a comprehensive\nreview of diffusion models for TSF, paying special attention to the sources of\nconditional information and the mechanisms for integrating this conditioning\nwithin the models. In analyzing existing approaches using diffusion models for\nTSF, we provide a systematic categorization and a comprehensive summary of them\nin this survey. Furthermore, we examine several foundational diffusion models\napplied to TSF, alongside commonly used datasets and evaluation metrics.\nFinally, we discuss current limitations in these approaches and potential\nfuture research directions. Overall, this survey details recent progress and\nfuture prospects for diffusion models in TSF, serving as a reference for\nresearchers in the field.", "published": "2025-07-19 07:04:04", "link": "http://arxiv.org/abs/2507.14507v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Neural Brownian Motion", "abstract": "This paper introduces the Neural-Brownian Motion (NBM), a new class of\nstochastic processes for modeling dynamics under learned uncertainty. The NBM\nis defined axiomatically by replacing the classical martingale property with\nrespect to linear expectation with one relative to a non-linear Neural\nExpectation Operator, $\\varepsilon^\\theta$, generated by a Backward Stochastic\nDifferential Equation (BSDE) whose driver $f_\\theta$ is parameterized by a\nneural network. Our main result is a representation theorem for a canonical\nNBM, which we define as a continuous $\\varepsilon^\\theta$-martingale with zero\ndrift under the physical measure. We prove that, under a key structural\nassumption on the driver, such a canonical NBM exists and is the unique strong\nsolution to a stochastic differential equation of the form ${\\rm d} M_t =\n\\nu_\\theta(t, M_t) {\\rm d} W_t$. Crucially, the volatility function\n$\\nu_\\theta$ is not postulated a priori but is implicitly defined by the\nalgebraic constraint $g_\\theta(t, M_t, \\nu_\\theta(t, M_t)) = 0$, where\n$g_\\theta$ is a specialization of the BSDE driver. We develop the stochastic\ncalculus for this process and prove a Girsanov-type theorem for the quadratic\ncase, showing that an NBM acquires a drift under a new, learned measure. The\ncharacter of this measure, whether pessimistic or optimistic, is endogenously\ndetermined by the learned parameters $\\theta$, providing a rigorous foundation\nfor models where the attitude towards uncertainty is a discoverable feature.", "published": "2025-07-19 06:09:52", "link": "http://arxiv.org/abs/2507.14499v1", "categories": ["math.PR", "cs.AI", "cs.LG", "math.OC", "stat.ML"], "primary_category": "math.PR"}
{"title": "Glitches in Decision Tree Ensemble Models", "abstract": "Many critical decision-making tasks are now delegated to machine-learned\nmodels, and it is imperative that their decisions are trustworthy and reliable,\nand their outputs are consistent across similar inputs. We identify a new\nsource of unreliable behaviors-called glitches-which may significantly impair\nthe reliability of AI models having steep decision boundaries. Roughly\nspeaking, glitches are small neighborhoods in the input space where the model's\noutput abruptly oscillates with respect to small changes in the input. We\nprovide a formal definition of glitches, and use well-known models and datasets\nfrom the literature to demonstrate that they have widespread existence and\nargue they usually indicate potential model inconsistencies in the neighborhood\nof where they are found. We proceed to the algorithmic search of glitches for\nwidely used gradient-boosted decision tree (GBDT) models. We prove that the\nproblem of detecting glitches is NP-complete for tree ensembles, already for\ntrees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP\nencoding of the problem, and its effectiveness and computational feasibility\nare demonstrated on a set of widely used GBDT benchmarks taken from the\nliterature.", "published": "2025-07-19 05:33:57", "link": "http://arxiv.org/abs/2507.14492v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Statistical and Algorithmic Foundations of Reinforcement Learning", "abstract": "As a paradigm for sequential decision making in unknown environments,\nreinforcement learning (RL) has received a flurry of attention in recent years.\nHowever, the explosion of model complexity in emerging applications and the\npresence of nonconvexity exacerbate the challenge of achieving efficient RL in\nsample-starved situations, where data collection is expensive, time-consuming,\nor even high-stakes (e.g., in clinical trials, autonomous systems, and online\nadvertising). How to understand and enhance the sample and computational\nefficacies of RL algorithms is thus of great interest. In this tutorial, we aim\nto introduce several important algorithmic and theoretical developments in RL,\nhighlighting the connections between new ideas and classical topics. Employing\nMarkov Decision Processes as the central mathematical model, we cover several\ndistinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL,\nrobust RL, and RL with human feedback), and present several mainstream RL\napproaches (i.e., model-based approach, value-based approach, and policy\noptimization). Our discussions gravitate around the issues of sample\ncomplexity, computational efficiency, as well as algorithm-dependent and\ninformation-theoretic lower bounds from a non-asymptotic viewpoint.", "published": "2025-07-19 02:42:41", "link": "http://arxiv.org/abs/2507.14444v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.OC", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Multi-Sampling-Frequency Naturalness MOS Prediction Using Self-Supervised Learning Model with Sampling-Frequency-Independent Layer", "abstract": "We introduce our submission to the AudioMOS Challenge (AMC) 2025 Track 3:\nmean opinion score (MOS) prediction for speech with multiple sampling\nfrequencies (SFs). Our submitted model integrates an SF-independent (SFI)\nconvolutional layer into a self-supervised learning (SSL) model to achieve SFI\nspeech feature extraction for MOS prediction. We present some strategies to\nimprove the MOS prediction performance of our model: distilling knowledge from\na pretrained non-SFI-SSL model and pretraining with a large-scale MOS dataset.\nOur submission to the AMC 2025 Track 3 ranked the first in one evaluation\nmetric and the fourth in the final ranking. We also report the results of our\nablation study to investigate essential factors of our model.", "published": "2025-07-19 14:41:51", "link": "http://arxiv.org/abs/2507.14647v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Rest is Silence: Leveraging Unseen Species Models for Computational Musicology", "abstract": "For many decades, musicologists have engaged in creating large databases\nserving different purposes for musicological research and scholarship. With the\nrise of fields like music information retrieval and digital musicology, there\nis now a constant and growing influx of musicologically relevant datasets and\ncorpora. In historical or observational settings, however, these datasets are\nnecessarily incomplete, and the true extent of a collection of interest remains\nunknown -- silent. Here, we apply, for the first time, so-called Unseen Species\nmodels (USMs) from ecology to areas of musicological activity. After\nintroducing the models formally, we show in four case studies how USMs can be\napplied to musicological data to address quantitative questions like: How many\ncomposers are we missing in RISM? What percentage of medieval sources of\nGregorian chant have we already cataloged? How many differences in music prints\ndo we expect to find between editions? How large is the coverage of songs from\ngenres of a folk music tradition? And, finally, how close are we in estimating\nthe size of the harmonic vocabulary of a large number of composers?", "published": "2025-07-19 14:34:47", "link": "http://arxiv.org/abs/2507.14638v1", "categories": ["cs.SD", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
{"title": "Adapting Whisper for Lightweight and Efficient Automatic Speech Recognition of Children for On-device Edge Applications", "abstract": "Reliability on cloud providers for ASR inference to support child-centered\nvoice-based applications is becoming challenging due to regulatory and privacy\nchallenges. Motivated by a privacy-preserving design, this study aims to\ndevelop a lightweight & efficient Whisper ASR system capable of running on a\nRaspberry Pi. Upon evaluation of the MyST corpus and by examining various\nfiltering strategies to fine-tune the `tiny.en' model, a Word Error Rate (WER)\nof 15.9% was achieved (11.8% filtered). A low-rank compression reduces the\nencoder size by 0.51M with 1.26x faster inference in GPU, with 11% relative WER\nincrease. During inference on Pi, the compressed version required ~2 GFLOPS\nfewer computations. The RTF for both the models ranged between [0.23-0.41] for\nvarious input audio durations. Analyzing the RAM usage and CPU temperature\nshowed that the PI was capable of handling both the tiny models, however it was\nnoticed that small models initiated additional overhead/thermal throttling.", "published": "2025-07-19 02:55:48", "link": "http://arxiv.org/abs/2507.14451v1", "categories": ["eess.AS", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition", "abstract": "EEG-based emotion recognition plays an important role in developing adaptive\nbrain-computer communication systems, yet faces two fundamental challenges in\npractical implementations: (1) effective integration of non-stationary\nspatial-temporal neural patterns, (2) robust adaptation to dynamic emotional\nintensity variations in real-world scenarios. This paper proposes SST-CL, a\nnovel framework integrating spatial-temporal transformers with curriculum\nlearning. Our method introduces two core components: a spatial encoder that\nmodels inter-channel relationships and a temporal encoder that captures\nmulti-scale dependencies through windowed attention mechanisms, enabling\nsimultaneous extraction of spatial correlations and temporal dynamics from EEG\nsignals. Complementing this architecture, an intensity-aware curriculum\nlearning strategy progressively guides training from high-intensity to\nlow-intensity emotional states through dynamic sample scheduling based on a\ndual difficulty assessment. Comprehensive experiments on three benchmark\ndatasets demonstrate state-of-the-art performance across various emotional\nintensity levels, with ablation studies confirming the necessity of both\narchitectural components and the curriculum learning mechanism.", "published": "2025-07-19 17:23:38", "link": "http://arxiv.org/abs/2507.14698v1", "categories": ["cs.LG", "cs.AI", "cs.HC", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Propagation Channel Modeling for LEO Satellite Missions Using Ray-Tracing Simulations", "abstract": "This work presents a high-resolution, ray-tracing-based channel modeling for\nLow Earth Orbit (LEO) satellite-to-ground links in a suburban environment at\nX-band. Using simulations conducted in Wireless InSite, we develop a parametric\nchannel model that characterizes both large- and small-scale fading effects\nacross different satellite elevation angles. Large-scale fading incorporates\nattenuation due to terrain-induced shadowing and dynamic environmental factors\nsuch as weather conditions, and is compared with 3GPP NTN channel model.\nAdditionally, we quantify link degradation resulting from ground station (GS)\nantenna misalignment, considering both fixed single-element and electronically\nsteerable phased-array antennas. Small-scale fading is modeled by fitting a\nshadowed and non-shadowed Rician distribution to the fading statistics at\nvarious satellite elevations. To the best of our knowledge, this is the first\nstudy to propose a comprehensive elevation-aware channel model for\nsatellite-to-ground propagation at X-band, integrating ray-traced environmental\ndynamics, elevation-dependent fading, and phased-array beam misalignment\neffects.", "published": "2025-07-19 13:38:30", "link": "http://arxiv.org/abs/2507.14622v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Spatially tailored spin wave excitation for spurious-free, low-loss magnetostatic wave filters with ultra-wide frequency tunability", "abstract": "Yttrium iron garnet magnetostatic wave (MSW) radio frequency (RF) cavity\nfilters are promising for sixth-generation (6G) communication systems due to\ntheir wide frequency tunability. However, the presence of severe spurious modes\narising from the finite cavity dimensions severely degrades the filter\nperformance. We present a half-cone transducer that spatially tailors spin wave\nexcitation to selectively enhance the primary cavity modes comprising the MSW\nfilter passband, while strongly suppressing the undesired spurious modes.\nTheoretical analysis, numerical simulations and experiments verify the\neffectiveness of the spatially tailored technique. We utilize the half-cone\ntransducer to demonstrate a spurious-free, single-cavity half-cone MSW filter\n(HC-MSWF) with an insertion loss (IL) of 2.4-3.2 dB over a frequency tuning\nrange of 6.3-16.8 GHz. Extending our study, we further demonstrate a\nspurious-free, dual-cavity HC-MSWF with an unprecedented tuning range of 21.7\nGHz (9.8-31.5 GHz) while maintaining a low IL of 2.9-3.8 dB. This significant\nadvance in performance will enable highly reconfigurable and robust 6G\nnetworks.", "published": "2025-07-19 04:03:58", "link": "http://arxiv.org/abs/2507.14469v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
