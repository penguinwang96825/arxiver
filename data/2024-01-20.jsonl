{"title": "InferAligner: Inference-Time Alignment for Harmlessness through\n  Cross-Model Guidance", "abstract": "With the rapid development of large language models (LLMs), they are not only\nused as general-purpose AI assistants but are also customized through further\nfine-tuning to meet the requirements of different applications. A pivotal\nfactor in the success of current LLMs is the alignment process. Current\nalignment methods, such as supervised fine-tuning (SFT) and reinforcement\nlearning from human feedback (RLHF), focus on training-time alignment and are\noften complex and cumbersome to implement. Therefore, we develop\n\\textbf{InferAligner}, a novel inference-time alignment method that utilizes\ncross-model guidance for harmlessness alignment. InferAligner utilizes safety\nsteering vectors extracted from safety-aligned model to modify the activations\nof the target model when responding to harmful inputs, thereby guiding the\ntarget model to provide harmless responses. Experimental results show that our\nmethod can be very effectively applied to domain-specific models in finance,\nmedicine, and mathematics, as well as to multimodal large language models\n(MLLMs) such as LLaVA. It significantly diminishes the Attack Success Rate\n(ASR) of both harmful instructions and jailbreak attacks, while maintaining\nalmost unchanged performance in downstream tasks.", "published": "2024-01-20 10:41:03", "link": "http://arxiv.org/abs/2401.11206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Argument Mining over Varying Rhetorical Structures", "abstract": "Rhetorical Structure Theory implies no single discourse interpretation of a\ntext, and the limitations of RST parsers further exacerbate inconsistent\nparsing of similar structures. Therefore, it is important to take into account\nthat the same argumentative structure can be found in semantically similar\ntexts with varying rhetorical structures. In this work, the differences between\nparaphrases within the same argument scheme are evaluated from a rhetorical\nperspective. The study proposes a deep dependency parsing model to assess the\nconnection between rhetorical and argument structures. The model utilizes\nrhetorical relations; RST structures of paraphrases serve as training data\naugmentations. The method allows for end-to-end argumentation analysis using a\nrhetorical tree instead of a word sequence. It is evaluated on the bilingual\nMicrotexts corpus, and the first results on fully-fledged argument parsing for\nthe Russian version of the corpus are reported. The results suggest that\nargument mining can benefit from multiple variants of discourse structure.", "published": "2024-01-20 12:00:40", "link": "http://arxiv.org/abs/2401.11218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying and Analyzing Performance-Critical Tokens in Large Language\n  Models", "abstract": "In-context learning (ICL) has emerged as an effective solution for few-shot\nlearning with large language models (LLMs). However, how LLMs leverage\ndemonstrations to specify a task and learn a corresponding computational\nfunction through ICL is underexplored. Drawing from the way humans learn from\ncontent-label mappings in demonstrations, we categorize the tokens in an ICL\nprompt into content, stopword, and template tokens. Our goal is to identify the\ntypes of tokens whose representations directly influence LLM's performance, a\nproperty we refer to as being performance-critical. By ablating representations\nfrom the attention of the test example, we find that the representations of\ninformative content tokens have less influence on performance compared to\ntemplate and stopword tokens, which contrasts with the human attention to\ninformative words. We give evidence that the representations of\nperformance-critical tokens aggregate information from the content tokens.\nMoreover, we demonstrate experimentally that lexical meaning, repetition, and\nstructural cues are the main distinguishing characteristics of these tokens.\nOur work sheds light on how large language models learn to perform tasks from\ndemonstrations and deepens our understanding of the roles different types of\ntokens play in large language models.", "published": "2024-01-20 20:55:21", "link": "http://arxiv.org/abs/2401.11323v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Duality in Open Information Extraction with Predicate Prompt", "abstract": "Open information extraction (OpenIE) aims to extract the schema-free triplets\nin the form of (\\emph{subject}, \\emph{predicate}, \\emph{object}) from a given\nsentence. Compared with general information extraction (IE), OpenIE poses more\nchallenges for the IE models, {especially when multiple complicated triplets\nexist in a sentence. To extract these complicated triplets more effectively, in\nthis paper we propose a novel generative OpenIE model, namely \\emph{DualOIE},\nwhich achieves a dual task at the same time as extracting some triplets from\nthe sentence, i.e., converting the triplets into the sentence.} Such dual task\nencourages the model to correctly recognize the structure of the given sentence\nand thus is helpful to extract all potential triplets from the sentence.\nSpecifically, DualOIE extracts the triplets in two steps: 1) first extracting a\nsequence of all potential predicates, 2) then using the predicate sequence as a\nprompt to induce the generation of triplets. Our experiments on two benchmarks\nand our dataset constructed from Meituan demonstrate that DualOIE achieves the\nbest performance among the state-of-the-art baselines. Furthermore, the online\nA/B test on Meituan platform shows that 0.93\\% improvement of QV-CTR and 0.56\\%\nimprovement of UV-CTR have been obtained when the triplets extracted by DualOIE\nwere leveraged in Meituan's search system.", "published": "2024-01-20 03:55:17", "link": "http://arxiv.org/abs/2401.11107v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Models for Clinical Decision Support by\n  Incorporating Clinical Practice Guidelines", "abstract": "Background Large Language Models (LLMs), enhanced with Clinical Practice\nGuidelines (CPGs), can significantly improve Clinical Decision Support (CDS).\nHowever, methods for incorporating CPGs into LLMs are not well studied. Methods\nWe develop three distinct methods for incorporating CPGs into LLMs: Binary\nDecision Tree (BDT), Program-Aided Graph Construction (PAGC), and\nChain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of\nthe proposed methods, we create a set of synthetic patient descriptions and\nconduct both automatic and human evaluation of the responses generated by four\nLLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was\nused as the baseline method. We focus on CDS for COVID-19 outpatient treatment\nas the case study. Results All four LLMs exhibit improved performance when\nenhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP\nand PAGC in automatic evaluation. All of the proposed methods demonstrated high\nperformance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate\nsuperior performance, as compared to plain LLMs with ZSP, in providing accurate\nrecommendations for COVID-19 outpatient treatment, which also highlights the\npotential for broader applications beyond the case study.", "published": "2024-01-20 05:10:46", "link": "http://arxiv.org/abs/2401.11120v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How the Advent of Ubiquitous Large Language Models both Stymie and\n  Turbocharge Dynamic Adversarial Question Generation", "abstract": "Dynamic adversarial question generation, where humans write examples to stump\na model, aims to create examples that are realistic and informative. However,\nthe advent of large language models (LLMs) has been a double-edged sword for\nhuman authors: more people are interested in seeing and pushing the limits of\nthese models, but because the models are so much stronger an opponent, they are\nharder to defeat. To understand how these models impact adversarial question\nwriting process, we enrich the writing guidance with LLMs and retrieval models\nfor the authors to reason why their questions are not adversarial. While\nauthors could create interesting, challenging adversarial questions, they\nsometimes resort to tricks that result in poor questions that are ambiguous,\nsubjective, or confusing not just to a computer but also to humans. To address\nthese issues, we propose new metrics and incentives for eliciting good,\nchallenging questions and present a new dataset of adversarially authored\nquestions.", "published": "2024-01-20 09:49:59", "link": "http://arxiv.org/abs/2401.11185v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Unfair TOS: An Automated Approach using Customized BERT", "abstract": "Terms of Service (ToS) form an integral part of any agreement as it defines\nthe legal relationship between a service provider and an end-user. Not only do\nthey establish and delineate reciprocal rights and responsibilities, but they\nalso provide users with information on essential aspects of contracts that\npertain to the use of digital spaces. These aspects include a wide range of\ntopics, including limitation of liability, data protection, etc. Users tend to\naccept the ToS without going through it before using any application or\nservice. Such ignorance puts them in a potentially weaker situation in case any\naction is required. Existing methodologies for the detection or classification\nof unfair clauses are however obsolete and show modest performance. In this\nresearch paper, we present SOTA(State of The Art) results on unfair clause\ndetection from ToS documents based on unprecedented custom BERT Fine-tuning in\nconjunction with SVC(Support Vector Classifier). The study shows proficient\nperformance with a macro F1-score of 0.922 at unfair clause detection, and\nsuperior performance is also shown in the classification of unfair clauses by\neach tag. Further, a comparative analysis is performed by answering research\nquestions on the Transformer models utilized. In order to further research and\nexperimentation the code and results are made available on\nhttps://github.com/batking24/Unfair-TOS-An-Automated-Approach-based-on-Fine-tuning-BERT-in-conjunction-with-ML.", "published": "2024-01-20 10:42:15", "link": "http://arxiv.org/abs/2401.11207v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented\n  Generation in Niche Domains, Exemplified by Korean Medicine", "abstract": "We propose a natural language prompt-based retrieval augmented generation\n(Prompt-RAG), a novel approach to enhance the performance of generative large\nlanguage models (LLMs) in niche domains. Conventional RAG methods mostly\nrequire vector embeddings, yet the suitability of generic LLM-based embedding\nrepresentations for specialized domains remains uncertain. To explore and\nexemplify this point, we compared vector embeddings from Korean Medicine (KM)\nand Conventional Medicine (CM) documents, finding that KM document embeddings\ncorrelated more with token overlaps and less with human-assessed document\nrelatedness, in contrast to CM embeddings. Prompt-RAG, distinct from\nconventional RAG models, operates without the need for embedding vectors. Its\nperformance was assessed through a Question-Answering (QA) chatbot application,\nwhere responses were evaluated for relevance, readability, and informativeness.\nThe results showed that Prompt-RAG outperformed existing models, including\nChatGPT and conventional vector embedding-based RAGs, in terms of relevance and\ninformativeness. Despite challenges like content structuring and response\nlatency, the advancements in LLMs are expected to encourage the use of\nPrompt-RAG, making it a promising tool for other domains in need of RAG\nmethods.", "published": "2024-01-20 14:59:43", "link": "http://arxiv.org/abs/2401.11246v1", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.3; J.3"], "primary_category": "cs.CL"}
{"title": "Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense\n  Passage Retrieval", "abstract": "Masked auto-encoder pre-training has emerged as a prevalent technique for\ninitializing and enhancing dense retrieval systems. It generally utilizes\nadditional Transformer decoder blocks to provide sustainable supervision\nsignals and compress contextual information into dense representations.\nHowever, the underlying reasons for the effectiveness of such a pre-training\ntechnique remain unclear. The usage of additional Transformer-based decoders\nalso incurs significant computational costs. In this study, we aim to shed\nlight on this issue by revealing that masked auto-encoder (MAE) pre-training\nwith enhanced decoding significantly improves the term coverage of input tokens\nin dense representations, compared to vanilla BERT checkpoints. Building upon\nthis observation, we propose a modification to the traditional MAE by replacing\nthe decoder of a masked auto-encoder with a completely simplified Bag-of-Word\nprediction task. This modification enables the efficient compression of lexical\nsignals into dense representations through unsupervised pre-training.\nRemarkably, our proposed method achieves state-of-the-art retrieval performance\non several large-scale retrieval benchmarks without requiring any additional\nparameters, which provides a 67% training speed-up compared to standard masked\nauto-encoder pre-training with enhanced decoding.", "published": "2024-01-20 15:02:33", "link": "http://arxiv.org/abs/2401.11248v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Narrative Review of Identity, Data, and Location Privacy Techniques in\n  Edge Computing and Mobile Crowdsourcing", "abstract": "As digital technology advances, the proliferation of connected devices poses\nsignificant challenges and opportunities in mobile crowdsourcing and edge\ncomputing. This narrative review focuses on the need for privacy protection in\nthese fields, emphasizing the increasing importance of data security in a\ndata-driven world. Through an analysis of contemporary academic literature,\nthis review provides an understanding of the current trends and privacy\nconcerns in mobile crowdsourcing and edge computing. We present insights and\nhighlight advancements in privacy-preserving techniques, addressing identity,\ndata, and location privacy. This review also discusses the potential directions\nthat can be useful resources for researchers, industry professionals, and\npolicymakers.", "published": "2024-01-20 19:32:56", "link": "http://arxiv.org/abs/2401.11305v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation", "abstract": "With the proliferation of large pre-trained language models (PLMs),\nfine-tuning all model parameters becomes increasingly inefficient, particularly\nwhen dealing with numerous downstream tasks that entail substantial training\nand storage costs. Several approaches aimed at achieving parameter-efficient\nfine-tuning (PEFT) have been proposed. Among them, Low-Rank Adaptation (LoRA)\nstands out as an archetypal method, incorporating trainable rank decomposition\nmatrices into each target module. Nevertheless, LoRA does not consider the\nvarying importance of each layer. To address these challenges, we introduce\nPRILoRA, which linearly allocates a different rank for each layer, in an\nincreasing manner, and performs pruning throughout the training process,\nconsidering both the temporary magnitude of weights and the accumulated\nstatistics of the input to any given layer. We validate the effectiveness of\nPRILoRA through extensive experiments on eight GLUE benchmarks, setting a new\nstate of the art.", "published": "2024-01-20 20:25:17", "link": "http://arxiv.org/abs/2401.11316v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Orion-14B: Open-source Multilingual Large Language Models", "abstract": "In this study, we introduce Orion-14B, a collection of multilingual large\nlanguage models with 14 billion parameters. We utilize a data scheduling\napproach to train a foundational model on a diverse corpus of 2.5 trillion\ntokens, sourced from texts in English, Chinese, Japanese, Korean, and other\nlanguages. Additionally, we fine-tuned a series of models tailored for\nconversational applications and other specific use cases. Our evaluation\nresults demonstrate that Orion-14B achieves state-of-the-art performance across\na broad spectrum of tasks. We make the Orion-14B model family and its\nassociated code publicly accessible https://github.com/OrionStarAI/Orion,\naiming to inspire future research and practical applications in the field.", "published": "2024-01-20 12:29:27", "link": "http://arxiv.org/abs/2401.12246v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating and Enhancing Large Language Models Performance in\n  Domain-specific Medicine: Osteoarthritis Management with DocOA", "abstract": "The efficacy of large language models (LLMs) in domain-specific medicine,\nparticularly for managing complex diseases such as osteoarthritis (OA), remains\nlargely unexplored. This study focused on evaluating and enhancing the clinical\ncapabilities of LLMs in specific domains, using osteoarthritis (OA) management\nas a case study. A domain specific benchmark framework was developed, which\nevaluate LLMs across a spectrum from domain-specific knowledge to clinical\napplications in real-world clinical scenarios. DocOA, a specialized LLM\ntailored for OA management that integrates retrieval-augmented generation (RAG)\nand instruction prompts, was developed. The study compared the performance of\nGPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human\nevaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less\neffective in the specialized domain of OA management, particularly in providing\npersonalized treatment recommendations. However, DocOA showed significant\nimprovements. This study introduces a novel benchmark framework which assesses\nthe domain-specific abilities of LLMs in multiple aspects, highlights the\nlimitations of generalized LLMs in clinical contexts, and demonstrates the\npotential of tailored approaches for developing domain-specific medical LLMs.", "published": "2024-01-20 03:41:23", "link": "http://arxiv.org/abs/2401.12998v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Information Retrieval and Extraction Tool for Covid-19 Related Papers", "abstract": "Background: The COVID-19 pandemic has caused severe impacts on health systems\nworldwide. Its critical nature and the increased interest of individuals and\norganizations to develop countermeasures to the problem has led to a surge of\nnew studies in scientific journals. Objetive: We sought to develop a tool that\nincorporates, in a novel way, aspects of Information Retrieval (IR) and\nExtraction (IE) applied to the COVID-19 Open Research Dataset (CORD-19). The\nmain focus of this paper is to provide researchers with a better search tool\nfor COVID-19 related papers, helping them find reference papers and hightlight\nrelevant entities in text. Method: We applied Latent Dirichlet Allocation (LDA)\nto model, based on research aspects, the topics of all English abstracts in\nCORD-19. Relevant named entities of each abstract were extracted and linked to\nthe corresponding UMLS concept. Regular expressions and the K-Nearest Neighbors\nalgorithm were used to rank relevant papers. Results: Our tool has shown the\npotential to assist researchers by automating a topic-based search of CORD-19\npapers. Nonetheless, we identified that more fine-tuned topic modeling\nparameters and increased accuracy of the research aspect classifier model could\nlead to a more accurate and reliable tool. Conclusion: We emphasize the need of\nnew automated tools to help researchers find relevant COVID-19 documents, in\naddition to automatically extracting useful information contained in them. Our\nwork suggests that combining different algorithms and models could lead to new\nways of browsing COVID-19 paper data.", "published": "2024-01-20 01:34:50", "link": "http://arxiv.org/abs/2401.16430v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Embedding Ontologies via Incorporating Extensional and Intensional\n  Knowledge", "abstract": "Ontologies contain rich knowledge within domain, which can be divided into\ntwo categories, namely extensional knowledge and intensional knowledge.\nExtensional knowledge provides information about the concrete instances that\nbelong to specific concepts in the ontology, while intensional knowledge\ndetails inherent properties, characteristics, and semantic associations among\nconcepts. However, existing ontology embedding approaches fail to take both\nextensional knowledge and intensional knowledge into fine consideration\nsimultaneously. In this paper, we propose a novel ontology embedding approach\nnamed EIKE (Extensional and Intensional Knowledge Embedding) by representing\nontologies in two spaces, called extensional space and intensional space. EIKE\npresents a unified framework for embedding instances, concepts and their\nrelations in an ontology, applying a geometry-based method to model extensional\nknowledge and a pretrained language model to model intensional knowledge, which\ncan capture both structure information and textual information. Experimental\nresults show that EIKE significantly outperforms state-of-the-art methods in\nthree datasets for both triple classification and link prediction, indicating\nthat EIKE provides a more comprehensive and representative perspective of the\ndomain.", "published": "2024-01-20 08:44:34", "link": "http://arxiv.org/abs/2402.01677v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "STICKERCONV: Generating Multimodal Empathetic Responses from Scratch", "abstract": "Stickers, while widely recognized for enhancing empathetic communication in\nonline interactions, remain underexplored in current empathetic dialogue\nresearch, notably due to the challenge of a lack of comprehensive datasets. In\nthis paper, we introduce the Agent for STICKERCONV (Agent4SC), which uses\ncollaborative agent interactions to realistically simulate human behavior with\nsticker usage, thereby enhancing multimodal empathetic communication. Building\non this foundation, we develop a multimodal empathetic dialogue dataset,\nSTICKERCONV, comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K\ndiverse conversational scenarios. This dataset serves as a benchmark for\nmultimodal empathetic generation. To advance further, we propose PErceive and\nGenerate Stickers (PEGS), a multimodal empathetic response generation\nframework, complemented by a comprehensive set of empathy evaluation metrics\nbased on LLM. Our experiments demonstrate PEGS's effectiveness in generating\ncontextually relevant and emotionally resonant multimodal empathetic responses,\ncontributing to the advancement of more nuanced and engaging empathetic\ndialogue systems.", "published": "2024-01-20 13:44:21", "link": "http://arxiv.org/abs/2402.01679v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word-Level ASR Quality Estimation for Efficient Corpus Sampling and\n  Post-Editing through Analyzing Attentions of a Reference-Free Metric", "abstract": "In the realm of automatic speech recognition (ASR), the quest for models that\nnot only perform with high accuracy but also offer transparency in their\ndecision-making processes is crucial. The potential of quality estimation (QE)\nmetrics is introduced and evaluated as a novel tool to enhance explainable\nartificial intelligence (XAI) in ASR systems. Through experiments and analyses,\nthe capabilities of the NoRefER (No Reference Error Rate) metric are explored\nin identifying word-level errors to aid post-editors in refining ASR\nhypotheses. The investigation also extends to the utility of NoRefER in the\ncorpus-building process, demonstrating its effectiveness in augmenting datasets\nwith insightful annotations. The diagnostic aspects of NoRefER are examined,\nrevealing its ability to provide valuable insights into model behaviors and\ndecision patterns. This has proven beneficial for prioritizing hypotheses in\npost-editing workflows and fine-tuning ASR models. The findings suggest that\nNoRefER is not merely a tool for error detection but also a comprehensive\nframework for enhancing ASR systems' transparency, efficiency, and\neffectiveness. To ensure the reproducibility of the results, all source codes\nof this study are made publicly available.", "published": "2024-01-20 16:48:55", "link": "http://arxiv.org/abs/2401.11268v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Toward Robust Multimodal Learning using Multimodal Foundational Models", "abstract": "Existing multimodal sentiment analysis tasks are highly rely on the\nassumption that the training and test sets are complete multimodal data, while\nthis assumption can be difficult to hold: the multimodal data are often\nincomplete in real-world scenarios. Therefore, a robust multimodal model in\nscenarios with randomly missing modalities is highly preferred. Recently,\nCLIP-based multimodal foundational models have demonstrated impressive\nperformance on numerous multimodal tasks by learning the aligned cross-modal\nsemantics of image and text pairs, but the multimodal foundational models are\nalso unable to directly address scenarios involving modality absence. To\nalleviate this issue, we propose a simple and effective framework, namely TRML,\nToward Robust Multimodal Learning using Multimodal Foundational Models. TRML\nemploys generated virtual modalities to replace missing modalities, and aligns\nthe semantic spaces between the generated and missing modalities. Concretely,\nwe design a missing modality inference module to generate virtual modaliites\nand replace missing modalities. We also design a semantic matching learning\nmodule to align semantic spaces generated and missing modalities. Under the\nprompt of complete modality, our model captures the semantics of missing\nmodalities by leveraging the aligned cross-modal semantic space. Experiments\ndemonstrate the superiority of our approach on three multimodal sentiment\nanalysis benchmark datasets, CMU-MOSI, CMU-MOSEI, and MELD.", "published": "2024-01-20 04:46:43", "link": "http://arxiv.org/abs/2401.13697v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Density Adaptive Attention is All You Need: Robust Parameter-Efficient\n  Fine-Tuning Across Multiple Modalities", "abstract": "We propose the Multi-Head Density Adaptive Attention Mechanism (DAAM), a\nnovel probabilistic attention framework that can be used for\nParameter-Efficient Fine-tuning (PEFT), and the Density Adaptive Transformer\n(DAT), designed to enhance information aggregation across multiple modalities,\nincluding Speech, Text, and Vision. DAAM integrates learnable mean and variance\ninto its attention mechanism, implemented in a multi-head framework, enabling\nit to collectively model any probability distribution for dynamic recalibration\nof feature significance. This method demonstrates significant improvements,\nespecially with highly non-stationary data, surpassing the state-of-the-art\nattention techniques in model performance, up to approximately +20% (abs.) in\naccuracy. Empirically, DAAM exhibits superior adaptability and efficacy across\na diverse range of tasks, including emotion recognition in speech, image\nclassification, and text classification, thereby establishing its robustness\nand versatility in handling data across multiple modalities. Furthermore, we\nintroduce the Importance Factor, a new learning-based metric that enhances the\nexplainability of models trained with DAAM-based methods.", "published": "2024-01-20 06:42:32", "link": "http://arxiv.org/abs/2401.11143v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "ASM: Audio Spectrogram Mixer", "abstract": "Transformer structures have demonstrated outstanding skills in the deep\nlearning space recently, significantly increasing the accuracy of models across\na variety of domains. Researchers have started to question whether such a\nsophisticated network structure is actually necessary and whether equally\noutstanding results can be reached with reduced inference cost due to its\ncomplicated network topology and high inference cost. In order to prove the\nMixer's efficacy on three datasets Speech Commands, UrbanSound8k, and CASIA\nChinese Sentiment Corpus this paper applies amore condensed version of the\nMixer to an audio classification task and conducts comparative experiments with\nthe Transformer-based Audio Spectrogram Transformer (AST)model. In addition,\nthis paper conducts comparative experiments on the application of several\nactivation functions in Mixer, namely GeLU, Mish, Swish and Acon-C.\nFurther-more, the use of various activation functions in Mixer, including GeLU,\nMish, Swish, and Acon-C, is compared in this research through comparison\nexperiments. Additionally, some AST model flaws are highlighted, and the model\nsuggested in this study is improved as a result. In conclusion, a model called\nthe Audio Spectrogram Mixer, which is the first model for audio classification\nwith Mixer, is suggested in this study and the model's future directions for\nimprovement are examined.", "published": "2024-01-20 03:33:18", "link": "http://arxiv.org/abs/2401.11102v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundShift: Exploring Sound Manipulations for Accessible Mixed-Reality\n  Awareness", "abstract": "Mixed-reality (MR) soundscapes blend real-world sound with virtual audio from\nhearing devices, presenting intricate auditory information that is hard to\ndiscern and differentiate. This is particularly challenging for blind or\nvisually impaired individuals, who rely on sounds and descriptions in their\neveryday lives. To understand how complex audio information is consumed, we\nanalyzed online forum posts within the blind community, identifying prevailing\nchallenges, needs, and desired solutions. We synthesized the results and\npropose SoundShift for increasing MR sound awareness, which includes six sound\nmanipulations: Transparency Shift, Envelope Shift, Position Shift, Style Shift,\nTime Shift, and Sound Append. To evaluate the effectiveness of SoundShift, we\nconducted a user study with 18 blind participants across three simulated MR\nscenarios, where participants identified specific sounds within intricate\nsoundscapes. We found that SoundShift increased MR sound awareness and\nminimized cognitive load. Finally, we developed three real-world example\napplications to demonstrate the practicality of SoundShift.", "published": "2024-01-20 02:57:50", "link": "http://arxiv.org/abs/2401.11095v2", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Generalizing Speaker Verification for Spoof Awareness in the Embedding\n  Space", "abstract": "It is now well-known that automatic speaker verification (ASV) systems can be\nspoofed using various types of adversaries. The usual approach to counteract\nASV systems against such attacks is to develop a separate spoofing\ncountermeasure (CM) module to classify speech input either as a bonafide, or a\nspoofed utterance. Nevertheless, such a design requires additional computation\nand utilization efforts at the authentication stage. An alternative strategy\ninvolves a single monolithic ASV system designed to handle both zero-effort\nimposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have\nthe potential to provide stronger protections and more economic computations.\nTo this end, we propose to generalize the standalone ASV (G-SASV) against\nspoofing attacks, where we leverage limited training data from CM to enhance a\nsimple backend in the embedding space, without the involvement of a separate CM\nmodule during the test (authentication) phase. We propose a novel yet simple\nbackend classifier based on deep neural networks and conduct the study via\ndomain adaptation and multi-task integration of spoof embeddings at the\ntraining stage. Experiments are conducted on the ASVspoof 2019 logical access\ndataset, where we improve the performance of statistical ASV backends on the\njoint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and\n49.8% in terms of equal error rates, respectively.", "published": "2024-01-20 07:30:22", "link": "http://arxiv.org/abs/2401.11156v2", "categories": ["cs.CR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Projected Belief Networks With Discriminative Alignment for Acoustic\n  Event Classification: Rivaling State of the Art CNNs", "abstract": "The projected belief network (PBN) is a generative stochastic network with\ntractable likelihood function based on a feed-forward neural network (FFNN).\nThe generative function operates by \"backing up\" through the FFNN. The PBN is\ntwo networks in one, a FFNN that operates in the forward direction, and a\ngenerative network that operates in the backward direction. Both networks\nco-exist based on the same parameter set, have their own cost functions, and\ncan be separately or jointly trained. The PBN therefore has the potential to\npossess the best qualities of both discriminative and generative classifiers.\nTo realize this potential, a separate PBN is trained on each class, maximizing\nthe generative likelihood function for the given class, while minimizing the\ndiscriminative cost for the FFNN against \"all other classes\". This technique,\ncalled discriminative alignment (PBN-DA), aligns the contours of the likelihood\nfunction to the decision boundaries and attains vastly improved classification\nperformance, rivaling that of state of the art discriminative networks. The\nmethod may be further improved using a hidden Markov model (HMM) as a component\nof the PBN, called PBN-DA-HMM. This paper provides a comprehensive treatment of\nPBN, PBN-DA, and PBN-DA-HMM. In addition, the results of two new classification\nexperiments are provided. The first experiment uses air-acoustic events, and\nthe second uses underwater acoustic data consisting of marine mammal calls. In\nboth experiments, PBN-DA-HMM attains comparable or better performance as a\nstate of the art CNN, and attain a factor of two error reduction when combined\nwith the CNN.", "published": "2024-01-20 10:27:04", "link": "http://arxiv.org/abs/2401.11199v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "G.3; I.2"], "primary_category": "cs.LG"}
{"title": "LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre\n  Memory Units", "abstract": "Transformer models have demonstrated high accuracy in numerous applications\nbut have high complexity and lack sequential processing capability making them\nill-suited for many streaming applications at the edge where devices are\nheavily resource-constrained. Thus motivated, many researchers have proposed\nreformulating the transformer models as RNN modules which modify the\nself-attention computation with explicit states. However, these approaches\noften incur significant performance degradation. The ultimate goal is to\ndevelop a model that has the following properties: parallel training, streaming\nand low-cost inference, and SOTA performance. In this paper, we propose a new\ndirection to achieve this goal. We show how architectural modifications to a\nrecurrent model can help push its performance toward Transformer models while\nretaining its sequential processing capability. Specifically, inspired by the\nrecent success of Legendre Memory Units (LMU) in sequence learning tasks, we\npropose LMUFormer, which augments the LMU with convolutional patch embedding\nand convolutional channel mixer. Moreover, we present a spiking version of this\narchitecture, which introduces the benefit of states within the patch embedding\nand channel mixer modules while simultaneously reducing the computing\ncomplexity. We evaluated our architectures on multiple sequence datasets. In\ncomparison to SOTA transformer-based models within the ANN domain on the SCv2\ndataset, our LMUFormer demonstrates comparable performance while necessitating\na remarkable 53 times reduction in parameters and a substantial 65 times\ndecrement in FLOPs. Additionally, owing to our model's proficiency in real-time\ndata processing, we can achieve a 32.03% reduction in sequence length, all\nwhile incurring an inconsequential decline in performance. Our code is publicly\navailable at https://github.com/zeyuliu1037/LMUFormer.git.", "published": "2024-01-20 01:10:18", "link": "http://arxiv.org/abs/2402.04882v1", "categories": ["cs.NE", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
