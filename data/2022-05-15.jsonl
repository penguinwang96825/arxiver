{"title": "Hero-Gang Neural Model For Named Entity Recognition", "abstract": "Named entity recognition (NER) is a fundamental and important task in NLP,\naiming at identifying named entities (NEs) from free text. Recently, since the\nmulti-head attention mechanism applied in the Transformer model can effectively\ncapture longer contextual information, Transformer-based models have become the\nmainstream methods and have achieved significant performance in this task.\nUnfortunately, although these models can capture effective global context\ninformation, they are still limited in the local feature and position\ninformation extraction, which is critical in NER. In this paper, to address\nthis limitation, we propose a novel Hero-Gang Neural structure (HGN), including\nthe Hero and Gang module, to leverage both global and local information to\npromote NER. Specifically, the Hero module is composed of a Transformer-based\nencoder to maintain the advantage of the self-attention mechanism, and the Gang\nmodule utilizes a multi-window recurrent module to extract local features and\nposition information under the guidance of the Hero module. Afterward, the\nproposed multi-window attention effectively combines global information and\nmultiple local features for predicting entity labels. Experimental results on\nseveral benchmark datasets demonstrate the effectiveness of our proposed model.", "published": "2022-05-15 04:33:31", "link": "http://arxiv.org/abs/2205.07177v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Pre-trained Language Models for Few-shot Intent Detection:\n  Supervised Pre-training and Isotropization", "abstract": "It is challenging to train a good intent classifier for a task-oriented\ndialogue system with only a few annotations. Recent studies have shown that\nfine-tuning pre-trained language models with a small amount of labeled\nutterances from public benchmarks in a supervised manner is extremely helpful.\nHowever, we find that supervised pre-training yields an anisotropic feature\nspace, which may suppress the expressive power of the semantic representations.\nInspired by recent research in isotropization, we propose to improve supervised\npre-training by regularizing the feature space towards isotropy. We propose two\nregularizers based on contrastive learning and correlation matrix respectively,\nand demonstrate their effectiveness through extensive experiments. Our main\nfinding is that it is promising to regularize supervised pre-training with\nisotropization to further improve the performance of few-shot intent detection.\nThe source code can be found at https://github.com/fanolabs/isoIntentBert-main.", "published": "2022-05-15 07:48:13", "link": "http://arxiv.org/abs/2205.07208v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Latent Concepts Learned in BERT", "abstract": "A large number of studies that analyze deep neural network models and their\nability to encode various linguistic and non-linguistic concepts provide an\ninterpretation of the inner mechanics of these models. The scope of the\nanalyses is limited to pre-defined concepts that reinforce the traditional\nlinguistic knowledge and do not reflect on how novel concepts are learned by\nthe model. We address this limitation by discovering and analyzing latent\nconcepts learned in neural network models in an unsupervised fashion and\nprovide interpretations from the model's perspective. In this work, we study:\ni) what latent concepts exist in the pre-trained BERT model, ii) how the\ndiscovered latent concepts align or diverge from classical linguistic hierarchy\nand iii) how the latent concepts evolve across layers. Our findings show: i) a\nmodel learns novel concepts (e.g. animal categories and demographic groups),\nwhich do not strictly adhere to any pre-defined categorization (e.g. POS,\nsemantic tags), ii) several latent concepts are based on multiple properties\nwhich may include semantics, syntax, and morphology, iii) the lower layers in\nthe model dominate in learning shallow lexical concepts while the higher layers\nlearn semantic relations and iv) the discovered latent concepts highlight\npotential biases learned in the model. We also release a novel BERT ConceptNet\ndataset (BCN) consisting of 174 concept labels and 1M annotated instances.", "published": "2022-05-15 09:45:34", "link": "http://arxiv.org/abs/2205.07237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings\n  for Complex Word Identification", "abstract": "Complex word identification (CWI) is a cornerstone process towards proper\ntext simplification. CWI is highly dependent on context, whereas its difficulty\nis augmented by the scarcity of available datasets which vary greatly in terms\nof domains and languages. As such, it becomes increasingly more difficult to\ndevelop a robust model that generalizes across a wide array of input examples.\nIn this paper, we propose a novel training technique for the CWI task based on\ndomain adaptation to improve the target character and context representations.\nThis technique addresses the problem of working with multiple domains, inasmuch\nas it creates a way of smoothing the differences between the explored datasets.\nMoreover, we also propose a similar auxiliary task, namely text simplification,\nthat can be used to complement lexical complexity prediction. Our model obtains\na boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast\nto vanilla training techniques, when considering the CompLex from the Lexical\nComplexity Prediction 2021 dataset. At the same time, we obtain an increase of\n3% in Pearson scores, while considering a cross-lingual setup relying on the\nComplex Word Identification 2018 dataset. In addition, our model yields\nstate-of-the-art results in terms of Mean Absolute Error.", "published": "2022-05-15 13:21:02", "link": "http://arxiv.org/abs/2205.07283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta Self-Refinement for Robust Learning with Weak Supervision", "abstract": "Training deep neural networks (DNNs) under weak supervision has attracted\nincreasing research attention as it can significantly reduce the annotation\ncost. However, labels from weak supervision can be noisy, and the high capacity\nof DNNs enables them to easily overfit the label noise, resulting in poor\ngeneralization. Recent methods leverage self-training to build noise-resistant\nmodels, in which a teacher trained under weak supervision is used to provide\nhighly confident labels for teaching the students. Nevertheless, the teacher\nderived from such frameworks may have fitted a substantial amount of noise and\ntherefore produce incorrect pseudo-labels with high confidence, leading to\nsevere error propagation. In this work, we propose Meta Self-Refinement (MSR),\na noise-resistant learning framework, to effectively combat label noise from\nweak supervision. Instead of relying on a fixed teacher trained with noisy\nlabels, we encourage the teacher to refine its pseudo-labels. At each training\nstep, MSR performs a meta gradient descent on the current mini-batch to\nmaximize the student performance on a clean validation set. Extensive\nexperimentation on eight NLP benchmarks demonstrates that MSR is robust against\nlabel noise in all settings and outperforms state-of-the-art methods by up to\n11.4% in accuracy and 9.26% in F1 score.", "published": "2022-05-15 14:04:30", "link": "http://arxiv.org/abs/2205.07290v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transkimmer: Transformer Learns to Layer-wise Skim", "abstract": "Transformer architecture has become the de-facto model for many machine\nlearning tasks from natural language processing and computer vision. As such,\nimproving its computational efficiency becomes paramount. One of the major\ncomputational inefficiency of Transformer-based models is that they spend the\nidentical amount of computation throughout all layers. Prior works have\nproposed to augment the Transformer model with the capability of skimming\ntokens to improve its computational efficiency. However, they suffer from not\nhaving effectual and end-to-end optimization of the discrete skimming\npredictor. To address the above limitations, we propose the Transkimmer\narchitecture, which learns to identify hidden state tokens that are not\nrequired by each layer. The skimmed tokens are then forwarded directly to the\nfinal output, thus reducing the computation of the successive layers. The key\nidea in Transkimmer is to add a parameterized predictor before each layer that\nlearns to make the skimming decision. We also propose to adopt\nreparameterization trick and add skim loss for the end-to-end training of\nTranskimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark\ncompared with vanilla BERT-base baseline with less than 1% accuracy\ndegradation.", "published": "2022-05-15 16:23:30", "link": "http://arxiv.org/abs/2205.07324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts\n  and Zero-shot Models", "abstract": "Recent research showed promising results on combining pretrained language\nmodels (LMs) with canonical utterance for few-shot semantic parsing. The\ncanonical utterance is often lengthy and complex due to the compositional\nstructure of formal languages. Learning to generate such canonical utterance\nrequires significant amount of data to reach high performance. Fine-tuning with\nonly few-shot samples, the LMs can easily forget pretrained knowledge, overfit\nspurious biases, and suffer from compositionally out-of-distribution\ngeneralization errors. To tackle these issues, we propose a novel few-shot\nsemantic parsing method -- SeqZero. SeqZero decomposes the problem into a\nsequence of sub-problems, which correspond to the sub-clauses of the formal\nlanguage. Based on the decomposition, the LMs only need to generate short\nanswers using prompts for predicting sub-clauses. Thus, SeqZero avoids\ngenerating a long canonical utterance at once. Moreover, SeqZero employs not\nonly a few-shot model but also a zero-shot model to alleviate the overfitting.\nIn particular, SeqZero brings out the merits from both models via ensemble\nequipped with our proposed constrained rescaling. SeqZero achieves SOTA\nperformance of BART-based models on GeoQuery and EcommerceQuery, which are two\nfew-shot datasets with compositional data split.", "published": "2022-05-15 21:13:15", "link": "http://arxiv.org/abs/2205.07381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Downstream Transformer Generation of Question-Answer Pairs with\n  Preprocessing and Postprocessing Pipelines", "abstract": "We present a system called TP3 to perform a downstream task of transformers\non generating question-answer pairs (QAPs) from a given article. TP3 first\nfinetunes pretrained transformers on QAP datasets, then uses a preprocessing\npipeline to select appropriate answers, feeds the relevant sentences and the\nanswer to the finetuned transformer to generate candidate QAPs, and finally\nuses a postprocessing pipeline to filter inadequate QAPs. In particular, using\npretrained T5 models as transformers and the SQuAD dataset as the finetruning\ndataset, we show that TP3 generates satisfactory number of QAPs with high\nqualities on the Gaokao-EN dataset.", "published": "2022-05-15 21:53:45", "link": "http://arxiv.org/abs/2205.07387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Generalizability of Fine-Tuned Models for Fake News Detection", "abstract": "The Covid-19 pandemic has caused a dramatic and parallel rise in dangerous\nmisinformation, denoted an `infodemic' by the CDC and WHO. Misinformation tied\nto the Covid-19 infodemic changes continuously; this can lead to performance\ndegradation of fine-tuned models due to concept drift. Degredation can be\nmitigated if models generalize well-enough to capture some cyclical aspects of\ndrifted data. In this paper, we explore generalizability of pre-trained and\nfine-tuned fake news detectors across 9 fake news datasets. We show that\nexisting models often overfit on their training dataset and have poor\nperformance on unseen data. However, on some subsets of unseen data that\noverlap with training data, models have higher accuracy. Based on this\nobservation, we also present KMeans-Proxy, a fast and effective method based on\nK-Means clustering for quickly identifying these overlapping subsets of unseen\ndata. KMeans-Proxy improves generalizability on unseen fake news datasets by\n0.1-0.2 f1-points across datasets. We present both our generalizability\nexperiments as well as KMeans-Proxy to further research in tackling the fake\nnews problem.", "published": "2022-05-15 00:00:49", "link": "http://arxiv.org/abs/2205.07154v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "From Cognitive to Computational Modeling: Text-based Risky\n  Decision-Making Guided by Fuzzy Trace Theory", "abstract": "Understanding, modelling and predicting human risky decision-making is\nchallenging due to intrinsic individual differences and irrationality. Fuzzy\ntrace theory (FTT) is a powerful paradigm that explains human decision-making\nby incorporating gists, i.e., fuzzy representations of information which\ncapture only its quintessential meaning. Inspired by Broniatowski and Reyna's\nFTT cognitive model, we propose a computational framework which combines the\neffects of the underlying semantics and sentiments on text-based\ndecision-making. In particular, we introduce Category-2-Vector to learn\ncategorical gists and categorical sentiments, and demonstrate how our\ncomputational model can be optimised to predict risky decision-making in groups\nand individuals.", "published": "2022-05-15 02:25:28", "link": "http://arxiv.org/abs/2205.07164v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adaptive Prompt Learning-based Few-Shot Sentiment Analysis", "abstract": "In the field of natural language processing, sentiment analysis via deep\nlearning has a excellent performance by using large labeled datasets.\nMeanwhile, labeled data are insufficient in many sentiment analysis, and\nobtaining these data is time-consuming and laborious. Prompt learning devotes\nto resolving the data deficiency by reformulating downstream tasks with the\nhelp of prompt. In this way, the appropriate prompt is very important for the\nperformance of the model. This paper proposes an adaptive prompting(AP)\nconstruction strategy using seq2seq-attention structure to acquire the semantic\ninformation of the input sequence. Then dynamically construct adaptive prompt\nwhich can not only improve the quality of the prompt, but also can effectively\ngeneralize to other fields by pre-trained prompt which is constructed by\nexisting public labeled data. The experimental results on FewCLUE datasets\ndemonstrate that the proposed method AP can effectively construct appropriate\nadaptive prompt regardless of the quality of hand-crafted prompt and outperform\nthe state-of-the-art baselines.", "published": "2022-05-15 08:34:48", "link": "http://arxiv.org/abs/2205.07220v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Toxic Degeneration with Empathetic Data: Exploring the\n  Relationship Between Toxicity and Empathy", "abstract": "Large pre-trained neural language models have supported the effectiveness of\nmany NLP tasks, yet are still prone to generating toxic language hindering the\nsafety of their use. Using empathetic data, we improve over recent work on\ncontrollable text generation that aims to reduce the toxicity of generated\ntext. We find we are able to dramatically reduce the size of fine-tuning data\nto 7.5-30k samples while at the same time making significant improvements over\nstate-of-the-art toxicity mitigation of up to 3.4% absolute reduction (26%\nrelative) from the original work on 2.3m samples, by strategically sampling\ndata based on empathy scores. We observe that the degree of improvement is\nsubject to specific communication components of empathy. In particular, the\ncognitive components of empathy significantly beat the original dataset in\nalmost all experiments, while emotional empathy was tied to less improvement\nand even underperforming random samples of the original data. This is a\nparticularly implicative insight for NLP work concerning empathy as until\nrecently the research and resources built for it have exclusively considered\nempathy as an emotional concept.", "published": "2022-05-15 09:37:15", "link": "http://arxiv.org/abs/2205.07233v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Not to Overfit or Underfit the Source Domains? An Empirical Study of\n  Domain Generalization in Question Answering", "abstract": "Machine learning models are prone to overfitting their training (source)\ndomains, which is commonly believed to be the reason why they falter in novel\ntarget domains. Here we examine the contrasting view that multi-source domain\ngeneralization (DG) is first and foremost a problem of mitigating source domain\nunderfitting: models not adequately learning the signal already present in\ntheir multi-domain training data. Experiments on a reading comprehension DG\nbenchmark show that as a model learns its source domains better -- using\nfamiliar methods such as knowledge distillation (KD) from a bigger model -- its\nzero-shot out-of-domain utility improves at an even faster pace. Improved\nsource domain learning also demonstrates superior out-of-domain generalization\nover three popular existing DG approaches that aim to limit overfitting. Our\nimplementation of KD-based domain generalization is available via PrimeQA at:\nhttps://ibm.biz/domain-generalization-with-kd.", "published": "2022-05-15 10:53:40", "link": "http://arxiv.org/abs/2205.07257v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Classifiers are Better Experts for Controllable Text Generation", "abstract": "This paper proposes a simple method for controllable text generation based on\nweighting logits with a free-form classifier, namely CAIF sampling. Using an\narbitrary text classifier, we adjust a small part of a language model's logits\nand guide text generation towards or away from classifier prediction. We\nexperimented with toxicity avoidance and sentiment control tasks and showed\nthat the proposed method significantly outperforms recent PPLM, GeDi, and\nDExperts on PPL and task accuracy metrics based on the external classifier of\ngenerated texts. In addition, compared to other approaches, it is easier to\nimplement and tune and has significantly fewer restrictions and requirements.", "published": "2022-05-15 12:58:35", "link": "http://arxiv.org/abs/2205.07276v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TiBERT: Tibetan Pre-trained Language Model", "abstract": "The pre-trained language model is trained on large-scale unlabeled text and\ncan achieve state-of-the-art results in many different downstream tasks.\nHowever, the current pre-trained language model is mainly concentrated in the\nChinese and English fields. For low resource language such as Tibetan, there is\nlack of a monolingual pre-trained model. To promote the development of Tibetan\nnatural language processing tasks, this paper collects the large-scale training\ndata from Tibetan websites and constructs a vocabulary that can cover 99.95$\\%$\nof the words in the corpus by using Sentencepiece. Then, we train the Tibetan\nmonolingual pre-trained language model named TiBERT on the data and vocabulary.\nFinally, we apply TiBERT to the downstream tasks of text classification and\nquestion generation, and compare it with classic models and multilingual\npre-trained models, the experimental results show that TiBERT can achieve the\nbest performance. Our model is published in http://tibert.cmli-nlp.com/", "published": "2022-05-15 14:45:08", "link": "http://arxiv.org/abs/2205.07303v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Long-term Control for Dialogue Generation: Methods and Evaluation", "abstract": "Current approaches for controlling dialogue response generation are primarily\nfocused on high-level attributes like style, sentiment, or topic. In this work,\nwe focus on constrained long-term dialogue generation, which involves more\nfine-grained control and requires a given set of control words to appear in\ngenerated responses. This setting requires a model to not only consider the\ngeneration of these control words in the immediate context, but also produce\nutterances that will encourage the generation of the words at some time in the\n(possibly distant) future. We define the problem of constrained long-term\ncontrol for dialogue generation, identify gaps in current methods for\nevaluation, and propose new metrics that better measure long-term control. We\nalso propose a retrieval-augmented method that improves performance of\nlong-term controlled generation via logit modification techniques. We show\nthrough experiments on three task-oriented dialogue datasets that our metrics\nbetter assess dialogue control relative to current alternatives and that our\nmethod outperforms state-of-the-art constrained generation baselines.", "published": "2022-05-15 18:33:37", "link": "http://arxiv.org/abs/2205.07352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain\n  Text-to-Speech", "abstract": "Style transfer for out-of-domain (OOD) speech synthesis aims to generate\nspeech samples with unseen style (e.g., speaker identity, emotion, and prosody)\nderived from an acoustic reference, while facing the following challenges: 1)\nThe highly dynamic style features in expressive voice are difficult to model\nand transfer; and 2) the TTS models should be robust enough to handle diverse\nOOD conditions that differ from the source data. This paper proposes\nGenerSpeech, a text-to-speech model towards high-fidelity zero-shot style\ntransfer of OOD custom voice. GenerSpeech decomposes the speech variation into\nthe style-agnostic and style-specific parts by introducing two components: 1) a\nmulti-level style adaptor to efficiently model a large range of style\nconditions, including global speaker and emotion characteristics, and the local\n(utterance, phoneme, and word-level) fine-grained prosodic representations; and\n2) a generalizable content adaptor with Mix-Style Layer Normalization to\neliminate style information in the linguistic content representation and thus\nimprove model generalization. Our evaluations on zero-shot style transfer\ndemonstrate that GenerSpeech surpasses the state-of-the-art models in terms of\naudio quality and style similarity. The extension studies to adaptive style\ntransfer further show that GenerSpeech performs robustly in the few-shot data\nsetting. Audio samples are available at https://GenerSpeech.github.io/", "published": "2022-05-15 08:16:02", "link": "http://arxiv.org/abs/2205.07211v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Textual Explanations and Critiques in Recommendation Systems", "abstract": "Artificial intelligence and machine learning algorithms have become\nubiquitous. Although they offer a wide range of benefits, their adoption in\ndecision-critical fields is limited by their lack of interpretability,\nparticularly with textual data. Moreover, with more data available than ever\nbefore, it has become increasingly important to explain automated predictions.\n  Generally, users find it difficult to understand the underlying computational\nprocesses and interact with the models, especially when the models fail to\ngenerate the outcomes or explanations, or both, correctly. This problem\nhighlights the growing need for users to better understand the models' inner\nworkings and gain control over their actions. This dissertation focuses on two\nfundamental challenges of addressing this need. The first involves explanation\ngeneration: inferring high-quality explanations from text documents in a\nscalable and data-driven manner. The second challenge consists in making\nexplanations actionable, and we refer to it as critiquing. This dissertation\nexamines two important applications in natural language processing and\nrecommendation tasks.\n  Overall, we demonstrate that interpretability does not come at the cost of\nreduced performance in two consequential applications. Our framework is\napplicable to other fields as well. This dissertation presents an effective\nmeans of closing the gap between promise and practice in artificial\nintelligence.", "published": "2022-05-15 11:59:23", "link": "http://arxiv.org/abs/2205.07268v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Topic Modelling on Consumer Financial Protection Bureau Data: An\n  Approach Using BERT Based Embeddings", "abstract": "Customers' reviews and comments are important for businesses to understand\nusers' sentiment about the products and services. However, this data needs to\nbe analyzed to assess the sentiment associated with topics/aspects to provide\nefficient customer assistance. LDA and LSA fail to capture the semantic\nrelationship and are not specific to any domain. In this study, we evaluate\nBERTopic, a novel method that generates topics using sentence embeddings on\nConsumer Financial Protection Bureau (CFPB) data. Our work shows that BERTopic\nis flexible and yet provides meaningful and diverse topics compared to LDA and\nLSA. Furthermore, domain-specific pre-trained embeddings (FinBERT) yield even\nbetter topics. We evaluated the topics on coherence score (c_v) and UMass.", "published": "2022-05-15 11:14:47", "link": "http://arxiv.org/abs/2205.07259v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT", "abstract": "This paper investigates self-supervised pre-training for audio-visual speaker\nrepresentation learning where a visual stream showing the speaker's mouth area\nis used alongside speech as inputs. Our study focuses on the Audio-Visual\nHidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose\naudio-visual speech pre-training framework. We conducted extensive experiments\nprobing the effectiveness of pre-training and visual modality. Experimental\nresults suggest that AV-HuBERT generalizes decently to speaker related\ndownstream tasks, improving label efficiency by roughly ten fold for both\naudio-only and audio-visual speaker verification. We also show that\nincorporating visual information, even just the lip area, greatly improves the\nperformance and noise robustness, reducing EER by 38% in the clean condition\nand 75% in noisy conditions.", "published": "2022-05-15 04:48:41", "link": "http://arxiv.org/abs/2205.07180v2", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conditional Vector Graphics Generation for Music Cover Images", "abstract": "Generative Adversarial Networks (GAN) have motivated a rapid growth of the\ndomain of computer image synthesis. As almost all the existing image synthesis\nalgorithms consider an image as a pixel matrix, the high-resolution image\nsynthesis is complicated.A good alternative can be vector images. However, they\nbelong to the highly sophisticated parametric space, which is a restriction for\nsolving the task of synthesizing vector graphics by GANs. In this paper, we\nconsider a specific application domain that softens this restriction\ndramatically allowing the usage of vector image synthesis.\n  Music cover images should meet the requirements of Internet streaming\nservices and printing standards, which imply high resolution of graphic\nmaterials without any additional requirements on the content of such images.\nExisting music cover image generation services do not analyze tracks\nthemselves; however, some services mostly consider only genre tags. To generate\nmusic covers as vector images that reflect the music and consist of simple\ngeometric objects, we suggest a GAN-based algorithm called CoverGAN. The\nassessment of resulting images is based on their correspondence to the music\ncompared with AttnGAN and DALL-E text-to-image generation according to title or\nlyrics. Moreover, the significance of the patterns found by CoverGAN has been\nevaluated in terms of the correspondence of the generated cover images to the\nmusical tracks. Listeners evaluate the music covers generated by the proposed\nalgorithm as quite satisfactory and corresponding to the tracks. Music cover\nimages generation code and demo are available at\nhttps://github.com/IzhanVarsky/CoverGAN.", "published": "2022-05-15 14:43:03", "link": "http://arxiv.org/abs/2205.07301v1", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
{"title": "cMelGAN: An Efficient Conditional Generative Model Based on Mel\n  Spectrograms", "abstract": "Analysing music in the field of machine learning is a very difficult problem\nwith numerous constraints to consider. The nature of audio data, with its very\nhigh dimensionality and widely varying scales of structure, is one of the\nprimary reasons why it is so difficult to model. There are many applications of\nmachine learning in music, like the classifying the mood of a piece of music,\nconditional music generation, or popularity prediction. The goal for this\nproject was to develop a genre-conditional generative model of music based on\nMel spectrograms and evaluate its performance by comparing it to existing\ngenerative music models that use note-based representations. We initially\nimplemented an autoregressive, RNN-based generative model called MelNet .\nHowever, due to its slow speed and low fidelity output, we decided to create a\nnew, fully convolutional architecture that is based on the MelGAN [4] and\nconditional GAN architectures, called cMelGAN.", "published": "2022-05-15 15:53:43", "link": "http://arxiv.org/abs/2205.07319v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Representations for New Sound Classes With Continual\n  Self-Supervised Learning", "abstract": "In this paper, we work on a sound recognition system that continually\nincorporates new sound classes. Our main goal is to develop a framework where\nthe model can be updated without relying on labeled data. For this purpose, we\npropose adopting representation learning, where an encoder is trained using\nunlabeled data. This learning framework enables the study and implementation of\na practically relevant use case where only a small amount of the labels is\navailable in a continual learning context. We also make the empirical\nobservation that a similarity-based representation learning method within this\nframework is robust to forgetting even if no explicit mechanism against\nforgetting is employed. We show that this approach obtains similar performance\ncompared to several distillation-based continual learning methods when employed\non self-supervised representation learning methods.", "published": "2022-05-15 22:15:21", "link": "http://arxiv.org/abs/2205.07390v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
