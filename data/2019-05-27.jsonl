{"title": "Harry Potter and the Action Prediction Challenge from Natural Language", "abstract": "We explore the challenge of action prediction from textual descriptions of\nscenes, a testbed to approximate whether text inference can be used to predict\nupcoming actions. As a case of study, we consider the world of the Harry Potter\nfantasy novels and inferring what spell will be cast next given a fragment of a\nstory. Spells act as keywords that abstract actions (e.g. 'Alohomora' to open a\ndoor) and denote a response to the environment. This idea is used to\nautomatically build HPAC, a corpus containing 82,836 samples and 85 actions. We\nthen evaluate different baselines. Among the tested models, an LSTM-based\napproach obtains the best performance for frequent actions and large scene\ndescriptions, but approaches such as logistic regression behave well on\ninfrequent actions.", "published": "2019-05-27 08:28:51", "link": "http://arxiv.org/abs/1905.11037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Specific polysemy of the brief sapiential units", "abstract": "In this paper we explain how we deal with the problems related to the\nconstitution of the Aliento database, the complexity of which has to do with\nthe type of phrases we work with, the differences between languages, the type\nof information we want to see emerge. The correct tagging of the specific\npolysemy of brief sapiential units is an important step in the preparation of\nthe text within the corpus which will be submitted to compute similarities and\nposterity of the units.", "published": "2019-05-27 08:54:26", "link": "http://arxiv.org/abs/1905.11836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Item Response Theory for Cognitive Diagnosis", "abstract": "Cognitive diagnosis is a fundamental and crucial task in many educational\napplications, e.g., computer adaptive test and cognitive assignments. Item\nResponse Theory (IRT) is a classical cognitive diagnosis method which can\nprovide interpretable parameters (i.e., student latent trait, question\ndiscrimination, and difficulty) for analyzing student performance. However,\ntraditional IRT ignores the rich information in question texts, cannot diagnose\nknowledge concept proficiency, and it is inaccurate to diagnose the parameters\nfor the questions which only appear several times. To this end, in this paper,\nwe propose a general Deep Item Response Theory (DIRT) framework to enhance\ntraditional IRT for cognitive diagnosis by exploiting semantic representation\nfrom question texts with deep learning. In DIRT, we first use a proficiency\nvector to represent students' proficiency in knowledge concepts and embed\nquestion texts and knowledge concepts to dense vectors by Word2Vec. Then, we\ndesign a deep diagnosis module to diagnose parameters in traditional IRT by\ndeep learning techniques. Finally, with the diagnosed parameters, we input them\ninto the logistic-like formula of IRT to predict student performance. Extensive\nexperimental results on real-world data clearly demonstrate the effectiveness\nand interpretation power of DIRT framework.", "published": "2019-05-27 03:35:30", "link": "http://arxiv.org/abs/1905.10957v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Levenshtein Transformer", "abstract": "Modern neural sequence generation models are built to either generate tokens\nstep-by-step from scratch or (iteratively) modify a sequence of tokens bounded\nby a fixed length. In this work, we develop Levenshtein Transformer, a new\npartially autoregressive model devised for more flexible and amenable sequence\ngeneration. Unlike previous approaches, the atomic operations of our model are\ninsertion and deletion. The combination of them facilitates not only generation\nbut also sequence refinement allowing dynamic length changes. We also propose a\nset of new training techniques dedicated at them, effectively exploiting one as\nthe other's learning signal thanks to their complementary nature. Experiments\napplying the proposed model achieve comparable performance but much-improved\nefficiency on both generation (e.g. machine translation, text summarization)\nand refinement tasks (e.g. automatic post-editing). We further confirm the\nflexibility of our model by showing a Levenshtein Transformer trained by\nmachine translation can straightforwardly be used for automatic post-editing.", "published": "2019-05-27 07:08:12", "link": "http://arxiv.org/abs/1905.11006v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compositional pre-training for neural semantic parsing", "abstract": "Semantic parsing is the process of translating natural language utterances\ninto logical forms, which has many important applications such as question\nanswering and instruction following. Sequence-to-sequence models have been very\nsuccessful across many NLP tasks. However, a lack of task-specific prior\nknowledge can be detrimental to the performance of these models. Prior work has\nused frameworks for inducing grammars over the training examples, which capture\nconditional independence properties that the model can leverage. Inspired by\nthe recent success stories such as BERT we set out to extend this augmentation\nframework into two stages. The first stage is to pre-train using a corpus of\naugmented examples in an unsupervised manner. The second stage is to fine-tune\nto a domain-specific task. In addition, since the pre-training stage is\nseparate from the training on the main task we also expand the universe of\npossible augmentations without causing catastrophic inference. We also propose\na novel data augmentation strategy that interchanges tokens that co-occur in\nsimilar contexts to produce new training pairs. We demonstrate that the\nproposed two-stage framework is beneficial for improving the parsing accuracy\nin a standard dataset called GeoQuery for the task of generating logical forms\nfrom a set of questions about the US geography.", "published": "2019-05-27 22:51:39", "link": "http://arxiv.org/abs/1905.11531v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequential Graph Dependency Parser", "abstract": "We propose a method for non-projective dependency parsing by incrementally\npredicting a set of edges. Since the edges do not have a pre-specified order,\nwe propose a set-based learning method. Our method blends graph, transition,\nand easy-first parsing, including a prior state of the parser as a special\ncase. The proposed transition-based method successfully parses near the state\nof the art on both projective and non-projective languages, without assuming a\ncertain parsing order.", "published": "2019-05-27 01:42:30", "link": "http://arxiv.org/abs/1905.10930v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "QuesNet: A Unified Representation for Heterogeneous Test Questions", "abstract": "Understanding learning materials (e.g. test questions) is a crucial issue in\nonline learning systems, which can promote many applications in education\ndomain. Unfortunately, many supervised approaches suffer from the problem of\nscarce human labeled data, whereas abundant unlabeled resources are highly\nunderutilized. To alleviate this problem, an effective solution is to use\npre-trained representations for question understanding. However, existing\npre-training methods in NLP area are infeasible to learn test question\nrepresentations due to several domain-specific characteristics in education.\nFirst, questions usually comprise of heterogeneous data including content text,\nimages and side information. Second, there exists both basic linguistic\ninformation as well as domain logic and knowledge. To this end, in this paper,\nwe propose a novel pre-training method, namely QuesNet, for comprehensively\nlearning question representations. Specifically, we first design a unified\nframework to aggregate question information with its heterogeneous inputs into\na comprehensive vector. Then we propose a two-level hierarchical pre-training\nalgorithm to learn better understanding of test questions in an unsupervised\nway. Here, a novel holed language model objective is developed to extract\nlow-level linguistic features, and a domain-oriented objective is proposed to\nlearn high-level logic and knowledge. Moreover, we show that QuesNet has good\ncapability of being fine-tuned in many question-based tasks. We conduct\nextensive experiments on large-scale real-world question data, where the\nexperimental results clearly demonstrate the effectiveness of QuesNet for\nquestion understanding as well as its superior applicability.", "published": "2019-05-27 03:08:17", "link": "http://arxiv.org/abs/1905.10949v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "An Empirical Study on Post-processing Methods for Word Embeddings", "abstract": "Word embeddings learnt from large corpora have been adopted in various\napplications in natural language processing and served as the general input\nrepresentations to learning systems. Recently, a series of post-processing\nmethods have been proposed to boost the performance of word embeddings on\nsimilarity comparison and analogy retrieval tasks, and some have been adapted\nto compose sentence representations. The general hypothesis behind these\nmethods is that by enforcing the embedding space to be more isotropic, the\nsimilarity between words can be better expressed. We view these methods as an\napproach to shrink the covariance/gram matrix, which is estimated by learning\nword vectors, towards a scaled identity matrix. By optimising an objective in\nthe semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we are\nable to search for the optimal shrinkage parameter, and provide a\npost-processing method to smooth the spectrum of learnt word vectors which\nyields improved performance on downstream tasks.", "published": "2019-05-27 04:49:45", "link": "http://arxiv.org/abs/1905.10971v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Commonsense Properties from Query Logs and Question Answering Forums", "abstract": "Commonsense knowledge about object properties, human behavior and general\nconcepts is crucial for robust AI applications. However, automatic acquisition\nof this knowledge is challenging because of sparseness and bias in online\nsources. This paper presents Quasimodo, a methodology and tool suite for\ndistilling commonsense properties from non-standard web sources. We devise\nnovel ways of tapping into search-engine query logs and QA forums, and\ncombining the resulting candidate assertions with statistical cues from\nencyclopedias, books and image tags in a corroboration step. Unlike prior work\non commonsense knowledge bases, Quasimodo focuses on salient properties that\nare typically associated with certain objects or concepts. Extensive\nevaluations, including extrinsic use-case studies, show that Quasimodo provides\nbetter coverage than state-of-the-art baselines with comparable quality.", "published": "2019-05-27 06:12:56", "link": "http://arxiv.org/abs/1905.10989v4", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "ET-GAN: Cross-Language Emotion Transfer Based on Cycle-Consistent\n  Generative Adversarial Networks", "abstract": "Despite the remarkable progress made in synthesizing emotional speech from\ntext, it is still challenging to provide emotion information to existing speech\nsegments. Previous methods mainly rely on parallel data, and few works have\nstudied the generalization ability for one model to transfer emotion\ninformation across different languages. To cope with such problems, we propose\nan emotion transfer system named ET-GAN, for learning language-independent\nemotion transfer from one emotion to another without parallel training samples.\nBased on cycle-consistent generative adversarial network, our method ensures\nthe transfer of only emotion information across speeches with simple loss\ndesigns. Besides, we introduce an approach for migrating emotion information\nacross different languages by using transfer learning. The experiment results\nshow that our method can efficiently generate high-quality emotional speech for\nany given emotion category, without aligned speech pairs.", "published": "2019-05-27 12:41:36", "link": "http://arxiv.org/abs/1905.11173v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AgentGraph: Towards Universal Dialogue Management with Structured Deep\n  Reinforcement Learning", "abstract": "Dialogue policy plays an important role in task-oriented spoken dialogue\nsystems. It determines how to respond to users. The recently proposed deep\nreinforcement learning (DRL) approaches have been used for policy optimization.\nHowever, these deep models are still challenging for two reasons: 1) Many\nDRL-based policies are not sample-efficient. 2) Most models don't have the\ncapability of policy transfer between different domains. In this paper, we\npropose a universal framework, AgentGraph, to tackle these two problems. The\nproposed AgentGraph is the combination of GNN-based architecture and DRL-based\nalgorithm. It can be regarded as one of the multi-agent reinforcement learning\napproaches. Each agent corresponds to a node in a graph, which is defined\naccording to the dialogue domain ontology. When making a decision, each agent\ncan communicate with its neighbors on the graph. Under AgentGraph framework, we\nfurther propose Dual GNN-based dialogue policy, which implicitly decomposes the\ndecision in each turn into a high-level global decision and a low-level local\ndecision. Experiments show that AgentGraph models significantly outperform\ntraditional reinforcement learning approaches on most of the 18 tasks of the\nPyDial benchmark. Moreover, when transferred from the source task to a target\ntask, these models not only have acceptable initial performance but also\nconverge much faster on the target task.", "published": "2019-05-27 14:27:13", "link": "http://arxiv.org/abs/1905.11259v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Combating Adversarial Misspellings with Robust Word Recognition", "abstract": "To combat adversarial spelling mistakes, we propose placing a word\nrecognition model in front of the downstream classifier. Our word recognition\nmodels build upon the RNN semi-character architecture, introducing several new\nbackoff strategies for handling rare and unseen words. Trained to recognize\nwords corrupted by random adds, drops, swaps, and keyboard mistakes, our method\nachieves 32% relative (and 3.3% absolute) error reduction over the vanilla\nsemi-character model. Notably, our pipeline confers robustness on the\ndownstream classifier, outperforming both adversarial training and\noff-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment\nanalysis, a single adversarially-chosen character attack lowers accuracy from\n90.3% to 45.8%. Our defense restores accuracy to 75%. Surprisingly, better word\nrecognition does not always entail greater robustness. Our analysis reveals\nthat robustness also depends upon a quantity that we denote the sensitivity.", "published": "2019-05-27 14:35:35", "link": "http://arxiv.org/abs/1905.11268v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Neural Networks for Relation Extraction from Biomedical Literature", "abstract": "Using different sources of information to support automated extracting of\nrelations between biomedical concepts contributes to the development of our\nunderstanding of biological systems. The primary comprehensive source of these\nrelations is biomedical literature. Several relation extraction approaches have\nbeen proposed to identify relations between concepts in biomedical literature,\nnamely, using neural networks algorithms. The use of multichannel architectures\ncomposed of multiple data representations, as in deep neural networks, is\nleading to state-of-the-art results. The right combination of data\nrepresentations can eventually lead us to even higher evaluation scores in\nrelation extraction tasks. Thus, biomedical ontologies play a fundamental role\nby providing semantic and ancestry information about an entity. The\nincorporation of biomedical ontologies has already been proved to enhance\nprevious state-of-the-art results.", "published": "2019-05-27 09:33:29", "link": "http://arxiv.org/abs/1905.11391v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Self-Attention Joint Model for Spoken Language Understanding in\n  Situational Dialog Applications", "abstract": "Spoken language understanding (SLU) acts as a critical component in\ngoal-oriented dialog systems. It typically involves identifying the speakers\nintent and extracting semantic slots from user utterances, which are known as\nintent detection (ID) and slot filling (SF). SLU problem has been intensively\ninvestigated in recent years. However, these methods just constrain SF results\ngrammatically, solve ID and SF independently, or do not fully utilize the\nmutual impact of the two tasks. This paper proposes a multi-head self-attention\njoint model with a conditional random field (CRF) layer and a prior mask. The\nexperiments show the effectiveness of our model, as compared with\nstate-of-the-art models. Meanwhile, online education in China has made great\nprogress in the last few years. But there are few intelligent educational\ndialog applications for students to learn foreign languages. Hence, we design\nan intelligent dialog robot equipped with different scenario settings to help\nstudents learn communication skills.", "published": "2019-05-27 10:22:20", "link": "http://arxiv.org/abs/1905.11393v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "VQVAE Unsupervised Unit Discovery and Multi-scale Code2Spec Inverter for\n  Zerospeech Challenge 2019", "abstract": "We describe our submitted system for the ZeroSpeech Challenge 2019. The\ncurrent challenge theme addresses the difficulty of constructing a speech\nsynthesizer without any text or phonetic labels and requires a system that can\n(1) discover subword units in an unsupervised way, and (2) synthesize the\nspeech with a target speaker's voice. Moreover, the system should also balance\nthe discrimination score ABX, the bit-rate compression rate, and the\nnaturalness and the intelligibility of the constructed voice. To tackle these\nproblems and achieve the best trade-off, we utilize a vector quantized\nvariational autoencoder (VQ-VAE) and a multi-scale codebook-to-spectrogram\n(Code2Spec) inverter trained by mean square error and adversarial loss. The\nVQ-VAE extracts the speech to a latent space, forces itself to map it into the\nnearest codebook and produces compressed representation. Next, the inverter\ngenerates a magnitude spectrogram to the target voice, given the codebook\nvectors from VQ-VAE. In our experiments, we also investigated several other\nclustering algorithms, including K-Means and GMM, and compared them with the\nVQ-VAE result on ABX scores and bit rates. Our proposed approach significantly\nimproved the intelligibility (in CER), the MOS, and discrimination ABX scores\ncompared to the official ZeroSpeech 2019 baseline or even the topline.", "published": "2019-05-27 18:59:54", "link": "http://arxiv.org/abs/1905.11449v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and\n  Question Answering", "abstract": "While natural language processing systems often focus on a single language,\nmultilingual transfer learning has the potential to improve performance,\nespecially for low-resource languages. We introduce XLDA, cross-lingual data\naugmentation, a method that replaces a segment of the input text with its\ntranslation in another language. XLDA enhances performance of all 14 tested\nlanguages of the cross-lingual natural language inference (XNLI) benchmark.\nWith improvements of up to $4.8\\%$, training with XLDA achieves\nstate-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast\nto, and performs markedly better than, a more naive approach that aggregates\nexamples in various languages in a way that each example is solely in one\nlanguage. On the SQuAD question answering task, we see that XLDA provides a\n$1.0\\%$ performance increase on the English evaluation set. Comprehensive\nexperiments suggest that most languages are effective as cross-lingual\naugmentors, that XLDA is robust to a wide range of translation quality, and\nthat XLDA is even more effective for randomly initialized models than for\npretrained models.", "published": "2019-05-27 19:44:33", "link": "http://arxiv.org/abs/1905.11471v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition", "abstract": "In this paper, we propose a novel soft and monotonic alignment mechanism used\nfor sequence transduction. It is inspired by the integrate-and-fire model in\nspiking neural networks and employed in the encoder-decoder framework consists\nof continuous functions, thus being named as: Continuous Integrate-and-Fire\n(CIF). Applied to the ASR task, CIF not only shows a concise calculation, but\nalso supports online recognition and acoustic boundary positioning, thus\nsuitable for various ASR scenarios. Several support strategies are also\nproposed to alleviate the unique problems of CIF-based model. With the joint\naction of these methods, the CIF-based model shows competitive performance.\nNotably, it achieves a word error rate (WER) of 2.86% on the test-clean of\nLibrispeech and creates new state-of-the-art result on Mandarin telephone ASR\nbenchmark.", "published": "2019-05-27 14:00:45", "link": "http://arxiv.org/abs/1905.11235v4", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge", "abstract": "In this paper, we present our system developed by the team from the New\nTechnologies for the Information Society (NTIS) research center of the\nUniversity of West Bohemia in Pilsen, for the Second DIHARD Speech Diarization\nChallenge. The base of our system follows the currently-standard approach of\nsegmentation, i/x-vector extraction, clustering, and resegmentation. The\nhyperparameters for each of the subsystems were selected according to the\ndomain classifier trained on the development set of DIHARD II. We compared our\nsystem with results from the Kaldi diarization (with i/x-vectors) and combined\nthese systems. At the time of writing of this abstract, our best submission\nachieved a DER of 23.47% and a JER of 48.99% on the evaluation set (in Track 1\nusing reference SAD).", "published": "2019-05-27 14:52:51", "link": "http://arxiv.org/abs/1905.11276v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio2Face: Generating Speech/Face Animation from Single Audio with\n  Attention-Based Bidirectional LSTM Networks", "abstract": "We propose an end to end deep learning approach for generating real-time\nfacial animation from just audio. Specifically, our deep architecture employs\ndeep bidirectional long short-term memory network and attention mechanism to\ndiscover the latent representations of time-varying contextual information\nwithin the speech and recognize the significance of different information\ncontributed to certain face status. Therefore, our model is able to drive\ndifferent levels of facial movements at inference and automatically keep up\nwith the corresponding pitch and latent speaking style in the input audio, with\nno assumption or further human intervention. Evaluation results show that our\nmethod could not only generate accurate lip movements from audio, but also\nsuccessfully regress the speaker's time-varying facial movements.", "published": "2019-05-27 11:40:21", "link": "http://arxiv.org/abs/1905.11142v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.LG"}
