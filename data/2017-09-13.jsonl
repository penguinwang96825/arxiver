{"title": "Dialogue Act Sequence Labeling using Hierarchical encoder with CRF", "abstract": "Dialogue Act recognition associate dialogue acts (i.e., semantic labels) to\nutterances in a conversation. The problem of associating semantic labels to\nutterances can be treated as a sequence labeling problem. In this work, we\nbuild a hierarchical recurrent neural network using bidirectional LSTM as a\nbase unit and the conditional random field (CRF) as the top layer to classify\neach utterance into its corresponding dialogue act. The hierarchical network\nlearns representations at multiple levels, i.e., word level, utterance level,\nand conversation level. The conversation level representations are input to the\nCRF layer, which takes into account not only all previous utterances but also\ntheir dialogue acts, thus modeling the dependency among both, labels and\nutterances, an important consideration of natural dialogue. We validate our\napproach on two different benchmark data sets, Switchboard and Meeting Recorder\nDialogue Act, and show performance improvement over the state-of-the-art\nmethods by $2.2\\%$ and $4.1\\%$ absolute points, respectively. It is worth\nnoting that the inter-annotator agreement on Switchboard data set is $84\\%$,\nand our method is able to achieve the accuracy of about $79\\%$ despite being\ntrained on the noisy data.", "published": "2017-09-13 11:01:02", "link": "http://arxiv.org/abs/1709.04250v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Flexible End-to-End Dialogue System for Knowledge Grounded Conversation", "abstract": "In knowledge grounded conversation, domain knowledge plays an important role\nin a special domain such as Music. The response of knowledge grounded\nconversation might contain multiple answer entities or no entity at all.\nAlthough existing generative question answering (QA) systems can be applied to\nknowledge grounded conversation, they either have at most one entity in a\nresponse or cannot deal with out-of-vocabulary entities. We propose a fully\ndata-driven generative dialogue system GenDS that is capable of generating\nresponses based on input message and related knowledge base (KB). To generate\narbitrary number of answer entities even when these entities never appear in\nthe training set, we design a dynamic knowledge enquirer which selects\ndifferent answer entities at different positions in a single response,\naccording to different local context. It does not rely on the representations\nof entities, enabling our model deal with out-of-vocabulary entities. We\ncollect a human-human conversation data (ConversMusic) with knowledge\nannotations. The proposed method is evaluated on CoversMusic and a public\nquestion answering dataset. Our proposed GenDS system outperforms baseline\nmethods significantly in terms of the BLEU, entity accuracy, entity recall and\nhuman evaluation. Moreover,the experiments also demonstrate that GenDS works\nbetter even on small datasets.", "published": "2017-09-13 11:59:06", "link": "http://arxiv.org/abs/1709.04264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Inference over Interaction Space", "abstract": "Natural Language Inference (NLI) task requires an agent to determine the\nlogical relationship between a natural language premise and a natural language\nhypothesis. We introduce Interactive Inference Network (IIN), a novel class of\nneural network architectures that is able to achieve high-level understanding\nof the sentence pair by hierarchically extracting semantic features from\ninteraction space. We show that an interaction tensor (attention weight)\ncontains semantic information to solve natural language inference, and a denser\ninteraction tensor contains richer semantic information. One instance of such\narchitecture, Densely Interactive Inference Network (DIIN), demonstrates the\nstate-of-the-art performance on large scale NLI copora and large-scale NLI\nalike corpus. It's noteworthy that DIIN achieve a greater than 20% error\nreduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to\nthe strongest published system.", "published": "2017-09-13 14:22:14", "link": "http://arxiv.org/abs/1709.04348v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Features of Genre and Method Variation in Translation: A\n  Computational Perspective", "abstract": "In this paper we describe the use of text classification methods to\ninvestigate genre and method variation in an English - German translation\ncorpus. For this purpose we use linguistically motivated features representing\ntexts using a combination of part-of-speech tags arranged in bigrams, trigrams,\nand 4-grams. The classification method used in this paper is a Bayesian\nclassifier with Laplace smoothing. We use the output of the classifiers to\ncarry out an extensive feature analysis on the main difference between genres\nand methods of translation.", "published": "2017-09-13 14:46:50", "link": "http://arxiv.org/abs/1709.04359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Review of Evaluation Techniques for Social Dialogue Systems", "abstract": "In contrast with goal-oriented dialogue, social dialogue has no clear measure\nof task success. Consequently, evaluation of these systems is notoriously hard.\nIn this paper, we review current evaluation methods, focusing on automatic\nmetrics. We conclude that turn-based metrics often ignore the context and do\nnot account for the fact that several replies are valid, while end-of-dialogue\nrewards are mainly hand-crafted. Both lack grounding in human perceptions.", "published": "2017-09-13 16:31:44", "link": "http://arxiv.org/abs/1709.04409v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis", "abstract": "This paper fills a gap in aspect-based sentiment analysis and aims to present\na new method for preparing and analysing texts concerning opinion and\ngenerating user-friendly descriptive reports in natural language. We present a\ncomprehensive set of techniques derived from Rhetorical Structure Theory and\nsentiment analysis to extract aspects from textual opinions and then build an\nabstractive summary of a set of opinions. Moreover, we propose aspect-aspect\ngraphs to evaluate the importance of aspects and to filter out unimportant ones\nfrom the summary. Additionally, the paper presents a prototype solution of data\nflow with interesting and valuable results. The proposed method's results\nproved the high accuracy of aspect detection when applied to the gold standard\ndataset.", "published": "2017-09-13 18:17:56", "link": "http://arxiv.org/abs/1709.04491v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Empower Sequence Labeling with Task-Aware Neural Language Model", "abstract": "Linguistic sequence labeling is a general modeling approach that encompasses\na variety of problems, such as part-of-speech tagging and named entity\nrecognition. Recent advances in neural networks (NNs) make it possible to build\nreliable models without handcrafted features. However, in many cases, it is\nhard to obtain sufficient annotations to train these models. In this study, we\ndevelop a novel neural framework to extract abundant knowledge hidden in raw\ntexts to empower the sequence labeling task. Besides word-level knowledge\ncontained in pre-trained word embeddings, character-aware neural language\nmodels are incorporated to extract character-level knowledge. Transfer learning\ntechniques are further adopted to mediate different components and guide the\nlanguage model towards the key knowledge. Comparing to previous methods, these\ntask-specific knowledge allows us to adopt a more concise model and conduct\nmore efficient training. Different from most transfer learning methods, the\nproposed framework does not rely on any additional supervision. It extracts\nknowledge from self-contained order information of training sequences.\nExtensive experiments on benchmark datasets demonstrate the effectiveness of\nleveraging character-level knowledge and the efficiency of co-training. For\nexample, on the CoNLL03 NER task, model training completes in about 6 hours on\na single GPU, reaching F1 score of 91.71$\\pm$0.10 without using any extra\nannotation.", "published": "2017-09-13 02:13:25", "link": "http://arxiv.org/abs/1709.04109v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing State-of-the-Art Sentiment Models on State-of-the-Art\n  Sentiment Datasets", "abstract": "There has been a good amount of progress in sentiment analysis over the past\n10 years, including the proposal of new methods and the creation of benchmark\ndatasets. In some papers, however, there is a tendency to compare models only\non one or two datasets, either because of time restraints or because the model\nis tailored to a specific task. Accordingly, it is hard to understand how well\na certain model generalizes across different tasks and datasets. In this paper,\nwe contribute to this situation by comparing several models on six different\nbenchmarks, which belong to different domains and additionally have different\nlevels of granularity (binary, 3-class, 4-class and 5-class). We show that\nBi-LSTMs perform well across datasets and that both LSTMs and Bi-LSTMs are\nparticularly good at fine-grained sentiment tasks (i. e., with more than two\nclasses). Incorporating sentiment information into word embeddings during\ntraining gives good results for datasets that are lexically similar to the\ntraining data. With our experiments, we contribute to a better understanding of\nthe performance of different model architectures on different data sets.\nConsequently, we detect novel state-of-the-art results on the SenTube datasets.", "published": "2017-09-13 09:43:02", "link": "http://arxiv.org/abs/1709.04219v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using NLU in Context for Question Answering: Improving on Facebook's\n  bAbI Tasks", "abstract": "For the next step in human to machine interaction, Artificial Intelligence\n(AI) should interact predominantly using natural language because, if it\nworked, it would be the fastest way to communicate. Facebook's toy tasks (bAbI)\nprovide a useful benchmark to compare implementations for conversational AI.\nWhile the published experiments so far have been based on exploiting the\ndistributional hypothesis with machine learning, our model exploits natural\nlanguage understanding (NLU) with the decomposition of language based on Role\nand Reference Grammar (RRG) and the brain-based Patom theory. Our combinatorial\nsystem for conversational AI based on linguistics has many advantages: passing\nbAbI task tests without parsing or statistics while increasing scalability. Our\nmodel validates both the training and test data to find 'garbage' input and\noutput (GIGO). It is not rules-based, nor does it use parts of speech, but\ninstead relies on meaning. While Deep Learning is difficult to debug and fix,\nevery step in our model can be understood and changed like any non-statistical\ncomputer program. Deep Learning's lack of explicable reasoning has raised\nopposition to AI, partly due to fear of the unknown. To support the goals of\nAI, we propose extended tasks to use human-level statements with tense, aspect\nand voice, and embedded clauses with junctures: and answers to be natural\nlanguage generation (NLG) instead of keywords. While machine learning permits\ninvalid training data to produce incorrect test responses, our system cannot\nbecause the context tracking would need to be intentionally broken. We believe\nno existing learning systems can currently solve these extended natural\nlanguage tests. There appears to be a knowledge gap between NLP researchers and\nlinguists, but ongoing competitive results such as these promise to narrow that\ngap.", "published": "2017-09-13 22:48:39", "link": "http://arxiv.org/abs/1709.04558v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Network Based Nonlinear Weighted Finite Automata", "abstract": "Weighted finite automata (WFA) can expressively model functions defined over\nstrings but are inherently linear models. Given the recent successes of\nnonlinear models in machine learning, it is natural to wonder whether\nex-tending WFA to the nonlinear setting would be beneficial. In this paper, we\npropose a novel model of neural network based nonlinearWFA model (NL-WFA) along\nwith a learning algorithm. Our learning algorithm is inspired by the spectral\nlearning algorithm for WFAand relies on a nonlinear decomposition of the\nso-called Hankel matrix, by means of an auto-encoder network. The expressive\npower of NL-WFA and the proposed learning algorithm are assessed on both\nsynthetic and real-world data, showing that NL-WFA can lead to smaller model\nsizes and infer complex grammatical structures from data.", "published": "2017-09-13 15:26:50", "link": "http://arxiv.org/abs/1709.04380v2", "categories": ["cs.FL", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.FL"}
{"title": "Analyzing Hidden Representations in End-to-End Automatic Speech\n  Recognition Systems", "abstract": "Neural models have become ubiquitous in automatic speech recognition systems.\nWhile neural networks are typically used as acoustic models in more complex\nsystems, recent studies have explored end-to-end speech recognition systems\nbased on neural networks, which can be trained to directly predict text from\ninput acoustic features. Although such systems are conceptually elegant and\nsimpler than traditional systems, it is less obvious how to interpret the\ntrained models. In this work, we analyze the speech representations learned by\na deep end-to-end model that is based on convolutional and recurrent layers,\nand trained with a connectionist temporal classification (CTC) loss. We use a\npre-trained model to generate frame-level features which are given to a\nclassifier that is trained on frame classification into phones. We evaluate\nrepresentations from different layers of the deep model and compare their\nquality for predicting phone labels. Our experiments shed light on important\naspects of the end-to-end model such as layer depth, model complexity, and\nother design choices.", "published": "2017-09-13 18:02:53", "link": "http://arxiv.org/abs/1709.04482v1", "categories": ["cs.CL", "cs.NE", "cs.SD", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On the Complex Network Structure of Musical Pieces: Analysis of Some Use\n  Cases from Different Music Genres", "abstract": "This paper focuses on the modeling of musical melodies as networks. Notes of\na melody can be treated as nodes of a network. Connections are created whenever\nnotes are played in sequence. We analyze some main tracks coming from different\nmusic genres, with melodies played using different musical instruments. We find\nout that the considered networks are, in general, scale free networks and\nexhibit the small world property. We measure the main metrics and assess\nwhether these networks can be considered as formed by sub-communities. Outcomes\nconfirm that peculiar features of the tracks can be extracted from this\nanalysis methodology. This approach can have an impact in several multimedia\napplications such as music didactics, multimedia entertainment, and digital\nmusic generation.", "published": "2017-09-13 15:04:30", "link": "http://arxiv.org/abs/1709.09708v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
