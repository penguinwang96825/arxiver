{"title": "Towards Sub-Word Level Compositions for Sentiment Analysis of\n  Hindi-English Code Mixed Text", "abstract": "Sentiment analysis (SA) using code-mixed data from social media has several\napplications in opinion mining ranging from customer satisfaction to social\ncampaign analysis in multilingual societies. Advances in this area are impeded\nby the lack of a suitable annotated dataset. We introduce a Hindi-English\n(Hi-En) code-mixed dataset for sentiment analysis and perform empirical\nanalysis comparing the suitability and performance of various state-of-the-art\nSA methods in social media.\n  In this paper, we introduce learning sub-word level representations in LSTM\n(Subword-LSTM) architecture instead of character-level or word-level\nrepresentations. This linguistic prior in our architecture enables us to learn\nthe information about sentiment value of important morphemes. This also seems\nto work well in highly noisy text containing misspellings as shown in our\nexperiments which is demonstrated in morpheme-level feature maps learned by our\nmodel. Also, we hypothesize that encoding this linguistic prior in the\nSubword-LSTM architecture leads to the superior performance. Our system attains\naccuracy 4-5% greater than traditional approaches on our dataset, and also\noutperforms the available system for sentiment analysis in Hi-En code-mixed\ntext by 18%.", "published": "2016-11-02 05:23:53", "link": "http://arxiv.org/abs/1611.00472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Context Dependent Messages in a Conversational Environment", "abstract": "While automatic response generation for building chatbot systems has drawn a\nlot of attention recently, there is limited understanding on when we need to\nconsider the linguistic context of an input text in the generation process. The\ntask is challenging, as messages in a conversational environment are short and\ninformal, and evidence that can indicate a message is context dependent is\nscarce. After a study of social conversation data crawled from the web, we\nobserved that some characteristics estimated from the responses of messages are\ndiscriminative for identifying context dependent messages. With the\ncharacteristics as weak supervision, we propose using a Long Short Term Memory\n(LSTM) network to learn a classifier. Our method carries out text\nrepresentation and classifier learning in a unified framework. Experimental\nresults show that the proposed method can significantly outperform baseline\nmethods on accuracy of classification.", "published": "2016-11-02 07:01:25", "link": "http://arxiv.org/abs/1611.00483v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ordinal Common-sense Inference", "abstract": "Humans have the capacity to draw common-sense inferences from natural\nlanguage: various things that are likely but not certain to hold based on\nestablished discourse, and are rarely stated explicitly. We propose an\nevaluation of automated common-sense inference based on an extension of\nrecognizing textual entailment: predicting ordinal human responses on the\nsubjective likelihood of an inference holding in a given context. We describe a\nframework for extracting common-sense knowledge from corpora, which is then\nused to construct a dataset for this ordinal entailment task. We train a neural\nsequence-to-sequence model on this dataset, which we use to score and generate\npossible inferences. Further, we annotate subsets of previously established\ndatasets via our ordinal annotation protocol in order to then analyze the\ndistinctions between these and what we have constructed.", "published": "2016-11-02 13:38:32", "link": "http://arxiv.org/abs/1611.00601v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fuzzy paraphrases in learning word representations with a lexicon", "abstract": "A synonym of a polysemous word is usually only the paraphrase of one sense\namong many. When lexicons are used to improve vector-space word\nrepresentations, such paraphrases are unreliable and bring noise to the\nvector-space. The prior works use a coefficient to adjust the overall learning\nof the lexicons. They regard the paraphrases equally. In this paper, we propose\na novel approach that regards the paraphrases diversely to alleviate the\nadverse effects of polysemy. We annotate each paraphrase with a degree of\nreliability. The paraphrases are randomly eliminated according to the degrees\nwhen our model learns word representations. In this way, our approach drops the\nunreliable paraphrases, keeping more reliable paraphrases at the same time. The\nexperimental results show that the proposed method improves the word vectors.\nOur approach is an attempt to address the polysemy problem keeping one vector\nper word. It makes the approach easier to use than the conventional methods\nthat estimate multiple vectors for a word. Our approach also outperforms the\nprior works in the experiments.", "published": "2016-11-02 16:38:18", "link": "http://arxiv.org/abs/1611.00674v9", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A FOFE-based Local Detection Approach for Named Entity Recognition and\n  Mention Detection", "abstract": "In this paper, we study a novel approach for named entity recognition (NER)\nand mention detection in natural language processing. Instead of treating NER\nas a sequence labelling problem, we propose a new local detection approach,\nwhich rely on the recent fixed-size ordinally forgetting encoding (FOFE) method\nto fully encode each sentence fragment and its left/right contexts into a\nfixed-size representation. Afterwards, a simple feedforward neural network is\nused to reject or predict entity label for each individual fragment. The\nproposed method has been evaluated in several popular NER and mention detection\ntasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016\nTri-lingual Entity Discovery and Linking (EDL) tasks. Our methods have yielded\npretty strong performance in all of these examined tasks. This local detection\napproach has shown many advantages over the traditional sequence labelling\nmethods.", "published": "2016-11-02 20:52:46", "link": "http://arxiv.org/abs/1611.00801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Asymmetric Opinions on Online Social Interrelationship with\n  Language and Network Features", "abstract": "Instead of studying the properties of social relationship from an objective\nview, in this paper, we focus on individuals' subjective and asymmetric\nopinions on their interrelationships. Inspired by the theories from\nsociolinguistics, we investigate two individuals' opinions on their\ninterrelationship with their interactive language features. Eliminating the\ndifference of personal language style, we clarify that the asymmetry of\ninteractive language feature values can indicate individuals' asymmetric\nopinions on their interrelationship. We also discuss how the degree of\nopinions' asymmetry is related to the individuals' personality traits.\nFurthermore, to measure the individuals' asymmetric opinions on\ninterrelationship concretely, we develop a novel model synthetizing interactive\nlanguage and social network features. The experimental results with Enron email\ndataset provide multiple evidences of the asymmetric opinions on\ninterrelationship, and also verify the effectiveness of the proposed model in\nmeasuring the degree of opinions' asymmetry.", "published": "2016-11-02 03:04:42", "link": "http://arxiv.org/abs/1611.00456v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Structure vs. Language: Investigating the Multi-factors of Asymmetric\n  Opinions on Online Social Interrelationship with a Case Study", "abstract": "Though current researches often study the properties of online social\nrelationship from an objective view, we also need to understand individuals'\nsubjective opinions on their interrelationships in social computing studies.\nInspired by the theories from sociolinguistics, the latest work indicates that\ninteractive language can reveal individuals' asymmetric opinions on their\ninterrelationship. In this work, in order to explain the opinions' asymmetry on\ninterrelationship with more latent factors, we extend the investigation from\nsingle relationship to the structural context in online social network. We\nanalyze the correlation between interactive language features and the\nstructural context of interrelationships. The structural context of vertex,\nedges and triangles in social network are considered. With statistical analysis\non Enron email dataset, we find that individuals' opinions (measured by\ninteractive language features) on their interrelationship are related to some\nof their important structural context in social network. This result can help\nus to understand and measure the individuals' opinions on their\ninterrelationship with more intrinsic information.", "published": "2016-11-02 03:11:10", "link": "http://arxiv.org/abs/1611.00457v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S.\n  Presidential Election", "abstract": "This paper describes a Naive-Bayesian predictive model for 2016 U.S.\nPresidential Election based on Twitter data. We use 33,708 tweets gathered\nsince December 16, 2015 until February 29, 2016. We introduce a simpler data\npreprocessing method to label the data and train the model. The model achieves\n95.8% accuracy on 10-fold cross validation and predicts Ted Cruz and Bernie\nSanders as Republican and Democratic nominee respectively. It achieves a\ncomparable result to those in its competitor methods.", "published": "2016-11-02 01:45:28", "link": "http://arxiv.org/abs/1611.00440v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "The Intelligent Voice 2016 Speaker Recognition System", "abstract": "This paper presents the Intelligent Voice (IV) system submitted to the NIST\n2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this\nyear was on developing speaker recognition technology which is robust for novel\nlanguages that are much more heterogeneous than those used in the current\nstate-of-the-art, using significantly less training data, that does not contain\nmeta-data from those languages. The system is based on the state-of-the-art\ni-vector/PLDA which is developed on the fixed training condition, and the\nresults are reported on the protocol defined on the development set of the\nchallenge.", "published": "2016-11-02 09:24:10", "link": "http://arxiv.org/abs/1611.00514v1", "categories": ["cs.SD", "cs.CL", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "abstract": "Neural networks (NN) have achieved state-of-the-art performance in various\napplications. Unfortunately in applications where training data is\ninsufficient, they are often prone to overfitting. One effective way to\nalleviate this problem is to exploit the Bayesian approach by using Bayesian\nneural networks (BNN). Another shortcoming of NN is the lack of flexibility to\ncustomize different distributions for the weights and neurons according to the\ndata, as is often done in probabilistic graphical models. To address these\nproblems, we propose a class of probabilistic neural networks, dubbed\nnatural-parameter networks (NPN), as a novel and lightweight Bayesian treatment\nof NN. NPN allows the usage of arbitrary exponential-family distributions to\nmodel the weights and neurons. Different from traditional NN and BNN, NPN takes\ndistributions as input and goes through layers of transformation before\nproducing distributions to match the target output distributions. As a Bayesian\ntreatment, efficient backpropagation (BP) is performed to learn the natural\nparameters for the distributions over both the weights and neurons. The output\ndistributions of each layer, as byproducts, may be used as second-order\nrepresentations for the associated tasks such as link prediction. Experiments\non real-world datasets show that NPN can achieve state-of-the-art performance.", "published": "2016-11-02 02:32:05", "link": "http://arxiv.org/abs/1611.00448v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in\n  the Blanks", "abstract": "Hybrid methods that utilize both content and rating information are commonly\nused in many recommender systems. However, most of them use either handcrafted\nfeatures or the bag-of-words representation as a surrogate for the content\ninformation but they are neither effective nor natural enough. To address this\nproblem, we develop a collaborative recurrent autoencoder (CRAE) which is a\ndenoising recurrent autoencoder (DRAE) that models the generation of content\nsequences in the collaborative filtering (CF) setting. The model generalizes\nrecent advances in recurrent deep learning from i.i.d. input to non-i.i.d.\n(CF-based) input and provides a new denoising scheme along with a novel\nlearnable pooling scheme for the recurrent autoencoder. To do this, we first\ndevelop a hierarchical Bayesian model for the DRAE and then generalize it to\nthe CF setting. The synergy between denoising and CF enables CRAE to make\naccurate recommendations while learning to fill in the blanks in sequences.\nExperiments on real-world datasets from different domains (CiteULike and\nNetflix) show that, by jointly modeling the order-aware generation of sequences\nfor the content information and performing CF for the ratings, CRAE is able to\nsignificantly outperform the state of the art on both the recommendation task\nbased on ratings and the sequence generation task based on content information.", "published": "2016-11-02 02:49:44", "link": "http://arxiv.org/abs/1611.00454v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
