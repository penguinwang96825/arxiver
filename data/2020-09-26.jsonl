{"title": "Learning to Plan and Realize Separately for Open-Ended Dialogue Systems", "abstract": "Achieving true human-like ability to conduct a conversation remains an\nelusive goal for open-ended dialogue systems. We posit this is because extant\napproaches towards natural language generation (NLG) are typically construed as\nend-to-end architectures that do not adequately model human generation\nprocesses. To investigate, we decouple generation into two separate phases:\nplanning and realization. In the planning phase, we train two planners to\ngenerate plans for response utterances. The realization phase uses response\nplans to produce an appropriate response. Through rigorous evaluations, both\nautomated and human, we demonstrate that decoupling the process into planning\nand realization performs better than an end-to-end approach.", "published": "2020-09-26 02:31:42", "link": "http://arxiv.org/abs/2009.12506v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "iNLTK: Natural Language Toolkit for Indic Languages", "abstract": "We present iNLTK, an open-source NLP library consisting of pre-trained\nlanguage models and out-of-the-box support for Data Augmentation, Textual\nSimilarity, Sentence Embeddings, Word Embeddings, Tokenization and Text\nGeneration in 13 Indic Languages. By using pre-trained models from iNLTK for\ntext classification on publicly available datasets, we significantly outperform\npreviously reported results. On these datasets, we also show that by using\npre-trained models and data augmentation from iNLTK, we can achieve more than\n95% of the previous best performance by using less than 10% of the training\ndata. iNLTK is already being widely used by the community and has 40,000+\ndownloads, 600+ stars and 100+ forks on GitHub. The library is available at\nhttps://github.com/goru001/inltk.", "published": "2020-09-26 08:21:32", "link": "http://arxiv.org/abs/2009.12534v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Aware Multi-turn Dialogue Modeling", "abstract": "In the retrieval-based multi-turn dialogue modeling, it remains a challenge\nto select the most appropriate response according to extracting salient\nfeatures in context utterances. As a conversation goes on, topic shift at\ndiscourse-level naturally happens through the continuous multi-turn dialogue\ncontext. However, all known retrieval-based systems are satisfied with\nexploiting local topic words for context utterance representation but fail to\ncapture such essential global topic-aware clues at discourse-level. Instead of\ntaking topic-agnostic n-gram utterance as processing unit for matching purpose\nin existing systems, this paper presents a novel topic-aware solution for\nmulti-turn dialogue modeling, which segments and extracts topic-aware\nutterances in an unsupervised way, so that the resulted model is capable of\ncapturing salient topic shift at discourse-level in need and thus effectively\ntrack topic flow during multi-turn conversation. Our topic-aware modeling is\nimplemented by a newly proposed unsupervised topic-aware segmentation algorithm\nand Topic-Aware Dual-attention Matching (TADAM) Network, which matches each\ntopic segment with the response in a dual cross-attention way. Experimental\nresults on three public datasets show TADAM can outperform the state-of-the-art\nmethod, especially by 3.3% on E-commerce dataset that has an obvious topic\nshift.", "published": "2020-09-26 08:43:06", "link": "http://arxiv.org/abs/2009.12539v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARPA: Armenian Paraphrase Detection Corpus and Models", "abstract": "In this work, we employ a semi-automatic method based on back translation to\ngenerate a sentential paraphrase corpus for the Armenian language. The initial\ncollection of sentences is translated from Armenian to English and back twice,\nresulting in pairs of lexically distant but semantically similar sentences. The\ngenerated paraphrases are then manually reviewed and annotated. Using the\nmethod train and test datasets are created, containing 2360 paraphrases in\ntotal. In addition, the datasets are used to train and evaluate BERTbased\nmodels for detecting paraphrase in Armenian, achieving results comparable to\nthe state-of-the-art of other languages.", "published": "2020-09-26 14:56:57", "link": "http://arxiv.org/abs/2009.12615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DWIE: an entity-centric dataset for multi-task document-level\n  information extraction", "abstract": "This paper presents DWIE, the 'Deutsche Welle corpus for Information\nExtraction', a newly created multi-task dataset that combines four main\nInformation Extraction (IE) annotation subtasks: (i) Named Entity Recognition\n(NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv)\nEntity Linking. DWIE is conceived as an entity-centric dataset that describes\ninteractions and properties of conceptual entities on the level of the complete\ndocument. This contrasts with currently dominant mention-driven approaches that\nstart from the detection and classification of named entity mentions in\nindividual sentences. Further, DWIE presented two main challenges when building\nand evaluating IE models for it. First, the use of traditional mention-level\nevaluation metrics for NER and RE tasks on entity-centric DWIE dataset can\nresult in measurements dominated by predictions on more frequently mentioned\nentities. We tackle this issue by proposing a new entity-driven metric that\ntakes into account the number of mentions that compose each of the predicted\nand ground truth entities. Second, the document-level multi-task annotations\nrequire the models to transfer information between entity mentions located in\ndifferent parts of the document, as well as between different tasks, in a joint\nlearning setting. To realize this, we propose to use graph-based neural message\npassing techniques between document-level mention spans. Our experiments show\nan improvement of up to 5.5 F1 percentage points when incorporating neural\ngraph propagation into our joint model. This demonstrates DWIE's potential to\nstimulate further research in graph neural networks for representation learning\nin multi-task IE. We make DWIE publicly available at\nhttps://github.com/klimzaporojets/DWIE.", "published": "2020-09-26 15:53:22", "link": "http://arxiv.org/abs/2009.12626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent Inference in Text Editing", "abstract": "In neural text editing, prevalent sequence-to-sequence based approaches\ndirectly map the unedited text either to the edited text or the editing\noperations, in which the performance is degraded by the limited source text\nencoding and long, varying decoding steps. To address this problem, we propose\na new inference method, Recurrence, that iteratively performs editing actions,\nsignificantly narrowing the problem space. In each iteration, encoding the\npartially edited text, Recurrence decodes the latent representation, generates\nan action of short, fixed-length, and applies the action to complete a single\nedit. For a comprehensive comparison, we introduce three types of text editing\ntasks: Arithmetic Operators Restoration (AOR), Arithmetic Equation\nSimplification (AES), Arithmetic Equation Correction (AEC). Extensive\nexperiments on these tasks with varying difficulties demonstrate that\nRecurrence achieves improvements over conventional inference methods.", "published": "2020-09-26 17:06:29", "link": "http://arxiv.org/abs/2009.12643v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Dyadic Conversations for Personality Inference", "abstract": "Nowadays, automatical personality inference is drawing extensive attention\nfrom both academia and industry. Conventional methods are mainly based on user\ngenerated contents, e.g., profiles, likes, and texts of an individual, on\nsocial media, which are actually not very reliable. In contrast, dyadic\nconversations between individuals can not only capture how one expresses\noneself, but also reflect how one reacts to different situations. Rich\ncontextual information in dyadic conversation can explain an individual's\nresponse during his or her conversation. In this paper, we propose a novel\naugmented Gated Recurrent Unit (GRU) model for learning unsupervised Personal\nConversational Embeddings (PCE) based on dyadic conversations between\nindividuals. We adjust the formulation of each layer of a conventional GRU with\nsequence to sequence learning and personal information of both sides of the\nconversation. Based on the learned PCE, we can infer the personality of each\nindividual. We conduct experiments on the Movie Script dataset, which is\ncollected from conversations between characters in movie scripts. We find that\nmodeling dyadic conversations between individuals can significantly improve\npersonality inference accuracy. Experimental results illustrate the successful\nperformance of our proposed method.", "published": "2020-09-26 01:25:42", "link": "http://arxiv.org/abs/2009.12496v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Metaphor Detection using Deep Contextualized Word Embeddings", "abstract": "Metaphors are ubiquitous in natural language, and their detection plays an\nessential role in many natural language processing tasks, such as language\nunderstanding, sentiment analysis, etc. Most existing approaches for metaphor\ndetection rely on complex, hand-crafted and fine-tuned feature pipelines, which\ngreatly limit their applicability. In this work, we present an end-to-end\nmethod composed of deep contextualized word embeddings, bidirectional LSTMs and\nmulti-head attention mechanism to address the task of automatic metaphor\ndetection. Our method, unlike many other existing approaches, requires only the\nraw text sequences as input features to detect the metaphoricity of a phrase.\nWe compare the performance of our method against the existing baselines on two\nbenchmark datasets, TroFi, and MOH-X respectively. Experimental evaluations\nconfirm the effectiveness of our approach.", "published": "2020-09-26 11:00:35", "link": "http://arxiv.org/abs/2009.12565v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Arabic Dialect Identification Systems for Written Texts: A\n  Survey", "abstract": "Arabic dialect identification is a specific task of natural language\nprocessing, aiming to automatically predict the Arabic dialect of a given text.\nArabic dialect identification is the first step in various natural language\nprocessing applications such as machine translation, multilingual\ntext-to-speech synthesis, and cross-language text generation. Therefore, in the\nlast decade, interest has increased in addressing the problem of Arabic dialect\nidentification. In this paper, we present a comprehensive survey of Arabic\ndialect identification research in written texts. We first define the problem\nand its challenges. Then, the survey extensively discusses in a critical manner\nmany aspects related to Arabic dialect identification task. So, we review the\ntraditional machine learning methods, deep learning architectures, and complex\nlearning approaches to Arabic dialect identification. We also detail the\nfeatures and techniques for feature representations used to train the proposed\nsystems. Moreover, we illustrate the taxonomy of Arabic dialects studied in the\nliterature, the various levels of text processing at which Arabic dialect\nidentification are conducted (e.g., token, sentence, and document level), as\nwell as the available annotated resources, including evaluation benchmark\ncorpora. Open challenges and issues are discussed at the end of the survey.", "published": "2020-09-26 15:33:16", "link": "http://arxiv.org/abs/2009.12622v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense\n  Reasoning", "abstract": "Generative commonsense reasoning which aims to empower machines to generate\nsentences with the capacity of reasoning over a set of concepts is a critical\nbottleneck for text generation. Even the state-of-the-art pre-trained language\ngeneration models struggle at this task and often produce implausible and\nanomalous sentences. One reason is that they rarely consider incorporating the\nknowledge graph which can provide rich relational information among the\ncommonsense concepts. To promote the ability of commonsense reasoning for text\ngeneration, we propose a novel knowledge graph augmented pre-trained language\ngeneration model KG-BART, which encompasses the complex relations of concepts\nthrough the knowledge graph and produces more logical and natural sentences as\noutput. Moreover, KG-BART can leverage the graph attention to aggregate the\nrich concept semantics that enhances the model generalization on unseen concept\nsets. Experiments on benchmark CommonGen dataset verify the effectiveness of\nour proposed approach by comparing with several strong pre-trained language\ngeneration models, particularly KG-BART outperforms BART by 5.80, 4.60, in\nterms of BLEU-3, 4. Moreover, we also show that the generated context by our\nmodel can work as background scenarios to benefit downstream commonsense QA\ntasks.", "published": "2020-09-26 19:57:49", "link": "http://arxiv.org/abs/2009.12677v2", "categories": ["cs.CL", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Techniques to Improve Q&A Accuracy with Transformer-based models on\n  Large Complex Documents", "abstract": "This paper discusses the effectiveness of various text processing techniques,\ntheir combinations, and encodings to achieve a reduction of complexity and size\nin a given text corpus. The simplified text corpus is sent to BERT (or similar\ntransformer based models) for question and answering and can produce more\nrelevant responses to user queries. This paper takes a scientific approach to\ndetermine the benefits and effectiveness of various techniques and concludes a\nbest-fit combination that produces a statistically significant improvement in\naccuracy.", "published": "2020-09-26 21:56:22", "link": "http://arxiv.org/abs/2009.12695v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Abusive Language Detection and Characterization of Twitter Behavior", "abstract": "In this work, abusive language detection in online content is performed using\nBidirectional Recurrent Neural Network (BiRNN) method. Here the main objective\nis to focus on various forms of abusive behaviors on Twitter and to detect\nwhether a speech is abusive or not. The results are compared for various\nabusive behaviors in social media, with Convolutional Neural Netwrok (CNN) and\nRecurrent Neural Network (RNN) methods and proved that the proposed BiRNN is a\nbetter deep learning model for automatic abusive speech detection.", "published": "2020-09-26 07:38:11", "link": "http://arxiv.org/abs/2009.14261v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "QuatRE: Relation-Aware Quaternions for Knowledge Graph Embeddings", "abstract": "We propose a simple yet effective embedding model to learn quaternion\nembeddings for entities and relations in knowledge graphs. Our model aims to\nenhance correlations between head and tail entities given a relation within the\nQuaternion space with Hamilton product. The model achieves this goal by further\nassociating each relation with two relation-aware rotations, which are used to\nrotate quaternion embeddings of the head and tail entities, respectively.\nExperimental results show that our proposed model produces state-of-the-art\nperformances on well-known benchmark datasets for knowledge graph completion.\nOur code is available at: \\url{https://github.com/daiquocnguyen/QuatRE}.", "published": "2020-09-26 04:44:25", "link": "http://arxiv.org/abs/2009.12517v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clustering-based Unsupervised Generative Relation Extraction", "abstract": "This paper focuses on the problem of unsupervised relation extraction.\nExisting probabilistic generative model-based relation extraction methods work\nby extracting sentence features and using these features as inputs to train a\ngenerative model. This model is then used to cluster similar relations.\nHowever, these methods do not consider correlations between sentences with the\nsame entity pair during training, which can negatively impact model\nperformance. To address this issue, we propose a Clustering-based Unsupervised\ngenerative Relation Extraction (CURE) framework that leverages an\n\"Encoder-Decoder\" architecture to perform self-supervised learning so the\nencoder can extract relation information. Given multiple sentences with the\nsame entity pair as inputs, self-supervised learning is deployed by predicting\nthe shortest path between entity pairs on the dependency graph of one of the\nsentences. After that, we extract the relation information using the\nwell-trained encoder. Then, entity pairs that share the same relation are\nclustered based on their corresponding relation information. Each cluster is\nlabeled with a few words based on the words in the shortest paths corresponding\nto the entity pairs in each cluster. These cluster labels also describe the\nmeaning of these relation clusters. We compare the triplets extracted by our\nproposed framework (CURE) and baseline methods with a ground-truth Knowledge\nBase. Experimental results show that our model performs better than\nstate-of-the-art models on both New York Times (NYT) and United Nations\nParallel Corpus (UNPC) standard datasets.", "published": "2020-09-26 20:36:40", "link": "http://arxiv.org/abs/2009.12681v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning-based N-ary Cross-Sentence Relation Extraction", "abstract": "The models of n-ary cross sentence relation extraction based on distant\nsupervision assume that consecutive sentences mentioning n entities describe\nthe relation of these n entities. However, on one hand, this assumption\nintroduces noisy labeled data and harms the models' performance. On the other\nhand, some non-consecutive sentences also describe one relation and these\nsentences cannot be labeled under this assumption. In this paper, we relax this\nstrong assumption by a weaker distant supervision assumption to address the\nsecond issue and propose a novel sentence distribution estimator model to\naddress the first problem. This estimator selects correctly labeled sentences\nto alleviate the effect of noisy data is a two-level agent reinforcement\nlearning model. In addition, a novel universal relation extractor with a hybrid\napproach of attention mechanism and PCNN is proposed such that it can be\ndeployed in any tasks, including consecutive and nonconsecutive sentences.\nExperiments demonstrate that the proposed model can reduce the impact of noisy\ndata and achieve better performance on general n-ary cross sentence relation\nextraction task compared to baseline models.", "published": "2020-09-26 20:39:55", "link": "http://arxiv.org/abs/2009.12683v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Neural Proof Nets", "abstract": "Linear logic and the linear {\\lambda}-calculus have a long standing tradition\nin the study of natural language form and meaning. Among the proof calculi of\nlinear logic, proof nets are of particular interest, offering an attractive\ngeometric representation of derivations that is unburdened by the bureaucratic\ncomplications of conventional prooftheoretic formats. Building on recent\nadvances in set-theoretic learning, we propose a neural variant of proof nets\nbased on Sinkhorn networks, which allows us to translate parsing as the problem\nof extracting syntactic primitives and permuting them into alignment. Our\nmethodology induces a batch-efficient, end-to-end differentiable architecture\nthat actualizes a formally grounded yet highly efficient neuro-symbolic parser.\nWe test our approach on {\\AE}Thel, a dataset of type-logical derivations for\nwritten Dutch, where it manages to correctly transcribe raw text sentences into\nproofs and terms of the linear {\\lambda}-calculus with an accuracy of as high\nas 70%.", "published": "2020-09-26 22:48:47", "link": "http://arxiv.org/abs/2009.12702v1", "categories": ["cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
