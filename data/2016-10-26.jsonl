{"title": "Content Selection in Data-to-Text Systems: A Survey", "abstract": "Data-to-text systems are powerful in generating reports from data\nautomatically and thus they simplify the presentation of complex data. Rather\nthan presenting data using visualisation techniques, data-to-text systems use\nnatural (human) language, which is the most common way for human-human\ncommunication. In addition, data-to-text systems can adapt their output content\nto users' preferences, background or interests and therefore they can be\npleasant for users to interact with. Content selection is an important part of\nevery data-to-text system, because it is the module that determines which from\nthe available information should be conveyed to the user. This survey initially\nintroduces the field of data-to-text generation, describes the general\ndata-to-text system architecture and then it reviews the state-of-the-art\ncontent selection methods. Finally, it provides recommendations for choosing an\napproach and discusses opportunities for future research.", "published": "2016-10-26 15:20:47", "link": "http://arxiv.org/abs/1610.08375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Broad Context Language Modeling as Reading Comprehension", "abstract": "Progress in text understanding has been driven by large datasets that test\nparticular capabilities, like recent datasets for reading comprehension\n(Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al.,\n2016), a word prediction task requiring broader context than the immediate\nsentence. We view LAMBADA as a reading comprehension problem and apply\ncomprehension models based on neural networks. Though these models are\nconstrained to choose a word from the context, they improve the state of the\nart on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural\nnetwork readers perform well in cases that involve selecting a name from the\ncontext based on dialogue or discourse cues but struggle when coreference\nresolution or external knowledge is needed.", "published": "2016-10-26 17:25:38", "link": "http://arxiv.org/abs/1610.08431v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distraction-Based Neural Networks for Document Summarization", "abstract": "Distributed representation learned with neural networks has recently shown to\nbe effective in modeling natural languages at fine granularities such as words,\nphrases, and even sentences. Whether and how such an approach can be extended\nto help model larger spans of text, e.g., documents, is intriguing, and further\ninvestigation would still be desirable. This paper aims to enhance neural\nnetwork models for such a purpose. A typical problem of document-level modeling\nis automatic summarization, which aims to model documents in order to generate\nsummaries. In this paper, we propose neural models to train computers not just\nto pay attention to specific regions and content of input documents with\nattention models, but also distract them to traverse between different content\nof a document so as to better grasp the overall meaning for summarization.\nWithout engineering any features, we train the models on two large datasets.\nThe models achieve the state-of-the-art performance, and they significantly\nbenefit from the distraction modeling, particularly when input documents are\nlong.", "published": "2016-10-26 18:57:00", "link": "http://arxiv.org/abs/1610.08462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Based Biomedical Word Sense Disambiguation with Neural Concept\n  Embeddings", "abstract": "Biomedical word sense disambiguation (WSD) is an important intermediate task\nin many natural language processing applications such as named entity\nrecognition, syntactic parsing, and relation extraction. In this paper, we\nemploy knowledge-based approaches that also exploit recent advances in neural\nword/concept embeddings to improve over the state-of-the-art in biomedical WSD\nusing the MSH WSD dataset as the test set. Our methods involve weak supervision\n- we do not use any hand-labeled examples for WSD to build our prediction\nmodels; however, we employ an existing well known named entity recognition and\nconcept mapping program, MetaMap, to obtain our concept vectors. Over the MSH\nWSD dataset, our linear time (in terms of numbers of senses and words in the\ntest instance) method achieves an accuracy of 92.24% which is an absolute 3%\nimprovement over the best known results obtained via unsupervised or\nknowledge-based means. A more expensive approach that we developed relies on a\nnearest neighbor framework and achieves an accuracy of 94.34%. Employing dense\nvector representations learned from unlabeled free text has been shown to\nbenefit many language processing tasks recently and our efforts show that\nbiomedical WSD is no exception to this trend. For a complex and rapidly\nevolving domain such as biomedicine, building labeled datasets for larger sets\nof ambiguous terms may be impractical. Here, we show that weak supervision that\nleverages recent advances in representation learning can rival supervised\napproaches in biomedical WSD. However, external knowledge bases (here sense\ninventories) play a key role in the improvements achieved.", "published": "2016-10-26 21:49:15", "link": "http://arxiv.org/abs/1610.08557v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embeddings and Their Use In Sentence Classification Tasks", "abstract": "This paper have two parts. In the first part we discuss word embeddings. We\ndiscuss the need for them, some of the methods to create them, and some of\ntheir interesting properties. We also compare them to image embeddings and see\nhow word embedding and image embedding can be combined to perform different\ntasks. In the second part we implement a convolutional neural network trained\non top of pre-trained word vectors. The network is used for several\nsentence-level classification tasks, and achieves state-of-art (or comparable)\nresults, demonstrating the great power of pre-trainted word embeddings over\nrandom ones.", "published": "2016-10-26 08:48:10", "link": "http://arxiv.org/abs/1610.08229v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
