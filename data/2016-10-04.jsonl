{"title": "Chinese Event Extraction Using DeepNeural Network with Word Embedding", "abstract": "A lot of prior work on event extraction has exploited a variety of features\nto represent events. Such methods have several drawbacks: 1) the features are\noften specific for a particular domain and do not generalize well; 2) the\nfeatures are derived from various linguistic analyses and are error-prone; and\n3) some features may be expensive and require domain expert. In this paper, we\ndevelop a Chinese event extraction system that uses word embedding vectors to\nrepresent language, and deep neural networks to learn the abstract feature\nrepresentation in order to greatly reduce the effort of feature engineering. In\naddition, in this framework, we leverage large amount of unlabeled data, which\ncan address the problem of limited labeled corpus for this task. Our\nexperiments show that our proposed method performs better compared to the\nsystem using rich language features, and using unlabeled data benefits the word\nembeddings. This study suggests the potential of DNN and word embedding for the\nevent extraction task.", "published": "2016-10-04 04:56:23", "link": "http://arxiv.org/abs/1610.00842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Computational Approach to Automatic Prediction of Drunk Texting", "abstract": "Alcohol abuse may lead to unsociable behavior such as crime, drunk driving,\nor privacy leaks. We introduce automatic drunk-texting prediction as the task\nof identifying whether a text was written when under the influence of alcohol.\nWe experiment with tweets labeled using hashtags as distant supervision. Our\nclassifiers use a set of N-gram and stylistic features to detect drunk tweets.\nOur observations present the first quantitative evidence that text contains\nsignals that can be exploited to detect drunk-texting.", "published": "2016-10-04 07:34:24", "link": "http://arxiv.org/abs/1610.00879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Word Embedding-based Features Useful for Sarcasm Detection?", "abstract": "This paper makes a simple increment to state-of-the-art in sarcasm detection\nresearch. Existing approaches are unable to capture subtle forms of context\nincongruity which lies at the heart of sarcasm. We explore if prior work can be\nenhanced using semantic similarity/discordance between word embeddings. We\naugment word embedding-based features to four feature sets reported in the\npast. We also experiment with four types of word embeddings. We observe an\nimprovement in sarcasm detection, irrespective of the word embedding used or\nthe original feature set to which our features are augmented. For example, this\naugmentation results in an improvement in F-score of around 4\\% for three out\nof these four feature sets, and a minor degradation in case of the fourth, when\nWord2Vec embeddings are used. Finally, a comparison of the four embeddings\nshows that Word2Vec and dependency weight-based features outperform LSA and\nGloVe, in terms of their benefit to sarcasm detection.", "published": "2016-10-04 07:50:06", "link": "http://arxiv.org/abs/1610.00883v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30\n  Translation Directions", "abstract": "In this paper we provide the largest published comparison of translation\nquality for phrase-based SMT and neural machine translation across 30\ntranslation directions. For ten directions we also include hierarchical\nphrase-based MT. Experiments are performed for the recently published United\nNations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus.\nIn the second part of the paper we investigate aspects of translation speed,\nintroducing AmuNMT, our efficient neural machine translation decoder. We\ndemonstrate that current neural machine translation could already be used for\nin-production systems when comparing words-per-second ratios.", "published": "2016-10-04 17:52:42", "link": "http://arxiv.org/abs/1610.01108v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Maximum Entropy Classification and Linear Regression for Author\n  Age Prediction", "abstract": "The evolution of the internet has created an abundance of unstructured data\non the web, a significant part of which is textual. The task of author\nprofiling seeks to find the demographics of people solely from their linguistic\nand content-based features in text. The ability to describe traits of authors\nclearly has applications in fields such as security and forensics, as well as\nmarketing. Instead of seeing age as just a classification problem, we also\nframe age as a regression one, but use an ensemble chain method that\nincorporates the power of both classification and regression to learn the\nauthors exact age.", "published": "2016-10-04 06:01:03", "link": "http://arxiv.org/abs/1610.00852v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Embracing data abundance: BookTest Dataset for Reading Comprehension", "abstract": "There is a practically unlimited amount of natural language data available.\nStill, recent work in text comprehension has focused on datasets which are\nsmall relative to current computing possibilities. This article is making a\ncase for the community to move to larger data and as a step in that direction\nit is proposing the BookTest, a new dataset similar to the popular Children's\nBook Test (CBT), however more than 60 times larger. We show that training on\nthe new data improves the accuracy of our Attention-Sum Reader model on the\noriginal CBT test data by a much larger margin than many recent attempts to\nimprove the model architecture. On one version of the dataset our ensemble even\nexceeds the human baseline provided by Facebook. We then show in our own human\nstudy that there is still space for further improvement.", "published": "2016-10-04 12:48:51", "link": "http://arxiv.org/abs/1610.00956v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Applications of Online Deep Learning for Crisis Response Using Social\n  Media Information", "abstract": "During natural or man-made disasters, humanitarian response organizations\nlook for useful information to support their decision-making processes. Social\nmedia platforms such as Twitter have been considered as a vital source of\nuseful information for disaster response and management. Despite advances in\nnatural language processing techniques, processing short and informal Twitter\nmessages is a challenging task. In this paper, we propose to use Deep Neural\nNetwork (DNN) to address two types of information needs of response\norganizations: 1) identifying informative tweets and 2) classifying them into\ntopical classes. DNNs use distributed representation of words and learn the\nrepresentation as well as higher level features automatically for the\nclassification task. We propose a new online algorithm based on stochastic\ngradient descent to train DNNs in an online fashion during disaster situations.\nWe test our models using a crisis-related real-world Twitter dataset.", "published": "2016-10-04 14:53:51", "link": "http://arxiv.org/abs/1610.01030v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tutorial on Answering Questions about Images with Deep Learning", "abstract": "Together with the development of more accurate methods in Computer Vision and\nNatural Language Understanding, holistic architectures that answer on questions\nabout the content of real-world images have emerged. In this tutorial, we build\na neural-based approach to answer questions about images. We base our tutorial\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\nmodels that we present here can achieve a competitive performance on both\ndatasets, in fact, they are among the best methods that use a combination of\nLSTM with a global, full frame CNN representation of an image. We hope that\nafter reading this tutorial, the reader will be able to use Deep Learning\nframeworks, such as Keras and introduced Kraino, to build various architectures\nthat will lead to a further performance improvement on this challenging task.", "published": "2016-10-04 16:29:28", "link": "http://arxiv.org/abs/1610.01076v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CV"}
