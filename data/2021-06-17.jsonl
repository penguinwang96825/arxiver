{"title": "An Empirical Study on Hyperparameter Optimization for Fine-Tuning\n  Pre-trained Language Models", "abstract": "The performance of fine-tuning pre-trained language models largely depends on\nthe hyperparameter configuration. In this paper, we investigate the performance\nof modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained\nlanguage models. First, we study and report three HPO algorithms' performances\non fine-tuning two state-of-the-art language models on the GLUE dataset. We\nfind that using the same time budget, HPO often fails to outperform grid search\ndue to two reasons: insufficient time budget and overfitting. We propose two\ngeneral strategies and an experimental procedure to systematically troubleshoot\nHPO's failure cases. By applying the procedure, we observe that HPO can succeed\nwith more appropriate settings in the search space and time budget; however, in\ncertain cases overfitting remains. Finally, we make suggestions for future\nwork. Our implementation can be found in\nhttps://github.com/microsoft/FLAML/tree/main/flaml/nlp/.", "published": "2021-06-17 01:58:32", "link": "http://arxiv.org/abs/2106.09204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end\n  Event Extraction", "abstract": "Event extraction is challenging due to the complex structure of event records\nand the semantic gap between text and event. Traditional methods usually\nextract event records by decomposing the complex structure prediction task into\nmultiple subtasks. In this paper, we propose Text2Event, a\nsequence-to-structure generation paradigm that can directly extract events from\nthe text in an end-to-end manner. Specifically, we design a\nsequence-to-structure network for unified event extraction, a constrained\ndecoding algorithm for event knowledge injection during inference, and a\ncurriculum learning algorithm for efficient model learning. Experimental\nresults show that, by uniformly modeling all tasks in a single model and\nuniversally predicting different labels, our method can achieve competitive\nperformance using only record-level annotations in both supervised learning and\ntransfer learning settings.", "published": "2021-06-17 04:00:18", "link": "http://arxiv.org/abs/2106.09232v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "De-biasing Distantly Supervised Named Entity Recognition via Causal\n  Intervention", "abstract": "Distant supervision tackles the data bottleneck in NER by automatically\ngenerating training instances via dictionary matching. Unfortunately, the\nlearning of DS-NER is severely dictionary-biased, which suffers from spurious\ncorrelations and therefore undermines the effectiveness and the robustness of\nthe learned models. In this paper, we fundamentally explain the dictionary bias\nvia a Structural Causal Model (SCM), categorize the bias into intra-dictionary\nand inter-dictionary biases, and identify their causes. Based on the SCM, we\nlearn de-biased DS-NER via causal interventions. For intra-dictionary bias, we\nconduct backdoor adjustment to remove the spurious correlations introduced by\nthe dictionary confounder. For inter-dictionary bias, we propose a causal\ninvariance regularizer which will make DS-NER models more robust to the\nperturbation of dictionaries. Experiments on four datasets and three DS-NER\nmodels show that our method can significantly improve the performance of\nDS-NER.", "published": "2021-06-17 04:01:02", "link": "http://arxiv.org/abs/2106.09233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Denoising Distantly Supervised Named Entity Recognition via a\n  Hypergeometric Probabilistic Model", "abstract": "Denoising is the essential step for distant supervision based named entity\nrecognition. Previous denoising methods are mostly based on instance-level\nconfidence statistics, which ignore the variety of the underlying noise\ndistribution on different datasets and entity types. This makes them difficult\nto be adapted to high noise rate settings. In this paper, we propose\nHypergeometric Learning (HGL), a denoising algorithm for distantly supervised\nNER that takes both noise distribution and instance-level confidence into\nconsideration. Specifically, during neural network training, we naturally model\nthe noise samples in each batch following a hypergeometric distribution\nparameterized by the noise-rate. Then each instance in the batch is regarded as\neither correct or noisy one according to its label confidence derived from\nprevious training step, as well as the noise distribution in this sampled\nbatch. Experiments show that HGL can effectively denoise the weakly-labeled\ndata retrieved from distant supervision, and therefore results in significant\nimprovements on the trained models.", "published": "2021-06-17 04:01:25", "link": "http://arxiv.org/abs/2106.09234v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-FACT: A New Benchmark Dataset for Multilingual Fact Checking", "abstract": "In this work, we introduce X-FACT: the largest publicly available\nmultilingual dataset for factual verification of naturally existing real-world\nclaims. The dataset contains short statements in 25 languages and is labeled\nfor veracity by expert fact-checkers. The dataset includes a multilingual\nevaluation benchmark that measures both out-of-domain generalization, and\nzero-shot capabilities of the multilingual models. Using state-of-the-art\nmultilingual transformer-based models, we develop several automated\nfact-checking models that, along with textual claims, make use of additional\nmetadata and evidence from news stories retrieved using a search engine.\nEmpirically, our best model attains an F-score of around 40%, suggesting that\nour dataset is a challenging benchmark for evaluation of multilingual\nfact-checking models.", "published": "2021-06-17 05:09:54", "link": "http://arxiv.org/abs/2106.09248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Interpreting: Speech Translation from Source or Interpreter?", "abstract": "Interpreters facilitate multi-lingual meetings but the affordable set of\nlanguages is often smaller than what is needed. Automatic simultaneous speech\ntranslation can extend the set of provided languages. We investigate if such an\nautomatic system should rather follow the original speaker, or an interpreter\nto achieve better translation quality at the cost of increased delay.\n  To answer the question, we release Europarl Simultaneous Interpreting Corpus\n(ESIC), 10 hours of recordings and transcripts of European Parliament speeches\nin English, with simultaneous interpreting into Czech and German. We evaluate\nquality and latency of speaker-based and interpreter-based spoken translation\nsystems from English to Czech. We study the differences in implicit\nsimplification and summarization of the human interpreter compared to a machine\ntranslation system trained to shorten the output to some extent. Finally, we\nperform human evaluation to measure information loss of each of these\napproaches.", "published": "2021-06-17 09:32:49", "link": "http://arxiv.org/abs/2106.09343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocNLI: A Large-scale Dataset for Document-level Natural Language\n  Inference", "abstract": "Natural language inference (NLI) is formulated as a unified framework for\nsolving various NLP problems such as relation extraction, question answering,\nsummarization, etc. It has been studied intensively in the past few years\nthanks to the availability of large-scale labeled datasets. However, most\nexisting studies focus on merely sentence-level inference, which limits the\nscope of NLI's application in downstream NLP problems. This work presents\nDocNLI -- a newly-constructed large-scale dataset for document-level NLI.\nDocNLI is transformed from a broad range of NLP problems and covers multiple\ngenres of text. The premises always stay in the document granularity, whereas\nthe hypotheses vary in length from single sentences to passages with hundreds\nof words. Additionally, DocNLI has pretty limited artifacts which unfortunately\nwidely exist in some popular sentence-level NLI datasets. Our experiments\ndemonstrate that, even without fine-tuning, a model pretrained on DocNLI shows\npromising performance on popular sentence-level benchmarks, and generalizes\nwell to out-of-domain NLP tasks that rely on inference at document granularity.\nTask-specific fine-tuning can bring further improvements. Data, code, and\npretrained models can be found at https://github.com/salesforce/DocNLI.", "published": "2021-06-17 13:02:26", "link": "http://arxiv.org/abs/2106.09449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DravidianCodeMix: Sentiment Analysis and Offensive Language\n  Identification Dataset for Dravidian Languages in Code-Mixed Text", "abstract": "This paper describes the development of a multilingual, manually annotated\ndataset for three under-resourced Dravidian languages generated from social\nmedia comments. The dataset was annotated for sentiment analysis and offensive\nlanguage identification for a total of more than 60,000 YouTube comments. The\ndataset consists of around 44,000 comments in Tamil-English, around 7,000\ncomments in Kannada-English, and around 20,000 comments in Malayalam-English.\nThe data was manually annotated by volunteer annotators and has a high\ninter-annotator agreement in Krippendorff's alpha. The dataset contains all\ntypes of code-mixing phenomena since it comprises user-generated content from a\nmultilingual country. We also present baseline experiments to establish\nbenchmarks on the dataset using machine learning methods. The dataset is\navailable on Github\n(https://github.com/bharathichezhiyan/DravidianCodeMix-Dataset) and Zenodo\n(https://zenodo.org/record/4750858\\#.YJtw0SYo\\_0M).", "published": "2021-06-17 13:13:26", "link": "http://arxiv.org/abs/2106.09460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks", "abstract": "In recent years, the extraction of opinions and information from\nuser-generated text has attracted a lot of interest, largely due to the\nunprecedented volume of content in Social Media. However, social researchers\nface some issues in adopting cutting-edge tools for these tasks, as they are\nusually behind commercial APIs, unavailable for other languages than English,\nor very complex to use for non-experts. To address these issues, we present\npysentimiento, a comprehensive multilingual Python toolkit designed for opinion\nmining and other Social NLP tasks. This open-source library brings\nstate-of-the-art models for Spanish, English, Italian, and Portuguese in an\neasy-to-use Python library, allowing researchers to leverage these techniques.\nWe present a comprehensive assessment of performance for several pre-trained\nlanguage models across a variety of tasks, languages, and datasets, including\nan evaluation of fairness in the results.", "published": "2021-06-17 13:15:07", "link": "http://arxiv.org/abs/2106.09462v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Element Intervention for Open Relation Extraction", "abstract": "Open relation extraction aims to cluster relation instances referring to the\nsame underlying relation, which is a critical step for general relation\nextraction. Current OpenRE models are commonly trained on the datasets\ngenerated from distant supervision, which often results in instability and\nmakes the model easily collapsed. In this paper, we revisit the procedure of\nOpenRE from a causal view. By formulating OpenRE using a structural causal\nmodel, we identify that the above-mentioned problems stem from the spurious\ncorrelations from entities and context to the relation type. To address this\nissue, we conduct \\emph{Element Intervention}, which intervenes on the context\nand entities respectively to obtain the underlying causal effects of them. We\nalso provide two specific implementations of the interventions based on entity\nranking and context contrasting. Experimental results on unsupervised relation\nextraction datasets show that our methods outperform previous state-of-the-art\nmethods and are robust across different datasets.", "published": "2021-06-17 14:37:13", "link": "http://arxiv.org/abs/2106.09558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Cross-Domain Text-to-SQL Semantic Parsing with Auxiliary Task", "abstract": "In this work, we focus on two crucial components in the cross-domain\ntext-to-SQL semantic parsing task: schema linking and value filling. To\nencourage the model to learn better encoding ability, we propose a column\nselection auxiliary task to empower the encoder with the relevance matching\ncapability by using explicit learning targets. Furthermore, we propose two\nvalue filling methods to build the bridge from the existing zero-shot semantic\nparsers to real-world applications, considering most of the existing parsers\nignore the values filling in the synthesized SQL. With experiments on Spider,\nour proposed framework improves over the baselines on the execution accuracy\nand exact set match accuracy when database contents are unavailable, and\ndetailed analysis sheds light on future work.", "published": "2021-06-17 15:15:58", "link": "http://arxiv.org/abs/2106.09588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying vaccine sentiment tweets by modelling domain-specific\n  representation and commonsense knowledge into context-aware attentive GRU", "abstract": "Vaccines are an important public health measure, but vaccine hesitancy and\nrefusal can create clusters of low vaccine coverage and reduce the\neffectiveness of vaccination programs. Social media provides an opportunity to\nestimate emerging risks to vaccine acceptance by including geographical\nlocation and detailing vaccine-related concerns. Methods for classifying social\nmedia posts, such as vaccine-related tweets, use language models (LMs) trained\non general domain text. However, challenges to measuring vaccine sentiment at\nscale arise from the absence of tonal stress and gestural cues and may not\nalways have additional information about the user, e.g., past tweets or social\nconnections. Another challenge in LMs is the lack of commonsense knowledge that\nare apparent in users metadata, i.e., emoticons, positive and negative words\netc. In this study, to classify vaccine sentiment tweets with limited\ninformation, we present a novel end-to-end framework consisting of\ninterconnected components that use domain-specific LM trained on\nvaccine-related tweets and models commonsense knowledge into a bidirectional\ngated recurrent network (CK-BiGRU) with context-aware attention. We further\nleverage syntactical, user metadata and sentiment information to capture the\nsentiment of a tweet. We experimented using two popular vaccine-related Twitter\ndatasets and demonstrate that our proposed approach outperforms\nstate-of-the-art models in identifying pro-vaccine, anti-vaccine and neutral\ntweets.", "published": "2021-06-17 15:16:08", "link": "http://arxiv.org/abs/2106.09589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge\n  Bases", "abstract": "Previous literatures show that pre-trained masked language models (MLMs) such\nas BERT can achieve competitive factual knowledge extraction performance on\nsome datasets, indicating that MLMs can potentially be a reliable knowledge\nsource. In this paper, we conduct a rigorous study to explore the underlying\npredicting mechanisms of MLMs over different extraction paradigms. By\ninvestigating the behaviors of MLMs, we find that previous decent performance\nmainly owes to the biased prompts which overfit dataset artifacts. Furthermore,\nincorporating illustrative cases and external contexts improve knowledge\nprediction mainly due to entity type guidance and golden answer leakage. Our\nfindings shed light on the underlying predicting mechanisms of MLMs, and\nstrongly question the previous conclusion that current MLMs can potentially\nserve as reliable factual knowledge bases.", "published": "2021-06-17 03:59:45", "link": "http://arxiv.org/abs/2106.09231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Biomedical Interpretable Entity Representations", "abstract": "Pre-trained language models induce dense entity representations that offer\nstrong performance on entity-centric NLP tasks, but such representations are\nnot immediately interpretable. This can be a barrier to model uptake in\nimportant domains such as biomedicine. There has been recent work on general\ninterpretable representation learning (Onoe and Durrett, 2020), but these\ndomain-agnostic representations do not readily transfer to the important domain\nof biomedicine. In this paper, we create a new entity type system and training\nset from a large corpus of biomedical texts by mapping entities to concepts in\na medical ontology, and from these to Wikipedia pages whose categories are our\ntypes. From this mapping we derive Biomedical Interpretable Entity\nRepresentations(BIERs), in which dimensions correspond to fine-grained entity\ntypes, and values are predicted probabilities that a given entity is of the\ncorresponding type. We propose a novel method that exploits BIER's final sparse\nand intermediate dense representations to facilitate model and entity type\ndebugging. We show that BIERs achieve strong performance in biomedical tasks\nincluding named entity disambiguation and entity label classification, and we\nprovide error analysis to highlight the utility of their interpretability,\nparticularly in low-supervision settings. Finally, we provide our induced 68K\nbiomedical type system, the corresponding 37 million triples of derived data\nused to train BIER models and our best performing model.", "published": "2021-06-17 13:52:10", "link": "http://arxiv.org/abs/2106.09502v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Worlds in Text", "abstract": "We provide a dataset that enables the creation of learning agents that can\nbuild knowledge graph-based world models of interactive narratives. Interactive\nnarratives -- or text-adventure games -- are partially observable environments\nstructured as long puzzles or quests in which an agent perceives and interacts\nwith the world purely through textual natural language. Each individual game\ntypically contains hundreds of locations, characters, and objects -- each with\ntheir own unique descriptions -- providing an opportunity to study the problem\nof giving language-based agents the structured memory necessary to operate in\nsuch worlds. Our dataset provides 24198 mappings between rich natural language\nobservations and: (1) knowledge graphs that reflect the world state in the form\nof a map; (2) natural language actions that are guaranteed to cause a change in\nthat particular world state. The training data is collected across 27 games in\nmultiple genres and contains a further 7836 heldout instances over 9 additional\ngames in the test set. We further provide baseline models using rules-based,\nquestion-answering, and sequence learning approaches in addition to an analysis\nof the data and corresponding learning tasks.", "published": "2021-06-17 15:02:16", "link": "http://arxiv.org/abs/2106.09578v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-head or Single-head? An Empirical Comparison for Transformer\n  Training", "abstract": "Multi-head attention plays a crucial role in the recent success of\nTransformer models, which leads to consistent performance improvements over\nconventional attention in various applications. The popular belief is that this\neffectiveness stems from the ability of jointly attending multiple positions.\nIn this paper, we first demonstrate that jointly attending multiple positions\nis not a unique feature of multi-head attention, as multi-layer single-head\nattention also attends multiple positions and is more effective. Then, we\nsuggest the main advantage of the multi-head attention is the training\nstability, since it has less number of layers than the single-head attention,\nwhen attending the same number of positions. For example, 24-layer 16-head\nTransformer (BERT-large) and 384-layer single-head Transformer has the same\ntotal attention head number and roughly the same model size, while the\nmulti-head one is significantly shallower. Meanwhile, we show that, with recent\nadvances in deep learning, we can successfully stabilize the training of the\n384-layer Transformer. As the training difficulty is no longer a bottleneck,\nsubstantially deeper single-head Transformer achieves consistent performance\nimprovements without tuning hyper-parameters.", "published": "2021-06-17 16:53:22", "link": "http://arxiv.org/abs/2106.09650v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scientific Language Models for Biomedical Knowledge Base Completion: An\n  Empirical Study", "abstract": "Biomedical knowledge graphs (KGs) hold rich information on entities such as\ndiseases, drugs, and genes. Predicting missing links in these graphs can boost\nmany important applications, such as drug design and repurposing. Recent work\nhas shown that general-domain language models (LMs) can serve as \"soft\" KGs,\nand that they can be fine-tuned for the task of KG completion. In this work, we\nstudy scientific LMs for KG completion, exploring whether we can tap into their\nlatent knowledge to enhance biomedical link prediction. We evaluate several\ndomain-specific LMs, fine-tuning them on datasets centered on drugs and\ndiseases that we represent as KGs and enrich with textual entity descriptions.\nWe integrate the LM-based models with KG embedding models, using a router\nmethod that learns to assign each input example to either type of model and\nprovides a substantial boost in performance. Finally, we demonstrate the\nadvantage of LM models in the inductive setting with novel scientific entities.\nOur datasets and code are made publicly available.", "published": "2021-06-17 17:55:33", "link": "http://arxiv.org/abs/2106.09700v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Information Retrieval Approach to Building Datasets for Hate Speech\n  Detection", "abstract": "Building a benchmark dataset for hate speech detection presents various\nchallenges. Firstly, because hate speech is relatively rare, random sampling of\ntweets to annotate is very inefficient in finding hate speech. To address this,\nprior datasets often include only tweets matching known \"hate words\". However,\nrestricting data to a pre-defined vocabulary may exclude portions of the\nreal-world phenomenon we seek to model. A second challenge is that definitions\nof hate speech tend to be highly varying and subjective. Annotators having\ndiverse prior notions of hate speech may not only disagree with one another but\nalso struggle to conform to specified labeling guidelines. Our key insight is\nthat the rarity and subjectivity of hate speech are akin to that of relevance\nin information retrieval (IR). This connection suggests that well-established\nmethodologies for creating IR test collections can be usefully applied to\ncreate better benchmark datasets for hate speech. To intelligently and\nefficiently select which tweets to annotate, we apply standard IR techniques of\n{\\em pooling} and {\\em active learning}. To improve both consistency and value\nof annotations, we apply {\\em task decomposition} and {\\em annotator rationale}\ntechniques. We share a new benchmark dataset for hate speech detection on\nTwitter that provides broader coverage of hate than prior datasets. We also\nshow a dramatic drop in accuracy of existing detection models when tested on\nthese broader forms of hate. Annotator rationales we collect not only justify\nlabeling decisions but also enable future work opportunities for\ndual-supervision and/or explanation generation in modeling. Further details of\nour approach can be found in the supplementary materials.", "published": "2021-06-17 19:25:39", "link": "http://arxiv.org/abs/2106.09775v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause\n  Extraction", "abstract": "Detecting what emotions are expressed in text is a well-studied problem in\nnatural language processing. However, research on finer grained emotion\nanalysis such as what causes an emotion is still in its infancy. We present\nsolutions that tackle both emotion recognition and emotion cause detection in a\njoint fashion. Considering that common-sense knowledge plays an important role\nin understanding implicitly expressed emotions and the reasons for those\nemotions, we propose novel methods that combine common-sense knowledge via\nadapted knowledge models with multi-task learning to perform joint emotion\nclassification and emotion cause tagging. We show performance improvement on\nboth tasks when including common-sense reasoning and a multitask framework. We\nprovide a thorough analysis to gain insights into model performance.", "published": "2021-06-17 20:11:04", "link": "http://arxiv.org/abs/2106.09790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Layer Pruning on Demand with Intermediate CTC", "abstract": "Deploying an end-to-end automatic speech recognition (ASR) model on\nmobile/embedded devices is a challenging task, since the device computational\npower and energy consumption requirements are dynamically changed in practice.\nTo overcome the issue, we present a training and pruning method for ASR based\non the connectionist temporal classification (CTC) which allows reduction of\nmodel depth at run-time without any extra fine-tuning. To achieve the goal, we\nadopt two regularization methods, intermediate CTC and stochastic depth, to\ntrain a model whose performance does not degrade much after pruning. We present\nan in-depth analysis of layer behaviors using singular vector canonical\ncorrelation analysis (SVCCA), and efficient strategies for finding layers which\nare safe to prune. Using the proposed method, we show that a Transformer-CTC\nmodel can be pruned in various depth on demand, improving real-time factor from\n0.005 to 0.002 on GPU, while each pruned sub-model maintains the accuracy of\nindividually trained model of the same depth.", "published": "2021-06-17 02:40:18", "link": "http://arxiv.org/abs/2106.09216v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional\n  Text-to-Speech Model", "abstract": "Recently, there has been an increasing interest in neural speech synthesis.\nWhile the deep neural network achieves the state-of-the-art result in\ntext-to-speech (TTS) tasks, how to generate a more emotional and more\nexpressive speech is becoming a new challenge to researchers due to the\nscarcity of high-quality emotion speech dataset and the lack of advanced\nemotional TTS model. In this paper, we first briefly introduce and publicly\nrelease a Mandarin emotion speech dataset including 9,724 samples with audio\nfiles and its emotion human-labeled annotation. After that, we propose a simple\nbut efficient architecture for emotional speech synthesis called EMSpeech.\nUnlike those models which need additional reference audio as input, our model\ncould predict emotion labels just from the input text and generate more\nexpressive speech conditioned on the emotion embedding. In the experiment\nphase, we first validate the effectiveness of our dataset by an emotion\nclassification task. Then we train our model on the proposed dataset and\nconduct a series of subjective evaluations. Finally, by showing a comparable\nperformance in the emotional speech synthesis task, we successfully demonstrate\nthe ability of the proposed model.", "published": "2021-06-17 08:34:21", "link": "http://arxiv.org/abs/2106.09317v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Self-supervised Method for Entity Alignment", "abstract": "Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing large-scale\nKGs. Over the course of its development, supervision has been considered\nnecessary for accurate alignments. Inspired by the recent progress of\nself-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Existing supervised methods for this task\nfocus on pulling each pair of positive (labeled) entities close to each other.\nHowever, our analysis suggests that the learning of entity alignment can\nactually benefit more from pushing sampled (unlabeled) negatives far away than\npulling positive aligned pairs close. We present SelfKG by leveraging this\ndiscovery to design a contrastive learning strategy across two KGs. Extensive\nexperiments on benchmark datasets demonstrate that SelfKG without supervision\ncan match or achieve comparable results with state-of-the-art supervised\nbaselines. The performance of SelfKG demonstrates self-supervised learning\noffers great potential for entity alignment in KGs.", "published": "2021-06-17 11:22:20", "link": "http://arxiv.org/abs/2106.09395v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large-Scale Chemical Language Representations Capture Molecular\n  Structure and Properties", "abstract": "Models based on machine learning can enable accurate and fast molecular\nproperty predictions, which is of interest in drug discovery and material\ndesign. Various supervised machine learning models have demonstrated promising\nperformance, but the vast chemical space and the limited availability of\nproperty labels make supervised learning challenging. Recently, unsupervised\ntransformer-based language models pretrained on a large unlabelled corpus have\nproduced state-of-the-art results in many downstream natural language\nprocessing tasks. Inspired by this development, we present molecular embeddings\nobtained by training an efficient transformer encoder model, MoLFormer, which\nuses rotary positional embeddings. This model employs a linear attention\nmechanism, coupled with highly distributed training, on SMILES sequences of 1.1\nbillion unlabelled molecules from the PubChem and ZINC datasets. We show that\nthe learned molecular representation outperforms existing baselines, including\nsupervised and self-supervised graph neural networks and language models, on\nseveral downstream tasks from ten benchmark datasets. They perform\ncompetitively on two others. Further analyses, specifically through the lens of\nattention, demonstrate that MoLFormer trained on chemical SMILES indeed learns\nthe spatial relationships between atoms within a molecule. These results\nprovide encouraging evidence that large-scale molecular language models can\ncapture sufficient chemical and structural information to predict various\ndistinct molecular properties, including quantum-chemical properties.", "published": "2021-06-17 14:33:55", "link": "http://arxiv.org/abs/2106.09553v3", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Learning Knowledge Graph-based World Models of Textual Environments", "abstract": "World models improve a learning agent's ability to efficiently operate in\ninteractive and situated environments. This work focuses on the task of\nbuilding world models of text-based game environments. Text-based games, or\ninteractive narratives, are reinforcement learning environments in which agents\nperceive and interact with the world using textual natural language. These\nenvironments contain long, multi-step puzzles or quests woven through a world\nthat is filled with hundreds of characters, locations, and objects. Our world\nmodel learns to simultaneously: (1) predict changes in the world caused by an\nagent's actions when representing the world as a knowledge graph; and (2)\ngenerate the set of contextually relevant natural language actions required to\noperate in the world. We frame this task as a Set of Sequences generation\nproblem by exploiting the inherent structure of knowledge graphs and actions\nand introduce both a transformer-based multi-task architecture and a loss\nfunction to train it. A zero-shot ablation study on never-before-seen textual\nworlds shows that our methodology significantly outperforms existing textual\nworld modeling techniques as well as the importance of each of our\ncontributions.", "published": "2021-06-17 15:45:54", "link": "http://arxiv.org/abs/2106.09608v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LoRA: Low-Rank Adaptation of Large Language Models", "abstract": "An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.", "published": "2021-06-17 17:37:18", "link": "http://arxiv.org/abs/2106.09685v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-mode Transformer Transducer with Stochastic Future Context", "abstract": "Automatic speech recognition (ASR) models make fewer errors when more\nsurrounding speech information is presented as context. Unfortunately,\nacquiring a larger future context leads to higher latency. There exists an\ninevitable trade-off between speed and accuracy. Naively, to fit different\nlatency requirements, people have to store multiple models and pick the best\none under the constraints. Instead, a more desirable approach is to have a\nsingle model that can dynamically adjust its latency based on different\nconstraints, which we refer to as Multi-mode ASR. A Multi-mode ASR model can\nfulfill various latency requirements during inference -- when a larger latency\nbecomes acceptable, the model can process longer future context to achieve\nhigher accuracy and when a latency budget is not flexible, the model can be\nless dependent on future context but still achieve reliable accuracy. In\npursuit of Multi-mode ASR, we propose Stochastic Future Context, a simple\ntraining procedure that samples one streaming configuration in each iteration.\nThrough extensive experiments on AISHELL-1 and LibriSpeech datasets, we show\nthat a Multi-mode ASR model rivals, if not surpasses, a set of competitive\nstreaming baselines trained with different latency budgets.", "published": "2021-06-17 18:42:11", "link": "http://arxiv.org/abs/2106.09760v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking", "abstract": "Entity linking (EL), the task of disambiguating mentions in text by linking\nthem to entities in a knowledge graph, is crucial for text understanding,\nquestion answering or conversational systems. Entity linking on short text\n(e.g., single sentence or question) poses particular challenges due to limited\ncontext. While prior approaches use either heuristics or black-box neural\nmethods, here we propose LNN-EL, a neuro-symbolic approach that combines the\nadvantages of using interpretable rules based on first-order logic with the\nperformance of neural learning. Even though constrained to using rules, LNN-EL\nperforms competitively against SotA black-box neural approaches, with the added\nbenefits of extensibility and transferability. In particular, we show that we\ncan easily blend existing rule templates given by a human expert, with multiple\ntypes of features (priors, BERT encodings, box embeddings, etc), and even\nscores resulting from previous EL methods, thus improving on such methods. For\ninstance, on the LC-QuAD-1.0 dataset, we show more than $4$\\% increase in F1\nscore over previous SotA. Finally, we show that the inductive bias offered by\nusing logic results in learned rules that transfer well across datasets, even\nwithout fine tuning, while maintaining high accuracy.", "published": "2021-06-17 20:22:45", "link": "http://arxiv.org/abs/2106.09795v1", "categories": ["cs.CL", "cs.AI", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Efficient Conformer with Prob-Sparse Attention Mechanism for\n  End-to-EndSpeech Recognition", "abstract": "End-to-end models are favored in automatic speech recognition (ASR) because\nof their simplified system structure and superior performance. Among these\nmodels, Transformer and Conformer have achieved state-of-the-art recognition\naccuracy in which self-attention plays a vital role in capturing important\nglobal information. However, the time and memory complexity of self-attention\nincreases squarely with the length of the sentence. In this paper, a\nprob-sparse self-attention mechanism is introduced into Conformer to sparse the\ncomputing process of self-attention in order to accelerate inference speed and\nreduce space consumption. Specifically, we adopt a Kullback-Leibler divergence\nbased sparsity measurement for each query to decide whether we compute the\nattention function on this query. By using the prob-sparse attention mechanism,\nwe achieve impressively 8% to 45% inference speed-up and 15% to 45% memory\nusage reduction of the self-attention module of Conformer Transducer while\nmaintaining the same level of error rate.", "published": "2021-06-17 04:04:04", "link": "http://arxiv.org/abs/2106.09236v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Level Transfer Learning from Near-Field to Far-Field Speaker\n  Verification", "abstract": "In far-field speaker verification, the performance of speaker embeddings is\nsusceptible to degradation when there is a mismatch between the conditions of\nenrollment and test speech. To solve this problem, we propose the feature-level\nand instance-level transfer learning in the teacher-student framework to learn\na domain-invariant embedding space. For the feature-level knowledge transfer,\nwe develop the contrastive loss to transfer knowledge from teacher model to\nstudent model, which can not only decrease the intra-class distance, but also\nenlarge the inter-class distance. Moreover, we propose the instance-level\npairwise distance transfer method to force the student model to preserve\npairwise instances distance from the well optimized embedding space of the\nteacher model. On FFSVC 2020 evaluation set, our EER on Full-eval trials is\nrelatively reduced by 13.9% compared with the fusion system result on\nPartial-eval trials of Task2. On Task1, compared with the winner's DenseNet\nresult on Partial-eval trials, our minDCF on Full-eval trials is relatively\nreduced by 6.3%. On Task3, the EER and minDCF of our proposed method on\nFull-eval trials are very close to the result of the fusion system on\nPartial-eval trials. Our results also outperform other competitive domain\nadaptation methods.", "published": "2021-06-17 08:37:29", "link": "http://arxiv.org/abs/2106.09320v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Localization based on enhanced low frequency interaural level difference", "abstract": "The processing of low-frequency interaural time differences is found to be\nproblematic among hearing-impaired people. The current generation of\nbeamformers does not consider this deficiency. In an attempt to tackle this\nissue, we propose to replace the inaudible interaural time differences in the\nlow-frequency region with the interaural level differences. In addition, a\nbeamformer is introduced and analyzed, which enhances the low-frequency\ninteraural level differences of the sound sources using a near-field\ntransformation. The proposed beamforming problem is relaxed to a convex problem\nusing semi-definite relaxation. The instrumental analysis suggests that the\nlow-frequency interaural level differences are enhanced without hindering the\nprovided intelligibility. A psychoacoustic localization test is done using a\nlistening experiment, which suggests that the replacement of time differences\ninto level differences improves the localization performance of normal-hearing\nlisteners for an anechoic scene but not for a reverberant scene.", "published": "2021-06-17 14:59:14", "link": "http://arxiv.org/abs/2106.09574v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Extracting Different Levels of Speech Information from EEG Using an\n  LSTM-Based Model", "abstract": "Decoding the speech signal that a person is listening to from the human brain\nvia electroencephalography (EEG) can help us understand how our auditory system\nworks. Linear models have been used to reconstruct the EEG from speech or vice\nversa. Recently, Artificial Neural Networks (ANNs) such as Convolutional Neural\nNetwork (CNN) and Long Short-Term Memory (LSTM) based architectures have\noutperformed linear models in modeling the relation between EEG and speech.\nBefore attempting to use these models in real-world applications such as\nhearing tests or (second) language comprehension assessment we need to know\nwhat level of speech information is being utilized by these models. In this\nstudy, we aim to analyze the performance of an LSTM-based model using different\nlevels of speech features. The task of the model is to determine which of two\ngiven speech segments is matched with the recorded EEG. We used low- and\nhigh-level speech features including: envelope, mel spectrogram, voice\nactivity, phoneme identity, and word embedding. Our results suggest that the\nmodel exploits information about silences, intensity, and broad phonetic\nclasses from the EEG. Furthermore, the mel spectrogram, which contains all this\ninformation, yields the highest accuracy (84%) among all the features.", "published": "2021-06-17 15:59:18", "link": "http://arxiv.org/abs/2106.09622v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis", "abstract": "This paper introduces WaveGrad 2, a non-autoregressive generative model for\ntext-to-speech synthesis. WaveGrad 2 is trained to estimate the gradient of the\nlog conditional density of the waveform given a phoneme sequence. The model\ntakes an input phoneme sequence, and through an iterative refinement process,\ngenerates an audio waveform. This contrasts to the original WaveGrad vocoder\nwhich conditions on mel-spectrogram features, generated by a separate model.\nThe iterative refinement process starts from Gaussian noise, and through a\nseries of refinement steps (e.g., 50 steps), progressively recovers the audio\nsequence. WaveGrad 2 offers a natural way to trade-off between inference speed\nand sample quality, through adjusting the number of refinement steps.\nExperiments show that the model can generate high fidelity audio, approaching\nthe performance of a state-of-the-art neural TTS system. We also report various\nablation studies over different model configurations. Audio samples are\navailable at https://wavegrad.github.io/v2.", "published": "2021-06-17 17:09:21", "link": "http://arxiv.org/abs/2106.09660v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PixInWav: Residual Steganography for Hiding Pixels in Audio", "abstract": "Steganography comprises the mechanics of hiding data in a host media that may\nbe publicly available. While previous works focused on unimodal setups (e.g.,\nhiding images in images, or hiding audio in audio), PixInWav targets the\nmultimodal case of hiding images in audio. To this end, we propose a novel\nresidual architecture operating on top of short-time discrete cosine transform\n(STDCT) audio spectrograms. Among our results, we find that the residual audio\nsteganography setup we propose allows independent encoding of the hidden image\nfrom the host audio without compromising quality. Accordingly, while previous\nworks require both host and hidden signals to hide a signal, PixInWav can\nencode images offline -- which can be later hidden, in a residual fashion, into\nany audio signal. Finally, we test our scheme in a lab setting to transmit\nimages over airwaves from a loudspeaker to a microphone verifying our\ntheoretical insights and obtaining promising results.", "published": "2021-06-17 20:55:44", "link": "http://arxiv.org/abs/2106.09814v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Voice2Series: Reprogramming Acoustic Models for Time Series\n  Classification", "abstract": "Learning to classify time series with limited data is a practical yet\nchallenging problem. Current methods are primarily based on hand-designed\nfeature extraction rules or domain-specific data augmentation. Motivated by the\nadvances in deep speech processing models and the fact that voice data are\nunivariate temporal signals, in this paper, we propose Voice2Series (V2S), a\nnovel end-to-end approach that reprograms acoustic models for time series\nclassification, through input transformation learning and output label mapping.\nLeveraging the representation learning power of a large-scale pre-trained\nspeech processing model, on 30 different time series tasks we show that V2S\nperforms competitive results on 19 time series classification tasks. We further\nprovide a theoretical justification of V2S by proving its population risk is\nupper bounded by the source risk and a Wasserstein distance accounting for\nfeature alignment via reprogramming. Our results offer new and effective means\nto time series classification.", "published": "2021-06-17 07:59:15", "link": "http://arxiv.org/abs/2106.09296v3", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
