{"title": "An Empirical Study on Neural Keyphrase Generation", "abstract": "Recent years have seen a flourishing of neural keyphrase generation (KPG)\nworks, including the release of several large-scale datasets and a host of new\nmodels to tackle them. Model performance on KPG tasks has increased\nsignificantly with evolving deep learning research. However, there lacks a\ncomprehensive comparison among different model designs, and a thorough\ninvestigation on related factors that may affect a KPG system's generalization\nperformance. In this empirical study, we aim to fill this gap by providing\nextensive experimental results and analyzing the most crucial factors impacting\nthe generalizability of KPG models. We hope this study can help clarify some of\nthe uncertainties surrounding the KPG task and facilitate future research on\nthis topic.", "published": "2020-09-22 00:11:32", "link": "http://arxiv.org/abs/2009.10229v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Causal Explanation Detection with Pyramid Salient-Aware Network", "abstract": "Causal explanation analysis (CEA) can assist us to understand the reasons\nbehind daily events, which has been found very helpful for understanding the\ncoherence of messages. In this paper, we focus on Causal Explanation Detection,\nan important subtask of causal explanation analysis, which determines whether a\ncausal explanation exists in one message. We design a Pyramid Salient-Aware\nNetwork (PSAN) to detect causal explanations on messages. PSAN can assist in\ncausal explanation detection via capturing the salient semantics of discourses\ncontained in their keywords with a bottom graph-based word-level salient\nnetwork. Furthermore, PSAN can modify the dominance of discourses via a top\nattention-based discourse-level salient network to enhance explanatory\nsemantics of messages. The experiments on the commonly used dataset of CEA\nshows that the PSAN outperforms the state-of-the-art method by 1.8% F1 value on\nthe Causal Explanation Detection task.", "published": "2020-09-22 02:35:45", "link": "http://arxiv.org/abs/2009.10288v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Coreference Resolution via a Multi-loss Neural Network without\n  Using Argument Information", "abstract": "Event coreference resolution(ECR) is an important task in Natural Language\nProcessing (NLP) and nearly all the existing approaches to this task rely on\nevent argument information. However, these methods tend to suffer from error\npropagation from the stage of event argument extraction. Besides, not every\nevent mention contains all arguments of an event, and argument information may\nconfuse the model that events have arguments to detect event coreference in\nreal text. Furthermore, the context information of an event is useful to infer\nthe coreference between events. Thus, in order to reduce the errors propagated\nfrom event argument extraction and use context information effectively, we\npropose a multi-loss neural network model that does not need any argument\ninformation to do the within-document event coreference resolution task and\nachieve a significant performance than the state-of-the-art methods.", "published": "2020-09-22 02:48:48", "link": "http://arxiv.org/abs/2009.10290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Reinforcement Learning for On-line Dialogue State Tracking", "abstract": "Dialogue state tracking (DST) is a crucial module in dialogue management. It\nis usually cast as a supervised training problem, which is not convenient for\non-line optimization. In this paper, a novel companion teaching based deep\nreinforcement learning (DRL) framework for on-line DST optimization is\nproposed. To the best of our knowledge, this is the first effort to optimize\nthe DST module within DRL framework for on-line task-oriented spoken dialogue\nsystems. In addition, dialogue policy can be further jointly updated.\nExperiments show that on-line DST optimization can effectively improve the\ndialogue manager performance while keeping the flexibility of using predefined\npolicy. Joint training of both DST and policy can further improve the\nperformance.", "published": "2020-09-22 05:08:48", "link": "http://arxiv.org/abs/2009.10321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributed Structured Actor-Critic Reinforcement Learning for Universal\n  Dialogue Management", "abstract": "The task-oriented spoken dialogue system (SDS) aims to assist a human user in\naccomplishing a specific task (e.g., hotel booking). The dialogue management is\na core part of SDS. There are two main missions in dialogue management:\ndialogue belief state tracking (summarising conversation history) and dialogue\ndecision-making (deciding how to reply to the user). In this work, we only\nfocus on devising a policy that chooses which dialogue action to respond to the\nuser. The sequential system decision-making process can be abstracted into a\npartially observable Markov decision process (POMDP). Under this framework,\nreinforcement learning approaches can be used for automated policy\noptimization. In the past few years, there are many deep reinforcement learning\n(DRL) algorithms, which use neural networks (NN) as function approximators,\ninvestigated for dialogue policy.", "published": "2020-09-22 05:39:31", "link": "http://arxiv.org/abs/2009.10326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Hierarchical Dialogue Policy with Graph Neural Networks", "abstract": "Dialogue policy training for composite tasks, such as restaurant reservation\nin multiple places, is a practically important and challenging problem.\nRecently, hierarchical deep reinforcement learning (HDRL) methods have achieved\ngood performance in composite tasks. However, in vanilla HDRL, both top-level\nand low-level policies are all represented by multi-layer perceptrons (MLPs)\nwhich take the concatenation of all observations from the environment as the\ninput for predicting actions. Thus, traditional HDRL approach often suffers\nfrom low sampling efficiency and poor transferability. In this paper, we\naddress these problems by utilizing the flexibility of graph neural networks\n(GNNs). A novel ComNet is proposed to model the structure of a hierarchical\nagent. The performance of ComNet is tested on composited tasks of the PyDial\nbenchmark. Experiments show that ComNet outperforms vanilla HDRL systems with\nperformance close to the upper bound. It not only achieves sample efficiency\nbut also is more robust to noise while maintaining the transferability to other\ncomposite tasks.", "published": "2020-09-22 07:23:02", "link": "http://arxiv.org/abs/2009.10355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Learning for Dialogue State Tracking", "abstract": "In task-oriented multi-turn dialogue systems, dialogue state refers to a\ncompact representation of the user goal in the context of dialogue history.\nDialogue state tracking (DST) is to estimate the dialogue state at each turn.\nDue to the dependency on complicated dialogue history contexts, DST data\nannotation is more expensive than single-sentence language understanding, which\nmakes the task more challenging. In this work, we formulate DST as a sequence\ngeneration problem and propose a novel dual-learning framework to make full use\nof unlabeled data. In the dual-learning framework, there are two agents: the\nprimal tracker agent (utterance-to-state generator) and the dual utterance\ngenerator agent (state-to-utterance genera-tor). Compared with traditional\nsupervised learning framework, dual learning can iteratively update both agents\nthrough the reconstruction error and reward signal respectively without labeled\ndata. Reward sparsity problem is hard to solve in previous DST methods. In this\nwork, the reformulation of DST as a sequence generation model effectively\nalleviates this problem. We call this primal tracker agent dual-DST.\nExperimental results on MultiWOZ2.1 dataset show that the proposed dual-DST\nworks very well, especially when labelled data is limited. It achieves\ncomparable performance to the system where labeled data is fully used.", "published": "2020-09-22 10:15:09", "link": "http://arxiv.org/abs/2009.10430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CREDIT: Coarse-to-Fine Sequence Generation for Dialogue State Tracking", "abstract": "In dialogue systems, a dialogue state tracker aims to accurately find a\ncompact representation of the current dialogue status, based on the entire\ndialogue history. While previous approaches often define dialogue states as a\ncombination of separate triples ({\\em domain-slot-value}), in this paper, we\nemploy a structured state representation and cast dialogue state tracking as a\nsequence generation problem. Based on this new formulation, we propose a {\\bf\nC}oa{\\bf R}s{\\bf E}-to-fine {\\bf DI}alogue state {\\bf T}racking ({\\bf CREDIT})\napproach. Taking advantage of the structured state representation, which is a\nmarked language sequence, we can further fine-tune the pre-trained model (by\nsupervised learning) by optimizing natural language metrics with the policy\ngradient method. Like all generative state tracking methods, CREDIT does not\nrely on pre-defined dialogue ontology enumerating all possible slot values.\nExperiments demonstrate our tracker achieves encouraging joint goal accuracy\nfor the five domains in MultiWOZ 2.0 and MultiWOZ 2.1 datasets.", "published": "2020-09-22 10:27:18", "link": "http://arxiv.org/abs/2009.10435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SUMBT+LaRL: Effective Multi-domain End-to-end Neural Task-oriented\n  Dialog System", "abstract": "The recent advent of neural approaches for developing each dialog component\nin task-oriented dialog systems has remarkably improved, yet optimizing the\noverall system performance remains a challenge. Besides, previous research on\nmodeling complicated multi-domain goal-oriented dialogs in end-to-end fashion\nhas been limited. In this paper, we present an effective multi-domain\nend-to-end trainable neural dialog system SUMBT+LaRL that incorporates two\nprevious strong models and facilitates them to be fully differentiable.\nSpecifically, the SUMBT+ estimates user-acts as well as dialog belief states,\nand the LaRL models latent system action spaces and generates responses given\nthe estimated contexts. We emphasize that the training framework of three steps\nsignificantly and stably increase dialog success rates: separately pretraining\nthe SUMBT+ and LaRL, fine-tuning the entire system, and then reinforcement\nlearning of dialog policy. We also introduce new reward criteria of\nreinforcement learning for dialog policy training. Then, we discuss\nexperimental results depending on the reward criteria and different dialog\nevaluation methods. Consequently, our model achieved the new state-of-the-art\nsuccess rate of 85.4% on corpus-based evaluation, and a comparable success rate\nof 81.40% on simulator-based evaluation provided by the DSTC8 challenge. To our\nbest knowledge, our work is the first comprehensive study of a modularized E2E\nmulti-domain dialog system that learning from each component to the entire\ndialog policy for task success.", "published": "2020-09-22 11:02:21", "link": "http://arxiv.org/abs/2009.10447v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-theoretic Semantics for Natural Language: an Algebraic Framework", "abstract": "Techniques in which words are represented as vectors have proved useful in\nmany applications in computational linguistics, however there is currently no\ngeneral semantic formalism for representing meaning in terms of vectors. We\npresent a framework for natural language semantics in which words, phrases and\nsentences are all represented as vectors, based on a theoretical analysis which\nassumes that meaning is determined by context.\n  In the theoretical analysis, we define a corpus model as a mathematical\nabstraction of a text corpus. The meaning of a string of words is assumed to be\na vector representing the contexts it occurs in in the corpus model. Based on\nthis assumption, we can show that the vector representations of words can be\nconsidered as elements of an algebra over a field. We note that in applications\nof vector spaces to representing meanings of words there is an underlying\nlattice structure; we interpret the partial ordering of the lattice as\ndescribing entailment between meanings. We also define the context-theoretic\nprobability of a string, and, based on this and the lattice structure, a degree\nof entailment between strings.\n  Together these properties form guidelines as to how to construct semantic\nrepresentations within the framework. A context theory is an implementation of\nthe framework; in an implementation strings are represented as vectors with the\nproperties deduced from the theoretical analysis.\n  We show how to incorporate logical semantics into context theories; this\nenables us to represent statistical information about uncertainty by taking\nweighted sums of individual representations. We also use the framework to\nanalyse approaches to the task of recognising textual entailment, to\nontological representations of meaning and to representing syntactic structure.\nFor the latter, we give new algebraic descriptions of link grammar.", "published": "2020-09-22 13:31:37", "link": "http://arxiv.org/abs/2009.10542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based\n  Sentiment Analysis", "abstract": "In this paper, we focus on the imbalance issue, which is rarely studied in\naspect term extraction and aspect sentiment classification when regarding them\nas sequence labeling tasks. Besides, previous works usually ignore the\ninteraction between aspect terms when labeling polarities. We propose a\nGRadient hArmonized and CascadEd labeling model (GRACE) to solve these\nproblems. Specifically, a cascaded labeling module is developed to enhance the\ninterchange between aspect terms and improve the attention of sentiment tokens\nwhen labeling sentiment polarities. The polarities sequence is designed to\ndepend on the generated aspect terms labels. To alleviate the imbalance issue,\nwe extend the gradient harmonized mechanism used in object detection to the\naspect-based sentiment analysis by adjusting the weight of each label\ndynamically. The proposed GRACE adopts a post-pretraining BERT as its backbone.\nExperimental results demonstrate that the proposed model achieves consistency\nimprovement on multiple benchmark datasets and generates state-of-the-art\nresults.", "published": "2020-09-22 13:55:34", "link": "http://arxiv.org/abs/2009.10557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoRC: Improving BERT Based Relation Classification Models via\n  Architecture Search", "abstract": "Although BERT based relation classification (RC) models have achieved\nsignificant improvements over the traditional deep learning models, it seems\nthat no consensus can be reached on what is the optimal architecture. Firstly,\nthere are multiple alternatives for entity span identification. Second, there\nare a collection of pooling operations to aggregate the representations of\nentities and contexts into fixed length vectors. Third, it is difficult to\nmanually decide which feature vectors, including their interactions, are\nbeneficial for classifying the relation types. In this work, we design a\ncomprehensive search space for BERT based RC models and employ neural\narchitecture search (NAS) method to automatically discover the design choices\nmentioned above. Experiments on seven benchmark RC tasks show that our method\nis efficient and effective in finding better architectures than the baseline\nBERT based RC model. Ablation study demonstrates the necessity of our search\nspace design and the effectiveness of our search method.", "published": "2020-09-22 16:55:49", "link": "http://arxiv.org/abs/2009.10680v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ghmerti at SemEval-2019 Task 6: A Deep Word- and Character-based\n  Approach to Offensive Language Identification", "abstract": "This paper presents the models submitted by Ghmerti team for subtasks A and B\nof the OffensEval shared task at SemEval 2019. OffensEval addresses the problem\nof identifying and categorizing offensive language in social media in three\nsubtasks; whether or not a content is offensive (subtask A), whether it is\ntargeted (subtask B) towards an individual, a group, or other entities (subtask\nC). The proposed approach includes character-level Convolutional Neural\nNetwork, word-level Recurrent Neural Network, and some preprocessing. The\nperformance achieved by the proposed model for subtask A is 77.93%\nmacro-averaged F1-score.", "published": "2020-09-22 20:13:48", "link": "http://arxiv.org/abs/2009.10792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Machine Learning Methods for Language and Dialect\n  Identification of Cuneiform Texts", "abstract": "Identification of the languages written using cuneiform symbols is a\ndifficult task due to the lack of resources and the problem of tokenization.\nThe Cuneiform Language Identification task in VarDial 2019 addresses the\nproblem of identifying seven languages and dialects written in cuneiform;\nSumerian and six dialects of Akkadian language: Old Babylonian, Middle\nBabylonian Peripheral, Standard Babylonian, Neo-Babylonian, Late Babylonian,\nand Neo-Assyrian. This paper describes the approaches taken by SharifCL team to\nthis problem in VarDial 2019. The best result belongs to an ensemble of Support\nVector Machines and a naive Bayes classifier, both working on character-level\nfeatures, with macro-averaged F1-score of 72.10%.", "published": "2020-09-22 20:17:45", "link": "http://arxiv.org/abs/2009.10794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training\n  Dynamics", "abstract": "Large datasets have become commonplace in NLP research. However, the\nincreased emphasis on data quantity has made it challenging to assess the\nquality of data. We introduce Data Maps---a model-based tool to characterize\nand diagnose datasets. We leverage a largely ignored source of information: the\nbehavior of the model on individual instances during training (training\ndynamics) for building data maps. This yields two intuitive measures for each\nexample---the model's confidence in the true class, and the variability of this\nconfidence across epochs---obtained in a single run of training. Experiments\nacross four datasets show that these model-dependent measures reveal three\ndistinct regions in the data map, each with pronounced characteristics. First,\nour data maps show the presence of \"ambiguous\" regions with respect to the\nmodel, which contribute the most towards out-of-distribution generalization.\nSecond, the most populous regions in the data are \"easy to learn\" for the\nmodel, and play an important role in model optimization. Finally, data maps\nuncover a region with instances that the model finds \"hard to learn\"; these\noften correspond to labeling errors. Our results indicate that a shift in focus\nfrom quantity to quality of data could lead to robust models and improved\nout-of-distribution generalization.", "published": "2020-09-22 20:19:41", "link": "http://arxiv.org/abs/2009.10795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keeping Up Appearances: Computational Modeling of Face Acts in\n  Persuasion Oriented Discussions", "abstract": "The notion of face refers to the public self-image of an individual that\nemerges both from the individual's own actions as well as from the interaction\nwith others. Modeling face and understanding its state changes throughout a\nconversation is critical to the study of maintenance of basic human needs in\nand through interaction. Grounded in the politeness theory of Brown and\nLevinson (1978), we propose a generalized framework for modeling face acts in\npersuasion conversations, resulting in a reliable coding manual, an annotated\ncorpus, and computational models. The framework reveals insights about\ndifferences in face act utilization between asymmetric roles in persuasion\nconversations. Using computational models, we are able to successfully identify\nface acts as well as predict a key conversational outcome (e.g. donation\nsuccess). Finally, we model a latent representation of the conversational state\nto analyze the impact of predicted face acts on the probability of a positive\nconversational outcome and observe several correlations that corroborate\nprevious findings.", "published": "2020-09-22 21:02:14", "link": "http://arxiv.org/abs/2009.10815v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling Style in Generated Dialogue", "abstract": "Open-domain conversation models have become good at generating\nnatural-sounding dialogue, using very large architectures with billions of\ntrainable parameters. The vast training data required to train these\narchitectures aggregates many different styles, tones, and qualities. Using\nthat data to train a single model makes it difficult to use the model as a\nconsistent conversational agent, e.g. with a stable set of persona traits and a\ntypical style of expression. Several architectures affording control mechanisms\nover generation architectures have been proposed, each with different\ntrade-offs. However, it remains unclear whether their use in dialogue is\nviable, and what the trade-offs look like with the most recent state-of-the-art\nconversational architectures. In this work, we adapt three previously proposed\ncontrollable generation architectures to open-domain dialogue generation,\ncontrolling the style of the generation to match one among about 200 possible\nstyles. We compare their respective performance and tradeoffs, and show how\nthey can be used to provide insights into existing conversational datasets, and\ngenerate a varied set of styled conversation replies.", "published": "2020-09-22 23:21:04", "link": "http://arxiv.org/abs/2009.10855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis", "abstract": "Evaluation metrics play a vital role in the growth of an area as it defines\nthe standard of distinguishing between good and bad models. In the area of code\nsynthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but\nthey are not suitable enough to evaluate codes, because BLEU is originally\ndesigned to evaluate the natural language, neglecting important syntactic and\nsemantic features of codes, and perfect accuracy is too strict thus it\nunderestimates different outputs with the same semantic logic. To remedy this,\nwe introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the\nstrength of BLEU in the n-gram match and further injects code syntax via\nabstract syntax trees (AST) and code semantics via data-flow. We conduct\nexperiments by evaluating the correlation coefficient between CodeBLEU and\nquality scores assigned by the programmers on three code synthesis tasks, i.e.,\ntext-to-code, code translation, and code refinement. Experimental results show\nthat our proposed CodeBLEU can achieve a better correlation with programmer\nassigned scores compared with BLEU and accuracy.", "published": "2020-09-22 03:10:49", "link": "http://arxiv.org/abs/2009.10297v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Global-to-Local Neural Networks for Document-Level Relation Extraction", "abstract": "Relation extraction (RE) aims to identify the semantic relations between\nnamed entities in text. Recent years have witnessed it raised to the document\nlevel, which requires complex reasoning with entities and mentions throughout\nan entire document. In this paper, we propose a novel model to document-level\nRE, by encoding the document information in terms of entity global and local\nrepresentations as well as context relation representations. Entity global\nrepresentations model the semantic information of all entities in the document,\nentity local representations aggregate the contextual information of multiple\nmentions of specific entities, and context relation representations encode the\ntopic information of other relations. Experimental results demonstrate that our\nmodel achieves superior performance on two public datasets for document-level\nRE. It is particularly effective in extracting relations between entities of\nlong distance and having multiple mentions.", "published": "2020-09-22 07:30:19", "link": "http://arxiv.org/abs/2009.10359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Logical foundations for hybrid type-logical grammars", "abstract": "This paper explores proof-theoretic aspects of hybrid type-logical grammars ,\na logic combining Lambek grammars with lambda grammars. We prove some basic\nproperties of the calculus, such as normalisation and the subformula property\nand also present both a sequent and a proof net calculus for hybrid\ntype-logical grammars. In addition to clarifying the logical foundations of\nhybrid type-logical grammars, the current study opens the way to variants and\nextensions of the original system, including but not limited to a\nnon-associative version and a multimodal version incorporating structural rules\nand unary modes.", "published": "2020-09-22 08:26:14", "link": "http://arxiv.org/abs/2009.10387v1", "categories": ["cs.CL", "math.LO"], "primary_category": "cs.CL"}
{"title": "SQuARE: Semantics-based Question Answering and Reasoning Engine", "abstract": "Understanding the meaning of a text is a fundamental challenge of natural\nlanguage understanding (NLU) and from its early days, it has received\nsignificant attention through question answering (QA) tasks. We introduce a\ngeneral semantics-based framework for natural language QA and also describe the\nSQuARE system, an application of this framework. The framework is based on the\ndenotational semantics approach widely used in programming language research.\nIn our framework, valuation function maps syntax tree of the text to its\ncommonsense meaning represented using basic knowledge primitives (the semantic\nalgebra) coded using answer set programming (ASP). We illustrate an application\nof this framework by using VerbNet primitives as our semantic algebra and a\nnovel algorithm based on partial tree matching that generates an answer set\nprogram that represents the knowledge in the text. A question posed against\nthat text is converted into an ASP query using the same framework and executed\nusing the s(CASP) goal-directed ASP system. Our approach is based purely on\n(commonsense) reasoning. SQuARE achieves 100% accuracy on all the five datasets\nof bAbI QA tasks that we have tested. The significance of our work is that,\nunlike other machine learning based approaches, ours is based on\n\"understanding\" the text and does not require any training. SQuARE can also\ngenerate an explanation for an answer while maintaining high accuracy.", "published": "2020-09-22 00:48:18", "link": "http://arxiv.org/abs/2009.10239v1", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI"}
{"title": "ALICE: Active Learning with Contrastive Natural Language Explanations", "abstract": "Training a supervised neural network classifier typically requires many\nannotated training samples. Collecting and annotating a large number of data\npoints are costly and sometimes even infeasible. Traditional annotation process\nuses a low-bandwidth human-machine communication interface: classification\nlabels, each of which only provides several bits of information. We propose\nActive Learning with Contrastive Explanations (ALICE), an expert-in-the-loop\ntraining framework that utilizes contrastive natural language explanations to\nimprove data efficiency in learning. ALICE learns to first use active learning\nto select the most informative pairs of label classes to elicit contrastive\nnatural language explanations from experts. Then it extracts knowledge from\nthese explanations using a semantic parser. Finally, it incorporates the\nextracted knowledge through dynamically changing the learning model's\nstructure. We applied ALICE in two visual recognition tasks, bird species\nclassification and social relationship classification. We found by\nincorporating contrastive explanations, our models outperform baseline models\nthat are trained with 40-100% more training data. We found that adding 1\nexplanation leads to similar performance gain as adding 13-30 labeled training\ndata points.", "published": "2020-09-22 01:02:07", "link": "http://arxiv.org/abs/2009.10259v1", "categories": ["cs.CL", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Constructing interval variables via faceted Rasch measurement and\n  multitask deep learning: a hate speech application", "abstract": "We propose a general method for measuring complex variables on a continuous,\ninterval spectrum by combining supervised deep learning with the Constructing\nMeasures approach to faceted Rasch item response theory (IRT). We decompose the\ntarget construct, hate speech in our case, into multiple constituent components\nthat are labeled as ordinal survey items. Those survey responses are\ntransformed via IRT into a debiased, continuous outcome measure. Our method\nestimates the survey interpretation bias of the human labelers and eliminates\nthat influence on the generated continuous measure. We further estimate the\nresponse quality of each labeler using faceted IRT, allowing responses from\nlow-quality labelers to be removed.\n  Our faceted Rasch scaling procedure integrates naturally with a multitask\ndeep learning architecture for automated prediction on new data. The ratings on\nthe theorized components of the target outcome are used as supervised, ordinal\nvariables for the neural networks' internal concept learning. We test the use\nof an activation function (ordinal softmax) and loss function (ordinal\ncross-entropy) designed to exploit the structure of ordinal outcome variables.\nOur multitask architecture leads to a new form of model interpretation because\neach continuous prediction can be directly explained by the constituent\ncomponents in the penultimate layer.\n  We demonstrate this new method on a dataset of 50,000 social media comments\nsourced from YouTube, Twitter, and Reddit and labeled by 11,000 U.S.-based\nAmazon Mechanical Turk workers to measure a continuous spectrum from hate\nspeech to counterspeech. We evaluate Universal Sentence Encoders, BERT, and\nRoBERTa as language representation models for the comment text, and compare our\npredictive accuracy to Google Jigsaw's Perspective API models, showing\nsignificant improvement over this standard benchmark.", "published": "2020-09-22 02:15:05", "link": "http://arxiv.org/abs/2009.10277v1", "categories": ["cs.CL", "cs.LG", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "End-to-End Speech Recognition and Disfluency Removal", "abstract": "Disfluency detection is usually an intermediate step between an automatic\nspeech recognition (ASR) system and a downstream task. By contrast, this paper\naims to investigate the task of end-to-end speech recognition and disfluency\nremoval. We specifically explore whether it is possible to train an ASR model\nto directly map disfluent speech into fluent transcripts, without relying on a\nseparate disfluency detection model. We show that end-to-end models do learn to\ndirectly generate fluent transcripts; however, their performance is slightly\nworse than a baseline pipeline approach consisting of an ASR system and a\ndisfluency detection model. We also propose two new metrics that can be used\nfor evaluating integrated ASR and disfluency models. The findings of this paper\ncan serve as a benchmark for further research on the task of end-to-end speech\nrecognition and disfluency removal in the future.", "published": "2020-09-22 03:11:37", "link": "http://arxiv.org/abs/2009.10298v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PodSumm -- Podcast Audio Summarization", "abstract": "The diverse nature, scale, and specificity of podcasts present a unique\nchallenge to content discovery systems. Listeners often rely on text\ndescriptions of episodes provided by the podcast creators to discover new\ncontent. Some factors like the presentation style of the narrator and\nproduction quality are significant indicators of subjective user preference but\nare difficult to quantify and not reflected in the text descriptions provided\nby the podcast creators. We propose the automated creation of podcast audio\nsummaries to aid in content discovery and help listeners to quickly preview\npodcast content before investing time in listening to an entire episode. In\nthis paper, we present a method to automatically construct a podcast summary\nvia guidance from the text-domain. Our method performs two key steps, namely,\naudio to text transcription and text summary generation. Motivated by a lack of\ndatasets for this task, we curate an internal dataset, find an effective scheme\nfor data augmentation, and design a protocol to gather summaries from\nannotators. We fine-tune a PreSumm[10] model with our augmented dataset and\nperform an ablation study. Our method achieves ROUGE-F(1/2/L) scores of\n0.63/0.53/0.63 on our dataset. We hope these results may inspire future\nresearch in this direction.", "published": "2020-09-22 04:49:33", "link": "http://arxiv.org/abs/2009.10315v1", "categories": ["cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "A Crowdsourced Open-Source Kazakh Speech Corpus and Initial Speech\n  Recognition Baseline", "abstract": "We present an open-source speech corpus for the Kazakh language. The Kazakh\nspeech corpus (KSC) contains around 332 hours of transcribed audio comprising\nover 153,000 utterances spoken by participants from different regions and age\ngroups, as well as both genders. It was carefully inspected by native Kazakh\nspeakers to ensure high quality. The KSC is the largest publicly available\ndatabase developed to advance various Kazakh speech and language processing\napplications. In this paper, we first describe the data collection and\npreprocessing procedures followed by a description of the database\nspecifications. We also share our experience and challenges faced during the\ndatabase construction, which might benefit other researchers planning to build\na speech corpus for a low-resource language. To demonstrate the reliability of\nthe database, we performed preliminary speech recognition experiments. The\nexperimental results imply that the quality of audio and transcripts is\npromising (2.8% character error rate and 8.7% word error rate on the test set).\nTo enable experiment reproducibility and ease the corpus usage, we also\nreleased an ESPnet recipe for our speech recognition models.", "published": "2020-09-22 05:57:15", "link": "http://arxiv.org/abs/2009.10334v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!", "abstract": "Despite efforts to distinguish three different evaluation setups (Bekoulis et\nal., 2018), numerous end-to-end Relation Extraction (RE) articles present\nunreliable performance comparison to previous work. In this paper, we first\nidentify several patterns of invalid comparisons in published papers and\ndescribe them to avoid their propagation. We then propose a small empirical\nstudy to quantify the impact of the most common mistake and evaluate it leads\nto overestimating the final RE performance by around 5% on ACE05. We also seize\nthis opportunity to study the unexplored ablations of two recent developments:\nthe use of language model pretraining (specifically BERT) and span-level NER.\nThis meta-analysis emphasizes the need for rigor in the report of both the\nevaluation setting and the datasets statistics and we call for unifying the\nevaluation setting in end-to-end RE.", "published": "2020-09-22 16:59:15", "link": "http://arxiv.org/abs/2009.10684v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lifelong Learning Dialogue Systems: Chatbots that Self-Learn On the Job", "abstract": "Dialogue systems, also called chatbots, are now used in a wide range of\napplications. However, they still have some major weaknesses. One key weakness\nis that they are typically trained from manually-labeled data and/or written\nwith handcrafted rules, and their knowledge bases (KBs) are also compiled by\nhuman experts. Due to the huge amount of manual effort involved, they are\ndifficult to scale and also tend to produce many errors ought to their limited\nability to understand natural language and the limited knowledge in their KBs.\nThus, the level of user satisfactory is often low. In this paper, we propose to\ndramatically improve this situation by endowing the system the ability to\ncontinually learn (1) new world knowledge, (2) new language expressions to\nground them to actions, and (3) new conversational skills, during conversation\nor \"on the job\" by themselves so that as the systems chat more and more with\nusers, they become more and more knowledgeable and are better and better able\nto understand diverse natural language expressions and improve their\nconversational skills. A key approach to achieving these is to exploit the\nmulti-user environment of such systems to self-learn through interactions with\nusers via verb and non-verb means. The paper discusses not only key challenges\nand promising directions to learn from users during conversation but also how\nto ensure the correctness of the learned knowledge.", "published": "2020-09-22 18:10:08", "link": "http://arxiv.org/abs/2009.10750v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "On Data Augmentation for Extreme Multi-label Classification", "abstract": "In this paper, we focus on data augmentation for the extreme multi-label\nclassification (XMC) problem. One of the most challenging issues of XMC is the\nlong tail label distribution where even strong models suffer from insufficient\nsupervision. To mitigate such label bias, we propose a simple and effective\naugmentation framework and a new state-of-the-art classifier. Our augmentation\nframework takes advantage of the pre-trained GPT-2 model to generate\nlabel-invariant perturbations of the input texts to augment the existing\ntraining data. As a result, it present substantial improvements over baseline\nmodels. Our contributions are two-factored: (1) we introduce a new\nstate-of-the-art classifier that uses label attention with RoBERTa and combine\nit with our augmentation framework for further improvement; (2) we present a\nbroad study on how effective are different augmentation methods in the XMC\ntask.", "published": "2020-09-22 19:31:08", "link": "http://arxiv.org/abs/2009.10778v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Message Passing for Hyper-Relational Knowledge Graphs", "abstract": "Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating\nadditional key-value pairs along with the main triple to disambiguate, or\nrestrict the validity of a fact. In this work, we propose a message passing\nbased graph encoder - StarE capable of modeling such hyper-relational KGs.\nUnlike existing approaches, StarE can encode an arbitrary number of additional\ninformation (qualifiers) along with the main triple while keeping the semantic\nroles of qualifiers and triples intact. We also demonstrate that existing\nbenchmarks for evaluating link prediction (LP) performance on hyper-relational\nKGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset\n- WD50K. Our experiments demonstrate that StarE based LP model outperforms\nexisting approaches across multiple benchmarks. We also confirm that leveraging\nqualifiers is vital for link prediction with gains up to 25 MRR points compared\nto triple-based representations.", "published": "2020-09-22 22:38:54", "link": "http://arxiv.org/abs/2009.10847v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "End-to-End Learning of Speech 2D Feature-Trajectory for Prosthetic Hands", "abstract": "Speech is one of the most common forms of communication in humans. Speech\ncommands are essential parts of multimodal controlling of prosthetic hands. In\nthe past decades, researchers used automatic speech recognition systems for\ncontrolling prosthetic hands by using speech commands. Automatic speech\nrecognition systems learn how to map human speech to text. Then, they used\nnatural language processing or a look-up table to map the estimated text to a\ntrajectory. However, the performance of conventional speech-controlled\nprosthetic hands is still unsatisfactory. Recent advancements in\ngeneral-purpose graphics processing units (GPGPUs) enable intelligent devices\nto run deep neural networks in real-time. Thus, architectures of intelligent\nsystems have rapidly transformed from the paradigm of composite subsystems\noptimization to the paradigm of end-to-end optimization. In this paper, we\npropose an end-to-end convolutional neural network (CNN) that maps speech 2D\nfeatures directly to trajectories for prosthetic hands. The proposed\nconvolutional neural network is lightweight, and thus it runs in real-time in\nan embedded GPGPU. The proposed method can use any type of speech 2D feature\nthat has local correlations in each dimension such as spectrogram, MFCC, or\nPNCC. We omit the speech to text step in controlling the prosthetic hand in\nthis paper. The network is written in Python with Keras library that has a\nTensorFlow backend. We optimized the CNN for NVIDIA Jetson TX2 developer kit.\nOur experiment on this CNN demonstrates a root-mean-square error of 0.119 and\n20ms running time to produce trajectory outputs corresponding to the voice\ninput data. To achieve a lower error in real-time, we can optimize a similar\nCNN for a more powerful embedded GPGPU such as NVIDIA AGX Xavier.", "published": "2020-09-22 02:31:00", "link": "http://arxiv.org/abs/2009.10283v1", "categories": ["eess.AS", "cs.LG", "cs.RO", "cs.SD", "cs.SY", "eess.SY", "68T40", "I.2"], "primary_category": "eess.AS"}
