{"title": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and Improved GRU", "abstract": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality.", "published": "2024-09-25 14:37:49", "link": "http://arxiv.org/abs/2410.20679v2", "categories": ["q-fin.ST", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.ST"}
{"title": "Interlacing Eigenvectors of Large Gaussian Matrices", "abstract": "We consider the eigenvectors of the principal minor of dimension $n< N$ of\nthe Dyson Brownian motion in $\\mathbb{R}^{N}$ and investigate their asymptotic\noverlaps with the eigenvectors of the full matrix in the limit of large\ndimension. We explicitly compute the limiting rescaled mean squared overlaps in\nthe large $n\\,, N$ limit with $n\\,/\\,N$ tending to a fixed ratio $q\\,$, for any\ninitial symmetric matrix $A\\,$. This is accomplished using a Burgers-type\nevolution equation for a specific resolvent. In the GOE case, our formula\nsimplifies, and we identify an eigenvector analogue of the well-known\ninterlacing of eigenvalues. We investigate in particular the case where $A$ has\nisolated eigenvalues. Our method is based on analysing the eigenvector flow\nunder the Dyson Brownian motion.", "published": "2024-09-25 16:49:43", "link": "http://arxiv.org/abs/2409.17086v1", "categories": ["math.PR", "cond-mat.stat-mech", "q-fin.MF"], "primary_category": "math.PR"}
{"title": "Trading through Earnings Seasons using Self-Supervised Contrastive Representation Learning", "abstract": "Earnings release is a key economic event in the financial markets and crucial\nfor predicting stock movements. Earnings data gives a glimpse into how a\ncompany is doing financially and can hint at where its stock might go next.\nHowever, the irregularity of its release cycle makes it a challenge to\nincorporate this data in a medium-frequency algorithmic trading model and the\nusefulness of this data fades fast after it is released, making it tough for\nmodels to stay accurate over time. Addressing this challenge, we introduce the\nContrastive Earnings Transformer (CET) model, a self-supervised learning\napproach rooted in Contrastive Predictive Coding (CPC), aiming to optimise the\nutilisation of earnings data. To ascertain its effectiveness, we conduct a\ncomparative study of CET against benchmark models across diverse sectors. Our\nresearch delves deep into the intricacies of stock data, evaluating how various\nmodels, and notably CET, handle the rapidly changing relevance of earnings data\nover time and over different sectors. The research outcomes shed light on CET's\ndistinct advantage in extrapolating the inherent value of earnings data over\ntime. Its foundation on CPC allows for a nuanced understanding, facilitating\nconsistent stock predictions even as the earnings data ages. This finding about\nCET presents a fresh approach to better use earnings data in algorithmic\ntrading for predicting stock price trends.", "published": "2024-09-25 22:09:59", "link": "http://arxiv.org/abs/2409.17392v1", "categories": ["cs.LG", "q-fin.TR"], "primary_category": "cs.LG"}
{"title": "The Impact of Designated Market Makers on Market Liquidity and Competition: A Simulation Approach", "abstract": "This paper conducts an empirical investigation into the effects of Designated\nMarket Makers (DMMs) on key market quality indicators, such as liquidity,\nbid-ask spreads, and order fulfillment ratios. Through agent-based simulations,\nthis study explores the impact of varying competition levels and incentive\nstructures among DMMs on market dynamics. It aims to demonstrate that DMMs are\ncrucial for enhancing market liquidity and stabilizing price spreads, thereby\naffirming their essential role in promoting market efficiency. Our findings\nconfirm the impact of the number of Designated Market Makers (DMMs) and asset\ndiversity on market liquidity. The result also suggests that an optimal level\nof competition among DMMs can maximize liquidity benefits while minimizing\nnegative impacts on price discovery. Additionally, the research indicates that\nthe benefits of increased number of DMMs diminish beyond a certain threshold,\nimplying that excessive incentives may not further improve market quality\nmetrics.", "published": "2024-09-25 03:32:50", "link": "http://arxiv.org/abs/2409.16589v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Understanding the Cognitive Complexity in Language Elicited by Product\n  Images", "abstract": "Product images (e.g., a phone) can be used to elicit a diverse set of\nconsumer-reported features expressed through language, including surface-level\nperceptual attributes (e.g., \"white\") and more complex ones, like perceived\nutility (e.g., \"battery\"). The cognitive complexity of elicited language\nreveals the nature of cognitive processes and the context required to\nunderstand them; cognitive complexity also predicts consumers' subsequent\nchoices. This work offers an approach for measuring and validating the\ncognitive complexity of human language elicited by product images, providing a\ntool for understanding the cognitive processes of human as well as virtual\nrespondents simulated by Large Language Models (LLMs). We also introduce a\nlarge dataset that includes diverse descriptive labels for product images,\nincluding human-rated complexity. We demonstrate that human-rated cognitive\ncomplexity can be approximated using a set of natural language models that,\ncombined, roughly capture the complexity construct. Moreover, this approach is\nminimally supervised and scalable, even in use cases with limited human\nassessment of complexity.", "published": "2024-09-25 00:26:11", "link": "http://arxiv.org/abs/2409.16521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Questions from Query Generation for Task-Adaptive\n  Retrieval", "abstract": "This paper studies the problem of information retrieval, to adapt to unseen\ntasks. Existing work generates synthetic queries from domain-specific documents\nto jointly train the retriever. However, the conventional query generator\nassumes the query as a question, thus failing to accommodate general search\nintents. A more lenient approach incorporates task-adaptive elements, such as\nfew-shot learning with an 137B LLM. In this paper, we challenge a trend\nequating query and question, and instead conceptualize query generation task as\na \"compilation\" of high-level intent into task-adaptive query. Specifically, we\npropose EGG, a query generator that better adapts to wide search intents\nexpressed in the BeIR benchmark. Our method outperforms baselines and existing\nmodels on four tasks with underexplored intents, while utilizing a query\ngenerator 47 times smaller than the previous state-of-the-art. Our findings\nreveal that instructing the LM with explicit search intent is a key aspect of\nmodeling an effective query generator.", "published": "2024-09-25 02:53:27", "link": "http://arxiv.org/abs/2409.16570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the First Shared Task on Clinical Text Generation: RRG24 and\n  \"Discharge Me!\"", "abstract": "Recent developments in natural language generation have tremendous\nimplications for healthcare. For instance, state-of-the-art systems could\nautomate the generation of sections in clinical reports to alleviate physician\nworkload and streamline hospital documentation. To explore these applications,\nwe present a shared task consisting of two subtasks: (1) Radiology Report\nGeneration (RRG24) and (2) Discharge Summary Generation (\"Discharge Me!\").\nRRG24 involves generating the 'Findings' and 'Impression' sections of radiology\nreports given chest X-rays. \"Discharge Me!\" involves generating the 'Brief\nHospital Course' and 'Discharge Instructions' sections of discharge summaries\nfor patients admitted through the emergency department. \"Discharge Me!\"\nsubmissions were subsequently reviewed by a team of clinicians. Both tasks\nemphasize the goal of reducing clinician burnout and repetitive workloads by\ngenerating documentation. We received 201 submissions from across 8 teams for\nRRG24, and 211 submissions from across 16 teams for \"Discharge Me!\".", "published": "2024-09-25 04:02:54", "link": "http://arxiv.org/abs/2409.16603v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual and Cross-Cultural Variation in Image Descriptions", "abstract": "Do speakers of different languages talk differently about what they see?\nBehavioural and cognitive studies report cultural effects on perception;\nhowever, these are mostly limited in scope and hard to replicate. In this work,\nwe conduct the first large-scale empirical study of cross-lingual variation in\nimage descriptions. Using a multimodal dataset with 31 languages and images\nfrom diverse locations, we develop a method to accurately identify entities\nmentioned in captions and present in the images, then measure how they vary\nacross languages. Our analysis reveals that pairs of languages that are\ngeographically or genetically closer tend to mention the same entities more\nfrequently. We also identify entity categories whose saliency is universally\nhigh (such as animate beings), low (clothing accessories) or displaying high\nvariance across languages (landscape). In a case study, we measure the\ndifferences in a specific language pair (e.g., Japanese mentions clothing far\nmore frequently than English). Furthermore, our method corroborates previous\nsmall-scale studies, including 1) Rosch et al. (1976)'s theory of basic-level\ncategories, demonstrating a preference for entities that are neither too\ngeneric nor too specific, and 2) Miyamoto et al. (2006)'s hypothesis that\nenvironments afford patterns of perception, such as entity counts. Overall, our\nwork reveals the presence of both universal and culture-specific patterns in\nentity mentions.", "published": "2024-09-25 05:57:09", "link": "http://arxiv.org/abs/2409.16646v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-trained Language Models Return Distinguishable Probability\n  Distributions to Unfaithfully Hallucinated Texts", "abstract": "In this work, we show the pre-trained language models return distinguishable\ngeneration probability and uncertainty distribution to unfaithfully\nhallucinated texts, regardless of their size and structure. By examining 24\nmodels on 6 data sets, we find out that 88-98% of cases return statistically\nsignificantly distinguishable generation probability and uncertainty\ndistributions. Using this general phenomenon, we showcase a\nhallucination-reducing training algorithm. Our algorithm outperforms other\nbaselines by achieving higher faithfulness metrics while maintaining sound\ngeneral text quality measures.", "published": "2024-09-25 06:22:30", "link": "http://arxiv.org/abs/2409.16658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Character-Centric Creative Story Generation via Imagination", "abstract": "Creative story generation has long been a goal of NLP research. While\nexisting methodologies have aimed to generate long and coherent stories, they\nfall significantly short of human capabilities in terms of diversity and\ncharacter depth. To address this, we introduce a novel story generation\nframework called CCI (Character-centric Creative story generation via\nImagination). CCI features two modules for creative story generation: IG\n(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we\nutilize a text-to-image model to create visual representations of key story\nelements, such as characters, backgrounds, and main plots, in a more novel and\nconcrete manner than text-only approaches. The MW module uses these story\nelements to generate multiple persona-description candidates and selects the\nbest one to insert into the story, thereby enhancing the richness and depth of\nthe narrative. We compared the stories generated by CCI and baseline models\nthrough statistical analysis, as well as human and LLM evaluations. The results\nshowed that the IG and MW modules significantly improve various aspects of the\nstories' creativity. Furthermore, our framework enables interactive multi-modal\nstory generation with users, opening up new possibilities for human-LLM\nintegration in cultural development. Project page : https://www.2024cci.p-e.kr/", "published": "2024-09-25 06:54:29", "link": "http://arxiv.org/abs/2409.16667v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-aware Causal Intervention for Counterfactual Detection", "abstract": "Counterfactual statements, which describe events that did not or cannot take\nplace, are beneficial to numerous NLP applications. Hence, we consider the\nproblem of counterfactual detection (CFD) and seek to enhance the CFD models.\nPrevious models are reliant on clue phrases to predict counterfactuality, so\nthey suffer from significant performance drop when clue phrase hints do not\nexist during testing. Moreover, these models tend to predict\nnon-counterfactuals over counterfactuals. To address these issues, we propose\nto integrate neural topic model into the CFD model to capture the global\nsemantics of the input statement. We continue to causally intervene the hidden\nrepresentations of the CFD model to balance the effect of the class labels.\nExtensive experiments show that our approach outperforms previous\nstate-of-the-art CFD and bias-resolving methods in both the CFD and other\nbias-sensitive tasks.", "published": "2024-09-25 06:55:33", "link": "http://arxiv.org/abs/2409.16668v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SynTQA: Synergistic Table-based Question Answering via Mixture of\n  Text-to-SQL and E2E TQA", "abstract": "Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main\napproaches for Table-based Question Answering task. Despite success on multiple\nbenchmarks, they have yet to be compared and their synergy remains unexplored.\nIn this paper, we identify different strengths and weaknesses through\nevaluating state-of-the-art models on benchmark datasets: Text-to-SQL\ndemonstrates superiority in handling questions involving arithmetic operations\nand long tables; E2E TQA excels in addressing ambiguous questions, non-standard\ntable schema, and complex table contents. To combine both strengths, we propose\na Synergistic Table-based Question Answering approach that integrate different\nmodels via answer selection, which is agnostic to any model types. Further\nexperiments validate that ensembling models by either feature-based or\nLLM-based answer selector significantly improves the performance over\nindividual models.", "published": "2024-09-25 07:18:45", "link": "http://arxiv.org/abs/2409.16682v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Omissions and Distortions in Transformer-based RDF-to-Text\n  Models", "abstract": "In Natural Language Generation (NLG), important information is sometimes\nomitted in the output text. To better understand and analyse how this type of\nmistake arises, we focus on RDF-to-Text generation and explore two methods of\nprobing omissions in the encoder output of BART (Lewis et al, 2020) and of T5\n(Raffel et al, 2019): (i) a novel parameter-free probing method based on the\ncomputation of cosine similarity between embeddings of RDF graphs and of RDF\ngraphs in which we removed some entities and (ii) a parametric probe which\nperforms binary classification on the encoder embeddings to detect omitted\nentities. We also extend our analysis to distorted entities, i.e. entities that\nare not fully correctly mentioned in the generated text (e.g. misspelling of\nentity, wrong units of measurement). We found that both omitted and distorted\nentities can be probed in the encoder's output embeddings. This suggests that\nthe encoder emits a weaker signal for these entities and therefore is\nresponsible for some loss of information. This also shows that probing methods\ncan be used to detect mistakes in the output of NLG models.", "published": "2024-09-25 07:54:16", "link": "http://arxiv.org/abs/2409.16707v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing\n  Systems", "abstract": "Role-playing systems powered by large language models (LLMs) have become\nincreasingly influential in emotional communication applications. However,\nthese systems are susceptible to character hallucinations, where the model\ndeviates from predefined character roles and generates responses that are\ninconsistent with the intended persona. This paper presents the first\nsystematic analysis of character hallucination from an attack perspective,\nintroducing the RoleBreak framework. Our framework identifies two core\nmechanisms-query sparsity and role-query conflict-as key factors driving\ncharacter hallucination. Leveraging these insights, we construct a novel\ndataset, RoleBreakEval, to evaluate existing hallucination mitigation\ntechniques. Our experiments reveal that even enhanced models trained to\nminimize hallucination remain vulnerable to attacks. To address these\nvulnerabilities, we propose a novel defence strategy, the Narrator Mode, which\ngenerates supplemental context through narration to mitigate role-query\nconflicts and improve query generalization. Experimental results demonstrate\nthat Narrator Mode significantly outperforms traditional refusal-based\nstrategies by reducing hallucinations, enhancing fidelity to character roles\nand queries, and improving overall narrative coherence.", "published": "2024-09-25 08:23:46", "link": "http://arxiv.org/abs/2409.16727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL", "abstract": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question and SQL construction plan, bridging the gap between\nthe query and the database structure. The pipeline leverages candidate\npredicate augmentation to mitigate erroneous or incomplete predicates in\ngenerated SQLs. Comprehensive evaluations on the BIRD benchmark illustrate that\nE-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. A further observation\nfrom our experiments reveals that incorporating schema filtering into the\ntranslation pipeline does not have a positive impact on performance when the\nmost advanced proprietary LLMs are used. Additionally, our experiments with\nsmall LLMs highlight the importance and positive impact of enriched questions\non their performance. Without fine-tuning, single-prompt SQL generation using\nenriched questions with DeepSeek Coder 7B Instruct 1.5v achieves 56.45%\nexecution accuracy on the BIRD development set.", "published": "2024-09-25 09:02:48", "link": "http://arxiv.org/abs/2409.16751v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating the Bias of Large Language Model Evaluation", "abstract": "Recently, there has been a trend of evaluating the Large Language Model (LLM)\nquality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to\nevaluate the current output quality. However, existing judges are proven to be\nbiased, namely they would favor answers which present better superficial\nquality (such as verbosity, fluency) while ignoring the instruction following\nability. In this work, we propose systematic research about the bias of\nLLM-as-a-Judge. Specifically, for closed-source judge models, we apply\ncalibration to mitigate the significance of superficial quality, both on\nprobability level and prompt level. For open-source judge models, we propose to\nmitigate the bias by contrastive training, with curated negative samples that\ndeviate from instruction but present better superficial quality. We apply our\nmethods on the bias evaluation benchmark, and experiment results show our\nmethods mitigate the bias by a large margin while maintaining a satisfactory\nevaluation accuracy.", "published": "2024-09-25 09:52:44", "link": "http://arxiv.org/abs/2409.16788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Few Hypocrites: Few-Shot Learning and Subtype Definitions for\n  Detecting Hypocrisy Accusations in Online Climate Change Debates", "abstract": "The climate crisis is a salient issue in online discussions, and hypocrisy\naccusations are a central rhetorical element in these debates. However, for\nlarge-scale text analysis, hypocrisy accusation detection is an understudied\ntool, most often defined as a smaller subtask of fallacious argument detection.\nIn this paper, we define hypocrisy accusation detection as an independent task\nin NLP, and identify different relevant subtypes of hypocrisy accusations. Our\nClimate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate\ndebate comments, expert-annotated into two different types of hypocrisy\naccusations: personal versus political hypocrisy. We evaluate few-shot\nin-context learning with 6 shots and 3 instruction-tuned Large Language Models\n(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate\nthat the GPT-4o and Llama-3 models in particular show promise in detecting\nhypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).\nHowever, context matters for a complex semantic concept such as hypocrisy\naccusations, and we find models struggle especially at identifying political\nhypocrisy accusations compared to personal moral hypocrisy. Our study\ncontributes new insights in hypocrisy detection and climate change discourse,\nand is a stepping stone for large-scale analysis of hypocrisy accusation in\nonline climate debates.", "published": "2024-09-25 10:56:28", "link": "http://arxiv.org/abs/2409.16807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shifting from endangerment to rebirth in the Artificial Intelligence\n  Age: An Ensemble Machine Learning Approach for Hawrami Text Classification", "abstract": "Hawrami, a dialect of Kurdish, is classified as an endangered language as it\nsuffers from the scarcity of data and the gradual loss of its speakers. Natural\nLanguage Processing projects can be used to partially compensate for data\navailability for endangered languages/dialects through a variety of approaches,\nsuch as machine translation, language model building, and corpora development.\nSimilarly, NLP projects such as text classification are in language\ndocumentation. Several text classification studies have been conducted for\nKurdish, but they were mainly dedicated to two particular dialects: Sorani\n(Central Kurdish) and Kurmanji (Northern Kurdish). In this paper, we introduce\nvarious text classification models using a dataset of 6,854 articles in Hawrami\nlabeled into 15 categories by two native speakers. We use K-nearest Neighbor\n(KNN), Linear Support Vector Machine (Linear SVM), Logistic Regression (LR),\nand Decision Tree (DT) to evaluate how well those methods perform the\nclassification task. The results indicate that the Linear SVM achieves a 96% of\naccuracy and outperforms the other approaches.", "published": "2024-09-25 12:52:21", "link": "http://arxiv.org/abs/2409.16884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pruning Multilingual Large Language Models for Multilingual Inference", "abstract": "Multilingual large language models (MLLMs), trained on multilingual balanced\ndata, demonstrate better zero-shot learning performance in non-English\nlanguages compared to large language models trained on English-dominant data.\nHowever, the disparity in performance between English and non-English languages\nremains a challenge yet to be fully addressed. A distinctive characteristic of\nMLLMs is their high-quality translation capabilities, indicating an acquired\nproficiency in aligning between languages. This study explores how to enhance\nthe zero-shot performance of MLLMs in non-English languages by leveraging their\nalignment capability between English and non-English languages. To achieve\nthis, we first analyze the behavior of MLLMs when performing translation and\nreveal that there are large magnitude features that play a critical role in the\ntranslation process. Inspired by these findings, we retain the weights\nassociated with operations involving the large magnitude features and prune\nother weights to force MLLMs to rely on these features for tasks beyond\ntranslation. We empirically demonstrate that this pruning strategy can enhance\nthe MLLMs' performance in non-English language.", "published": "2024-09-25 13:15:50", "link": "http://arxiv.org/abs/2409.16911v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness", "abstract": "The increasing capability and widespread usage of large language models\n(LLMs) highlight the desirability of automatic detection of LLM-generated text.\nZero-shot detectors, due to their training-free nature, have received\nconsiderable attention and notable success. In this paper, we identify a new\nfeature, token cohesiveness, that is useful for zero-shot detection, and we\ndemonstrate that LLM-generated text tends to exhibit higher token cohesiveness\nthan human-written text. Based on this observation, we devise TOCSIN, a generic\ndual-channel detection paradigm that uses token cohesiveness as a plug-and-play\nmodule to improve existing zero-shot detectors. To calculate token\ncohesiveness, TOCSIN only requires a few rounds of random token deletion and\nsemantic difference measurement, making it particularly suitable for a\npractical black-box setting where the source model used for generation is not\naccessible. Extensive experiments with four state-of-the-art base detectors on\nvarious datasets, source models, and evaluation settings demonstrate the\neffectiveness and generality of the proposed approach. Code available at:\n\\url{https://github.com/Shixuan-Ma/TOCSIN}.", "published": "2024-09-25 13:18:57", "link": "http://arxiv.org/abs/2409.16914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Temporal Ambiguity in Questions", "abstract": "Detecting and answering ambiguous questions has been a challenging task in\nopen-domain question answering. Ambiguous questions have different answers\ndepending on their interpretation and can take diverse forms. Temporally\nambiguous questions are one of the most common types of such questions. In this\npaper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QA\ndataset consisting of 8,162 open-domain questions derived from existing\ndatasets. Our annotations focus on capturing temporal ambiguity to study the\ntask of detecting temporally ambiguous questions. We propose a novel approach\nby using diverse search strategies based on disambiguated versions of the\nquestions. We also introduce and test non-search, competitive baselines for\ndetecting temporal ambiguity using zero-shot and few-shot approaches.", "published": "2024-09-25 15:59:58", "link": "http://arxiv.org/abs/2409.17046v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition", "abstract": "Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nexactly should be attributed? This involves identifying the specific\ninformation units within an answer that require grounding. In this paper, we\npropose and investigate a novel approach to the factual decomposition of\ngenerated answers for attribution, employing template-based in-context\nlearning. To accomplish this, we utilize the question and integrate negative\nsampling during few-shot in-context learning for decomposition. This approach\nenhances the semantic understanding of both abstractive and extractive answers.\nWe examine the impact of answer decomposition by providing a thorough\nexamination of various attribution approaches, ranging from retrieval-based\ntechniques to LLM-based attributors.", "published": "2024-09-25 16:32:35", "link": "http://arxiv.org/abs/2409.17073v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Level of Toxicity Against Distinct Groups in Bangla Social\n  Media Comments: A Comprehensive Investigation", "abstract": "Social media platforms have a vital role in the modern world, serving as\nconduits for communication, the exchange of ideas, and the establishment of\nnetworks. However, the misuse of these platforms through toxic comments, which\ncan range from offensive remarks to hate speech, is a concerning issue. This\nstudy focuses on identifying toxic comments in the Bengali language targeting\nthree specific groups: transgender people, indigenous people, and migrant\npeople, from multiple social media sources. The study delves into the intricate\nprocess of identifying and categorizing toxic language while considering the\nvarying degrees of toxicity: high, medium, and low. The methodology involves\ncreating a dataset, manual annotation, and employing pre-trained transformer\nmodels like Bangla-BERT, bangla-bert-base, distil-BERT, and\nBert-base-multilingual-cased for classification. Diverse assessment metrics\nsuch as accuracy, recall, precision, and F1-score are employed to evaluate the\nmodel's effectiveness. The experimental findings reveal that Bangla-BERT\nsurpasses alternative models, achieving an F1-score of 0.8903. This research\nexposes the complexity of toxicity in Bangla social media dialogues, revealing\nits differing impacts on diverse demographic groups.", "published": "2024-09-25 17:48:59", "link": "http://arxiv.org/abs/2409.17130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Transliterations Improve Crosslingual Alignment", "abstract": "Recent studies have shown that post-aligning multilingual pretrained language\nmodels (mPLMs) using alignment objectives on both original and transliterated\ndata can improve crosslingual alignment. This improvement further leads to\nbetter crosslingual transfer performance. However, it remains unclear how and\nwhy a better crosslingual alignment is achieved, as this technique only\ninvolves transliterations, and does not use any parallel data. This paper\nattempts to explicitly evaluate the crosslingual alignment and identify the key\nelements in transliteration-based approaches that contribute to better\nperformance. For this, we train multiple models under varying setups for two\npairs of related languages: (1) Polish and Ukrainian and (2) Hindi and Urdu. To\nassess alignment, we define four types of similarities based on sentence\nrepresentations. Our experimental results show that adding transliterations\nalone improves the overall similarities, even for random sentence pairs. With\nthe help of auxiliary transliteration-based alignment objectives, especially\nthe contrastive objective, the model learns to distinguish matched from random\npairs, leading to better crosslingual alignment. However, we also show that\nbetter alignment does not always yield better downstream performance,\nsuggesting that further research is needed to clarify the connection between\nalignment and performance. The code implementation is based on\n\\url{https://github.com/cisnlp/Transliteration-PPA}.", "published": "2024-09-25 20:05:45", "link": "http://arxiv.org/abs/2409.17326v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Internalizing ASR with Implicit Chain of Thought for Efficient\n  Speech-to-Speech Conversational LLM", "abstract": "Current speech-based LLMs are predominantly trained on extensive ASR and TTS\ndatasets, excelling in tasks related to these domains. However, their ability\nto handle direct speech-to-speech conversations remains notably constrained.\nThese models often rely on an ASR-to-TTS chain-of-thought pipeline, converting\nspeech into text for processing before generating audio responses, which\nintroduces latency and loses audio features. We propose a method that\nimplicitly internalizes ASR chain of thought into a speech LLM, enhancing its\nnative speech understanding capabilities. Our approach reduces latency and\nimproves the model's native understanding of speech, paving the way for more\nefficient and natural real-time audio interactions. We also release a\nlarge-scale synthetic conversational dataset to facilitate further research.", "published": "2024-09-25 20:59:12", "link": "http://arxiv.org/abs/2409.17353v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "data2lang2vec: Data Driven Typological Features Completion", "abstract": "Language typology databases enhance multi-lingual Natural Language Processing\n(NLP) by improving model adaptability to diverse linguistic structures. The\nwidely-used lang2vec toolkit integrates several such databases, but its\ncoverage remains limited at 28.9\\%. Previous work on automatically increasing\ncoverage predicts missing values based on features from other languages or\nfocuses on single features, we propose to use textual data for better-informed\nfeature prediction. To this end, we introduce a multi-lingual Part-of-Speech\n(POS) tagger, achieving over 70\\% accuracy across 1,749 languages, and\nexperiment with external statistical features and a variety of machine learning\nalgorithms. We also introduce a more realistic evaluation setup, focusing on\nlikely to be missing typology features, and show that our approach outperforms\nprevious work in both setups.", "published": "2024-09-25 21:32:57", "link": "http://arxiv.org/abs/2409.17373v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaling Behavior for Large Language Models regarding Numeral Systems: An\n  Example using Pythia", "abstract": "Though Large Language Models (LLMs) have shown remarkable abilities in\nmathematics reasoning, they are still struggling with performing numeric\noperations accurately, such as addition and multiplication. Numbers can be\ntokenized into tokens in various ways by different LLMs and affect the numeric\noperations performance. Currently, there are two representatives: 1) Tokenize\ninto $1$-digit, and 2) Tokenize into $1\\sim 3$ digit. The difference is roughly\nequivalent to using different numeral systems (namely base $10$ or base\n$10^{3}$). In light of this, we study the scaling behavior of different numeral\nsystems in the context of transformer-based large language models. We\nempirically show that a base $10$ system is consistently more data-efficient\nthan a base $10^{2}$ or $10^{3}$ system across training data scale, model sizes\nunder from-scratch training settings, while different number systems have very\nsimilar fine-tuning performances. We attribute this to higher token frequencies\nof a base $10$ system. Additionally, we reveal extrapolation behavior patterns\non addition and multiplication. We identify that base $100$ and base $1000$\nsystems struggle on token-level discernment and token-level operations. We also\nsheds light on the mechanism learnt by the models.", "published": "2024-09-25 22:08:31", "link": "http://arxiv.org/abs/2409.17391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Investment Opinion Ranking through Argument-Based Sentiment\n  Analysis", "abstract": "In the era of rapid Internet and social media platform development,\nindividuals readily share their viewpoints online. The overwhelming quantity of\nthese posts renders comprehensive analysis impractical. This necessitates an\nefficient recommendation system to filter and present significant, relevant\nopinions. Our research introduces a dual-pronged argument mining technique to\nimprove recommendation system effectiveness, considering both professional and\namateur investor perspectives. Our first strategy involves using the\ndiscrepancy between target and closing prices as an opinion indicator. The\nsecond strategy applies argument mining principles to score investors'\nopinions, subsequently ranking them by these scores. Experimental results\nconfirm the effectiveness of our approach, demonstrating its ability to\nidentify opinions with higher profit potential. Beyond profitability, our\nresearch extends to risk analysis, examining the relationship between\nrecommended opinions and investor behaviors. This offers a holistic view of\npotential outcomes following the adoption of these recommended opinions.", "published": "2024-09-25 23:00:20", "link": "http://arxiv.org/abs/2409.17417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-Finetuning with Impact Duration Awareness for Stock Movement\n  Prediction", "abstract": "Understanding the duration of news events' impact on the stock market is\ncrucial for effective time-series forecasting, yet this facet is largely\noverlooked in current research. This paper addresses this research gap by\nintroducing a novel dataset, the Impact Duration Estimation Dataset (IDED),\nspecifically designed to estimate impact duration based on investor opinions.\nOur research establishes that pre-finetuning language models with IDED can\nenhance performance in text-based stock movement predictions. In addition, we\njuxtapose our proposed pre-finetuning task with sentiment analysis\npre-finetuning, further affirming the significance of learning impact duration.\nOur findings highlight the promise of this novel research direction in stock\nmovement prediction, offering a new avenue for financial forecasting. We also\nprovide the IDED and pre-finetuned language models under the CC BY-NC-SA 4.0\nlicense for academic use, fostering further exploration in this field.", "published": "2024-09-25 23:06:55", "link": "http://arxiv.org/abs/2409.17419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Extending Direct Preference Optimization to Accommodate Ties", "abstract": "We derive and investigate two DPO variants that explicitly model the\npossibility of declaring a tie in pair-wise comparisons. We replace the\nBradley-Terry model in DPO with two well-known modeling extensions, by Rao and\nKupper and by Davidson, that assign probability to ties as alternatives to\nclear preferences. Our experiments in neural machine translation and\nsummarization show that explicitly labeled ties can be added to the datasets\nfor these DPO variants without the degradation in task performance that is\nobserved when the same tied pairs are presented to DPO. We find empirically\nthat the inclusion of ties leads to stronger regularization with respect to the\nreference policy as measured by KL divergence, and we see this even for DPO in\nits original form. These findings motivate and enable the inclusion of tied\npairs in preference optimization as opposed to simply discarding them.", "published": "2024-09-25 23:38:15", "link": "http://arxiv.org/abs/2409.17431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Language Models to Win Debates with Self-Play Improves Judge\n  Accuracy", "abstract": "We test the robustness of debate as a method of scalable oversight by\ntraining models to debate with data generated via self-play. In a long-context\nreading comprehension task, we find that language model based evaluators answer\nquestions more accurately when judging models optimized to win debates. By\ncontrast, we find no such relationship for consultancy models trained to\npersuade a judge without an opposing debater present. In quantitative and\nqualitative comparisons between our debate models and novel consultancy\nbaselines, we find evidence that debate training encourages stronger and more\ninformative arguments, showing promise that it can help provide high-quality\nsupervision for tasks that are difficult to directly evaluate.", "published": "2024-09-25 05:28:33", "link": "http://arxiv.org/abs/2409.16636v1", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.6"], "primary_category": "cs.CL"}
{"title": "Domain-Independent Automatic Generation of Descriptive Texts for\n  Time-Series Data", "abstract": "Due to scarcity of time-series data annotated with descriptive texts,\ntraining a model to generate descriptive texts for time-series data is\nchallenging. In this study, we propose a method to systematically generate\ndomain-independent descriptive texts from time-series data. We identify two\ndistinct approaches for creating pairs of time-series data and descriptive\ntexts: the forward approach and the backward approach. By implementing the\nnovel backward approach, we create the Temporal Automated Captions for\nObservations (TACO) dataset. Experimental results demonstrate that a\ncontrastive learning based model trained using the TACO dataset is capable of\ngenerating descriptive texts for time-series data in novel domains.", "published": "2024-09-25 06:04:03", "link": "http://arxiv.org/abs/2409.16647v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SWE2: SubWord Enriched and Significant Word Emphasized Framework for\n  Hate Speech Detection", "abstract": "Hate speech detection on online social networks has become one of the\nemerging hot topics in recent years. With the broad spread and fast propagation\nspeed across online social networks, hate speech makes significant impacts on\nsociety by increasing prejudice and hurting people. Therefore, there are\naroused attention and concern from both industry and academia. In this paper,\nwe address the hate speech problem and propose a novel hate speech detection\nframework called SWE2, which only relies on the content of messages and\nautomatically identifies hate speech. In particular, our framework exploits\nboth word-level semantic information and sub-word knowledge. It is intuitively\npersuasive and also practically performs well under a situation with/without\ncharacter-level adversarial attack. Experimental results show that our proposed\nmodel achieves 0.975 accuracy and 0.953 macro F1, outperforming 7\nstate-of-the-art baselines under no adversarial attack. Our model robustly and\nsignificantly performed well under extreme adversarial attack (manipulation of\n50% messages), achieving 0.967 accuracy and 0.934 macro F1.", "published": "2024-09-25 07:05:44", "link": "http://arxiv.org/abs/2409.16673v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for\n  Superior Planning and Decision-Making", "abstract": "Long-term memory is significant for agents, in which insights play a crucial\nrole. However, the emergence of irrelevant insight and the lack of general\ninsight can greatly undermine the effectiveness of insight. To solve this\nproblem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an\nembodied agent designed to improve LLMs' planning and decision-making ability\nby summarizing and utilizing insight effectively across different scales. MSI\nachieves this through the experience selector, insight generator, and insight\nselector. Leveraging a three-part pipeline, MSI can generate task-specific and\nhigh-level insight, store it in a database, and then use relevant insight from\nit to aid in decision-making. Our experiments show that MSI outperforms another\ninsight strategy when planning by GPT3.5. Moreover, We delve into the\nstrategies for selecting seed experience and insight, aiming to provide LLM\nwith more useful and relevant insight for better decision-making. Our\nobservations also indicate that MSI exhibits better robustness when facing\ndomain-shifting scenarios.", "published": "2024-09-25 07:21:51", "link": "http://arxiv.org/abs/2409.16686v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?", "abstract": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research.", "published": "2024-09-25 07:55:36", "link": "http://arxiv.org/abs/2409.16710v2", "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CE"}
{"title": "PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning", "abstract": "Low-rank adaptation (LoRA) and its variants have recently gained much\ninterest due to their ability to avoid excessive inference costs. However, LoRA\nstill encounters the following challenges: (1) Limitation of low-rank\nassumption; and (2) Its initialization method may be suboptimal. To this end,\nwe propose PMSS(Pre-trained Matrices Skeleton Selection), which enables\nhigh-rank updates with low costs while leveraging semantic and linguistic\ninformation inherent in pre-trained weight. It achieves this by selecting\nskeletons from the pre-trained weight matrix and only learning a small matrix\ninstead. Experiments demonstrate that PMSS outperforms LoRA and other\nfine-tuning methods across tasks with much less trainable parameters. We\ndemonstrate its effectiveness, especially in handling complex tasks such as\nDROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math\nreasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of\nGSM8K). The code and model will be released soon.", "published": "2024-09-25 08:20:24", "link": "http://arxiv.org/abs/2409.16722v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodeInsight: A Curated Dataset of Practical Coding Solutions from Stack\n  Overflow", "abstract": "We introduce a novel dataset tailored for code generation, aimed at aiding\ndevelopers in common tasks. Our dataset provides examples that include a\nclarified intent, code snippets associated, and an average of three related\nunit tests. It encompasses a range of libraries such as \\texttt{Pandas},\n\\texttt{Numpy}, and \\texttt{Regex}, along with more than 70 standard libraries\nin Python code derived from Stack Overflow. Comprising 3,409 crafted examples\nby Python experts, our dataset is designed for both model finetuning and\nstandalone evaluation. To complete unit tests evaluation, we categorize\nexamples in order to get more fine grained analysis, enhancing the\nunderstanding of models' strengths and weaknesses in specific coding tasks. The\nexamples have been refined to reduce data contamination, a process confirmed by\nthe performance of three leading models: Mistral 7B, CodeLLaMa 13B, and\nStarcoder 15B. We further investigate data-contamination testing GPT-4\nperformance on a part of our dataset. The benchmark can be accessed at\n\\url{https://github.com/NathanaelBeau/CodeInsight}.", "published": "2024-09-25 11:18:52", "link": "http://arxiv.org/abs/2409.16819v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Exposing Assumptions in AI Benchmarks through Cognitive Modelling", "abstract": "Cultural AI benchmarks often rely on implicit assumptions about measured\nconstructs, leading to vague formulations with poor validity and unclear\ninterrelations. We propose exposing these assumptions using explicit cognitive\nmodels formulated as Structural Equation Models. Using cross-lingual alignment\ntransfer as an example, we show how this approach can answer key research\nquestions and identify missing datasets. This framework grounds benchmark\nconstruction theoretically and guides dataset development to improve construct\nmeasurement. By embracing transparency, we move towards more rigorous,\ncumulative AI evaluation science, challenging researchers to critically examine\ntheir assessment foundations.", "published": "2024-09-25 11:55:02", "link": "http://arxiv.org/abs/2409.16849v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question\n  Answering", "abstract": "Time-Sensitive Question Answering (TSQA) demands the effective utilization of\nspecific temporal contexts, encompassing multiple time-evolving facts, to\naddress time-sensitive questions. This necessitates not only the parsing of\ntemporal information within questions but also the identification and\nunderstanding of time-evolving facts to generate accurate answers. However,\ncurrent large language models still have limited sensitivity to temporal\ninformation and their inadequate temporal reasoning capabilities. In this\npaper, we propose a novel framework that enhances temporal awareness and\nreasoning through Temporal Information-Aware Embedding and Granular Contrastive\nReinforcement Learning. Experimental results on four TSQA datasets demonstrate\nthat our framework significantly outperforms existing LLMs in TSQA tasks,\nmarking a step forward in bridging the performance gap between machine and\nhuman temporal understanding and reasoning.", "published": "2024-09-25 13:13:21", "link": "http://arxiv.org/abs/2409.16909v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating OCR-Sensitive Neurons to Improve Entity Recognition in\n  Historical Documents", "abstract": "This paper investigates the presence of OCR-sensitive neurons within the\nTransformer architecture and their influence on named entity recognition (NER)\nperformance on historical documents. By analysing neuron activation patterns in\nresponse to clean and noisy text inputs, we identify and then neutralise\nOCR-sensitive neurons to improve model performance. Based on two open access\nlarge language models (Llama2 and Mistral), experiments demonstrate the\nexistence of OCR-sensitive regions and show improvements in NER performance on\nhistorical newspapers and classical commentaries, highlighting the potential of\ntargeted neuron modulation to improve models' performance on noisy text.", "published": "2024-09-25 13:45:23", "link": "http://arxiv.org/abs/2409.16934v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical\n  Impacts, Constraints, and Emerging Questions", "abstract": "There have been rapid advancements in the capabilities of large language\nmodels (LLMs) in recent years, greatly revolutionizing the field of natural\nlanguage processing (NLP) and artificial intelligence (AI) to understand and\ninteract with human language. Therefore, in this work, we conduct a systematic\ninvestigation of the literature to identify the prominent themes and directions\nof LLM developments, impacts, and limitations. Our findings illustrate the\naims, methodologies, limitations, and future directions of LLM research. It\nincludes responsible development considerations, algorithmic improvements,\nethical challenges, and societal implications of LLM development. Overall, this\npaper provides a rigorous and comprehensive overview of current research in LLM\nand identifies potential directions for future development. The article\nhighlights the application areas that could have a positive impact on society\nalong with the ethical considerations.", "published": "2024-09-25 14:36:30", "link": "http://arxiv.org/abs/2409.16974v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs", "abstract": "Large Language Models (LLMs) are widely used in both industry and academia\nfor various tasks, yet evaluating the consistency of generated text responses\ncontinues to be a challenge. Traditional metrics like ROUGE and BLEU show a\nweak correlation with human judgment. More sophisticated metrics using Natural\nLanguage Inference (NLI) have shown improved correlations but are complex to\nimplement, require domain-specific training due to poor cross-domain\ngeneralization, and lack explainability. More recently, prompt-based metrics\nusing LLMs as evaluators have emerged; while they are easier to implement, they\nstill lack explainability and depend on task-specific prompts, which limits\ntheir generalizability. This work introduces Automated eXplainable Consistency\nEvaluation using LLMs (AXCEL), a prompt-based consistency metric which offers\nexplanations for the consistency scores by providing detailed reasoning and\npinpointing inconsistent text spans. AXCEL is also a generalizable metric which\ncan be adopted to multiple tasks without changing the prompt. AXCEL outperforms\nboth non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting\ninconsistencies across summarization by 8.7%, free text generation by 6.2%, and\ndata-to-text conversion tasks by 29.4%. We also evaluate the influence of\nunderlying LLMs on prompt based metric performance and recalibrate the SOTA\nprompt-based metrics with the latest LLMs for fair comparison. Further, we show\nthat AXCEL demonstrates strong performance using open source LLMs.", "published": "2024-09-25 14:45:52", "link": "http://arxiv.org/abs/2409.16984v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Models Can and Should Embrace the Communicative Nature of\n  Human-Generated Math", "abstract": "Math is constructed by people for people: just as natural language corpora\nreflect not just propositions but the communicative goals of language users,\nthe math data that models are trained on reflects not just idealized\nmathematical entities but rich communicative intentions. While there are\nimportant advantages to treating math in a purely symbolic manner, we here\nhypothesize that there are benefits to treating math as situated linguistic\ncommunication and that language models are well suited for this goal, in ways\nthat are not fully appreciated. We illustrate these points with two case\nstudies. First, we ran an experiment in which we found that language models\ninterpret the equals sign in a humanlike way -- generating systematically\ndifferent word problems for the same underlying equation arranged in different\nways. Second, we found that language models prefer proofs to be ordered in\nnaturalistic ways, even though other orders would be logically equivalent. We\nadvocate for AI systems that learn from and represent the communicative\nintentions latent in human-generated math.", "published": "2024-09-25 15:08:08", "link": "http://arxiv.org/abs/2409.17005v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AutoLLM-CARD: Towards a Description and Landscape of Large Language\n  Models", "abstract": "With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As more papers are published, researchers and developers face the\nchallenge of information overload. Thus, developing a system that can\nautomatically extract and organise key information about LLMs from academic\npapers is particularly important. The standard format for documenting\ninformation about LLMs is the LLM model card (\\textbf{LLM-Card}). We propose a\nmethod for automatically generating LLM model cards from scientific\npublications. We use Named Entity Recognition (\\textbf{NER}) and Relation\nExtraction (\\textbf{RE}) methods that automatically extract key information\nabout LLMs from the papers, helping researchers to access information about\nLLMs efficiently. These features include model \\textit{licence}, model\n\\textit{name}, and model \\textit{application}. With these features, we can form\na model card for each paper. We processed 106 academic papers by defining three\ndictionaries -- LLM's name, licence, and application. 11,051 sentences were\nextracted through dictionary lookup, and the dataset was constructed through\nmanual review of the final selection of 129 sentences with a link between the\nname and the \\textit{licence}, and 106 sentences with a link between the model\nname and the \\textit{application}. The resulting resource is relevant for LLM\ncard illustrations using relational knowledge graphs. Our code and findings can\ncontribute to automatic LLM card generation. Data and code in\n\\textsc{autoLLM-Card} will be shared and freely available at\n\\url{https://github.com/shengwei-tian/dependency-parser-visualization}", "published": "2024-09-25 15:15:57", "link": "http://arxiv.org/abs/2409.17011v3", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Can Vision Language Models Learn from Visual Demonstrations of Ambiguous\n  Spatial Reasoning?", "abstract": "Large vision-language models (VLMs) have become state-of-the-art for many\ncomputer vision tasks, with in-context learning (ICL) as a popular adaptation\nstrategy for new ones. But can VLMs learn novel concepts purely from visual\ndemonstrations, or are they limited to adapting to the output format of ICL\nexamples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks\n(SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks\nin-context. We find that VLMs fail to do this zero-shot, and sometimes continue\nto fail after finetuning. However, adding simpler data to the training by\ncurriculum learning leads to improved ICL performance.", "published": "2024-09-25 16:45:02", "link": "http://arxiv.org/abs/2409.17080v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Handy Appetizer", "abstract": "This book explores the role of Artificial Intelligence (AI), Machine Learning\n(ML), and Deep Learning (DL) in driving the progress of big data analytics and\nmanagement. The book focuses on simplifying the complex mathematical concepts\nbehind deep learning, offering intuitive visualizations and practical case\nstudies to help readers understand how neural networks and technologies like\nConvolutional Neural Networks (CNNs) work. It introduces several classic models\nand technologies such as Transformers, GPT, ResNet, BERT, and YOLO,\nhighlighting their applications in fields like natural language processing,\nimage recognition, and autonomous driving. The book also emphasizes the\nimportance of pre-trained models and how they can enhance model performance and\naccuracy, with instructions on how to apply these models in various real-world\nscenarios. Additionally, it provides an overview of key big data management\ntechnologies like SQL and NoSQL databases, as well as distributed computing\nframeworks such as Apache Hadoop and Spark, explaining their importance in\nmanaging and processing vast amounts of data. Ultimately, the book underscores\nthe value of mastering deep learning and big data management skills as critical\ntools for the future workforce, making it an essential resource for both\nbeginners and experienced professionals.", "published": "2024-09-25 17:31:45", "link": "http://arxiv.org/abs/2409.17120v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Effective, Robust and Fairness-aware Hate Speech Detection Framework", "abstract": "With the widespread online social networks, hate speeches are spreading\nfaster and causing more damage than ever before. Existing hate speech detection\nmethods have limitations in several aspects, such as handling data\ninsufficiency, estimating model uncertainty, improving robustness against\nmalicious attacks, and handling unintended bias (i.e., fairness). There is an\nurgent need for accurate, robust, and fair hate speech classification in online\nsocial networks. To bridge the gap, we design a data-augmented, fairness\naddressed, and uncertainty estimated novel framework. As parts of the\nframework, we propose Bidirectional Quaternion-Quasi-LSTM layers to balance\neffectiveness and efficiency. To build a generalized model, we combine five\ndatasets collected from three platforms. Experiment results show that our model\noutperforms eight state-of-the-art methods under both no attack scenario and\nvarious attack scenarios, indicating the effectiveness and robustness of our\nmodel. We share our code along with combined dataset for better future research", "published": "2024-09-25 07:01:51", "link": "http://arxiv.org/abs/2409.17191v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BabyLlama-2: Ensemble-Distilled Models Consistently Outperform Teachers\n  With Limited Data", "abstract": "We present BabyLlama-2, a 345 million parameter model distillation-pretrained\nfrom two teachers on a 10 million word corpus for the BabyLM competition. On\nBLiMP and SuperGLUE benchmarks, BabyLlama-2 outperforms baselines trained on\nboth 10 and 100 million word datasets with the same data mix, as well as its\nteacher models. Through an extensive hyperparameter sweep, we demonstrate that\nthe advantages of distillation cannot be attributed to suboptimal\nhyperparameter selection of the teachers. Our findings underscore the need for\nfurther investigation into distillation techniques, particularly in\ndata-limited settings.", "published": "2024-09-25 19:46:49", "link": "http://arxiv.org/abs/2409.17312v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Building Multilingual Datasets for Predicting Mental Health Severity\n  through LLMs: Prospects and Challenges", "abstract": "Large Language Models (LLMs) are increasingly being integrated into various\nmedical fields, including mental health support systems. However, there is a\ngap in research regarding the effectiveness of LLMs in non-English mental\nhealth support applications. To address this problem, we present a novel\nmultilingual adaptation of widely-used mental health datasets, translated from\nEnglish into six languages (e.g., Greek, Turkish, French, Portuguese, German,\nand Finnish). This dataset enables a comprehensive evaluation of LLM\nperformance in detecting mental health conditions and assessing their severity\nacross multiple languages. By experimenting with GPT and Llama, we observe\nconsiderable variability in performance across languages, despite being\nevaluated on the same translated dataset. This inconsistency underscores the\ncomplexities inherent in multilingual mental health support, where\nlanguage-specific nuances and mental health data coverage can affect the\naccuracy of the models. Through comprehensive error analysis, we emphasize the\nrisks of relying exclusively on LLMs in medical settings (e.g., their potential\nto contribute to misdiagnoses). Moreover, our proposed approach offers\nsignificant cost savings for multilingual tasks, presenting a major advantage\nfor broad-scale implementation.", "published": "2024-09-25 22:14:34", "link": "http://arxiv.org/abs/2409.17397v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Post-hoc Reward Calibration: A Case Study on Length Bias", "abstract": "Reinforcement Learning from Human Feedback aligns the outputs of Large\nLanguage Models with human values and preferences. Central to this process is\nthe reward model (RM), which translates human feedback into training signals\nfor optimising LLM behaviour. However, RMs can develop biases by exploiting\nspurious correlations in their training data, such as favouring outputs based\non length or style rather than true quality. These biases can lead to incorrect\noutput rankings, sub-optimal model evaluations, and the amplification of\nundesirable behaviours in LLMs alignment. This paper addresses the challenge of\ncorrecting such biases without additional data and training, introducing the\nconcept of Post-hoc Reward Calibration. We first propose an intuitive approach\nto estimate the bias term and, thus, remove it to approximate the underlying\ntrue reward. We then extend the approach to a more general and robust form with\nthe Locally Weighted Regression. Focusing on the prevalent length bias, we\nvalidate our proposed approaches across three experimental settings,\ndemonstrating consistent improvements: (1) a 3.11 average performance gain\nacross 33 reward models on the RewardBench dataset; (2) enhanced alignment of\nRM rankings with GPT-4 evaluations and human preferences based on the\nAlpacaEval benchmark; and (3) improved Length-Controlled win rate of the RLHF\nprocess in multiple LLM--RM combinations. Our method is computationally\nefficient and generalisable to other types of bias and RMs, offering a scalable\nand robust solution for mitigating biases in LLM alignment. Our code and\nresults are available at https://github.com/ZeroYuHuang/Reward-Calibration.", "published": "2024-09-25 22:30:42", "link": "http://arxiv.org/abs/2409.17407v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "From Deception to Detection: The Dual Roles of Large Language Models in\n  Fake News", "abstract": "Fake news poses a significant threat to the integrity of information\necosystems and public trust. The advent of Large Language Models (LLMs) holds\nconsiderable promise for transforming the battle against fake news. Generally,\nLLMs represent a double-edged sword in this struggle. One major concern is that\nLLMs can be readily used to craft and disseminate misleading information on a\nlarge scale. This raises the pressing questions: Can LLMs easily generate\nbiased fake news? Do all LLMs have this capability? Conversely, LLMs offer\nvaluable prospects for countering fake news, thanks to their extensive\nknowledge of the world and robust reasoning capabilities. This leads to other\ncritical inquiries: Can we use LLMs to detect fake news, and do they outperform\ntypical detection models? In this paper, we aim to address these pivotal\nquestions by exploring the performance of various LLMs. Our objective is to\nexplore the capability of various LLMs in effectively combating fake news,\nmarking this as the first investigation to analyze seven such models. Our\nresults reveal that while some models adhere strictly to safety protocols,\nrefusing to generate biased or misleading content, other models can readily\nproduce fake news across a spectrum of biases. Additionally, our results show\nthat larger models generally exhibit superior detection abilities and that\nLLM-generated fake news are less likely to be detected than human-written ones.\nFinally, our findings demonstrate that users can benefit from LLM-generated\nexplanations in identifying fake news.", "published": "2024-09-25 22:57:29", "link": "http://arxiv.org/abs/2409.17416v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and\n  Dynamic Workflows", "abstract": "Despite recent advancements in large language models (LLMs), their\nperformance on complex reasoning problems requiring multi-step thinking and\ncombining various skills is still limited. To address this, we propose a novel\nframework HDFlow for complex reasoning with LLMs that combines fast and slow\nthinking modes in an adaptive manner. Our approach consists of two key\ncomponents: 1) a new approach for slow, deliberate reasoning called Dynamic\nWorkflow, which automatically decomposes complex problems into more manageable\nsub-tasks and dynamically designs a workflow to assemble specialized LLM or\nsymbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general\nframework that dynamically combines fast and slow thinking based on problem\ncomplexity. Finally, we propose an easy-to-scale method for automatically\nsynthesizing a large-scale dataset of 27K challenging reasoning problems for\ncomplex reasoning and a hybrid thinking tuning method that trains smaller LLMs\non this dataset to internalize the fast/slow hybrid reasoning strategies.\nExperiments on four reasoning benchmark datasets demonstrate that our slow\nthinking with dynamic workflows significantly outperforms Chain-of-Thought, and\nhybrid thinking achieves the highest accuracy while providing an effective\nbalance between computational efficiency and performance. Fine-tuning using our\nhybrid thinking approach also significantly boosts the complex reasoning\ncapabilities of open-source language models. The results showcase the promise\nof slow thinking, dynamic workflows, and hybrid thinking in expanding the\nfrontier of complex problem-solving with LLMs\\footnote{Code and data will be\nreleased at \\url{https://github.com/wenlinyao/HDFlow}.}.", "published": "2024-09-25 23:52:17", "link": "http://arxiv.org/abs/2409.17433v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications", "abstract": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.", "published": "2024-09-25 07:03:31", "link": "http://arxiv.org/abs/2409.19020v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in\n  Scholarly Publications", "abstract": "Recent studies have evaluated the creativity/novelty of large language models\n(LLMs) primarily from a semantic perspective, using benchmarks from cognitive\nscience. However, accessing the novelty in scholarly publications is a largely\nunexplored area in evaluating LLMs. In this paper, we introduce a scholarly\nnovelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in\nscholarly papers. SchNovel consists of 15000 pairs of papers across six fields\nsampled from the arXiv dataset with publication dates spanning 2 to 10 years\napart. In each pair, the more recently published paper is assumed to be more\nnovel. Additionally, we propose RAG-Novelty, which simulates the review process\ntaken by human reviewers by leveraging the retrieval of similar papers to\nassess novelty. Extensive experiments provide insights into the capabilities of\ndifferent LLMs to assess novelty and demonstrate that RAG-Novelty outperforms\nrecent baseline models.", "published": "2024-09-25 04:12:38", "link": "http://arxiv.org/abs/2409.16605v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Claim-Guided Textual Backdoor Attack for Practical Applications", "abstract": "Recent advances in natural language processing and the increased use of large\nlanguage models have exposed new security vulnerabilities, such as backdoor\nattacks. Previous backdoor attacks require input manipulation after model\ndistribution to activate the backdoor, posing limitations in real-world\napplicability. Addressing this gap, we introduce a novel Claim-Guided Backdoor\nAttack (CGBA), which eliminates the need for such manipulations by utilizing\ninherent textual claims as triggers. CGBA leverages claim extraction,\nclustering, and targeted training to trick models to misbehave on targeted\nclaims without affecting their performance on clean data. CGBA demonstrates its\neffectiveness and stealthiness across various datasets and models,\nsignificantly enhancing the feasibility of practical backdoor attacks. Our code\nand data will be available at https://github.com/PaperCGBA/CGBA.", "published": "2024-09-25 04:53:27", "link": "http://arxiv.org/abs/2409.16618v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation", "abstract": "Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) \\etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints can be found at\nhttps://github.com/bytedance/SALMONN.", "published": "2024-09-25 05:44:44", "link": "http://arxiv.org/abs/2409.16644v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Recognition Rescoring with Large Speech-Text Foundation Models", "abstract": "Large language models (LLM) have demonstrated the ability to understand human\nlanguage by leveraging large amount of text data. Automatic speech recognition\n(ASR) systems are often limited by available transcribed speech data and\nbenefit from a second pass rescoring using LLM. Recently multi-modal large\nlanguage models, particularly speech and text foundational models have\ndemonstrated strong spoken language understanding. Speech-Text foundational\nmodels leverage large amounts of unlabelled and labelled data both in speech\nand text modalities to model human language. In this work, we propose novel\ntechniques to use multi-modal LLM for ASR rescoring. We also explore\ndiscriminative training to further improve the foundational model rescoring\nperformance. We demonstrate cross-modal knowledge transfer in speech-text LLM\ncan benefit rescoring. Our experiments demonstrate up-to 20% relative\nimprovements over Whisper large ASR and up-to 15% relative improvements over\ntext-only LLM.", "published": "2024-09-25 06:17:23", "link": "http://arxiv.org/abs/2409.16654v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emotional Dimension Control in Language Model-Based Text-to-Speech:\n  Spanning a Broad Spectrum of Human Emotions", "abstract": "Current emotional text-to-speech systems face challenges in conveying the\nfull spectrum of human emotions, largely due to the inherent complexity of\nhuman emotions and the limited range of emotional labels in existing speech\ndatasets. To address these limitations, this paper introduces a TTS framework\nthat provides flexible user control over three emotional dimensions - pleasure,\narousal, and dominance - enabling the synthesis of a diverse array of emotional\nstyles. The framework leverages an emotional dimension predictor, trained soley\non categorical labels from speech data and grounded in earlier psychological\nresearch, which is seamlessly integrated into a language model-based TTS\nsystem. Experimental results demonstrates that the proposed framework\neffectively learns emotional styles from expressive speech, eliminating the\nneed for explicit emotion labels during TTS training, while enhancing the\nnaturalness and diversity of synthesized emotional speech.", "published": "2024-09-25 07:16:16", "link": "http://arxiv.org/abs/2409.16681v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms", "abstract": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization.", "published": "2024-09-25 07:38:02", "link": "http://arxiv.org/abs/2409.16694v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Holistic Automated Red Teaming for Large Language Models through\n  Top-Down Test Case Generation and Multi-turn Interaction", "abstract": "Automated red teaming is an effective method for identifying misaligned\nbehaviors in large language models (LLMs). Existing approaches, however, often\nfocus primarily on improving attack success rates while overlooking the need\nfor comprehensive test case coverage. Additionally, most of these methods are\nlimited to single-turn red teaming, failing to capture the multi-turn dynamics\nof real-world human-machine interactions. To overcome these limitations, we\npropose HARM (Holistic Automated Red teaMing), which scales up the diversity of\ntest cases using a top-down approach based on an extensible, fine-grained risk\ntaxonomy. Our method also leverages a novel fine-tuning strategy and\nreinforcement learning techniques to facilitate multi-turn adversarial probing\nin a human-like manner. Experimental results demonstrate that our framework\nenables a more systematic understanding of model vulnerabilities and offers\nmore targeted guidance for the alignment process.", "published": "2024-09-25 09:44:48", "link": "http://arxiv.org/abs/2409.16783v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "The Role of Language Models in Modern Healthcare: A Comprehensive Review", "abstract": "The application of large language models (LLMs) in healthcare has gained\nsignificant attention due to their ability to process complex medical data and\nprovide insights for clinical decision-making. These models have demonstrated\nsubstantial capabilities in understanding and generating natural language,\nwhich is crucial for medical documentation, diagnostics, and patient\ninteraction. This review examines the trajectory of language models from their\nearly stages to the current state-of-the-art LLMs, highlighting their strengths\nin healthcare applications and discussing challenges such as data privacy,\nbias, and ethical considerations. The potential of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensure their ethical and\neffective integration into medical practice.", "published": "2024-09-25 12:15:15", "link": "http://arxiv.org/abs/2409.16860v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Robotic Backchanneling in Online Conversation Facilitation: A\n  Cross-Generational Study", "abstract": "Japan faces many challenges related to its aging society, including\nincreasing rates of cognitive decline in the population and a shortage of\ncaregivers. Efforts have begun to explore solutions using artificial\nintelligence (AI), especially socially embodied intelligent agents and robots\nthat can communicate with people. Yet, there has been little research on the\ncompatibility of these agents with older adults in various everyday situations.\nTo this end, we conducted a user study to evaluate a robot that functions as a\nfacilitator for a group conversation protocol designed to prevent cognitive\ndecline. We modified the robot to use backchannelling, a natural human way of\nspeaking, to increase receptiveness of the robot and enjoyment of the group\nconversation experience. We conducted a cross-generational study with young\nadults and older adults. Qualitative analyses indicated that younger adults\nperceived the backchannelling version of the robot as kinder, more trustworthy,\nand more acceptable than the non-backchannelling robot. Finally, we found that\nthe robot's backchannelling elicited nonverbal backchanneling in older\nparticipants.", "published": "2024-09-25 13:08:43", "link": "http://arxiv.org/abs/2409.16899v1", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "A Roadmap for Embodied and Social Grounding in LLMs", "abstract": "The fusion of Large Language Models (LLMs) and robotic systems has led to a\ntransformative paradigm in the robotic field, offering unparalleled\ncapabilities not only in the communication domain but also in skills like\nmultimodal input handling, high-level reasoning, and plan generation. The\ngrounding of LLMs knowledge into the empirical world has been considered a\ncrucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless,\nconnecting LLMs' representations to the external world with multimodal\napproaches or with robots' bodies is not enough to let them understand the\nmeaning of the language they are manipulating. Taking inspiration from humans,\nthis work draws attention to three necessary elements for an agent to grasp and\nexperience the world. The roadmap for LLMs grounding is envisaged in an active\nbodily system as the reference point for experiencing the environment, a\ntemporally structured experience for a coherent, self-related interaction with\nthe external world, and social skills to acquire a common-grounded shared\nexperience.", "published": "2024-09-25 13:09:23", "link": "http://arxiv.org/abs/2409.16900v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "I.2.7; I.2.9; J.4; F.3.2; D.3.1"], "primary_category": "cs.RO"}
{"title": "Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech\n  Recognition", "abstract": "This paper addresses the challenge of integrating low-resource languages into\nmultilingual automatic speech recognition (ASR) systems. We introduce a novel\napplication of weighted cross-entropy, typically used for unbalanced datasets,\nto facilitate the integration of low-resource languages into pre-trained\nmultilingual ASR models within the context of continual multilingual learning.\nWe fine-tune the Whisper multilingual ASR model on five high-resource languages\nand one low-resource language, employing language-weighted dynamic\ncross-entropy and data augmentation. The results show a remarkable 6.69% word\nerror rate (WER) reduction for the low-resource language compared to the\nfine-tuned model without applying our approach, and a 48.86% WER reduction\ncompared to the original Whisper model. In addition, our approach yields an\naverage WER reduction of 3.29% across the six languages, showing no degradation\nfor the high-resource languages.", "published": "2024-09-25 14:09:09", "link": "http://arxiv.org/abs/2409.16954v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM\n  Personalization", "abstract": "Large language models (LLMs) have revolutionized how we interact with\ntechnology, but their personalization to individual user preferences remains a\nsignificant challenge, particularly in on-device applications. Traditional\nmethods often depend heavily on labeled datasets and can be resource-intensive.\nTo address these issues, we present Adaptive Self-Supervised Learning\nStrategies (ASLS), which utilizes self-supervised learning techniques to\npersonalize LLMs dynamically. The framework comprises a user profiling layer\nfor collecting interaction data and a neural adaptation layer for real-time\nmodel fine-tuning. This innovative approach enables continuous learning from\nuser feedback, allowing the model to generate responses that align closely with\nuser-specific contexts. The adaptive mechanisms of ASLS minimize computational\ndemands and enhance personalization efficiency. Experimental results across\nvarious user scenarios illustrate the superior performance of ASLS in boosting\nuser engagement and satisfaction, highlighting its potential to redefine LLMs\nas highly responsive and context-aware systems on-device.", "published": "2024-09-25 14:35:06", "link": "http://arxiv.org/abs/2409.16973v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Counterfactual Token Generation in Large Language Models", "abstract": "\"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm\nof her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]\nLyra's eyes welled up with tears as she realized the bitter truth - she had\nsacrificed everything for fleeting riches, and lost the love of her crew, her\nfamily, and herself.\" Although this story, generated by a large language model,\nis captivating, one may wonder -- how would the story have unfolded if the\nmodel had chosen \"Captain Maeve\" as the protagonist instead? We cannot know.\nState-of-the-art large language models are stateless -- they maintain no\ninternal memory or state. Given a prompt, they generate a sequence of tokens as\nan output using an autoregressive process. As a consequence, they cannot reason\nabout counterfactual alternatives to tokens they have generated in the past. In\nthis work, our goal is to enhance them with this functionality. To this end, we\ndevelop a causal model of token generation that builds upon the Gumbel-Max\nstructural causal model. Our model allows any large language model to perform\ncounterfactual token generation at almost no cost in comparison with vanilla\ntoken generation, it is embarrassingly simple to implement, and it does not\nrequire any fine-tuning nor prompt engineering. We implement our model on Llama\n3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a\nquantitative analysis of counterfactually generated text. We conclude with a\ndemonstrative application of counterfactual token generation for bias\ndetection, unveiling interesting insights about the model of the world\nconstructed by large language models.", "published": "2024-09-25 15:30:24", "link": "http://arxiv.org/abs/2409.17027v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not", "abstract": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.", "published": "2024-09-25 15:54:29", "link": "http://arxiv.org/abs/2409.17044v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using LLM for Real-Time Transcription and Summarization of\n  Doctor-Patient Interactions into ePuskesmas in Indonesia", "abstract": "One of the key issues contributing to inefficiency in Puskesmas is the\ntime-consuming nature of doctor-patient interactions. Doctors need to conduct\nthorough consultations, which include diagnosing the patient's condition,\nproviding treatment advice, and transcribing detailed notes into medical\nrecords. In regions with diverse linguistic backgrounds, doctors often have to\nask clarifying questions, further prolonging the process. While diagnosing is\nessential, transcription and summarization can often be automated using AI to\nimprove time efficiency and help doctors enhance care quality and enable early\ndiagnosis and intervention. This paper proposes a solution using a localized\nlarge language model (LLM) to transcribe, translate, and summarize\ndoctor-patient conversations. We utilize the Whisper model for transcription\nand GPT-3 to summarize them into the ePuskemas medical records format. This\nsystem is implemented as an add-on to an existing web browser extension,\nallowing doctors to fill out patient forms while talking. By leveraging this\nsolution for real-time transcription, translation, and summarization, doctors\ncan improve the turnaround time for patient care while enhancing the quality of\nrecords, which become more detailed and insightful for future visits. This\ninnovation addresses challenges like overcrowded facilities and the\nadministrative burden on healthcare providers in Indonesia. We believe this\nsolution will help doctors save time, provide better care, and produce more\naccurate medical records, representing a significant step toward modernizing\nhealthcare and ensuring patients receive timely, high-quality care, even in\nresource-constrained settings.", "published": "2024-09-25 16:13:42", "link": "http://arxiv.org/abs/2409.17054v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale", "abstract": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX", "published": "2024-09-25 17:28:13", "link": "http://arxiv.org/abs/2409.17115v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FineZip : Pushing the Limits of Large Language Models for Practical\n  Lossless Text Compression", "abstract": "While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.", "published": "2024-09-25 17:58:35", "link": "http://arxiv.org/abs/2409.17141v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art\n  Vision-Language Models", "abstract": "Today's most advanced vision-language models (VLMs) remain proprietary. The\nstrongest open-weight models rely heavily on synthetic data from proprietary\nVLMs to achieve good performance, effectively distilling these closed VLMs into\nopen ones. As a result, the community has been missing foundational knowledge\nabout how to build performant VLMs from scratch. We present Molmo, a new family\nof VLMs that are state-of-the-art in their class of openness. Our key\ncontribution is a collection of new datasets called PixMo, including a dataset\nof highly detailed image captions for pre-training, a free-form image Q&A\ndataset for fine-tuning, and an innovative 2D pointing dataset, all collected\nwithout the use of external VLMs. The success of our approach relies on careful\nmodeling choices, a well-tuned training pipeline, and, most critically, the\nquality of our newly collected datasets. Our best-in-class 72B model not only\noutperforms others in the class of open weight and data models, but also\noutperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini\n1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and\non a large human evaluation. Our model weights, new datasets, and source code\nare available at https://molmo.allenai.org/blog.", "published": "2024-09-25 17:59:51", "link": "http://arxiv.org/abs/2409.17146v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language\n  Navigation", "abstract": "This study presents a novel evaluation framework for the Vision-Language\nNavigation (VLN) task. It aims to diagnose current models for various\ninstruction categories at a finer-grained level. The framework is structured\naround the context-free grammar (CFG) of the task. The CFG serves as the basis\nfor the problem decomposition and the core premise of the instruction\ncategories design. We propose a semi-automatic method for CFG construction with\nthe help of Large-Language Models (LLMs). Then, we induct and generate data\nspanning five principal instruction categories (i.e. direction change, landmark\nrecognition, region recognition, vertical movement, and numerical\ncomprehension). Our analysis of different models reveals notable performance\ndiscrepancies and recurrent issues. The stagnation of numerical comprehension,\nheavy selective biases over directional concepts, and other interesting\nfindings contribute to the development of future language-guided navigation\nsystems.", "published": "2024-09-25 19:49:39", "link": "http://arxiv.org/abs/2409.17313v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs\n  with 1000x Input Token Reduction", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long context inputs, but this comes at the cost of increased\ncomputational resources and latency. Our research introduces a novel approach\nfor the long context bottleneck to accelerate LLM inference and reduce GPU\nmemory consumption. Our research demonstrates that LLMs can identify relevant\ntokens in the early layers before generating answers to a query. Leveraging\nthis insight, we propose an algorithm that uses early layers of an LLM as\nfilters to select and compress input tokens, significantly reducing the context\nlength for subsequent processing. Our method, GemFilter, demonstrates\nsubstantial improvements in both speed and memory efficiency compared to\nexisting techniques, such as standard attention and SnapKV/H2O. Notably, it\nachieves a 2.4$\\times$ speedup and 30\\% reduction in GPU memory usage compared\nto SOTA methods. Evaluation on the Needle in a Haystack task shows that\nGemFilter significantly outperforms standard attention, SnapKV and demonstrates\ncomparable performance on the LongBench challenge. GemFilter is simple,\ntraining-free, and broadly applicable across different LLMs. Crucially, it\nprovides interpretability by allowing humans to inspect the selected input\nsequence. These findings not only offer practical benefits for LLM deployment,\nbut also enhance our understanding of LLM internal mechanisms, paving the way\nfor further optimizations in LLM design and inference. Our code is available at\n\\url{https://github.com/SalesforceAIResearch/GemFilter}.", "published": "2024-09-25 23:14:47", "link": "http://arxiv.org/abs/2409.17422v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Application of AI-based Models for Online Fraud Detection and Analysis", "abstract": "Fraud is a prevalent offence that extends beyond financial loss, causing\npsychological and physical harm to victims. The advancements in online\ncommunication technologies alowed for online fraud to thrive in this vast\nnetwork, with fraudsters increasingly using these channels for deception. With\nthe progression of technologies like AI, there is a growing concern that fraud\nwill scale up, using sophisticated methods, like deep-fakes in phishing\ncampaigns, all generated by language generation models like ChatGPT. However,\nthe application of AI in detecting and analyzing online fraud remains\nunderstudied. We conduct a Systematic Literature Review on AI and NLP\ntechniques for online fraud detection. The review adhered the PRISMA-ScR\nprotocol, with eligibility criteria including relevance to online fraud, use of\ntext data, and AI methodologies. We screened 2,457 academic records, 350 met\nour eligibility criteria, and included 223. We report the state-of-the-art NLP\ntechniques for analysing various online fraud categories; the training data\nsources; the NLP algorithms and models built; and the performance metrics\nemployed for model evaluation. We find that current research on online fraud is\ndivided into various scam activitiesand identify 16 different frauds that\nresearchers focus on. This SLR enhances the academic understanding of AI-based\ndetection methods for online fraud and offers insights for policymakers, law\nenforcement, and businesses on safeguarding against such activities. We\nconclude that focusing on specific scams lacks generalization, as multiple\nmodels are required for different fraud types. The evolving nature of scams\nlimits the effectiveness of models trained on outdated data. We also identify\nissues in data limitations, training bias reporting, and selective presentation\nof metrics in model performance reporting, which can lead to potential biases\nin model evaluation.", "published": "2024-09-25 14:47:03", "link": "http://arxiv.org/abs/2409.19022v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient\n  Modification", "abstract": "Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed\nthe success of prompt tuning and adapter tuning, while the classic model\nfine-tuning on inherent parameters seems to be overlooked. It is believed that\nfine-tuning the parameters of VLMs with few-shot samples corrupts the\npre-trained knowledge since fine-tuning the CLIP model even degrades\nperformance. In this paper, we revisit this viewpoint, and propose a new\nperspective: fine-tuning the specific parameters instead of all will uncover\nthe power of classic model fine-tuning on VLMs. Through our meticulous study,\nwe propose ClipFit, a simple yet effective method to fine-tune CLIP without\nintroducing any overhead of extra parameters. We demonstrate that by only\nfine-tuning the specific bias terms and normalization layers, ClipFit can\nimprove the performance of zero-shot CLIP by 7.27\\% average harmonic mean\naccuracy. Lastly, to understand how fine-tuning in CLIPFit affects the\npre-trained models, we conducted extensive experimental analyses w.r.t. changes\nin internal parameters and representations. We found that low-level text bias\nlayers and the first layer normalization layer change much more than other\nlayers. The code is available at \\url{https://github.com/minglllli/CLIPFit}.", "published": "2024-09-25 08:07:18", "link": "http://arxiv.org/abs/2409.16718v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Cross-lingual Speech Emotion Recognition: Humans vs. Self-Supervised\n  Models", "abstract": "Utilizing Self-Supervised Learning (SSL) models for Speech Emotion\nRecognition (SER) has proven effective, yet limited research has explored\ncross-lingual scenarios. This study presents a comparative analysis between\nhuman performance and SSL models, beginning with a layer-wise analysis and an\nexploration of parameter-efficient fine-tuning strategies in monolingual,\ncross-lingual, and transfer learning contexts. We further compare the SER\nability of models and humans at both utterance- and segment-levels.\nAdditionally, we investigate the impact of dialect on cross-lingual SER through\nhuman evaluation. Our findings reveal that models, with appropriate knowledge\ntransfer, can adapt to the target language and achieve performance comparable\nto native speakers. We also demonstrate the significant effect of dialect on\nSER for individuals without prior linguistic and paralinguistic background.\nMoreover, both humans and models exhibit distinct behaviors across different\nemotions. These results offer new insights into the cross-lingual SER\ncapabilities of SSL models, underscoring both their similarities to and\ndifferences from human emotion perception.", "published": "2024-09-25 13:27:17", "link": "http://arxiv.org/abs/2409.16920v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-Supervised Cognitive State Classification from Speech with\n  Multi-View Pseudo-Labeling", "abstract": "The lack of labeled data is a common challenge in speech classification\ntasks, particularly those requiring extensive subjective assessment, such as\ncognitive state classification. In this work, we propose a Semi-Supervised\nLearning (SSL) framework, introducing a novel multi-view pseudo-labeling method\nthat leverages both acoustic and linguistic characteristics to select the most\nconfident data for training the classification model. Acoustically, unlabeled\ndata are compared to labeled data using the Frechet audio distance, calculated\nfrom embeddings generated by multiple audio encoders. Linguistically, large\nlanguage models are prompted to revise automatic speech recognition\ntranscriptions and predict labels based on our proposed task-specific\nknowledge. High-confidence data are identified when pseudo-labels from both\nsources align, while mismatches are treated as low-confidence data. A bimodal\nclassifier is then trained to iteratively label the low-confidence data until a\npredefined criterion is met. We evaluate our SSL framework on emotion\nrecognition and dementia detection tasks. Experimental results demonstrate that\nour method achieves competitive performance compared to fully supervised\nlearning using only 30% of the labeled data and significantly outperforms two\nselected baselines.", "published": "2024-09-25 13:51:19", "link": "http://arxiv.org/abs/2409.16937v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles", "abstract": "Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by deliberative democracy, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.", "published": "2024-09-25 17:38:39", "link": "http://arxiv.org/abs/2409.17213v6", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.", "published": "2024-09-25 18:35:45", "link": "http://arxiv.org/abs/2409.17270v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Incorporating Spatial Cues in Modular Speaker Diarization for\n  Multi-channel Multi-party Meetings", "abstract": "Although fully end-to-end speaker diarization systems have made significant\nprogress in recent years, modular systems often achieve superior results in\nreal-world scenarios due to their greater adaptability and robustness.\nHistorically, modular speaker diarization methods have seldom discussed how to\nleverage spatial cues from multi-channel speech. This paper proposes a\nthree-stage modular system to enhance single-channel neural speaker diarization\nsystems and recognition performance by utilizing spatial cues from\nmulti-channel speech to provide more accurate initialization for each stage of\nneural speaker diarization (NSD) decoding: (1) Overlap detection and continuous\nspeech separation (CSS) on multi-channel speech are used to obtain cleaner\nsingle speaker speech segments for clustering, followed by the first NSD\ndecoding pass. (2) The results from the first pass initialize a complex Angular\nCentral Gaussian Mixture Model (cACGMM) to estimate speaker-wise masks on\nmulti-channel speech, and through Overlap-add and Mask-to-VAD, achieve\ninitialization with lower speaker error (SpkErr), followed by the second NSD\ndecoding pass. (3) The second decoding results are used for guided source\nseparation (GSS), recognizing and filtering short segments containing less one\nword to obtain cleaner speech segments, followed by re-clustering and the final\nNSD decoding pass. We presented the progressively explored evaluation results\nfrom the CHiME-8 NOTSOFAR-1 (Natural Office Talkers in Settings Of Far-field\nAudio Recordings) challenge, demonstrating the effectiveness of our system and\nits contribution to improving recognition performance. Our final system\nachieved the first place in the challenge.", "published": "2024-09-25 10:49:14", "link": "http://arxiv.org/abs/2409.16803v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MT2KD: Towards A General-Purpose Encoder for Speech, Speaker, and Audio\n  Events", "abstract": "With the advances in deep learning, the performance of end-to-end (E2E)\nsingle-task models for speech and audio processing has been constantly\nimproving. However, it is still challenging to build a general-purpose model\nwith high performance on multiple tasks, since different speech and audio\nprocessing tasks usually require different training data, input features, or\nmodel architectures to achieve optimal performance. In this work, MT2KD, a\nnovel two-stage multi-task learning framework is proposed to build a\ngeneral-purpose speech and audio encoder that jointly performs three\nfundamental tasks: automatic speech recognition (ASR), audio tagging (AT) and\nspeaker verification (SV). In the first stage, multi-teacher knowledge\ndistillation (KD) is applied to align the feature spaces of three single-task\nhigh-performance teacher encoders into a single student encoder using the same\nunlabelled data. In the second stage, multi-task supervised fine-tuning is\ncarried out by initialising the model from the first stage and training on the\nseparate labelled data of each single task. Experiments demonstrate that the\nproposed multi-task training pipeline significantly outperforms a baseline\nmodel trained with multi-task learning from scratch. The final system achieves\ngood performance on ASR, AT and SV: with less than 4% relative word-error-rate\nincrease on ASR, only 1.9 lower mean averaged precision on AT and 0.23%\nabsolute higher equal error rate on SV compared to the best-performing\nsingle-task encoders, using only a 66M total model parameters.", "published": "2024-09-25 15:15:42", "link": "http://arxiv.org/abs/2409.17010v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring synthetic data for cross-speaker style transfer in style\n  representation based TTS", "abstract": "Incorporating cross-speaker style transfer in text-to-speech (TTS) models is\nchallenging due to the need to disentangle speaker and style information in\naudio. In low-resource expressive data scenarios, voice conversion (VC) can\ngenerate expressive speech for target speakers, which can then be used to train\nthe TTS model. However, the quality and style transfer ability of the VC model\nare crucial for the overall TTS model quality. In this work, we explore the use\nof synthetic data generated by a VC model to assist the TTS model in\ncross-speaker style transfer tasks. Additionally, we employ pre-training of the\nstyle encoder using timbre perturbation and prototypical angular loss to\nmitigate speaker leakage. Our results show that using VC synthetic data can\nimprove the naturalness and speaker similarity of TTS in cross-speaker\nscenarios. Furthermore, we extend this approach to a cross-language scenario,\nenhancing accent transfer.", "published": "2024-09-25 21:16:15", "link": "http://arxiv.org/abs/2409.17364v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in\n  Any-to-One Voice Conversion", "abstract": "The creation of artificial polyglot voices remains a challenging task,\ndespite considerable progress in recent years. This paper investigates\nself-supervised learning for voice conversion to create native-sounding\npolyglot voices. We introduce a novel cross-lingual any-to-one voice conversion\nsystem that is able to preserve the source accent without the need for\nmultilingual data from the target speaker. In addition, we show a novel\ncross-lingual fine-tuning strategy that further improves the accent and reduces\nthe training data requirements. Objective and subjective evaluations with\nEnglish, Spanish, French and Mandarin Chinese confirm that our approach\nimproves on state-of-the-art methods, enhancing the speech intelligibility and\noverall quality of the converted speech, especially in cross-lingual scenarios.\nAudio samples are available at https://giuseppe-ruggiero.github.io/a2o-vc-demo/", "published": "2024-09-25 22:02:54", "link": "http://arxiv.org/abs/2409.17387v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Effect of Perceptual Metrics on Music Representation Learning for\n  Genre Classification", "abstract": "The subjective quality of natural signals can be approximated with objective\nperceptual metrics. Designed to approximate the perceptual behaviour of human\nobservers, perceptual metrics often reflect structures found in natural signals\nand neurological pathways. Models trained with perceptual metrics as loss\nfunctions can capture perceptually meaningful features from the structures held\nwithin these metrics. We demonstrate that using features extracted from\nautoencoders trained with perceptual losses can improve performance on music\nunderstanding tasks, i.e. genre classification, over using these metrics\ndirectly as distances when learning a classifier. This result suggests improved\ngeneralisation to novel signals when using perceptual metrics as loss functions\nfor representation learning.", "published": "2024-09-25 16:29:21", "link": "http://arxiv.org/abs/2409.17069v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
