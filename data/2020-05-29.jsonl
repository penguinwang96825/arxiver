{"title": "Neural Simultaneous Speech Translation Using Alignment-Based Chunking", "abstract": "In simultaneous machine translation, the objective is to determine when to\nproduce a partial translation given a continuous stream of source words, with a\ntrade-off between latency and quality. We propose a neural machine translation\n(NMT) model that makes dynamic decisions when to continue feeding on input or\ngenerate output words. The model is composed of two main components: one to\ndynamically decide on ending a source chunk, and another that translates the\nconsumed chunk. We train the components jointly and in a manner consistent with\nthe inference conditions. To generate chunked training data, we propose a\nmethod that utilizes word alignment while also preserving enough context. We\ncompare models with bidirectional and unidirectional encoders of different\ndepths, both on real speech and text input. Our results on the IWSLT 2020\nEnglish-to-German task outperform a wait-k baseline by 2.6 to 3.7% BLEU\nabsolute.", "published": "2020-05-29 10:20:48", "link": "http://arxiv.org/abs/2005.14489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massive Choice, Ample Tasks (MaChAmp): A Toolkit for Multi-task Learning\n  in NLP", "abstract": "Transfer learning, particularly approaches that combine multi-task learning\nwith pre-trained contextualized embeddings and fine-tuning, have advanced the\nfield of Natural Language Processing tremendously in recent years. In this\npaper we present MaChAmp, a toolkit for easy fine-tuning of contextualized\nembeddings in multi-task settings. The benefits of MaChAmp are its flexible\nconfiguration options, and the support of a variety of natural language\nprocessing tasks in a uniform toolkit, from text classification and sequence\nlabeling to dependency parsing, masked language modeling, and text generation.", "published": "2020-05-29 16:54:50", "link": "http://arxiv.org/abs/2005.14672v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Deep Learning Approaches for Hate Speech Detection in\n  Social Media", "abstract": "The phenomenal growth on the internet has helped in empowering individual's\nexpressions, but the misuse of freedom of expression has also led to the\nincrease of various cyber crimes and anti-social activities. Hate speech is one\nsuch issue that needs to be addressed very seriously as otherwise, this could\npose threats to the integrity of the social fabrics.\n  In this paper, we proposed deep learning approaches utilizing various\nembeddings for detecting various types of hate speeches in social media.\nDetecting hate speech from a large volume of text, especially tweets which\ncontains limited contextual information also poses several practical\nchallenges.\n  Moreover, the varieties in user-generated data and the presence of various\nforms of hate speech makes it very challenging to identify the degree and\nintention of the message. Our experiments on three publicly available datasets\nof different domains shows a significant improvement in accuracy and F1-score.", "published": "2020-05-29 17:28:46", "link": "http://arxiv.org/abs/2005.14690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Leaderboards: A survey of methods for revealing weaknesses in\n  Natural Language Inference data and models", "abstract": "Recent years have seen a growing number of publications that analyse Natural\nLanguage Inference (NLI) datasets for superficial cues, whether they undermine\nthe complexity of the tasks underlying those datasets and how they impact those\nmodels that are optimised and evaluated on this data. This structured survey\nprovides an overview of the evolving research area by categorising reported\nweaknesses in models and datasets and the methods proposed to reveal and\nalleviate those weaknesses for the English language. We summarise and discuss\nthe findings and conclude with a set of recommendations for possible future\nresearch directions. We hope it will be a useful resource for researchers who\npropose new datasets, to have a set of tools to assess the suitability and\nquality of their data to evaluate various phenomena of interest, as well as\nthose who develop novel architectures, to further understand the implications\nof their improvements with respect to their model's acquired capabilities.", "published": "2020-05-29 17:55:57", "link": "http://arxiv.org/abs/2005.14709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Importance of Suppressing Domain Style in Authorship Analysis", "abstract": "The prerequisite of many approaches to authorship analysis is a\nrepresentation of writing style. But despite decades of research, it still\nremains unclear to what extent commonly used and widely accepted\nrepresentations like character trigram frequencies actually represent an\nauthor's writing style, in contrast to more domain-specific style components or\neven topic. We address this shortcoming for the first time in a novel\nexperimental setup of fixed authors but swapped domains between training and\ntesting. With this setup, we reveal that approaches using character trigram\nfeatures are highly susceptible to favor domain information when applied\nwithout attention to domains, suffering drops of up to 55.4 percentage points\nin classification accuracy under domain swapping. We further propose a new\nremedy based on domain-adversarial learning and compare it to ones from the\nliterature based on heuristic rules. Both can work well, reducing accuracy\nlosses under domain swapping to 3.6% and 3.9%, respectively.", "published": "2020-05-29 17:58:19", "link": "http://arxiv.org/abs/2005.14714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Lexical Substitution Approaches based on Neural\n  Language Models", "abstract": "Lexical substitution in context is an extremely powerful technology that can\nbe used as a backbone of various NLP applications, such as word sense\ninduction, lexical relation extraction, data augmentation, etc. In this paper,\nwe present a large-scale comparative study of popular neural language and\nmasked language models (LMs and MLMs), such as context2vec, ELMo, BERT, XLNet,\napplied to the task of lexical substitution. We show that already competitive\nresults achieved by SOTA LMs/MLMs can be further improved if information about\nthe target word is injected properly, and compare several target injection\nmethods. In addition, we provide analysis of the types of semantic relations\nbetween the target and substitutes generated by different models providing\ninsights into what kind of words are really generated or given by annotators as\nsubstitutes.", "published": "2020-05-29 18:43:22", "link": "http://arxiv.org/abs/2006.00031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Explainable Opinion Graphs from Review", "abstract": "The Web is a major resource of both factual and subjective information. While\nthere are significant efforts to organize factual information into knowledge\nbases, there is much less work on organizing opinions, which are abundant in\nsubjective data, into a structured format.\n  We present ExplainIt, a system that extracts and organizes opinions into an\nopinion graph, which are useful for downstream applications such as generating\nexplainable review summaries and facilitating search over opinion phrases. In\nsuch graphs, a node represents a set of semantically similar opinions extracted\nfrom reviews and an edge between two nodes signifies that one node explains the\nother. ExplainIt mines explanations in a supervised method and groups similar\nopinions together in a weakly supervised way before combining the clusters of\nopinions together with their explanation relationships into an opinion graph.\nWe experimentally demonstrate that the explanation relationships generated in\nthe opinion graph are of good quality and our labeled datasets for explanation\nmining and grouping opinions are publicly available.", "published": "2020-05-29 23:11:48", "link": "http://arxiv.org/abs/2006.00119v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Multilingual Machine Translation by Alternately Freezing\n  Language-Specific Encoders-Decoders", "abstract": "We propose a modular architecture of language-specific encoder-decoders that\nconstitutes a multilingual machine translation system that can be incrementally\nextended to new languages without the need for retraining the existing system\nwhen adding new languages. Differently from previous works, we simultaneously\ntrain $N$ languages in all translation directions by alternately freezing\nencoder or decoder modules, which indirectly forces the system to train in a\ncommon intermediate representation for all languages. Experimental results from\nmultilingual machine translation show that we can successfully train this\nmodular architecture improving on the initial languages while falling slightly\nbehind when adding new languages or doing zero-shot translation. Additional\ncomparison of the quality of sentence representation in the task of natural\nlanguage inference shows that the alternately freezing training is also\nbeneficial in this direction.", "published": "2020-05-29 19:00:59", "link": "http://arxiv.org/abs/2006.01594v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Controlling Length in Image Captioning", "abstract": "We develop and evaluate captioning models that allow control of caption\nlength. Our models can leverage this control to generate captions of different\nstyle and descriptiveness.", "published": "2020-05-29 05:03:15", "link": "http://arxiv.org/abs/2005.14386v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Harbsafe-162. A Domain-Specific Data Set for the Intrinsic Evaluation of\n  Semantic Representations for Terminological Data", "abstract": "The article presents Harbsafe-162, a domain-specific data set for evaluating\ndistributional semantic models. It originates from a cooperation by Technische\nUniversit\\\"at Braunschweig and the German Commission for Electrical, Electronic\n& Information Technologies of DIN and VDE, the Harbsafe project. One objective\nof the project is to apply distributional semantic models to terminological\nentries, that is, complex lexical data comprising of at least one or several\nterms, term phrases and a definition. This application is needed to solve a\nmore complex problem: the harmonization of terminologies of standards and\nstandards bodies (i.e. resolution of doublettes and inconsistencies). Due to a\nlack of evaluation data sets for terminological entries, the creation of\nHarbsafe-162 was a necessary step towards harmonization assistance.\nHarbsafe-162 covers data from nine electrotechnical standards in the domain of\nfunctional safety, IT security, and dependability. An intrinsic evaluation\nmethod in the form of a similarity rating task has been applied in which two\nlinguists and three domain experts from standardization participated. The data\nset is used to evaluate a specific implementation of an established sentence\nembedding model. This implementation proves to be satisfactory for the\ndomain-specific data so that further implementations for harmonization\nassistance may be brought forward by the project. Considering recent criticism\non intrinsic evaluation methods, the article concludes with an evaluation of\nHarbsafe-162 and joins a more general discussion about the nature of similarity\nrating tasks. Harbsafe-162 has been made available for the community.", "published": "2020-05-29 13:56:31", "link": "http://arxiv.org/abs/2005.14576v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Improving Unsupervised Sparsespeech Acoustic Models with Categorical\n  Reparameterization", "abstract": "The Sparsespeech model is an unsupervised acoustic model that can generate\ndiscrete pseudo-labels for untranscribed speech. We extend the Sparsespeech\nmodel to allow for sampling over a random discrete variable, yielding\npseudo-posteriorgrams. The degree of sparsity in this posteriorgram can be\nfully controlled after the model has been trained. We use the Gumbel-Softmax\ntrick to approximately sample from a discrete distribution in the neural\nnetwork and this allows us to train the network efficiently with standard\nbackpropagation. The new and improved model is trained and evaluated on the\nLibri-Light corpus, a benchmark for ASR with limited or no supervision. The\nmodel is trained on 600h and 6000h of English read speech. We evaluate the\nimproved model using the ABX error measure and a semi-supervised setting with\n10h of transcribed speech. We observe a relative improvement of up to 31.4% on\nABX error rates across speakers on the test set with the improved Sparsespeech\nmodel on 600h of speech data and further improvements when we scale the model\nto 6000h.", "published": "2020-05-29 13:58:36", "link": "http://arxiv.org/abs/2005.14578v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Using Large Pretrained Language Models for Answering User Queries from\n  Product Specifications", "abstract": "While buying a product from the e-commerce websites, customers generally have\na plethora of questions. From the perspective of both the e-commerce service\nprovider as well as the customers, there must be an effective question\nanswering system to provide immediate answers to the user queries. While\ncertain questions can only be answered after using the product, there are many\nquestions which can be answered from the product specification itself. Our work\ntakes a first step in this direction by finding out the relevant product\nspecifications, that can help answering the user questions. We propose an\napproach to automatically create a training dataset for this problem. We\nutilize recently proposed XLNet and BERT architectures for this problem and\nfind that they provide much better performance than the Siamese model,\npreviously applied for this problem. Our model gives a good performance even\nwhen trained on one vertical and tested across different verticals.", "published": "2020-05-29 14:52:33", "link": "http://arxiv.org/abs/2005.14613v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SLAM-Inspired Simultaneous Contextualization and Interpreting for\n  Incremental Conversation Sentences", "abstract": "Distributed representation of words has improved the performance for many\nnatural language tasks. In many methods, however, only one meaning is\nconsidered for one label of a word, and multiple meanings of polysemous words\ndepending on the context are rarely handled. Although research works have dealt\nwith polysemous words, they determine the meanings of such words according to a\nbatch of large documents. Hence, there are two problems with applying these\nmethods to sequential sentences, as in a conversation that contains ambiguous\nexpressions. The first problem is that the methods cannot sequentially deal\nwith the interdependence between context and word interpretation, in which\ncontext is decided by word interpretations and the word interpretations are\ndecided by the context. Context estimation must thus be performed in parallel\nto pursue multiple interpretations. The second problem is that the previous\nmethods use large-scale sets of sentences for offline learning of new\ninterpretations, and the steps of learning and inference are clearly separated.\nSuch methods using offline learning cannot obtain new interpretations during a\nconversation. Hence, to dynamically estimate the conversation context and\ninterpretations of polysemous words in sequential sentences, we propose a\nmethod of Simultaneous Contextualization And INterpreting (SCAIN) based on the\ntraditional Simultaneous Localization And Mapping (SLAM) algorithm. By using\nthe SCAIN algorithm, we can sequentially optimize the interdependence between\ncontext and word interpretation while obtaining new interpretations online. For\nexperimental evaluation, we created two datasets: one from Wikipedia's\ndisambiguation pages and the other from real conversations. For both datasets,\nthe results confirmed that SCAIN could effectively achieve sequential\noptimization of the interdependence and acquisition of new interpretations.", "published": "2020-05-29 16:40:27", "link": "http://arxiv.org/abs/2005.14662v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stance Prediction for Contemporary Issues: Data and Experiments", "abstract": "We investigate whether pre-trained bidirectional transformers with sentiment\nand emotion information improve stance detection in long discussions of\ncontemporary issues. As a part of this work, we create a novel stance detection\ndataset covering 419 different controversial issues and their related pros and\ncons collected by procon.org in nonpartisan format. Experimental results show\nthat a shallow recurrent neural network with sentiment or emotion information\ncan reach competitive results compared to fine-tuned BERT with 20x fewer\nparameters. We also use a simple approach that explains which input phrases\ncontribute to stance detection.", "published": "2020-05-29 19:54:07", "link": "http://arxiv.org/abs/2006.00052v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A frame semantics based approach to comparative study of digitized\n  corpus", "abstract": "in this paper, we present a corpus linguistics based approach applied to\nanalyzing digitized classical multilingual novels and narrative texts, from a\nsemantic point of view. Digitized novels such as \"the hobbit (Tolkien J. R. R.,\n1937)\" and \"the hound of the Baskervilles (Doyle A. C. 1901-1902)\", which were\nwidely translated to dozens of languages, provide rich materials for analyzing\nlanguages differences from several perspectives and within a number of\ndisciplines like linguistics, philosophy and cognitive science. Taking motion\nevents conceptualization as a case study, this paper, focus on the morphologic,\nsyntactic, and semantic annotation process of English-Arabic aligned corpus\ncreated from a digitized novels, in order to re-examine the linguistic\nencodings of motion events in English and Arabic in terms of Frame Semantics.\nThe present study argues that differences in motion events conceptualization\nacross languages can be described with frame structure and frame-to-frame\nrelations.", "published": "2020-05-29 22:56:25", "link": "http://arxiv.org/abs/2006.00113v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Noise Robust Named Entity Understanding for Voice Assistants", "abstract": "Named Entity Recognition (NER) and Entity Linking (EL) play an essential role\nin voice assistant interaction, but are challenging due to the special\ndifficulties associated with spoken user queries. In this paper, we propose a\nnovel architecture that jointly solves the NER and EL tasks by combining them\nin a joint reranking module. We show that our proposed framework improves NER\naccuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features\nused also lead to better accuracies in other natural language understanding\ntasks, such as domain classification and semantic parsing.", "published": "2020-05-29 06:14:53", "link": "http://arxiv.org/abs/2005.14408v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SAFER: A Structure-free Approach for Certified Robustness to Adversarial\n  Word Substitutions", "abstract": "State-of-the-art NLP models can often be fooled by human-unaware\ntransformations such as synonymous word substitution. For security reasons, it\nis of critical importance to develop models with certified robustness that can\nprovably guarantee that the prediction is can not be altered by any possible\nsynonymous word substitution. In this work, we propose a certified robust\nmethod based on a new randomized smoothing technique, which constructs a\nstochastic ensemble by applying random word substitutions on the input\nsentences, and leverage the statistical properties of the ensemble to provably\ncertify the robustness. Our method is simple and structure-free in that it only\nrequires the black-box queries of the model outputs, and hence can be applied\nto any pre-trained models (such as BERT) and any types of models (world-level\nor subword-level). Our method significantly outperforms recent state-of-the-art\nmethods for certified robustness on both IMDB and Amazon text classification\ntasks. To the best of our knowledge, we are the first work to achieve certified\nrobustness on large systems such as BERT with practically meaningful certified\naccuracy.", "published": "2020-05-29 07:15:19", "link": "http://arxiv.org/abs/2005.14424v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sub-Band Knowledge Distillation Framework for Speech Enhancement", "abstract": "In single-channel speech enhancement, methods based on full-band spectral\nfeatures have been widely studied. However, only a few methods pay attention to\nnon-full-band spectral features. In this paper, we explore a knowledge\ndistillation framework based on sub-band spectral mapping for single-channel\nspeech enhancement. Specifically, we divide the full frequency band into\nmultiple sub-bands and pre-train an elite-level sub-band enhancement model\n(teacher model) for each sub-band. These teacher models are dedicated to\nprocessing their own sub-bands. Next, under the teacher models' guidance, we\ntrain a general sub-band enhancement model (student model) that works for all\nsub-bands. Without increasing the number of model parameters and computational\ncomplexity, the student model's performance is further improved. To evaluate\nour proposed method, we conducted a large number of experiments on an\nopen-source data set. The final experimental results show that the guidance\nfrom the elite-level teacher models dramatically improves the student model's\nperformance, which exceeds the full-band model by employing fewer parameters.", "published": "2020-05-29 07:55:12", "link": "http://arxiv.org/abs/2005.14435v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SNR-Based Teachers-Student Technique for Speech Enhancement", "abstract": "It is very challenging for speech enhancement methods to achieves robust\nperformance under both high signal-to-noise ratio (SNR) and low SNR\nsimultaneously. In this paper, we propose a method that integrates an SNR-based\nteachers-student technique and time-domain U-Net to deal with this problem.\nSpecifically, this method consists of multiple teacher models and a student\nmodel. We first train the teacher models under multiple small-range SNRs that\ndo not coincide with each other so that they can perform speech enhancement\nwell within the specific SNR range. Then, we choose different teacher models to\nsupervise the training of the student model according to the SNR of the\ntraining data. Eventually, the student model can perform speech enhancement\nunder both high SNR and low SNR. To evaluate the proposed method, we\nconstructed a dataset with an SNR ranging from -20dB to 20dB based on the\npublic dataset. We experimentally analyzed the effectiveness of the SNR-based\nteachers-student technique and compared the proposed method with several\nstate-of-the-art methods.", "published": "2020-05-29 08:13:01", "link": "http://arxiv.org/abs/2005.14441v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analyzing COVID-19 on Online Social Media: Trends, Sentiments and\n  Emotions", "abstract": "At the time of writing, the ongoing pandemic of coronavirus disease\n(COVID-19) has caused severe impacts on society, economy and people's daily\nlives. People constantly express their opinions on various aspects of the\npandemic on social media, making user-generated content an important source for\nunderstanding public emotions and concerns. In this paper, we perform a\ncomprehensive analysis on the affective trajectories of the American people and\nthe Chinese people based on Twitter and Weibo posts between January 20th, 2020\nand May 11th 2020. Specifically, by identifying people's sentiments, emotions\n(i.e., anger, disgust, fear, happiness, sadness, surprise) and the emotional\ntriggers (e.g., what a user is angry/sad about) we are able to depict the\ndynamics of public affect in the time of COVID-19. By contrasting two very\ndifferent countries, China and the Unites States, we reveal sharp differences\nin people's views on COVID-19 in different cultures. Our study provides a\ncomputational approach to unveiling public emotions and concerns on the\npandemic in real-time, which would potentially help policy-makers better\nunderstand people's need and thus make optimal policy.", "published": "2020-05-29 09:24:38", "link": "http://arxiv.org/abs/2005.14464v3", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Detection of Bangla Fake News using MNB and SVM Classifier", "abstract": "Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.", "published": "2020-05-29 15:38:54", "link": "http://arxiv.org/abs/2005.14627v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prosody leaks into the memories of words", "abstract": "The average predictability (aka informativity) of a word in context has been\nshown to condition word duration (Seyfarth, 2014). All else being equal, words\nthat tend to occur in more predictable environments are shorter than words that\ntend to occur in less predictable environments. One account of the\ninformativity effect on duration is that the acoustic details of probabilistic\nreduction are stored as part of a word's mental representation. Other research\nhas argued that predictability effects are tied to prosodic structure in\nintegral ways. With the aim of assessing a potential prosodic basis for\ninformativity effects in speech production, this study extends past work in two\ndirections; it investigated informativity effects in another large language,\nMandarin Chinese, and broadened the study beyond word duration to additional\nacoustic dimensions, pitch and intensity, known to index prosodic prominence.\nThe acoustic information of content words was extracted from a large telephone\nconversation speech corpus with over 400,000 tokens and 6,000 word types spoken\nby 1,655 individuals and analyzed for the effect of informativity using\nfrequency statistics estimated from a 431 million word subtitle corpus. Results\nindicated that words with low informativity have shorter durations, replicating\nthe effect found in English. In addition, informativity had significant effects\non maximum pitch and intensity, two phonetic dimensions related to prosodic\nprominence. Extending this interpretation, these results suggest that\npredictability is closely linked to prosodic prominence, and that the lexical\nrepresentation of a word includes phonetic details associated with its average\nprosodic prominence in discourse. In other words, the lexicon absorbs prosodic\ninfluences on speech production.", "published": "2020-05-29 17:58:33", "link": "http://arxiv.org/abs/2005.14716v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.6.5; J.4; J.5"], "primary_category": "cs.CL"}
{"title": "Design and Implementation of a Virtual 3D Educational Environment to\n  improve Deaf Education", "abstract": "Advances in NLP, knowledge representation and computer graphic technologies\ncan provide us insights into the development of educational tool for Deaf\npeople. Actual education materials and tools for deaf pupils present several\nproblems, since textbooks are designed to support normal students in the\nclassroom and most of them are not suitable for people with hearing\ndisabilities. Virtual Reality (VR) technologies appear to be a good tool and a\npromising framework in the education of pupils with hearing disabilities. In\nthis paper, we present a current research tasks surrounding the design and\nimplementation of a virtual 3D educational environment based on X3D and H-Anim\nstandards. The system generates and animates automatically Sign language\nsentence from a semantic representation that encode the whole meaning of the\nArabic input text. Some aspects and issues in Sign language generation will be\ndiscussed, including the model of Sign representation that facilitate reuse and\nreduces the time of Sign generation, conversion of semantic components to sign\nfeatures representation with regard to Sign language linguistics\ncharacteristics and how to generate realistic smooth gestural sequences using\nX3D content to performs transition between signs for natural-looking of\nanimated avatar. Sign language sentences were evaluated by Algerian native Deaf\npeople. The goal of the project is the development of a machine translation\nsystem from Arabic to Algerian Sign Language that can be used as educational\ntool for Deaf children in algerian primary schools.", "published": "2020-05-29 22:56:43", "link": "http://arxiv.org/abs/2006.00114v1", "categories": ["cs.CL", "cs.GR", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Deep Job Understanding at LinkedIn", "abstract": "As the world's largest professional network, LinkedIn wants to create\neconomic opportunity for everyone in the global workforce. One of its most\ncritical missions is matching jobs with processionals. Improving job targeting\naccuracy and hire efficiency align with LinkedIn's Member First Motto. To\nachieve those goals, we need to understand unstructured job postings with noisy\ninformation. We applied deep transfer learning to create domain-specific job\nunderstanding models. After this, jobs are represented by professional\nentities, including titles, skills, companies, and assessment questions. To\ncontinuously improve LinkedIn's job understanding ability, we designed an\nexpert feedback loop where we integrated job understanding models into\nLinkedIn's products to collect job posters' feedback. In this demonstration, we\npresent LinkedIn's job posting flow and demonstrate how the integrated deep job\nunderstanding work improves job posters' satisfaction and provides significant\nmetric lifts in LinkedIn's job recommendation system.", "published": "2020-05-29 20:04:59", "link": "http://arxiv.org/abs/2006.12425v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI", "I.2.4; I.2.7; I.2.8"], "primary_category": "cs.IR"}
{"title": "Acoustic scene classification in DCASE 2020 Challenge: generalization\n  across devices and low complexity solutions", "abstract": "This paper presents the details of Task 1: Acoustic Scene Classification in\nthe DCASE 2020 Challenge. The task consists of two subtasks: classification of\ndata from multiple devices, requiring good generalization properties, and\nclassification using low-complexity solutions. Here we describe the datasets\nand baseline systems. After the challenge submission deadline, challenge\nresults and analysis of the submissions will be added.", "published": "2020-05-29 15:30:06", "link": "http://arxiv.org/abs/2005.14623v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The INESC-ID Multi-Modal System for the ADReSS 2020 Challenge", "abstract": "This paper describes a multi-modal approach for the automatic detection of\nAlzheimer's disease proposed in the context of the INESC-ID Human Language\nTechnology Laboratory participation in the ADReSS 2020 challenge. Our\nclassification framework takes advantage of both acoustic and textual feature\nembeddings, which are extracted independently and later combined. Speech\nsignals are encoded into acoustic features using DNN speaker embeddings\nextracted from pre-trained models. For textual input, contextual embedding\nvectors are first extracted using an English Bert model and then used either to\ndirectly compute sentence embeddings or to feed a bidirectional LSTM-RNNs with\nattention. Finally, an SVM classifier with linear kernel is used for the\nindividual evaluation of the three systems. Our best system, based on the\ncombination of linguistic and acoustic information, attained a classification\naccuracy of 81.25%. Results have shown the importance of linguistic features in\nthe classification of Alzheimer's Disease, which outperforms the acoustic ones\nin terms of accuracy. Early stage features fusion did not provide additional\nimprovements, confirming that the discriminant ability conveyed by speech in\nthis case is smooth out by linguistic data.", "published": "2020-05-29 16:16:02", "link": "http://arxiv.org/abs/2005.14646v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Assessment of Parkinson's Disease Medication State through Automatic\n  Speech Analysis", "abstract": "Parkinson's disease (PD) is a progressive degenerative disorder of the\ncentral nervous system characterized by motor and non-motor symptoms. As the\ndisease progresses, patients alternate periods in which motor symptoms are\nmitigated due to medication intake (ON state) and periods with motor\ncomplications (OFF state). The time that patients spend in the OFF condition is\ncurrently the main parameter employed to assess pharmacological interventions\nand to evaluate the efficacy of different active principles. In this work, we\npresent a system that combines automatic speech processing and deep learning\ntechniques to classify the medication state of PD patients by leveraging\npersonal speech-based bio-markers. We devise a speaker-dependent approach and\ninvestigate the relevance of different acoustic-prosodic feature sets. Results\nshow an accuracy of 90.54% in a test task with mixed speech and an accuracy of\n95.27% in a semi-spontaneous speech task. Overall, the experimental assessment\nshows the potentials of this approach towards the development of reliable,\nremote daily monitoring and scheduling of medication intake of PD patients.", "published": "2020-05-29 16:18:15", "link": "http://arxiv.org/abs/2005.14647v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving EEG based continuous speech recognition using GAN", "abstract": "In this paper we demonstrate that it is possible to generate more meaningful\nelectroencephalography (EEG) features from raw EEG features using generative\nadversarial networks (GAN) to improve the performance of EEG based continuous\nspeech recognition systems. We improve the results demonstrated by authors in\n[1] using their data sets for for some of the test time experiments and for\nother cases our results were comparable with theirs. Our proposed approach can\nbe implemented without using any additional sensor information, whereas in [1]\nauthors used additional features like acoustic or articulatory information to\nimprove the performance of EEG based continuous speech recognition systems.", "published": "2020-05-29 06:11:33", "link": "http://arxiv.org/abs/2006.01260v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Understanding effect of speech perception in EEG based speech\n  recognition systems", "abstract": "The electroencephalography (EEG) signals recorded in parallel with speech are\nused to perform isolated and continuous speech recognition. During speaking\nprocess, one also hears his or her own speech and this speech perception is\nalso reflected in the recorded EEG signals. In this paper we investigate\nwhether it is possible to separate out this speech perception component from\nEEG signals in order to design more robust EEG based speech recognition\nsystems. We further demonstrate predicting EEG signals recorded in parallel\nwith speaking from EEG signals recorded in parallel with passive listening and\nvice versa with very low normalized root mean squared error (RMSE). We finally\ndemonstrate both isolated and continuous speech recognition using EEG signals\nrecorded in parallel with listening, speaking and improve the previous\nconnectionist temporal classification (CTC) model results demonstrated by\nauthors in [1] using their data set.", "published": "2020-05-29 05:56:09", "link": "http://arxiv.org/abs/2006.01261v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Predicting Different Acoustic Features from EEG and towards direct\n  synthesis of Audio Waveform from EEG", "abstract": "In [1,2] authors provided preliminary results for synthesizing speech from\nelectroencephalography (EEG) features where they first predict acoustic\nfeatures from EEG features and then the speech is reconstructed from the\npredicted acoustic features using griffin lim reconstruction algorithm. In this\npaper we first introduce a deep learning model that takes raw EEG waveform\nsignals as input and directly produces audio waveform as output. We then\ndemonstrate predicting 16 different acoustic features from EEG features. We\ndemonstrate our results for both spoken and listen condition in this paper. The\nresults presented in this paper shows how different acoustic features are\nrelated to non-invasive neural EEG signals recorded during speech perception\nand production.", "published": "2020-05-29 05:50:03", "link": "http://arxiv.org/abs/2006.01262v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Large Scale Audiovisual Learning of Sounds with Weakly Labeled Data", "abstract": "Recognizing sounds is a key aspect of computational audio scene analysis and\nmachine perception. In this paper, we advocate that sound recognition is\ninherently a multi-modal audiovisual task in that it is easier to differentiate\nsounds using both the audio and visual modalities as opposed to one or the\nother. We present an audiovisual fusion model that learns to recognize sounds\nfrom weakly labeled video recordings. The proposed fusion model utilizes an\nattention mechanism to dynamically combine the outputs of the individual audio\nand visual models. Experiments on the large scale sound events dataset,\nAudioSet, demonstrate the efficacy of the proposed model, which outperforms the\nsingle-modal models, and state-of-the-art fusion and multi-modal models. We\nachieve a mean Average Precision (mAP) of 46.16 on Audioset, outperforming\nprior state of the art by approximately +4.35 mAP (relative: 10.4%).", "published": "2020-05-29 01:30:14", "link": "http://arxiv.org/abs/2006.01595v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV", "stat.ML"], "primary_category": "eess.AS"}
