{"title": "Finding Minimum Matching Cuts in $H$-free Graphs and Graphs of Bounded Radius and Diameter", "abstract": "A matching cut is a matching that is also an edge cut. In the problem Minimum\nMatching Cut, we ask for a matching cut with the minimum number of edges in the\nmatching. We give polynomial-time algorithms for $P_7$-free, $S_{1,1,2}$-free\nand $(P_6 + P_4)$-free graphs, which also solve several open cases for the\nwell-studied problem Matching Cut. In addition, we show NP-hardness for\n$3P_3$-free graphs, implying that Minimum Matching Cut and Matching Cut differ\nin complexity on certain graph classes. We also give complexity dichotomies for\nboth general and bipartite graphs of bounded radius and diameter.", "published": "2025-02-26 08:45:55", "link": "http://arxiv.org/abs/2502.18942v1", "categories": ["math.CO", "cs.CC", "cs.DM"], "primary_category": "math.CO"}
{"title": "Framework for asset-liability management with fixed-term securities", "abstract": "We consider an optimal investment-consumption problem for a\nutility-maximizing investor who has access to assets with different liquidity\nand whose consumption rate as well as terminal wealth are subject to\nlower-bound constraints. Assuming utility functions that satisfy standard\nconditions, we develop a methodology for deriving the optimal strategies in\nsemi-closed form. Our methodology is based on the generalized martingale\napproach and the decomposition of the problem into subproblems. We illustrate\nour approach by deriving explicit formulas for agents with power-utility\nfunctions and discuss potential extensions of the proposed framework.", "published": "2025-02-26 15:14:33", "link": "http://arxiv.org/abs/2502.19213v1", "categories": ["q-fin.MF", "math.OC", "q-fin.PM", "91G10 (Primary), 90B50 (Secondary)"], "primary_category": "q-fin.MF"}
{"title": "Corporate Fraud Detection in Rich-yet-Noisy Financial Graph", "abstract": "Corporate fraud detection aims to automatically recognize companies that\nconduct wrongful activities such as fraudulent financial statements or illegal\ninsider trading. Previous learning-based methods fail to effectively integrate\nrich interactions in the company network. To close this gap, we collect 18-year\nfinancial records in China to form three graph datasets with fraud labels. We\nanalyze the characteristics of the financial graphs, highlighting two\npronounced issues: (1) information overload: the dominance of (noisy)\nnon-company nodes over company nodes hinders the message-passing process in\nGraph Convolution Networks (GCN); and (2) hidden fraud: there exists a large\npercentage of possible undetected violations in the collected data. The hidden\nfraud problem will introduce noisy labels in the training dataset and\ncompromise fraud detection results. To handle such challenges, we propose a\nnovel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage\nLearning (${\\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to\nmitigate the information overload and effectively learns rich representations.\nThe proposed model adopts a two-stage learning method to enhance robustness\nagainst hidden frauds. Extensive experimental results not only confirm the\nimportance of interactions but also show the superiority of ${\\rm KeGCN}_{R}$\nover a number of strong baselines in terms of fraud detection effectiveness and\nrobustness.", "published": "2025-02-26 17:05:54", "link": "http://arxiv.org/abs/2502.19305v1", "categories": ["cs.LG", "cs.AI", "q-fin.RM", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "Random Forest-of-Thoughts: Uncertainty-aware Reasoning for Computational\n  Social Science", "abstract": "Social surveys in computational social science are well-designed by elaborate\ndomain theories that can effectively reflect the interviewee's deep thoughts\nwithout concealing their true feelings. The candidate questionnaire options\nhighly depend on the interviewee's previous answer, which results in the\ncomplexity of social survey analysis, the time, and the expertise required. The\nability of large language models (LLMs) to perform complex reasoning is\nwell-enhanced by prompting learning such as Chain-of-thought (CoT) but still\nconfined to left-to-right decision-making processes or limited paths during\ninference. This means they can fall short in problems that require exploration\nand uncertainty searching. In response, a novel large language model prompting\nmethod, called Random Forest of Thoughts (RFoT), is proposed for generating\nuncertainty reasoning to fit the area of computational social science. The RFoT\nallows LLMs to perform deliberate decision-making by generating diverse thought\nspace and randomly selecting the sub-thoughts to build the forest of thoughts.\nIt can extend the exploration and prediction of overall performance, benefiting\nfrom the extensive research space of response. The method is applied to\noptimize computational social science analysis on two datasets covering a\nspectrum of social survey analysis problems. Our experiments show that RFoT\nsignificantly enhances language models' abilities on two novel social survey\nanalysis problems requiring non-trivial reasoning.", "published": "2025-02-26 00:52:44", "link": "http://arxiv.org/abs/2502.18729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Prompt Optimization via Heuristic Search: A Survey", "abstract": "Recent advances in Large Language Models have led to remarkable achievements\nacross a variety of Natural Language Processing tasks, making prompt\nengineering increasingly central to guiding model outputs. While manual methods\ncan be effective, they typically rely on intuition and do not automatically\nrefine prompts over time. In contrast, automatic prompt optimization employing\nheuristic-based search algorithms can systematically explore and improve\nprompts with minimal human oversight. This survey proposes a comprehensive\ntaxonomy of these methods, categorizing them by where optimization occurs, what\nis optimized, what criteria drive the optimization, which operators generate\nnew prompts, and which iterative search algorithms are applied. We further\nhighlight specialized datasets and tools that support and accelerate automated\nprompt refinement. We conclude by discussing key open challenges pointing\ntoward future opportunities for more robust and versatile LLM applications.", "published": "2025-02-26 01:42:08", "link": "http://arxiv.org/abs/2502.18746v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance", "abstract": "Despite Greece's pivotal role in the global economy, large language models\n(LLMs) remain underexplored for Greek financial context due to the linguistic\ncomplexity of Greek and the scarcity of domain-specific datasets. Previous\nefforts in multilingual financial natural language processing (NLP) have\nexposed considerable performance disparities, yet no dedicated Greek financial\nbenchmarks or Greek-specific financial LLMs have been developed until now. To\nbridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation\nBenchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with\nGreek domain-specific data. Plutus-ben addresses five core financial NLP tasks\nin Greek: numeric and textual named entity recognition, question answering,\nabstractive summarization, and topic classification, thereby facilitating\nsystematic and reproducible LLM assessments. To underpin these tasks, we\npresent three novel, high-quality Greek financial datasets, thoroughly\nannotated by expert native Greek speakers, augmented by two existing resources.\nOur comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek\nfinancial NLP remains challenging due to linguistic complexity, domain-specific\nterminology, and financial reasoning gaps. These findings underscore the\nlimitations of cross-lingual transfer, the necessity for financial expertise in\nGreek-trained models, and the challenges of adapting financial LLMs to Greek\ntext. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to\npromote reproducible research and advance Greek financial NLP, fostering\nbroader multilingual inclusivity in finance.", "published": "2025-02-26 03:04:01", "link": "http://arxiv.org/abs/2502.18772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Few-Shot Learning for Text Classification", "abstract": "The rise of Large Language Models (LLMs) has boosted the use of Few-Shot\nLearning (FSL) methods in natural language processing, achieving acceptable\nperformance even when working with limited training data. The goal of FSL is to\neffectively utilize a small number of annotated samples in the learning\nprocess. However, the performance of FSL suffers when unsuitable support\nsamples are chosen. This problem arises due to the heavy reliance on a limited\nnumber of support samples, which hampers consistent performance improvement\neven when more support samples are added. To address this challenge, we propose\nan active learning-based instance selection mechanism that identifies effective\nsupport instances from the unlabeled pool and can work with different LLMs. Our\nexperiments on five tasks show that our method frequently improves the\nperformance of FSL. We make our implementation available on GitHub.", "published": "2025-02-26 03:30:13", "link": "http://arxiv.org/abs/2502.18782v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning\n  in LMs", "abstract": "Do LLMs offer insights into human language learning? A common argument\nagainst this idea is that because their architecture and training paradigm are\nso vastly different from humans, LLMs can learn arbitrary inputs as easily as\nnatural languages. In this paper, we test this claim by training LMs to model\nimpossible and typologically unattested languages. Unlike previous work, which\nhas focused exclusively on English, we conduct experiments on 12 natural\nlanguages from 4 language families. Our results show that while GPT-2 small can\nprimarily distinguish attested languages from their impossible counterparts, it\ndoes not achieve perfect separation between all the attested languages and all\nthe impossible ones. We further test whether GPT-2 small distinguishes\ntypologically attested from unattested languages with different NP orders by\nmanipulating word order based on Greenberg's Universal 20. We find that the\nmodel's perplexity scores do not distinguish attested vs. unattested word\norders, as long as the unattested variants maintain constituency structure.\nThese findings suggest that language models exhibit some human-like inductive\nbiases, though these biases are weaker than those found in human learners.", "published": "2025-02-26 04:01:36", "link": "http://arxiv.org/abs/2502.18795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Grow Less Humanlike beyond Phase Transition", "abstract": "LMs' alignment with human reading behavior (i.e. psychometric predictive\npower; PPP) is known to improve during pretraining up to a tipping point,\nbeyond which it either plateaus or degrades. Various factors, such as word\nfrequency, recency bias in attention, and context size, have been theorized to\naffect PPP, yet there is no current account that explains why such a tipping\npoint exists, and how it interacts with LMs' pretraining dynamics more\ngenerally. We hypothesize that the underlying factor is a pretraining phase\ntransition, characterized by the rapid emergence of specialized attention\nheads. We conduct a series of correlational and causal experiments to show that\nsuch a phase transition is responsible for the tipping point in PPP. We then\nshow that, rather than producing attention patterns that contribute to the\ndegradation in PPP, phase transitions alter the subsequent learning dynamics of\nthe model, such that further training keeps damaging PPP.", "published": "2025-02-26 04:17:19", "link": "http://arxiv.org/abs/2502.18802v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Judge as A Judge: Improving the Evaluation of Retrieval-Augmented\n  Generation through the Judge-Consistency of Large Language Models", "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in\nalleviating hallucinations for Large Language Models (LLMs). However, existing\nautomated evaluation metrics cannot fairly evaluate the outputs generated by\nRAG models during training and evaluation. LLM-based judgment models provide\nthe potential to produce high-quality judgments, but they are highly sensitive\nto evaluation prompts, leading to inconsistencies when judging the output of\nRAG models. This paper introduces the Judge-Consistency (ConsJudge) method,\nwhich aims to enhance LLMs to generate more accurate evaluations for RAG\nmodels. Specifically, ConsJudge prompts LLMs to generate different judgments\nbased on various combinations of judgment dimensions, utilize the\njudge-consistency to evaluate these judgments and select the accepted and\nrejected judgments for DPO training. Our experiments show that ConsJudge can\neffectively provide more accurate judgments for optimizing RAG models across\nvarious RAG models and datasets. Further analysis reveals that judgments\ngenerated by ConsJudge have a high agreement with the superior LLM. All codes\nare available at https://github.com/OpenBMB/ConsJudge.", "published": "2025-02-26 04:50:43", "link": "http://arxiv.org/abs/2502.18817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evidence-Driven Marker Extraction for Social Media Suicide Risk\n  Detection", "abstract": "Early detection of suicide risk from social media text is crucial for timely\nintervention. While Large Language Models (LLMs) offer promising capabilities\nin this domain, challenges remain in terms of interpretability and\ncomputational efficiency. This paper introduces Evidence-Driven LLM (ED-LLM), a\nnovel approach for clinical marker extraction and suicide risk classification.\nED-LLM employs a multi-task learning framework, jointly training a Mistral-7B\nbased model to identify clinical marker spans and classify suicide risk levels.\nThis evidence-driven strategy enhances interpretability by explicitly\nhighlighting textual evidence supporting risk assessments. Evaluated on the\nCLPsych datasets, ED-LLM demonstrates competitive performance in risk\nclassification and superior capability in clinical marker span identification\ncompared to baselines including fine-tuned LLMs, traditional machine learning,\nand prompt-based methods. The results highlight the effectiveness of multi-task\nlearning for interpretable and efficient LLM-based suicide risk assessment,\npaving the way for clinically relevant applications.", "published": "2025-02-26 04:58:03", "link": "http://arxiv.org/abs/2502.18823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Movie Reviews Using BERT", "abstract": "Sentiment Analysis (SA) or opinion mining is analysis of emotions and\nopinions from any kind of text. SA helps in tracking peoples viewpoints and it\nis an important factor when it comes to social media monitoring product and\nbrand recognition customer satisfaction customer loyalty advertising and\npromotions success and product acceptance. That is why SA is one of the active\nresearch areas in Natural Language Processing (NLP). SA is applied on data\nsourced from various media platforms to mine sentiment knowledge from them.\nVarious approaches have been deployed in the literature to solve the problem.\nMost techniques devise complex and sophisticated frameworks in order to attain\noptimal accuracy. This work aims to finetune Bidirectional Encoder\nRepresentations from Transformers (BERT) with Bidirectional Long Short-Term\nMemory (BiLSTM) for movie reviews sentiment analysis and still provide better\naccuracy than the State-of-The-Art (SOTA) methods. The paper also shows how\nsentiment analysis can be applied if someone wants to recommend a certain movie\nfor example by computing overall polarity of its sentiments predicted by the\nmodel. That is our proposed method serves as an upper-bound baseline in\nprediction of a predominant reaction to a movie. To compute overall polarity a\nheuristic algorithm is applied to BERTBiLSTM output vector. Our model can be\nextended to three-class four-class or any fine-grained classification and apply\noverall polarity computation again. This is intended to be exploited in future\nwork.", "published": "2025-02-26 05:30:19", "link": "http://arxiv.org/abs/2502.18841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Rewriting Approaches for Different Conversational Tasks", "abstract": "Conversational assistants often require a question rewriting algorithm that\nleverages a subset of past interactions to provide a more meaningful (accurate)\nanswer to the user's question or request. However, the exact rewriting approach\nmay often depend on the use case and application-specific tasks supported by\nthe conversational assistant, among other constraints. In this paper, we\nsystematically investigate two different approaches, denoted as rewriting and\nfusion, on two fundamentally different generation tasks, including a\ntext-to-text generation task and a multimodal generative task that takes as\ninput text and generates a visualization or data table that answers the user's\nquestion. Our results indicate that the specific rewriting or fusion approach\nhighly depends on the underlying use case and generative task. In particular,\nwe find that for a conversational question-answering assistant, the query\nrewriting approach performs best, whereas for a data analysis assistant that\ngenerates visualizations and data tables based on the user's conversation with\nthe assistant, the fusion approach works best. Notably, we explore two datasets\nfor the data analysis assistant use case, for short and long conversations, and\nwe find that query fusion always performs better, whereas for the\nconversational text-based question-answering, the query rewrite approach\nperforms best.", "published": "2025-02-26 06:05:29", "link": "http://arxiv.org/abs/2502.18860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Structured Output with Schema Reinforcement\n  Learning", "abstract": "This study investigates the structured generation capabilities of large\nlanguage models (LLMs), focusing on producing valid JSON outputs against a\ngiven schema. Despite the widespread use of JSON in integrating language models\nwith programs, there is a lack of comprehensive analysis and benchmarking of\nthese capabilities. We explore various aspects of JSON generation, such as\nstructure understanding, escaping, and natural language description, to\ndetermine how to assess and enable LLMs to generate valid responses. Building\nupon this, we propose SchemaBench features around 40K different JSON schemas to\nobtain and assess models' abilities in generating valid JSON. We find that the\nlatest LLMs are still struggling to generate a valid JSON string. Moreover, we\ndemonstrate that incorporating reinforcement learning with a Fine-grained\nSchema Validator can further enhance models' understanding of JSON schema,\nleading to improved performance. Our models demonstrate significant improvement\nin both generating JSON outputs and downstream tasks.", "published": "2025-02-26 06:45:29", "link": "http://arxiv.org/abs/2502.18878v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens", "abstract": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.", "published": "2025-02-26 07:10:08", "link": "http://arxiv.org/abs/2502.18890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles", "abstract": "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.", "published": "2025-02-26 09:26:54", "link": "http://arxiv.org/abs/2502.18968v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenTool: Enhancing Tool Generalization in Language Models through\n  Zero-to-One and Weak-to-Strong Simulation", "abstract": "Large Language Models (LLMs) can enhance their capabilities as AI assistants\nby integrating external tools, allowing them to access a wider range of\ninformation. While recent LLMs are typically fine-tuned with tool usage\nexamples during supervised fine-tuning (SFT), questions remain about their\nability to develop robust tool-usage skills and can effectively generalize to\nunseen queries and tools. In this work, we present GenTool, a novel training\nframework that prepares LLMs for diverse generalization challenges in tool\nutilization. Our approach addresses two fundamental dimensions critical for\nreal-world applications: Zero-to-One Generalization, enabling the model to\naddress queries initially lacking a suitable tool by adopting and utilizing one\nwhen it becomes available, and Weak-to-Strong Generalization, allowing models\nto leverage enhanced versions of existing tools to solve queries. To achieve\nthis, we develop synthetic training data simulating these two dimensions of\ntool usage and introduce a two-stage fine-tuning approach: optimizing tool\nranking, then refining tool selection. Through extensive experiments across\nfour generalization scenarios, we demonstrate that our method significantly\nenhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters,\nachieving performance that surpasses GPT-4o. Furthermore, our analysis also\nprovides valuable insights into the challenges LLMs encounter in tool\ngeneralization.", "published": "2025-02-26 09:54:33", "link": "http://arxiv.org/abs/2502.18990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MathClean: A Benchmark for Synthetic Mathematical Data Cleaning", "abstract": "With the rapid development of large language models (LLMs), the quality of\ntraining data has become crucial. Among the various types of training data,\nmathematical data plays a key role in enabling LLMs to acquire strong reasoning\nabilities. While high-quality open-source data is important, it is often\ninsufficient for pre-training, necessitating the addition of synthetic math\nproblems. However, synthetic math questions and answers can introduce\ninaccuracies, which may degrade both the training data and web data. Therefore,\nan effective method for cleaning synthetic math data is essential. In this\npaper, we propose the MathClean benchmark to evaluate the effectiveness of math\ndata cleaning models. The MathClean benchmark consists of 2,000 correct\nquestions and 2,000 erroneous questions with additional 2,000 correct and\nerroneous answers sourced from augmented data based on GSM8K and MATH.\nMoreover, we also annotate error types for each question or answer, since it\ncan assess whether models can correctly identify the error categories for\nfuture improvements. Finally, we present comprehensive evaluations using\nstate-of-the-art (SOTA) models. Our results demonstrate that even strong models\nlike GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the\nutility of MathClean. Our code and data is available at\nhttps://github.com/YuYingLi0/MathClean.", "published": "2025-02-26 11:17:50", "link": "http://arxiv.org/abs/2502.19058v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A\n  Comparative Study Using the Consensual Assessment Technique", "abstract": "The Consensual Assessment Technique (CAT) evaluates creativity through\nholistic expert judgments. We investigate the use of two advanced Large\nLanguage Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a\nmethodology inspired by the CAT. Using a dataset of 90 poems, we found that\nthese LLMs can surpass the results achieved by non-expert human judges at\nmatching a ground truth based on publication venue, particularly when assessing\nsmaller subsets of poems. Claude-3-Opus exhibited slightly superior performance\nthan GPT-4o. We show that LLMs are viable tools for accurately assessing\npoetry, paving the way for their broader application into other creative\ndomains.", "published": "2025-02-26 11:43:25", "link": "http://arxiv.org/abs/2502.19064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the quality of Web-mined Parallel Corpora of Low-Resource\n  Languages using Debiasing Heuristics", "abstract": "Parallel Data Curation (PDC) techniques aim to filter out noisy parallel\nsentences from the web-mined corpora. Prior research has demonstrated that\nranking sentence pairs using similarity scores on sentence embeddings derived\nfrom Pre-trained Multilingual Language Models (multiPLMs) and training the NMT\nsystems with the top-ranked samples, produces superior NMT performance than\nwhen trained using the full dataset. However, previous research has shown that\nthe choice of multiPLM significantly impacts the ranking quality. This paper\ninvestigates the reasons behind this disparity across multiPLMs. Using the\nweb-mined corpora CCMatrix and CCAligned for En$\\rightarrow$Si,\nEn$\\rightarrow$Ta and Si$\\rightarrow$Ta, we show that different multiPLMs\n(LASER3, XLM-R, and LaBSE) are biased towards certain types of sentences, which\nallows noisy sentences to creep into the top-ranked samples. We show that by\nemploying a series of heuristics, this noise can be removed to a certain\nextent. This results in improving the results of NMT systems trained with\nweb-mined corpora and reduces the disparity across multiPLMs.", "published": "2025-02-26 11:56:43", "link": "http://arxiv.org/abs/2502.19074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic\n  Activation for LLMs", "abstract": "Dense large language models(LLMs) face critical efficiency bottlenecks as\nthey rigidly activate all parameters regardless of input complexity. While\nexisting sparsity methods(static pruning or dynamic activation) address this\npartially, they either lack adaptivity to contextual or model structural\ndemands or incur prohibitive computational overhead. Inspired by human brain's\ndual-process mechanisms - predictive coding (N400) for backbone sparsity and\nstructural reanalysis (P600) for complex context - we propose CLADA, a\n\\textit{\\textbf{C}ognitive-\\textbf{L}oad-\\textbf{A}ware \\textbf{D}ynamic\n\\textbf{A}ctivation} framework that synergizes statistical sparsity with\nsemantic adaptability. Our key insight is that LLM activations exhibit two\ncomplementary patterns: 1) \\textit{Global statistical sparsity} driven by\nsequence-level prefix information, and 2) \\textit{Local semantic adaptability}\nmodulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs\na hierarchical thresholding strategy: a baseline from offline error-controlled\noptimization ensures 40\\%+ sparsity, dynamically adjusted by real-time\ncognitive signals. Evaluations across six mainstream LLMs and nine benchmarks\ndemonstrate that CLADA achieves \\textbf{~20\\% average speedup with <2\\%\naccuracy drop}, outperforming Griffin (5\\%+ degradation) and TT (negligible\nspeedup). Crucially, we establish the first formal connection between\nneurolinguistic event-related potential (ERP) components and LLM efficiency\nmechanisms through multi-level regression analysis ($R^2=0.17$ for\nsparsity-adaptation synergy). Requiring no retraining or architectural changes,\nCLADA offers a deployable solution for resource-aware LLM inference while\nadvancing biologically-inspired AI design. Our code is available at\n\\href{https://github.com/Oldify/CLADA}{CLADA}.", "published": "2025-02-26 12:11:16", "link": "http://arxiv.org/abs/2502.19078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm", "abstract": "Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval.", "published": "2025-02-26 12:46:36", "link": "http://arxiv.org/abs/2502.19103v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine\n  Translation", "abstract": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1, we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german.", "published": "2025-02-26 12:46:59", "link": "http://arxiv.org/abs/2502.19104v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Memory Alignment: Mitigating Factual Hallucinations with\n  Generalized Improvement", "abstract": "Large Language Models (LLMs) often struggle to align their responses with\nobjective facts, resulting in the issue of factual hallucinations, which can be\ndifficult to detect and mislead users without relevant knowledge. While\npost-training techniques have been employed to mitigate the issue, existing\nmethods usually suffer from poor generalization and trade-offs in different\ncapabilities. In this paper, we propose to address it by directly augmenting\nLLM's fundamental ability to precisely leverage its existing memory--the\nknowledge acquired from pre-training data. We introduce self-memory alignment\n(SMA), which fine-tunes the model on self-generated responses to precise and\nsimple factual questions through preference optimization. Furthermore, we\nconstruct FactualBench, a comprehensive and precise factual QA dataset\ncontaining 181k Chinese data spanning 21 domains, to facilitate both evaluation\nand training. Extensive experiments show that SMA significantly improves LLMs'\noverall performance, with consistent enhancement across various benchmarks\nconcerning factuality, as well as helpfulness and comprehensive skills.", "published": "2025-02-26 13:34:52", "link": "http://arxiv.org/abs/2502.19127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BIG-Bench Extra Hard", "abstract": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.", "published": "2025-02-26 14:50:50", "link": "http://arxiv.org/abs/2502.19187v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiGT: Layout-infused Generative Transformer for Visual Question\n  Answering on Vietnamese Receipts", "abstract": "Document Visual Question Answering (Document VQA) challenges multimodal\nsystems to holistically handle textual, layout, and visual modalities to\nprovide appropriate answers. Document VQA has gained popularity in recent years\ndue to the increasing amount of documents and the high demand for digitization.\nNonetheless, most of document VQA datasets are developed in high-resource\nlanguages such as English. In this paper, we present ReceiptVQA\n(\\textbf{Receipt} \\textbf{V}isual \\textbf{Q}uestion \\textbf{A}nswering), the\ninitial large-scale document VQA dataset in Vietnamese dedicated to receipts, a\ndocument kind with high commercial potentials. The dataset encompasses\n\\textbf{9,000+} receipt images and \\textbf{60,000+} manually annotated\nquestion-answer pairs. In addition to our study, we introduce LiGT\n(\\textbf{L}ayout-\\textbf{i}nfused \\textbf{G}enerative \\textbf{T}ransformer), a\nlayout-aware encoder-decoder architecture designed to leverage embedding layers\nof language models to operate layout embeddings, minimizing the use of\nadditional neural modules. Experiments on ReceiptVQA show that our architecture\nyielded promising performance, achieving competitive results compared with\noutstanding baselines. Furthermore, throughout analyzing experimental results,\nwe found evident patterns that employing encoder-only model architectures has\nconsiderable disadvantages in comparison to architectures that can generate\nanswers. We also observed that it is necessary to combine multiple modalities\nto tackle our dataset, despite the critical role of semantic understanding from\nlanguage models. We hope that our work will encourage and facilitate future\ndevelopment in Vietnamese document VQA, contributing to a diverse multimodal\nresearch community in the Vietnamese language.", "published": "2025-02-26 15:09:28", "link": "http://arxiv.org/abs/2502.19202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiConAD: A Unified Multilingual Conversational Dataset for Early\n  Alzheimer's Detection", "abstract": "Dementia is a progressive cognitive syndrome with Alzheimer's disease (AD) as\nthe leading cause. Conversation-based AD detection offers a cost-effective\nalternative to clinical methods, as language dysfunction is an early biomarker\nof AD. However, most prior research has framed AD detection as a binary\nclassification problem, limiting the ability to identify Mild Cognitive\nImpairment (MCI)-a crucial stage for early intervention. Also, studies\nprimarily rely on single-language datasets, mainly in English, restricting\ncross-language generalizability. To address this gap, we make three key\ncontributions. First, we introduce a novel, multilingual dataset for AD\ndetection by unifying 16 publicly available dementia-related conversational\ndatasets. This corpus spans English, Spanish, Chinese, and Greek and\nincorporates both audio and text data derived from a variety of cognitive\nassessment tasks. Second, we perform finer-grained classification, including\nMCI, and evaluate various classifiers using sparse and dense text\nrepresentations. Third, we conduct experiments in monolingual and multilingual\nsettings, finding that some languages benefit from multilingual training while\nothers perform better independently. This study highlights the challenges in\nmultilingual AD detection and enables future research on both language-specific\napproaches and techniques aimed at improving model generalization and\nrobustness.", "published": "2025-02-26 15:12:37", "link": "http://arxiv.org/abs/2502.19208v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in\n  Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in\nLarge Language Models (LLMs) but can still produce inconsistent or unsupported\ncontent. Although LLM-as-a-Judge is widely used for RAG hallucination detection\ndue to its implementation simplicity, it faces two main challenges: the absence\nof comprehensive evaluation benchmarks and the lack of domain-optimized judge\nmodels. To bridge these gaps, we introduce \\textbf{Bi'an}, a novel framework\nfeaturing a bilingual benchmark dataset and lightweight judge models. The\ndataset supports rigorous evaluation across multiple RAG scenarios, while the\njudge models are fine-tuned from compact open-source LLMs. Extensive\nexperimental evaluations on Bi'anBench show our 14B model outperforms baseline\nmodels with over five times larger parameter scales and rivals state-of-the-art\nclosed-source LLMs. We will release our data and models soon at\nhttps://github.com/OpenSPG/KAG.", "published": "2025-02-26 15:12:59", "link": "http://arxiv.org/abs/2502.19209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Negation-Induced Forgetting in LLMs", "abstract": "The study explores whether Large Language Models (LLMs) exhibit\nnegation-induced forgetting (NIF), a cognitive phenomenon observed in humans\nwhere negating incorrect attributes of an object or event leads to diminished\nrecall of this object or event compared to affirming correct attributes (Mayo\net al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental\nframework to test this effect in ChatGPT-3.5, GPT-4o mini and\nLlama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with\nnegated information being less likely to be recalled than affirmed information.\nGPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did\nnot exhibit NIF. The findings provide initial evidence of negation-induced\nforgetting in some LLMs, suggesting that similar cognitive biases may emerge in\nthese models. This work is a preliminary step in understanding how\nmemory-related phenomena manifest in LLMs.", "published": "2025-02-26 15:13:20", "link": "http://arxiv.org/abs/2502.19211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two Heads Are Better Than One: Dual-Model Verbal Reflection at\n  Inference-Time", "abstract": "Large Language Models (LLMs) often struggle with complex reasoning scenarios.\nWhile preference optimization methods enhance reasoning performance through\ntraining, they often lack transparency in why one reasoning outcome is\npreferred over another. Verbal reflection techniques improve explainability but\nare limited in LLMs' critique and refinement capacity. To address these\nchallenges, we introduce a contrastive reflection synthesis pipeline that\nenhances the accuracy and depth of LLM-generated reflections. We further\npropose a dual-model reasoning framework within a verbal reinforcement learning\nparadigm, decoupling inference-time self-reflection into specialized, trained\nmodels for reasoning critique and refinement. Extensive experiments show that\nour framework outperforms traditional preference optimization methods across\nall evaluation metrics. Our findings also show that \"two heads are better than\none\", demonstrating that a collaborative Reasoner-Critic model achieves\nsuperior reasoning performance and transparency, compared to single-model\napproaches.", "published": "2025-02-26 15:41:41", "link": "http://arxiv.org/abs/2502.19230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangled VAD Representations via a Variational Framework for\n  Political Stance Detection", "abstract": "The stance detection task aims to categorise the stance regarding specified\ntargets. Current methods face challenges in effectively integrating sentiment\ninformation for stance detection. Moreover, the role of highly granular\nsentiment labelling in stance detection has been largely overlooked. This study\npresents a novel stance detection framework utilizing a variational autoencoder\n(VAE) to disentangle latent emotional features-value, arousal, and dominance\n(VAD)-from political discourse on social media. This approach addresses\nlimitations in current methods, particularly in in-target and cross-target\nstance detection scenarios. This research uses an advanced emotional annotation\ntool to annotate seven-class sentiment labels for P-STANCE. Evaluations on\nbenchmark datasets, including P-STANCE and SemEval-2016, reveal that\nPoliStance-VAE achieves state-of-the-art performance, surpassing models like\nBERT, BERTweet, and GPT-4o. PoliStance-VAE offers a robust and interpretable\nsolution for stance detection, demonstrating the effectiveness of integrating\nnuanced emotional representations. This framework paves the way for\nadvancements in natural language processing tasks, particularly those requiring\ndetailed emotional understanding.", "published": "2025-02-26 16:31:48", "link": "http://arxiv.org/abs/2502.19276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CritiQ: Mining Data Quality Criteria from Human Preferences", "abstract": "Language model heavily depends on high-quality data for optimal performance.\nExisting approaches rely on manually designed heuristics, the perplexity of\nexisting models, training classifiers, or careful prompt engineering, which\nrequire significant expert experience and human annotation effort while\nintroduce biases. We introduce CritiQ, a novel data selection method that\nautomatically mines criteria from human preferences for data quality with only\n$\\sim$30 human-annotated pairs and performs efficient data selection. The main\ncomponent, CritiQ Flow, employs a manager agent to evolve quality criteria and\nworker agents to make pairwise judgments. We build a knowledge base that\nextracts quality criteria from previous work to boost CritiQ Flow. Compared to\nperplexity- and classifier- based methods, verbal criteria are more\ninterpretable and possess reusable value. After deriving the criteria, we train\nthe CritiQ Scorer to give quality scores and perform efficient data selection.\nWe demonstrate the effectiveness of our method in the code, math, and logic\ndomains, achieving high accuracy on human-annotated test sets. To validate the\nquality of the selected data, we continually train Llama 3.1 models and observe\nimproved performance on downstream tasks compared to uniform sampling. Ablation\nstudies validate the benefits of the knowledge base and the reflection process.\nWe analyze how criteria evolve and the effectiveness of majority voting.", "published": "2025-02-26 16:33:41", "link": "http://arxiv.org/abs/2502.19279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets", "abstract": "Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.", "published": "2025-02-26 17:32:07", "link": "http://arxiv.org/abs/2502.19339v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?", "abstract": "Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models.", "published": "2025-02-26 17:59:27", "link": "http://arxiv.org/abs/2502.19361v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Mighty ToRR: A Benchmark for Table Reasoning and Robustness", "abstract": "Despite its real-world significance, model performance on tabular data\nremains underexplored, leaving uncertainty about which model to rely on and\nwhich prompt configuration to adopt. To address this gap, we create ToRR, a\nbenchmark for Table Reasoning and Robustness, measuring model performance and\nrobustness on table-related tasks. The benchmark includes 10 datasets that\ncover different types of table reasoning capabilities across varied domains.\nToRR goes beyond model performance rankings, and is designed to reflect whether\nmodels can handle tabular data consistently and robustly, across a variety of\ncommon table representation formats. We present a leaderboard as well as\ncomprehensive analyses of the results of leading models over ToRR. Our results\nreveal a striking pattern of brittle model behavior, where even strong models\nare unable to perform robustly on tabular data tasks. Although no specific\ntable format leads to consistently better performance, we show that testing\nover multiple formats is crucial for reliably estimating model capabilities.\nMoreover, we show that the reliability boost from testing multiple prompts can\nbe equivalent to adding more test examples. Overall, our findings show that\ntable understanding and reasoning tasks remain a significant challenge.", "published": "2025-02-26 18:56:38", "link": "http://arxiv.org/abs/2502.19412v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stay Focused: Problem Drift in Multi-Agent Debate", "abstract": "Multi-agent debate - multiple instances of large language models discussing\nproblems in turn-based interaction - has shown promise for solving knowledge\nand reasoning tasks. However, these methods show limitations, particularly when\nscaling them to longer reasoning chains. In this study, we unveil a new issue\nof multi-agent debate: discussions drift away from the initial problem over\nmultiple turns. We define this phenomenon as problem drift and quantify its\npresence across ten tasks (i.e., three generative, three knowledge, three\nreasoning, and one instruction-following task). To identify the reasons for\nthis issue, we perform a human study with eight experts on discussions\nsuffering from problem drift, who find the most common issues are a lack of\nprogress (35% of cases), low-quality feedback (26% of cases), and a lack of\nclarity (25% of cases). To systematically address the issue of problem drift,\nwe propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem\ndrift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of\nproblem drift cases. Our study can be seen as a first step to understanding a\nkey limitation of multi-agent debate, highlighting pathways for improving their\neffectiveness in the future.", "published": "2025-02-26 20:54:51", "link": "http://arxiv.org/abs/2502.19559v1", "categories": ["cs.CL", "A.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Where Are We? Evaluating LLM Performance on African Languages", "abstract": "Africa's rich linguistic heritage remains underrepresented in NLP, largely\ndue to historical policies that favor foreign languages and create significant\ndata inequities. In this paper, we integrate theoretical insights on Africa's\nlanguage landscape with an empirical evaluation using Sahara - a comprehensive\nbenchmark curated from large-scale, publicly accessible datasets capturing the\ncontinent's linguistic diversity. By systematically assessing the performance\nof leading large language models (LLMs) on Sahara, we demonstrate how\npolicy-induced data variations directly impact model effectiveness across\nAfrican languages. Our findings reveal that while a few languages perform\nreasonably well, many Indigenous languages remain marginalized due to sparse\ndata. Leveraging these insights, we offer actionable recommendations for policy\nreforms and inclusive data practices. Overall, our work underscores the urgent\nneed for a dual approach - combining theoretical understanding with empirical\nevaluation - to foster linguistic diversity in AI for African communities.", "published": "2025-02-26 21:49:54", "link": "http://arxiv.org/abs/2502.19582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Word Embeddings in the LLM Era", "abstract": "Large Language Models (LLMs) have recently shown remarkable advancement in\nvarious NLP tasks. As such, a popular trend has emerged lately where NLP\nresearchers extract word/sentence/document embeddings from these large\ndecoder-only models and use them for various inference tasks with promising\nresults. However, it is still unclear whether the performance improvement of\nLLM-induced embeddings is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder\n(USE). This is the central question we investigate in the paper by\nsystematically comparing classical decontextualized and contextualized word\nembeddings with the same for LLM-induced embeddings. Our results show that LLMs\ncluster semantically related words more tightly and perform better on analogy\ntasks in decontextualized settings. However, in contextualized settings,\nclassical models like SimCSE often outperform LLMs in sentence-level similarity\nassessment tasks, highlighting their continued relevance for fine-grained\nsemantics.", "published": "2025-02-26 22:45:08", "link": "http://arxiv.org/abs/2502.19607v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond RNNs: Benchmarking Attention-Based Image Captioning Models", "abstract": "Image captioning is a challenging task at the intersection of computer vision\nand natural language processing, requiring models to generate meaningful\ntextual descriptions of images. Traditional approaches rely on recurrent neural\nnetworks (RNNs), but recent advancements in attention mechanisms have\ndemonstrated significant improvements. This study benchmarks the performance of\nattention-based image captioning models against RNN-based approaches using the\nMS-COCO dataset. We evaluate the effectiveness of Bahdanau attention in\nenhancing the alignment between image features and generated captions. The\nmodels are assessed using natural language processing metrics such as BLEU,\nMETEOR, GLEU, and WER. Our results show that attention-based models outperform\nRNNs in generating more accurate and semantically rich captions, with better\nalignment to human evaluation. This work provides insights into the impact of\nattention mechanisms in image captioning and highlights areas for future\nimprovements.", "published": "2025-02-26 01:05:18", "link": "http://arxiv.org/abs/2502.18734v1", "categories": ["cs.CV", "cs.CL", "68T45, 68T50", "I.2.10; I.2.7; I.4.8"], "primary_category": "cs.CV"}
{"title": "Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for\n  Automatic Alignment in Large Language Models", "abstract": "Recent advancements in Large Language Model (LLM) alignment have sought to\nmitigate the cost of human annotations by leveraging pretrained models to\ngenerate preference data. However, existing methods often compare responses\nfrom models with substantially different capabilities, yielding superficial\ndistinctions that fail to provide meaningful guidance on what constitutes a\nsuperior response. To address this limitation, we propose Kinship-Aware\npReference MApping (KARMA), a novel framework that systematically pairs\nresponses from models with comparable competencies. By constraining preference\ncomparisons to outputs of similar complexity and quality, KARMA enhances the\ninformativeness of preference data and improves the granularity of alignment\nsignals. Empirical evaluations demonstrate that our kinship-aware approach\nleads to more consistent and interpretable alignment outcomes, ultimately\nfacilitating a more principled and reliable pathway for aligning LLM behavior\nwith human preferences.", "published": "2025-02-26 01:36:40", "link": "http://arxiv.org/abs/2502.18744v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Optimal Multi-draft Speculative Decoding", "abstract": "Large Language Models (LLMs) have become an indispensable part of natural\nlanguage processing tasks. However, autoregressive sampling has become an\nefficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent\napproach where, when generating each token, a small draft model generates\nmultiple drafts, and the target LLM verifies them in parallel, ensuring that\nthe final output conforms to the target model distribution. The two main design\nchoices in MDSD are the draft sampling method and the verification algorithm.\nFor a fixed draft sampling method, the optimal acceptance rate is a solution to\nan optimal transport problem, but the complexity of this problem makes it\ndifficult to solve for the optimal acceptance rate and measure the gap between\nexisting verification algorithms and the theoretical upper bound. This paper\ndiscusses the dual of the optimal transport problem, providing a way to\nefficiently compute the optimal acceptance rate. For the first time, we measure\nthe theoretical upper bound of MDSD efficiency for vocabulary sizes in the\nthousands and quantify the gap between existing verification algorithms and\nthis bound. We also compare different draft sampling methods based on their\noptimal acceptance rates. Our results show that the draft sampling method\nstrongly influences the optimal acceptance rate, with sampling without\nreplacement outperforming sampling with replacement. Additionally, existing\nverification algorithms do not reach the theoretical upper bound for both\nwithout replacement and with replacement sampling. Our findings suggest that\ncarefully designed draft sampling methods can potentially improve the optimal\nacceptance rate and enable the development of verification algorithms that\nclosely match the theoretical upper bound.", "published": "2025-02-26 03:22:44", "link": "http://arxiv.org/abs/2502.18779v1", "categories": ["cs.CL", "cs.DS"], "primary_category": "cs.CL"}
{"title": "ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions", "abstract": "Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt.", "published": "2025-02-26 04:10:18", "link": "http://arxiv.org/abs/2502.18798v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-LLM Collaborative Search for Complex Problem Solving", "abstract": "Large language models (LLMs) often struggle with complex reasoning tasks due\nto their limitations in addressing the vast reasoning space and inherent\nambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA)\nparadigm, a novel approach leveraging the collective expertise of multiple LLMs\nto enhance search-based reasoning. MoSA integrates diverse reasoning pathways\nby combining independent exploration with iterative refinement among LLMs,\nmitigating the limitations of single-model approaches. Using Monte Carlo Tree\nSearch (MCTS) as a backbone, MoSA enables multiple agents to propose and\naggregate reasoning steps, resulting in improved accuracy. Our comprehensive\nevaluation across four reasoning benchmarks demonstrates MoSA's consistent\nperformance improvements over single-agent and other multi-agent baselines,\nparticularly in complex mathematical and commonsense reasoning tasks.", "published": "2025-02-26 06:31:04", "link": "http://arxiv.org/abs/2502.18873v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust\n  Framework", "abstract": "Large Language Models (LLMs) are being used more and more extensively for\nautomated evaluation in various scenarios. Previous studies have attempted to\nfine-tune open-source LLMs to replicate the evaluation explanations and\njudgments of powerful proprietary models, such as GPT-4. However, these methods\nare largely limited to text-based analyses under predefined general criteria,\nresulting in reduced adaptability for unseen instructions and demonstrating\ninstability in evaluating adherence to quantitative and structural constraints.\nTo address these limitations, we propose a novel evaluation framework, ARJudge,\nthat adaptively formulates evaluation criteria and synthesizes both text-based\nand code-driven analyses to evaluate LLM responses. ARJudge consists of two\ncomponents: a fine-tuned Analyzer that generates multi-faceted evaluation\nanalyses and a tuning-free Refiner that combines and refines all analyses to\nmake the final judgment. We construct a Composite Analysis Corpus that\nintegrates tasks for evaluation criteria generation alongside text-based and\ncode-driven analysis generation to train the Analyzer. Our results demonstrate\nthat ARJudge outperforms existing fine-tuned evaluators in effectiveness and\nrobustness. Furthermore, it demonstrates the importance of multi-faceted\nevaluation and code-driven analyses in enhancing evaluation capabilities.", "published": "2025-02-26 06:31:45", "link": "http://arxiv.org/abs/2502.18874v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Pruning State-Space LLMs", "abstract": "Recent work proposed state-space models (SSMs) as an efficient alternative to\ntransformer-based LLMs. Can these models be pruned to further reduce their\ncomputation costs? We adapt several pruning methods to the SSM structure, and\napply them to four SSM-based LLMs across multiple tasks. We find that such\nmodels are quite robust to some pruning methods (e.g. WANDA), while using other\nmethods lead to fast performance degradation.", "published": "2025-02-26 07:04:20", "link": "http://arxiv.org/abs/2502.18886v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "END: Early Noise Dropping for Efficient and Effective Context Denoising", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, they are often\ndistracted by irrelevant or noisy context in input sequences that degrades\noutput quality. This problem affects both long- and short-context scenarios,\nsuch as retrieval-augmented generation, table question-answering, and\nin-context learning. We reveal that LLMs can implicitly identify whether input\nsequences contain useful information at early layers, prior to token\ngeneration. Leveraging this insight, we introduce Early Noise Dropping\n(\\textsc{END}), a novel approach to mitigate this issue without requiring\nfine-tuning the LLMs. \\textsc{END} segments input sequences into chunks and\nemploys a linear prober on the early layers of LLMs to differentiate between\ninformative and noisy chunks. By discarding noisy chunks early in the process,\n\\textsc{END} preserves critical information, reduces distraction, and lowers\ncomputational overhead. Extensive experiments demonstrate that \\textsc{END}\nsignificantly improves both performance and efficiency across different LLMs on\nmultiple evaluation datasets. Furthermore, by investigating LLMs' implicit\nunderstanding to the input with the prober, this work also deepens\nunderstanding of how LLMs do reasoning with contexts internally.", "published": "2025-02-26 08:07:17", "link": "http://arxiv.org/abs/2502.18915v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Kanana: Compute-efficient Bilingual Language Models", "abstract": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.", "published": "2025-02-26 08:36:20", "link": "http://arxiv.org/abs/2502.18934v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for\n  Large Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, highlighting the urgent need for comprehensive safety\nevaluations. In particular, the enhanced Chinese language proficiency of LLMs,\ncombined with the unique characteristics and complexity of Chinese expressions,\nhas driven the emergence of Chinese-specific benchmarks for safety assessment.\nHowever, these benchmarks generally fall short in effectively exposing LLM\nsafety vulnerabilities. To address the gap, we introduce JailBench, the first\ncomprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in\nLLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese\ncontext. To improve generation efficiency, we employ a novel Automatic\nJailbreak Prompt Engineer (AJPE) framework for JailBench construction, which\nincorporates jailbreak techniques to enhance assessing effectiveness and\nleverages LLMs to automatically scale up the dataset through context-learning.\nThe proposed JailBench is extensively evaluated over 13 mainstream LLMs and\nachieves the highest attack success rate against ChatGPT compared to existing\nChinese benchmarks, underscoring its efficacy in identifying latent\nvulnerabilities in LLMs, as well as illustrating the substantial room for\nimprovement in the security and trustworthiness of LLMs within the Chinese\ncontext. Our benchmark is publicly available at\nhttps://github.com/STAIR-BUPT/JailBench.", "published": "2025-02-26 08:36:42", "link": "http://arxiv.org/abs/2502.18935v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Label-Only Membership Inference Attack against Pre-trained Large\n  Language Models", "abstract": "Membership Inference Attacks (MIAs) aim to predict whether a data sample\nbelongs to the model's training set or not. Although prior research has\nextensively explored MIAs in Large Language Models (LLMs), they typically\nrequire accessing to complete output logits (\\ie, \\textit{logits-based\nattacks}), which are usually not available in practice. In this paper, we study\nthe vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only\nsetting}, where the adversary can only access generated tokens (text). We first\nreveal that existing label-only MIAs have minor effects in attacking\npre-trained LLMs, although they are highly effective in inferring fine-tuning\ndatasets used for personalized LLMs. We find that their failure stems from two\nmain reasons, including better generalization and overly coarse perturbation.\nSpecifically, due to the extensive pre-training corpora and exposing each\nsample only a few times, LLMs exhibit minimal robustness differences between\nmembers and non-members. This makes token-level perturbations too coarse to\ncapture such differences.\n  To alleviate these problems, we propose \\textbf{PETAL}: a label-only\nmembership inference attack based on \\textbf{PE}r-\\textbf{T}oken\nsem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages\ntoken-level semantic similarity to approximate output probabilities and\nsubsequently calculate the perplexity. It finally exposes membership based on\nthe common assumption that members are `better' memorized and have smaller\nperplexity. We conduct extensive experiments on the WikiMIA benchmark and the\nmore challenging MIMIR benchmark. Empirically, our PETAL performs better than\nthe extensions of existing label-only attacks against personalized LLMs and\neven on par with other advanced logit-based attacks across all metrics on five\nprevalent open-source LLMs.", "published": "2025-02-26 08:47:19", "link": "http://arxiv.org/abs/2502.18943v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Low-Confidence Gold: Refining Low-Confidence Samples for Efficient\n  Instruction Tuning", "abstract": "The effectiveness of instruction fine-tuning for Large Language Models is\nfundamentally constrained by the quality and efficiency of training datasets.\nThis work introduces Low-Confidence Gold (LCG), a novel filtering framework\nthat employs centroid-based clustering and confidence-guided selection for\nidentifying valuable instruction pairs. Through a semi-supervised approach\nusing a lightweight classifier trained on representative samples, LCG curates\nhigh-quality subsets while preserving data diversity. Experimental evaluation\ndemonstrates that models fine-tuned on LCG-filtered subsets of 6K samples\nachieve superior performance compared to existing methods, with substantial\nimprovements on MT-bench and consistent gains across comprehensive evaluation\nmetrics. The framework's efficacy while maintaining model performance\nestablishes a promising direction for efficient instruction tuning.", "published": "2025-02-26 09:37:21", "link": "http://arxiv.org/abs/2502.18978v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PEToolLLM: Towards Personalized Tool Learning in Large Language Models", "abstract": "Tool learning has emerged as a promising direction by extending Large\nLanguage Models' (LLMs) capabilities with external tools. Existing tool\nlearning studies primarily focus on the general-purpose tool-use capability,\nwhich addresses explicit user requirements in instructions. However, they\noverlook the importance of personalized tool-use capability, leading to an\ninability to handle implicit user preferences. To address the limitation, we\nfirst formulate the task of personalized tool learning, which integrates user's\ninteraction history towards personalized tool usage. To fill the gap of missing\nbenchmarks, we construct PEToolBench, featuring diverse user preferences\nreflected in interaction history under three distinct personalized settings,\nand encompassing a wide range of tool-use scenarios. Moreover, we propose a\nframework PEToolLLaMA to adapt LLMs to the personalized tool learning task,\nwhich is trained through supervised fine-tuning and direct preference\noptimization. Extensive experiments on PEToolBench demonstrate the superiority\nof PEToolLLaMA over existing LLMs.", "published": "2025-02-26 09:43:08", "link": "http://arxiv.org/abs/2502.18980v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MEBench: Benchmarking Large Language Models for Cross-Document\n  Multi-Entity Question Answering", "abstract": "Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures.", "published": "2025-02-26 09:56:51", "link": "http://arxiv.org/abs/2502.18993v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Binary Neural Networks for Large Language Model: A Survey", "abstract": "Large language models (LLMs) have wide applications in the field of natural\nlanguage processing(NLP), such as GPT-4 and Llama. However, with the\nexponential growth of model parameter sizes, LLMs bring significant resource\noverheads. Low-bit quantization, as a key technique, reduces memory usage and\ncomputational demands by decreasing the bit-width of model parameters,\nactivations, and gradients. Previous quantization methods for LLMs have largely\nemployed Post-Training Quantization (PTQ) and Quantization-Aware Training\n(QAT). PTQ does not require any retraining of the original model, while QAT\ninvolves optimizing precision during training to achieve the best quantization\nparameters. The BitNet team proposed a radically different approach, where\nquantization is performed from the start of model training, utilizing\nlow-precision binary weights during the training process. This approach has led\nto the emergence of many binary quantization techniques for large language\nmodels. This paper provides a comprehensive review of these binary quantization\ntechniques. Specifically, we will introduce binary quantization techniques in\ndeep neural networks and further explore their application to LLMs, reviewing\ntheir various contributions, implementations, and applications.", "published": "2025-02-26 10:14:19", "link": "http://arxiv.org/abs/2502.19008v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across\n  Indic Languages", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation from natural language prompts, revolutionizing software\ndevelopment workflows. As we advance towards agent-based development paradigms,\nthese models form the cornerstone of next-generation software development\nlifecycles. However, current benchmarks for evaluating multilingual code\ngeneration capabilities are predominantly English-centric, limiting their\napplicability across the global developer community. To address this\nlimitation, we present IndicEval-XL, a comprehensive benchmark for code\ngeneration that incorporates 6 major Indic languages, collectively spoken by\napproximately 14\\% of the world's population. Our benchmark bridges these\nlanguages with 12 programming languages, creating a robust evaluation\nframework. This work is particularly significant given India's representation\nof one-eighth of the global population and the crucial role Indic languages\nplay in Indian society. IndicEval-XL represents a significant step toward\nexpanding the linguistic diversity in code generation systems and evaluation\nframeworks. By developing resources that support multiple languages, we aim to\nmake AI-powered development tools more inclusive and accessible to developers\nof various linguistic backgrounds. To facilitate further research and\ndevelopment in this direction, we make our dataset and evaluation benchmark\npublicly available at https://github.com/telekom/IndicEval-XL", "published": "2025-02-26 11:48:42", "link": "http://arxiv.org/abs/2502.19067v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Conformal Linguistic Calibration: Trading-off between Factuality and\n  Specificity", "abstract": "Language model outputs are not always reliable; this prompts research into\nmethods for adapting model responses based on uncertainty. Common approaches\ninclude: \\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unifying view of both\napproaches, Conformal Linguistic Calibration (CLC), reinterpreting linguistic\ncalibration as answer set prediction. We begin by presenting a unified\nframework that connects abstention and linguistic calibration through the lens\nof linguistic pragmatics. We then describe an implementation that allows for\ncontrolling the level of imprecision in model responses. Experimental results\nshow that our method produces calibrated outputs with conformal guarantees on\nfactual accuracy. Furthermore, our approach enables fine-tuning models to\nperform uncertainty-aware adaptive claim rewriting, offering a controllable\nbalance between factuality and specificity.", "published": "2025-02-26 13:01:49", "link": "http://arxiv.org/abs/2502.19110v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Amulet: ReAlignment During Test Time for Personalized Preference\n  Adaptation of LLMs", "abstract": "How to align large language models (LLMs) with user preferences from a static\ngeneral dataset has been frequently studied. However, user preferences are\nusually personalized, changing, and diverse regarding culture, values, or time.\nThis leads to the problem that the actual user preferences often do not\ncoincide with those trained by the model developers in the practical use of\nLLMs. Since we cannot collect enough data and retrain for every demand,\nresearching efficient real-time preference adaptation methods based on the\nbackbone LLMs during test time is important. To this end, we introduce Amulet,\na novel, training-free framework that formulates the decoding process of every\ntoken as a separate online learning problem with the guidance of simple\nuser-provided prompts, thus enabling real-time optimization to satisfy users'\npersonalized preferences. To reduce the computational cost brought by this\noptimization process for each token, we additionally provide a closed-form\nsolution for each iteration step of the optimization process, thereby reducing\nthe computational time cost to a negligible level. The detailed experimental\nresults demonstrate that Amulet can achieve significant performance\nimprovements in rich settings with combinations of different LLMs, datasets,\nand user preferences, while maintaining acceptable computational efficiency.", "published": "2025-02-26 14:07:37", "link": "http://arxiv.org/abs/2502.19148v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with\n  PseudoEval", "abstract": "Existing code generation benchmarks for Large Language Models (LLMs) such as\nHumanEval and MBPP are designed to study LLMs' end-to-end performance, where\nthe benchmarks feed a problem description in natural language as input and\nexamine the generated code in specific programming languages. However, the\nevaluation scores revealed in this way provide a little hint as to the\nbottleneck of the code generation -- whether LLMs are struggling with their\nproblem-solving capability or language-coding capability. To answer this\nquestion, we construct PseudoEval, a multilingual code generation benchmark\nthat provides a solution written in pseudocode as input. By doing so, the\nbottleneck of code generation in various programming languages could be\nisolated and identified. Our study yields several interesting findings. For\nexample, we identify that the bottleneck of LLMs in Python programming is\nproblem-solving, while Rust is struggling relatively more in language-coding.\nAlso, our study indicates that problem-solving capability may transfer across\nprogramming languages, while language-coding needs more language-specific\neffort, especially for undertrained programming languages. Finally, we release\nthe pipeline of constructing PseudoEval to facilitate the extension to existing\nbenchmarks. PseudoEval is available at:\nhttps://anonymous.4open.science/r/PseudocodeACL25-7B74.", "published": "2025-02-26 14:08:17", "link": "http://arxiv.org/abs/2502.19149v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "When Personalization Meets Reality: A Multi-Faceted Analysis of\n  Personalized Preference Learning", "abstract": "While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems.", "published": "2025-02-26 14:14:58", "link": "http://arxiv.org/abs/2502.19158v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Linguistic Indicators for Stereotype Assessment with Large\n  Language Models", "abstract": "Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.", "published": "2025-02-26 14:15:28", "link": "http://arxiv.org/abs/2502.19160v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis", "abstract": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models have shown promise in supporting DDx,\nexisting approaches face key limitations, including single-dataset evaluations,\nisolated optimization of components, unrealistic assumptions about complete\npatient profiles, and single-attempt diagnosis. We introduce a Modular\nExplainable DDx Agent (MEDDxAgent) framework designed for interactive DDx,\nwhere diagnostic reasoning evolves through iterative learning, rather than\nassuming a complete patient profile is accessible. MEDDxAgent integrates three\nmodular components: (1) an orchestrator (DDxDriver), (2) a history taking\nsimulator, and (3) two specialized agents for knowledge retrieval and diagnosis\nstrategy. To ensure robust evaluation, we introduce a comprehensive DDx\nbenchmark covering respiratory, skin, and rare diseases. We analyze single-turn\ndiagnostic approaches and demonstrate the importance of iterative refinement\nwhen patient profiles are not available at the outset. Our broad evaluation\ndemonstrates that MEDDxAgent achieves over 10% accuracy improvements in\ninteractive DDx across both large and small LLMs, while offering critical\nexplainability into its diagnostic reasoning process.", "published": "2025-02-26 14:31:43", "link": "http://arxiv.org/abs/2502.19175v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating\n  the Interconnectedness of Knowledge", "abstract": "Various studies have attempted to remove sensitive or private knowledge from\na language model to prevent its unauthorized exposure. However, prior studies\nhave overlooked the complex and interconnected nature of knowledge, where\nrelated knowledge must be carefully examined. Specifically, they have failed to\nevaluate whether an unlearning method faithfully erases interconnected\nknowledge that should be removed, retaining knowledge that appears relevant but\nexists in a completely different context. To resolve this problem, we first\ndefine a new concept called superficial unlearning, which refers to the\nphenomenon where an unlearning method either fails to erase the interconnected\nknowledge it should remove or unintentionally erases irrelevant knowledge.\nBased on the definition, we introduce a new benchmark, FaithUn, to analyze and\nevaluate the faithfulness of unlearning in real-world knowledge QA settings.\nFurthermore, we propose a novel unlearning method, KLUE, which updates only\nknowledge-related neurons to achieve faithful unlearning. KLUE identifies\nknowledge neurons using an explainability method and updates only those neurons\nusing selected unforgotten samples. Experimental results demonstrate that\nwidely-used unlearning methods fail to ensure faithful unlearning, while our\nmethod shows significant effectiveness in real-world QA unlearning.", "published": "2025-02-26 15:11:03", "link": "http://arxiv.org/abs/2502.19207v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable\n  Correctness Signals for Reliable Reward Systems", "abstract": "Reward models (RMs) are crucial for the training and inference-time scaling\nup of large language models (LLMs). However, existing reward models primarily\nfocus on human preferences, neglecting verifiable correctness signals which\nhave shown strong potential in training LLMs. In this paper, we propose agentic\nreward modeling, a reward system that combines reward models with verifiable\ncorrectness signals from different aspects to provide reliable rewards. We\nempirically implement a reward agent, named RewardAgent, that combines human\npreference rewards with two verifiable signals: factuality and instruction\nfollowing, to provide more reliable rewards. We conduct comprehensive\nexperiments on existing reward model benchmarks and inference time best-of-n\nsearches on real-world downstream tasks. RewardAgent significantly outperforms\nvanilla reward models, demonstrating its effectiveness. We further construct\ntraining preference pairs using RewardAgent and train an LLM with the DPO\nobjective, achieving superior performance on various NLP benchmarks compared to\nconventional reward models. Our codes are publicly released to facilitate\nfurther research (https://github.com/THU-KEG/Agentic-Reward-Modeling).", "published": "2025-02-26 17:19:12", "link": "http://arxiv.org/abs/2502.19328v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Controlled Diversity: Length-optimized Natural Language Generation", "abstract": "LLMs are not generally able to adjust the length of their outputs based on\nstrict length requirements, a capability that would improve their usefulness in\napplications that require adherence to diverse user and system requirements. We\npresent an approach to train LLMs to acquire this capability by augmenting\nexisting data and applying existing fine-tuning techniques, which we compare\nbased on the trained models' adherence to the length requirement and overall\nresponse quality relative to the baseline model. Our results demonstrate that\nthese techniques can be successfully applied to train LLMs to adhere to length\nrequirements, with the trained models generating texts which better align to\nthe length requirements. Our results indicate that our method may change the\nresponse quality when using training data that was not generated by the\nbaseline model. This allows simultaneous alignment to another training\nobjective in certain scenarios, but is undesirable otherwise. Training on a\ndataset containing the model's own responses eliminates this issue.", "published": "2025-02-26 17:38:58", "link": "http://arxiv.org/abs/2502.19347v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DataMan: Data Manager for Pre-training Large Language Models", "abstract": "The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.", "published": "2025-02-26 18:01:19", "link": "http://arxiv.org/abs/2502.19363v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Residual Speech Embeddings for Tone Classification: Removing Linguistic\n  Content to Enhance Paralinguistic Analysis", "abstract": "Self-supervised learning models for speech processing, such as wav2vec2,\nHuBERT, WavLM, and Whisper, generate embeddings that capture both linguistic\nand paralinguistic information, making it challenging to analyze tone\nindependently of spoken content. In this work, we introduce a method for\ndisentangling paralinguistic features from linguistic content by regressing\nspeech embeddings onto their corresponding text embeddings and using the\nresiduals as a representation of vocal tone. We evaluate this approach across\nmultiple self-supervised speech embeddings, demonstrating that residual\nembeddings significantly improve tone classification performance compared to\nraw speech embeddings. Our results show that this method enhances linear\nseparability, enabling improved classification even with simple models such as\nlogistic regression. Visualization of the residual embeddings further confirms\nthe successful removal of linguistic information while preserving tone-related\nfeatures. These findings highlight the potential of residual embeddings for\napplications in sentiment analysis, speaker characterization, and\nparalinguistic speech processing.", "published": "2025-02-26 18:32:15", "link": "http://arxiv.org/abs/2502.19387v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning Code-Edit Embedding to Model Student Debugging Behavior", "abstract": "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior.", "published": "2025-02-26 18:54:39", "link": "http://arxiv.org/abs/2502.19407v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Norm Growth and Stability Challenges in Localized Sequential Knowledge\n  Editing", "abstract": "This study investigates the impact of localized updates to large language\nmodels (LLMs), specifically in the context of knowledge editing - a task aimed\nat incorporating or modifying specific facts without altering broader model\ncapabilities. We first show that across different post-training interventions\nlike continuous pre-training, full fine-tuning and LORA-based fine-tuning, the\nFrobenius norm of the updated matrices always increases. This increasing norm\nis especially detrimental for localized knowledge editing, where only a subset\nof matrices are updated in a model . We reveal a consistent phenomenon across\nvarious editing techniques, including fine-tuning, hypernetwork-based\napproaches, and locate-and-edit methods: the norm of the updated matrix\ninvariably increases with successive updates. Such growth disrupts model\nbalance, particularly when isolated matrices are updated while the rest of the\nmodel remains static, leading to potential instability and degradation of\ndownstream performance. Upon deeper investigations of the intermediate\nactivation vectors, we find that the norm of internal activations decreases and\nis accompanied by shifts in the subspaces occupied by these activations, which\nshows that these activation vectors now occupy completely different regions in\nthe representation space compared to the unedited model. With our paper, we\nhighlight the technical challenges with continuous and localized sequential\nknowledge editing and their implications for maintaining model stability and\nutility.", "published": "2025-02-26 18:58:30", "link": "http://arxiv.org/abs/2502.19416v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cognitive networks highlight differences and similarities in the STEM\n  mindsets of human and LLM-simulated trainees, experts and academics", "abstract": "Understanding attitudes towards STEM means quantifying the cognitive and\nemotional ways in which individuals, and potentially large language models too,\nconceptualise such subjects. This study uses behavioural forma mentis networks\n(BFMNs) to investigate the STEM-focused mindset, i.e. ways of associating and\nperceiving ideas, of 177 human participants and 177 artificial humans simulated\nby GPT-3.5. Participants were split in 3 groups - trainees, experts and\nacademics - to compare the influence of expertise level on their mindset. The\nresults revealed that human forma mentis networks exhibited significantly\nhigher clustering coefficients compared to GPT-3.5, indicating that human\nmindsets displayed a tendency to form and close triads of conceptual\nassociations while recollecting STEM ideas. Human experts, in particular,\ndemonstrated robust clustering coefficients, reflecting better integration of\nSTEM concepts into their cognitive networks. In contrast, GPT-3.5 produced\nsparser mindsets. Furthermore, both human and GPT mindsets framed mathematics\nin neutral or positive terms, differently from STEM high schoolers, researchers\nand other large language models sampled in other works. This research\ncontributes to understanding how mindset structure can provide cognitive\ninsights about memory structure and machine limitations.", "published": "2025-02-26 20:02:51", "link": "http://arxiv.org/abs/2502.19529v1", "categories": ["cs.CL", "cs.AI", "68T01 (Primary), 05C82 (Secondary)"], "primary_category": "cs.CL"}
{"title": "Distill Not Only Data but Also Rewards: Can Smaller Language Models\n  Surpass Larger Ones?", "abstract": "Distilling large language models (LLMs) typically involves transferring the\nteacher model's responses through supervised fine-tuning (SFT). However, this\napproach neglects the potential to distill both data (output content) and\nreward signals (quality evaluations). Extracting reliable reward signals\ndirectly from teacher models is challenging, as LLMs are optimized for\ngeneration rather than evaluation, often resulting in biased or inconsistent\nassessments. To address this limitation, we propose a novel distillation\npipeline that transfers both responses and rewards. Our method generates\npseudo-rewards through a self-supervised mechanism that leverages the inherent\nstructure of both teacher and student responses, enabling reward learning\nwithout explicit external evaluation. The reward model subsequently guides\nreinforcement learning (RL), allowing iterative refinement of the student model\nafter an SFT warm-up phase. Experiments on GSM8K and MMLU-PRO demonstrate that\nour method consistently outperforms traditional SFT-based approaches, enabling\nstudent models to surpass the performance of their teachers. This work\nhighlights the potential for scalable, efficient distillation through\nstructured self-supervised reward learning, reducing dependence on external\nreward supervision.", "published": "2025-02-26 20:50:11", "link": "http://arxiv.org/abs/2502.19557v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NeoBERT: A Next-Generation BERT", "abstract": "Recent innovations in architecture, pre-training, and fine-tuning have led to\nthe remarkable in-context learning and reasoning abilities of large\nauto-regressive language models such as LLaMA and DeepSeek. In contrast,\nencoders like BERT and RoBERTa have not seen the same level of progress despite\nbeing foundational for many downstream NLP applications. To bridge this gap, we\nintroduce NeoBERT, a next-generation encoder that redefines the capabilities of\nbidirectional models by integrating state-of-the-art advancements in\narchitecture, modern data, and optimized pre-training methodologies. NeoBERT is\ndesigned for seamless adoption: it serves as a plug-and-play replacement for\nexisting base models, relies on an optimal depth-to-width ratio, and leverages\nan extended context length of 4,096 tokens. Despite its compact 250M parameter\nfootprint, it achieves state-of-the-art results on the massive MTEB benchmark,\noutperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under\nidentical fine-tuning conditions. In addition, we rigorously evaluate the\nimpact of each modification on GLUE and design a uniform fine-tuning and\nevaluation framework for MTEB. We release all code, data, checkpoints, and\ntraining scripts to accelerate research and real-world adoption.", "published": "2025-02-26 22:00:22", "link": "http://arxiv.org/abs/2502.19587v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Hate Speech Detection Using Large Language Models and\n  Geographical Contextualization", "abstract": "The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.", "published": "2025-02-26 22:59:36", "link": "http://arxiv.org/abs/2502.19612v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6; K.4.2"], "primary_category": "cs.CL"}
{"title": "Is Your Paper Being Reviewed by an LLM? A New Benchmark Dataset and\n  Approach for Detecting AI Text in Peer Review", "abstract": "Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in large language models (LLMs), a new risk to the peer review\nprocess is that negligent reviewers will rely on LLMs to perform the often time\nconsuming process of reviewing a paper. However, there is a lack of existing\nresources for benchmarking the detectability of AI text in the domain of peer\nreview.\n  To address this deficiency, we introduce a comprehensive dataset containing a\ntotal of 788,984 AI-written peer reviews paired with corresponding human\nreviews, covering 8 years of papers submitted to each of two leading AI\nresearch conferences (ICLR and NeurIPS). We use this new resource to evaluate\nthe ability of 18 existing AI text detection algorithms to distinguish between\npeer reviews written by humans and different state-of-the-art LLMs. Motivated\nby the shortcomings of existing methods, we propose a new detection approach\nwhich surpasses existing methods in the identification of AI written peer\nreviews. Our work reveals the difficulty of identifying AI-generated text at\nthe individual peer review level, highlighting the urgent need for new tools\nand methods to detect this unethical use of generative AI.", "published": "2025-02-26 23:04:05", "link": "http://arxiv.org/abs/2502.19614v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's\n  Mathematical Reasoning", "abstract": "Recent advances in Large Language Models (LLMs) have raised interest in their\nformal reasoning capabilities, particularly in mathematics. While closed LLMs\nlike GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains\nunclear whether small to medium-sized open LLMs can achieve similar\nperformance, questioning their reliability. To close this gap, we propose a\npost-training approach leveraging a mixture of opinions (MoO) from weaker\nancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that,\neach post-training sample is augmented with Chain-of-Thought (CoT) reasoning\nsteps and answers from ancillary LLMs, enabling the main LLM to learn from\ndiverse perspectives. We compare MoO with standard supervised fine-tuning\n(SFT), few-shot prompting, and the Mixture of Agents (MoA) method on\nmathematical reasoning benchmarks. Our results show that incorporating weaker\nLLMs' opinions improves mathematical reasoning by an average of 5%,\nhighlighting the value of diverse perspectives in reasoning tasks.", "published": "2025-02-26 23:22:02", "link": "http://arxiv.org/abs/2502.19622v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improved YOLOv12 with LLM-Generated Synthetic Data for Enhanced Apple\n  Detection and Benchmarking Against YOLOv11 and YOLOv10", "abstract": "This study evaluated the performance of the YOLOv12 object detection model,\nand compared against the performances YOLOv11 and YOLOv10 for apple detection\nin commercial orchards based on the model training completed entirely on\nsynthetic images generated by Large Language Models (LLMs). The YOLOv12n\nconfiguration achieved the highest precision at 0.916, the highest recall at\n0.969, and the highest mean Average Precision (mAP@50) at 0.978. In comparison,\nthe YOLOv11 series was led by YOLO11x, which achieved the highest precision at\n0.857, recall at 0.85, and mAP@50 at 0.91. For the YOLOv10 series, YOLOv10b and\nYOLOv10l both achieved the highest precision at 0.85, with YOLOv10n achieving\nthe highest recall at 0.8 and mAP@50 at 0.89. These findings demonstrated that\nYOLOv12, when trained on realistic LLM-generated datasets surpassed its\npredecessors in key performance metrics. The technique also offered a\ncost-effective solution by reducing the need for extensive manual data\ncollection in the agricultural field. In addition, this study compared the\ncomputational efficiency of all versions of YOLOv12, v11 and v10, where\nYOLOv11n reported the lowest inference time at 4.7 ms, compared to YOLOv12n's\n5.6 ms and YOLOv10n's 5.9 ms. Although YOLOv12 is new and more accurate than\nYOLOv11, and YOLOv10, YOLO11n still stays the fastest YOLO model among YOLOv10,\nYOLOv11 and YOLOv12 series of models. (Index: YOLOv12, YOLOv11, YOLOv10,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO Object detection)", "published": "2025-02-26 20:24:01", "link": "http://arxiv.org/abs/2503.00057v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Where is my Glass Slipper? AI, Poetry and Art", "abstract": "This literature review interrogates the intersections between artificial\nintelligence, poetry, and art, offering a comprehensive exploration of both\nhistorical evolution and current debates in digital creative practices. It\ntraces the development of computer-generated poetry from early template-based\nsystems to generative models, critically assessing evaluative frameworks such\nas adaptations of the Turing Test, the FACE model, and ProFTAP. It also\nexamines how these frameworks endeavour to measure creativity, semantic\ncoherence, and cultural relevance in AI-generated texts, whilst highlighting\nthe persistent challenges in replicating the nuance of human poetic expression.\n  The review contributes a Marketing Theory discussion that deconstructs the\nfigurative marketing narratives employed by AI companies, which utilise\nsanitised language and anthropomorphic metaphors to humanise their\ntechnologies. This discussion reveals the reductive nature of such narratives\nand underscores the tension between algorithmic precision and the realities of\nhuman creativity.The review also incorporates an auto-ethnographic account that\noffers a self-reflexive commentary on its own composition. By acknowledging the\nuse of AI in crafting this review, the auto-ethnographic account destabilises\nconventional notions of authorship and objectivity, resonating with\ndeconstruction and challenging logocentric assumptions in academic discourse.\n  Ultimately, the review calls for a re-evaluation of creative processes that\nrecognises the interdependence of technological innovation and human\nsubjectivity. It advocates for interdisciplinary dialogue addressing ethical,\ncultural, and philosophical concerns, while reimagining the boundaries of\nartistic production.", "published": "2025-02-26 14:57:03", "link": "http://arxiv.org/abs/2503.05781v1", "categories": ["cs.CY", "cs.CL", "I.2.7; J.5"], "primary_category": "cs.CY"}
{"title": "Talking to the brain: Using Large Language Models as Proxies to Model\n  Brain Semantic Representation", "abstract": "Traditional psychological experiments utilizing naturalistic stimuli face\nchallenges in manual annotation and ecological validity. To address this, we\nintroduce a novel paradigm leveraging multimodal large language models (LLMs)\nas proxies to extract rich semantic information from naturalistic images\nthrough a Visual Question Answering (VQA) strategy for analyzing human visual\nsemantic representation. LLM-derived representations successfully predict\nestablished neural activity patterns measured by fMRI (e.g., faces, buildings),\nvalidating its feasibility and revealing hierarchical semantic organization\nacross cortical regions. A brain semantic network constructed from LLM-derived\nrepresentations identifies meaningful clusters reflecting functional and\ncontextual associations. This innovative methodology offers a powerful solution\nfor investigating brain semantic organization with naturalistic stimuli,\novercoming limitations of traditional annotation methods and paving the way for\nmore ecologically valid explorations of human cognition.", "published": "2025-02-26 00:40:28", "link": "http://arxiv.org/abs/2502.18725v1", "categories": ["cs.AI", "cs.CL", "q-bio.NC"], "primary_category": "cs.AI"}
{"title": "Reward Shaping to Mitigate Reward Hacking in RLHF", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning\nlarge language models (LLMs) with human values. However, RLHF is susceptible to\nreward hacking, where the agent exploits flaws in the reward function rather\nthan learning the intended behavior, thus degrading alignment. While reward\nshaping helps stabilize RLHF and partially mitigate reward hacking, a\nsystematic investigation into shaping techniques and their underlying\nprinciples remains lacking. To bridge this gap, we present a comprehensive\nstudy of the prevalent reward shaping methods. Our analysis suggests three key\ndesign principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid\ninitial growth followed by gradual convergence, and (3) RL reward is best\nformulated as a function of centered reward. Guided by these insights, we\npropose Preference As Reward (PAR), a novel approach that leverages the latent\npreferences embedded within the reward model itself as the signal for\nreinforcement learning. We evaluated PAR on two base models, Gemma2-2B and\nLlama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.\nExperimental results demonstrate PAR's superior performance over other reward\nshaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at\nleast 5 percentage points higher than competing approaches. Furthermore, PAR\nexhibits remarkable data efficiency, requiring only a single reference reward\nfor optimal performance, and maintains robustness against reward hacking even\nafter two full epochs of training. Code is available at\nhttps://github.com/PorUna-byte/PAR.", "published": "2025-02-26 02:57:59", "link": "http://arxiv.org/abs/2502.18770v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with\n  Competitive Performance", "abstract": "We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves\ncompetitive performance to GPT-4o. M2-omni employs a unified multimodal\nsequence modeling framework, which empowers Large Language Models(LLMs) to\nacquire comprehensive cross-modal understanding and generation capabilities.\nSpecifically, M2-omni can process arbitrary combinations of audio, video,\nimage, and text modalities as input, generating multimodal sequences\ninterleaving with audio, image, or text outputs, thereby enabling an advanced\nand interactive real-time experience. The training of such an omni-MLLM is\nchallenged by significant disparities in data quantity and convergence rates\nacross modalities. To address these challenges, we propose a step balance\nstrategy during pre-training to handle the quantity disparities in\nmodality-specific data. Additionally, a dynamically adaptive balance strategy\nis introduced during the instruction tuning stage to synchronize the\nmodality-wise training progress, ensuring optimal convergence. Notably, we\nprioritize preserving strong performance on pure text tasks to maintain the\nrobustness of M2-omni's language understanding capability throughout the\ntraining process. To our best knowledge, M2-omni is currently a very\ncompetitive open-source model to GPT-4o, characterized by its comprehensive\nmodality and task support, as well as its exceptional performance. We expect\nM2-omni will advance the development of omni-MLLMs, thus facilitating future\nresearch in this domain.", "published": "2025-02-26 03:21:12", "link": "http://arxiv.org/abs/2502.18778v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Seeing the Forest for the Trees: A Large Scale, Continuously Updating\n  Meta-Analysis of Frontier LLMs", "abstract": "The surge of LLM studies makes synthesizing their findings challenging.\nMeta-analysis can uncover important trends across studies, but its use is\nlimited by the time-consuming nature of manual data extraction. Our study\npresents a semi-automated approach for meta-analysis that accelerates data\nextraction using LLMs. It automatically identifies relevant arXiv papers,\nextracts experimental results and related attributes, and organizes them into a\nstructured dataset. We conduct a comprehensive meta-analysis of frontier LLMs\nusing an automatically extracted dataset, reducing the effort of paper\nsurveying and data extraction by more than 93\\% compared to manual approaches.\nWe validate our dataset by showing that it reproduces key findings from a\nrecent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new\ninsights that go beyond it, showing for example that in-context examples\nbenefit multimodal tasks but offer limited gains in mathematical tasks compared\nto CoT. Our automatically updatable dataset enables continuous tracking of\ntarget models by extracting evaluation studies as new data becomes available.\nThrough our scientific artifacts and empirical analysis, we provide novel\ninsights into LLMs while facilitating ongoing meta-analyses of their behavior.", "published": "2025-02-26 03:56:34", "link": "http://arxiv.org/abs/2502.18791v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph\n  Traversal and Redundancy Removal", "abstract": "In recent years, Large Language Models (LLMs) have faced increasing demands\nto selectively remove sensitive information, protect privacy, and comply with\ncopyright regulations through unlearning, by Machine Unlearning. While\nevaluating unlearning effectiveness is crucial, existing benchmarks are limited\nin scale and comprehensiveness, typically containing only a few hundred test\ncases. We identify two critical challenges in generating holistic audit\ndatasets: ensuring audit adequacy and handling knowledge redundancy between\nforget and retain dataset. To address these challenges, we propose HANKER, an\nautomated framework for holistic audit dataset generation leveraging knowledge\ngraphs to achieve fine-grained coverage and eliminate redundant knowledge.\nApplying HANKER to the popular MUSE benchmark, we successfully generated over\n69,000 and 111,000 audit cases for the News and Books datasets respectively,\nidentifying thousands of knowledge memorization instances that the previous\nbenchmark failed to detect. Our empirical analysis uncovers how knowledge\nredundancy significantly skews unlearning effectiveness metrics, with redundant\ninstances artificially inflating the observed memorization measurements ROUGE\nfrom 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the\nnecessity of systematic deduplication for accurate assessment.", "published": "2025-02-26 04:39:22", "link": "http://arxiv.org/abs/2502.18810v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; D.2.5; I.2.0"], "primary_category": "cs.AI"}
{"title": "Sliding Window Attention Training for Efficient Large Language Models", "abstract": "Recent advances in transformer-based Large Language Models (LLMs) have\ndemonstrated remarkable capabilities across various tasks. However, their\nquadratic computational complexity concerning sequence length remains a\nsignificant bottleneck for processing long documents. As a result, many efforts\nlike sparse attention and state space models have been proposed to improve the\nefficiency of LLMs over long sequences. Though effective, these approaches\ncompromise the performance or introduce structural complexity. This calls for a\nsimple yet efficient model that preserves the fundamental Transformer\narchitecture. To this end, we introduce SWAT, which enables efficient\nlong-context handling via Sliding Window Attention Training. This paper first\nattributes the inefficiency of Transformers to the attention sink phenomenon\nresulting from the high variance of softmax operation. Then, we replace softmax\nwith the sigmoid function and utilize a balanced ALiBi and Rotary Position\nEmbedding for efficient information compression and retention. Experiments\ndemonstrate that SWAT achieves SOTA performance compared with state-of-the-art\nlinear recurrent architectures on eight benchmarks. Code is available at\nhttps://anonymous.4open.science/r/SWAT-attention.", "published": "2025-02-26 05:31:44", "link": "http://arxiv.org/abs/2502.18845v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Causal Lens for Evaluating Faithfulness Metrics", "abstract": "Large Language Models (LLMs) offer natural language explanations as an\nalternative to feature attribution methods for model interpretability. However,\ndespite their plausibility, they may not reflect the model's internal reasoning\nfaithfully, which is crucial for understanding the model's true decision-making\nprocesses. Although several faithfulness metrics have been proposed, a unified\nevaluation framework remains absent. To address this gap, we present Causal\nDiagnosticity, a framework to evaluate faithfulness metrics for natural\nlanguage explanations. Our framework employs the concept of causal\ndiagnosticity, and uses model-editing methods to generate faithful-unfaithful\nexplanation pairs. Our benchmark includes four tasks: fact-checking, analogy,\nobject counting, and multi-hop reasoning. We evaluate a variety of faithfulness\nmetrics, including post-hoc explanation and chain-of-thought-based methods. We\nfind that all tested faithfulness metrics often fail to surpass a random\nbaseline. Our work underscores the need for improved metrics and more reliable\ninterpretability methods in LLMs.", "published": "2025-02-26 05:35:53", "link": "http://arxiv.org/abs/2502.18848v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English\n  Code-Switching Dialogues for Speech Recognition", "abstract": "Code-switching (CS), the alternation between two or more languages within a\nsingle conversation, presents significant challenges for automatic speech\nrecognition (ASR) systems. Existing Mandarin-English code-switching datasets\noften suffer from limitations in size, spontaneity, and the lack of full-length\ndialogue recordings with transcriptions, hindering the development of robust\nASR models for real-world conversational scenarios. This paper introduces\nCS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset\ncomprising 104 hours of spontaneous conversations from 200 speakers. Unlike\nprevious datasets, CS-Dialogue provides full-length dialogue recordings with\ncomplete transcriptions, capturing naturalistic code-switching patterns in\ncontinuous speech. We describe the data collection and annotation processes,\npresent detailed statistics of the dataset, and establish benchmark ASR\nperformance using state-of-the-art models. Our experiments, using Transformer,\nConformer, and Branchformer, demonstrate the challenges of code-switching ASR,\nand show that existing pre-trained models such as Whisper still have the space\nto improve. The CS-Dialogue dataset will be made freely available for all\nacademic purposes.", "published": "2025-02-26 07:59:55", "link": "http://arxiv.org/abs/2502.18913v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical\n  Capabilities of LLM Tutors", "abstract": "Evaluating the pedagogical capabilities of AI-based tutoring models is\ncritical for making guided progress in the field. Yet, we lack a reliable,\neasy-to-use, and simple-to-run evaluation that reflects the pedagogical\nabilities of models. To fill this gap, we present MathTutorBench, an\nopen-source benchmark for holistic tutoring model evaluation. MathTutorBench\ncontains a collection of datasets and metrics that broadly cover tutor\nabilities as defined by learning sciences research in dialog-based teaching. To\nscore the pedagogical quality of open-ended teacher responses, we train a\nreward model and show it can discriminate expert from novice teacher responses\nwith high accuracy. We evaluate a wide set of closed- and open-weight models on\nMathTutorBench and find that subject expertise, indicated by solving ability,\ndoes not immediately translate to good teaching. Rather, pedagogy and subject\nexpertise appear to form a trade-off that is navigated by the degree of\ntutoring specialization of the model. Furthermore, tutoring appears to become\nmore challenging in longer dialogs, where simpler questioning strategies begin\nto fail. We release the benchmark, code, and leaderboard openly to enable rapid\nbenchmarking of future models.", "published": "2025-02-26 08:43:47", "link": "http://arxiv.org/abs/2502.18940v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "(Mis)Fitting: A Survey of Scaling Laws", "abstract": "Modern foundation models rely heavily on using scaling laws to guide crucial\ntraining decisions. Researchers often extrapolate the optimal architecture and\nhyper parameters settings from smaller training runs by describing the\nrelationship between, loss, or task performance, and scale. All components of\nthis process vary, from the specific equation being fit, to the training setup,\nto the optimization method. Each of these factors may affect the fitted law,\nand therefore, the conclusions of a given study. We discuss discrepancies in\nthe conclusions that several prior works reach, on questions such as the\noptimal token to parameter ratio. We augment this discussion with our own\nanalysis of the critical impact that changes in specific details may effect in\na scaling study, and the resulting altered conclusions. Additionally, we survey\nover 50 papers that study scaling trends: while 45 of these papers quantify\nthese trends using a power law, most under-report crucial details needed to\nreproduce their findings. To mitigate this, we we propose a checklist for\nauthors to consider while contributing to scaling law research.", "published": "2025-02-26 09:27:54", "link": "http://arxiv.org/abs/2502.18969v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Ground-level Viewpoint Vision-and-Language Navigation in Continuous\n  Environments", "abstract": "Vision-and-Language Navigation (VLN) empowers agents to associate\ntime-sequenced visual observations with corresponding instructions to make\nsequential decisions. However, generalization remains a persistent challenge,\nparticularly when dealing with visually diverse scenes or transitioning from\nsimulated environments to real-world deployment. In this paper, we address the\nmismatch between human-centric instructions and quadruped robots with a\nlow-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav)\napproach to mitigate this issue. This work represents the first attempt to\nhighlight the generalization gap in VLN across varying heights of visual\nobservation in realistic robot deployments. Our approach leverages weighted\nhistorical observations as enriched spatiotemporal contexts for instruction\nfollowing, effectively managing feature collisions within cells by assigning\nappropriate weights to identical features across different viewpoints. This\nenables low-height robots to overcome challenges such as visual obstructions\nand perceptual mismatches. Additionally, we transfer the connectivity graph\nfrom the HM3D and Gibson datasets as an extra resource to enhance spatial\npriors and a more comprehensive representation of real-world scenarios, leading\nto improved performance and generalizability of the waypoint predictor in\nreal-world environments. Extensive experiments demonstrate that our\nGround-level Viewpoint Navigation (GVnav) approach significantly improves\nperformance in both simulated environments and real-world deployments with\nquadruped robots.", "published": "2025-02-26 10:30:40", "link": "http://arxiv.org/abs/2502.19024v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Improving Customer Service with Automatic Topic Detection in User Emails", "abstract": "This study introduces a novel natural language processing pipeline that\nenhances customer service efficiency at Telekom Srbija, a leading Serbian\ntelecommunications company, through automated email topic detection and\nlabeling. Central to the pipeline is BERTopic, a modular framework that allows\nunsupervised topic modeling. After a series of preprocessing and postprocessing\nsteps, we assign one of 12 topics and several additional labels to incoming\nemails, allowing the customer service to filter and access them through a\ncustom-made application. The model's performance was evaluated by assessing the\nspeed and correctness of the automatically assigned topics, with a weighted\naverage processing time of 0.041 seconds per email and a weighted average F1\nscore of 0.96. The pipeline shows broad applicability across languages,\nparticularly to those that are low-resourced and morphologically rich. The\nsystem now operates in the company's production environment, streamlining\ncustomer service operations through automated email classification.", "published": "2025-02-26 13:10:38", "link": "http://arxiv.org/abs/2502.19115v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Voting or Consensus? Decision-Making in Multi-Agent Debate", "abstract": "Much of the success of multi-agent debates depends on carefully choosing the\nright parameters. Among them, the decision-making protocol stands out.\nSystematic comparison of decision protocols is difficult because studies alter\nmultiple discussion parameters beyond the protocol. So far, it has been largely\nunknown how decision-making addresses the challenges of different tasks. This\nwork systematically evaluates the impact of seven decision protocols (e.g.,\nmajority voting, unanimity consensus). We change only one variable at a time\n(i.e., decision protocol) to analyze how different methods affect the\ncollaboration between agents and test different protocols on knowledge (MMLU,\nMMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQuAD 2.0). Our\nresults show that voting protocols improve performance by 13.2% in reasoning\ntasks and consensus protocols by 2.8% in knowledge tasks over the other\ndecision protocol. Increasing the number of agents improves performance, while\nmore discussion rounds before voting reduces it. To improve decision-making by\nincreasing answer diversity, we propose two new methods, All-Agents Drafting\n(AAD) and Collective Improvement (CI). Our methods improve task performance by\nup to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the\nimportance of decision-making in multi-agent debates beyond scaling.", "published": "2025-02-26 13:39:18", "link": "http://arxiv.org/abs/2502.19130v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA"}
{"title": "TestNUC: Enhancing Test-Time Computing Approaches through Neighboring\n  Unlabeled Data Consistency", "abstract": "Test-time computing approaches, which leverage additional computational\nresources during inference, have been proven effective in enhancing large\nlanguage model performance. This work introduces a novel, linearly scaling\napproach, TestNUC, that improves test-time predictions by leveraging the local\nconsistency of neighboring unlabeled data-it classifies an input instance by\nconsidering not only the model's prediction on that instance but also on\nneighboring unlabeled instances. We evaluate TestNUC across eight diverse\ndatasets, spanning intent classification, topic mining, domain discovery, and\nemotion detection, demonstrating its consistent superiority over baseline\nmethods such as standard prompting and self-consistency. Furthermore, TestNUC\ncan be seamlessly integrated with existing test-time computing approaches,\nsubstantially boosting their performance. Our analysis reveals that TestNUC\nscales effectively with increasing amounts of unlabeled data and performs\nrobustly across different embedding models, making it practical for real-world\napplications. Our code is available at https://github.com/HenryPengZou/TestNUC.", "published": "2025-02-26 14:17:56", "link": "http://arxiv.org/abs/2502.19163v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Between Circuits and Chomsky: Pre-pretraining on Formal Languages\n  Imparts Linguistic Biases", "abstract": "Pretraining language models on formal languages can improve their acquisition\nof natural language, but it is unclear which features of the formal language\nimpart an inductive bias that leads to effective transfer. Drawing on insights\nfrom linguistics and complexity theory, we hypothesize that effective transfer\noccurs when the formal language both captures dependency structures in natural\nlanguage and remains within the computational limitations of the model\narchitecture. Focusing on transformers, we find that formal languages with both\nthese properties enable language models to achieve lower loss on natural\nlanguage and better linguistic generalization compared to other languages. In\nfact, pre-pretraining, or training on formal-then-natural language, reduces\nloss more efficiently than the same amount of natural language. For a\n1B-parameter language model trained on roughly 1.6B tokens of natural language,\npre-pretraining achieves the same loss and better linguistic generalization\nwith a 33% smaller token budget. We also give mechanistic evidence of\ncross-task transfer from formal to natural language: attention heads acquired\nduring formal language pretraining remain crucial for the model's performance\non syntactic evaluations.", "published": "2025-02-26 15:55:55", "link": "http://arxiv.org/abs/2502.19249v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Drop-Upcycling: Training Sparse Mixture of Experts with Partial\n  Re-initialization", "abstract": "The Mixture of Experts (MoE) architecture reduces the training and inference\ncost significantly compared to a dense model of equivalent capacity. Upcycling\nis an approach that initializes and trains an MoE model using a pre-trained\ndense model. While upcycling leads to initial performance gains, the training\nprogresses slower than when trained from scratch, leading to suboptimal\nperformance in the long term. We propose Drop-Upcycling - a method that\neffectively addresses this problem. Drop-Upcycling combines two seemingly\ncontradictory approaches: utilizing the knowledge of pre-trained dense models\nwhile statistically re-initializing some parts of the weights. This approach\nstrategically promotes expert specialization, significantly enhancing the MoE\nmodel's efficiency in knowledge acquisition. Extensive large-scale experiments\ndemonstrate that Drop-Upcycling significantly outperforms previous MoE\nconstruction methods in the long term, specifically when training on hundreds\nof billions of tokens or more. As a result, our MoE model with 5.9B active\nparameters achieves comparable performance to a 13B dense model in the same\nmodel family, while requiring approximately 1/4 of the training FLOPs. All\nexperimental resources, including source code, training data, model checkpoints\nand logs, are publicly available to promote reproducibility and future research\non MoE.", "published": "2025-02-26 16:06:36", "link": "http://arxiv.org/abs/2502.19261v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding", "abstract": "Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations.", "published": "2025-02-26 18:50:09", "link": "http://arxiv.org/abs/2502.19400v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.AI"}
{"title": "ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal\n  Large Language Models", "abstract": "Reasoning over sequences of images remains a challenge for multimodal large\nlanguage models (MLLMs). While recent models incorporate multi-image data\nduring pre-training, they still struggle to recognize sequential structures,\noften treating images independently. This work introduces ImageChain, a\nframework that enhances MLLMs with sequential reasoning capabilities over image\ndata by modeling visual sequences as a multi-turn conversation. In ImageChain,\nimages are interleaved with corresponding textual descriptions to form a\ncontrolled dialogue that explicitly captures temporal dependencies and\nnarrative progression. Our method optimizes for the task of next-scene\ndescription, where the model generates a context-aware description of an\nupcoming scene based on preceding visual and textual cues. We demonstrate that\nour approach improves performance on the next-scene description task --\nachieving an average improvement from 3.7% to 19% in SimRate, a metric that\nquantifies semantic similarity to human-annotated ground truths. Moreover,\nImageChain achieves robust zero-shot out-of-domain performance in applications\nranging from comics to robotics. Extensive experiments validate that\ninstruction-tuning in a multimodal, multi-turn conversation design is key to\nbridging the gap between static image understanding and temporally-aware\nreasoning.", "published": "2025-02-26 18:55:06", "link": "http://arxiv.org/abs/2502.19409v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and\n  Reasoning-Driven Code Intelligence in LLMs", "abstract": "In large language models (LLMs), code and reasoning reinforce each other:\ncode offers an abstract, modular, and logic-driven structure that supports\nreasoning, while reasoning translates high-level goals into smaller, executable\nsteps that drive more advanced code intelligence. In this study, we examine how\ncode serves as a structured medium for enhancing reasoning: it provides\nverifiable execution paths, enforces logical decomposition, and enables runtime\nvalidation. We also explore how improvements in reasoning have transformed code\nintelligence from basic completion to advanced capabilities, enabling models to\naddress complex software engineering tasks through planning and debugging.\nFinally, we identify key challenges and propose future research directions to\nstrengthen this synergy, ultimately improving LLM's performance in both areas.", "published": "2025-02-26 18:55:42", "link": "http://arxiv.org/abs/2502.19411v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs", "abstract": "Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We urge the\ncommunity to adopt a new idea: convert scholarly documents into Knowledge Units\nusing LLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits: (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright.", "published": "2025-02-26 18:56:52", "link": "http://arxiv.org/abs/2502.19413v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Conversational Planning for Personal Plans", "abstract": "The language generation and reasoning capabilities of large language models\n(LLMs) have enabled conversational systems with impressive performance in a\nvariety of tasks, from code generation, to composing essays, to passing STEM\nand legal exams, to a new paradigm for knowledge search. Besides those\nshort-term use applications, LLMs are increasingly used to help with real-life\ngoals or tasks that take a long time to complete, involving multiple sessions\nacross days, weeks, months, or even years. Thus to enable conversational\nsystems for long term interactions and tasks, we need language-based agents\nthat can plan for long horizons. Traditionally, such capabilities were\naddressed by reinforcement learning agents with hierarchical planning\ncapabilities. In this work, we explore a novel architecture where the LLM acts\nas the meta-controller deciding the agent's next macro-action, and tool use\naugmented LLM-based option policies execute the selected macro-action. We\ninstantiate this framework for a specific set of macro-actions enabling\nadaptive planning for users' personal plans through conversation and follow-up\nquestions collecting user feedback. We show how this paradigm can be applicable\nin scenarios ranging from tutoring for academic and non-academic tasks to\nconversational coaching for personal health plans.", "published": "2025-02-26 19:04:26", "link": "http://arxiv.org/abs/2502.19500v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training\n  for Reducing Hallucination in QA Agents", "abstract": "The deployment of Large Language Models (LLMs) in customer support is\nconstrained by hallucination-generating false information-and the high cost of\nproprietary models. To address these challenges, we propose a\nretrieval-augmented question-answering (QA) pipeline and explore how to balance\nhuman input and automation. Using a dataset of questions about a Samsung Smart\nTV user manual, we demonstrate that synthetic data generated by LLMs\noutperforms crowdsourced data in reducing hallucination in finetuned models. We\nalso compare self-training (fine-tuning models on their own outputs) and\nknowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o),\nand find that self-training achieves comparable hallucination reduction. We\nconjecture that this surprising finding can be attributed to increased exposure\nbias issues in the knowledge distillation case and support this conjecture with\npost hoc analysis. We also improve robustness to unanswerable questions and\nretrieval failures with contextualized \"I don't know\" responses. These findings\nshow that scalable, cost-efficient QA systems can be built using synthetic data\nand self-training with open-source models, reducing reliance on proprietary\ntools or costly human annotations.", "published": "2025-02-26 20:34:58", "link": "http://arxiv.org/abs/2502.19545v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Repurposing the scientific literature with vision-language models", "abstract": "Research in AI for Science often focuses on using AI technologies to augment\ncomponents of the scientific process, or in some cases, the entire scientific\nmethod; how about AI for scientific publications? Peer-reviewed journals are\nfoundational repositories of specialized knowledge, written in\ndiscipline-specific language that differs from general Internet content used to\ntrain most large language models (LLMs) and vision-language models (VLMs). We\nhypothesized that by combining a family of scientific journals with generative\nAI models, we could invent novel tools for scientific communication, education,\nand clinical care. We converted 23,000 articles from Neurosurgery Publications\ninto a multimodal database - NeuroPubs - of 134 million words and 78,000\nimage-caption pairs to develop six datasets for building AI models. We showed\nthat the content of NeuroPubs uniquely represents neurosurgery-specific\nclinical contexts compared with broader datasets and PubMed. For publishing, we\nemployed generalist VLMs to automatically generate graphical abstracts from\narticles. Editorial board members rated 70% of these as ready for publication\nwithout further edits. For education, we generated 89,587 test questions in the\nstyle of the ABNS written board exam, which trainee and faculty neurosurgeons\nfound indistinguishable from genuine examples 54% of the time. We used these\nquestions alongside a curriculum learning process to track knowledge\nacquisition while training our 34 billion-parameter VLM (CNS-Obsidian). In a\nblinded, randomized controlled trial, we demonstrated the non-inferiority of\nCNS-Obsidian to GPT-4o (p = 0.1154) as a diagnostic copilot for a neurosurgical\nservice. Our findings lay a novel foundation for AI with Science and establish\na framework to elevate scientific communication using state-of-the-art\ngenerative artificial intelligence while maintaining rigorous quality\nstandards.", "published": "2025-02-26 20:35:37", "link": "http://arxiv.org/abs/2502.19546v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "When Large Language Models Meet Speech: A Survey on Integration\n  Approaches", "abstract": "Recent advancements in large language models (LLMs) have spurred interest in\nexpanding their application beyond text-based tasks. A large number of studies\nhave explored integrating other modalities with LLMs, notably speech modality,\nwhich is naturally related to text. This paper surveys the integration of\nspeech with LLMs, categorizing the methodologies into three primary approaches:\ntext-based, latent-representation-based, and audio-token-based integration. We\nalso demonstrate how these methods are applied across various speech-related\napplications and highlight the challenges in this field to offer inspiration\nfor", "published": "2025-02-26 20:40:49", "link": "http://arxiv.org/abs/2502.19548v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Know How Much They Know?", "abstract": "Large Language Models (LLMs) have emerged as highly capable systems and are\nincreasingly being integrated into various uses. However, the rapid pace of\ntheir deployment has outpaced a comprehensive understanding of their internal\nmechanisms and a delineation of their capabilities and limitations. A desired\nattribute of an intelligent system is its ability to recognize the scope of its\nown knowledge. To investigate whether LLMs embody this characteristic, we\ndevelop a benchmark designed to challenge these models to enumerate all\ninformation they possess on specific topics. This benchmark evaluates whether\nthe models recall excessive, insufficient, or the precise amount of\ninformation, thereby indicating their awareness of their own knowledge. Our\nfindings reveal that all tested LLMs, given sufficient scale, demonstrate an\nunderstanding of how much they know about specific topics. While different\narchitectures exhibit varying rates of this capability's emergence, the results\nsuggest that awareness of knowledge may be a generalizable attribute of LLMs.\nFurther research is needed to confirm this potential and fully elucidate the\nunderlying mechanisms.", "published": "2025-02-26 21:33:06", "link": "http://arxiv.org/abs/2502.19573v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A City of Millions: Mapping Literary Social Networks At Scale", "abstract": "We release 70,509 high-quality social networks extracted from multilingual\nfiction and nonfiction narratives. We additionally provide metadata for\n$\\sim$30,000 of these texts (73\\% nonfiction and 27\\% fiction) written between\n1800 and 1999 in 58 languages. This dataset provides information on historical\nsocial worlds at an unprecedented scale, including data for 2,510,021\nindividuals in 2,805,482 pair-wise relationships annotated for affinity and\nrelationship type. We achieve this scale by automating previously manual\nmethods of extracting social networks; specifically, we adapt an existing\nannotation task as a language model prompt, ensuring consistency at scale with\nthe use of structured output. This dataset serves as a unique resource for\nhumanities and social science research by providing data on cognitive models of\nsocial realities.", "published": "2025-02-26 22:11:47", "link": "http://arxiv.org/abs/2502.19590v2", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Time-MQA: Time Series Multi-Task Question Answering with Context\n  Enhancement", "abstract": "Time series data are foundational in finance, healthcare, and energy domains.\nHowever, most existing methods and datasets remain focused on a narrow spectrum\nof tasks, such as forecasting or anomaly detection. To bridge this gap, we\nintroduce Time Series Multi-Task Question Answering (Time-MQA), a unified\nframework that enables natural language queries across multiple time series\ntasks - numerical analytical tasks and open-ended question answering with\nreasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset\ncontaining $\\sim$200k question-answer pairs derived from diverse time series\nspanning environment, traffic, etc. This comprehensive resource covers various\ntime series lengths and promotes robust model development. We further\ndemonstrate how continually pre-training large language models (Mistral 7B,\nLlama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning\ncapabilities, moving beyond mere numeric tasks and enabling more advanced and\nintuitive interactions with temporal data. The complete TSQA dataset, models,\nexecutable codes, user study questionnaires for evaluation, and results have\nall been open-sourced.", "published": "2025-02-26 13:47:13", "link": "http://arxiv.org/abs/2503.01875v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BEYONDWORDS is All You Need: Agentic Generative AI based Social Media\n  Themes Extractor", "abstract": "Thematic analysis of social media posts provides a major understanding of\npublic discourse, yet traditional methods often struggle to capture the\ncomplexity and nuance of unstructured, large-scale text data. This study\nintroduces a novel methodology for thematic analysis that integrates tweet\nembeddings from pre-trained language models, dimensionality reduction using and\nmatrix factorization, and generative AI to identify and refine latent themes.\nOur approach clusters compressed tweet representations and employs generative\nAI to extract and articulate themes through an agentic Chain of Thought (CoT)\nprompting, with a secondary LLM for quality assurance. This methodology is\napplied to tweets from the autistic community, a group that increasingly uses\nsocial media to discuss their experiences and challenges. By automating the\nthematic extraction process, the aim is to uncover key insights while\nmaintaining the richness of the original discourse. This autism case study\ndemonstrates the utility of the proposed approach in improving thematic\nanalysis of social media data, offering a scalable and adaptable framework that\ncan be applied to diverse contexts. The results highlight the potential of\ncombining machine learning and Generative AI to enhance the depth and accuracy\nof theme identification in online communities.", "published": "2025-02-26 18:18:37", "link": "http://arxiv.org/abs/2503.01880v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Medical Hallucinations in Foundation Models and Their Impact on\n  Healthcare", "abstract": "Foundation Models that are capable of processing and generating multi-modal\ndata have transformed AI's role in medicine. However, a key limitation of their\nreliability is hallucination, where inaccurate or fabricated information can\nimpact clinical decisions and patient safety. We define medical hallucination\nas any instance in which a model generates misleading medical content. This\npaper examines the unique characteristics, causes, and implications of medical\nhallucinations, with a particular focus on how these errors manifest themselves\nin real-world clinical scenarios. Our contributions include (1) a taxonomy for\nunderstanding and addressing medical hallucinations, (2) benchmarking models\nusing medical hallucination dataset and physician-annotated LLM responses to\nreal medical cases, providing direct insight into the clinical impact of\nhallucinations, and (3) a multi-national clinician survey on their experiences\nwith medical hallucinations. Our results reveal that inference techniques such\nas Chain-of-Thought (CoT) and Search Augmented Generation can effectively\nreduce hallucination rates. However, despite these improvements, non-trivial\nlevels of hallucination persist. These findings underscore the ethical and\npractical imperative for robust detection and mitigation strategies,\nestablishing a foundation for regulatory policies that prioritize patient\nsafety and maintain clinical integrity as AI becomes more integrated into\nhealthcare. The feedback from clinicians highlights the urgent need for not\nonly technical advances but also for clearer ethical and regulatory guidelines\nto ensure patient safety. A repository organizing the paper resources,\nsummaries, and additional information is available at\nhttps://github.com/mitmedialab/medical hallucination.", "published": "2025-02-26 02:30:44", "link": "http://arxiv.org/abs/2503.05777v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DreamNet: A Multimodal Framework for Semantic and Emotional Analysis of\n  Sleep Narratives", "abstract": "Dream narratives provide a unique window into human cognition and emotion,\nyet their systematic analysis using artificial intelligence has been\nunderexplored. We introduce DreamNet, a novel deep learning framework that\ndecodes semantic themes and emotional states from textual dream reports,\noptionally enhanced with REM-stage EEG data. Leveraging a transformer-based\narchitecture with multimodal attention, DreamNet achieves 92.1% accuracy and\n88.4% F1-score in text-only mode (DNet-T) on a curated dataset of 1,500\nanonymized dream narratives, improving to 99.0% accuracy and 95.2% F1-score\nwith EEG integration (DNet-M). Strong dream-emotion correlations (e.g.,\nfalling-anxiety, r = 0.91, p < 0.01) highlight its potential for mental health\ndiagnostics, cognitive science, and personalized therapy. This work provides a\nscalable tool, a publicly available enriched dataset, and a rigorous\nmethodology, bridging AI and psychological research.", "published": "2025-02-26 09:10:07", "link": "http://arxiv.org/abs/2503.05778v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07", "I.2.7; I.2.6; J.3"], "primary_category": "cs.LG"}
{"title": "Human-Centered AI in Multidisciplinary Medical Discussions: Evaluating\n  the Feasibility of a Chat-Based Approach to Case Assessment", "abstract": "In this study, we investigate the feasibility of using a human-centered\nartificial intelligence (AI) chat platform where medical specialists\ncollaboratively assess complex cases. As the target population for this\nplatform, we focus on patients with cardiovascular diseases who are in a state\nof multimorbidity, that is, suffering from multiple chronic conditions. We\nevaluate simulated cases with multiple diseases using a chat application by\ncollaborating with physicians to assess feasibility, efficiency gains through\nAI utilization, and the quantification of discussion content. We constructed\nsimulated cases based on past case reports, medical errors reports and complex\ncases of cardiovascular diseases experienced by the physicians. The analysis of\ndiscussions across five simulated cases demonstrated a significant reduction in\nthe time required for summarization using AI, with an average reduction of\n79.98\\%. Additionally, we examined hallucination rates in AI-generated\nsummaries used in multidisciplinary medical discussions. The overall\nhallucination rate ranged from 1.01\\% to 5.73\\%, with an average of 3.62\\%,\nwhereas the harmful hallucination rate varied from 0.00\\% to 2.09\\%, with an\naverage of 0.49\\%. Furthermore, morphological analysis demonstrated that\nmultidisciplinary assessments enabled a more complex and detailed\nrepresentation of medical knowledge compared with single physician assessments.\nWe examined structural differences between multidisciplinary and single\nphysician assessments using centrality metrics derived from the knowledge\ngraph. In this study, we demonstrated that AI-assisted summarization\nsignificantly reduced the time required for medical discussions while\nmaintaining structured knowledge representation. These findings can support the\nfeasibility of AI-assisted chat-based discussions as a human-centered approach\nto multidisciplinary medical decision-making.", "published": "2025-02-26 01:02:47", "link": "http://arxiv.org/abs/2503.16464v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Multi-Perspective Attention Mechanism for Bias-Aware Sequential\n  Recommendation", "abstract": "In the era of advancing information technology, recommender systems have\nemerged as crucial tools for dealing with information overload. However,\ntraditional recommender systems still have limitations in capturing the dynamic\nevolution of user behavior. To better understand and predict user behavior,\nespecially taking into account the complexity of temporal evolution, sequential\nrecommender systems have gradually become the focus of research. Currently,\nmany sequential recommendation algorithms ignore the amplification effects of\nprevalent biases, which leads to recommendation results being susceptible to\nthe Matthew Effect. Additionally, it will impose limitations on the recommender\nsystem's ability to deeply perceive and capture the dynamic shifts in user\npreferences, thereby diminishing the extent of its recommendation reach. To\naddress this issue effectively, we propose a recommendation system based on\nsequential information and attention mechanism called Multi-Perspective\nAttention Bias Sequential Recommendation (MABSRec). Firstly, we reconstruct\nuser sequences into three short types and utilize graph neural networks for\nitem weighting. Subsequently, an adaptive multi-bias perspective attention\nmodule is proposed to enhance the accuracy of recommendations. Experimental\nresults show that the MABSRec model exhibits significant advantages in all\nevaluation metrics, demonstrating its excellent performance in the sequence\nrecommendation task.", "published": "2025-02-26 14:16:58", "link": "http://arxiv.org/abs/2504.05323v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2"], "primary_category": "cs.IR"}
{"title": "Evaluating Intelligence via Trial and Error", "abstract": "Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require $10^{26}$ parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is $10^{7}$ times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take $70$ years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.", "published": "2025-02-26 05:59:45", "link": "http://arxiv.org/abs/2502.18858v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in\n  LLMs Elicits Effective Personalization to Real Users", "abstract": "Effective personalization of LLMs is critical for a broad range of\nuser-interfacing applications such as virtual assistants and content curation.\nInspired by the strong in-context learning capabilities of LLMs, we propose\nFew-Shot Preference Optimization (FSPO), which reframes reward modeling as a\nmeta-learning problem. Under this framework, an LLM learns to quickly adapt to\na user via a few labeled preferences from that user, constructing a\npersonalized reward function for them. Additionally, since real-world\npreference data is scarce and challenging to collect at scale, we propose\ncareful design choices to construct synthetic preference datasets for\npersonalization, generating over 1M synthetic personalized preferences using\npublicly available LLMs. In particular, to successfully transfer from synthetic\ndata to real users, we find it crucial for the data to exhibit both high\ndiversity and coherent, self-consistent structure. We evaluate FSPO on\npersonalized open-ended generation for up to 1,500 synthetic users across\nacross three domains: movie reviews, pedagogical adaptation based on\neducational background, and general question answering, along with a controlled\nhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in\ngenerating responses that are personalized to synthetic users and a 72% winrate\nwith real human users in open-ended question answering.", "published": "2025-02-26 17:08:46", "link": "http://arxiv.org/abs/2502.19312v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Shh, don't say that! Domain Certification in LLMs", "abstract": "Large language models (LLMs) are often deployed to perform constrained tasks,\nwith narrow domains. For example, customer support bots can be built on top of\nLLMs, relying on their broad language understanding and capabilities to enhance\nperformance. However, these LLMs are adversarially susceptible, potentially\ngenerating outputs outside the intended domain. To formalize, assess, and\nmitigate this risk, we introduce domain certification; a guarantee that\naccurately characterizes the out-of-domain behavior of language models. We then\npropose a simple yet effective approach, which we call VALID that provides\nadversarial bounds as a certificate. Finally, we evaluate our method across a\ndiverse set of datasets, demonstrating that it yields meaningful certificates,\nwhich bound the probability of out-of-domain samples tightly with minimum\npenalty to refusal behavior.", "published": "2025-02-26 17:13:19", "link": "http://arxiv.org/abs/2502.19320v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards an AI co-scientist", "abstract": "Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.", "published": "2025-02-26 06:17:13", "link": "http://arxiv.org/abs/2502.18864v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "physics.soc-ph", "q-bio.OT"], "primary_category": "cs.AI"}
{"title": "Clip-TTS: Contrastive Text-content and Mel-spectrogram, A High-Quality\n  Text-to-Speech Method based on Contextual Semantic Understanding", "abstract": "Traditional text-to-speech (TTS) methods primarily focus on establishing a\nmapping between phonemes and mel-spectrograms. However, during the phoneme\nencoding stage, there is often a lack of real mel-spectrogram auxiliary\ninformation, which results in the encoding process lacking true semantic\nunderstanding. At the same time, traditional TTS systems often struggle to\nbalance the inference speed of the model with the quality of the synthesized\nspeech. Methods that generate high-quality synthesized speech tend to have\nslower inference speeds, while faster inference methods often sacrifice speech\nquality. In this paper, I propose Clip-TTS, a TTS method based on the Clip\narchitecture. This method uses the Clip framework to establish a connection\nbetween text content and real mel-spectrograms during the text encoding stage,\nenabling the text encoder to directly learn the true semantics of the global\ncontext, thereby ensuring the quality of the synthesized speech. In terms of\nmodel architecture, I adopt the basic structure of Transformer, which allows\nClip-TTS to achieve fast inference speeds. Experimental results show that on\nthe LJSpeech and Baker datasets, the speech generated by Clip-TTS achieves\nstate-of-the-art MOS scores, and it also performs excellently on multi-emotion\ndatasets.Audio samples are available at: https://ltydd1314.github.io/.", "published": "2025-02-26 07:09:33", "link": "http://arxiv.org/abs/2502.18889v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech Synthesis", "abstract": "While recent zero-shot text-to-speech (TTS) models have significantly\nimproved speech quality and expressiveness, mainstream systems still suffer\nfrom issues related to speech-text alignment modeling: 1) models without\nexplicit speech-text alignment modeling exhibit less robustness, especially for\nhard sentences in practical applications; 2) predefined alignment-based models\nsuffer from naturalness constraints of forced alignments. This paper introduces\n\\textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment\nalgorithm that guides the latent diffusion transformer (DiT). Specifically, we\nprovide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of\nalignment without limiting the search space, thereby achieving high\nnaturalness. Moreover, we employ a multi-condition classifier-free guidance\nstrategy for accent intensity adjustment and adopt the piecewise rectified flow\ntechnique to accelerate the generation process. Experiments demonstrate that\nMegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports\nhighly flexible control over accent intensity. Notably, our system can generate\nhigh-quality one-minute speech with only 8 sampling steps. Audio samples are\navailable at https://sditdemo.github.io/sditdemo/.", "published": "2025-02-26 08:22:00", "link": "http://arxiv.org/abs/2502.18924v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DualSpec: Text-to-spatial-audio Generation via Dual-Spectrogram Guided\n  Diffusion Model", "abstract": "Text-to-audio (TTA), which generates audio signals from textual descriptions,\nhas received huge attention in recent years. However, recent works focused on\ntext to monaural audio only. As we know, spatial audio provides more immersive\nauditory experience than monaural audio, e.g. in virtual reality. To address\nthis issue, we propose a text-to-spatial-audio (TTSA) generation framework\nnamed DualSpec.Specifically, it first trains variational autoencoders (VAEs)\nfor extracting the latent acoustic representations from sound event audio.\nThen, given text that describes sound events and event directions, the proposed\nmethod uses the encoder of a pretrained large language model to transform the\ntext into text features. Finally, it trains a diffusion model from the latent\nacoustic representations and text features for the spatial audio generation. In\nthe inference stage, only the text description is needed to generate spatial\naudio. Particularly, to improve the synthesis quality and azimuth accuracy of\nthe spatial sound events simultaneously, we propose to use two kinds of\nacoustic features. One is the Mel spectrograms which is good for improving the\nsynthesis quality, and the other is the short-time Fourier transform\nspectrograms which is good at improving the azimuth accuracy. We provide a\npipeline of constructing spatial audio dataset with text prompts, for the\ntraining of the VAEs and diffusion model. We also introduce new spatial-aware\nevaluation metrics to quantify the azimuth errors of the generated spatial\naudio recordings. Experimental results demonstrate that the proposed method can\ngenerate spatial audio with high directional and event consistency.", "published": "2025-02-26 09:01:59", "link": "http://arxiv.org/abs/2502.18952v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Nexus-O: An Omni-Perceptive And -Interactive Model for Language, Audio,\n  And Vision", "abstract": "Human beings perceive the real world through a spectrum of sensory\nmodalities, encompassing auditory, visual, and linguistic faculties. The\njourney towards achieving Artificial General Intelligence (AGI) necessitates\nthe development of models that can emulate these multifaceted perceptual\ncapabilities and comprehensively understand these diversified data. To this\nend, we introduce \\textbf{Nexus-O}, an industry-level \\textbf{omni-perceptive\nand -interactive} model capable of efficiently processing Audio, Image, Video,\nand Text data in any combination and output audio/text in an end-to-end way. We\nsystematically investigate Nexus-O by addressing three key research questions:\nFirst, how can models be efficiently designed and trained to achieve tri-modal\nalignment, understanding and reasoning capabilities across multiple modalities?\nSecond, what approaches can be implemented to evaluate tri-modal model\nrobustness, ensuring reliable performance and applicability in real-world\nscenarios? Third, what strategies can be employed to curate and obtain\nhigh-quality, real-life scenario speech datasets? For the first question, we\ndesign and pre-train Nexus-O based on the vision-language model, rather than\nthe language model. By pre-training the model over high-quality synthetic audio\ndata, our model is capable of tri-modal perception and interaction. For the\nsecond question, we introduce a new audio testbed, Nexus-O-audio, comprising\ndiverse Automatic Speech Recognition (ASR) samples, spanning various real-world\nscenarios, such as corporate meetings and live stream. For the third question,\nwe design the speech data synthesis pipeline to obtain high-quality speech\ntraining datasets, covering various real-world scenarios. Comprehensive\nexperimentation and an in-depth analysis of tri-modal alignment over latent\nspace demonstrate the advantages of our model on downstream tasks.", "published": "2025-02-26 17:26:36", "link": "http://arxiv.org/abs/2503.01879v2", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
