{"title": "The Secret is in the Spectra: Predicting Cross-lingual Task Performance\n  with Spectral Similarity Measures", "abstract": "Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of\nlanguages at hand: e.g., previous work has suggested there is a connection\nbetween the expected success of bilingual lexicon induction (BLI) and the\nassumption of (approximate) isomorphism between monolingual embedding spaces.\nIn this work we present a large-scale study focused on the correlations between\nmonolingual embedding space similarity and task performance, covering thousands\nof language pairs and four different tasks: BLI, parsing, POS tagging and MT.\nWe hypothesize that statistics of the spectrum of each monolingual embedding\nspace indicate how well they can be aligned. We then introduce several\nisomorphism measures between two embedding spaces, based on the relevant\nstatistics of their individual spectra. We empirically show that 1) language\nsimilarity scores derived from such spectral isomorphism measures are strongly\nassociated with performance observed in different cross-lingual tasks, and 2)\nour spectral-based measures consistently outperform previous standard\nisomorphism measures, while being computationally more tractable and easier to\ninterpret. Finally, our measures capture complementary information to\ntypologically driven language distance measures, and the combination of\nmeasures from the two families yields even higher task performance\ncorrelations.", "published": "2020-01-30 00:09:53", "link": "http://arxiv.org/abs/2001.11136v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Importance of Word Order Information in Cross-lingual Sequence\n  Labeling", "abstract": "Word order variances generally exist in different languages. In this paper,\nwe hypothesize that cross-lingual models that fit into the word order of the\nsource language might fail to handle target languages. To verify this\nhypothesis, we investigate whether making models insensitive to the word order\nof the source language can improve the adaptation performance in target\nlanguages. To do so, we reduce the source language word order information\nfitted to sequence encoders and observe the performance changes. In addition,\nbased on this hypothesis, we propose a new method for fine-tuning multilingual\nBERT in downstream cross-lingual sequence labeling tasks. Experimental results\non dialogue natural language understanding, part-of-speech tagging, and named\nentity recognition tasks show that reducing word order information fitted to\nthe model can achieve better zero-shot cross-lingual performance. Furthermore,\nour proposed methods can also be applied to strong cross-lingual baselines, and\nimprove their performances.", "published": "2020-01-30 03:35:44", "link": "http://arxiv.org/abs/2001.11164v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LowResourceEval-2019: a shared task on morphological analysis for\n  low-resource languages", "abstract": "The paper describes the results of the first shared task on morphological\nanalysis for the languages of Russia, namely, Evenki, Karelian, Selkup, and\nVeps. For the languages in question, only small-sized corpora are available.\nThe tasks include morphological analysis, word form generation and morpheme\nsegmentation. Four teams participated in the shared task. Most of them use\nmachine-learning approaches, outperforming the existing rule-based ones. The\narticle describes the datasets prepared for the shared tasks and contains\nanalysis of the participants' solutions. Language corpora having different\nformats were transformed into CONLL-U format. The universal format makes the\ndatasets comparable to other language corpura and facilitates using them in\nother NLP tasks.", "published": "2020-01-30 12:47:50", "link": "http://arxiv.org/abs/2001.11285v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Parameter Space Factorization for Zero-Shot Learning across Tasks and\n  Languages", "abstract": "Most combinations of NLP tasks and language varieties lack in-domain examples\nfor supervised training because of the paucity of annotated data. How can\nneural models make sample-efficient generalizations from task-language\ncombinations with available data to low-resource ones? In this work, we propose\na Bayesian generative model for the space of neural parameters. We assume that\nthis space can be factorized into latent variables for each language and each\ntask. We infer the posteriors over such latent variables based on data from\nseen task-language combinations through variational inference. This enables\nzero-shot classification on unseen combinations at prediction time. For\ninstance, given training data for named entity recognition (NER) in Vietnamese\nand for part-of-speech (POS) tagging in Wolof, our model can perform accurate\npredictions for NER in Wolof. In particular, we experiment with a typologically\ndiverse sample of 33 languages from 4 continents and 11 families, and show that\nour model yields comparable or better results than state-of-the-art, zero-shot\ncross-lingual transfer methods. Moreover, we demonstrate that approximate\nBayesian model averaging results in smoother predictive distributions, whose\nentropy inversely correlates with accuracy. Hence, the proposed framework also\noffers robust estimates of prediction uncertainty. Our code is located at\ngithub.com/cambridgeltl/parameter-factorization", "published": "2020-01-30 16:58:56", "link": "http://arxiv.org/abs/2001.11453v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Parse, Generate! A Sequence to Sequence Architecture for\n  Task-Oriented Semantic Parsing", "abstract": "Virtual assistants such as Amazon Alexa, Apple Siri, and Google Assistant\noften rely on a semantic parsing component to understand which action(s) to\nexecute for an utterance spoken by its users. Traditionally, rule-based or\nstatistical slot-filling systems have been used to parse \"simple\" queries; that\nis, queries that contain a single action and can be decomposed into a set of\nnon-overlapping entities. More recently, shift-reduce parsers have been\nproposed to process more complex utterances. These methods, while powerful,\nimpose specific limitations on the type of queries that can be parsed; namely,\nthey require a query to be representable as a parse tree.\n  In this work, we propose a unified architecture based on Sequence to Sequence\nmodels and Pointer Generator Network to handle both simple and complex queries.\nUnlike other works, our approach does not impose any restriction on the\nsemantic parse schema. Furthermore, experiments show that it achieves state of\nthe art performance on three publicly available datasets (ATIS, SNIPS, Facebook\nTOP), relatively improving between 3.3% and 7.7% in exact match accuracy over\nprevious systems. Finally, we show the effectiveness of our approach on two\ninternal datasets.", "published": "2020-01-30 17:11:00", "link": "http://arxiv.org/abs/2001.11458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-attention-based BiGRU and capsule network for named entity\n  recognition", "abstract": "Named entity recognition(NER) is one of the tasks of natural language\nprocessing(NLP). In view of the problem that the traditional character\nrepresentation ability is weak and the neural network method is unable to\ncapture the important sequence information. An self-attention-based\nbidirectional gated recurrent unit(BiGRU) and capsule network(CapsNet) for NER\nis proposed. This model generates character vectors through bidirectional\nencoder representation of transformers(BERT) pre-trained model. BiGRU is used\nto capture sequence context features, and self-attention mechanism is proposed\nto give different focus on the information captured by hidden layer of BiGRU.\nFinally, we propose to use CapsNet for entity recognition. We evaluated the\nrecognition performance of the model on two datasets. Experimental results show\nthat the model has better performance without relying on external dictionary\ninformation.", "published": "2020-01-30 21:51:58", "link": "http://arxiv.org/abs/2002.00735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Efficient Architecture for Predicting the Case of Characters using\n  Sequence Models", "abstract": "The dearth of clean textual data often acts as a bottleneck in several\nnatural language processing applications. The data available often lacks proper\ncase (uppercase or lowercase) information. This often comes up when text is\nobtained from social media, messaging applications and other online platforms.\nThis paper attempts to solve this problem by restoring the correct case of\ncharacters, commonly known as Truecasing. Doing so improves the accuracy of\nseveral processing tasks further down in the NLP pipeline. Our proposed\narchitecture uses a combination of convolutional neural networks (CNN),\nbi-directional long short-term memory networks (LSTM) and conditional random\nfields (CRF), which work at a character level without any explicit feature\nengineering. In this study we compare our approach to previous statistical and\ndeep learning based approaches. Our method shows an increment of 0.83 in F1\nscore over the current state of the art. Since truecasing acts as a\npreprocessing step in several applications, every increment in the F1 score\nleads to a significant improvement in the language processing tasks.", "published": "2020-01-30 06:54:39", "link": "http://arxiv.org/abs/2002.00738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing the diagrammatic semiotic mode", "abstract": "As the use and diversity of diagrams across many disciplines grows, there is\nan increasing interest in the diagrams research community concerning how such\ndiversity might be documented and explained. In this article, we argue that one\nway of achieving increased reliability, coverage, and utility for a general\nclassification of diagrams is to draw on recently developed semiotic principles\ndeveloped within the field of multimodality. To this end, we sketch out the\ninternal details of what may tentatively be termed the diagrammatic semiotic\nmode. This provides a natural account of how diagrammatic representations may\nintegrate natural language, various forms of graphics, diagrammatic elements\nsuch as arrows, lines and other expressive resources into coherent\norganisations, while still respecting the crucial diagrammatic contributions of\nvisual organisation. We illustrate the proposed approach using two recent\ndiagram corpora and show how a multimodal approach supports the empirical\nanalysis of diagrammatic representations, especially in identifying\ndiagrammatic constituents and describing their interrelations in a manner that\nmay be generalised across diagram types and be used to characterise distinct\nkinds of functionality.", "published": "2020-01-30 09:17:32", "link": "http://arxiv.org/abs/2001.11224v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Mining in Clinical Trial Text: Transformers for Classification and\n  Question Answering Tasks", "abstract": "This research on data extraction methods applies recent advances in natural\nlanguage processing to evidence synthesis based on medical texts. Texts of\ninterest include abstracts of clinical trials in English and in multilingual\ncontexts. The main focus is on information characterized via the Population,\nIntervention, Comparator, and Outcome (PICO) framework, but data extraction is\nnot limited to these fields. Recent neural network architectures based on\ntransformers show capacities for transfer learning and increased performance on\ndownstream natural language processing tasks such as universal reading\ncomprehension, brought forward by this architecture's use of contextualized\nword embeddings and self-attention mechanisms. This paper contributes to\nsolving problems related to ambiguity in PICO sentence prediction tasks, as\nwell as highlighting how annotations for training named entity recognition\nsystems are used to train a high-performing, but nevertheless flexible\narchitecture for question answering in systematic review automation.\nAdditionally, it demonstrates how the problem of insufficient amounts of\ntraining annotations for PICO entity extraction is tackled by augmentation. All\nmodels in this paper were created with the aim to support systematic review\n(semi)automation. They achieve high F1 scores, and demonstrate the feasibility\nof applying transformer-based classification methods to support data mining in\nthe biomedical literature.", "published": "2020-01-30 11:45:59", "link": "http://arxiv.org/abs/2001.11268v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harnessing Code Switching to Transcend the Linguistic Barrier", "abstract": "Code mixing (or code switching) is a common phenomenon observed in\nsocial-media content generated by a linguistically diverse user-base. Studies\nshow that in the Indian sub-continent, a substantial fraction of social media\nposts exhibit code switching. While the difficulties posed by code mixed\ndocuments to further downstream analyses are well-understood, lending\nvisibility to code mixed documents under certain scenarios may have utility\nthat has been previously overlooked. For instance, a document written in a\nmixture of multiple languages can be partially accessible to a wider audience;\nthis could be particularly useful if a considerable fraction of the audience\nlacks fluency in one of the component languages. In this paper, we provide a\nsystematic approach to sample code mixed documents leveraging a polyglot\nembedding based method that requires minimal supervision. In the context of the\n2019 India-Pakistan conflict triggered by the Pulwama terror attack, we\ndemonstrate an untapped potential of harnessing code mixing for human\nwell-being: starting from an existing hostility diffusing \\emph{hope speech}\nclassifier solely trained on English documents, code mixed documents are\nutilized as a bridge to retrieve \\emph{hope speech} content written in a\nlow-resource but widely used language - Romanized Hindi. Our proposed pipeline\nrequires minimal supervision and holds promise in substantially reducing web\nmoderation efforts.", "published": "2020-01-30 11:25:06", "link": "http://arxiv.org/abs/2001.11258v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Training for Aspect-Based Sentiment Analysis with BERT", "abstract": "Aspect-Based Sentiment Analysis (ABSA) deals with the extraction of\nsentiments and their targets. Collecting labeled data for this task in order to\nhelp neural networks generalize better can be laborious and time-consuming. As\nan alternative, similar data to the real-world examples can be produced\nartificially through an adversarial process which is carried out in the\nembedding space. Although these examples are not real sentences, they have been\nshown to act as a regularization method which can make neural networks more\nrobust. In this work, we apply adversarial training, which was put forward by\nGoodfellow et al. (2014), to the post-trained BERT (BERT-PT) language model\nproposed by Xu et al. (2019) on the two major tasks of Aspect Extraction and\nAspect Sentiment Classification in sentiment analysis. After improving the\nresults of post-trained BERT by an ablation study, we propose a novel\narchitecture called BERT Adversarial Training (BAT) to utilize adversarial\ntraining in ABSA. The proposed model outperforms post-trained BERT in both\ntasks. To the best of our knowledge, this is the first study on the application\nof adversarial training in ABSA.", "published": "2020-01-30 13:53:58", "link": "http://arxiv.org/abs/2001.11316v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Brand Intelligence Analytics", "abstract": "Leveraging the power of big data represents an opportunity for brand managers\nto reveal patterns and trends in consumer perceptions, while monitoring\npositive or negative associations of the brand with desired topics. This\nchapter describes the functionalities of the SBS Brand Intelligence App (SBS\nBI), which has been designed to assess brand importance and provides brand\nanalytics through the analysis of (big) textual data. To better describe the\nSBS BI's functionalities, we present a case study focused on the 2020 US\nDemocratic Presidential Primaries. We downloaded 50,000 online articles from\nthe Event Registry database, which contains both mainstream and blog news\ncollected from around the world. These online news articles were transformed\ninto networks of co-occurring words and analyzed by combining methods and tools\nfrom social network analysis and text mining.", "published": "2020-01-30 17:57:56", "link": "http://arxiv.org/abs/2001.11479v2", "categories": ["cs.SE", "cs.CL", "cs.SI"], "primary_category": "cs.SE"}
{"title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong\n  Baselines for Grammar Induction", "abstract": "With the recent success and popularity of pre-trained language models (LMs)\nin natural language processing, there has been a rise in efforts to understand\ntheir inner workings. In line with such interest, we propose a novel method\nthat assists us in investigating the extent to which pre-trained LMs capture\nthe syntactic notion of constituency. Our method provides an effective way of\nextracting constituency trees from the pre-trained LMs without training. In\naddition, we report intriguing findings in the induced trees, including the\nfact that pre-trained LMs outperform other approaches in correctly demarcating\nadverb phrases in sentences.", "published": "2020-01-30 11:06:49", "link": "http://arxiv.org/abs/2002.00737v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Cepstrum: Spatial Feature Extracted from Partially Connected\n  Microphones", "abstract": "In this paper, we propose an effective and robust method of spatial feature\nextraction for acoustic scene analysis utilizing partially synchronized and/or\nclosely located distributed microphones. In the proposed method, a new cepstrum\nfeature utilizing a graph-based basis transformation to extract spatial\ninformation from distributed microphones, while taking into account whether any\npairs of microphones are synchronized and/or closely located, is introduced.\nSpecifically, in the proposed graph-based cepstrum, the log-amplitude of a\nmultichannel observation is converted to a feature vector utilizing the inverse\ngraph Fourier transform, which is a method of basis transformation of a signal\non a graph. Results of experiments using real environmental sounds show that\nthe proposed graph-based cepstrum robustly extracts spatial information with\nconsideration of the microphone connections. Moreover, the results indicate\nthat the proposed method more robustly classifies acoustic scenes than\nconventional spatial features when the observed sounds have a large\nsynchronization mismatch between partially synchronized microphone groups.", "published": "2020-01-30 17:45:13", "link": "http://arxiv.org/abs/2001.11894v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Oral Billiards", "abstract": "We propose a physical model of speech to explain its precision and\nrobustness. We begin by reducing the dynamics to the bare minimum of polygonal\nbilliards. The symbolic stability of the billiard trajectories against\nvariations in action and the oral cavity geometry forms the basis for precision\nand robustness in articulation. This stability survives forcing and dissipation\nto underpin reliable encoding of the trajectories into acoustic emissions. The\nkinematics of oral billiards and the cyclical nature of the forcing mechanism\nengender a grammar of the syllable independent of any language. The symbolic\ndynamics of oral billiards is rendered nearly maximally observable by their\nconcomitant acoustic emissions. Speech recognition is the set of computations\non the sub-maximally informative acoustic observables from which the symbolic\ndynamics of oral billiards may be inferred.", "published": "2020-01-30 17:34:31", "link": "http://arxiv.org/abs/2002.00791v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound field reconstruction in rooms: inpainting meets super-resolution", "abstract": "In this paper, a deep-learning-based method for sound field reconstruction is\nproposed. It is shown the possibility to reconstruct the magnitude of the sound\npressure in the frequency band 30-300 Hz for an entire room by using a very low\nnumber of irregularly distributed microphones arbitrarily arranged. Moreover,\nthe approach is agnostic to the location of the measurements in the Euclidean\nspace. In particular, the presented approach uses a limited number of arbitrary\ndiscrete measurements of the magnitude of the sound field pressure in order to\nextrapolate this field to a higher-resolution grid of discrete points in space\nwith a low computational complexity. The method is based on a U-net-like neural\nnetwork with partial convolutions trained solely on simulated data, which\nitself is constructed from numerical simulations of Green's function across\nthousands of common rectangular rooms. Although extensible to three dimensions\nand different room shapes, the method focuses on reconstructing a\ntwo-dimensional plane of a rectangular room from measurements of the\nthree-dimensional sound field. Experiments using simulated data together with\nan experimental validation in a real listening room are shown. The results\nsuggest a performance which may exceed conventional reconstruction techniques\nfor a low number of microphones and computational requirements.", "published": "2020-01-30 11:31:59", "link": "http://arxiv.org/abs/2001.11263v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditioning Autoencoder Latent Spaces for Real-Time Timbre\n  Interpolation and Synthesis", "abstract": "We compare standard autoencoder topologies' performances for timbre\ngeneration. We demonstrate how different activation functions used in the\nautoencoder's bottleneck distributes a training corpus's embedding. We show\nthat the choice of sigmoid activation in the bottleneck produces a more bounded\nand uniformly distributed embedding than a leaky rectified linear unit\nactivation. We propose a one-hot encoded chroma feature vector for use in both\ninput augmentation and latent space conditioning. We measure the performance of\nthese networks, and characterize the latent embeddings that arise from the use\nof this chroma conditioning vector. An open source, real-time timbre synthesis\nalgorithm in Python is outlined and shared.", "published": "2020-01-30 13:06:26", "link": "http://arxiv.org/abs/2001.11296v1", "categories": ["eess.AS", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BUT Opensat 2019 Speech Recognition System", "abstract": "The paper describes the BUT Automatic Speech Recognition (ASR) systems\nsubmitted for OpenSAT evaluations under two domain categories such as low\nresourced languages and public safety communications. The first was challenging\ndue to lack of training data, therefore various architectures and multilingual\napproaches were employed. The combination led to superior performance. The\nsecond domain was challenging due to recording in extreme conditions such as\nspecific channel, speaker under stress and high levels of noise. Data\naugmentation process was inevitable to get reasonably good performance.", "published": "2020-01-30 14:35:34", "link": "http://arxiv.org/abs/2001.11360v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Continuous speech separation: dataset and analysis", "abstract": "This paper describes a dataset and protocols for evaluating continuous speech\nseparation algorithms. Most prior studies on speech separation use\npre-segmented signals of artificially mixed speech utterances which are mostly\n\\emph{fully} overlapped, and the algorithms are evaluated based on\nsignal-to-distortion ratio or similar performance metrics. However, in natural\nconversations, a speech signal is continuous, containing both overlapped and\noverlap-free components. In addition, the signal-based metrics have very weak\ncorrelations with automatic speech recognition (ASR) accuracy. We think that\nnot only does this make it hard to assess the practical relevance of the tested\nalgorithms, it also hinders researchers from developing systems that can be\nreadily applied to real scenarios. In this paper, we define continuous speech\nseparation (CSS) as a task of generating a set of non-overlapped speech signals\nfrom a \\textit{continuous} audio stream that contains multiple utterances that\nare \\emph{partially} overlapped by a varying degree. A new real recorded\ndataset, called LibriCSS, is derived from LibriSpeech by concatenating the\ncorpus utterances to simulate a conversation and capturing the audio replays\nwith far-field microphones. A Kaldi-based ASR evaluation protocol is also\nestablished by using a well-trained multi-conditional acoustic model. By using\nthis dataset, several aspects of a recently proposed speaker-independent CSS\nalgorithm are investigated. The dataset and evaluation scripts are available to\nfacilitate the research in this direction.", "published": "2020-01-30 18:01:31", "link": "http://arxiv.org/abs/2001.11482v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Channel-Attention Dense U-Net for Multichannel Speech Enhancement", "abstract": "Supervised deep learning has gained significant attention for speech\nenhancement recently. The state-of-the-art deep learning methods perform the\ntask by learning a ratio/binary mask that is applied to the mixture in the\ntime-frequency domain to produce the clean speech. Despite the great\nperformance in the single-channel setting, these frameworks lag in performance\nin the multichannel setting as the majority of these methods a) fail to exploit\nthe available spatial information fully, and b) still treat the deep\narchitecture as a black box which may not be well-suited for multichannel audio\nprocessing. This paper addresses these drawbacks, a) by utilizing complex ratio\nmasking instead of masking on the magnitude of the spectrogram, and more\nimportantly, b) by introducing a channel-attention mechanism inside the deep\narchitecture to mimic beamforming. We propose Channel-Attention Dense U-Net, in\nwhich we apply the channel-attention unit recursively on feature maps at every\nlayer of the network, enabling the network to perform non-linear beamforming.\nWe demonstrate the superior performance of the network against the\nstate-of-the-art approaches on the CHiME-3 dataset.", "published": "2020-01-30 19:56:52", "link": "http://arxiv.org/abs/2001.11542v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
