{"title": "NICT's Neural and Statistical Machine Translation Systems for the WMT18\n  News Translation Task", "abstract": "This paper presents the NICT's participation to the WMT18 shared news\ntranslation task. We participated in the eight translation directions of four\nlanguage pairs: Estonian-English, Finnish-English, Turkish-English and\nChinese-English. For each translation direction, we prepared state-of-the-art\nstatistical (SMT) and neural (NMT) machine translation systems. Our NMT systems\nwere trained with the transformer architecture using the provided parallel data\nenlarged with a large quantity of back-translated monolingual data that we\ngenerated with a new incremental training framework. Our primary submissions to\nthe task are the result of a simple combination of our SMT and NMT systems. Our\nsystems are ranked first for the Estonian-English and Finnish-English language\npairs (constraint) according to BLEU-cased.", "published": "2018-09-19 07:41:55", "link": "http://arxiv.org/abs/1809.07037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NICT's Corpus Filtering Systems for the WMT18 Parallel Corpus Filtering\n  Task", "abstract": "This paper presents the NICT's participation in the WMT18 shared parallel\ncorpus filtering task. The organizers provided 1 billion words German-English\ncorpus crawled from the web as part of the Paracrawl project. This corpus is\ntoo noisy to build an acceptable neural machine translation (NMT) system. Using\nthe clean data of the WMT18 shared news translation task, we designed several\nfeatures and trained a classifier to score each sentence pairs in the noisy\ndata. Finally, we sampled 100 million and 10 million words and built\ncorresponding NMT systems. Empirical results show that our NMT systems trained\non sampled data achieve promising performance.", "published": "2018-09-19 07:52:16", "link": "http://arxiv.org/abs/1809.07043v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Topic Conversational Models", "abstract": "Latent variable models have been a preferred choice in conversational\nmodeling compared to sequence-to-sequence (seq2seq) models which tend to\ngenerate generic and repetitive responses. Despite so, training latent variable\nmodels remains to be difficult. In this paper, we propose Latent Topic\nConversational Model (LTCM) which augments seq2seq with a neural latent topic\ncomponent to better guide response generation and make training easier. The\nneural topic component encodes information from the source sentence to build a\nglobal \"topic\" distribution over words, which is then consulted by the seq2seq\nmodel at each generation step. We study in details how the latent\nrepresentation is learnt in both the vanilla model and LTCM. Our extensive\nexperiments contribute to better understanding and training of conditional\nlatent models for languages. Our results show that by sampling from the learnt\nlatent representations, LTCM can generate diverse and interesting responses. In\na subjective human evaluation, the judges also confirm that LTCM is the overall\npreferred option.", "published": "2018-09-19 08:58:23", "link": "http://arxiv.org/abs/1809.07070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "String Transduction with Target Language Models and Insertion Handling", "abstract": "Many character-level tasks can be framed as sequence-to-sequence\ntransduction, where the target is a word from a natural language. We show that\nleveraging target language models derived from unannotated target corpora,\ncombined with a precise alignment of the training data, yields state-of-the art\nresults on cognate projection, inflection generation, and phoneme-to-grapheme\nconversion.", "published": "2018-09-19 13:39:40", "link": "http://arxiv.org/abs/1809.07182v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised cross-lingual matching of product classifications", "abstract": "Unsupervised cross-lingual embeddings mapping has provided a unique tool for\ncompletely unsupervised translation even for languages with different scripts.\nIn this work we use this method for the task of unsupervised cross-lingual\nmatching of product classifications. Our work also investigates limitations of\nunsupervised vector alignment and we also suggest two other techniques for\naligning product classifications based on their descriptions: using\nhierarchical information and translations.", "published": "2018-09-19 15:07:45", "link": "http://arxiv.org/abs/1809.07234v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Interpretable Textual Neuron Representations for NLP", "abstract": "Input optimization methods, such as Google Deep Dream, create interpretable\nrepresentations of neurons for computer vision DNNs. We propose and evaluate\nways of transferring this technology to NLP. Our results suggest that gradient\nascent with a gumbel softmax layer produces n-gram representations that\noutperform naive corpus search in terms of target neuron activation. The\nrepresentations highlight differences in syntax awareness between the language\nand visual models of the Imaginet architecture.", "published": "2018-09-19 16:32:47", "link": "http://arxiv.org/abs/1809.07291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset for Document Grounded Conversations", "abstract": "This paper introduces a document grounded dataset for text conversations. We\ndefine \"Document Grounded Conversations\" as conversations that are about the\ncontents of a specified document. In this dataset the specified documents were\nWikipedia articles about popular movies. The dataset contains 4112\nconversations with an average of 21.43 turns per conversation. This positions\nthis dataset to not only provide a relevant chat history while generating\nresponses but also provide a source of information that the models could use.\nWe describe two neural architectures that provide benchmark performance on the\ntask of generating the next response. We also evaluate our models for\nengagement and fluency, and find that the information from the document helps\nin generating more engaging and fluent responses.", "published": "2018-09-19 18:22:44", "link": "http://arxiv.org/abs/1809.07358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Linguistic Pattern Ordering in Hierarchical Natural\n  Language Generation", "abstract": "Natural language generation (NLG) is a critical component in spoken dialogue\nsystem, which can be divided into two phases: (1) sentence planning: deciding\nthe overall sentence structure, (2) surface realization: determining specific\nword forms and flattening the sentence structure into a string. With the rise\nof deep learning, most modern NLG models are based on a sequence-to-sequence\n(seq2seq) model, which basically contains an encoder-decoder structure; these\nNLG models generate sentences from scratch by jointly optimizing sentence\nplanning and surface realization. However, such simple encoder-decoder\narchitecture usually fail to generate complex and long sentences, because the\ndecoder has difficulty learning all grammar and diction knowledge well. This\npaper introduces an NLG model with a hierarchical attentional decoder, where\nthe hierarchy focuses on leveraging linguistic knowledge in a specific order.\nThe experiments show that the proposed method significantly outperforms the\ntraditional seq2seq model with a smaller model size, and the design of the\nhierarchical attentional decoder can be applied to various NLG systems.\nFurthermore, different generation strategies based on linguistic patterns are\ninvestigated and analyzed in order to guide future NLG research work.", "published": "2018-09-19 05:40:35", "link": "http://arxiv.org/abs/1809.07629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Monolingual sentence matching for text simplification", "abstract": "This work improves monolingual sentence alignment for text simplification,\nspecifically for text in standard and simple Wikipedia. We introduce a\nconvolutional neural network structure to model similarity between two\nsentences. Due to the limitation of available parallel corpora, the model is\ntrained in a semi-supervised way, by using the output of a knowledge-based high\nperformance aligning system. We apply the resulting similarity score to rescore\nthe knowledge-based output, and adapt the model by a small hand-aligned\ndataset. Experiments show that both rescoring and adaptation improve the\nperformance of knowledge-based method.", "published": "2018-09-19 04:15:36", "link": "http://arxiv.org/abs/1809.08703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Speech Synthesis with Transformer Network", "abstract": "Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2)\nare proposed and achieve state-of-the-art performance, they still suffer from\ntwo problems: 1) low efficiency during training and inference; 2) hard to model\nlong dependency using current recurrent neural networks (RNNs). Inspired by the\nsuccess of Transformer network in neural machine translation (NMT), in this\npaper, we introduce and adapt the multi-head attention mechanism to replace the\nRNN structures and also the original attention mechanism in Tacotron2. With the\nhelp of multi-head self-attention, the hidden states in the encoder and decoder\nare constructed in parallel, which improves the training efficiency. Meanwhile,\nany two inputs at different times are connected directly by self-attention\nmechanism, which solves the long range dependency problem effectively. Using\nphoneme sequences as input, our Transformer TTS network generates mel\nspectrograms, followed by a WaveNet vocoder to output the final audio results.\nExperiments are conducted to test the efficiency and performance of our new\nnetwork. For the efficiency, our Transformer TTS network can speed up the\ntraining about 4.25 times faster compared with Tacotron2. For the performance,\nrigorous human tests show that our proposed model achieves state-of-the-art\nperformance (outperforms Tacotron2 with a gap of 0.048) and is very close to\nhuman quality (4.39 vs 4.44 in MOS).", "published": "2018-09-19 07:41:17", "link": "http://arxiv.org/abs/1809.08895v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Interpret Satellite Images Using Wikipedia", "abstract": "Despite recent progress in computer vision, fine-grained interpretation of\nsatellite images remains challenging because of a lack of labeled training\ndata. To overcome this limitation, we propose using Wikipedia as a previously\nuntapped source of rich, georeferenced textual information with global\ncoverage. We construct a novel large-scale, multi-modal dataset by pairing\ngeo-referenced Wikipedia articles with satellite imagery of their corresponding\nlocations. To prove the efficacy of this dataset, we focus on the African\ncontinent and train a deep network to classify images based on labels extracted\nfrom articles. We then fine-tune the model on a human annotated dataset and\ndemonstrate that this weak form of supervision can drastically reduce the\nquantity of human annotated labels and time required for downstream tasks.", "published": "2018-09-19 21:58:14", "link": "http://arxiv.org/abs/1809.10236v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MTLE: A Multitask Learning Encoder of Visual Feature Representations for\n  Video and Movie Description", "abstract": "Learning visual feature representations for video analysis is a daunting task\nthat requires a large amount of training samples and a proper generalization\nframework. Many of the current state of the art methods for video captioning\nand movie description rely on simple encoding mechanisms through recurrent\nneural networks to encode temporal visual information extracted from video\ndata. In this paper, we introduce a novel multitask encoder-decoder framework\nfor automatic semantic description and captioning of video sequences. In\ncontrast to current approaches, our method relies on distinct decoders that\ntrain a visual encoder in a multitask fashion. Our system does not depend\nsolely on multiple labels and allows for a lack of training data working even\nwith datasets where only one single annotation is viable per video. Our method\nshows improved performance over current state of the art methods in several\nmetrics on multi-caption and single-caption datasets. To the best of our\nknowledge, our method is the first method to use a multitask approach for\nencoding video features. Our method demonstrates its robustness on the Large\nScale Movie Description Challenge (LSMDC) 2017 where our method won the movie\ndescription task and its results were ranked among other competitors as the\nmost helpful for the visually impaired.", "published": "2018-09-19 15:50:18", "link": "http://arxiv.org/abs/1809.07257v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Dialogue-based Navigation with Multivariate Adaptation driven by\n  Intention and Politeness for Social Robots", "abstract": "Service robots need to show appropriate social behaviour in order to be\ndeployed in social environments such as healthcare, education, retail, etc.\nSome of the main capabilities that robots should have are navigation and\nconversational skills. If the person is impatient, the person might want a\nrobot to navigate faster and vice versa. Linguistic features that indicate\npoliteness can provide social cues about a person's patient and impatient\nbehaviour. The novelty presented in this paper is to dynamically incorporate\npoliteness in robotic dialogue systems for navigation. Understanding the\npoliteness in users' speech can be used to modulate the robot behaviour and\nresponses. Therefore, we developed a dialogue system to navigate in an indoor\nenvironment, which produces different robot behaviours and responses based on\nusers' intention and degree of politeness. We deploy and test our system with\nthe Pepper robot that adapts to the changes in user's politeness.", "published": "2018-09-19 16:08:50", "link": "http://arxiv.org/abs/1809.07269v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Modeling Online Discourse with Coupled Distributed Topics", "abstract": "In this paper, we propose a deep, globally normalized topic model that\nincorporates structural relationships connecting documents in socially\ngenerated corpora, such as online forums. Our model (1) captures discursive\ninteractions along observed reply links in addition to traditional topic\ninformation, and (2) incorporates latent distributed representations arranged\nin a deep architecture, which enables a GPU-based mean-field inference\nprocedure that scales efficiently to large data. We apply our model to a new\nsocial media dataset consisting of 13M comments mined from the popular internet\nforum Reddit, a domain that poses significant challenges to models that do not\naccount for relationships connecting user comments. We evaluate against\nexisting methods across multiple metrics including perplexity and metadata\nprediction, and qualitatively analyze the learned interaction patterns.", "published": "2018-09-19 16:21:12", "link": "http://arxiv.org/abs/1809.07282v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Switching divergences for spectral learning in blind speech\n  dereverberation", "abstract": "When recorded in an enclosed room, a sound signal will most certainly get\naffected by reverberation. This not only undermines audio quality, but also\nposes a problem for many human-machine interaction technologies that use speech\nas their input. In this work, a new blind, two-stage dereverberation approach\nbased in a generalized \\beta-divergence as a fidelity term over a non-negative\nrepresentation is proposed. The first stage consists of learning the spectral\nstructure of the signal solely from the observed spectrogram, while the second\nstage is devoted to model reverberation. Both steps are taken by minimizing a\ncost function in which the aim is put either in constructing a dictionary or a\ngood representation by changing the divergence involved. In addition, an\napproach for finding an optimal fidelity parameter for dictionary learning is\nproposed. An algorithm for implementing the proposed method is described and\ntested against state-of-the-art methods. Results show improvements for both\nartificial reverberation and real recordings.", "published": "2018-09-19 19:09:46", "link": "http://arxiv.org/abs/1809.07375v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "New insights on the optimality of parameterized wiener filters for\n  speech enhancement applications", "abstract": "This work presents a unified framework for defining a family of noise\nreduction techniques for speech enhancement applications. The proposed approach\nprovides a unique theoretical foundation for some widely-applied soft and hard\ntime-frequency masks, which encompasses the well-known Wiener filter and the\nheuristically-designed Binary mask. These techniques can now be considered as\noptimal solutions of the same minimization problem. The proposed cost function\nis defined by two design parameters that not only establish a desired trade-off\nbetween noise reduction and speech distortion, but also provide an insightful\nrelationship with the mask morphology. Such characteristic may be useful for\napplications that require online adaptation of the suppression function\naccording to variations of the acoustic scenario. Simulation examples indicate\nthat the derived conformable suppression mask has approximately the same\nquality and intelligibility performance capability of the classical\nheuristically-defined parametric Wiener filter. The proposed approach may be of\nspecial interest for real-time embedded speech enhancement applications such as\nhearing aids and cochlear implants.", "published": "2018-09-19 19:33:51", "link": "http://arxiv.org/abs/1809.07384v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
