{"title": "Morphologically Aware Word-Level Translation", "abstract": "We propose a novel morphologically aware probability model for bilingual\nlexicon induction, which jointly models lexeme translation and inflectional\nmorphology in a structured way. Our model exploits the basic linguistic\nintuition that the lexeme is the key lexical unit of meaning, while\ninflectional morphology provides additional syntactic information. This\napproach leads to substantial performance improvements - 19% average\nimprovement in accuracy across 6 language pairs over the state of the art in\nthe supervised setting and 16% in the weakly supervised setting. As another\ncontribution, we highlight issues associated with modern BLI that stem from\nignoring inflectional morphology, and propose three suggestions for improving\nthe task.", "published": "2020-11-15 17:54:49", "link": "http://arxiv.org/abs/2011.07593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Target Guided Emotion Aware Chat Machine", "abstract": "The consistency of a response to a given post at semantic-level and\nemotional-level is essential for a dialogue system to deliver human-like\ninteractions. However, this challenge is not well addressed in the literature,\nsince most of the approaches neglect the emotional information conveyed by a\npost while generating responses. This article addresses this problem by\nproposing a unifed end-to-end neural architecture, which is capable of\nsimultaneously encoding the semantics and the emotions in a post and leverage\ntarget information for generating more intelligent responses with appropriately\nexpressed emotions. Extensive experiments on real-world data demonstrate that\nthe proposed method outperforms the state-of-the-art methods in terms of both\ncontent coherence and emotion appropriateness.", "published": "2020-11-15 01:55:37", "link": "http://arxiv.org/abs/2011.07432v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Challenge of Diacritics in Yoruba Embeddings", "abstract": "The major contributions of this work include the empirical establishment of a\nbetter performance for Yoruba embeddings from undiacritized (normalized)\ndataset and provision of new analogy sets for evaluation. The Yoruba language,\nbeing a tonal language, utilizes diacritics (tonal marks) in written form. We\nshow that this affects embedding performance by creating embeddings from\nexactly the same Wikipedia dataset but with the second one normalized to be\nundiacritized. We further compare average intrinsic performance with two other\nwork (using analogy test set & WordSim) and we obtain the best performance in\nWordSim and corresponding Spearman correlation.", "published": "2020-11-15 19:02:46", "link": "http://arxiv.org/abs/2011.07605v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep multi-modal networks for book genre classification based on its\n  cover", "abstract": "Book covers are usually the very first impression to its readers and they\noften convey important information about the content of the book. Book genre\nclassification based on its cover would be utterly beneficial to many modern\nretrieval systems, considering that the complete digitization of books is an\nextremely expensive task. At the same time, it is also an extremely challenging\ntask due to the following reasons: First, there exists a wide variety of book\ngenres, many of which are not concretely defined. Second, book covers, as\ngraphic designs, vary in many different ways such as colors, styles, textual\ninformation, etc, even for books of the same genre. Third, book cover designs\nmay vary due to many external factors such as country, culture, target reader\npopulations, etc. With the growing competitiveness in the book industry, the\nbook cover designers and typographers push the cover designs to its limit in\nthe hope of attracting sales. The cover-based book classification systems\nbecome a particularly exciting research topic in recent years. In this paper,\nwe propose a multi-modal deep learning framework to solve this problem. The\ncontribution of this paper is four-fold. First, our method adds an extra\nmodality by extracting texts automatically from the book covers. Second,\nimage-based and text-based, state-of-the-art models are evaluated thoroughly\nfor the task of book cover classification. Third, we develop an efficient and\nsalable multi-modal framework based on the images and texts shown on the covers\nonly. Fourth, a thorough analysis of the experimental results is given and\nfuture works to improve the performance is suggested. The results show that the\nmulti-modal framework significantly outperforms the current state-of-the-art\nimage-based models. However, more efforts and resources are needed for this\nclassification task in order to reach a satisfactory level.", "published": "2020-11-15 23:27:43", "link": "http://arxiv.org/abs/2011.07658v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge\n  Bases", "abstract": "Codifying commonsense knowledge in machines is a longstanding goal of\nartificial intelligence. Recently, much progress toward this goal has been made\nwith automatic knowledge base (KB) construction techniques. However, such\ntechniques focus primarily on the acquisition of positive (true) KB statements,\neven though negative (false) statements are often also important for\ndiscriminative reasoning over commonsense KBs. As a first step toward the\nlatter, this paper proposes NegatER, a framework that ranks potential negatives\nin commonsense KBs using a contextual language model (LM). Importantly, as most\nKBs do not contain negatives, NegatER relies only on the positive knowledge in\nthe LM and does not require ground-truth negative examples. Experiments\ndemonstrate that, compared to multiple contrastive data augmentation\napproaches, NegatER yields negatives that are more grammatical, coherent, and\ninformative -- leading to statistically significant accuracy improvements in a\nchallenging KB completion task and confirming that the positive knowledge in\nLMs can be \"re-purposed\" to generate negative knowledge.", "published": "2020-11-15 10:55:26", "link": "http://arxiv.org/abs/2011.07497v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "DORB: Dynamically Optimizing Multiple Rewards with Bandits", "abstract": "Policy gradients-based reinforcement learning has proven to be a promising\napproach for directly optimizing non-differentiable evaluation metrics for\nlanguage generation tasks. However, optimizing for a specific metric reward\nleads to improvements in mostly that metric only, suggesting that the model is\ngaming the formulation of that metric in a particular way without often\nachieving real qualitative improvements. Hence, it is more beneficial to make\nthe model optimize multiple diverse metric rewards jointly. While appealing,\nthis is challenging because one needs to manually decide the importance and\nscaling weights of these metric rewards. Further, it is important to consider\nusing a dynamic combination and curriculum of metric rewards that flexibly\nchanges over time. Considering the above aspects, in our work, we automate the\noptimization of multiple metric rewards simultaneously via a multi-armed bandit\napproach (DORB), where at each round, the bandit chooses which metric reward to\noptimize next, based on expected arm gains. We use the Exp3 algorithm for\nbandits and formulate two approaches for bandit rewards: (1) Single\nMulti-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit\n(HM-Bandit). We empirically show the effectiveness of our approaches via\nvarious automatic metrics and human evaluation on two important NLG tasks:\nquestion generation and data-to-text generation, including on an unseen-test\ntransfer setup. Finally, we present interpretable analyses of the learned\nbandit curriculum over the optimized rewards.", "published": "2020-11-15 21:57:47", "link": "http://arxiv.org/abs/2011.07635v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open4Business(O4B): An Open Access Dataset for Summarizing Business\n  Documents", "abstract": "A major challenge in fine-tuning deep learning models for automatic\nsummarization is the need for large domain specific datasets. One of the\nbarriers to curating such data from resources like online publications is\nnavigating the license regulations applicable to their re-use, especially for\ncommercial purposes. As a result, despite the availability of several business\njournals there are no large scale datasets for summarizing business documents.\nIn this work, we introduce Open4Business(O4B),a dataset of 17,458 open access\nbusiness articles and their reference summaries. The dataset introduces a new\nchallenge for summarization in the business domain, requiring highly\nabstractive and more concise summaries as compared to other existing datasets.\nAdditionally, we evaluate existing models on it and consequently show that\nmodels trained on O4B and a 7x larger non-open access dataset achieve\ncomparable performance on summarization. We release the dataset, along with the\ncode which can be leveraged to similarly gather data for multiple domains.", "published": "2020-11-15 22:00:07", "link": "http://arxiv.org/abs/2011.07636v3", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in\n  Dynamic Environments", "abstract": "For embodied agents, navigation is an important ability but not an isolated\ngoal. Agents are also expected to perform specific tasks after reaching the\ntarget location, such as picking up objects and assembling them into a\nparticular arrangement. We combine Vision-and-Language Navigation, assembling\nof collected objects, and object referring expression comprehension, to create\na novel joint navigation-and-assembly task, named ArraMon. During this task,\nthe agent (similar to a PokeMON GO player) is asked to find and collect\ndifferent target objects one-by-one by navigating based on natural language\ninstructions in a complex, realistic outdoor environment, but then also ARRAnge\nthe collected objects part-by-part in an egocentric grid-layout environment. To\nsupport this task, we implement a 3D dynamic environment simulator and collect\na dataset (in English; and also extended to Hindi) with human-written\nnavigation and assembling instructions, and the corresponding ground truth\ntrajectories. We also filter the collected instructions via a verification\nstage, leading to a total of 7.7K task instances (30.8K instructions and\npaths). We present results for several baseline models (integrated and biased)\nand metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance\ngap demonstrates that our task is challenging and presents a wide scope for\nfuture work. Our dataset, simulator, and code are publicly available at:\nhttps://arramonunc.github.io", "published": "2020-11-15 23:30:36", "link": "http://arxiv.org/abs/2011.07660v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.CL"}
{"title": "To Schedule or not to Schedule: Extracting Task Specific Temporal\n  Entities and Associated Negation Constraints", "abstract": "State of the art research for date-time entity extraction from text is task\nagnostic. Consequently, while the methods proposed in literature perform well\nfor generic date-time extraction from texts, they don't fare as well on task\nspecific date-time entity extraction where only a subset of the date-time\nentities present in the text are pertinent to solving the task. Furthermore,\nsome tasks require identifying negation constraints associated with the\ndate-time entities to correctly reason over time. We showcase a novel model for\nextracting task-specific date-time entities along with their negation\nconstraints. We show the efficacy of our method on the task of date-time\nunderstanding in the context of scheduling meetings for an email-based digital\nAI scheduling assistant. Our method achieves an absolute gain of 19\\% f-score\npoints compared to baseline methods in detecting the date-time entities\nrelevant to scheduling meetings and a 4\\% improvement over baseline methods for\ndetecting negation constraints over date-time entities.", "published": "2020-11-15 10:07:19", "link": "http://arxiv.org/abs/2012.02594v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "hyper-sinh: An Accurate and Reliable Function from Shallow to Deep\n  Learning in TensorFlow and Keras", "abstract": "This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation\nfunction suitable for Deep Learning (DL)-based algorithms for supervised\nlearning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in\nthe open source Python libraries TensorFlow and Keras, is thus described and\nvalidated as an accurate and reliable activation function for both shallow and\ndeep neural networks. Improvements in accuracy and reliability in image and\ntext classification tasks on five (N = 5) benchmark data sets available from\nKeras are discussed. Experimental results demonstrate the overall competitive\nclassification performance of both shallow and deep neural networks, obtained\nvia this novel function. This function is evaluated with respect to gold\nstandard activation functions, demonstrating its overall competitive accuracy\nand reliability for both image and text classification.", "published": "2020-11-15 23:38:59", "link": "http://arxiv.org/abs/2011.07661v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.NE", "68T07, 68T10, 68T45, 68T50, 68U35", "I.2.1; I.2.7; I.2.10; I.4.9; I.5.1; I.5.4; I.5.5"], "primary_category": "cs.CV"}
{"title": "Automatic dysarthric speech detection exploiting pairwise distance-based\n  convolutional neural networks", "abstract": "Automatic dysarthric speech detection can provide reliable and cost-effective\ncomputer-aided tools to assist the clinical diagnosis and management of\ndysarthria. In this paper we propose a novel automatic dysarthric speech\ndetection approach based on analyses of pairwise distance matrices using\nconvolutional neural networks (CNNs). We represent utterances through\narticulatory posteriors and consider pairs of phonetically-balanced\nrepresentations, with one representation from a healthy speaker (i.e., the\nreference representation) and the other representation from the test speaker\n(i.e., test representation). Given such pairs of reference and test\nrepresentations, features are first extracted using a feature extraction\nfront-end, a frame-level distance matrix is computed, and the obtained distance\nmatrix is considered as an image by a CNN-based binary classifier. The feature\nextraction, distance matrix computation, and CNN-based classifier are jointly\noptimized in an end-to-end framework. Experimental results on two databases of\nhealthy and dysarthric speakers for different languages and pathologies show\nthat the proposed approach yields a high dysarthric speech detection\nperformance, outperforming other CNN-based baseline approaches.", "published": "2020-11-15 14:57:49", "link": "http://arxiv.org/abs/2011.07545v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-task single channel speech enhancement using speech presence\n  probability as a secondary task training target", "abstract": "To cope with reverberation and noise in single channel acoustic scenarios,\ntypical supervised deep neural network~(DNN)-based techniques learn a mapping\nfrom reverberant and noisy input features to a user-defined target. Commonly\nused targets are the desired signal magnitude, a time-frequency mask such as\nthe Wiener gain, or the interference power spectral density and\nsignal-to-interference ratio that can be used to compute a time-frequency mask.\nIn this paper, we propose to incorporate multi-task learning in such DNN-based\nenhancement techniques by using speech presence probability (SPP) estimation as\na secondary task assisting the target estimation in the main task. The\nadvantage of multi-task learning lies in sharing domain-specific information\nbetween the two tasks (i.e., target and SPP estimation) and learning more\ngeneralizable and robust representations. To simultaneously learn both tasks,\nwe propose to use the adaptive weighting method of losses derived from the\nhomoscedastic uncertainty of tasks. Simulation results show that the\ndereverberation and noise reduction performance of a single-task DNN trained to\ndirectly estimate the Wiener gain is higher than the performance of single-task\nDNNs trained to estimate the desired signal magnitude, the interference power\nspectral density, or the signal-to-interference ratio. Incorporating the\nproposed multi-task learning scheme to jointly estimate the Wiener gain and the\nSPP increases the dereverberation and noise reduction further.", "published": "2020-11-15 15:04:50", "link": "http://arxiv.org/abs/2011.07547v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Automatic and perceptual discrimination between dysarthria, apraxia of\n  speech, and neurotypical speech", "abstract": "Automatic techniques in the context of motor speech disorders (MSDs) are\ntypically two-class techniques aiming to discriminate between dysarthria and\nneurotypical speech or between dysarthria and apraxia of speech (AoS). Further,\nalthough such techniques are proposed to support the perceptual assessment of\nclinicians, the automatic and perceptual classification accuracy has never been\ncompared. In this paper, we investigate a three-class automatic technique and a\nset of handcrafted features for the discrimination of dysarthria, AoS and\nneurotypical speech. Instead of following the commonly used One-versus-One or\nOne-versus-Rest approaches for multi-class classification, a hierarchical\napproach is proposed. Further, a perceptual study is conducted where speech and\nlanguage pathologists are asked to listen to recordings of dysarthria, AoS, and\nneurotypical speech and decide which class the recordings belong to. The\nproposed automatic technique is evaluated on the same recordings and the\nautomatic and perceptual classification performance are compared. The presented\nresults show that the hierarchical classification approach yields a higher\nclassification accuracy than baseline One-versus-One and One-versus-Rest\napproaches. Further, the presented results show that the automatic approach\nyields a higher classification accuracy than the perceptual assessment of\nspeech and language pathologists, demonstrating the potential advantages of\nintegrating automatic tools in clinical practice.", "published": "2020-11-15 14:48:28", "link": "http://arxiv.org/abs/2011.07542v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Event Recognition through the lens of Adversary", "abstract": "As audio/visual classification models are widely deployed for sensitive tasks\nlike content filtering at scale, it is critical to understand their robustness\nalong with improving the accuracy. This work aims to study several key\nquestions related to multimodal learning through the lens of adversarial\nnoises: 1) The trade-off between early/middle/late fusion affecting its\nrobustness and accuracy 2) How do different frequency/time domain features\ncontribute to the robustness? 3) How do different neural modules contribute to\nthe adversarial noise? In our experiment, we construct adversarial examples to\nattack state-of-the-art neural models trained on Google AudioSet. We compare\nhow much attack potency in terms of adversarial perturbation of size $\\epsilon$\nusing different $L_p$ norms we would need to \"deactivate\" the victim model.\nUsing adversarial noise to ablate multimodal models, we are able to provide\ninsights into what is the best potential fusion strategy to balance the model\nparameters/accuracy and robustness trade-off and distinguish the robust\nfeatures versus the non-robust features that various neural networks model tend\nto learn.", "published": "2020-11-15 01:36:09", "link": "http://arxiv.org/abs/2011.07430v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Improving Speech Enhancement Performance by Leveraging Contextual Broad\n  Phonetic Class Information", "abstract": "Previous studies have confirmed that by augmenting acoustic features with the\nplace/manner of articulatory features, the speech enhancement (SE) process can\nbe guided to consider the broad phonetic properties of the input speech when\nperforming enhancement to attain performance improvements. In this paper, we\nexplore the contextual information of articulatory attributes as additional\ninformation to further benefit SE. More specifically, we propose to improve the\nSE performance by leveraging losses from an end-to-end automatic speech\nrecognition (E2E-ASR) model that predicts the sequence of broad phonetic\nclasses (BPCs). We also developed multi-objective training with ASR and\nperceptual losses to train the SE system based on a BPC-based E2E-ASR.\nExperimental results from speech denoising, speech dereverberation, and\nimpaired speech enhancement tasks confirmed that contextual BPC information\nimproves SE performance. Moreover, the SE model trained with the BPC-based\nE2E-ASR outperforms that with the phoneme-based E2E-ASR. The results suggest\nthat objectives with misclassification of phonemes by the ASR system may lead\nto imperfect feedback, and BPC could be a potentially better choice. Finally,\nit is noted that combining the most-confusable phonetic targets into the same\nBPC when calculating the additional objective can effectively improve the SE\nperformance.", "published": "2020-11-15 03:56:37", "link": "http://arxiv.org/abs/2011.07442v5", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Frame Similarity using Siamese networks for Audio-to-Score\n  Alignment", "abstract": "Audio-to-score alignment aims at generating an accurate mapping between a\nperformance audio and the score of a given piece. Standard alignment methods\nare based on Dynamic Time Warping (DTW) and employ handcrafted features, which\ncannot be adapted to different acoustic conditions. We propose a method to\novercome this limitation using learned frame similarity for audio-to-score\nalignment. We focus on offline audio-to-score alignment of piano music.\nExperiments on music data from different acoustic conditions demonstrate that\nour method achieves higher alignment accuracy than a standard DTW-based method\nthat uses handcrafted features, and generates robust alignments whilst being\nadaptable to different domains at the same time.", "published": "2020-11-15 14:58:03", "link": "http://arxiv.org/abs/2011.07546v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Contrastive Learning of Sound Event Representations", "abstract": "Self-supervised representation learning can mitigate the limitations in\nrecognition tasks with few manually labeled data but abundant unlabeled\ndata---a common scenario in sound event research. In this work, we explore\nunsupervised contrastive learning as a way to learn sound event\nrepresentations. To this end, we propose to use the pretext task of contrasting\ndifferently augmented views of sound events. The views are computed primarily\nvia mixing of training examples with unrelated backgrounds, followed by other\ndata augmentations. We analyze the main components of our method via ablation\nexperiments. We evaluate the learned representations using linear evaluation,\nand in two in-domain downstream sound event classification tasks, namely, using\nlimited manually labeled data, and using noisy labeled data. Our results\nsuggest that unsupervised contrastive pre-training can mitigate the impact of\ndata scarcity and increase robustness against noisy labels, outperforming\nsupervised baselines.", "published": "2020-11-15 19:50:14", "link": "http://arxiv.org/abs/2011.07616v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Respiratory Distress Detection from Telephone Speech using Acoustic and\n  Prosodic Features", "abstract": "With the widespread use of telemedicine services, automatic assessment of\nhealth conditions via telephone speech can significantly impact public health.\nThis work summarizes our preliminary findings on automatic detection of\nrespiratory distress using well-known acoustic and prosodic features. Speech\nsamples are collected from de-identified telemedicine phonecalls from a\nhealthcare provider in Bangladesh. The recordings include conversational speech\nsamples of patients talking to doctors showing mild or severe respiratory\ndistress or asthma symptoms. We hypothesize that respiratory distress may alter\nspeech features such as voice quality, speaking pattern, loudness, and\nspeech-pause duration. To capture these variations, we utilize a set of\nwell-known acoustic and prosodic features with a Support Vector Machine (SVM)\nclassifier for detecting the presence of respiratory distress. Experimental\nevaluations are performed using a 3-fold cross-validation scheme, ensuring\npatient-independent data splits. We obtained an overall accuracy of 86.4\\% in\ndetecting respiratory distress from the speech recordings using the acoustic\nfeature set. Correlation analysis reveals that the top-performing features\ninclude loudness, voice rate, voice duration, and pause duration.", "published": "2020-11-15 13:32:45", "link": "http://arxiv.org/abs/2011.09270v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
