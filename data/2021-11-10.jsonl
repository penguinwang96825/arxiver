{"title": "Pre-trained Transformer-Based Approach for Arabic Question Answering : A\n  Comparative Study", "abstract": "Question answering(QA) is one of the most challenging yet widely investigated\nproblems in Natural Language Processing (NLP). Question-answering (QA) systems\ntry to produce answers for given questions. These answers can be generated from\nunstructured or structured text. Hence, QA is considered an important research\narea that can be used in evaluating text understanding systems. A large volume\nof QA studies was devoted to the English language, investigating the most\nadvanced techniques and achieving state-of-the-art results. However, research\nefforts in the Arabic question-answering progress at a considerably slower pace\ndue to the scarcity of research efforts in Arabic QA and the lack of large\nbenchmark datasets. Recently many pre-trained language models provided high\nperformance in many Arabic NLP problems. In this work, we evaluate the\nstate-of-the-art pre-trained transformers models for Arabic QA using four\nreading comprehension datasets which are Arabic-SQuAD, ARCD, AQAD, and\nTyDiQA-GoldP datasets. We fine-tuned and compared the performance of the\nAraBERTv2-base model, AraBERTv0.2-large model, and AraELECTRA model. In the\nlast, we provide an analysis to understand and interpret the low-performance\nresults obtained by some models.", "published": "2021-11-10 12:33:18", "link": "http://arxiv.org/abs/2111.05671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language\n  Understanding", "abstract": "Meta learning with auxiliary languages has demonstrated promising\nimprovements for cross-lingual natural language processing. However, previous\nstudies sample the meta-training and meta-testing data from the same language,\nwhich limits the ability of the model for cross-lingual transfer. In this\npaper, we propose XLA-MAML, which performs direct cross-lingual adaption in the\nmeta-learning stage. We conduct zero-shot and few-shot experiments on Natural\nLanguage Inference and Question Answering. The experimental results demonstrate\nthe effectiveness of our method across different languages, tasks, and\npretrained models. We also give analysis on various cross-lingual specific\nsettings for meta-learning including sampling strategy and parallelism.", "published": "2021-11-10 16:53:50", "link": "http://arxiv.org/abs/2111.05805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Corpus of Discourse Structure in Humans and Computers", "abstract": "We present a novel corpus of 445 human- and computer-generated documents,\ncomprising about 27,000 clauses, annotated for semantic clause types and\ncoherence relations that allow for nuanced comparison of artificial and natural\ndiscourse modes. The corpus covers both formal and informal discourse, and\ncontains documents generated using fine-tuned GPT-2 (Zellers et al., 2019) and\nGPT-3(Brown et al., 2020). We showcase the usefulness of this corpus for\ndetailed discourse analysis of text generation by providing preliminary\nevidence that less numerous, shorter and more often incoherent clause relations\nare associated with lower perceived quality of computer-generated narratives\nand arguments.", "published": "2021-11-10 20:56:08", "link": "http://arxiv.org/abs/2111.05940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Wind in Our Sails: Developing a Reusable and Maintainable Dutch\n  Maritime History Knowledge Graph", "abstract": "Digital sources are more prevalent than ever but effectively using them can\nbe challenging. One core challenge is that digitized sources are often\ndistributed, thus forcing researchers to spend time collecting, interpreting,\nand aligning different sources. A knowledge graph can accelerate research by\nproviding a single connected source of truth that humans and machines can\nquery. During two design-test cycles, we convert four data sets from the\nhistorical maritime domain into a knowledge graph. The focus during these\ncycles is on creating a sustainable and usable approach that can be adopted in\nother linked data conversion efforts. Furthermore, our knowledge graph is\navailable for maritime historians and other interested users to investigate the\ndaily business of the Dutch East India Company through a unified portal.", "published": "2021-11-10 09:54:01", "link": "http://arxiv.org/abs/2111.05605v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "CLIP2TV: Align, Match and Distill for Video-Text Retrieval", "abstract": "Modern video-text retrieval frameworks basically consist of three parts:\nvideo encoder, text encoder and the similarity head. With the success on both\nvisual and textual representation learning, transformer based encoders and\nfusion methods have also been adopted in the field of video-text retrieval. In\nthis report, we present CLIP2TV, aiming at exploring where the critical\nelements lie in transformer based methods. To achieve this, We first revisit\nsome recent works on multi-modal learning, then introduce some techniques into\nvideo-text retrieval, finally evaluate them through extensive experiments in\ndifferent configurations. Notably, CLIP2TV achieves 52.9@R1 on MSR-VTT dataset,\noutperforming the previous SOTA result by 4.1%.", "published": "2021-11-10 10:05:11", "link": "http://arxiv.org/abs/2111.05610v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding COVID-19 Vaccine Reaction through Comparative Analysis on\n  Twitter", "abstract": "Although multiple COVID-19 vaccines have been available for several months\nnow, vaccine hesitancy continues to be at high levels in the United States. In\npart, the issue has also become politicized, especially since the presidential\nelection in November. Understanding vaccine hesitancy during this period in the\ncontext of social media, including Twitter, can provide valuable guidance both\nto computational social scientists and policy makers. Rather than studying a\nsingle Twitter corpus, this paper takes a novel view of the problem by\ncomparatively studying two Twitter datasets collected between two different\ntime periods (one before the election, and the other, a few months after) using\nthe same, carefully controlled data collection and filtering methodology. Our\nresults show that there was a significant shift in discussion from politics to\nCOVID-19 vaccines from fall of 2020 to spring of 2021. By using clustering and\nmachine learning-based methods in conjunction with sampling and qualitative\nanalysis, we uncover several fine-grained reasons for vaccine hesitancy, some\nof which have become more (or less) important over time. Our results also\nunderscore the intense polarization and politicization of this issue over the\nlast year.", "published": "2021-11-10 17:39:10", "link": "http://arxiv.org/abs/2111.05823v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Two-Stage Approach towards Generalization in Knowledge Base Question\n  Answering", "abstract": "Most existing approaches for Knowledge Base Question Answering (KBQA) focus\non a specific underlying knowledge base either because of inherent assumptions\nin the approach, or because evaluating it on a different knowledge base\nrequires non-trivial changes. However, many popular knowledge bases share\nsimilarities in their underlying schemas that can be leveraged to facilitate\ngeneralization across knowledge bases. To achieve this generalization, we\nintroduce a KBQA framework based on a 2-stage architecture that explicitly\nseparates semantic parsing from the knowledge base interaction, facilitating\ntransfer learning across datasets and knowledge graphs. We show that\npretraining on datasets with a different underlying knowledge base can\nnevertheless provide significant performance gains and reduce sample\ncomplexity. Our approach achieves comparable or state-of-the-art performance\nfor LC-QuAD (DBpedia), WebQSP (Freebase), SimpleQuestions (Wikidata) and MetaQA\n(Wikimovies-KG).", "published": "2021-11-10 17:45:33", "link": "http://arxiv.org/abs/2111.05825v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Critical Sentence Identification in Legal Cases Using Multi-Class\n  Classification", "abstract": "Inherently, the legal domain contains a vast amount of data in text format.\nTherefore it requires the application of Natural Language Processing (NLP) to\ncater to the analytically demanding needs of the domain. The advancement of NLP\nis spreading through various domains, such as the legal domain, in forms of\npractical applications and academic research. Identifying critical sentences,\nfacts and arguments in a legal case is a tedious task for legal professionals.\nIn this research we explore the usage of sentence embeddings for multi-class\nclassification to identify critical sentences in a legal case, in the\nperspective of the main parties present in the case. In addition, a\ntask-specific loss function is defined in order to improve the accuracy\nrestricted by the straightforward use of categorical cross entropy loss.", "published": "2021-11-10 14:58:29", "link": "http://arxiv.org/abs/2111.05721v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prune Once for All: Sparse Pre-Trained Language Models", "abstract": "Transformer-based language models are applied to a wide range of applications\nin natural language processing. However, they are inefficient and difficult to\ndeploy. In recent years, many compression algorithms have been proposed to\nincrease the implementation efficiency of large Transformer-based models on\ntarget hardware. In this work we present a new method for training sparse\npre-trained Transformer language models by integrating weight pruning and model\ndistillation. These sparse pre-trained models can be used to transfer learning\nfor a wide range of tasks while maintaining their sparsity pattern. We\ndemonstrate our method with three known architectures to create sparse\npre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed\nsparse pre-trained models we trained transfer their knowledge to five different\ndownstream natural language tasks with minimal accuracy loss. Moreover, we show\nhow to further compress the sparse models' weights to 8bit precision using\nquantization-aware training. For example, with our sparse pre-trained\nBERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a\ncompression ratio of $40$X for the encoder with less than $1\\%$ accuracy loss.\nTo the best of our knowledge, our results show the best compression-to-accuracy\nratio for BERT-Base, BERT-Large, and DistilBERT.", "published": "2021-11-10 15:52:40", "link": "http://arxiv.org/abs/2111.05754v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BagBERT: BERT-based bagging-stacking for multi-topic classification", "abstract": "This paper describes our submission on the COVID-19 literature annotation\ntask at Biocreative VII. We proposed an approach that exploits the knowledge of\nthe globally non-optimal weights, usually rejected, to build a rich\nrepresentation of each label. Our proposed approach consists of two stages: (1)\nA bagging of various initializations of the training data that features weakly\ntrained weights, (2) A stacking of heterogeneous vocabulary models based on\nBERT and RoBERTa Embeddings. The aggregation of these weak insights performs\nbetter than a classical globally efficient model. The purpose is the\ndistillation of the richness of knowledge to a simpler and lighter model. Our\nsystem obtains an Instance-based F1 of 92.96 and a Label-based micro-F1 of\n91.35.", "published": "2021-11-10 17:00:36", "link": "http://arxiv.org/abs/2111.05808v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recent Advances in Automated Question Answering In Biomedical Domain", "abstract": "The objective of automated Question Answering (QA) systems is to provide\nanswers to user queries in a time efficient manner. The answers are usually\nfound in either databases (or knowledge bases) or a collection of documents\ncommonly referred to as the corpus. In the past few decades there has been a\nproliferation of acquisition of knowledge and consequently there has been an\nexponential growth in new scientific articles in the field of biomedicine.\nTherefore, it has become difficult to keep track of all the information in the\ndomain, even for domain experts. With the improvements in commercial search\nengines, users can type in their queries and get a small set of documents most\nrelevant for answering their query, as well as relevant snippets from the\ndocuments in some cases. However, it may be still tedious and time consuming to\nmanually look for the required information or answers. This has necessitated\nthe development of efficient QA systems which aim to find exact and precise\nanswers to user provided natural language questions in the domain of\nbiomedicine. In this paper, we introduce the basic methodologies used for\ndeveloping general domain QA systems, followed by a thorough investigation of\ndifferent aspects of biomedical QA systems, including benchmark datasets and\nseveral proposed approaches, both using structured databases and collection of\ntexts. We also explore the limitations of current systems and explore potential\navenues for further advancement.", "published": "2021-11-10 20:51:29", "link": "http://arxiv.org/abs/2111.05937v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Scaling ASR Improves Zero and Few Shot Learning", "abstract": "With 4.5 million hours of English speech from 10 different sources across 120\ncountries and models of up to 10 billion parameters, we explore the frontiers\nof scale for automatic speech recognition. We propose data selection techniques\nto efficiently scale training data to find the most valuable samples in massive\ndatasets. To efficiently scale model sizes, we leverage various optimizations\nsuch as sparse transducer loss and model sharding. By training 1-10B parameter\nuniversal English ASR models, we push the limits of speech recognition\nperformance across many domains. Furthermore, our models learn powerful speech\nrepresentations with zero and few-shot capabilities on novel domains and styles\nof speech, exceeding previous results across multiple in-house and public\nbenchmarks. For speakers with disorders due to brain damage, our best zero-shot\nand few-shot models achieve 22% and 60% relative improvement on the AphasiaBank\ntest set, respectively, while realizing the best performance on public social\nmedia videos. Furthermore, the same universal model reaches equivalent\nperformance with 500x less in-domain data on the SPGISpeech financial-domain\ndataset.", "published": "2021-11-10 21:18:59", "link": "http://arxiv.org/abs/2111.05948v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-language Information Retrieval", "abstract": "Two key assumptions shape the usual view of ranked retrieval: (1) that the\nsearcher can choose words for their query that might appear in the documents\nthat they wish to see, and (2) that ranking retrieved documents will suffice\nbecause the searcher will be able to recognize those which they wished to find.\nWhen the documents to be searched are in a language not known by the searcher,\nneither assumption is true. In such cases, Cross-Language Information Retrieval\n(CLIR) is needed. This chapter reviews the state of the art for CLIR and\noutlines some open research questions.", "published": "2021-11-10 23:37:23", "link": "http://arxiv.org/abs/2111.05988v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Improving the Chamberlin Digital State Variable Filter", "abstract": "The state variable filter configuration is a classic analogue design which\nhas been employed in many electronic music applications. A digital\nimplementation of this filter was put forward by Chamberlin, which has been\ndeployed in both software and hardware forms. While this has proven to be a\nstraightforward and successful digital filter design, it suffers from some\nissues, which have already been identified in the literature. From a modified\nChamberlin block diagram, we derive the transfer functions describing its three\nbasic responses, highpass, bandpass, and lowpass. An analysis of these leads to\nthe development of an improvement, which attempts to better shape the filter\nspectrum. From these new transfer functions, a set of filter equations is\ndeveloped. Finally, the approach is compared to an alternative time-domain\nbased re-organisation of update equations, which is shown to deliver a similar\nresult.", "published": "2021-11-10 09:33:23", "link": "http://arxiv.org/abs/2111.05592v2", "categories": ["cs.SD", "eess.AS", "94A12", "J.5"], "primary_category": "cs.SD"}
{"title": "OSSEM: one-shot speaker adaptive speech enhancement using meta learning", "abstract": "Although deep learning (DL) has achieved notable progress in speech\nenhancement (SE), further research is still required for a DL-based SE system\nto adapt effectively and efficiently to particular speakers. In this study, we\npropose a novel meta-learning-based speaker-adaptive SE approach (called OSSEM)\nthat aims to achieve SE model adaptation in a one-shot manner. OSSEM consists\nof a modified transformer SE network and a speaker-specific masking (SSM)\nnetwork. In practice, the SSM network takes an enrolled speaker embedding\nextracted using ECAPA-TDNN to adjust the input noisy feature through masking.\nTo evaluate OSSEM, we designed a modified Voice Bank-DEMAND dataset, in which\none utterance from the testing set was used for model adaptation, and the\nremaining utterances were used for testing the performance. Moreover, we set\nrestrictions allowing the enhancement process to be conducted in real time, and\nthus designed OSSEM to be a causal SE system. Experimental results first show\nthat OSSEM can effectively adapt a pretrained SE model to a particular speaker\nwith only one utterance, thus yielding improved SE results. Meanwhile, OSSEM\nexhibits a competitive performance compared to state-of-the-art causal SE\nsystems.", "published": "2021-11-10 14:21:28", "link": "http://arxiv.org/abs/2111.05703v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HASA-net: A non-intrusive hearing-aid speech assessment network", "abstract": "Without the need of a clean reference, non-intrusive speech assessment\nmethods have caught great attention for objective evaluations. Recently, deep\nneural network (DNN) models have been applied to build non-intrusive speech\nassessment approaches and confirmed to provide promising performance. However,\nmost DNN-based approaches are designed for normal-hearing listeners without\nconsidering hearing-loss factors. In this study, we propose a DNN-based hearing\naid speech assessment network (HASA-Net), formed by a bidirectional long\nshort-term memory (BLSTM) model, to predict speech quality and intelligibility\nscores simultaneously according to input speech signals and specified\nhearing-loss patterns. To the best of our knowledge, HASA-Net is the first work\nto incorporate quality and intelligibility assessments utilizing a unified\nDNN-based non-intrusive model for hearing aids. Experimental results show that\nthe predicted speech quality and intelligibility scores of HASA-Net are highly\ncorrelated to two well-known intrusive hearing-aid evaluation metrics, hearing\naid speech quality index (HASQI) and hearing aid speech perception index\n(HASPI), respectively.", "published": "2021-11-10 14:10:13", "link": "http://arxiv.org/abs/2111.05691v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal End-to-End Group Emotion Recognition using Cross-Modal\n  Attention", "abstract": "Classifying group-level emotions is a challenging task due to complexity of\nvideo, in which not only visual, but also audio information should be taken\ninto consideration. Existing works on multimodal emotion recognition are using\nbulky approach, where pretrained neural networks are used as a feature\nextractors and then extracted features are being fused. However, this approach\ndoes not consider attributes of multimodal data and feature extractors cannot\nbe fine-tuned for specific task which can be disadvantageous for overall model\naccuracy. To this end, our impact is twofold: (i) we train model end-to-end,\nwhich allows early layers of neural network to be adapted with taking into\naccount later, fusion layers, of two modalities; (ii) all layers of our model\nwas fine-tuned for downstream task of emotion recognition, so there were no\nneed to train neural networks from scratch. Our model achieves best validation\naccuracy of 60.37% which is approximately 8.5% higher, than VGAF dataset\nbaseline and is competitive with existing works, audio and video modalities.", "published": "2021-11-10 19:19:26", "link": "http://arxiv.org/abs/2111.05890v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Generic Deep Learning Based Cough Analysis System from Clinically\n  Validated Samples for Point-of-Need Covid-19 Test and Severity Levels", "abstract": "We seek to evaluate the detection performance of a rapid primary screening\ntool of Covid-19 solely based on the cough sound from 8,380 clinically\nvalidated samples with laboratory molecular-test (2,339 Covid-19 positives and\n6,041 Covid-19 negatives). Samples were clinically labeled according to the\nresults and severity based on quantitative RT-PCR (qRT-PCR) analysis, cycle\nthreshold, and lymphocytes count from the patients. Our proposed generic method\nis an algorithm based on Empirical Mode Decomposition (EMD) with subsequent\nclassification based on a tensor of audio features and a deep artificial neural\nnetwork classifier with convolutional layers called DeepCough'. Two different\nversions of DeepCough based on the number of tensor dimensions, i.e.\nDeepCough2D and DeepCough3D, have been investigated. These methods have been\ndeployed in a multi-platform proof-of-concept Web App CoughDetect to administer\nthis test anonymously. Covid-19 recognition results rates achieved a promising\nAUC (Area Under Curve) of 98.800.83%, sensitivity of 96.431.85%, and\nspecificity of 96.201.74%, and 81.08%5.05% AUC for the recognition of three\nseverity levels. Our proposed web tool and underpinning algorithm for the\nrobust, fast, point-of-need identification of Covid-19 facilitates the rapid\ndetection of the infection. We believe that it has the potential to\nsignificantly hamper the Covid-19 pandemic across the world.", "published": "2021-11-10 19:39:26", "link": "http://arxiv.org/abs/2111.05895v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Structure from Silence: Learning Scene Structure from Ambient Sound", "abstract": "From whirling ceiling fans to ticking clocks, the sounds that we hear subtly\nvary as we move through a scene. We ask whether these ambient sounds convey\ninformation about 3D scene structure and, if so, whether they provide a useful\nlearning signal for multimodal models. To study this, we collect a dataset of\npaired audio and RGB-D recordings from a variety of quiet indoor scenes. We\nthen train models that estimate the distance to nearby walls, given only audio\nas input. We also use these recordings to learn multimodal representations\nthrough self-supervision, by training a network to associate images with their\ncorresponding sounds. These results suggest that ambient sound conveys a\nsurprising amount of information about scene structure, and that it is a useful\nsignal for learning multimodal features.", "published": "2021-11-10 18:55:14", "link": "http://arxiv.org/abs/2111.05846v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
