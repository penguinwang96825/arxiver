{"title": "Understanding Chat Messages for Sticker Recommendation in Messaging Apps", "abstract": "Stickers are popularly used in messaging apps such as Hike to visually\nexpress a nuanced range of thoughts and utterances to convey exaggerated\nemotions. However, discovering the right sticker from a large and ever\nexpanding pool of stickers while chatting can be cumbersome. In this paper, we\ndescribe a system for recommending stickers in real time as the user is typing\nbased on the context of the conversation. We decompose the sticker\nrecommendation (SR) problem into two steps. First, we predict the message that\nthe user is likely to send in the chat. Second, we substitute the predicted\nmessage with an appropriate sticker. Majority of Hike's messages are in the\nform of text which is transliterated from users' native language to the Roman\nscript. This leads to numerous orthographic variations of the same message and\nmakes accurate message prediction challenging. To address this issue, we learn\ndense representations of chat messages employing character level convolution\nnetwork in an unsupervised manner. We use them to cluster the messages that\nhave the same meaning. In the subsequent steps, we predict the message cluster\ninstead of the message. Our approach does not depend on human labelled data\n(except for validation), leading to fully automatic updation and tuning\npipeline for the underlying models. We also propose a novel hybrid message\nprediction model, which can run with low latency on low-end phones that have\nsevere computational limitations. Our described system has been deployed for\nmore than $6$ months and is being used by millions of users along with hundreds\nof thousands of expressive stickers.", "published": "2019-02-07 16:01:43", "link": "http://arxiv.org/abs/1902.02704v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect Specific Opinion Expression Extraction using Attention based\n  LSTM-CRF Network", "abstract": "Opinion phrase extraction is one of the key tasks in fine-grained sentiment\nanalysis. While opinion expressions could be generic subjective expressions,\naspect specific opinion expressions contain both the aspect as well as the\nopinion expression within the original sentence context. In this work, we\nformulate the task as an instance of token-level sequence labeling. When\nmultiple aspects are present in a sentence, detection of opinion phrase\nboundary becomes difficult and label of each word depend not only upon the\nsurrounding words but also with the concerned aspect. We propose a neural\nnetwork architecture with bidirectional LSTM (Bi-LSTM) and a novel attention\nmechanism. Bi-LSTM layer learns the various sequential pattern among the words\nwithout requiring any hand-crafted features. The attention mechanism captures\nthe importance of context words on a particular aspect opinion expression when\nmultiple aspects are present in a sentence via location and content based\nmemory. A Conditional Random Field (CRF) model is incorporated in the final\nlayer to explicitly model the dependencies among the output labels.\nExperimental results on Hotel dataset from Tripadvisor.com showed that our\napproach outperformed several state-of-the-art baselines.", "published": "2019-02-07 16:12:41", "link": "http://arxiv.org/abs/1902.02709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Autoencoding Variational Inference for Aspect-based Opinion\n  Summary", "abstract": "Aspect-based Opinion Summary (AOS), consisting of aspect discovery and\nsentiment classification steps, has recently been emerging as one of the most\ncrucial data mining tasks in e-commerce systems. Along this direction, the\nLDA-based model is considered as a notably suitable approach, since this model\noffers both topic modeling and sentiment classification. However, unlike\ntraditional topic modeling, in the context of aspect discovery it is often\nrequired some initial seed words, whose prior knowledge is not easy to be\nincorporated into LDA models. Moreover, LDA approaches rely on sampling\nmethods, which need to load the whole corpus into memory, making them hardly\nscalable. In this research, we study an alternative approach for AOS problem,\nbased on Autoencoding Variational Inference (AVI). Firstly, we introduce the\nAutoencoding Variational Inference for Aspect Discovery (AVIAD) model, which\nextends the previous work of Autoencoding Variational Inference for Topic\nModels (AVITM) to embed prior knowledge of seed words. This work includes\nenhancement of the previous AVI architecture and also modification of the loss\nfunction. Ultimately, we present the Autoencoding Variational Inference for\nJoint Sentiment/Topic (AVIJST) model. In this model, we substantially extend\nthe AVI model to support the JST model, which performs topic modeling for\ncorresponding sentiment. The experimental results show that our proposed models\nenjoy higher topic coherent, faster convergence time and better accuracy on\nsentiment classification, as compared to their LDA-based counterparts.", "published": "2019-02-07 07:44:03", "link": "http://arxiv.org/abs/1902.02507v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in\n  Multi-Task Learning", "abstract": "Multi-task learning shares information between related tasks, sometimes\nreducing the number of parameters required. State-of-the-art results across\nmultiple natural language understanding tasks in the GLUE benchmark have\npreviously used transfer from a single large task: unsupervised pre-training\nwith BERT, where a separate BERT model was fine-tuned for each task. We explore\nmulti-task approaches that share a single BERT model with a small number of\nadditional task-specific parameters. Using new adaptation modules, PALs or\n`projected attention layers', we match the performance of separately fine-tuned\nmodels on the GLUE benchmark with roughly 7 times fewer parameters, and obtain\nstate-of-the-art results on the Recognizing Textual Entailment dataset.", "published": "2019-02-07 15:05:46", "link": "http://arxiv.org/abs/1902.02671v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Target Speaker Extraction for Overlapped Multi-Talker Speaker\n  Verification", "abstract": "The performance of speaker verification degrades significantly when the test\nspeech is corrupted by interference speakers. Speaker diarization does well to\nseparate speakers if the speakers are temporally overlapped. However, if\nmulti-talkers speak at the same time, we need the technique to separate the\nspeech in the spectral domain. This paper proposes an overlapped multi-talker\nspeaker verification framework by using target speaker extraction methods.\nSpecifically, given the target speaker information, the target speaker's speech\nis firstly extracted from the overlapped multi-talker speech by a target\nspeaker extraction module. Then, the extracted speech is passed to the speaker\nverification system. Experimental results show that the proposed approach\nsignificantly improves the performance of overlapped multi-talker speaker\nverification and achieves 65.7% relative EER reduction.", "published": "2019-02-07 09:55:50", "link": "http://arxiv.org/abs/1902.02546v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end losses based on speaker basis vectors and all-speaker hard\n  negative mining for speaker verification", "abstract": "In recent years, speaker verification has primarily performed using deep\nneural networks that are trained to output embeddings from input features such\nas spectrograms or Mel-filterbank energies. Studies that design various loss\nfunctions, including metric learning have been widely explored. In this study,\nwe propose two end-to-end loss functions for speaker verification using the\nconcept of speaker bases, which are trainable parameters. One loss function is\ndesigned to further increase the inter-speaker variation, and the other is\ndesigned to conduct the identical concept with hard negative mining. Each\nspeaker basis is designed to represent the corresponding speaker in the process\nof training deep neural networks. In contrast to the conventional loss\nfunctions that can consider only a limited number of speakers included in a\nmini-batch, the proposed loss functions can consider all the speakers in the\ntraining set regardless of the mini-batch composition. In particular, the\nproposed loss functions enable hard negative mining and calculations of\nbetween-speaker variations with consideration of all speakers. Through\nexperiments on VoxCeleb1 and VoxCeleb2 datasets, we confirmed that the proposed\nloss functions could supplement conventional softmax and center loss functions.", "published": "2019-02-07 02:55:02", "link": "http://arxiv.org/abs/1902.02455v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conv-codes: Audio Hashing For Bird Species Classification", "abstract": "In this work, we propose a supervised, convex representation based audio\nhashing framework for bird species classification. The proposed framework\nutilizes archetypal analysis, a matrix factorization technique, to obtain\nconvex-sparse representations of a bird vocalization. These convex\nrepresentations are hashed using Bloom filters with non-cryptographic hash\nfunctions to obtain compact binary codes, designated as conv-codes. The\nconv-codes extracted from the training examples are clustered using\nclass-specific k-medoids clustering with Jaccard coefficient as the similarity\nmetric. A hash table is populated using the cluster centers as keys while hash\nvalues/slots are pointers to the species identification information. During\ntesting, the hash table is searched to find the species information\ncorresponding to a cluster center that exhibits maximum similarity with the\ntest conv-code. Hence, the proposed framework classifies a bird vocalization in\nthe conv-code space and requires no explicit classifier or reconstruction error\ncalculations. Apart from that, based on min-hash and direct addressing, we also\npropose a variant of the proposed framework that provides faster and effective\nclassification. The performances of both these frameworks are compared with\nexisting bird species classification frameworks on the audio recordings of 50\ndifferent bird species.", "published": "2019-02-07 07:18:39", "link": "http://arxiv.org/abs/1902.02498v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hide and Speak: Towards Deep Neural Networks for Speech Steganography", "abstract": "Steganography is the science of hiding a secret message within an ordinary\npublic message, which is referred to as Carrier. Traditionally, digital signal\nprocessing techniques, such as least significant bit encoding, were used for\nhiding messages. In this paper, we explore the use of deep neural networks as\nsteganographic functions for speech data. We showed that steganography models\nproposed for vision are less suitable for speech, and propose a new model that\nincludes the short-time Fourier transform and inverse-short-time Fourier\ntransform as differentiable layers within the network, thus imposing a vital\nconstraint on the network outputs. We empirically demonstrated the\neffectiveness of the proposed method comparing to deep learning based on\nseveral speech datasets and analyzed the results quantitatively and\nqualitatively. Moreover, we showed that the proposed approach could be applied\nto conceal multiple messages in a single carrier using multiple decoders or a\nsingle conditional decoder. Lastly, we evaluated our model under different\nchannel distortions. Qualitative experiments suggest that modifications to the\ncarrier are unnoticeable by human listeners and that the decoded messages are\nhighly intelligible.", "published": "2019-02-07 13:48:28", "link": "http://arxiv.org/abs/1902.03083v2", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
