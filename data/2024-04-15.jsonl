{"title": "Automatic Knowledge Graph Construction for Judicial Cases", "abstract": "In this paper, we explore the application of cognitive intelligence in legal\nknowledge, focusing on the development of judicial artificial intelligence.\nUtilizing natural language processing (NLP) as the core technology, we propose\na method for the automatic construction of case knowledge graphs for judicial\ncases. Our approach centers on two fundamental NLP tasks: entity recognition\nand relationship extraction. We compare two pre-trained models for entity\nrecognition to establish their efficacy. Additionally, we introduce a\nmulti-task semantic relationship extraction model that incorporates\ntranslational embedding, leading to a nuanced contextualized case knowledge\nrepresentation. Specifically, in a case study involving a \"Motor Vehicle\nTraffic Accident Liability Dispute,\" our approach significantly outperforms the\nbaseline model. The entity recognition F1 score improved by 0.36, while the\nrelationship extraction F1 score increased by 2.37. Building on these results,\nwe detail the automatic construction process of case knowledge graphs for\njudicial cases, enabling the assembly of knowledge graphs for hundreds of\nthousands of judgments. This framework provides robust semantic support for\napplications of judicial AI, including the precise categorization and\nrecommendation of related cases.", "published": "2024-04-15 02:08:28", "link": "http://arxiv.org/abs/2404.09416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap between Different Vocabularies for LLM Ensemble", "abstract": "Ensembling different large language models (LLMs) to unleash their\ncomplementary potential and harness their individual strengths is highly\nvaluable. Nevertheless, vocabulary discrepancies among various LLMs have\nconstrained previous studies to either selecting or blending completely\ngenerated outputs. This limitation hinders the dynamic correction and\nenhancement of outputs during the generation process, resulting in a limited\ncapacity for effective ensemble. To address this issue, we propose a novel\nmethod to Ensemble LLMs via Vocabulary Alignment (EVA). EVA bridges the lexical\ngap among various LLMs, enabling meticulous ensemble at each generation step.\nSpecifically, we first learn mappings between the vocabularies of different\nLLMs with the assistance of overlapping tokens. Subsequently, these mappings\nare employed to project output distributions of LLMs into a unified space,\nfacilitating a fine-grained ensemble. Finally, we design a filtering strategy\nto exclude models that generate unfaithful tokens. Experimental results on\ncommonsense reasoning, arithmetic reasoning, machine translation, and\ndata-to-text generation tasks demonstrate the superiority of our approach\ncompared with individual LLMs and previous ensemble methods conducted on\ncomplete outputs. Further analyses confirm that our approach can leverage\nknowledge from different language models and yield consistent improvement.", "published": "2024-04-15 06:28:20", "link": "http://arxiv.org/abs/2404.09492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Recall of Large Language Models: A Model Collaboration\n  Approach for Relational Triple Extraction", "abstract": "Relation triple extraction, which outputs a set of triples from long\nsentences, plays a vital role in knowledge acquisition. Large language models\ncan accurately extract triples from simple sentences through few-shot learning\nor fine-tuning when given appropriate instructions. However, they often miss\nout when extracting from complex sentences. In this paper, we design an\nevaluation-filtering framework that integrates large language models with small\nmodels for relational triple extraction tasks. The framework includes an\nevaluation model that can extract related entity pairs with high precision. We\npropose a simple labeling principle and a deep neural network to build the\nmodel, embedding the outputs as prompts into the extraction process of the\nlarge model. We conduct extensive experiments to demonstrate that the proposed\nmethod can assist large language models in obtaining more accurate extraction\nresults, especially from complex sentences containing multiple relational\ntriples. Our evaluation model can also be embedded into traditional extraction\nmodels to enhance their extraction precision from complex sentences.", "published": "2024-04-15 09:03:05", "link": "http://arxiv.org/abs/2404.09593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resilience of Large Language Models for Noisy Instructions", "abstract": "As the rapidly advancing domain of natural language processing (NLP), large\nlanguage models (LLMs) have emerged as powerful tools for interpreting human\ncommands and generating text across various tasks. Nonetheless, the resilience\nof LLMs to handle text containing inherent errors, stemming from human\ninteractions and collaborative systems, has not been thoroughly explored. Our\nstudy investigates the resilience of LLMs against five common types of\ndisruptions including 1) ASR (Automatic Speech Recognition) errors, 2) OCR\n(Optical Character Recognition) errors, 3) grammatical mistakes, 4)\ntypographical errors, and 5) distractive content. We aim to investigate how\nthese models react by deliberately embedding these errors into instructions.\nOur findings reveal that while some LLMs show a degree of resistance to certain\ntypes of noise, their overall performance significantly suffers. This\nemphasizes the importance of further investigation into enhancing model\nresilience. In response to the observed decline in performance, our study also\nevaluates a \"re-pass\" strategy, designed to purify the instructions of noise\nbefore the LLMs process them. Our analysis indicates that correcting noisy\ninstructions, particularly for open-source LLMs, presents significant\nchallenges.", "published": "2024-04-15 12:55:08", "link": "http://arxiv.org/abs/2404.09754v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity,\n  Bias and Propensity for Hallucinations", "abstract": "This paper introduces fourteen novel datasets for the evaluation of Large\nLanguage Models' safety in the context of enterprise tasks. A method was\ndevised to evaluate a model's safety, as determined by its ability to follow\ninstructions and output factual, unbiased, grounded, and appropriate content.\nIn this research, we used OpenAI GPT as point of comparison since it excels at\nall levels of safety. On the open-source side, for smaller models, Meta Llama2\nperforms well at factuality and toxicity but has the highest propensity for\nhallucination. Mistral hallucinates the least but cannot handle toxicity well.\nIt performs well in a dataset mixing several tasks and safety vectors in a\nnarrow vertical domain. Gemma, the newly introduced open-source model based on\nGoogle Gemini, is generally balanced but trailing behind. When engaging in\nback-and-forth conversation (multi-turn prompts), we find that the safety of\nopen-source models degrades significantly. Aside from OpenAI's GPT, Mistral is\nthe only model that still performed well in multi-turn tests.", "published": "2024-04-15 13:40:08", "link": "http://arxiv.org/abs/2404.09785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of Preference Noise on the Alignment Performance of Generative\n  Language Models", "abstract": "A key requirement in developing Generative Language Models (GLMs) is to have\ntheir values aligned with human values. Preference-based alignment is a widely\nused paradigm for this purpose, in which preferences over generation pairs are\nfirst elicited from human annotators or AI systems, and then fed into some\nalignment techniques, e.g., Direct Preference Optimization. However, a\nsubstantial percent (20 - 40%) of the preference pairs used in GLM alignment\nare noisy, and it remains unclear how the noise affects the alignment\nperformance and how to mitigate its negative impact. In this paper, we propose\na framework to inject desirable amounts and types of noise to the preferences,\nand systematically study the impact of preference noise on the alignment\nperformance in two tasks (summarization and dialogue generation). We find that\nthe alignment performance can be highly sensitive to the noise rates in the\npreference data: e.g., a 10 percentage points (pp) increase of the noise rate\ncan lead to 30 pp drop in the alignment performance (in win rate). To mitigate\nthe impact of noise, confidence-based data filtering shows significant benefit\nwhen certain types of noise are present. We hope our work can help the\ncommunity better understand and mitigate the impact of preference noise in GLM\nalignment.", "published": "2024-04-15 14:21:53", "link": "http://arxiv.org/abs/2404.09824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatShop: Interactive Information Seeking with Language Agents", "abstract": "The desire and ability to seek new information strategically are fundamental\nto human learning but often overlooked in current language agent evaluation. We\nanalyze a popular web shopping task designed to test language agents' ability\nto perform strategic exploration and discover that it can be reformulated and\nsolved as a single-turn retrieval task without the need for interactive\ninformation seeking. This finding encourages us to rethink realistic\nconstraints on information access that would necessitate strategic information\nseeking. We then redesign the task to introduce a notion of task ambiguity and\nthe role of a shopper, serving as a dynamic party with whom the agent\nstrategically interacts in an open-ended conversation to make informed\ndecisions. Our experiments demonstrate that the proposed task can effectively\nevaluate the agent's ability to explore and gradually accumulate information\nthrough multi-turn interactions. Additionally, we show that large language\nmodel-simulated shoppers serve as a good proxy for real human shoppers,\nrevealing similar error patterns in agents.", "published": "2024-04-15 16:35:41", "link": "http://arxiv.org/abs/2404.09911v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Benchmarks and Interventions for Combating Hallucinations\n  in LLMs", "abstract": "Large language models (LLMs) are prone to hallucinations, which sparked a\nwidespread effort to detect and prevent them. Recent work attempts to mitigate\nhallucinations by intervening in the model's generation, typically computing\nrepresentative vectors of hallucinations vs. grounded generations, for steering\nthe model's hidden states away from a hallucinatory state. However, common\nstudies employ different setups and do not properly separate different possible\ncauses of hallucinations, making interventions misguided. In this work, we\nintroduce a method for categorizing examples based on the model's prior\nknowledge, named WACK. We construct WACK benchmarks that support interventions\nin two settings: open-book and closed-book question answering. Using the\nbenchmarks, we perform an extensive investigation of the effect of different\nchoices for intervention, such as the intervened components, and how often and\nhow strongly to intervene. We find that intervention success varies depending\non the component, with the attention blocks performing well and the residual\nstream proving detrimental to language modeling capabilities. We also show that\ninterventions can benefit from representative vectors collected before, rather\nthan after, a hallucination occurs. Finally, we introduce a new dynamic\nintervention, which intervenes only if needed, and thus is more robust than\nstandard static interventions. The code is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .", "published": "2024-04-15 17:48:46", "link": "http://arxiv.org/abs/2404.09971v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Memory Sharing for Large Language Model based Agents", "abstract": "The adaptation of Large Language Model (LLM)-based agents to execute tasks\nvia natural language prompts represents a significant advancement, notably\neliminating the need for explicit retraining or fine tuning, but are\nconstrained by the comprehensiveness and diversity of the provided examples,\nleading to outputs that often diverge significantly from expected results,\nespecially when it comes to the open-ended questions. This paper introduces the\nMemory Sharing, a framework which integrates the real-time memory filter,\nstorage and retrieval to enhance the In-Context Learning process. This\nframework allows for the sharing of memories among multiple agents, whereby the\ninteractions and shared memories between different agents effectively enhance\nthe diversity of the memories. The collective self-enhancement through\ninteractive learning among multiple agents facilitates the evolution from\nindividual intelligence to collective intelligence. Besides, the dynamically\ngrowing memory pool is utilized not only to improve the quality of responses\nbut also to train and enhance the retriever. We evaluated our framework across\nthree distinct domains involving specialized tasks of agents. The experimental\nresults demonstrate that the MS framework significantly improves the agents'\nperformance in addressing open-ended questions.", "published": "2024-04-15 17:57:30", "link": "http://arxiv.org/abs/2404.09982v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effects of Fine-tuning Language Models for Text-Based\n  Reinforcement Learning", "abstract": "Text-based reinforcement learning involves an agent interacting with a\nfictional environment using observed text and admissible actions in natural\nlanguage to complete a task. Previous works have shown that agents can succeed\nin text-based interactive environments even in the complete absence of semantic\nunderstanding or other linguistic capabilities. The success of these agents in\nplaying such games suggests that semantic understanding may not be important\nfor the task. This raises an important question about the benefits of LMs in\nguiding the agents through the game states. In this work, we show that rich\nsemantic understanding leads to efficient training of text-based RL agents.\nMoreover, we describe the occurrence of semantic degeneration as a consequence\nof inappropriate fine-tuning of language models in text-based reinforcement\nlearning (TBRL). Specifically, we describe the shift in the semantic\nrepresentation of words in the LM, as well as how it affects the performance of\nthe agent in tasks that are semantically similar to the training games. We\nbelieve these results may help develop better strategies to fine-tune agents in\ntext-based RL scenarios.", "published": "2024-04-15 23:05:57", "link": "http://arxiv.org/abs/2404.10174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Name Entity Recognition on StackOverflow", "abstract": "StackOverflow, with its vast question repository and limited labeled\nexamples, raise an annotation challenge for us. We address this gap by\nproposing RoBERTa+MAML, a few-shot named entity recognition (NER) method\nleveraging meta-learning. Our approach, evaluated on the StackOverflow NER\ncorpus (27 entity types), achieves a 5% F1 score improvement over the baseline.\nWe improved the results further domain-specific phrase processing enhance\nresults.", "published": "2024-04-15 01:43:14", "link": "http://arxiv.org/abs/2404.09405v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucination in Abstractive Summarization with\n  Domain-Conditional Mutual Information", "abstract": "A primary challenge in abstractive summarization is hallucination -- the\nphenomenon where a model generates plausible text that is absent in the source\ntext. We hypothesize that the domain (or topic) of the source text triggers the\nmodel to generate text that is highly probable in the domain, neglecting the\ndetails of the source text. To alleviate this model bias, we introduce a\ndecoding strategy based on domain-conditional pointwise mutual information.\nThis strategy adjusts the generation probability of each token by comparing it\nwith the token's marginal probability within the domain of the source text.\nAccording to evaluation on the XSUM dataset, our method demonstrates\nimprovement in terms of faithfulness and source relevance. The code is publicly\navailable at \\url{https://github.com/qqplot/dcpmi}.", "published": "2024-04-15 06:06:43", "link": "http://arxiv.org/abs/2404.09480v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large language models and linguistic intentionality", "abstract": "Do large language models like Chat-GPT or LLaMa meaningfully use the words\nthey produce? Or are they merely clever prediction machines, simulating\nlanguage use by producing statistically plausible text? There have already been\nsome initial attempts to answer this question by showing that these models meet\nthe criteria for entering meaningful states according to metasemantic theories\nof mental content. In this paper, I will argue for a different approach - that\nwe should instead consider whether language models meet the criteria given by\nour best metasemantic theories of linguistic content. In that vein, I will\nillustrate how this can be done by applying two such theories to the case of\nlanguage models: Gareth Evans' (1982) account of naming practices and Ruth\nMillikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it\nis a mistake to think that the failure of LLMs to meet plausible conditions for\nmental intentionality thereby renders their outputs meaningless, and that a\ndistinguishing feature of linguistic intentionality - dependency on a\npre-existing linguistic system - allows for the plausible result LLM outputs\nare meaningful.", "published": "2024-04-15 08:37:26", "link": "http://arxiv.org/abs/2404.09576v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformers, Contextualism, and Polysemy", "abstract": "The transformer architecture, introduced by Vaswani et al. (2017), is at the\nheart of the remarkable recent progress in the development of language models,\nincluding widely-used chatbots such as Chat-GPT and Claude. In this paper, I\nargue that we can extract from the way the transformer architecture works a\ntheory of the relationship between context and meaning. I call this the\ntransformer theory, and I argue that it is novel with regard to two related\nphilosophical debates: the contextualism debate regarding the extent of\ncontext-sensitivity across natural language, and the polysemy debate regarding\nhow polysemy should be captured within an account of word meaning.", "published": "2024-04-15 08:38:43", "link": "http://arxiv.org/abs/2404.09577v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modelling Language", "abstract": "This paper argues that large language models have a valuable scientific role\nto play in serving as scientific models of a language. Linguistic study should\nnot only be concerned with the cognitive processes behind linguistic\ncompetence, but also with language understood as an external, social entity.\nOnce this is recognized, the value of large language models as scientific\nmodels becomes clear. This paper defends this position against a number of\narguments to the effect that language models provide no linguistic insight. It\nalso draws upon recent work in philosophy of science to show how large language\nmodels could serve as scientific models.", "published": "2024-04-15 08:40:01", "link": "http://arxiv.org/abs/2404.09579v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "If there's a Trigger Warning, then where's the Trigger? Investigating\n  Trigger Warnings at the Passage Level", "abstract": "Trigger warnings are labels that preface documents with sensitive content if\nthis content could be perceived as harmful by certain groups of readers. Since\nwarnings about a document intuitively need to be shown before reading it,\nauthors usually assign trigger warnings at the document level. What parts of\ntheir writing prompted them to assign a warning, however, remains unclear. We\ninvestigate for the first time the feasibility of identifying the triggering\npassages of a document, both manually and computationally. We create a dataset\nof 4,135 English passages, each annotated with one of eight common trigger\nwarnings. In a large-scale evaluation, we then systematically evaluate the\neffectiveness of fine-tuned and few-shot classifiers, and their\ngeneralizability. We find that trigger annotation belongs to the group of\nsubjective annotation tasks in NLP, and that automatic trigger classification\nremains challenging but feasible.", "published": "2024-04-15 09:37:52", "link": "http://arxiv.org/abs/2404.09615v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Learn Your Reference Model for Real Good Alignment", "abstract": "Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches.", "published": "2024-04-15 10:44:31", "link": "http://arxiv.org/abs/2404.09656v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data\n  Annotation", "abstract": "The quality of the dataset is crucial for ensuring optimal performance and\nreliability of downstream task models. However, datasets often contain noisy\ndata inadvertently included during the construction process. Numerous attempts\nhave been made to correct this issue through human annotators. However, hiring\nand managing human annotators is expensive and time-consuming. As an\nalternative, recent studies are exploring the use of large language models\n(LLMs) for data annotation.\n  In this study, we present a case study that extends the application of\nLLM-based data annotation to enhance the quality of existing datasets through a\ncleansing strategy. Specifically, we leverage approaches such as\nchain-of-thought and majority voting to imitate human annotation and classify\nunrelated documents from the Multi-News dataset, which is widely used for the\nmulti-document summarization task. Through our proposed cleansing method, we\nintroduce an enhanced Multi-News+. By employing LLMs for data cleansing, we\ndemonstrate an efficient and effective approach to improving dataset quality\nwithout relying on expensive human annotation efforts.", "published": "2024-04-15 11:36:10", "link": "http://arxiv.org/abs/2404.09682v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantization of Large Language Models with an Overdetermined Basis", "abstract": "In this paper, we introduce an algorithm for data quantization based on the\nprinciples of Kashin representation. This approach hinges on decomposing any\ngiven vector, matrix, or tensor into two factors. The first factor maintains a\nsmall infinity norm, while the second exhibits a similarly constrained norm\nwhen multiplied by an orthogonal matrix. Surprisingly, the entries of factors\nafter decomposition are well-concentrated around several peaks, which allows us\nto efficiently replace them with corresponding centroids for quantization\npurposes. We study the theoretical properties of the proposed approach and\nrigorously evaluate our compression algorithm in the context of next-word\nprediction tasks and on a set of downstream tasks for text classification. Our\nfindings demonstrate that Kashin Quantization achieves competitive or superior\nquality in model performance while ensuring data compression, marking a\nsignificant advancement in the field of data quantization.", "published": "2024-04-15 12:38:46", "link": "http://arxiv.org/abs/2404.09737v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Personalized Collaborative Fine-Tuning for On-Device Large Language\n  Models", "abstract": "We explore on-device self-supervised collaborative fine-tuning of large\nlanguage models with limited local data availability. Taking inspiration from\nthe collaborative learning community, we introduce three distinct\ntrust-weighted gradient aggregation schemes: weight similarity-based,\nprediction similarity-based and validation performance-based. To minimize\ncommunication overhead, we integrate Low-Rank Adaptation (LoRA) and only\nexchange LoRA weight updates. Our protocols, driven by prediction and\nperformance metrics, surpass both FedAvg and local fine-tuning methods, which\nis particularly evident in realistic scenarios with more diverse local data\ndistributions. The results underscore the effectiveness of our approach in\naddressing heterogeneity and scarcity within local datasets.", "published": "2024-04-15 12:54:31", "link": "http://arxiv.org/abs/2404.09753v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KG-CTG: Citation Generation through Knowledge Graph-guided Large\n  Language Models", "abstract": "Citation Text Generation (CTG) is a task in natural language processing (NLP)\nthat aims to produce text that accurately cites or references a cited document\nwithin a source document. In CTG, the generated text draws upon contextual cues\nfrom both the source document and the cited paper, ensuring accurate and\nrelevant citation information is provided. Previous work in the field of\ncitation generation is mainly based on the text summarization of documents.\nFollowing this, this paper presents a framework, and a comparative study to\ndemonstrate the use of Large Language Models (LLMs) for the task of citation\ngeneration. Also, we have shown the improvement in the results of citation\ngeneration by incorporating the knowledge graph relations of the papers in the\nprompt for the LLM to better learn the relationship between the papers. To\nassess how well our model is performing, we have used a subset of standard\nS2ORC dataset, which only consists of computer science academic research papers\nin the English Language. Vicuna performs best for this task with 14.15 Meteor,\n12.88 Rouge-1, 1.52 Rouge-2, and 10.94 Rouge-L. Also, Alpaca performs best, and\nimproves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by\nincluding knowledge graphs.", "published": "2024-04-15 13:06:32", "link": "http://arxiv.org/abs/2404.09763v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Negation Triplet Extraction with Syntactic Dependency and Semantic\n  Consistency", "abstract": "Previous works of negation understanding mainly focus on negation cue\ndetection and scope resolution, without identifying negation subject which is\nalso significant to the downstream tasks. In this paper, we propose a new\nnegation triplet extraction (NTE) task which aims to extract negation subject\nalong with negation cue and scope. To achieve NTE, we devise a novel\nSyntax&Semantic-Enhanced Negation Extraction model, namely SSENE, which is\nbuilt based on a generative pretrained language model (PLM) {of Encoder-Decoder\narchitecture} with a multi-task learning framework. Specifically, the given\nsentence's syntactic dependency tree is incorporated into the PLM's encoder to\ndiscover the correlations between the negation subject, cue and scope.\nMoreover, the semantic consistency between the sentence and the extracted\ntriplet is ensured by an auxiliary task learning. Furthermore, we have\nconstructed a high-quality Chinese dataset NegComment based on the users'\nreviews from the real-world platform of Meituan, upon which our evaluations\nshow that SSENE achieves the best NTE performance compared to the baselines.\nOur ablation and case studies also demonstrate that incorporating the syntactic\ninformation helps the PLM's recognize the distant dependency between the\nsubject and cue, and the auxiliary task learning is helpful to extract the\nnegation triplets with more semantic consistency.", "published": "2024-04-15 14:28:33", "link": "http://arxiv.org/abs/2404.09830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Glitch Tokens in Large Language Models: Categorization Taxonomy and\n  Effective Detection", "abstract": "With the expanding application of Large Language Models (LLMs) in various\ndomains, it becomes imperative to comprehensively investigate their unforeseen\nbehaviors and consequent outcomes. In this study, we introduce and\nsystematically explore the phenomenon of \"glitch tokens\", which are anomalous\ntokens produced by established tokenizers and could potentially compromise the\nmodels' quality of response. Specifically, we experiment on seven top popular\nLLMs utilizing three distinct tokenizers and involving a totally of 182,517\ntokens. We present categorizations of the identified glitch tokens and symptoms\nexhibited by LLMs when interacting with glitch tokens. Based on our observation\nthat glitch tokens tend to cluster in the embedding space, we propose\nGlitchHunter, a novel iterative clustering-based technique, for efficient\nglitch token detection. The evaluation shows that our approach notably\noutperforms three baseline methods on eight open-source LLMs. To the best of\nour knowledge, we present the first comprehensive study on glitch tokens. Our\nnew detection further provides valuable insights into mitigating\ntokenization-related errors in LLMs.", "published": "2024-04-15 16:06:36", "link": "http://arxiv.org/abs/2404.09894v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Detecting AI Generated Text Based on NLP and Machine Learning Approaches", "abstract": "Recent advances in natural language processing (NLP) may enable artificial\nintelligence (AI) models to generate writing that is identical to human written\nform in the future. This might have profound ethical, legal, and social\nrepercussions. This study aims to address this problem by offering an accurate\nAI detector model that can differentiate between electronically produced text\nand human-written text. Our approach includes machine learning methods such as\nXGB Classifier, SVM, BERT architecture deep learning models. Furthermore, our\nresults show that the BERT performs better than previous models in identifying\ninformation generated by AI from information provided by humans. Provide a\ncomprehensive analysis of the current state of AI-generated text identification\nin our assessment of pertinent studies. Our testing yielded positive findings,\nshowing that our strategy is successful, with the BERT emerging as the most\nprobable answer. We analyze the research's societal implications, highlighting\nthe possible advantages for various industries while addressing sustainability\nissues pertaining to morality and the environment. The XGB classifier and SVM\ngive 0.84 and 0.81 accuracy in this article, respectively. The greatest\naccuracy in this research is provided by the BERT model, which provides 0.93%\naccuracy.", "published": "2024-04-15 16:37:44", "link": "http://arxiv.org/abs/2404.10032v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Chinchilla Scaling: A replication attempt", "abstract": "Hoffmann et al. (2022) propose three methods for estimating a compute-optimal\nscaling law. We attempt to replicate their third estimation procedure, which\ninvolves fitting a parametric loss function to a reconstruction of data from\ntheir plots. We find that the reported estimates are inconsistent with their\nfirst two estimation methods, fail at fitting the extracted data, and report\nimplausibly narrow confidence intervals--intervals this narrow would require\nover 600,000 experiments, while they likely only ran fewer than 500. In\ncontrast, our rederivation of the scaling law using the third approach yields\nresults that are compatible with the findings from the first two estimation\nprocedures described by Hoffmann et al.", "published": "2024-04-15 19:19:56", "link": "http://arxiv.org/abs/2404.10102v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Modeling Emotions and Ethics with Large Language Models", "abstract": "This paper explores the integration of human-like emotions and ethical\nconsiderations into Large Language Models (LLMs). We first model eight\nfundamental human emotions, presented as opposing pairs, and employ\ncollaborative LLMs to reinterpret and express these emotions across a spectrum\nof intensity. Our focus extends to embedding a latent ethical dimension within\nLLMs, guided by a novel self-supervised learning algorithm with human feedback\n(SSHF). This approach enables LLMs to perform self-evaluations and adjustments\nconcerning ethical guidelines, enhancing their capability to generate content\nthat is not only emotionally resonant but also ethically aligned. The\nmethodologies and case studies presented herein illustrate the potential of\nLLMs to transcend mere text and image generation, venturing into the realms of\nempathetic interaction and principled decision-making, thereby setting a new\nprecedent in the development of emotionally aware and ethically conscious AI\nsystems.", "published": "2024-04-15 05:30:26", "link": "http://arxiv.org/abs/2404.13071v2", "categories": ["cs.CL", "cs.AI", "I.2.0"], "primary_category": "cs.CL"}
{"title": "Towards Compositionally Generalizable Semantic Parsing in Large Language\n  Models: A Survey", "abstract": "Compositional generalization is the ability of a model to generalize to\ncomplex, previously unseen types of combinations of entities from just having\nseen the primitives. This type of generalization is particularly relevant to\nthe semantic parsing community for applications such as task-oriented dialogue,\ntext-to-SQL parsing, and information retrieval, as they can harbor infinite\ncomplexity. Despite the success of large language models (LLMs) in a wide range\nof NLP tasks, unlocking perfect compositional generalization still remains one\nof the few last unsolved frontiers. The past few years has seen a surge of\ninterest in works that explore the limitations of, methods to improve, and\nevaluation metrics for compositional generalization capabilities of LLMs for\nsemantic parsing tasks. In this work, we present a literature survey geared at\nsynthesizing recent advances in analysis, methods, and evaluation schemes to\noffer a starting point for both practitioners and researchers in this area.", "published": "2024-04-15 10:44:58", "link": "http://arxiv.org/abs/2404.13074v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM Evaluators Recognize and Favor Their Own Generations", "abstract": "Self-evaluation using large language models (LLMs) has proven valuable not\nonly in benchmarking but also methods like reward modeling, constitutional AI,\nand self-refinement. But new biases are introduced due to the same LLM acting\nas both the evaluator and the evaluatee. One such bias is self-preference,\nwhere an LLM evaluator scores its own outputs higher than others' while human\nannotators consider them of equal quality. But do LLMs actually recognize their\nown outputs when they give those texts higher scores, or is it just a\ncoincidence? In this paper, we investigate if self-recognition capability\ncontributes to self-preference. We discover that, out of the box, LLMs such as\nGPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from\nother LLMs and humans. By fine-tuning LLMs, we discover a linear correlation\nbetween self-recognition capability and the strength of self-preference bias;\nusing controlled experiments, we show that the causal explanation resists\nstraightforward confounders. We discuss how self-recognition can interfere with\nunbiased evaluations and AI safety more generally.", "published": "2024-04-15 16:49:59", "link": "http://arxiv.org/abs/2404.13076v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Evaluation of Speech Foundation Models", "abstract": "The foundation model paradigm leverages a shared foundation model to achieve\nstate-of-the-art (SOTA) performance for various tasks, requiring minimal\ndownstream-specific modeling and data annotation. This approach has proven\ncrucial in the field of Natural Language Processing (NLP). However, the speech\nprocessing community lacks a similar setup to explore the paradigm\nsystematically. In this work, we establish the Speech processing Universal\nPERformance Benchmark (SUPERB) to study the effectiveness of the paradigm for\nspeech. We propose a unified multi-tasking framework to address speech\nprocessing tasks in SUPERB using a frozen foundation model followed by\ntask-specialized, lightweight prediction heads. Combining our results with\ncommunity submissions, we verify that the foundation model paradigm is\npromising for speech, and our multi-tasking framework is simple yet effective,\nas the best-performing foundation model shows competitive generalizability\nacross most SUPERB tasks. For reproducibility and extensibility, we have\ndeveloped a long-term maintained platform that enables deterministic\nbenchmarking, allows for result sharing via an online leaderboard, and promotes\ncollaboration through a community-driven benchmark database to support new\ndevelopment cycles. Finally, we conduct a series of analyses to offer an\nin-depth understanding of SUPERB and speech foundation models, including\ninformation flows across tasks inside the models, the correctness of the\nweighted-sum benchmarking protocol and the statistical significance and\nrobustness of the benchmark.", "published": "2024-04-15 00:03:16", "link": "http://arxiv.org/abs/2404.09385v2", "categories": ["eess.AS", "cs.CL", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MMCode: Benchmarking Multimodal Large Language Models for Code\n  Generation with Visually Rich Programming Problems", "abstract": "Programming often involves converting detailed and complex specifications\ninto code, a process during which developers typically utilize visual aids to\nmore effectively convey concepts. While recent developments in Large Multimodal\nModels have demonstrated remarkable abilities in visual reasoning and\nmathematical tasks, there is little work on investigating whether these models\ncan effectively interpret visual elements for code generation. To this end, we\npresent MMCode, the first multi-modal coding dataset for evaluating algorithmic\nproblem-solving skills in visually rich contexts. MMCode contains 3,548\nquestions and 6,620 images collected from real-world programming challenges\nharvested from 10 code competition websites, presenting significant challenges\ndue to the extreme demand for reasoning abilities. Our experiment results show\nthat current state-of-the-art models struggle to solve these problems. The\nresults highlight the lack of powerful vision-code models, and we hope MMCode\ncan serve as an inspiration for future works in this domain. The data and code\nare publicly available at https://github.com/likaixin2000/MMCode.", "published": "2024-04-15 06:15:46", "link": "http://arxiv.org/abs/2404.09486v2", "categories": ["cs.CL", "cs.CV", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Prepacking: A Simple Method for Fast Prefilling and Increased Throughput\n  in Large Language Models", "abstract": "During inference for transformer-based large language models (LLM),\nprefilling is the computation of the key-value (KV) cache for input tokens in\nthe prompt prior to autoregressive generation. For longer input prompt lengths,\nprefilling will incur a significant overhead on decoding time. In this work, we\nhighlight the following pitfall of prefilling: for batches containing\nhigh-varying prompt lengths, significant computation is wasted by the standard\npractice of padding sequences to the maximum length. As LLMs increasingly\nsupport longer context lengths, potentially up to 10 million tokens, variations\nin prompt lengths within a batch become more pronounced. To address this, we\npropose Prepacking, a simple yet effective method to optimize prefilling\ncomputation. To avoid redundant computation on pad tokens, prepacking combines\nprompts of varying lengths into a sequence and packs multiple sequences into a\ncompact batch using a bin-packing algorithm. It then modifies the attention\nmask and positional encoding to compute multiple prefilled KV-caches for\nmultiple prompts within a single sequence. On standard curated dataset\ncontaining prompts with varying lengths, we obtain a significant speed and\nmemory efficiency improvements as compared to the default padding-based\nprefilling computation within Huggingface across a range of base model\nconfigurations and inference serving scenarios.", "published": "2024-04-15 07:49:10", "link": "http://arxiv.org/abs/2404.09529v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reliability Estimation of News Media Sources: Birds of a Feather Flock\n  Together", "abstract": "Evaluating the reliability of news sources is a routine task for journalists\nand organizations committed to acquiring and disseminating accurate\ninformation. Recent research has shown that predicting sources' reliability\nrepresents an important first-prior step in addressing additional challenges\nsuch as fake news detection and fact-checking. In this paper, we introduce a\nnovel approach for source reliability estimation that leverages reinforcement\nlearning strategies for estimating the reliability degree of news sources.\nContrary to previous research, our proposed approach models the problem as the\nestimation of a reliability degree, and not a reliability label, based on how\nall the news media sources interact with each other on the Web. We validated\nthe effectiveness of our method on a news media reliability dataset that is an\norder of magnitude larger than comparable existing datasets. Results show that\nthe estimated reliability degrees strongly correlates with journalists-provided\nscores (Spearman=0.80) and can effectively predict reliability labels\n(macro-avg. F$_1$ score=81.05). We release our implementation and dataset,\naiming to provide a valuable resource for the NLP community working on\ninformation verification.", "published": "2024-04-15 08:27:47", "link": "http://arxiv.org/abs/2404.09565v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Real-world Instance-specific Image Goal Navigation: Bridging Domain Gaps\n  via Contrastive Learning", "abstract": "Improving instance-specific image goal navigation (InstanceImageNav), which\nlocates the identical object in a real-world environment from a query image, is\nessential for robotic systems to assist users in finding desired objects. The\nchallenge lies in the domain gap between low-quality images observed by the\nmoving robot, characterized by motion blur and low-resolution, and high-quality\nquery images provided by the user. Such domain gaps could significantly reduce\nthe task success rate but have not been the focus of previous work. To address\nthis, we propose a novel method called Few-shot Cross-quality Instance-aware\nAdaptation (CrossIA), which employs contrastive learning with an instance\nclassifier to align features between massive low- and few high-quality images.\nThis approach effectively reduces the domain gap by bringing the latent\nrepresentations of cross-quality images closer on an instance basis.\nAdditionally, the system integrates an object image collection with a\npre-trained deblurring model to enhance the observed image quality. Our method\nfine-tunes the SimSiam model, pre-trained on ImageNet, using CrossIA. We\nevaluated our method's effectiveness through an InstanceImageNav task with 20\ndifferent types of instances, where the robot identifies the same instance in a\nreal-world environment as a high-quality query image. Our experiments showed\nthat our method improves the task success rate by up to three times compared to\nthe baseline, a conventional approach based on SuperGlue. These findings\nhighlight the potential of leveraging contrastive learning and image\nenhancement techniques to bridge the domain gap and improve object localization\nin robotic applications. The project website is\nhttps://emergentsystemlabstudent.github.io/DomainBridgingNav/.", "published": "2024-04-15 10:24:32", "link": "http://arxiv.org/abs/2404.09645v2", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration", "abstract": "The emergence of Large Multimodal Models (LMMs) marks a significant milestone\nin the development of artificial intelligence. Insurance, as a vast and complex\ndiscipline, involves a wide variety of data forms in its operational processes,\nincluding text, images, and videos, thereby giving rise to diverse multimodal\ntasks. Despite this, there has been limited systematic exploration of\nmultimodal tasks specific to insurance, nor a thorough investigation into how\nLMMs can address these challenges. In this paper, we explore GPT-4V's\ncapabilities in the insurance domain. We categorize multimodal tasks by\nfocusing primarily on visual aspects based on types of insurance (e.g., auto,\nhousehold/commercial property, health, and agricultural insurance) and\ninsurance stages (e.g., risk assessment, risk monitoring, and claims\nprocessing). Our experiment reveals that GPT-4V exhibits remarkable abilities\nin insurance-related tasks, demonstrating not only a robust understanding of\nmultimodal content in the insurance domain but also a comprehensive knowledge\nof insurance scenarios. However, there are notable shortcomings: GPT-4V\nstruggles with detailed risk rating and loss assessment, suffers from\nhallucination in image understanding, and shows variable support for different\nlanguages. Through this work, we aim to bridge the insurance domain with\ncutting-edge LMM technology, facilitate interdisciplinary exchange and\ndevelopment, and provide a foundation for the continued advancement and\nevolution of future research endeavors.", "published": "2024-04-15 11:45:30", "link": "http://arxiv.org/abs/2404.09690v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured\n  Compression for Large Language Models", "abstract": "Large language models (LLMs) show excellent performance in difficult tasks,\nbut they often require massive memories and computational resources. How to\nreduce the parameter scale of LLMs has become research hotspots. In this study,\nwe make an important observation that the multi-head self-attention (MHA)\nsub-layer of Transformer exhibits noticeable low-rank structure, while the\nfeed-forward network (FFN) sub-layer does not. With this regard, we design a\nmixed compression model, which organically combines Low-Rank matrix\napproximation And structured Pruning (LoRAP). For the MHA sub-layer, we propose\nan input activation weighted singular value decomposition method to strengthen\nthe low-rank characteristic. Furthermore, we discover that the weight matrices\nin MHA sub-layer have different low-rank degrees. Thus, a novel parameter\nallocation scheme according to the discrepancy of low-rank degrees is devised.\nFor the FFN sub-layer, we propose a gradient-free structured channel pruning\nmethod. During the pruning, we get an interesting finding that the least\nimportant 1% of parameter actually play a vital role in model performance.\nExtensive evaluations on zero-shot perplexity and zero-shot task classification\nindicate that our proposal is superior to previous structured compression\nrivals under multiple compression ratios.", "published": "2024-04-15 11:53:22", "link": "http://arxiv.org/abs/2404.09695v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Are Large Language Models Reliable Argument Quality Annotators?", "abstract": "Evaluating the quality of arguments is a crucial aspect of any system\nleveraging argument mining. However, it is a challenge to obtain reliable and\nconsistent annotations regarding argument quality, as this usually requires\ndomain-specific expertise of the annotators. Even among experts, the assessment\nof argument quality is often inconsistent due to the inherent subjectivity of\nthis task. In this paper, we study the potential of using state-of-the-art\nlarge language models (LLMs) as proxies for argument quality annotators. To\nassess the capability of LLMs in this regard, we analyze the agreement between\nmodel, human expert, and human novice annotators based on an established\ntaxonomy of argument quality dimensions. Our findings highlight that LLMs can\nproduce consistent annotations, with a moderately high agreement with human\nexperts across most of the quality dimensions. Moreover, we show that using\nLLMs as additional annotators can significantly improve the agreement between\nannotators. These results suggest that LLMs can serve as a valuable tool for\nautomated argument quality assessment, thus streamlining and accelerating the\nevaluation of large argument datasets.", "published": "2024-04-15 11:54:27", "link": "http://arxiv.org/abs/2404.09696v1", "categories": ["cs.CL", "cs.AI", "cs.ET"], "primary_category": "cs.CL"}
{"title": "Unveiling Imitation Learning: Exploring the Impact of Data Falsity to\n  Large Language Model", "abstract": "Many recent studies endeavor to improve open-source language models through\nimitation learning, and re-training on the synthetic instruction data from\nstate-of-the-art proprietary models like ChatGPT and GPT-4. However, the innate\nnature of synthetic data inherently contains noisy data, giving rise to a\nsubstantial presence of low-quality data replete with erroneous responses, and\nflawed reasoning. Although we intuitively grasp the potential harm of noisy\ndata, we lack a quantitative understanding of its impact. To this end, this\npaper explores the correlation between the degree of noise and its impact on\nlanguage models through instruction tuning. We first introduce the\nFalsity-Controllable (FACO) dataset, which comprises pairs of true answers with\ncorresponding reasoning, as well as false pairs to manually control the falsity\nratio of the dataset.Through our extensive experiments, we found multiple\nintriguing findings of the correlation between the factuality of the dataset\nand instruction tuning: Specifically, we verified falsity of the instruction is\nhighly relevant to various benchmark scores. Moreover, when LLMs are trained\nwith false instructions, they learn to lie and generate fake unfaithful\nanswers, even though they know the correct answer for the user request.\nAdditionally, we noted that once the language model is trained with a dataset\ncontaminated by noise, restoring its original performance is possible, but it\nfailed to reach full performance.", "published": "2024-04-15 12:20:09", "link": "http://arxiv.org/abs/2404.09717v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Anatomy of Industrial Scale Multilingual ASR", "abstract": "This paper describes AssemblyAI's industrial-scale automatic speech\nrecognition (ASR) system, designed to meet the requirements of large-scale,\nmultilingual ASR serving various application needs. Our system leverages a\ndiverse training dataset comprising unsupervised (12.5M hours), supervised\n(188k hours), and pseudo-labeled (1.6M hours) data across four languages. We\nprovide a detailed description of our model architecture, consisting of a\nfull-context 600M-parameter Conformer encoder pre-trained with BEST-RQ and an\nRNN-T decoder fine-tuned jointly with the encoder. Our extensive evaluation\ndemonstrates competitive word error rates (WERs) against larger and more\ncomputationally expensive models, such as Whisper large and Canary-1B.\nFurthermore, our architectural choices yield several key advantages, including\nan improved code-switching capability, a 5x inference speedup compared to an\noptimized Whisper baseline, a 30% reduction in hallucination rate on speech\ndata, and a 90% reduction in ambient noise compared to Whisper, along with\nsignificantly improved time-stamp accuracy. Throughout this work, we adopt a\nsystem-centric approach to analyzing various aspects of fully-fledged ASR\nmodels to gain practically relevant insights useful for real-world services\noperating at scale.", "published": "2024-04-15 14:48:43", "link": "http://arxiv.org/abs/2404.09841v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Software Engineering Methods For AI-Driven Deductive Legal Reasoning", "abstract": "The recent proliferation of generative artificial intelligence (AI)\ntechnologies such as pre-trained large language models (LLMs) has opened up new\nfrontiers in computational law. An exciting area of development is the use of\nAI to automate the deductive rule-based reasoning inherent in statutory and\ncontract law. This paper argues that such automated deductive legal reasoning\ncan now be viewed from the lens of software engineering, treating LLMs as\ninterpreters of natural-language programs with natural-language inputs. We show\nhow it is possible to apply principled software engineering techniques to\nenhance AI-driven legal reasoning of complex statutes and to unlock new\napplications in automated meta-reasoning such as mutation-guided example\ngeneration and metamorphic property-based testing.", "published": "2024-04-15 15:33:29", "link": "http://arxiv.org/abs/2404.09868v2", "categories": ["cs.SE", "cs.CL", "cs.CY"], "primary_category": "cs.SE"}
{"title": "Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table\n  Retrieval", "abstract": "Retrieving relevant tables containing the necessary information to accurately\nanswer a given question over tables is critical to open-domain\nquestion-answering (QA) systems. Previous methods assume the answer to such a\nquestion can be found either in a single table or multiple tables identified\nthrough question decomposition or rewriting. However, neither of these\napproaches is sufficient, as many questions require retrieving multiple tables\nand joining them through a join plan that cannot be discerned from the user\nquery itself. If the join plan is not considered in the retrieval stage, the\nsubsequent steps of reasoning and answering based on those retrieved tables are\nlikely to be incorrect. To address this problem, we introduce a method that\nuncovers useful join relations for any query and database during table\nretrieval. We use a novel re-ranking method formulated as a mixed-integer\nprogram that considers not only table-query relevance but also table-table\nrelevance that requires inferring join relationships. Our method outperforms\nthe state-of-the-art approaches for table retrieval by up to 9.3% in F1 score\nand for end-to-end QA by up to 5.4% in accuracy.", "published": "2024-04-15 15:55:01", "link": "http://arxiv.org/abs/2404.09889v3", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Progressive Knowledge Graph Completion", "abstract": "Knowledge Graph Completion (KGC) has emerged as a promising solution to\naddress the issue of incompleteness within Knowledge Graphs (KGs). Traditional\nKGC research primarily centers on triple classification and link prediction.\nNevertheless, we contend that these tasks do not align well with real-world\nscenarios and merely serve as surrogate benchmarks. In this paper, we\ninvestigate three crucial processes relevant to real-world construction\nscenarios: (a) the verification process, which arises from the necessity and\nlimitations of human verifiers; (b) the mining process, which identifies the\nmost promising candidates for verification; and (c) the training process, which\nharnesses verified data for subsequent utilization; in order to achieve a\ntransition toward more realistic challenges. By integrating these three\nprocesses, we introduce the Progressive Knowledge Graph Completion (PKGC) task,\nwhich simulates the gradual completion of KGs in real-world scenarios.\nFurthermore, to expedite PKGC processing, we propose two acceleration modules:\nOptimized Top-$k$ algorithm and Semantic Validity Filter. These modules\nsignificantly enhance the efficiency of the mining procedure. Our experiments\ndemonstrate that performance in link prediction does not accurately reflect\nperformance in PKGC. A more in-depth analysis reveals the key factors\ninfluencing the results and provides potential directions for future research.", "published": "2024-04-15 16:16:59", "link": "http://arxiv.org/abs/2404.09897v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Foundational Challenges in Assuring Alignment and Safety of Large\n  Language Models", "abstract": "This work identifies 18 foundational challenges in assuring the alignment and\nsafety of large language models (LLMs). These challenges are organized into\nthree different categories: scientific understanding of LLMs, development and\ndeployment methods, and sociotechnical challenges. Based on the identified\nchallenges, we pose $200+$ concrete research questions.", "published": "2024-04-15 16:58:28", "link": "http://arxiv.org/abs/2404.09932v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Tango 2: Aligning Diffusion-based Text-to-Audio Generations through\n  Direct Preference Optimization", "abstract": "Generative multimodal content is increasingly prevalent in much of the\ncontent creation arena, as it has the potential to allow artists and media\npersonnel to create pre-production mockups by quickly bringing their ideas to\nlife. The generation of audio from text prompts is an important aspect of such\nprocesses in the music and film industry. Many of the recent diffusion-based\ntext-to-audio models focus on training increasingly sophisticated diffusion\nmodels on a large set of datasets of prompt-audio pairs. These models do not\nexplicitly focus on the presence of concepts or events and their temporal\nordering in the output audio with respect to the input prompt. Our hypothesis\nis focusing on how these aspects of audio generation could improve audio\ngeneration performance in the presence of limited data. As such, in this work,\nusing an existing text-to-audio model Tango, we synthetically create a\npreference dataset where each prompt has a winner audio output and some loser\naudio outputs for the diffusion model to learn from. The loser outputs, in\ntheory, have some concepts from the prompt missing or in an incorrect order. We\nfine-tune the publicly available Tango text-to-audio model using diffusion-DPO\n(direct preference optimization) loss on our preference dataset and show that\nit leads to improved audio output over Tango and AudioLDM2, in terms of both\nautomatic- and manual-evaluation metrics.", "published": "2024-04-15 17:31:22", "link": "http://arxiv.org/abs/2404.09956v4", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Context Does Matter: Implications for Crowdsourced Evaluation Labels in\n  Task-Oriented Dialogue Systems", "abstract": "Crowdsourced labels play a crucial role in evaluating task-oriented dialogue\nsystems (TDSs). Obtaining high-quality and consistent ground-truth labels from\nannotators presents challenges. When evaluating a TDS, annotators must fully\ncomprehend the dialogue before providing judgments. Previous studies suggest\nusing only a portion of the dialogue context in the annotation process.\nHowever, the impact of this limitation on label quality remains unexplored.\nThis study investigates the influence of dialogue context on annotation\nquality, considering the truncated context for relevance and usefulness\nlabeling. We further propose to use large language models (LLMs) to summarize\nthe dialogue context to provide a rich and short description of the dialogue\ncontext and study the impact of doing so on the annotator's performance.\nReducing context leads to more positive ratings. Conversely, providing the\nentire dialogue context yields higher-quality relevance ratings but introduces\nambiguity in usefulness ratings. Using the first user utterance as context\nleads to consistent ratings, akin to those obtained using the entire dialogue,\nwith significantly reduced annotation effort. Our findings show how task\ndesign, particularly the availability of dialogue context, affects the quality\nand consistency of crowdsourced evaluation labels.", "published": "2024-04-15 17:56:39", "link": "http://arxiv.org/abs/2404.09980v1", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MMInA: Benchmarking Multihop Multimodal Internet Agents", "abstract": "Autonomous embodied agents live on an Internet of multimedia websites. Can\nthey hop around multimodal websites to complete complex user tasks? Existing\nbenchmarks fail to assess them in a realistic, evolving environment for their\nembodiment across websites. To answer this question, we present MMInA, a\nmultihop and multimodal benchmark to evaluate the embodied agents for\ncompositional Internet tasks, with several appealing properties: 1) Evolving\nreal-world multimodal websites. Our benchmark uniquely operates on evolving\nreal-world websites, ensuring a high degree of realism and applicability to\nnatural user tasks. Our data includes 1,050 human-written tasks covering\nvarious domains such as shopping and travel, with each task requiring the agent\nto autonomously extract multimodal information from web pages as observations;\n2) Multihop web browsing. Our dataset features naturally compositional tasks\nthat require information from or actions on multiple websites to solve, to\nassess long-range reasoning capabilities on web tasks; 3) Holistic evaluation.\nWe propose a novel protocol for evaluating an agent's progress in completing\nmultihop tasks. We experiment with both standalone (multimodal) language models\nand heuristic-based web agents. Extensive experiments demonstrate that while\nlong-chain multihop web tasks are easy for humans, they remain challenging for\nstate-of-the-art web agents. We identify that agents are more likely to fail on\nthe early hops when solving tasks of more hops, which results in lower task\nsuccess rates. To address this issue, we propose a simple memory augmentation\napproach replaying past action trajectories to reflect. Our method\nsignificantly improved both the single-hop and multihop web browsing abilities\nof agents. See our code and data at https://mmina.cliangyu.com", "published": "2024-04-15 17:59:50", "link": "http://arxiv.org/abs/2404.09992v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AIGeN: An Adversarial Approach for Instruction Generation in VLN", "abstract": "In the last few years, the research interest in Vision-and-Language\nNavigation (VLN) has grown significantly. VLN is a challenging task that\ninvolves an agent following human instructions and navigating in a previously\nunknown environment to reach a specified goal. Recent work in literature\nfocuses on different ways to augment the available datasets of instructions for\nimproving navigation performance by exploiting synthetic training data. In this\nwork, we propose AIGeN, a novel architecture inspired by Generative Adversarial\nNetworks (GANs) that produces meaningful and well-formed synthetic instructions\nto improve navigation agents' performance. The model is composed of a\nTransformer decoder (GPT-2) and a Transformer encoder (BERT). During the\ntraining phase, the decoder generates sentences for a sequence of images\ndescribing the agent's path to a particular point while the encoder\ndiscriminates between real and fake instructions. Experimentally, we evaluate\nthe quality of the generated instructions and perform extensive ablation\nstudies. Additionally, we generate synthetic instructions for 217K trajectories\nusing AIGeN on Habitat-Matterport 3D Dataset (HM3D) and show an improvement in\nthe performance of an off-the-shelf VLN method. The validation analysis of our\nproposal is conducted on REVERIE and R2R and highlights the promising aspects\nof our proposal, achieving state-of-the-art performance.", "published": "2024-04-15 18:00:30", "link": "http://arxiv.org/abs/2404.10054v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "PRODIS -- a speech database and a phoneme-based language model for the\n  study of predictability effects in Polish", "abstract": "We present a speech database and a phoneme-level language model of Polish.\nThe database and model are designed for the analysis of prosodic and discourse\nfactors and their impact on acoustic parameters in interaction with\npredictability effects. The database is also the first large, publicly\navailable Polish speech corpus of excellent acoustic quality that can be used\nfor phonetic analysis and training of multi-speaker speech technology systems.\nThe speech in the database is processed in a pipeline that achieves a 90%\ndegree of automation. It incorporates state-of-the-art, freely available tools\nenabling database expansion or adaptation to additional languages.", "published": "2024-04-15 20:03:58", "link": "http://arxiv.org/abs/2404.10112v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language Model Cascades: Token-level uncertainty and beyond", "abstract": "Recent advances in language models (LMs) have led to significant improvements\nin quality on complex NLP tasks, but at the expense of increased inference\ncosts. Cascading offers a simple strategy to achieve more favorable\ncost-quality tradeoffs: here, a small model is invoked for most \"easy\"\ninstances, while a few \"hard\" instances are deferred to the large model. While\nthe principles underpinning cascading are well-studied for classification tasks\n- with deferral based on predicted class uncertainty favored theoretically and\npractically - a similar understanding is lacking for generative LM tasks. In\nthis work, we initiate a systematic study of deferral rules for LM cascades. We\nbegin by examining the natural extension of predicted class uncertainty to\ngenerative LM tasks, namely, the predicted sequence uncertainty. We show that\nthis measure suffers from the length bias problem, either over- or\nunder-emphasizing outputs based on their lengths. This is because LMs produce a\nsequence of uncertainty values, one for each output token; and moreover, the\nnumber of output tokens is variable across examples. To mitigate this issue, we\npropose to exploit the richer token-level uncertainty information implicit in\ngenerative LMs. We argue that naive predicted sequence uncertainty corresponds\nto a simple aggregation of these uncertainties. By contrast, we show that\nincorporating token-level uncertainty through learned post-hoc deferral rules\ncan significantly outperform such simple aggregation strategies, via\nexperiments on a range of natural language benchmarks with FLAN-T5 models. We\nfurther show that incorporating embeddings from the smaller model and\nintermediate layers of the larger model can give an additional boost in the\noverall cost-quality tradeoff.", "published": "2024-04-15 21:02:48", "link": "http://arxiv.org/abs/2404.10136v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ANCHOR: LLM-driven News Subject Conditioning for Text-to-Image Synthesis", "abstract": "Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing\nsynthesized image quality, but current datasets evaluate model performance only\non descriptive, instruction-based prompts. Real-world news image captions take\na more pragmatic approach, providing high-level situational and Named-Entity\n(NE) information and limited physical object descriptions, making them\nabstractive. To evaluate the ability of T2I models to capture intended subjects\nfrom news captions, we introduce the Abstractive News Captions with High-level\ncOntext Representation (ANCHOR) dataset, containing 70K+ samples sourced from 5\ndifferent news media organizations. With Large Language Models (LLM) achieving\nsuccess in language and commonsense reasoning tasks, we explore the ability of\ndifferent LLMs to identify and understand key subjects from abstractive\ncaptions. Our proposed method Subject-Aware Finetuning (SAFE), selects and\nenhances the representation of key subjects in synthesized images by leveraging\nLLM-generated subject weights. It also adapts to the domain distribution of\nnews images and captions through custom Domain Fine-tuning, outperforming\ncurrent T2I baselines on ANCHOR. By launching the ANCHOR dataset, we hope to\nmotivate research in furthering the Natural Language Understanding (NLU)\ncapabilities of T2I models.", "published": "2024-04-15 21:19:10", "link": "http://arxiv.org/abs/2404.10141v1", "categories": ["cs.CV", "cs.CL", "cs.MM", "65D19"], "primary_category": "cs.CV"}
{"title": "TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table\n  Decomposition", "abstract": "Table reasoning is a challenging task that requires understanding both\nnatural language questions and structured tabular data. Large language models\n(LLMs) have shown impressive capabilities in natural language understanding and\ngeneration, but they often struggle with large tables due to their limited\ninput length. In this paper, we propose TabSQLify, a novel method that\nleverages text-to-SQL generation to decompose tables into smaller and relevant\nsub-tables, containing only essential information for answering questions or\nverifying statements, before performing the reasoning task. In our\ncomprehensive evaluation on four challenging datasets, our approach\ndemonstrates comparable or superior performance compared to prevailing methods\nreliant on full tables as input. Moreover, our method can reduce the input\ncontext length significantly, making it more scalable and efficient for\nlarge-scale table reasoning applications. Our method performs remarkably well\non the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the\nTabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass\nother LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can\nreduce the table size significantly alleviating the computational load on LLMs\nwhen handling large tables without compromising performance.", "published": "2024-04-15 21:42:20", "link": "http://arxiv.org/abs/2404.10150v1", "categories": ["cs.CL", "cs.DB", "cs.IR"], "primary_category": "cs.CL"}
{"title": "State Space Model for New-Generation Network Alternative to\n  Transformers: A Survey", "abstract": "In the post-deep learning era, the Transformer architecture has demonstrated\nits powerful performance across pre-trained big models and various downstream\ntasks. However, the enormous computational demands of this architecture have\ndeterred many researchers. To further reduce the complexity of attention\nmodels, numerous efforts have been made to design more efficient methods. Among\nthem, the State Space Model (SSM), as a possible replacement for the\nself-attention based Transformer model, has drawn more and more attention in\nrecent years. In this paper, we give the first comprehensive review of these\nworks and also provide experimental comparisons and analysis to better\ndemonstrate the features and advantages of SSM. Specifically, we first give a\ndetailed description of principles to help the readers quickly capture the key\nideas of SSM. After that, we dive into the reviews of existing SSMs and their\nvarious applications, including natural language processing, computer vision,\ngraph, multi-modal and multi-media, point cloud/event stream, time series data,\nand other domains. In addition, we give statistical comparisons and analysis of\nthese models and hope it helps the readers to understand the effectiveness of\ndifferent structures on various tasks. Then, we propose possible research\npoints in this direction to better promote the development of the theoretical\nmodel and application of SSM. More related works will be continuously updated\non the following GitHub:\nhttps://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.", "published": "2024-04-15 07:24:45", "link": "http://arxiv.org/abs/2404.09516v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Compression Represents Intelligence Linearly", "abstract": "There is a belief that learning to compress well will lead to intelligence.\nRecently, language modeling has been shown to be equivalent to compression,\nwhich offers a compelling rationale for the success of large language models\n(LLMs): the development of more advanced language models is essentially\nenhancing compression which facilitates intelligence. Despite such appealing\ndiscussions, little empirical evidence is present for the interplay between\ncompression and intelligence. In this work, we examine their relationship in\nthe context of LLMs, treating LLMs as data compressors. Given the abstract\nconcept of \"intelligence\", we adopt the average downstream benchmark scores as\na surrogate, specifically targeting intelligence related to knowledge and\ncommonsense, coding, and mathematical reasoning. Across 12 benchmarks, our\nstudy brings together 31 public LLMs that originate from diverse organizations.\nRemarkably, we find that LLMs' intelligence -- reflected by average benchmark\nscores -- almost linearly correlates with their ability to compress external\ntext corpora. These results provide concrete evidence supporting the belief\nthat superior compression indicates greater intelligence. Furthermore, our\nfindings suggest that compression efficiency, as an unsupervised metric derived\nfrom raw text corpora, serves as a reliable evaluation measure that is linearly\nassociated with the model capabilities. We open-source our compression datasets\nas well as our data collection pipelines to facilitate future researchers to\nassess compression properly.", "published": "2024-04-15 17:03:41", "link": "http://arxiv.org/abs/2404.09937v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Deferred NAM: Low-latency Top-K Context Injection via Deferred Context\n  Encoding for Non-Streaming ASR", "abstract": "Contextual biasing enables speech recognizers to transcribe important phrases\nin the speaker's context, such as contact names, even if they are rare in, or\nabsent from, the training data. Attention-based biasing is a leading approach\nwhich allows for full end-to-end cotraining of the recognizer and biasing\nsystem and requires no separate inference-time components. Such biasers\ntypically consist of a context encoder; followed by a context filter which\nnarrows down the context to apply, improving per-step inference time; and,\nfinally, context application via cross attention. Though much work has gone\ninto optimizing per-frame performance, the context encoder is at least as\nimportant: recognition cannot begin before context encoding ends. Here, we show\nthe lightweight phrase selection pass can be moved before context encoding,\nresulting in a speedup of up to 16.1 times and enabling biasing to scale to 20K\nphrases with a maximum pre-decoding delay under 33ms. With the addition of\nphrase- and wordpiece-level cross-entropy losses, our technique also achieves\nup to a 37.5% relative WER reduction over the baseline without the losses and\nlightweight phrase selection pass.", "published": "2024-04-15 23:28:13", "link": "http://arxiv.org/abs/2404.10180v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Analyzing phonetic structure of Mandarin using Audacity", "abstract": "Mandarin Chinese is the official language in China, Taiwan, and Singapore. It\nis also the main non-official language spoken predominantly at home in Toronto\nand Vancouver. This article employs the audio software Audacity and leverages\ntheoretical knowledge to conduct a comprehensive analysis of Mandarin Chinese.\nThe study initiates with an overview of the fundamental principles underlying\nMandarin pronunciation, aiming to provide insights into its phonetic structure.", "published": "2024-04-15 03:16:20", "link": "http://arxiv.org/abs/2406.09426v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scoring Time Intervals using Non-Hierarchical Transformer For Automatic\n  Piano Transcription", "abstract": "The neural semi-Markov Conditional Random Field (semi-CRF) framework has\ndemonstrated promise for event-based piano transcription. In this framework,\nall events (notes or pedals) are represented as closed time intervals tied to\nspecific event types. The neural semi-CRF approach requires an interval scoring\nmatrix that assigns a score for every candidate interval. However, designing an\nefficient and expressive architecture for scoring intervals is not trivial.\nThis paper introduces a simple method for scoring intervals using scaled inner\nproduct operations that resemble how attention scoring is done in transformers.\nWe show theoretically that, due to the special structure from encoding the\nnon-overlapping intervals, under a mild condition, the inner product operations\nare expressive enough to represent an ideal scoring matrix that can yield the\ncorrect transcription result. We then demonstrate that an encoder-only\nstructured non-hierarchical transformer backbone, operating only on a\nlow-time-resolution feature map, is capable of transcribing piano notes and\npedals with high accuracy and time precision. The experiment shows that our\napproach achieves the new state-of-the-art performance across all subtasks in\nterms of the F1 measure on the Maestro dataset.", "published": "2024-04-15 05:35:09", "link": "http://arxiv.org/abs/2404.09466v6", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
