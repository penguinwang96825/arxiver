{"title": "Comparative Analysis of N-gram Text Representation on Igbo Text Document\n  Similarity", "abstract": "The improvement in Information Technology has encouraged the use of Igbo in\nthe creation of text such as resources and news articles online. Text\nsimilarity is of great importance in any text-based applications. This paper\npresents a comparative analysis of n-gram text representation on Igbo text\ndocument similarity. It adopted Euclidean similarity measure to determine the\nsimilarities between Igbo text documents represented with two word-based n-gram\ntext representation (unigram and bigram) models. The evaluation of the\nsimilarity measure is based on the adopted text representation models. The\nmodel is designed with Object-Oriented Methodology and implemented with Python\nprogramming language with tools from Natural Language Toolkits (NLTK). The\nresult shows that unigram represented text has highest distance values whereas\nbigram has the lowest corresponding distance values. The lower the distance\nvalue, the more similar the two documents and better the quality of the model\nwhen used for a task that requires similarity measure. The similarity of two\ndocuments increases as the distance value moves down to zero (0). Ideally, the\nresult analyzed revealed that Igbo text document similarity measured on bigram\nrepresented text gives accurate similarity result. This will give better,\neffective and accurate result when used for tasks such as text classification,\nclustering and ranking on Igbo text.", "published": "2020-04-01 12:24:47", "link": "http://arxiv.org/abs/2004.00375v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Transfer Learning for Punctuation Restoration", "abstract": "Previous studies demonstrate that word embeddings and part-of-speech (POS)\ntags are helpful for punctuation restoration tasks. However, two drawbacks\nstill exist. One is that word embeddings are pre-trained by unidirectional\nlanguage modeling objectives. Thus the word embeddings only contain\nleft-to-right context information. The other is that POS tags are provided by\nan external POS tagger. So computation cost will be increased and incorrect\npredicted tags may affect the performance of restoring punctuation marks during\ndecoding. This paper proposes adversarial transfer learning to address these\nproblems. A pre-trained bidirectional encoder representations from transformers\n(BERT) model is used to initialize a punctuation model. Thus the transferred\nmodel parameters carry both left-to-right and right-to-left representations.\nFurthermore, adversarial multi-task learning is introduced to learn task\ninvariant knowledge for punctuation prediction. We use an extra POS tagging\ntask to help the training of the punctuation predicting task. Adversarial\ntraining is utilized to prevent the shared parameters from containing task\nspecific information. We only use the punctuation predicting task to restore\nmarks during decoding stage. Therefore, it will not need extra computation and\nnot introduce incorrect tags from the POS tagger. Experiments are conducted on\nIWSLT2011 datasets. The results demonstrate that the punctuation predicting\nmodels obtain further performance improvement with task invariant knowledge\nfrom the POS tagging task. Our best model outperforms the previous\nstate-of-the-art model trained only with lexical features by up to 9.2%\nabsolute overall F_1-score on test set.", "published": "2020-04-01 06:19:56", "link": "http://arxiv.org/abs/2004.00248v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "More Grounded Image Captioning by Distilling Image-Text Matching Model", "abstract": "Visual attention not only improves the performance of image captioners, but\nalso serves as a visual interpretation to qualitatively measure the caption\nrationality and model transparency. Specifically, we expect that a captioner\ncan fix its attentive gaze on the correct objects while generating the\ncorresponding words. This ability is also known as grounded image captioning.\nHowever, the grounding accuracy of existing captioners is far from\nsatisfactory. To improve the grounding accuracy while retaining the captioning\nquality, it is expensive to collect the word-region alignment as strong\nsupervision. To this end, we propose a Part-of-Speech (POS) enhanced image-text\nmatching model (SCAN \\cite{lee2018stacked}): POS-SCAN, as the effective\nknowledge distillation for more grounded image captioning. The benefits are\ntwo-fold: 1) given a sentence and an image, POS-SCAN can ground the objects\nmore accurately than SCAN; 2) POS-SCAN serves as a word-region alignment\nregularization for the captioner's visual attention module. By showing\nbenchmark experimental results, we demonstrate that conventional image\ncaptioners equipped with POS-SCAN can significantly improve the grounding\naccuracy without strong supervision. Last but not the least, we explore the\nindispensable Self-Critical Sequence Training (SCST) \\cite{Rennie_2017_CVPR} in\nthe context of grounded image captioning and show that the image-text matching\nscore can serve as a reward for more grounded captioning\n\\footnote{https://github.com/YuanEZhou/Grounded-Image-Captioning}.", "published": "2020-04-01 12:42:06", "link": "http://arxiv.org/abs/2004.00390v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deep Entity Matching with Pre-Trained Language Models", "abstract": "We present Ditto, a novel entity matching system based on pre-trained\nTransformer-based language models. We fine-tune and cast EM as a sequence-pair\nclassification problem to leverage such models with a simple architecture. Our\nexperiments show that a straightforward application of language models such as\nBERT, DistilBERT, or RoBERTa pre-trained on large text corpora already\nsignificantly improves the matching quality and outperforms previous\nstate-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We\nalso developed three optimization techniques to further improve Ditto's\nmatching capability. Ditto allows domain knowledge to be injected by\nhighlighting important pieces of input information that may be of interest when\nmaking matching decisions. Ditto also summarizes strings that are too long so\nthat only the essential information is retained and used for EM. Finally, Ditto\nadapts a SOTA technique on data augmentation for text to EM to augment the\ntraining data with (difficult) examples. This way, Ditto is forced to learn\n\"harder\" to improve the model's matching capability. The optimizations we\ndeveloped further boost the performance of Ditto by up to 9.8%. Perhaps more\nsurprisingly, we establish that Ditto can achieve the previous SOTA results\nwith at most half the number of labeled data. Finally, we demonstrate Ditto's\neffectiveness on a real-world large-scale EM task. On matching two company\ndatasets consisting of 789K and 412K records, Ditto achieves a high F1 score of\n96.5%.", "published": "2020-04-01 17:14:10", "link": "http://arxiv.org/abs/2004.00584v3", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Igbo-English Machine Translation: An Evaluation Benchmark", "abstract": "Although researchers and practitioners are pushing the boundaries and\nenhancing the capacities of NLP tools and methods, works on African languages\nare lagging. A lot of focus on well resourced languages such as English,\nJapanese, German, French, Russian, Mandarin Chinese etc. Over 97% of the\nworld's 7000 languages, including African languages, are low resourced for NLP\ni.e. they have little or no data, tools, and techniques for NLP research. For\ninstance, only 5 out of 2965, 0.19% authors of full text papers in the ACL\nAnthology extracted from the 5 major conferences in 2018 ACL, NAACL, EMNLP,\nCOLING and CoNLL, are affiliated to African institutions. In this work, we\ndiscuss our effort toward building a standard machine translation benchmark\ndataset for Igbo, one of the 3 major Nigerian languages. Igbo is spoken by more\nthan 50 million people globally with over 50% of the speakers are in\nsoutheastern Nigeria. Igbo is low resourced although there have been some\nefforts toward developing IgboNLP such as part of speech tagging and diacritic\nrestoration", "published": "2020-04-01 18:06:21", "link": "http://arxiv.org/abs/2004.00648v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style\n  Word Generator", "abstract": "Audio Visual Scene-aware Dialog (AVSD) is the task of generating a response\nfor a question with a given scene, video, audio, and the history of previous\nturns in the dialog. Existing systems for this task employ the transformers or\nrecurrent neural network-based architecture with the encoder-decoder framework.\nEven though these techniques show superior performance for this task, they have\nsignificant limitations: the model easily overfits only to memorize the\ngrammatical patterns; the model follows the prior distribution of the\nvocabularies in a dataset. To alleviate the problems, we propose a Multimodal\nSemantic Transformer Network. It employs a transformer-based architecture with\nan attention-based word embedding layer that generates words by querying word\nembeddings. With this design, our model keeps considering the meaning of the\nwords at the generation stage. The empirical results demonstrate the\nsuperiority of our proposed model that outperforms most of the previous works\nfor the AVSD task.", "published": "2020-04-01 07:10:08", "link": "http://arxiv.org/abs/2004.08299v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved RawNet with Feature Map Scaling for Text-independent Speaker\n  Verification using Raw Waveforms", "abstract": "Recent advances in deep learning have facilitated the design of speaker\nverification systems that directly input raw waveforms. For example, RawNet\nextracts speaker embeddings from raw waveforms, which simplifies the process\npipeline and demonstrates competitive performance. In this study, we improve\nRawNet by scaling feature maps using various methods. The proposed mechanism\nutilizes a scale vector that adopts a sigmoid non-linear function. It refers to\na vector with dimensionality equal to the number of filters in a given feature\nmap. Using a scale vector, we propose to scale the feature map\nmultiplicatively, additively, or both. In addition, we investigate replacing\nthe first convolution layer with the sinc-convolution layer of SincNet.\nExperiments performed on the VoxCeleb1 evaluation dataset demonstrate the\neffectiveness of the proposed methods, and the best performing system reduces\nthe equal error rate by half compared to the original RawNet. Expanded\nevaluation results obtained using the VoxCeleb1-E and VoxCeleb-H protocols\nmarginally outperform existing state-of-the-art systems.", "published": "2020-04-01 15:51:56", "link": "http://arxiv.org/abs/2004.00526v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Better Sign Language Translation with STMC-Transformer", "abstract": "Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR)\nsystem to extract sign language glosses from videos. Then, a translation system\ngenerates spoken language translations from the sign language glosses. This\npaper focuses on the translation system and introduces the STMC-Transformer\nwhich improves on the current state-of-the-art by over 5 and 7 BLEU\nrespectively on gloss-to-text and video-to-text translation of the\nPHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase\nof over 16 BLEU.\n  We also demonstrate the problem in current methods that rely on gloss\nsupervision. The video-to-text translation of our STMC-Transformer outperforms\ntranslation of GT glosses. This contradicts previous claims that GT gloss\ntranslation acts as an upper bound for SLT performance and reveals that glosses\nare an inefficient representation of sign language. For future SLT research, we\ntherefore suggest an end-to-end training of the recognition and translation\nmodels, or using a different sign language annotation scheme.", "published": "2020-04-01 17:20:04", "link": "http://arxiv.org/abs/2004.00588v2", "categories": ["cs.CL", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved Source Counting and Separation for Monaural Mixture", "abstract": "Single-channel speech separation in time domain and frequency domain has been\nwidely studied for voice-driven applications over the past few years. Most of\nprevious works assume known number of speakers in advance, however, which is\nnot easily accessible through monaural mixture in practice. In this paper, we\npropose a novel model of single-channel multi-speaker separation by jointly\nlearning the time-frequency feature and the unknown number of speakers.\nSpecifically, our model integrates the time-domain convolution encoded feature\nmap and the frequency-domain spectrogram by attention mechanism, and the\nintegrated features are projected into high-dimensional embedding vectors which\nare then clustered with deep attractor network to modify the encoded feature.\nMeanwhile, the number of speakers is counted by computing the Gerschgorin disks\nof the embedding vectors which are orthogonal for different speakers. Finally,\nthe modified encoded feature is inverted to the sound waveform using a linear\ndecoder. Experimental evaluation on the GRID dataset shows that the proposed\nmethod with a single model can accurately estimate the number of speakers with\n96.7 % probability of success, while achieving the state-of-the-art separation\nresults on multi-speaker mixtures in terms of scale-invariant signal-to-noise\nratio improvement (SI-SNRi) and signal-to-distortion ratio improvement (SDRi).", "published": "2020-04-01 00:19:41", "link": "http://arxiv.org/abs/2004.00175v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On The Differences Between Song and Speech Emotion Recognition: Effect\n  of Feature Sets, Feature Types, and Classifiers", "abstract": "In this paper, we evaluate the different features sets, feature types, and\nclassifiers on both song and speech emotion recognition. Three feature sets:\nGeMAPS, pyAudioAnalysis, and LibROSA; two feature types: low-level descriptors\nand high-level statistical functions; and four classifiers: multilayer\nperceptron, LSTM, GRU, and convolution neural networks are examined on both\nsong and speech data with the same parameter values. The results show no\nremarkable difference between song and speech data using the same method. In\naddition, high-level statistical functions of acoustic features gained higher\nperformance scores than low-level descriptors in this classification task. This\nresult strengthens the previous finding on the regression task which reported\nthe advantage use of high-level features.", "published": "2020-04-01 02:16:38", "link": "http://arxiv.org/abs/2004.00200v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Can Machine Learning Be Used to Recognize and Diagnose Coughs?", "abstract": "Emerging wireless technologies, such as 5G and beyond, are bringing new use\ncases to the forefront, one of the most prominent being machine learning\nempowered health care. One of the notable modern medical concerns that impose\nan immense worldwide health burden are respiratory infections. Since cough is\nan essential symptom of many respiratory infections, an automated system to\nscreen for respiratory diseases based on raw cough data would have a multitude\nof beneficial research and medical applications. In literature, machine\nlearning has already been successfully used to detect cough events in\ncontrolled environments. In this paper, we present a low complexity, automated\nrecognition and diagnostic tool for screening respiratory infections that\nutilizes Convolutional Neural Networks (CNNs) to detect cough within\nenvironment audio and diagnose three potential illnesses (i.e., bronchitis,\nbronchiolitis and pertussis) based on their unique cough audio features. Both\nproposed detection and diagnosis models achieve an accuracy of over 89%, while\nalso remaining computationally efficient. Results show that the proposed system\nis successfully able to detect and separate cough events from background noise.\nMoreover, the proposed single diagnosis model is capable of distinguishing\nbetween different illnesses without the need of separate models.", "published": "2020-04-01 20:14:36", "link": "http://arxiv.org/abs/2004.01495v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Towards democratizing music production with AI-Design of Variational\n  Autoencoder-based Rhythm Generator as a DAW plugin", "abstract": "There has been significant progress in the music generation technique\nutilizing deep learning. However, it is still hard for musicians and artists to\nuse these techniques in their daily music-making practice. This paper proposes\na Variational Autoencoder\\cite{Kingma2014}(VAE)-based rhythm generation system,\nin which musicians can train a deep learning model only by selecting target\nMIDI files, then generate various rhythms with the model. The author has\nimplemented the system as a plugin software for a DAW (Digital Audio\nWorkstation), namely a Max for Live device for Ableton Live. Selected\nprofessional/semi-professional musicians and music producers have used the\nplugin, and they proved that the plugin is a useful tool for making music\ncreatively. The plugin, source code, and demo videos are available online.", "published": "2020-04-01 10:50:14", "link": "http://arxiv.org/abs/2004.01525v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
