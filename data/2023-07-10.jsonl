{"title": "HistRED: A Historical Document-Level Relation Extraction Dataset", "abstract": "Despite the extensive applications of relation extraction (RE) tasks in\nvarious domains, little has been explored in the historical context, which\ncontains promising data across hundreds and thousands of years. To promote the\nhistorical RE research, we present HistRED constructed from Yeonhaengnok.\nYeonhaengnok is a collection of records originally written in Hanja, the\nclassical Chinese writing, which has later been translated into Korean. HistRED\nprovides bilingual annotations such that RE can be performed on Korean and\nHanja texts. In addition, HistRED supports various self-contained subtexts with\ndifferent lengths, from a sentence level to a document level, supporting\ndiverse context settings for researchers to evaluate the robustness of their RE\nmodels. To demonstrate the usefulness of our dataset, we propose a bilingual RE\nmodel that leverages both Korean and Hanja contexts to predict relations\nbetween entities. Our model outperforms monolingual baselines on HistRED,\nshowing that employing multiple language contexts supplements the RE\npredictions. The dataset is publicly available at:\nhttps://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.", "published": "2023-07-10 00:24:27", "link": "http://arxiv.org/abs/2307.04285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Cross-lingual Transfer via Phonemic Transcription Integration", "abstract": "Previous cross-lingual transfer methods are restricted to orthographic\nrepresentation learning via textual scripts. This limitation hampers\ncross-lingual transfer and is biased towards languages sharing similar\nwell-known scripts. To alleviate the gap between languages from different\nwriting scripts, we propose PhoneXL, a framework incorporating phonemic\ntranscriptions as an additional linguistic modality beyond the traditional\northographic transcriptions for cross-lingual transfer. Particularly, we\npropose unsupervised alignment objectives to capture (1) local one-to-one\nalignment between the two different modalities, (2) alignment via\nmulti-modality contexts to leverage information from additional modalities, and\n(3) alignment via multilingual contexts where additional bilingual dictionaries\nare incorporated. We also release the first phonemic-orthographic alignment\ndataset on two token-level tasks (Named Entity Recognition and Part-of-Speech\nTagging) among the understudied but interconnected\nChinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals\nphonemic transcription provides essential information beyond the orthography to\nenhance cross-lingual transfer and bridge the gap among CJKV languages, leading\nto consistent improvements on cross-lingual token-level tasks over\northographic-based multilingual PLMs.", "published": "2023-07-10 06:17:33", "link": "http://arxiv.org/abs/2307.04361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TIM: Teaching Large Language Models to Translate with Comparison", "abstract": "Open-sourced large language models (LLMs) have demonstrated remarkable\nefficacy in various tasks with instruction tuning. However, these models can\nsometimes struggle with tasks that require more specialized knowledge such as\ntranslation. One possible reason for such deficiency is that instruction tuning\naims to generate fluent and coherent text that continues from a given\ninstruction without being constrained by any task-specific requirements.\nMoreover, it can be more challenging for tuning smaller LLMs with lower-quality\ntraining data. To address this issue, we propose a novel framework using\nexamples in comparison to teach LLMs to learn translation. Our approach\ninvolves presenting the model with examples of correct and incorrect\ntranslations and using a preference loss to guide the model's learning. We\nevaluate our method on WMT2022 test sets and show that it outperforms existing\nmethods. Our findings offer a new perspective on fine-tuning LLMs for\ntranslation tasks and provide a promising solution for generating high-quality\ntranslations. Please refer to Github for more details:\nhttps://github.com/lemon0830/TIM.", "published": "2023-07-10 08:15:40", "link": "http://arxiv.org/abs/2307.04408v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Biomedical Text Summarization and Question-Answering: On the\n  Utility of Domain-Specific Pre-Training", "abstract": "Biomedical summarization requires large datasets to train for text\ngeneration. We show that while transfer learning offers a viable option for\naddressing this challenge, an in-domain pre-training does not always offer\nadvantages in a BioASQ summarization task. We identify a suitable model\narchitecture and use it to show a benefit of a general-domain pre-training\nfollowed by a task-specific fine-tuning in the context of a BioASQ\nsummarization task, leading to a novel three-step fine-tuning approach that\nworks with only a thousand in-domain examples. Our results indicate that a\nLarge Language Model without domain-specific pre-training can have a\nsignificant edge in some domain-specific biomedical text generation tasks.", "published": "2023-07-10 08:32:45", "link": "http://arxiv.org/abs/2307.04412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Computational Modeling of Meaning: Embodied Cognition Intertwined\n  with Emotion", "abstract": "This document chronicles this author's attempt to explore how words come to\nmean what they do, with a particular focus on child language acquisition and\nwhat that means for models of language understanding.\\footnote{I say\n\\emph{historical} because I synthesize the ideas based on when I discovered\nthem and how those ideas influenced my later thinking.} I explain the setting\nfor child language learning, how embodiment -- being able to perceive and enact\nin the world, including knowledge of concrete and abstract concepts -- is\ncrucial, and how emotion and cognition relate to each other and the language\nlearning process. I end with what I think are some of the requirements for a\nlanguage-learning agent that learns language in a setting similar to that of\nchildren. This paper can act as a potential guide for ongoing and future work\nin modeling language.", "published": "2023-07-10 12:31:27", "link": "http://arxiv.org/abs/2307.04518v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BeaverTails: Towards Improved Safety Alignment of LLM via a\n  Human-Preference Dataset", "abstract": "In this paper, we introduce the BeaverTails dataset, aimed at fostering\nresearch on safety alignment in large language models (LLMs). This dataset\nuniquely separates annotations of helpfulness and harmlessness for\nquestion-answering pairs, thus offering distinct perspectives on these crucial\nattributes. In total, we have gathered safety meta-labels for 333,963\nquestion-answer (QA) pairs and 361,903 pairs of expert comparison data for both\nthe helpfulness and harmlessness metrics. We further showcase applications of\nBeaverTails in content moderation and reinforcement learning with human\nfeedback (RLHF), emphasizing its potential for practical safety measures in\nLLMs. We believe this dataset provides vital resources for the community,\ncontributing towards the safe development and deployment of LLMs. Our project\npage is available at the following URL:\nhttps://sites.google.com/view/pku-beavertails.", "published": "2023-07-10 15:56:17", "link": "http://arxiv.org/abs/2307.04657v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Identifier: A Natural Text Parsing-based Framework For Entity\n  Relation Extraction", "abstract": "The field of programming has a diversity of paradigms that are used according\nto the working framework. While current neural code generation methods are able\nto learn and generate code directly from text, we believe that this approach is\nnot optimal for certain code tasks, particularly the generation of classes in\nan object-oriented project. Specifically, we use natural language processing\ntechniques to extract structured information from requirements descriptions, in\norder to automate the generation of CRUD (Create, Read, Update, Delete) class\ncode. To facilitate this process, we introduce a pipeline for extracting entity\nand relation information, as well as a representation called an \"Entity Tree\"\nto model this information. We also create a dataset to evaluate the\neffectiveness of our approach.", "published": "2023-07-10 20:30:27", "link": "http://arxiv.org/abs/2307.04892v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Extraction as Question Generation and Answering", "abstract": "Recent work on Event Extraction has reframed the task as Question Answering\n(QA), with promising results. The advantage of this approach is that it\naddresses the error propagation issue found in traditional token-based\nclassification approaches by directly predicting event arguments without\nextracting candidates first. However, the questions are typically based on\nfixed templates and they rarely leverage contextual information such as\nrelevant arguments. In addition, prior QA-based approaches have difficulty\nhandling cases where there are multiple arguments for the same role. In this\npaper, we propose QGA-EE, which enables a Question Generation (QG) model to\ngenerate questions that incorporate rich contextual information instead of\nusing fixed templates. We also propose dynamic templates to assist the training\nof QG model. Experiments show that QGA-EE outperforms all prior\nsingle-task-based models on the ACE05 English dataset.", "published": "2023-07-10 01:46:15", "link": "http://arxiv.org/abs/2307.05567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Equitable Text in Dialogue from Biased Training\n  Data", "abstract": "The ingrained principles of fairness in a dialogue system's decision-making\nprocess and generated responses are crucial for user engagement, satisfaction,\nand task achievement. Absence of equitable and inclusive principles can hinder\nthe formation of common ground, which in turn negatively impacts the overall\nperformance of the system. For example, misusing pronouns in a user interaction\nmay cause ambiguity about the intended subject. Yet, there is no comprehensive\nstudy of equitable text generation in dialogue. Aptly, in this work, we use\ntheories of computational learning to study this problem. We provide formal\ndefinitions of equity in text generation, and further, prove formal connections\nbetween learning human-likeness and learning equity: algorithms for improving\nequity ultimately reduce to algorithms for improving human-likeness (on\naugmented data). With this insight, we also formulate reasonable conditions\nunder which text generation algorithms can learn to generate equitable text\nwithout any modifications to the biased training data on which they learn. To\nexemplify our theory in practice, we look at a group of algorithms for the\nGuessWhat?! visual dialogue game and, using this example, test our theory\nempirically. Our theory accurately predicts relative-performance of multiple\nalgorithms in generating equitable text as measured by both human and automated\nevaluation.", "published": "2023-07-10 01:44:13", "link": "http://arxiv.org/abs/2307.04303v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft\n  Prompting and Calibrated Confidence Estimation", "abstract": "Large pre-trained language models achieve impressive results across many\ntasks. However, recent works point out that pre-trained language models may\nmemorize a considerable fraction of their training data, leading to the privacy\nrisk of information leakage. In this paper, we propose a method named Ethicist\nfor targeted training data extraction through loss smoothed soft prompting and\ncalibrated confidence estimation, investigating how to recover the suffix in\nthe training data when given a prefix. To elicit memorization in the attacked\nmodel, we tune soft prompt embeddings while keeping the model fixed. We further\npropose a smoothing loss that smooths the loss distribution of the suffix\ntokens to make it easier to sample the correct suffix. In order to select the\nmost probable suffix from a collection of sampled suffixes and estimate the\nprediction confidence, we propose a calibrated confidence estimation method,\nwhich normalizes the confidence of the generated suffixes with a local\nestimation. We show that Ethicist significantly improves the extraction\nperformance on a recently proposed public benchmark. We also investigate\nseveral factors influencing the data extraction performance, including decoding\nstrategy, model scale, prefix length, and suffix length. Our code is available\nat https://github.com/thu-coai/Targeted-Data-Extraction.", "published": "2023-07-10 08:03:41", "link": "http://arxiv.org/abs/2307.04401v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Factuality of Abstractive Summarization via Contrastive Reward\n  Learning", "abstract": "Modern abstractive summarization models often generate summaries that contain\nhallucinated or contradictory information. In this paper, we propose a simple\nbut effective contrastive learning framework that incorporates recent\ndevelopments in reward learning and factuality metrics. Empirical studies\ndemonstrate that the proposed framework enables summarization models to learn\nfrom feedback of factuality metrics using contrastive reward learning, leading\nto more factual summaries by human evaluations. This suggests that further\nadvances in learning and evaluation algorithms can feed directly into providing\nmore factual summaries.", "published": "2023-07-10 12:01:18", "link": "http://arxiv.org/abs/2307.04507v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring Lexical Diversity in Texts: The Twofold Length Problem", "abstract": "The impact of text length on the estimation of lexical diversity has captured\nthe attention of the scientific community for more than a century. Numerous\nindices have been proposed, and many studies have been conducted to evaluate\nthem, but the problem remains. This methodological review provides a critical\nanalysis not only of the most commonly used indices in language learning\nstudies, but also of the length problem itself, as well as of the methodology\nfor evaluating the proposed solutions. The analysis of three datasets of\nEnglish language-learners' texts revealed that indices that reduce all texts to\nthe same length using a probabilistic or an algorithmic approach solve the\nlength dependency problem; however, all these indices failed to address the\nsecond problem, which is their sensitivity to the parameter that determines the\nlength to which the texts are reduced. The paper concludes with recommendations\nfor optimizing lexical diversity analysis.", "published": "2023-07-10 15:10:56", "link": "http://arxiv.org/abs/2307.04626v2", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented\n  Dialogue with Symbolic Scene Representation", "abstract": "SimpleMTOD is a simple language model which recasts several sub-tasks in\nmultimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is\nbuilt on a large-scale transformer-based auto-regressive architecture, which\nhas already proven to be successful in uni-modal task-oriented dialogues, and\neffectively leverages transfer learning from pre-trained GPT-2. In-order to\ncapture the semantics of visual scenes, we introduce both local and\nde-localized tokens for objects within a scene. De-localized tokens represent\nthe type of an object rather than the specific object itself and so possess a\nconsistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art\nBLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0\ntest-std dataset while performing on par in other multimodal sub-tasks:\nDisambiguation, Coreference Resolution, and Dialog State Tracking. This is\ndespite taking a minimalist approach for extracting visual (and non-visual)\ninformation. In addition the model does not rely on task-specific architectural\nchanges such as classification heads.", "published": "2023-07-10 21:16:46", "link": "http://arxiv.org/abs/2307.04907v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hate Speech Detection via Dual Contrastive Learning", "abstract": "The fast spread of hate speech on social media impacts the Internet\nenvironment and our society by increasing prejudice and hurting people.\nDetecting hate speech has aroused broad attention in the field of natural\nlanguage processing. Although hate speech detection has been addressed in\nrecent work, this task still faces two inherent unsolved challenges. The first\nchallenge lies in the complex semantic information conveyed in hate speech,\nparticularly the interference of insulting words in hate speech detection. The\nsecond challenge is the imbalanced distribution of hate speech and non-hate\nspeech, which may significantly deteriorate the performance of models. To\ntackle these challenges, we propose a novel dual contrastive learning (DCL)\nframework for hate speech detection. Our framework jointly optimizes the\nself-supervised and the supervised contrastive learning loss for capturing\nspan-level information beyond the token-level emotional semantics used in\nexisting models, particularly detecting speech containing abusive and insulting\nwords. Moreover, we integrate the focal loss into the dual contrastive learning\nframework to alleviate the problem of data imbalance. We conduct experiments on\ntwo publicly available English datasets, and experimental results show that the\nproposed model outperforms the state-of-the-art models and precisely detects\nhate speeches.", "published": "2023-07-10 13:23:36", "link": "http://arxiv.org/abs/2307.05578v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting LLM-Generated Text in Computing Education: A Comparative Study\n  for ChatGPT Cases", "abstract": "Due to the recent improvements and wide availability of Large Language Models\n(LLMs), they have posed a serious threat to academic integrity in education.\nModern LLM-generated text detectors attempt to combat the problem by offering\neducators with services to assess whether some text is LLM-generated. In this\nwork, we have collected 124 submissions from computer science students before\nthe creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this\ndata to evaluate eight publicly-available LLM-generated text detectors through\nthe measures of accuracy, false positives, and resilience. The purpose of this\nwork is to inform the community of what LLM-generated text detectors work and\nwhich do not, but also to provide insights for educators to better maintain\nacademic integrity in their courses. Our results find that CopyLeaks is the\nmost accurate LLM-generated text detector, GPTKit is the best LLM-generated\ntext detector to reduce false positives, and GLTR is the most resilient\nLLM-generated text detector. We also express concerns over 52 false positives\n(of 114 human written submissions) generated by GPTZero. Finally, we note that\nall LLM-generated text detectors are less accurate with code, other languages\n(aside from English), and after the use of paraphrasing tools (like QuillBot).\nModern detectors are still in need of improvements so that they can offer a\nfull-proof solution to help maintain academic integrity. Further, their\nusability can be improved by facilitating a smooth API integration, providing\nclear documentation of their features and the understandability of their\nmodel(s), and supporting more commonly used languages.", "published": "2023-07-10 12:18:34", "link": "http://arxiv.org/abs/2307.07411v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "PapagAI:Automated Feedback for Reflective Essays", "abstract": "Written reflective practice is a regular exercise pre-service teachers\nperform during their higher education. Usually, their lecturers are expected to\nprovide individual feedback, which can be a challenging task to perform on a\nregular basis. In this paper, we present the first open-source automated\nfeedback tool based on didactic theory and implemented as a hybrid AI system.\nWe describe the components and discuss the advantages and disadvantages of our\nsystem compared to the state-of-art generative large language models. The main\nobjective of our work is to enable better learning outcomes for students and to\ncomplement the teaching activities of lecturers.", "published": "2023-07-10 11:05:51", "link": "http://arxiv.org/abs/2307.07523v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "RLTF: Reinforcement Learning from Unit Test Feedback", "abstract": "The goal of program synthesis, or code generation, is to generate executable\ncode based on given descriptions. Recently, there has been an increasing number\nof studies employing reinforcement learning (RL) to improve the performance of\nlarge language models (LLMs) for code. However, current representative works\neither rely solely on offline frameworks, limiting the exploration of new\nsample spaces, or fall short in the utilization of unit test signals, not\naccounting for specific error locations within the code. To address these\nissues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,\na novel online RL framework with unit test feedback of multi-granularity for\nrefining code LLMs. Our approach generates data in real-time during training\nand simultaneously utilizes fine-grained feedback signals to guide the model\ntowards producing higher-quality code. Extensive experiments show that RLTF\nachieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our\ncode is available at: https://github.com/Zyq-scut/RLTF.", "published": "2023-07-10 05:18:18", "link": "http://arxiv.org/abs/2307.04349v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Large Language Models as General Pattern Machines", "abstract": "We observe that pre-trained large language models (LLMs) are capable of\nautoregressively completing complex token sequences -- from arbitrary ones\nprocedurally generated by probabilistic context-free grammars (PCFG), to more\nrich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a\ngeneral AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern\ncompletion proficiency can be partially retained even when the sequences are\nexpressed using tokens randomly sampled from the vocabulary. These results\nsuggest that without any additional training, LLMs can serve as general\nsequence modelers, driven by in-context learning. In this work, we investigate\nhow these zero-shot capabilities may be applied to problems in robotics -- from\nextrapolating sequences of numbers that represent states over time to complete\nsimple motions, to least-to-most prompting of reward-conditioned trajectories\nthat can discover and represent closed-loop policies (e.g., a stabilizing\ncontroller for CartPole). While difficult to deploy today for real systems due\nto latency, context size limitations, and compute costs, the approach of using\nLLMs to drive low-level control may provide an exciting glimpse into how the\npatterns among words could be transferred to actions.", "published": "2023-07-10 17:32:13", "link": "http://arxiv.org/abs/2307.04721v2", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency\n  in coding algorithms and data structures", "abstract": "The transformative influence of Large Language Models (LLMs) is profoundly\nreshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT\ndistinguishes itself within these models, demonstrating remarkable performance\nin multi-turn conversations and exhibiting code proficiency across an array of\nlanguages. In this paper, we carry out a comprehensive evaluation of ChatGPT's\ncoding capabilities based on what is to date the largest catalog of coding\nchallenges. Our focus is on the python programming language and problems\ncentered on data structures and algorithms, two topics at the very foundations\nof Computer Science. We evaluate ChatGPT for its ability to generate correct\nsolutions to the problems fed to it, its code quality, and nature of run-time\nerrors thrown by its code. Where ChatGPT code successfully executes, but fails\nto solve the problem at hand, we look into patterns in the test cases passed in\norder to gain some insights into how wrong ChatGPT code is in these kinds of\nsituations. To infer whether ChatGPT might have directly memorized some of the\ndata that was used to train it, we methodically design an experiment to\ninvestigate this phenomena. Making comparisons with human performance whenever\nfeasible, we investigate all the above questions from the context of both its\nunderlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics\nwithin the main topics, and on problems having varying degrees of difficulty.", "published": "2023-07-10 08:20:34", "link": "http://arxiv.org/abs/2307.05360v3", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Linear Alignment of Vision-language Models for Image Captioning", "abstract": "Recently, vision-language models like CLIP have advanced the state of the art\nin a variety of multi-modal tasks including image captioning and caption\nevaluation. Many approaches leverage CLIP for cross-modal retrieval to\ncondition pre-trained language models on visual input. However, CLIP generally\nsuffers from a mis-alignment of image and text modalities in the joint\nembedding space. We investigate efficient methods to linearly re-align the\njoint embedding space for the downstream task of image captioning. This leads\nto an efficient training protocol that merely requires computing a closed-form\nsolution for a linear mapping in the joint CLIP space. Consequently, we propose\na lightweight captioning method called ReCap, which can be trained up to 1000\ntimes faster than existing lightweight methods. Moreover, we propose two new\nlearning-based image-captioning metrics built on CLIP score along with our\nproposed alignment. We evaluate ReCap on MS-COCO, Flickr30k, VizWiz and MSRVTT.\nOn the former two, ReCap performs comparably to state-of-the-art lightweight\nmethods using rule-based metrics while outperforming them on most of the\nCLIP-based metrics. On the latter two benchmarks, ReCap consistently\noutperforms competitors across all metrics and exhibits strong transfer\ncapabilities and resilience to noise. Finally, we demonstrate that our proposed\nmetrics correlate stronger with human judgement than existing metrics on the\nFlickr8k-Expert, Flickr8k-Crowdflower, and THumB datasets.", "published": "2023-07-10 17:59:21", "link": "http://arxiv.org/abs/2307.05591v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Exploring Large Language Model for Graph Data Understanding in Online\n  Job Recommendations", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks, demonstrating their exceptional capabilities in various domains.\nHowever, their potential for behavior graph understanding in job\nrecommendations remains largely unexplored. This paper focuses on unveiling the\ncapability of large language models in understanding behavior graphs and\nleveraging this understanding to enhance recommendations in online recruitment,\nincluding the promotion of out-of-distribution (OOD) application. We present a\nnovel framework that harnesses the rich contextual information and semantic\nrepresentations provided by large language models to analyze behavior graphs\nand uncover underlying patterns and relationships. Specifically, we propose a\nmeta-path prompt constructor that leverages LLM recommender to understand\nbehavior graphs for the first time and design a corresponding path augmentation\nmodule to alleviate the prompt bias introduced by path-based sequence input. By\nleveraging this capability, our framework enables personalized and accurate job\nrecommendations for individual users. We evaluate the effectiveness of our\napproach on a comprehensive dataset and demonstrate its ability to improve the\nrelevance and quality of recommended quality. This research not only sheds\nlight on the untapped potential of large language models but also provides\nvaluable insights for developing advanced recommendation systems in the\nrecruitment market. The findings contribute to the growing field of natural\nlanguage processing and offer practical implications for enhancing job search\nexperiences. We release the code at https://github.com/WLiK/GLRec.", "published": "2023-07-10 11:29:41", "link": "http://arxiv.org/abs/2307.05722v3", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Retrieval of phonemes and Kohonen algorithm", "abstract": "A phoneme-retrieval technique is proposed, which is due to the particular way\nof the construction of the network. An initial set of neurons is given. The\nnumber of these neurons is approximately equal to the number of typical\nstructures of the data. For example if the network is built for voice retrieval\nthen the number of neurons must be equal to the number of characteristic\nphonemes of the alphabet of the language spoken by the social group to which\nthe particular person belongs. Usually this task is very complicated and the\nnetwork can depend critically on the samples used for the learning. If the\nnetwork is built for image retrieval then it works only if the data to be\nretrieved belong to a particular set of images. If the network is built for\nvoice recognition it works only for some particular set of words. A typical\nexample is the words used for the flight of airplanes. For example a command\nlike the \"airplane should make a turn of 120 degrees towards the east\" can be\neasily recognized by the network if a suitable learning procedure is used.", "published": "2023-07-10 17:25:07", "link": "http://arxiv.org/abs/2307.07407v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for\n  Radiology Report Summarization", "abstract": "In this paper, we introduce CheXOFA, a new pre-trained vision-language model\n(VLM) for the chest X-ray domain. Our model is initially pre-trained on various\nmultimodal datasets within the general domain before being transferred to the\nchest X-ray domain. Following a prominent VLM, we unify various domain-specific\ntasks into a simple sequence-to-sequence schema. It enables the model to\neffectively learn the required knowledge and skills from limited resources in\nthe domain. Demonstrating superior performance on the benchmark datasets\nprovided by the BioNLP shared task, our model benefits from its training across\nmultiple tasks and domains. With subtle techniques including ensemble and\nfactual calibration, our system achieves first place on the RadSum23\nleaderboard for the hidden test set.", "published": "2023-07-10 21:18:01", "link": "http://arxiv.org/abs/2307.07409v1", "categories": ["cs.CL", "cs.AI", "eess.IV"], "primary_category": "cs.CL"}
{"title": "ChatGPT for Digital Forensic Investigation: The Good, The Bad, and The\n  Unknown", "abstract": "The disruptive application of ChatGPT (GPT-3.5, GPT-4) to a variety of\ndomains has become a topic of much discussion in the scientific community and\nsociety at large. Large Language Models (LLMs), e.g., BERT, Bard, Generative\nPre-trained Transformers (GPTs), LLaMA, etc., have the ability to take\ninstructions, or prompts, from users and generate answers and solutions based\non very large volumes of text-based training data. This paper assesses the\nimpact and potential impact of ChatGPT on the field of digital forensics,\nspecifically looking at its latest pre-trained LLM, GPT-4. A series of\nexperiments are conducted to assess its capability across several digital\nforensic use cases including artefact understanding, evidence searching, code\ngeneration, anomaly detection, incident response, and education. Across these\ntopics, its strengths and risks are outlined and a number of general\nconclusions are drawn. Overall this paper concludes that while there are some\npotential low-risk applications of ChatGPT within digital forensics, many are\neither unsuitable at present, since the evidence would need to be uploaded to\nthe service, or they require sufficient knowledge of the topic being asked of\nthe tool to identify incorrect assumptions, inaccuracies, and mistakes.\nHowever, to an appropriately knowledgeable user, it could act as a useful\nsupporting tool in some circumstances.", "published": "2023-07-10 20:07:30", "link": "http://arxiv.org/abs/2307.10195v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Study on the Correlation between Objective Evaluations and Subjective\n  Speech Quality and Intelligibility", "abstract": "Subjective tests are the gold standard for evaluating speech quality and\nintelligibility; however, they are time-consuming and expensive. Thus,\nobjective measures that align with human perceptions are crucial. This study\nevaluates the correlation between commonly used objective measures and\nsubjective speech quality and intelligibility using a Chinese speech dataset.\nMoreover, new objective measures are proposed that combine current objective\nmeasures using deep learning techniques to predict subjective quality and\nintelligibility. The proposed deep learning model reduces the amount of\ntraining data without significantly affecting prediction performance. We\nanalyzed the deep learning model to understand how objective measures reflect\nsubjective quality and intelligibility. We also explored the impact of\nincluding subjective speech quality ratings on speech intelligibility\nprediction. Our findings offer valuable insights into the relationship between\nobjective measures and human perceptions.", "published": "2023-07-10 12:25:24", "link": "http://arxiv.org/abs/2307.04517v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Timbre transfer using image-to-image denoising diffusion implicit models", "abstract": "Timbre transfer techniques aim at converting the sound of a musical piece\ngenerated by one instrument into the same one as if it was played by another\ninstrument, while maintaining as much as possible the content in terms of\nmusical characteristics such as melody and dynamics. Following their recent\nbreakthroughs in deep learning-based generation, we apply Denoising Diffusion\nModels (DDMs) to perform timbre transfer. Specifically, we apply the recently\nproposed Denoising Diffusion Implicit Models (DDIMs) that enable to accelerate\nthe sampling procedure. Inspired by the recent application of DDMs to image\ntranslation problems we formulate the timbre transfer task similarly, by first\nconverting the audio tracks into log mel spectrograms and by conditioning the\ngeneration of the desired timbre spectrogram through the input timbre\nspectrogram. We perform both one-to-one and many-to-many timbre transfer, by\nconverting audio waveforms containing only single instruments and multiple\ninstruments, respectively. We compare the proposed technique with existing\nstate-of-the-art methods both through listening tests and objective measures in\norder to demonstrate the effectiveness of the proposed model.", "published": "2023-07-10 14:28:56", "link": "http://arxiv.org/abs/2307.04586v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Behavioral Analysis of Pathological Speaker Embeddings of Patients\n  During Oncological Treatment of Oral Cancer", "abstract": "In this paper, we analyze the behavior of speaker embeddings of patients\nduring oral cancer treatment. First, we found that pre- and post-treatment\nspeaker embeddings differ significantly, notifying a substantial change in\nvoice characteristics. However, a partial recovery to pre-operative voice\ntraits is observed after 12 months post-operation. Secondly, the same-speaker\nsimilarity at distinct treatment stages is similar to healthy speakers,\nindicating that the embeddings can capture characterizing features of even\nseverely impaired speech. Finally, a speaker verification analysis signifies a\nstable false positive rate and variable false negative rate when combining\nspeech samples of different treatment stages. This indicates robustness of the\nembeddings towards other speakers, while still capturing the changing voice\ncharacteristics during treatment. To the best of our knowledge, this is the\nfirst analysis of speaker embeddings during oral cancer treatment of patients.", "published": "2023-07-10 17:53:21", "link": "http://arxiv.org/abs/2307.04744v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Predicting Tuberculosis from Real-World Cough Audio Recordings and\n  Metadata", "abstract": "Tuberculosis (TB) is an infectious disease caused by the bacterium\nMycobacterium tuberculosis and primarily affects the lungs, as well as other\nbody parts. TB is spread through the air when an infected person coughs,\nsneezes, or talks. Medical doctors diagnose TB in patients via clinical\nexaminations and specialized tests. However, coughing is a common symptom of\nrespiratory diseases such as TB. Literature suggests that cough sounds coming\nfrom different respiratory diseases can be distinguished by both medical\ndoctors and computer algorithms. Therefore, cough recordings associated with\npatients with and without TB seems to be a reasonable avenue of investigation.\nIn this work, we utilize a very large dataset of TB and non-TB cough audio\nrecordings obtained from the south-east of Africa, India, and the south-east of\nAsia using a fully automated phone-based application (Hyfe), without manual\nannotation. We fit statistical classifiers based on spectral and time domain\nfeatures with and without clinical metadata. A stratified grouped\ncross-validation approach shows that an average Area Under Curve (AUC) of\napproximately 0.70 $\\pm$ 0.05 both for a cough-level and a participant-level\nclassification can be achieved using cough sounds alone. The addition of\ndemographic and clinical factors increases performance, resulting in an average\nAUC of approximately 0.81 $\\pm$ 0.05. Our results suggest mobile phone-based\napplications that integrate clinical symptoms and cough sound analysis could\nhelp community health workers and, most importantly, health service programs to\nimprove TB case-finding efforts while reducing costs, which could substantially\nimprove public health.", "published": "2023-07-10 18:21:48", "link": "http://arxiv.org/abs/2307.04842v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Demand-Driven Perspective on Generative Audio AI", "abstract": "To achieve successful deployment of AI research, it is crucial to understand\nthe demands of the industry. In this paper, we present the results of a survey\nconducted with professional audio engineers, in order to determine research\npriorities and define various research tasks. We also summarize the current\nchallenges in audio quality and controllability based on the survey. Our\nanalysis emphasizes that the availability of datasets is currently the main\nbottleneck for achieving high-quality audio generation. Finally, we suggest\npotential solutions for some revealed issues with empirical evidence.", "published": "2023-07-10 00:58:28", "link": "http://arxiv.org/abs/2307.04292v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "HCLAS-X: Hierarchical and Cascaded Lyrics Alignment System Using\n  Multimodal Cross-Correlation", "abstract": "In this work, we address the challenge of lyrics alignment, which involves\naligning the lyrics and vocal components of songs. This problem requires the\nalignment of two distinct modalities, namely text and audio. To overcome this\nchallenge, we propose a model that is trained in a supervised manner, utilizing\nthe cross-correlation matrix of latent representations between vocals and\nlyrics. Our system is designed in a hierarchical and cascaded manner. It\npredicts synced time first on a sentence-level and subsequently on a\nword-level. This design enables the system to process long sequences, as the\ncross-correlation uses quadratic memory with respect to sequence length. In our\nexperiments, we demonstrate that our proposed system achieves a significant\nimprovement in mean average error, showcasing its robustness in comparison to\nthe previous state-of-the-art model. Additionally, we conduct a qualitative\nanalysis of the system after successfully deploying it in several music\nstreaming services.", "published": "2023-07-10 07:22:06", "link": "http://arxiv.org/abs/2307.04377v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023\n  Speech-to-Speech Translation Task", "abstract": "This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech\ntranslation (S2ST) task which aims to translate from English speech of\nmulti-source to Chinese speech. The system is built in a cascaded manner\nconsisting of automatic speech recognition (ASR), machine translation (MT), and\ntext-to-speech (TTS). We make tremendous efforts to handle the challenging\nmulti-source input. Specifically, to improve the robustness to multi-source\nspeech input, we adopt various data augmentation strategies and a ROVER-based\nscore fusion on multiple ASR model outputs. To better handle the noisy ASR\ntranscripts, we introduce a three-stage fine-tuning strategy to improve\ntranslation accuracy. Finally, we build a TTS model with high naturalness and\nsound quality, which leverages a two-stage framework, using network bottleneck\nfeatures as a robust intermediate representation for speaker timbre and\nlinguistic content disentanglement. Based on the two-stage framework,\npre-trained speaker embedding is leveraged as a condition to transfer the\nspeaker timbre in the source English speech to the translated Chinese speech.\nExperimental results show that our system has high translation accuracy, speech\nnaturalness, sound quality, and speaker similarity. Moreover, it shows good\nrobustness to multi-source data.", "published": "2023-07-10 15:15:17", "link": "http://arxiv.org/abs/2307.04630v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vocal Tract Area Estimation by Gradient Descent", "abstract": "Articulatory features can provide interpretable and flexible controls for the\nsynthesis of human vocalizations by allowing the user to directly modify\nparameters like vocal strain or lip position. To make this manipulation through\nresynthesis possible, we need to estimate the features that result in a desired\nvocalization directly from audio recordings. In this work, we propose a\nwhite-box optimization technique for estimating glottal source parameters and\nvocal tract shapes from audio recordings of human vowels. The approach is based\non inverse filtering and optimizing the frequency response of a wave\\-guide\nmodel of the vocal tract with gradient descent, propagating error gradients\nthrough the mapping of articulatory features to the vocal tract area function.\nWe apply this method to the task of matching the sound of the Pink Trombone, an\ninteractive articulatory synthesizer, to a given vocalization. We find that our\nmethod accurately recovers control functions for audio generated by the Pink\nTrombone itself. We then compare our technique against evolutionary\noptimization algorithms and a neural network trained to predict control\nparameters from audio. A subjective evaluation finds that our approach\noutperforms these black-box optimization baselines on the task of reproducing\nhuman vocalizations.", "published": "2023-07-10 16:59:49", "link": "http://arxiv.org/abs/2307.04702v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Collaborative Song Dataset (CoSoD): An annotated dataset of multi-artist\n  collaborations in popular music", "abstract": "The Collaborative Song Dataset (CoSoD) is a corpus of 331 multi-artist\ncollaborations from the 2010-2019 Billboard \"Hot 100\" year-end charts. The\ncorpus is annotated with formal sections, aspects of vocal production\n(including reverberation, layering, panning, and gender of the performers), and\nrelevant metadata. CoSoD complements other popular music datasets by focusing\nexclusively on musical collaborations between independent acts. In addition to\nfacilitating the study of song form and vocal production, CoSoD allows for the\nin-depth study of gender as it relates to various timbral, pitch, and formal\nparameters in musical collaborations. In this paper, we detail the contents of\nthe dataset and outline the annotation process. We also present an experiment\nusing CoSoD that examines how the use of reverberation, layering, and panning\nare related to the gender of the artist. In this experiment, we find that men's\nvoices are on average treated with less reverberation and occupy a more narrow\nposition in the stereo mix than women's voices.", "published": "2023-07-10 15:57:42", "link": "http://arxiv.org/abs/2307.05588v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Edge Storage Management Recipe with Zero-Shot Data Compression for Road\n  Anomaly Detection", "abstract": "Recent studies show edge computing-based road anomaly detection systems which\nmay also conduct data collection simultaneously. However, the edge computers\nwill have small data storage but we need to store the collected audio samples\nfor a long time in order to update existing models or develop a novel method.\nTherefore, we should consider an approach for efficient storage management\nmethods while preserving high-fidelity audio. A hardware-perspective approach,\nsuch as using a low-resolution microphone, is an intuitive way to reduce file\nsize but is not recommended because it fundamentally cuts off high-frequency\ncomponents. On the other hand, a computational file compression approach that\nencodes collected high-resolution audio into a compact code should be\nrecommended because it also provides a corresponding decoding method. Motivated\nby this, we propose a way of simple yet effective pre-trained autoencoder-based\ndata compression method. The pre-trained autoencoder is trained for the purpose\nof audio super-resolution so it can be utilized to encode or decode any\narbitrary sampling rate. Moreover, it will reduce the communication cost for\ndata transmission from the edge to the central server. Via the comparative\nexperiments, we confirm that the zero-shot audio compression and decompression\nhighly preserve anomaly detection performance while enhancing storage and\ntransmission efficiency.", "published": "2023-07-10 01:30:21", "link": "http://arxiv.org/abs/2307.04298v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Piano Transcription with Hierarchical Frequency-Time\n  Transformer", "abstract": "Taking long-term spectral and temporal dependencies into account is essential\nfor automatic piano transcription. This is especially helpful when determining\nthe precise onset and offset for each note in the polyphonic piano content. In\nthis case, we may rely on the capability of self-attention mechanism in\nTransformers to capture these long-term dependencies in the frequency and time\naxes. In this work, we propose hFT-Transformer, which is an automatic music\ntranscription method that uses a two-level hierarchical frequency-time\nTransformer architecture. The first hierarchy includes a convolutional block in\nthe time axis, a Transformer encoder in the frequency axis, and a Transformer\ndecoder that converts the dimension in the frequency axis. The output is then\nfed into the second hierarchy which consists of another Transformer encoder in\nthe time axis. We evaluated our method with the widely used MAPS and MAESTRO\nv3.0.0 datasets, and it demonstrated state-of-the-art performance on all the\nF1-scores of the metrics among Frame, Note, Note with Offset, and Note with\nOffset and Velocity estimations.", "published": "2023-07-10 02:04:43", "link": "http://arxiv.org/abs/2307.04305v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting an External Microphone for Binaural RTF-Vector-Based\n  Direction of Arrival Estimation for Multiple Speakers", "abstract": "In hearing aid applications, an important objective is to accurately estimate\nthe direction of arrival (DOA) of multiple speakers in noisy and reverberant\nenvironments. Recently, we proposed a binaural DOA estimation method, where the\nDOAs of the speakers are estimated by selecting the directions for which the\nso-called Hermitian angle spectrum between the estimated relative transfer\nfunction (RTF) vector and a database of prototype anechoic RTF vectors is\nmaximized. The RTF vector is estimated using the covariance whitening (CW)\nmethod, which requires a computationally complex generalized eigenvalue\ndecomposition. The spatial spectrum is obtained by only considering frequencies\nwhere it is likely that one speaker dominates over the other speakers, noise\nand reverberation. In this contribution, we exploit the availability of an\nexternal microphone that is spatially separated from the hearing aid\nmicrophones and consider a low-complexity RTF vector estimation method that\nassumes a low spatial coherence between the undesired components in the\nexternal microphone and the hearing aid microphones. Using recordings of two\nspeakers and diffuse-like babble noise in acoustic environments with mild\nreverberation and low signal-to-noise ratio, simulation results show that the\nproposed method yields a comparable DOA estimation performance as the CW method\nat a lower computational complexity.", "published": "2023-07-10 10:13:12", "link": "http://arxiv.org/abs/2307.04460v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "EchoVest: Real-Time Sound Classification and Depth Perception Expressed\n  through Transcutaneous Electrical Nerve Stimulation", "abstract": "Over 1.5 billion people worldwide live with hearing impairment. Despite\nvarious technologies that have been created for individuals with such\ndisabilities, most of these technologies are either extremely expensive or\ninaccessible for everyday use in low-medium income countries. In order to\ncombat this issue, we have developed a new assistive device, EchoVest, for\nblind/deaf people to intuitively become more aware of their environment.\nEchoVest transmits vibrations to the user's body by utilizing transcutaneous\nelectric nerve stimulation (TENS) based on the source of the sounds. EchoVest\nalso provides various features, including sound localization, sound\nclassification, noise reduction, and depth perception. We aimed to outperform\nCNN-based machine-learning models, the most commonly used machine learning\nmodel for classification tasks, in accuracy and computational costs. To do so,\nwe developed and employed a novel audio pipeline that adapts the Audio\nSpectrogram Transformer (AST) model, an attention-based model, for our sound\nclassification purposes, and Fast Fourier Transforms for noise reduction. The\napplication of Otsu's Method helped us find the optimal thresholds for\nbackground noise sound filtering and gave us much greater accuracy. In order to\ncalculate direction and depth accurately, we applied Complex Time Difference of\nArrival algorithms and SOTA localization. Our last improvement was to use blind\nsource separation to make our algorithms applicable to multiple microphone\ninputs. The final algorithm achieved state-of-the-art results on numerous\ncheckpoints, including a 95.7\\% accuracy on the ESC-50 dataset for\nenvironmental sound classification.", "published": "2023-07-10 14:43:32", "link": "http://arxiv.org/abs/2307.04604v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "VampNet: Music Generation via Masked Acoustic Token Modeling", "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music\nsynthesis, compression, inpainting, and variation. We use a variable masking\nschedule during training which allows us to sample coherent music from the\nmodel by applying a variety of masking approaches (called prompts) during\ninference. VampNet is non-autoregressive, leveraging a bidirectional\ntransformer architecture that attends to all tokens in a forward pass. With\njust 36 sampling passes, VampNet can generate coherent high-fidelity musical\nwaveforms. We show that by prompting VampNet in various ways, we can apply it\nto tasks like music compression, inpainting, outpainting, continuation, and\nlooping with variation (vamping). Appropriately prompted, VampNet is capable of\nmaintaining style, genre, instrumentation, and other high-level aspects of the\nmusic. This flexible prompting capability makes VampNet a powerful music\nco-creation tool. Code and audio samples are available online.", "published": "2023-07-10 16:42:03", "link": "http://arxiv.org/abs/2307.04686v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Spatial Features from Audio-Visual Correspondence in Egocentric\n  Videos", "abstract": "We propose a self-supervised method for learning representations based on\nspatial audio-visual correspondences in egocentric videos. Our method uses a\nmasked auto-encoding framework to synthesize masked binaural (multi-channel)\naudio through the synergy of audio and vision, thereby learning useful spatial\nrelationships between the two modalities. We use our pretrained features to\ntackle two downstream video tasks requiring spatial understanding in social\nscenarios: active speaker detection and spatial audio denoising. Through\nextensive experiments, we show that our features are generic enough to improve\nover multiple state-of-the-art baselines on both tasks on two challenging\negocentric video datasets that offer binaural audio, EgoCom and EasyCom.\nProject: http://vision.cs.utexas.edu/projects/ego_av_corr.", "published": "2023-07-10 17:58:17", "link": "http://arxiv.org/abs/2307.04760v4", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
