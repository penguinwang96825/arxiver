{"title": "Unmasking and Quantifying Racial Bias of Large Language Models in\n  Medical Report Generation", "abstract": "Large language models like GPT-3.5-turbo and GPT-4 hold promise for\nhealthcare professionals, but they may inadvertently inherit biases during\ntheir training, potentially affecting their utility in medical applications.\nDespite few attempts in the past, the precise impact and extent of these biases\nremain uncertain. Through both qualitative and quantitative analyses, we find\nthat these models tend to project higher costs and longer hospitalizations for\nWhite populations and exhibit optimistic views in challenging medical scenarios\nwith much higher survival rates. These biases, which mirror real-world\nhealthcare disparities, are evident in the generation of patient backgrounds,\nthe association of specific diseases with certain races, and disparities in\ntreatment recommendations, etc. Our findings underscore the critical need for\nfuture research to address and mitigate biases in language models, especially\nin critical healthcare applications, to ensure fair and accurate outcomes for\nall patients.", "published": "2024-01-25 00:34:16", "link": "http://arxiv.org/abs/2401.13867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic embedded topic models and change-point detection for exploring\n  literary-historical hypotheses", "abstract": "We present a novel combination of dynamic embedded topic models and\nchange-point detection to explore diachronic change of lexical semantic\nmodality in classical and early Christian Latin. We demonstrate several methods\nfor finding and characterizing patterns in the output, and relating them to\ntraditional scholarship in Comparative Literature and Classics. This simple\napproach to unsupervised models of semantic change can be applied to any\nsuitable corpus, and we conclude with future directions and refinements aiming\nto allow noisier, less-curated materials to meet that threshold.", "published": "2024-01-25 02:50:03", "link": "http://arxiv.org/abs/2401.13905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data\n  Artifacts", "abstract": "Researchers recently found out that sometimes language models achieve high\naccuracy on benchmark data set, but they can not generalize very well with even\nlittle changes to the original data set. This is sometimes due to data\nartifacts, model is learning the spurious correlation between tokens and\nlabels, instead of the semantics and logic. In this work, we analyzed SNLI data\nand visualized such spurious correlations. We proposed an adaptive up-sampling\nalgorithm to correct the data artifacts, which is simple and effective, and\ndoes not need human edits or annotation. We did an experiment applying the\nalgorithm to fix the data artifacts in SNLI data and the model trained with\ncorrected data performed significantly better than the model trained with raw\nSNLI data, overall, as well as on the subset we corrected.", "published": "2024-01-25 02:54:53", "link": "http://arxiv.org/abs/2401.13907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Text Watermark for Large Language Models", "abstract": "The advancement of Large Language Models (LLMs) has led to increasing\nconcerns about the misuse of AI-generated text, and watermarking for\nLLM-generated text has emerged as a potential solution. However, it is\nchallenging to generate high-quality watermarked text while maintaining strong\nsecurity, robustness, and the ability to detect watermarks without prior\nknowledge of the prompt or model. This paper proposes an adaptive watermarking\nstrategy to address this problem. To improve the text quality and maintain\nrobustness, we adaptively add watermarking to token distributions with high\nentropy measured using an auxiliary model and keep the low entropy token\ndistributions untouched. For the sake of security and to further minimize the\nwatermark's impact on text quality, instead of using a fixed green/red list\ngenerated from a random secret key, which can be vulnerable to decryption and\nforgery, we adaptively scale up the output logits in proportion based on the\nsemantic embedding of previously generated text using a well designed semantic\nmapping model. Our experiments involving various LLMs demonstrate that our\napproach achieves comparable robustness performance to existing watermark\nmethods. Additionally, the text generated by our method has perplexity\ncomparable to that of \\emph{un-watermarked} LLMs while maintaining security\neven under various attacks.", "published": "2024-01-25 03:57:12", "link": "http://arxiv.org/abs/2401.13927v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Uncertainty-Aware Language Agent", "abstract": "While Language Agents have achieved promising success by placing Large\nLanguage Models at the core of a more versatile design that dynamically\ninteracts with the external world, the existing approaches neglect the notion\nof uncertainty during these interactions. We present the Uncertainty-Aware\nLanguage Agent (UALA), a framework that orchestrates the interaction between\nthe agent and the external world using uncertainty quantification. Compared\nwith other well-known counterparts like ReAct, our extensive experiments across\n3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes\ndemonstrate that UALA brings a significant improvement of performance, while\nhaving a substantially lower reliance on the external world (i.e., reduced\nnumber of tool calls and tokens). Our analyses provide various insights\nincluding the great potential of UALA compared with agent fine-tuning, and\nunderscore the unreliability of verbalised confidence of LLMs as a proxy for\nuncertainty.", "published": "2024-01-25 08:48:21", "link": "http://arxiv.org/abs/2401.14016v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection", "abstract": "In the universe of Natural Language Processing, Transformer-based language\nmodels like BERT and (Chat)GPT have emerged as lexical superheroes with great\npower to solve open research problems. In this paper, we specifically focus on\nthe temporal problem of semantic change, and evaluate their ability to solve\ntwo diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and\nHistoWiC. In particular, we investigate the potential of a novel, off-the-shelf\ntechnology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a\nfamily of models that currently stand as the state-of-the-art for modeling\nsemantic change. Our experiments represent the first attempt to assess the use\nof (Chat)GPT for studying semantic change. Our results indicate that ChatGPT\nperforms significantly worse than the foundational GPT version. Furthermore,\nour results demonstrate that (Chat)GPT achieves slightly lower performance than\nBERT in detecting long-term changes but performs significantly worse in\ndetecting short-term changes.", "published": "2024-01-25 09:36:58", "link": "http://arxiv.org/abs/2401.14040v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Affinity, Rationality, and Diversity of Hierarchical Topic\n  Modeling", "abstract": "Hierarchical topic modeling aims to discover latent topics from a corpus and\norganize them into a hierarchy to understand documents with desirable semantic\ngranularity. However, existing work struggles with producing topic hierarchies\nof low affinity, rationality, and diversity, which hampers document\nunderstanding. To overcome these challenges, we in this paper propose Transport\nPlan and Context-aware Hierarchical Topic Model (TraCo). Instead of early\nsimple topic dependencies, we propose a transport plan dependency method. It\nconstrains dependencies to ensure their sparsity and balance, and also\nregularizes topic hierarchy building with them. This improves affinity and\ndiversity of hierarchies. We further propose a context-aware disentangled\ndecoder. Rather than previously entangled decoding, it distributes different\nsemantic granularity to topics at different levels by disentangled decoding.\nThis facilitates the rationality of hierarchies. Experiments on benchmark\ndatasets demonstrate that our method surpasses state-of-the-art baselines,\neffectively improving the affinity, rationality, and diversity of hierarchical\ntopic modeling with better performance on downstream tasks.", "published": "2024-01-25 11:47:58", "link": "http://arxiv.org/abs/2401.14113v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Conversational Recommender System as a Language\n  Processing Task", "abstract": "Conversational recommender systems (CRS) aim to recommend relevant items to\nusers by eliciting user preference through natural language conversation. Prior\nwork often utilizes external knowledge graphs for items' semantic information,\na language model for dialogue generation, and a recommendation module for\nranking relevant items. This combination of multiple components suffers from a\ncumbersome training process, and leads to semantic misalignment issues between\ndialogue generation and item recommendation. In this paper, we represent items\nin natural language and formulate CRS as a natural language processing task.\nAccordingly, we leverage the power of pre-trained language models to encode\nitems, understand user intent via conversation, perform item recommendation\nthrough semantic matching, and generate dialogues. As a unified model, our\nPECRS (Parameter-Efficient CRS), can be optimized in a single stage, without\nrelying on non-textual metadata such as a knowledge graph. Experiments on two\nbenchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of\nPECRS on recommendation and conversation. Our code is available at:\nhttps://github.com/Ravoxsg/efficient_unified_crs.", "published": "2024-01-25 14:07:34", "link": "http://arxiv.org/abs/2401.14194v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicitly Representing Syntax Improves Sentence-to-layout Prediction of\n  Unexpected Situations", "abstract": "Recognizing visual entities in a natural language sentence and arranging them\nin a 2D spatial layout require a compositional understanding of language and\nspace. This task of layout prediction is valuable in text-to-image synthesis as\nit allows localized and controlled in-painting of the image. In this\ncomparative study it is shown that we can predict layouts from language\nrepresentations that implicitly or explicitly encode sentence syntax, if the\nsentences mention similar entity-relationships to the ones seen during\ntraining. To test compositional understanding, we collect a test set of\ngrammatically correct sentences and layouts describing compositions of entities\nand relations that unlikely have been seen during training. Performance on this\ntest set substantially drops, showing that current models rely on correlations\nin the training data and have difficulties in understanding the structure of\nthe input sentences. We propose a novel structural loss function that better\nenforces the syntactic structure of the input sentence and show large\nperformance gains in the task of 2D spatial layout prediction conditioned on\ntext. The loss has the potential to be used in other generation tasks where a\ntree-like structure underlies the conditioning modality. Code, trained models\nand the USCOCO evaluation set are available via github.", "published": "2024-01-25 14:53:30", "link": "http://arxiv.org/abs/2401.14212v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Natural Language Capability of Code Large Language Model", "abstract": "Code large language models (Code LLMs) have demonstrated remarkable\nperformance in code generation. Nonetheless, most existing works focus on\nboosting code LLMs from the perspective of programming capabilities, while\ntheir natural language capabilities receive less attention. To fill this gap,\nwe thus propose a novel framework, comprising two modules: AttentionExtractor,\nwhich is responsible for extracting key phrases from the user's natural\nlanguage requirements, and AttentionCoder, which leverages these extracted\nphrases to generate target code to solve the requirement. This framework\npioneers an innovative idea by seamlessly integrating code LLMs with\ntraditional natural language processing tools. To validate the effectiveness of\nthe framework, we craft a new code generation benchmark, called MultiNL-H,\ncovering five natural languages. Extensive experimental results demonstrate the\neffectiveness of our proposed framework.", "published": "2024-01-25 15:33:20", "link": "http://arxiv.org/abs/2401.14242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts", "abstract": "While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts", "published": "2024-01-25 18:06:19", "link": "http://arxiv.org/abs/2401.14360v2", "categories": ["cs.CL", "68T50 (Primary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Modular Adaptation of Multilingual Encoders to Written Swiss German\n  Dialect", "abstract": "Creating neural text encoders for written Swiss German is challenging due to\na dearth of training data combined with dialectal variation. In this paper, we\nbuild on several existing multilingual encoders and adapt them to Swiss German\nusing continued pre-training. Evaluation on three diverse downstream tasks\nshows that simply adding a Swiss German adapter to a modular encoder achieves\n97.5% of fully monolithic adaptation performance. We further find that for the\ntask of retrieving Swiss German sentences given Standard German queries,\nadapting a character-level model is more effective than the other adaptation\nstrategies. We release our code and the models trained for our experiments at\nhttps://github.com/ZurichNLP/swiss-german-text-encoders", "published": "2024-01-25 18:59:32", "link": "http://arxiv.org/abs/2401.14400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongHealth: A Question Answering Benchmark with Long Clinical Documents", "abstract": "Background: Recent advancements in large language models (LLMs) offer\npotential benefits in healthcare, particularly in processing extensive patient\nrecords. However, existing benchmarks do not fully assess LLMs' capability in\nhandling real-world, lengthy clinical data.\n  Methods: We present the LongHealth benchmark, comprising 20 detailed\nfictional patient cases across various diseases, with each case containing\n5,090 to 6,754 words. The benchmark challenges LLMs with 400 multiple-choice\nquestions in three categories: information extraction, negation, and sorting,\nchallenging LLMs to extract and interpret information from large clinical\ndocuments.\n  Results: We evaluated nine open-source LLMs with a minimum of 16,000 tokens\nand also included OpenAI's proprietary and cost-efficient GPT-3.5 Turbo for\ncomparison. The highest accuracy was observed for Mixtral-8x7B-Instruct-v0.1,\nparticularly in tasks focused on information retrieval from single and multiple\npatient documents. However, all models struggled significantly in tasks\nrequiring the identification of missing information, highlighting a critical\narea for improvement in clinical data interpretation.\n  Conclusion: While LLMs show considerable potential for processing long\nclinical documents, their current accuracy levels are insufficient for reliable\nclinical use, especially in scenarios requiring the identification of missing\ninformation. The LongHealth benchmark provides a more realistic assessment of\nLLMs in a healthcare setting and highlights the need for further model\nrefinement for safe and effective clinical application.\n  We make the benchmark and evaluation code publicly available.", "published": "2024-01-25 19:57:00", "link": "http://arxiv.org/abs/2401.14490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially\n  Euphemistic Terms", "abstract": "This study investigates the computational processing of euphemisms, a\nuniversal linguistic phenomenon, across multiple languages. We train a\nmultilingual transformer model (XLM-RoBERTa) to disambiguate potentially\neuphemistic terms (PETs) in multilingual and cross-lingual settings. In line\nwith current trends, we demonstrate that zero-shot learning across languages\ntakes place. We also show cases where multilingual models perform better on the\ntask compared to monolingual models by a statistically significant margin,\nindicating that multilingual data presents additional opportunities for models\nto learn about cross-lingual, computational properties of euphemisms. In a\nfollow-up analysis, we focus on universal euphemistic \"categories\" such as\ndeath and bodily functions among others. We test to see whether cross-lingual\ndata of the same domain is more important than within-language data of other\ndomains to further understand the nature of the cross-lingual transfer.", "published": "2024-01-25 21:38:30", "link": "http://arxiv.org/abs/2401.14526v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Looking Right is Sometimes Right: Investigating the Capabilities of\n  Decoder-only LLMs for Sequence Labeling", "abstract": "Pre-trained language models based on masked language modeling (MLM) excel in\nnatural language understanding (NLU) tasks. While fine-tuned MLM-based encoders\nconsistently outperform causal language modeling decoders of comparable size,\nrecent decoder-only large language models (LLMs) perform on par with smaller\nMLM-based encoders. Although their performance improves with scale, LLMs fall\nshort of achieving state-of-the-art results in information extraction (IE)\ntasks, many of which are formulated as sequence labeling (SL). We hypothesize\nthat LLMs' poor SL performance stems from causal masking, which prevents the\nmodel from attending to tokens on the right of the current token. Yet, how\nexactly and to what extent LLMs' performance on SL can be improved remains\nunclear. We explore techniques for improving the SL performance of open LLMs on\nIE tasks by applying layer-wise removal of the causal mask (CM) during LLM\nfine-tuning. This approach yields performance gains competitive with\nstate-of-the-art SL models, matching or outperforming the results of CM removal\nfrom all blocks. Our findings hold for diverse SL tasks, demonstrating that\nopen LLMs with layer-dependent CM removal outperform strong MLM-based encoders\nand even instruction-tuned LLMs.", "published": "2024-01-25 22:50:48", "link": "http://arxiv.org/abs/2401.14556v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Structured Language Alternations in Historical Documents by\n  Combining Language Identification with Fourier Analysis", "abstract": "In this study, we present a generalizable workflow to identify documents in a\nhistoric language with a nonstandard language and script combination,\nArmeno-Turkish. We introduce the task of detecting distinct patterns of\nmultilinguality based on the frequency of structured language alternations\nwithin a document.", "published": "2024-01-25 23:54:34", "link": "http://arxiv.org/abs/2401.14569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A comparative study of zero-shot inference with large language models\n  and supervised modeling in breast cancer pathology classification", "abstract": "Although supervised machine learning is popular for information extraction\nfrom clinical notes, creating large annotated datasets requires extensive\ndomain expertise and is time-consuming. Meanwhile, large language models (LLMs)\nhave demonstrated promising transfer learning capability. In this study, we\nexplored whether recent LLMs can reduce the need for large-scale data\nannotations. We curated a manually-labeled dataset of 769 breast cancer\npathology reports, labeled with 13 categories, to compare zero-shot\nclassification capability of the GPT-4 model and the GPT-3.5 model with\nsupervised classification performance of three model architectures: random\nforests classifier, long short-term memory networks with attention (LSTM-Att),\nand the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either\nsignificantly better than or as well as the best supervised model, the LSTM-Att\nmodel (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance\nbetween labels, the differences were more prominent. Frequent sources of GPT-4\nerrors included inferences from multiple samples and complex task design. On\ncomplex tasks where large annotated datasets cannot be easily collected, LLMs\ncan reduce the burden of large-scale data labeling. However, if the use of LLMs\nis prohibitive, the use of simpler supervised models with large annotated\ndatasets can provide comparable results. LLMs demonstrated the potential to\nspeed up the execution of clinical NLP studies by reducing the need for\ncurating large annotated datasets. This may result in an increase in the\nutilization of NLP-based variables and outcomes in observational clinical\nstudies.", "published": "2024-01-25 02:05:31", "link": "http://arxiv.org/abs/2401.13887v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal\n  Models", "abstract": "The rapid advancement of large language models (LLMs) has led to a new era\nmarked by the development of autonomous applications in real-world scenarios,\nwhich drives innovation in creating advanced web agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe establish a new benchmark by compiling real-world tasks from 15 popular\nwebsites and introduce an automatic evaluation protocol leveraging multimodal\nunderstanding abilities of GPT-4V to evaluate open-ended web agents. We show\nthat WebVoyager achieves a 59.1% task success rate on our benchmark,\nsignificantly surpassing the performance of both GPT-4 (All Tools) and the\nWebVoyager (text-only) setups, underscoring the exceptional capability of\nWebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement\nwith human judgment, indicating its effectiveness in providing reliable and\naccurate assessments of web agents.", "published": "2024-01-25 03:33:18", "link": "http://arxiv.org/abs/2401.13919v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent\n  Self-Evolution", "abstract": "This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy\nfor enhancing the adaptability and flexibility of AI agents through inter-task\nself-evolution. Unlike existing methods focused on intra-task learning, ICE\npromotes the transfer of knowledge between tasks for genuine self-evolution,\nsimilar to human experience learning. The strategy dynamically investigates\nplanning and execution trajectories, consolidates them into simplified\nworkflows and pipelines, and exploits them for improved task execution. Our\nexperiments on the XAgent framework demonstrate ICE's effectiveness, reducing\nAPI calls by as much as 80% and significantly decreasing the demand for the\nmodel's capability. Specifically, when combined with GPT-3.5, ICE's performance\nmatches that of raw GPT-4 across various agent tasks. We argue that this\nself-evolution approach represents a paradigm shift in agent design,\ncontributing to a more robust AI community and ecosystem, and moving a step\ncloser to full autonomy.", "published": "2024-01-25 07:47:49", "link": "http://arxiv.org/abs/2401.13996v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ConstraintChecker: A Plugin for Large Language Models to Reason on\n  Commonsense Knowledge Bases", "abstract": "Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has\nbeen explored as a way to acquire new commonsense knowledge based on reference\nknowledge in the original CSKBs and external prior knowledge. Despite the\nadvancement of Large Language Models (LLM) and prompt engineering techniques in\nvarious reasoning tasks, they still struggle to deal with CSKB reasoning. One\nof the problems is that it is hard for them to acquire explicit relational\nconstraints in CSKBs from only in-context exemplars, due to a lack of symbolic\nreasoning capabilities (Bengio et al., 2021). To this end, we proposed\n**ConstraintChecker**, a plugin over prompting techniques to provide and check\nexplicit constraints. When considering a new knowledge instance,\nConstraintChecker employs a rule-based module to produce a list of constraints,\nthen it uses a zero-shot learning module to check whether this knowledge\ninstance satisfies all constraints. The acquired constraint-checking result is\nthen aggregated with the output of the main prompting technique to produce the\nfinal output. Experimental results on CSKB Reasoning benchmarks demonstrate the\neffectiveness of our method by bringing consistent improvements over all\nprompting methods. Codes and data are available at\n\\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.", "published": "2024-01-25 08:03:38", "link": "http://arxiv.org/abs/2401.14003v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation\n  for Generative AI", "abstract": "In the dynamic landscape of generative NLP, traditional text processing\npipelines limit research flexibility and reproducibility, as they are tailored\nto specific dataset, task, and model combinations. The escalating complexity,\ninvolving system prompts, model-specific formats, instructions, and more, calls\nfor a shift to a structured, modular, and customizable solution. Addressing\nthis need, we present Unitxt, an innovative library for customizable textual\ndata preparation and evaluation tailored to generative language models. Unitxt\nnatively integrates with common libraries like HuggingFace and LM-eval-harness\nand deconstructs processing flows into modular components, enabling easy\ncustomization and sharing between practitioners. These components encompass\nmodel-specific formats, task prompts, and many other comprehensive dataset\nprocessing definitions. The Unitxt-Catalog centralizes these components,\nfostering collaboration and exploration in modern textual data workflows.\nBeyond being a tool, Unitxt is a community-driven platform, empowering users to\nbuild, share, and advance their pipelines collaboratively. Join the Unitxt\ncommunity at https://github.com/IBM/unitxt!", "published": "2024-01-25 08:57:33", "link": "http://arxiv.org/abs/2401.14019v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Goal-oriented Prompt Engineering for Large Language Models: A\n  Survey", "abstract": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks and prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not only as an overview of current prompt\nengineering methods, but also aims to highlight the limitation of designing\nprompts based on an anthropomorphic assumption that expects LLMs to think like\nhumans. From our review of 50 representative studies, we demonstrate that a\ngoal-oriented prompt formulation, which guides LLMs to follow established human\nlogical thinking, significantly improves the performance of LLMs. Furthermore,\nWe introduce a novel taxonomy that categorizes goal-oriented prompting methods\ninto five interconnected stages and we demonstrate the broad applicability of\nour framework. With four future directions proposed, we hope to further\nemphasize the power and potential of goal-oriented prompt engineering in all\nfields.", "published": "2024-01-25 09:47:55", "link": "http://arxiv.org/abs/2401.14043v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ta'keed: The First Generative Fact-Checking System for Arabic Claims", "abstract": "This paper introduces Ta'keed, an explainable Arabic automatic fact-checking\nsystem. While existing research often focuses on classifying claims as \"True\"\nor \"False,\" there is a limited exploration of generating explanations for claim\ncredibility, particularly in Arabic. Ta'keed addresses this gap by assessing\nclaim truthfulness based on retrieved snippets, utilizing two main components:\ninformation retrieval and LLM-based claim verification. We compiled the\nArFactEx, a testing gold-labelled dataset with manually justified references,\nto evaluate the system. The initial model achieved a promising F1 score of 0.72\nin the classification task. Meanwhile, the system's generated explanations are\ncompared with gold-standard explanations syntactically and semantically. The\nstudy recommends evaluating using semantic similarities, resulting in an\naverage cosine similarity score of 0.76. Additionally, we explored the impact\nof varying snippet quantities on claim classification accuracy, revealing a\npotential correlation, with the model using the top seven hits outperforming\nothers with an F1 score of 0.77.", "published": "2024-01-25 10:43:00", "link": "http://arxiv.org/abs/2401.14067v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on\n  Few-shot Inference via Debiased Domain Abstraction", "abstract": "As a novel and effective fine-tuning paradigm based on large-scale\npre-trained language models (PLMs), prompt-tuning aims to reduce the gap\nbetween downstream tasks and pre-training objectives. While prompt-tuning has\nyielded continuous advancements in various tasks, such an approach still\nremains a persistent defect: prompt-tuning methods fail to generalize to\nspecific few-shot patterns. From the perspective of distribution analyses, we\ndisclose that the intrinsic issues behind the phenomenon are the\nover-multitudinous conceptual knowledge contained in PLMs and the abridged\nknowledge for target downstream domains, which jointly result in that PLMs\nmis-locate the knowledge distributions corresponding to the target domains in\nthe universal knowledge embedding space. To this end, we intuitively explore to\napproximate the unabridged target domains of downstream tasks in a debiased\nmanner, and then abstract such domains to generate discriminative prompts,\nthereby providing the de-ambiguous guidance for PLMs. Guided by such an\nintuition, we propose a simple yet effective approach, namely BayesPrompt, to\nlearn prompts that contain the domain discriminative information against the\ninterference from domain-irrelevant knowledge. BayesPrompt primitively\nleverages known distributions to approximate the debiased factual distributions\nof target domains and further uniformly samples certain representative features\nfrom the approximated distributions to generate the ultimate prompts for PLMs.\nWe provide theoretical insights with the connection to domain adaptation.\nEmpirically, our method achieves state-of-the-art performance on benchmarks.", "published": "2024-01-25 13:20:47", "link": "http://arxiv.org/abs/2401.14166v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Can Large Language Models Understand Spatial-Temporal Data?", "abstract": "While Large Language Models (LLMs) dominate tasks like natural language\nprocessing and computer vision, harnessing their power for spatial-temporal\nforecasting remains challenging. The disparity between sequential text and\ncomplex spatial-temporal data hinders this application. To address this issue,\nthis paper introduces STG-LLM, an innovative approach empowering LLMs for\nspatial-temporal forecasting. We tackle the data mismatch by proposing: 1)\nSTG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph\ndata into concise tokens capturing both spatial and temporal relationships; 2)\nSTG-Adapter: This minimalistic adapter, consisting of linear encoding and\ndecoding layers, bridges the gap between tokenized data and LLM comprehension.\nBy fine-tuning only a small set of parameters, it can effectively grasp the\nsemantics of tokens generated by STG-Tokenizer, while preserving the original\nnatural language understanding capabilities of LLMs. Extensive experiments on\ndiverse spatial-temporal benchmark datasets show that STG-LLM successfully\nunlocks LLM potential for spatial-temporal forecasting. Remarkably, our\napproach achieves competitive performance on par with dedicated SOTA methods.", "published": "2024-01-25 14:03:15", "link": "http://arxiv.org/abs/2401.14192v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Commonsense-augmented Memory Construction and Management in Long-term\n  Conversations via Context-aware Persona Refinement", "abstract": "Memorizing and utilizing speakers' personas is a common practice for response\ngeneration in long-term conversations. Yet, human-authored datasets often\nprovide uninformative persona sentences that hinder response quality. This\npaper presents a novel framework that leverages commonsense-based persona\nexpansion to address such issues in long-term conversation. While prior work\nfocuses on not producing personas that contradict others, we focus on\ntransforming contradictory personas into sentences that contain rich speaker\ninformation, by refining them based on their contextual backgrounds with\ndesigned strategies. As the pioneer of persona expansion in multi-session\nsettings, our framework facilitates better response generation via human-like\npersona refinement. The supplementary video of our work is available at\nhttps://caffeine-15bbf.web.app/.", "published": "2024-01-25 14:54:33", "link": "http://arxiv.org/abs/2401.14215v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer\n  Models for Classifying Depression Severity in English and Luganda", "abstract": "Depression is a global burden and one of the most challenging mental health\nconditions to control. Experts can detect its severity early using the Beck\nDepression Inventory (BDI) questionnaire, administer appropriate medication to\npatients, and impede its progression. Due to the fear of potential\nstigmatization, many patients turn to social media platforms like Reddit for\nadvice and assistance at various stages of their journey. This research\nextracts text from Reddit to facilitate the diagnostic process. It employs a\nproposed labeling approach to categorize the text and subsequently fine-tunes\nthe Longformer model. The model's performance is compared against baseline\nmodels, including Naive Bayes, Random Forest, Support Vector Machines, and\nGradient Boosting. Our findings reveal that the Longformer model outperforms\nthe baseline models in both English (48%) and Luganda (45%) languages on a\ncustom-made dataset.", "published": "2024-01-25 15:28:07", "link": "http://arxiv.org/abs/2401.14240v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across\n  Time", "abstract": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence - into a\nlong \"encoding vector\" that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity traveling across single cortical areas or multiple\nregions at the whole-brain scale could implement a similar encoding principle.\nBy encapsulating recent input history into a single spatial pattern at each\nmoment in time, cortical waves may enable temporal context to be extracted from\nsequences of sensory inputs, the same computational principle used in\ntransformers.", "published": "2024-01-25 16:01:49", "link": "http://arxiv.org/abs/2401.14267v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RomanSetu: Efficiently unlocking multilingual capabilities of Large\n  Language Models via Romanization", "abstract": "This study addresses the challenge of extending Large Language Models (LLMs)\nto non-English languages that use non-Roman scripts. We propose an approach\nthat utilizes the romanized form of text as an interface for LLMs,\nhypothesizing that its frequent informal use and shared tokens with English\nenhance cross-lingual alignment. Our approach involves the continual\npretraining of an English LLM like Llama 2 on romanized text of non-English,\nnon-Roman script languages, followed by instruction tuning on romanized data.\nThe results indicate that romanized text not only reduces token fertility by\n2x-4x but also matches or outperforms native script representation across\nvarious NLU, NLG, and MT tasks. Moreover, the embeddings computed on romanized\ntext exhibit closer alignment with their English translations than those from\nthe native script. Our approach presents a promising direction for leveraging\nthe power of English LLMs in languages traditionally underrepresented in NLP.\nOur code is available on https://github.com/AI4Bharat/romansetu.", "published": "2024-01-25 16:11:41", "link": "http://arxiv.org/abs/2401.14280v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse\n  Worlds", "abstract": "Large Language Model (LLM) alignment aims to ensure that LLM outputs match\nwith human values. Researchers have demonstrated the severity of alignment\nproblems with a large spectrum of jailbreak techniques that can induce LLMs to\nproduce malicious content during conversations. Finding the corresponding\njailbreaking prompts usually requires substantial human intelligence or\ncomputation resources. In this paper, we report that LLMs have different levels\nof alignment in various contexts. As such, by systematically constructing many\ncontexts, called worlds, leveraging a Domain Specific Language describing\npossible worlds (e.g., time, location, characters, actions and languages) and\nthe corresponding compiler, we can cost-effectively expose latent alignment\nissues. Given the low cost of our method, we are able to conduct a large scale\nstudy regarding LLM alignment issues in different worlds. Our results show that\nour method outperforms the-state-of-the-art jailbreaking techniques on both\neffectiveness and efficiency. In addition, our results indicate that existing\nLLMs are extremely vulnerable to nesting worlds and programming language\nworlds. They imply that existing alignment training focuses on the real-world\nand is lacking in various (virtual) worlds where LLMs can be exploited.", "published": "2024-01-25 02:57:40", "link": "http://arxiv.org/abs/2402.01706v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis", "abstract": "Automated sentiment analysis using Large Language Model (LLM)-based models\nlike ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic\nresearch and in industrial applications. However, assessment and validation of\ntheir performance in case of ambiguous or ironic text is still poor. In this\nstudy, we constructed nuanced and ambiguous scenarios, we translated them in 10\nlanguages, and we predicted their associated sentiment using popular LLMs. The\nresults are validated against post-hoc human responses. Ambiguous scenarios are\noften well-coped by ChatGPT and Gemini, but we recognise significant biases and\ninconsistent performance across models and evaluated human languages. This work\nprovides a standardised methodology for automated sentiment analysis evaluation\nand makes a call for action to further improve the algorithms and their\nunderlying data, to improve their performance, interpretability and\napplicability.", "published": "2024-01-25 23:15:45", "link": "http://arxiv.org/abs/2402.01715v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LocMoE: A Low-Overhead MoE for Large Language Model Training", "abstract": "The Mixtures-of-Experts (MoE) model is a widespread distributed and\nintegrated learning method for large language models (LLM), which is favored\ndue to its ability to sparsify and expand models efficiently. However, the\nperformance of MoE is limited by load imbalance and high latency of All-to-All\ncommunication, along with relatively redundant computation owing to large\nexpert capacity. Load imbalance may result from existing routing policies that\nconsistently tend to select certain experts. The frequent inter-node\ncommunication in the All-to-All procedure also significantly prolongs the\ntraining time. To alleviate the above performance problems, we propose a novel\nrouting strategy that combines load balance and locality by converting partial\ninter-node communication to that of intra-node. Notably, we elucidate that\nthere is a minimum threshold for expert capacity, calculated through the\nmaximal angular deviation between the gating weights of the experts and the\nassigned tokens. We port these modifications on the PanGu-Sigma model based on\nthe MindSpore framework with multi-level routing and conduct experiments on\nAscend clusters. The experiment results demonstrate that the proposed LocMoE\nreduces training time per epoch by 12.68% to 22.24% compared to classical\nrouters, such as hash router and switch router, without impacting the model\naccuracy.", "published": "2024-01-25 03:36:39", "link": "http://arxiv.org/abs/2401.13920v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Routoo: Learning to Route to Large Language Models Effectively", "abstract": "LLMs with superior response quality--particularly larger or closed-source\nmodels--often come with higher inference costs, making their deployment\ninefficient and costly. Meanwhile, developing foundational LLMs from scratch is\nbecoming increasingly resource-intensive and impractical for many applications.\nTo address the challenge of balancing quality and cost, we introduce Routoo, an\narchitecture designed to optimize the selection of LLMs for specific prompts\nbased on performance, cost, and efficiency. Routoo provides controllability\nover the trade-off between inference cost and quality, enabling significant\nreductions in inference costs for a given quality requirement. Routoo comprises\ntwo key components: a performance predictor and cost-aware selector. The\nperformance predictor is a lightweight LLM that estimates the expected\nperformance of various underlying LLMs on a given prompt without executing\nthem. The cost-aware selector module then selects the most suitable model based\non these predictions and constraints such as cost and latency, significantly\nreducing inference costs for the same quality. We evaluated Routoo using the\nMMLU benchmark across 57 domains employing open-source models. Our results show\nthat Routoo matches the performance of the Mixtral 8x7b model while reducing\ninference costs by one-third. Additionally, by allowing increased costs, Routoo\nsurpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an\naccuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly\nmatches GPT4's performance at half the cost and exceeds it with a 25% cost\nreduction. These outcomes highlight Routoo's potential to significantly reduce\ninference costs without compromising quality, and even to establish new\nstate-of-the-art results by leveraging the collective capabilities of multiple\nLLMs.", "published": "2024-01-25 06:45:32", "link": "http://arxiv.org/abs/2401.13979v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Consistent Natural-Language Explanations via\n  Explanation-Consistency Finetuning", "abstract": "Large language models (LLMs) often generate convincing, fluent explanations.\nHowever, different from humans, they often generate inconsistent explanations\non different inputs. For example, an LLM may generate the explanation \"all\nbirds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile\nanswer \"no\" to the related question \"Can penguins fly?\". Explanations should be\nconsistent across related examples so that they allow a human to simulate the\nLLM's decision process on multiple examples. We propose explanation-consistency\nfinetuning (EC-finetuning), a method that adapts LLMs to generate more\nconsistent natural-language explanations on related examples. EC-finetuning\ninvolves finetuning LLMs on synthetic data that is carefully constructed to\ncontain consistent explanations. Across a variety of question-answering\ndatasets in various domains, EC-finetuning yields a 10.0% relative explanation\nconsistency improvement on four finetuning datasets, and generalizes to seven\nout-of-distribution datasets not seen during finetuning (+4.5% relative). Code\nis available at https://github.com/yandachen/explanation-consistency-finetuning .", "published": "2024-01-25 07:04:30", "link": "http://arxiv.org/abs/2401.13986v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question\n  Understanding and Reasoning", "abstract": "Multi-modal large language models(MLLMs) have achieved remarkable progress\nand demonstrated powerful knowledge comprehension and reasoning abilities.\nHowever, the mastery of domain-specific knowledge, which is essential for\nevaluating the intelligence of MLLMs, continues to be a challenge. Current\nmulti-modal benchmarks for domain-specific knowledge concentrate on\nmultiple-choice questions and are predominantly available in English, which\nimposes limitations on the comprehensiveness of the evaluation. To this end, we\nintroduce CMMU, a novel benchmark for multi-modal and multi-type question\nunderstanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7\nsubjects, covering knowledge from primary to high school. The questions can be\ncategorized into 3 types: multiple-choice, multiple-response, and\nfill-in-the-blank, bringing greater challenges to MLLMs. In addition, we\npropose an evaluation strategy called Positional Error Variance for assessing\nmultiple-choice questions. The strategy aims to perform a quantitative analysis\nof position bias. We evaluate seven open-source MLLMs along with GPT4-V,\nGemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a\nsignificant challenge to the recent MLLMs. The data and code are available at\nhttps://github.com/FlagOpen/CMMU.", "published": "2024-01-25 08:22:10", "link": "http://arxiv.org/abs/2401.14011v3", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation", "abstract": "Retrieval-augmented language models (RaLM) have demonstrated the potential to\nsolve knowledge-intensive natural language processing (NLP) tasks by combining\na non-parametric knowledge base with a parametric language model. Instead of\nfine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to\nthe latest data and better source attribution mechanisms. Among various RaLM\napproaches, iterative RaLM delivers a better generation quality due to a more\nfrequent interaction between the retriever and the language model. Despite the\nbenefits, iterative RaLM usually encounters high overheads due to the frequent\nretrieval step. To this end, we propose RaLMSpec, a speculation-inspired\nframework that provides generic speed-up over iterative RaLM while preserving\nthe same model outputs through speculative retrieval and batched verification.\nBy further incorporating prefetching, optimal speculation stride scheduler, and\nasynchronous verification, RaLMSpec can automatically exploit the acceleration\npotential to the fullest. For naive iterative RaLM serving, extensive\nevaluations over three language models on four downstream QA datasets\ndemonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x,\n1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever,\napproximate dense retriever, and sparse retriever respectively compared with\nthe baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to\n7.59x and 2.45x when the retriever is an exact dense retriever and approximate\ndense retriever, respectively, compared with the baseline.", "published": "2024-01-25 09:06:44", "link": "http://arxiv.org/abs/2401.14021v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "CompactifAI: Extreme Compression of Large Language Models using\n  Quantum-Inspired Tensor Networks", "abstract": "Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly\nin generative Artificial Intelligence (AI), but their immense size poses\nsignificant challenges, such as huge training and inference costs, substantial\nenergy demands, and limitations for on-site deployment. Traditional compression\nmethods such as pruning, distillation, and low-rank approximation focus on\nreducing the effective number of neurons in the network, while quantization\nfocuses on reducing the numerical precision of individual weights to reduce the\nmodel size while keeping the number of neurons fixed. While these compression\nmethods have been relatively successful in practice, there is no compelling\nreason to believe that truncating the number of neurons is an optimal strategy.\nIn this context, this paper introduces CompactifAI, an innovative LLM\ncompression approach using quantum-inspired Tensor Networks that focuses on the\nmodel's correlation space instead, allowing for a more controlled, refined and\ninterpretable model compression. Our method is versatile and can be implemented\nwith - or on top of - other compression techniques. As a benchmark, we\ndemonstrate that a combination of CompactifAI with quantization allows to\nreduce a 93% the memory size of LlaMA 7B, reducing also 70% the number of\nparameters, accelerating 50% the training and 25% the inference times of the\nmodel, and just with a small accuracy drop of 2% - 3%, going much beyond of\nwhat is achievable today by other compression techniques. Our methods also\nallow to perform a refined layer sensitivity profiling, showing that deeper\nlayers tend to be more suitable for tensor network compression, which is\ncompatible with recent observations on the ineffectiveness of those layers for\nLLM performance. Our results imply that standard LLMs are, in fact, heavily\noverparametrized, and do not need to be large at all.", "published": "2024-01-25 11:45:21", "link": "http://arxiv.org/abs/2401.14109v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Convolutional Neural Networks can achieve binary bail judgement\n  classification", "abstract": "There is an evident lack of implementation of Machine Learning (ML) in the\nlegal domain in India, and any research that does take place in this domain is\nusually based on data from the higher courts of law and works with English\ndata. The lower courts and data from the different regional languages of India\nare often overlooked. In this paper, we deploy a Convolutional Neural Network\n(CNN) architecture on a corpus of Hindi legal documents. We perform a bail\nPrediction task with the help of a CNN model and achieve an overall accuracy of\n93\\% which is an improvement on the benchmark accuracy, set by Kapoor et al.\n(2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.", "published": "2024-01-25 12:31:41", "link": "http://arxiv.org/abs/2401.14135v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied\n  Environments via Reinforcement Learning", "abstract": "Despite the impressive performance across numerous tasks, large language\nmodels (LLMs) often fail in solving simple decision-making tasks due to the\nmisalignment of the knowledge in LLMs with environments. On the contrary,\nreinforcement learning (RL) agents learn policies from scratch, which makes\nthem always align with environments but difficult to incorporate prior\nknowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a\nnovel general online framework that deploys LLMs as decision-making agents to\nefficiently interact and align with embodied environments via RL without\nrequiring any prepared datasets or prior knowledge of the environments.\nFirstly, we query the joint probabilities of each valid action with LLMs to\nform behavior policies. Then, to enhance the stability and robustness of the\npolicies, we propose two normalization methods and summarize four prompt design\nprinciples. Finally, we design a novel parameter-efficient training\narchitecture where the actor and critic share one frozen LLM equipped with\nlow-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to\nevaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency\nand performance compared to the conventional RL method, PPO, and prompt tuning\nmethod, SayCan, in both classical decision-making environment, Overcooked, and\nsimulated household environment, VirtualHome. ii) Benefiting from LLMs'\nopen-vocabulary feature, TWOSOME shows superior generalization ability to\nunseen tasks. iii) Under our framework, there is no significant loss of the\nLLMs' original ability during online PPO finetuning.", "published": "2024-01-25 13:03:20", "link": "http://arxiv.org/abs/2401.14151v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The\n  Rise of Code Intelligence", "abstract": "The rapid development of large language models has revolutionized code\nintelligence in software development. However, the predominance of\nclosed-source models has restricted extensive research and development. To\naddress this, we introduce the DeepSeek-Coder series, a range of open-source\ncode models with sizes from 1.3B to 33B, trained from scratch on 2 trillion\ntokens. These models are pre-trained on a high-quality project-level code\ncorpus and employ a fill-in-the-blank task with a 16K window to enhance code\ngeneration and infilling. Our extensive evaluations demonstrate that\nDeepSeek-Coder not only achieves state-of-the-art performance among open-source\ncode models across multiple benchmarks but also surpasses existing\nclosed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted\ncommercial use.", "published": "2024-01-25 14:17:53", "link": "http://arxiv.org/abs/2401.14196v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Assessing the Portability of Parameter Matrices Trained by\n  Parameter-Efficient Finetuning Methods", "abstract": "As the cost of training ever larger language models has grown, so has the\ninterest in reusing previously learnt knowledge. Transfer learning methods have\nshown how reusing non-task-specific knowledge can help in subsequent\ntask-specific learning. In this paper, we investigate the inverse: porting\nwhole functional modules that encode task-specific knowledge from one model to\nanother. We designed a study comprising 1,440 training/testing runs to test the\nportability of modules trained by parameter-efficient finetuning (PEFT)\ntechniques, using sentiment analysis as an example task. We test portability in\na wide range of scenarios, involving different PEFT techniques and different\npretrained host models, among other dimensions. We compare the performance of\nported modules with that of equivalent modules trained (i) from scratch, and\n(ii) from parameters sampled from the same distribution as the ported module.\nWe find that the ported modules far outperform the two alternatives tested, but\nthat there are interesting performance differences between the four PEFT\ntechniques. We conclude that task-specific knowledge in the form of\nstructurally modular sets of parameters as produced by PEFT techniques is\nhighly portable, but that degree of success depends on type of PEFT and on\ndifferences between originating and receiving pretrained models.", "published": "2024-01-25 15:11:07", "link": "http://arxiv.org/abs/2401.14228v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Demystifying Chains, Trees, and Graphs of Thoughts", "abstract": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.", "published": "2024-01-25 16:34:00", "link": "http://arxiv.org/abs/2401.14295v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation", "abstract": "The lack of high-quality data for content-grounded generation tasks has been\nidentified as a major obstacle to advancing these tasks. To address this gap,\nwe propose Genie, a novel method for automatically generating high-quality\ncontent-grounded data. It consists of three stages: (a) Content Preparation,\n(b) Generation: creating task-specific examples from the content (e.g.,\nquestion-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\nthe quality and faithfulness of the generated data. We showcase this\nmethodology by generating three large-scale synthetic data, making wishes, for\nLong-Form Question-Answering (LFQA), summarization, and information extraction.\nIn a human evaluation, our generated data was found to be natural and of high\nquality. Furthermore, we compare models trained on our data with models trained\non human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\nSummarization. We show that our models are on par with or outperforming models\ntrained on human-generated data and consistently outperforming them in\nfaithfulness. Finally, we applied our method to create LFQA data within the\nmedical domain and compared a model trained on it with models trained on other\ndomains.", "published": "2024-01-25 18:14:57", "link": "http://arxiv.org/abs/2401.14367v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced\n  Understanding and Generation", "abstract": "The recent advances in natural language processing have predominantly favored\nwell-resourced English-centric models, resulting in a significant gap with\nlow-resource languages. In this work, we introduce the language model TURNA,\nwhich is developed for the low-resource language Turkish and is capable of both\nnatural language understanding and generation tasks. TURNA is pretrained with\nan encoder-decoder architecture based on the unified framework UL2 with a\ndiverse corpus that we specifically curated for this purpose. We evaluated\nTURNA with three generation tasks and five understanding tasks for Turkish. The\nresults show that TURNA outperforms several multilingual models in both\nunderstanding and generation tasks, and competes with monolingual Turkish\nmodels in understanding tasks. TURNA is made available at\nhttps://huggingface.co/boun-tabi-LMG/TURNA .", "published": "2024-01-25 18:24:13", "link": "http://arxiv.org/abs/2401.14373v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Sensitivities and Inconsistent Predictions: Measuring the\n  Fragility of NLI Models", "abstract": "Recent studies of the emergent capabilities of transformer-based Natural\nLanguage Understanding (NLU) models have indicated that they have an\nunderstanding of lexical and compositional semantics. We provide evidence that\nsuggests these claims should be taken with a grain of salt: we find that\nstate-of-the-art Natural Language Inference (NLI) models are sensitive towards\nminor semantics preserving surface-form variations, which lead to sizable\ninconsistent model decisions during inference. Notably, this behaviour differs\nfrom valid and in-depth comprehension of compositional semantics, however does\nneither emerge when evaluating model accuracy on standard benchmarks nor when\nprobing for syntactic, monotonic, and logically robust reasoning. We propose a\nnovel framework to measure the extent of semantic sensitivity. To this end, we\nevaluate NLI models on adversarially generated examples containing minor\nsemantics-preserving surface-form input noise. This is achieved using\nconditional text generation, with the explicit condition that the NLI model\npredicts the relationship between the original and adversarial inputs as a\nsymmetric equivalence entailment. We systematically study the effects of the\nphenomenon across NLI models for $\\textbf{in-}$ and $\\textbf{out-of-}$ domain\nsettings. Our experiments show that semantic sensitivity causes performance\ndegradations of $12.92\\%$ and $23.71\\%$ average over $\\textbf{in-}$ and\n$\\textbf{out-of-}$ domain settings, respectively. We further perform ablation\nstudies, analysing this phenomenon across models, datasets, and variations in\ninference and show that semantic sensitivity can lead to major inconsistency\nwithin model predictions.", "published": "2024-01-25 14:47:05", "link": "http://arxiv.org/abs/2401.14440v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Wordflow: Social Prompt Engineering for Large Language Models", "abstract": "Large language models (LLMs) require well-crafted prompts for effective use.\nPrompt engineering, the process of designing prompts, is challenging,\nparticularly for non-experts who are less familiar with AI technologies. While\nresearchers have proposed techniques and tools to assist LLM users in prompt\ndesign, these works primarily target AI application developers rather than\nnon-experts. To address this research gap, we propose social prompt\nengineering, a novel paradigm that leverages social computing techniques to\nfacilitate collaborative prompt design. To investigate social prompt\nengineering, we introduce Wordflow, an open-source and social text editor that\nenables everyday users to easily create, run, share, and discover LLM prompts.\nAdditionally, by leveraging modern web technologies, Wordflow allows users to\nrun LLMs locally and privately in their browsers. Two usage scenarios highlight\nhow social prompt engineering and our tool can enhance laypeople's interaction\nwith LLMs. Wordflow is publicly accessible at\nhttps://poloclub.github.io/wordflow.", "published": "2024-01-25 18:58:11", "link": "http://arxiv.org/abs/2401.14447v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "K-QA: A Real-World Medical Q&A Benchmark", "abstract": "Ensuring the accuracy of responses provided by large language models (LLMs)\nis crucial, particularly in clinical settings where incorrect information may\ndirectly impact patient health. To address this challenge, we construct K-QA, a\ndataset containing 1,212 patient questions originating from real-world\nconversations held on K Health (an AI-driven clinical platform). We employ a\npanel of in-house physicians to answer and manually decompose a subset of K-QA\ninto self-contained statements. Additionally, we formulate two NLI-based\nevaluation metrics approximating recall and precision: (1) comprehensiveness,\nmeasuring the percentage of essential clinical information in the generated\nanswer and (2) hallucination rate, measuring the number of statements from the\nphysician-curated response contradicted by the LLM answer. Finally, we use K-QA\nalong with these metrics to evaluate several state-of-the-art models, as well\nas the effect of in-context learning and medically-oriented augmented retrieval\nschemes developed by the authors. Our findings indicate that in-context\nlearning improves the comprehensiveness of the models, and augmented retrieval\nis effective in reducing hallucinations. We make K-QA available to to the\ncommunity to spur research into medically accurate NLP applications.", "published": "2024-01-25 20:11:04", "link": "http://arxiv.org/abs/2401.14493v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do", "abstract": "Advances in the performance of large language models (LLMs) have led some\nresearchers to propose the emergence of theory of mind (ToM) in artificial\nintelligence (AI). LLMs can attribute beliefs, desires, intentions, and\nemotions, and they will improve in their accuracy. Rather than employing the\ncharacteristically human method of empathy, they learn to attribute mental\nstates by recognizing linguistic patterns in a dataset that typically do not\ninclude that individual. We ask whether LLMs' inability to empathize precludes\nthem from honoring an individual's right to be an exception, that is, from\nmaking assessments of character and predictions of behavior that reflect\nappropriate sensitivity to a person's individuality. Can LLMs seriously\nconsider an individual's claim that their case is different based on internal\nmental states like beliefs, desires, and intentions, or are they limited to\njudging that case based on its similarities to others? We propose that the\nmethod of empathy has special significance for honoring the right to be an\nexception that is distinct from the value of predictive accuracy, at which LLMs\nexcel. We conclude by considering whether using empathy to consider exceptional\ncases has intrinsic or merely practical value and we introduce conceptual and\nempirical avenues for advancing this investigation.", "published": "2024-01-25 21:30:06", "link": "http://arxiv.org/abs/2401.14523v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Relative Value Biases in Large Language Models", "abstract": "Studies of reinforcement learning in humans and animals have demonstrated a\npreference for options that yielded relatively better outcomes in the past,\neven when those options are associated with lower absolute reward. The present\nstudy tested whether large language models would exhibit a similar bias. We had\ngpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between\npairs of options with the goal of maximizing payoffs. A complete record of\nprevious outcomes was included in each prompt. Both models exhibited relative\nvalue decision biases similar to those observed in humans and animals. Making\nrelative comparisons among outcomes more explicit magnified the bias, whereas\nprompting the models to estimate expected outcomes caused the bias to\ndisappear. These results have implications for the potential mechanisms that\ncontribute to context-dependent choice in human agents.", "published": "2024-01-25 21:49:32", "link": "http://arxiv.org/abs/2401.14530v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Modelling Approaches to Adaptive Machine Translation", "abstract": "Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nin-domain data scarcity is common in translation settings, due to the lack of\nspecialised datasets and terminology, or inconsistency and inaccuracy of\navailable in-domain translations. In such scenarios where there is insufficient\nin-domain data to fine-tune MT models, producing translations that are\nconsistent with the relevant context is challenging. While real-time adaptation\ncan make use of smaller amounts of in-domain data to improve the translation on\nthe fly, it remains challenging due to supported context limitations and\nefficiency constraints. Large language models (LLMs) have recently shown\ninteresting capabilities of in-context learning, where they learn to replicate\ncertain input-output text generation patterns, without further fine-tuning.\nSuch capabilities have opened new horizons for domain-specific data\naugmentation and real-time adaptive MT. This work attempts to address two main\nrelevant questions: 1) in scenarios involving human interaction and continuous\nfeedback, can we employ language models to improve the quality of adaptive MT\nat inference time? and 2) in the absence of sufficient in-domain data, can we\nuse pre-trained large-scale language models to improve the process of MT domain\nadaptation?", "published": "2024-01-25 23:02:54", "link": "http://arxiv.org/abs/2401.14559v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Beyond Behaviorist Representational Harms: A Plan for Measurement and\n  Mitigation", "abstract": "Algorithmic harms are commonly categorized as either allocative or\nrepresentational. This study specifically addresses the latter, focusing on an\nexamination of current definitions of representational harms to discern what is\nincluded and what is not. This analysis motivates our expansion beyond\nbehavioral definitions to encompass harms to cognitive and affective states.\nThe paper outlines high-level requirements for measurement: identifying the\nnecessary expertise to implement this approach and illustrating it through a\ncase study. Our work highlights the unique vulnerabilities of large language\nmodels to perpetrating representational harms, particularly when these harms go\nunmeasured and unmitigated. The work concludes by presenting proposed\nmitigations and delineating when to employ them. The overarching aim of this\nresearch is to establish a framework for broadening the definition of\nrepresentational harms and to translate insights from fairness research into\npractical measurement and mitigation praxis.", "published": "2024-01-25 00:54:10", "link": "http://arxiv.org/abs/2402.01705v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech\n  Generators", "abstract": "The rapid and wide-scale adoption of AI to generate human speech poses a\nrange of significant ethical and safety risks to society that need to be\naddressed. For example, a growing number of speech generation incidents are\nassociated with swatting attacks in the United States, where anonymous\nperpetrators create synthetic voices that call police officers to close down\nschools and hospitals, or to violently gain access to innocent citizens' homes.\nIncidents like this demonstrate that multimodal generative AI risks and harms\ndo not exist in isolation, but arise from the interactions of multiple\nstakeholders and technical AI systems. In this paper we analyse speech\ngeneration incidents to study how patterns of specific harms arise. We find\nthat specific harms can be categorised according to the exposure of affected\nindividuals, that is to say whether they are a subject of, interact with,\nsuffer due to, or are excluded from speech generation systems. Similarly,\nspecific harms are also a consequence of the motives of the creators and\ndeployers of the systems. Based on these insights we propose a conceptual\nframework for modelling pathways to ethical and safety harms of AI, which we\nuse to develop a taxonomy of harms of speech generators. Our relational\napproach captures the complexity of risks and harms in sociotechnical AI\nsystems, and yields a taxonomy that can support appropriate policy\ninterventions and decision making for the responsible development and release\nof speech generation models.", "published": "2024-01-25 11:47:06", "link": "http://arxiv.org/abs/2402.01708v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Socially Aware Synthetic Data Generation for Suicidal Ideation Detection\n  Using Large Language Models", "abstract": "Suicidal ideation detection is a vital research area that holds great\npotential for improving mental health support systems. However, the sensitivity\nsurrounding suicide-related data poses challenges in accessing large-scale,\nannotated datasets necessary for training effective machine learning models. To\naddress this limitation, we introduce an innovative strategy that leverages the\ncapabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to\ncreate synthetic data for suicidal ideation detection. Our data generation\napproach is grounded in social factors extracted from psychology literature and\naims to ensure coverage of essential information related to suicidal ideation.\nIn our study, we benchmarked against state-of-the-art NLP classification\nmodels, specifically, those centered around the BERT family structures. When\ntrained on the real-world dataset, UMD, these conventional models tend to yield\nF1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed\nby social factors, offers consistent F1-scores of 0.82 for both models,\nsuggesting that the richness of topics in synthetic data can bridge the\nperformance gap across different model complexities. Most impressively, when we\ncombined a mere 30% of the UMD dataset with our synthetic data, we witnessed a\nsubstantial increase in performance, achieving an F1-score of 0.88 on the UMD\ntest set. Such results underscore the cost-effectiveness and potential of our\napproach in confronting major challenges in the field, such as data scarcity\nand the quest for diversity in data representation.", "published": "2024-01-25 18:25:05", "link": "http://arxiv.org/abs/2402.01712v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting Large Language Models for Zero-Shot Clinical Prediction with\n  Structured Longitudinal Electronic Health Record Data", "abstract": "The inherent complexity of structured longitudinal Electronic Health Records\n(EHR) data poses a significant challenge when integrated with Large Language\nModels (LLMs), which are traditionally tailored for natural language\nprocessing. Motivated by the urgent need for swift decision-making during new\ndisease outbreaks, where traditional predictive models often fail due to a lack\nof historical data, this research investigates the adaptability of LLMs, like\nGPT-4, to EHR data. We particularly focus on their zero-shot capabilities,\nwhich enable them to make predictions in scenarios in which they haven't been\nexplicitly trained. In response to the longitudinal, sparse, and\nknowledge-infused nature of EHR data, our prompting approach involves taking\ninto account specific EHR characteristics such as units and reference ranges,\nand employing an in-context learning strategy that aligns with clinical\ncontexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets\ndemonstrate that with our elaborately designed prompting framework, LLMs can\nimprove prediction performance in key tasks such as mortality, length-of-stay,\nand 30-day readmission by about 35\\%, surpassing ML models in few-shot\nsettings. Our research underscores the potential of LLMs in enhancing clinical\ndecision-making, especially in urgent healthcare situations like the outbreak\nof emerging diseases with no labeled data. The code is publicly available at\nhttps://github.com/yhzhu99/llm4healthcare for reproducibility.", "published": "2024-01-25 20:14:50", "link": "http://arxiv.org/abs/2402.01713v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TrICy: Trigger-guided Data-to-text Generation with Intent aware\n  Attention-Copy", "abstract": "Data-to-text (D2T) generation is a crucial task in many natural language\nunderstanding (NLU) applications and forms the foundation of task-oriented\ndialog systems. In the context of conversational AI solutions that can work\ndirectly with local data on the user's device, architectures utilizing large\npre-trained language models (PLMs) are impractical for on-device deployment due\nto a high memory footprint. To this end, we propose TrICy, a novel lightweight\nframework for an enhanced D2T task that generates text sequences based on the\nintent in context and may further be guided by user-provided triggers. We\nleverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words\naccurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L:\n70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom\ndataset related to text messaging applications, showcase our architecture's\neffectiveness. Moreover, we show that by leveraging an optional trigger input,\ndata-to-text generation quality increases significantly and achieves the new\nSOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that\nTrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively\nover LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some\nscenarios, performance improvement due to triggers is observed even when they\nare absent in training.", "published": "2024-01-25 20:17:06", "link": "http://arxiv.org/abs/2402.01714v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating GPT-3.5's Awareness and Summarization Abilities for European\n  Constitutional Texts with Shared Topics", "abstract": "Constitutions are foundational legal documents that underpin the governmental\nand societal structures. As such, they are a reflection of a nation's cultural\nand social uniqueness, but also contribute to establish topics of universal\nimportance, like citizens' rights and duties (RD). In this work, using the\nrenowned GPT-3.5, we leverage generative large language models to understand\nconstitutional passages that transcend national boundaries. A key contribution\nof our study is the introduction of a novel application of abstractive\nsummarization on a multi-source collection of constitutional texts, with a\nfocus on European countries' constitution passages related to RD topics. Our\nresults show the meaningfulness of GPT-3.5 to produce informative, coherent and\nfaithful summaries capturing RD topics across European countries.", "published": "2024-01-25 21:34:53", "link": "http://arxiv.org/abs/2401.14524v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Intelli-Z: Toward Intelligible Zero-Shot TTS", "abstract": "Although numerous recent studies have suggested new frameworks for zero-shot\nTTS using large-scale, real-world data, studies that focus on the\nintelligibility of zero-shot TTS are relatively scarce. Zero-shot TTS demands\nadditional efforts to ensure clear pronunciation and speech quality due to its\ninherent requirement of replacing a core parameter (speaker embedding or\nacoustic prompt) with a new one at the inference stage. In this study, we\npropose a zero-shot TTS model focused on intelligibility, which we refer to as\nIntelli-Z. Intelli-Z learns speaker embeddings by using multi-speaker TTS as\nits teacher and is trained with a cycle-consistency loss to include mismatched\ntext-speech pairs for training. Additionally, it selectively aggregates speaker\nembeddings along the temporal dimension to minimize the interference of the\ntext content of reference speech at the inference stage. We substantiate the\neffectiveness of the proposed methods with an ablation study. The Mean Opinion\nScore (MOS) increases by 9% for unseen speakers when the first two methods are\napplied, and it further improves by 16% when selective temporal aggregation is\napplied.", "published": "2024-01-25 03:37:23", "link": "http://arxiv.org/abs/2401.13921v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Combined Generative and Predictive Modeling for Speech Super-resolution", "abstract": "Speech super-resolution (SR) is the task that restores high-resolution speech\nfrom low-resolution input. Existing models employ simulated data and\nconstrained experimental settings, which limit generalization to real-world SR.\nPredictive models are known to perform well in fixed experimental settings, but\ncan introduce artifacts in adverse conditions. On the other hand, generative\nmodels learn the distribution of target data and have a better capacity to\nperform well on unseen conditions. In this study, we propose a novel two-stage\napproach that combines the strengths of predictive and generative models.\nSpecifically, we employ a diffusion-based model that is conditioned on the\noutput of a predictive model. Our experiments demonstrate that the model\nsignificantly outperforms single-stage counterparts and existing strong\nbaselines on benchmark SR datasets. Furthermore, we introduce a repainting\ntechnique during the inference of the diffusion process, enabling the proposed\nmodel to regenerate high-frequency components even in mismatched conditions. An\nadditional contribution is the collection of and evaluation on real SR\nrecordings, using the same microphone at different native sampling rates. We\nmake this dataset freely accessible, to accelerate progress towards real-world\nspeech super-resolution.", "published": "2024-01-25 16:04:51", "link": "http://arxiv.org/abs/2401.14269v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Design of Input Condition Invariant Speech Enhancement", "abstract": "Building a single universal speech enhancement (SE) system that can handle\narbitrary input is a demanded but underexplored research topic. Towards this\nultimate goal, one direction is to build a single model that handles diverse\naudio duration, sampling frequencies, and microphone variations in noisy and\nreverberant scenarios, which we define here as \"input condition invariant SE\".\nSuch a model was recently proposed showing promising performance; however, its\nmulti-channel performance degraded severely in real conditions. In this paper\nwe propose novel architectures to improve the input condition invariant SE\nmodel so that performance in simulated conditions remains competitive while\nreal condition degradation is much mitigated. For this purpose, we redesign the\nkey components that comprise such a system. First, we identify that the\nchannel-modeling module's generalization to unseen scenarios can be sub-optimal\nand redesign this module. We further introduce a two-stage training strategy to\nenhance training efficiency. Second, we propose two novel dual-path\ntime-frequency blocks, demonstrating superior performance with fewer parameters\nand computational costs compared to the existing method. All proposals\ncombined, experiments on various public datasets validate the efficacy of the\nproposed model, with significantly improved performance on real conditions.\nRecipe with full model details is released at https://github.com/espnet/espnet.", "published": "2024-01-25 16:07:17", "link": "http://arxiv.org/abs/2401.14271v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VALL-T: Decoder-Only Generative Transducer for Robust and\n  Decoding-Controllable Text-to-Speech", "abstract": "Recent TTS models with decoder-only Transformer architecture, such as\nSPEAR-TTS and VALL-E, achieve impressive naturalness and demonstrate the\nability for zero-shot adaptation given a speech prompt. However, such\ndecoder-only TTS models lack monotonic alignment constraints, sometimes leading\nto hallucination issues such as mispronunciation, word skipping and repeating.\nTo address this limitation, we propose VALL-T, a generative Transducer model\nthat introduces shifting relative position embeddings for input phoneme\nsequence, explicitly indicating the monotonic generation process while\nmaintaining the architecture of decoder-only Transformer. Consequently, VALL-T\nretains the capability of prompt-based zero-shot adaptation and demonstrates\nbetter robustness against hallucinations with a relative reduction of 28.3% in\nthe word error rate.", "published": "2024-01-25 17:19:01", "link": "http://arxiv.org/abs/2401.14321v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down\n  Fusion", "abstract": "Audio-visual speech separation has gained significant traction in recent\nyears due to its potential applications in various fields such as speech\nrecognition, diarization, scene analysis and assistive technologies. Designing\na lightweight audio-visual speech separation network is important for\nlow-latency applications, but existing methods often require higher\ncomputational costs and more parameters to achieve better separation\nperformance. In this paper, we present an audio-visual speech separation model\ncalled Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for\naudio-visual speech separation, which builds upon the architecture of TDANet,\nan audio-only speech separation method. TDANet serves as the architectural\nfoundation for the auditory and visual networks within TDFNet, offering an\nefficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet\nachieves a performance increase of up to 10\\% across all performance metrics\ncompared with the previous SOTA method CTCNet. Remarkably, these results are\nachieved using fewer parameters and only 28\\% of the multiply-accumulate\noperations (MACs) of CTCNet. In essence, our method presents a highly effective\nand efficient solution to the challenges of speech separation within the\naudio-visual domain, making significant strides in harnessing visual\ninformation optimally.", "published": "2024-01-25 13:47:22", "link": "http://arxiv.org/abs/2401.14185v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ICASSP 2024 Speech Signal Improvement Challenge", "abstract": "The ICASSP 2024 Speech Signal Improvement Grand Challenge is intended to\nstimulate research in the area of improving the speech signal quality in\ncommunication systems. This marks our second challenge, building upon the\nsuccess from the previous ICASSP 2023 Grand Challenge. We enhance the\ncompetition by introducing a dataset synthesizer, enabling all participating\nteams to start at a higher baseline, an objective metric for our extended P.804\ntests, transcripts for the 2023 test set, and we add Word Accuracy (WAcc) as a\nmetric. We evaluate a total of 13 systems in the real-time track and 11 systems\nin the non-real-time track using both subjective P.804 and objective Word\nAccuracy metrics.", "published": "2024-01-25 18:08:00", "link": "http://arxiv.org/abs/2401.14444v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Musical Roots: Applying Audio Embeddings to Empower Influence\n  Attribution for a Generative Music Model", "abstract": "Every artist has a creative process that draws inspiration from previous\nartists and their works. Today, \"inspiration\" has been automated by generative\nmusic models. The black box nature of these models obscures the identity of the\nworks that influence their creative output. As a result, users may\ninadvertently appropriate, misuse, or copy existing artists' works. We\nestablish a replicable methodology to systematically identify similar pieces of\nmusic audio in a manner that is useful for understanding training data\nattribution. A key aspect of our approach is to harness an effective music\naudio similarity measure. We compare the effect of applying CLMR and CLAP\nembeddings to similarity measurement in a set of 5 million audio clips used to\ntrain VampNet, a recent open source generative music model. We validate this\napproach with a human listening study. We also explore the effect that\nmodifications of an audio example (e.g., pitch shifting, time stretching,\nbackground noise) have on similarity measurements. This work is foundational to\nincorporating automated influence attribution into generative modeling, which\npromises to let model creators and users move from ignorant appropriation to\ninformed creation. Audio samples that accompany this paper are available at\nhttps://tinyurl.com/exploring-musical-roots.", "published": "2024-01-25 22:20:42", "link": "http://arxiv.org/abs/2401.14542v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
