{"title": "FAQ-based Question Answering via Knowledge Anchors", "abstract": "Question answering (QA) aims to understand questions and find appropriate\nanswers. In real-world QA systems, Frequently Asked Question (FAQ) based QA is\nusually a practical and effective solution, especially for some complicated\nquestions (e.g., How and Why). Recent years have witnessed the great successes\nof knowledge graphs (KGs) in KBQA systems, while there are still few works\nfocusing on making full use of KGs in FAQ-based QA. In this paper, we propose a\nnovel Knowledge Anchor based Question Answering (KAQA) framework for FAQ-based\nQA to better understand questions and retrieve more appropriate answers. More\nspecifically, KAQA mainly consists of three modules: knowledge graph\nconstruction, query anchoring and query-document matching. We consider entities\nand triples of KGs in texts as knowledge anchors to precisely capture the core\nsemantics, which brings in higher precision and better interpretability. The\nmulti-channel matching strategy also enables most sentence matching models to\nbe flexibly plugged in our KAQA framework to fit different real-world\ncomputation limitations. In experiments, we evaluate our models on both offline\nand online query-document matching tasks on a real-world FAQ-based QA system in\nWeChat Search, with detailed analysis, ablation tests and case studies. The\nsignificant improvements confirm the effectiveness and robustness of the KAQA\nframework in real-world FAQ-based QA.", "published": "2019-11-14 04:18:55", "link": "http://arxiv.org/abs/1911.05930v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Recurrent Units for Cloze-style Reading Comprehension", "abstract": "Recurrent Neural Networks (RNN) are known as powerful models for handling\nsequential data, and especially widely utilized in various natural language\nprocessing tasks. In this paper, we propose Contextual Recurrent Units (CRU)\nfor enhancing local contextual representations in neural networks. The proposed\nCRU injects convolutional neural networks (CNN) into the recurrent units to\nenhance the ability to model the local context and reducing word ambiguities\neven in bi-directional RNNs. We tested our CRU model on sentence-level and\ndocument-level modeling NLP tasks: sentiment classification and reading\ncomprehension. Experimental results show that the proposed CRU model could give\nsignificant improvements over traditional CNN or RNN models, including\nbidirectional conditions, as well as various state-of-the-art systems on both\ntasks, showing its promising future of extensibility to other NLP tasks as\nwell.", "published": "2019-11-14 06:33:24", "link": "http://arxiv.org/abs/1911.05960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training a code-switching language model with monolingual data", "abstract": "A lack of code-switching data complicates the training of code-switching (CS)\nlanguage models. We propose an approach to train such CS language models on\nmonolingual data only. By constraining and normalizing the output projection\nmatrix in RNN-based language models, we bring embeddings of different languages\ncloser to each other. Numerical and visualization results show that the\nproposed approaches remarkably improve the performance of CS language models\ntrained on monolingual data. The proposed approaches are comparable or even\nbetter than training CS language models with artificially generated CS data. We\nadditionally use unsupervised bilingual word translation to analyze whether\nsemantically equivalent words in different languages are mapped together.", "published": "2019-11-14 09:09:10", "link": "http://arxiv.org/abs/1911.06003v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with\n  Minimal Resources", "abstract": "For languages with no annotated resources, transferring knowledge from\nrich-resource languages is an effective solution for named entity recognition\n(NER). While all existing methods directly transfer from source-learned model\nto a target language, in this paper, we propose to fine-tune the learned model\nwith a few similar examples given a test case, which could benefit the\nprediction by leveraging the structural and semantic information conveyed in\nsuch similar examples. To this end, we present a meta-learning algorithm to\nfind a good model parameter initialization that could fast adapt to the given\ntest case and propose to construct multiple pseudo-NER tasks for meta-training\nby computing sentence similarities. To further improve the model's\ngeneralization ability across different languages, we introduce a masking\nscheme and augment the loss function with an additional maximum term during\nmeta-training. We conduct extensive experiments on cross-lingual named entity\nrecognition with minimal resources over five target languages. The results show\nthat our approach significantly outperforms existing state-of-the-art methods\nacross the board.", "published": "2019-11-14 15:01:15", "link": "http://arxiv.org/abs/1911.06161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Eighth Dialog System Technology Challenge", "abstract": "This paper introduces the Eighth Dialog System Technology Challenge. In line\nwith recent challenges, the eighth edition focuses on applying end-to-end\ndialog technologies in a pragmatic way for multi-domain task-completion, noetic\nresponse selection, audio visual scene-aware dialog, and schema-guided dialog\nstate tracking tasks. This paper describes the task definition, provided\ndatasets, and evaluation set-up for each track. We also summarize the results\nof the submitted systems to highlight the overall trends of the\nstate-of-the-art technologies for the tasks.", "published": "2019-11-14 21:42:48", "link": "http://arxiv.org/abs/1911.06394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Persona Consistent Dialogues by Exploiting Natural Language\n  Inference", "abstract": "Consistency is one of the major challenges faced by dialogue agents. A\nhuman-like dialogue agent should not only respond naturally, but also maintain\na consistent persona. In this paper, we exploit the advantages of natural\nlanguage inference (NLI) technique to address the issue of generating persona\nconsistent dialogues. Different from existing work that re-ranks the retrieved\nresponses through an NLI model, we cast the task as a reinforcement learning\nproblem and propose to exploit the NLI signals from response-persona pairs as\nrewards for the process of dialogue generation. Specifically, our generator\nemploys an attention-based encoder-decoder to generate persona-based responses.\nOur evaluator consists of two components: an adversarially trained naturalness\nmodule and an NLI based consistency module. Moreover, we use another\nwell-performed NLI model in the evaluation of persona-consistency. Experimental\nresults on both human and automatic metrics, including the model-based\nconsistency evaluation, demonstrate that the proposed approach outperforms\nstrong generative baselines, especially in the persona-consistency of generated\nresponses.", "published": "2019-11-14 01:47:53", "link": "http://arxiv.org/abs/1911.05889v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Ethanos: Lightweight Bootstrapping for Ethereum", "abstract": "As ethereum blockchain has become popular, the number of users and\ntransactions has skyrocketed, causing an explosive increase of its data size.\nAs a result, ordinary clients using PCs or smartphones cannot easily bootstrap\nas a full node, but rely on other full nodes such as the miners to run or\nverify transactions. This may affect the security of ethereum, so light\nbootstrapping techniques such as fast sync has been proposed to download only\nparts of full data, yet the space overhead is still too high. One of the\nbiggest space overhead that cannot easily be reduced is caused by saving the\nstate of all accounts in the block's state trie. Fortunately, we found that\nmore than 90% of accounts are inactive and old transactions are hard to be\nmanipulated. Based on these observations, this paper propose a novel\noptimization technique called ethanos that can reduce bootstrapping cost by\nsweeping inactive accounts periodically and by not downloading old\ntransactions. If an inactive account becomes active, ethanos restore its state\nby running a restoration transaction. Also, ethanos gives incentives for\narchive nodes to maintain the old transactions for possible re-verification. We\nimplemented ethanos by instrumenting the go-ethereum (geth) client and\nevaluated with the real 113 million transactions from 14 million accounts\nbetween 7M-th and 8M-th blocks in ethereum. Our experimental result shows that\nethanos can reduce the size of the account state by half, which, if combined\nwith removing old transactions, may reduce the storage size for bootstrapping\nto around 1GB. This would be reasonable enough for ordinary clients to\nbootstrap on their personal devices.", "published": "2019-11-14 05:55:17", "link": "http://arxiv.org/abs/1911.05953v1", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Iterative Answer Prediction with Pointer-Augmented Multimodal\n  Transformers for TextVQA", "abstract": "Many visual scenes contain text that carries crucial information, and it is\nthus essential to understand text in images for downstream reasoning tasks. For\nexample, a deep water label on a warning sign warns people about the danger in\nthe scene. Recent work has explored the TextVQA task that requires reading and\nunderstanding text in images to answer a question. However, existing approaches\nfor TextVQA are mostly based on custom pairwise fusion mechanisms between a\npair of two modalities and are restricted to a single prediction step by\ncasting TextVQA as a classification task. In this work, we propose a novel\nmodel for the TextVQA task based on a multimodal transformer architecture\naccompanied by a rich representation for text in images. Our model naturally\nfuses different modalities homogeneously by embedding them into a common\nsemantic space where self-attention is applied to model inter- and intra-\nmodality context. Furthermore, it enables iterative answer decoding with a\ndynamic pointer network, allowing the model to form an answer through\nmulti-step prediction instead of one-step classification. Our model outperforms\nexisting approaches on three benchmark datasets for the TextVQA task by a large\nmargin.", "published": "2019-11-14 17:32:10", "link": "http://arxiv.org/abs/1911.06258v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Question-Conditioned Counterfactual Image Generation for VQA", "abstract": "While Visual Question Answering (VQA) models continue to push the\nstate-of-the-art forward, they largely remain black-boxes - failing to provide\ninsight into how or why an answer is generated. In this ongoing work, we\npropose addressing this shortcoming by learning to generate counterfactual\nimages for a VQA model - i.e. given a question-image pair, we wish to generate\na new image such that i) the VQA model outputs a different answer, ii) the new\nimage is minimally different from the original, and iii) the new image is\nrealistic. Our hope is that providing such counterfactual examples allows users\nto investigate and understand the VQA model's internal mechanisms.", "published": "2019-11-14 19:37:33", "link": "http://arxiv.org/abs/1911.06352v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sparse associative memory based on contextual code learning for\n  disambiguating word senses", "abstract": "In recent literature, contextual pretrained Language Models (LMs)\ndemonstrated their potential in generalizing the knowledge to several Natural\nLanguage Processing (NLP) tasks including supervised Word Sense Disambiguation\n(WSD), a challenging problem in the field of Natural Language Understanding\n(NLU). However, word representations from these models are still very dense,\ncostly in terms of memory footprint, as well as minimally interpretable. In\norder to address such issues, we propose a new supervised biologically inspired\ntechnique for transferring large pre-trained language model representations\ninto a compressed representation, for the case of WSD. Our produced\nrepresentation contributes to increase the general interpretability of the\nframework and to decrease memory footprint, while enhancing performance.", "published": "2019-11-14 23:31:02", "link": "http://arxiv.org/abs/1911.06415v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Troll Writing as a Linguistic Phenomenon", "abstract": "The current study yielded a number of important findings. We managed to build\na neural network that achieved an accuracy score of 91 per cent in classifying\ntroll and genuine tweets. By means of regression analysis, we identified a\nnumber of features that make a tweet more susceptible to correct labelling and\nfound that they are inherently present in troll tweets as a special type of\ndiscourse. We hypothesised that those features are grounded in the\nsociolinguistic limitations of troll writing, which can be best described as a\ncombination of two factors: speaking with a purpose and trying to mask the\npurpose of speaking. Next, we contended that the orthogonal nature of these\nfactors must necessarily result in the skewed distribution of many different\nlanguage parameters of troll messages. Having chosen as an example distribution\nof the topics and vocabulary associated with those topics, we showed some very\npronounced distributional anomalies, thus confirming our prediction.", "published": "2019-11-14 19:49:50", "link": "http://arxiv.org/abs/1911.08946v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "HUSE: Hierarchical Universal Semantic Embeddings", "abstract": "There is a recent surge of interest in cross-modal representation learning\ncorresponding to images and text. The main challenge lies in mapping images and\ntext to a shared latent space where the embeddings corresponding to a similar\nsemantic concept lie closer to each other than the embeddings corresponding to\ndifferent semantic concepts, irrespective of the modality. Ranking losses are\ncommonly used to create such shared latent space -- however, they do not impose\nany constraints on inter-class relationships resulting in neighboring clusters\nto be completely unrelated. The works in the domain of visual semantic\nembeddings address this problem by first constructing a semantic embedding\nspace based on some external knowledge and projecting image embeddings onto\nthis fixed semantic embedding space. These works are confined only to image\ndomain and constraining the embeddings to a fixed space adds additional burden\non learning. This paper proposes a novel method, HUSE, to learn cross-modal\nrepresentation with semantic information. HUSE learns a shared latent space\nwhere the distance between any two universal embeddings is similar to the\ndistance between their corresponding class embeddings in the semantic embedding\nspace. HUSE also uses a classification objective with a shared classification\nlayer to make sure that the image and text embeddings are in the same shared\nlatent space. Experiments on UPMC Food-101 show our method outperforms previous\nstate-of-the-art on retrieval, hierarchical precision and classification\nresults.", "published": "2019-11-14 07:45:32", "link": "http://arxiv.org/abs/1911.05978v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Sato: Contextual Semantic Type Detection in Tables", "abstract": "Detecting the semantic types of data columns in relational tables is\nimportant for various data preparation and information retrieval tasks such as\ndata cleaning, schema matching, data discovery, and semantic search. However,\nexisting detection approaches either perform poorly with dirty data, support\nonly a limited number of semantic types, fail to incorporate the table context\nof columns or rely on large sample sizes for training data. We introduce Sato,\na hybrid machine learning model to automatically detect the semantic types of\ncolumns in tables, exploiting the signals from the context as well as the\ncolumn values. Sato combines a deep learning model trained on a large-scale\ntable corpus with topic modeling and structured prediction to achieve\nsupport-weighted and macro average F1 scores of 0.925 and 0.735, respectively,\nexceeding the state-of-the-art performance by a significant margin. We\nextensively analyze the overall and per-type performance of Sato, discussing\nhow individual modeling components, as well as feature categories, contribute\nto its performance.", "published": "2019-11-14 18:51:59", "link": "http://arxiv.org/abs/1911.06311v3", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Radically Compositional Cognitive Concepts", "abstract": "Despite ample evidence that our concepts, our cognitive architecture, and\nmathematics itself are all deeply compositional, few models take advantage of\nthis structure. We therefore propose a radically compositional approach to\ncomputational neuroscience, drawing on the methods of applied category theory.\nWe describe how these tools grant us a means to overcome complexity and improve\ninterpretability, and supply a rigorous common language for scientific\nmodelling, analogous to the type theories of computer science. As a case study,\nwe sketch how to translate from compositional narrative concepts to neural\ncircuits and back again.", "published": "2019-11-14 18:20:36", "link": "http://arxiv.org/abs/1911.06602v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "q-bio.NC"}
{"title": "Query Expansion for Patent Searching using Word Embedding and\n  Professional Crowdsourcing", "abstract": "The patent examination process includes a search of previous work to verify\nthat a patent application describes a novel invention. Patent examiners\nprimarily use keyword-based searches to uncover prior art. A critical part of\nkeyword searching is query expansion, which is the process of including\nalternate terms such as synonyms and other related words, since the same\nconcepts are often described differently in the literature. Patent terminology\nis often domain specific. By curating technology-specific corpora and training\nword embedding models based on these corpora, we are able to automatically\nidentify the most relevant expansions of a given word or phrase. We compare the\nperformance of several automated query expansion techniques against expert\nspecified expansions. Furthermore, we explore a novel mechanism to extract\nrelated terms not just based on one input term but several terms in conjunction\nby computing their centroid and identifying the nearest neighbors to this\ncentroid. Highly skilled patent examiners are often the best and most reliable\nsource of identifying related terms. By designing a user interface that allows\nexaminers to interact with the word embedding suggestions, we are able to use\nthese interactions to power crowdsourced modes of related terms. Learning from\nusers allows us to overcome several challenges such as identifying words that\nare bleeding edge and have not been published in the corpus yet. This paper\nstudies the effectiveness of word embedding and crowdsourced models across 11\ndisparate technical areas.", "published": "2019-11-14 22:34:02", "link": "http://arxiv.org/abs/1911.11069v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Coincidence, Categorization, and Consolidation: Learning to Recognize\n  Sounds with Minimal Supervision", "abstract": "Humans do not acquire perceptual abilities in the way we train machines.\nWhile machine learning algorithms typically operate on large collections of\nrandomly-chosen, explicitly-labeled examples, human acquisition relies more\nheavily on multimodal unsupervised learning (as infants) and active learning\n(as children). With this motivation, we present a learning framework for sound\nrepresentation and recognition that combines (i) a self-supervised objective\nbased on a general notion of unimodal and cross-modal coincidence, (ii) a\nclustering objective that reflects our need to impose categorical structure on\nour experiences, and (iii) a cluster-based active learning procedure that\nsolicits targeted weak supervision to consolidate categories into relevant\nsemantic classes. By training a combined sound\nembedding/clustering/classification network according to these criteria, we\nachieve a new state-of-the-art unsupervised audio representation and\ndemonstrate up to a 20-fold reduction in the number of labels required to reach\na desired classification performance.", "published": "2019-11-14 02:07:47", "link": "http://arxiv.org/abs/1911.05894v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Scene-Aware Audio Rendering via Deep Acoustic Analysis", "abstract": "We present a new method to capture the acoustic characteristics of real-world\nrooms using commodity devices, and use the captured characteristics to generate\nsimilar sounding sources with virtual models. Given the captured audio and an\napproximate geometric model of a real-world room, we present a novel\nlearning-based method to estimate its acoustic material properties. Our\napproach is based on deep neural networks that estimate the reverberation time\nand equalization of the room from recorded audio. These estimates are used to\ncompute material properties related to room reverberation using a novel\nmaterial optimization objective. We use the estimated acoustic material\ncharacteristics for audio rendering using interactive geometric sound\npropagation and highlight the performance on many real-world scenarios. We also\nperform a user study to evaluate the perceptual similarity between the recorded\nsounds and our rendered audio.", "published": "2019-11-14 17:04:00", "link": "http://arxiv.org/abs/1911.06245v2", "categories": ["cs.SD", "cs.GR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker independence of neural vocoders and their effect on parametric\n  resynthesis speech enhancement", "abstract": "Traditional speech enhancement systems produce speech with compromised\nquality. Here we propose to use the high quality speech generation capability\nof neural vocoders for better quality speech enhancement. We term this\nparametric resynthesis (PR). In previous work, we showed that PR systems\ngenerate high quality speech for a single speaker using two neural vocoders,\nWaveNet and WaveGlow. Both these vocoders are traditionally speaker dependent.\nHere we first show that when trained on data from enough speakers, these\nvocoders can generate speech from unseen speakers, both male and female, with\nsimilar quality as seen speakers in training. Next using these two vocoders and\na new vocoder LPCNet, we evaluate the noise reduction quality of PR on unseen\nspeakers and show that objective signal and overall quality is higher than the\nstate-of-the-art speech enhancement systems Wave-U-Net, Wavenet-denoise, and\nSEGAN. Moreover, in subjective quality, multiple-speaker PR out-performs the\noracle Wiener mask.", "published": "2019-11-14 17:45:44", "link": "http://arxiv.org/abs/1911.06266v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence\n  Modelling", "abstract": "Convolutional neural networks (CNNs) with dilated filters such as the Wavenet\nor the Temporal Convolutional Network (TCN) have shown good results in a\nvariety of sequence modelling tasks. However, efficiently modelling long-term\ndependencies in these sequences is still challenging. Although the receptive\nfield of these models grows exponentially with the number of layers, computing\nthe convolutions over very long sequences of features in each layer is time and\nmemory-intensive, prohibiting the use of longer receptive fields in practice.\nTo increase efficiency, we make use of the \"slow feature\" hypothesis stating\nthat many features of interest are slowly varying over time. For this, we use a\nU-Net architecture that computes features at multiple time-scales and adapt it\nto our auto-regressive scenario by making convolutions causal. We apply our\nmodel (\"Seq-U-Net\") to a variety of tasks including language and audio\ngeneration. In comparison to TCN and Wavenet, our network consistently saves\nmemory and computation time, with speed-ups for training and inference of over\n4x in the audio generation experiment in particular, while achieving a\ncomparable performance in all tasks.", "published": "2019-11-14 21:39:20", "link": "http://arxiv.org/abs/1911.06393v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep learning methods in speaker recognition: a review", "abstract": "This paper summarizes the applied deep learning practices in the field of\nspeaker recognition, both verification and identification. Speaker recognition\nhas been a widely used field topic of speech technology. Many research works\nhave been carried out and little progress has been achieved in the past 5-6\nyears. However, as deep learning techniques do advance in most machine learning\nfields, the former state-of-the-art methods are getting replaced by them in\nspeaker recognition too. It seems that DL becomes the now state-of-the-art\nsolution for both speaker verification and identification. The standard\nx-vectors, additional to i-vectors, are used as baseline in most of the novel\nworks. The increasing amount of gathered data opens up the territory to DL,\nwhere they are the most effective.", "published": "2019-11-14 12:32:07", "link": "http://arxiv.org/abs/1911.06615v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
