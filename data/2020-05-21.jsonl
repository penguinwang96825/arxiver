{"title": "Stance Prediction and Claim Verification: An Arabic Perspective", "abstract": "This work explores the application of textual entailment in news claim\nverification and stance prediction using a new corpus in Arabic. The publicly\navailable corpus comes in two perspectives: a version consisting of 4,547 true\nand false claims and a version consisting of 3,786 pairs (claim, evidence). We\ndescribe the methodology for creating the corpus and the annotation process.\nUsing the introduced corpus, we also develop two machine learning baselines for\ntwo proposed tasks: claim verification and stance prediction. Our best model\nutilizes pretraining (BERT) and achieves 76.7 F1 on the stance prediction task\nand 64.3 F1 on the claim verification task. Our preliminary experiments shed\nsome light on the limits of automatic claim verification that relies on claims\ntext only. Results hint that while the linguistic features and world knowledge\nlearned during pretraining are useful for stance prediction, such learned\nrepresentations from pretraining are insufficient for verifying claims without\naccess to context or evidence.", "published": "2020-05-21 01:17:46", "link": "http://arxiv.org/abs/2005.10410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-to-Text Pre-Training for Data-to-Text Tasks", "abstract": "We study the pre-train + fine-tune strategy for data-to-text tasks. Our\nexperiments indicate that text-to-text pre-training in the form of T5, enables\nsimple, end-to-end transformer based models to outperform pipelined neural\narchitectures tailored for data-to-text generation, as well as alternative\nlanguage model based pre-training techniques such as BERT and GPT-2.\nImportantly, T5 pre-training leads to better generalization, as evidenced by\nlarge improvements on out-of-domain test sets. We hope our work serves as a\nuseful baseline for future research, as transfer learning becomes ever more\nprevalent for data-to-text tasks.", "published": "2020-05-21 02:46:15", "link": "http://arxiv.org/abs/2005.10433v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fluent Response Generation for Conversational Question Answering", "abstract": "Question answering (QA) is an important aspect of open-domain conversational\nagents, garnering specific research focus in the conversational QA (ConvQA)\nsubtask. One notable limitation of recent ConvQA efforts is the response being\nanswer span extraction from the target corpus, thus ignoring the natural\nlanguage generation (NLG) aspect of high-quality conversational agents. In this\nwork, we propose a method for situating QA responses within a SEQ2SEQ NLG\napproach to generate fluent grammatical answer responses while maintaining\ncorrectness. From a technical perspective, we use data augmentation to generate\ntraining data for an end-to-end system. Specifically, we develop Syntactic\nTransformations (STs) to produce question-specific candidate answer responses\nand rank them using a BERT-based classifier (Devlin et al., 2019). Human\nevaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the\nproposed model outperforms baseline CoQA and QuAC models in generating\nconversational responses. We further show our model's scalability by conducting\ntests on the CoQA dataset. The code and data are available at\nhttps://github.com/abaheti95/QADialogSystem.", "published": "2020-05-21 04:57:01", "link": "http://arxiv.org/abs/2005.10464v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LaCulturaNonSiFerma -- Report su uso e la diffusione degli hashtag delle\n  istituzioni culturali italiane durante il periodo di lockdown", "abstract": "This report presents an analysis of #hashtags used by Italian Cultural\nHeritage institutions to promote and communicate cultural content during the\nCOVID-19 lock-down period in Italy. Several activities to support and engage\nusers' have been proposed using social media. Most of these activities present\none or more #hashtags which help to aggregate content and create a community on\nspecific topics. Results show that on one side Italian institutions have been\nvery proactive in adapting to the pandemic scenario and on the other side\nusers' reacted very positively increasing their participation in the proposed\nactivities.", "published": "2020-05-21 09:03:03", "link": "http://arxiv.org/abs/2005.10527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiMWE: Building a Multi-lingual Multi-Word Expression (MWE) Parallel\n  Corpora", "abstract": "Multi-word expressions (MWEs) are a hot topic in research in natural language\nprocessing (NLP), including topics such as MWE detection, MWE decomposition,\nand research investigating the exploitation of MWEs in other NLP fields such as\nMachine Translation. However, the availability of bilingual or multi-lingual\nMWE corpora is very limited. The only bilingual MWE corpora that we are aware\nof is from the PARSEME (PARSing and Multi-word Expressions) EU Project. This is\na small collection of only 871 pairs of English-German MWEs. In this paper, we\npresent multi-lingual and bilingual MWE corpora that we have extracted from\nroot parallel corpora. Our collections are 3,159,226 and 143,042 bilingual MWE\npairs for German-English and Chinese-English respectively after filtering. We\nexamine the quality of these extracted bilingual MWEs in MT experiments. Our\ninitial experiments applying MWEs in MT show improved translation performances\non MWE terms in qualitative analysis and better general evaluation scores in\nquantitative analysis, on both German-English and Chinese-English language\npairs. We follow a standard experimental pipeline to create our MultiMWE\ncorpora which are available online. Researchers can use this free corpus for\ntheir own models or use them in a knowledge base as model features.", "published": "2020-05-21 11:46:44", "link": "http://arxiv.org/abs/2005.10583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Quality Estimation for Neural Machine Translation", "abstract": "Quality Estimation (QE) is an important component in making Machine\nTranslation (MT) useful in real-world applications, as it is aimed to inform\nthe user on the quality of the MT output at test time. Existing approaches\nrequire large amounts of expert annotated data, computation and time for\ntraining. As an alternative, we devise an unsupervised approach to QE where no\ntraining or access to additional resources besides the MT system itself is\nrequired. Different from most of the current work that treats the MT system as\na black box, we explore useful information that can be extracted from the MT\nsystem as a by-product of translation. By employing methods for uncertainty\nquantification, we achieve very good correlation with human judgments of\nquality, rivalling state-of-the-art supervised QE models. To evaluate our\napproach we collect the first dataset that enables work on both black-box and\nglass-box approaches to QE.", "published": "2020-05-21 12:38:06", "link": "http://arxiv.org/abs/2005.10608v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Finite-State Morphology of Kurdish", "abstract": "Morphological analysis is the study of the formation and structure of words.\nIt plays a crucial role in various tasks in Natural Language Processing (NLP)\nand Computational Linguistics (CL) such as machine translation and text and\nspeech generation. Kurdish is a less-resourced multi-dialect Indo-European\nlanguage with highly inflectional morphology. In this paper, as the first\nattempt of its kind, the morphology of the Kurdish language (Sorani dialect) is\ndescribed from a computational point of view. We extract morphological rules\nwhich are transformed into finite-state transducers for generating and\nanalyzing words. The result of this research assists in conducting studies on\nlanguage generation for Kurdish and enhances the Information Retrieval (IR)\ncapacity for the language while leveraging the Kurdish NLP and CL into a more\nadvanced computational level.", "published": "2020-05-21 13:55:07", "link": "http://arxiv.org/abs/2005.10652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RuBQ: A Russian Dataset for Question Answering over Wikidata", "abstract": "The paper presents RuBQ, the first Russian knowledge base question answering\n(KBQA) dataset. The high-quality dataset consists of 1,500 Russian questions of\nvarying complexity, their English machine translations, SPARQL queries to\nWikidata, reference answers, as well as a Wikidata sample of triples containing\nentities with Russian labels. The dataset creation started with a large\ncollection of question-answer pairs from online quizzes. The data underwent\nautomatic filtering, crowd-assisted entity linking, automatic generation of\nSPARQL queries, and their subsequent in-house verification.", "published": "2020-05-21 14:06:15", "link": "http://arxiv.org/abs/2005.10659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in\n  Multitask End-to-End Speech Translation", "abstract": "Speech translation (ST) aims to learn transformations from speech in the\nsource language to the text in the target language. Previous works show that\nmultitask learning improves the ST performance, in which the recognition\ndecoder generates the text of the source language, and the translation decoder\nobtains the final translations based on the output of the recognition decoder.\nBecause whether the output of the recognition decoder has the correct semantics\nis more critical than its accuracy, we propose to improve the multitask ST\nmodel by utilizing word embedding as the intermediate.", "published": "2020-05-21 14:22:35", "link": "http://arxiv.org/abs/2005.10678v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Frankfurt Latin Lexicon: From Morphological Expansion and Word\n  Embeddings to SemioGraphs", "abstract": "In this article we present the Frankfurt Latin Lexicon (FLL), a lexical\nresource for Medieval Latin that is used both for the lemmatization of Latin\ntexts and for the post-editing of lemmatizations. We describe recent advances\nin the development of lemmatizers and test them against the Capitularies corpus\n(comprising Frankish royal edicts, mid-6th to mid-9th century), a corpus\ncreated as a reference for processing Medieval Latin. We also consider the\npost-correction of lemmatizations using a limited crowdsourcing process aimed\nat continuous review and updating of the FLL. Starting from the texts resulting\nfrom this lemmatization process, we describe the extension of the FLL by means\nof word embeddings, whose interactive traversing by means of SemioGraphs\ncompletes the digital enhanced hermeneutic circle. In this way, the article\nargues for a more comprehensive understanding of lemmatization, encompassing\nclassical machine learning as well as intellectual post-corrections and, in\nparticular, human computation in the form of interpretation processes based on\ngraph representations of the underlying lexical resources.", "published": "2020-05-21 17:16:53", "link": "http://arxiv.org/abs/2005.10790v1", "categories": ["cs.CL", "H.4; I.7; J.5"], "primary_category": "cs.CL"}
{"title": "Evaluating Neural Morphological Taggers for Sanskrit", "abstract": "Neural sequence labelling approaches have achieved state of the art results\nin morphological tagging. We evaluate the efficacy of four standard sequence\nlabelling models on Sanskrit, a morphologically rich, fusional Indian language.\nAs its label space can theoretically contain more than 40,000 labels, systems\nthat explicitly model the internal structure of a label are more suited for the\ntask, because of their ability to generalise to labels not seen during\ntraining. We find that although some neural models perform better than others,\none of the common causes for error for all of these models is mispredictions\ndue to syncretism.", "published": "2020-05-21 20:36:32", "link": "http://arxiv.org/abs/2005.10893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MTSS: Learn from Multiple Domain Teachers and Become a Multi-domain\n  Dialogue Expert", "abstract": "How to build a high-quality multi-domain dialogue system is a challenging\nwork due to its complicated and entangled dialogue state space among each\ndomain, which seriously limits the quality of dialogue policy, and further\naffects the generated response. In this paper, we propose a novel method to\nacquire a satisfying policy and subtly circumvent the knotty dialogue state\nrepresentation problem in the multi-domain setting. Inspired by real school\nteaching scenarios, our method is composed of multiple domain-specific teachers\nand a universal student. Each individual teacher only focuses on one specific\ndomain and learns its corresponding domain knowledge and dialogue policy based\non a precisely extracted single domain dialogue state representation. Then,\nthese domain-specific teachers impart their domain knowledge and policies to a\nuniversal student model and collectively make this student model a multi-domain\ndialogue expert. Experiment results show that our method reaches competitive\nresults with SOTAs in both multi-domain and single domain setting.", "published": "2020-05-21 03:40:02", "link": "http://arxiv.org/abs/2005.10450v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SafeComp: Protocol For Certifying Cloud Computations Integrity", "abstract": "We define a problem of certifying computation integrity performed by some\nremote party we do not necessarily trust. We present a multi-party interactive\nprotocol called SafeComp that solves this problem under specified constraints.\nComparing to the nearest related work, our protocol reduces a proof\nconstruction complexity from $O(n \\log{n})$ to $O(n)$, turning a communication\ncomplexity to exactly one round using a certificate of a comparable length.", "published": "2020-05-21 17:08:39", "link": "http://arxiv.org/abs/2005.10786v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Extracting Daily Dosage from Medication Instructions in EHRs: An\n  Automated Approach and Lessons Learned", "abstract": "Medication timelines have been shown to be effective in helping physicians\nvisualize complex patient medication information. A key feature in many such\ndesigns is a longitudinal representation of a medication's daily dosage and its\nchanges over time. However, daily dosage as a discrete value is generally not\nprovided and needs to be derived from free text instructions (Sig). Existing\nworks in daily dosage extraction are narrow in scope, targeting dosage\nextraction for a single drug from clinical notes. Here, we present an automated\napproach to calculate daily dosage for all medications, combining deep\nlearning-based named entity extractor with lexicon dictionaries and regular\nexpressions, achieving 0.98 precision and 0.95 recall on an expert-generated\ndataset of 1,000 Sigs. We also analyze our expert-generated dataset, discuss\nthe challenges in understanding the complex information contained in Sigs, and\nprovide insights to guide future work in the general-purpose daily dosage\ncalculation task.", "published": "2020-05-21 20:55:22", "link": "http://arxiv.org/abs/2005.10899v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Training Keyword Spotting Models on Non-IID Data with Federated Learning", "abstract": "We demonstrate that a production-quality keyword-spotting model can be\ntrained on-device using federated learning and achieve comparable false accept\nand false reject rates to a centrally-trained model. To overcome the\nalgorithmic constraints associated with fitting on-device data (which are\ninherently non-independent and identically distributed), we conduct thorough\nempirical studies of optimization algorithms and hyperparameter configurations\nusing large-scale federated simulations. To overcome resource constraints, we\nreplace memory intensive MTR data augmentation with SpecAugment, which reduces\nthe false reject rate by 56%. Finally, to label examples (given the zero\nvisibility into on-device data), we explore teacher-student training.", "published": "2020-05-21 00:53:33", "link": "http://arxiv.org/abs/2005.10406v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automated Question Answer medical model based on Deep Learning\n  Technology", "abstract": "Artificial intelligence can now provide more solutions for different\nproblems, especially in the medical field. One of those problems the lack of\nanswers to any given medical/health-related question. The Internet is full of\nforums that allow people to ask some specific questions and get great answers\nfor them. Nevertheless, browsing these questions in order to locate one similar\nto your own, also finding a satisfactory answer is a difficult and\ntime-consuming task. This research will introduce a solution to this problem by\nautomating the process of generating qualified answers to these questions and\ncreating a kind of digital doctor. Furthermore, this research will train an\nend-to-end model using the framework of RNN and the encoder-decoder to generate\nsensible and useful answers to a small set of medical/health issues. The\nproposed model was trained and evaluated using data from various online\nservices, such as WebMD, HealthTap, eHealthForums, and iCliniq.", "published": "2020-05-21 01:40:01", "link": "http://arxiv.org/abs/2005.10416v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Symptom extraction from the narratives of personal experiences with\n  COVID-19 on Reddit", "abstract": "Social media discussion of COVID-19 provides a rich source of information\ninto how the virus affects people's lives that is qualitatively different from\ntraditional public health datasets. In particular, when individuals self-report\ntheir experiences over the course of the virus on social media, it can allow\nfor identification of the emotions each stage of symptoms engenders in the\npatient. Posts to the Reddit forum r/COVID19Positive contain first-hand\naccounts from COVID-19 positive patients, giving insight into personal\nstruggles with the virus. These posts often feature a temporal structure\nindicating the number of days after developing symptoms the text refers to.\nUsing topic modelling and sentiment analysis, we quantify the change in\ndiscussion of COVID-19 throughout individuals' experiences for the first 14\ndays since symptom onset. Discourse on early symptoms such as fever, cough, and\nsore throat was concentrated towards the beginning of the posts, while language\nindicating breathing issues peaked around ten days. Some conversation around\ncritical cases was also identified and appeared at a roughly constant rate. We\nidentified two clear clusters of positive and negative emotions associated with\nthe evolution of these symptoms and mapped their relationships. Our results\nprovide a perspective on the patient experience of COVID-19 that complements\nother medical data streams and can potentially reveal when mental health issues\nmight appear.", "published": "2020-05-21 03:54:51", "link": "http://arxiv.org/abs/2005.10454v1", "categories": ["cs.CL", "cs.SI", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Simplified Self-Attention for Transformer-based End-to-End Speech\n  Recognition", "abstract": "Transformer models have been introduced into end-to-end speech recognition\nwith state-of-the-art performance on various tasks owing to their superiority\nin modeling long-term dependencies. However, such improvements are usually\nobtained through the use of very large neural networks. Transformer models\nmainly include two submodules - position-wise feedforward layers and\nself-attention (SAN) layers. In this paper, to reduce the model complexity\nwhile maintaining good performance, we propose a simplified self-attention\n(SSAN) layer which employs FSMN memory block instead of projection layers to\nform query and key vectors for transformer-based end-to-end speech recognition.\nWe evaluate the SSAN-based and the conventional SAN-based transformers on the\npublic AISHELL-1, internal 1000-hour and 20,000-hour large-scale Mandarin\ntasks. Results show that our proposed SSAN-based transformer model can achieve\nover 20% relative reduction in model parameters and 6.7% relative CER reduction\non the AISHELL-1 task. With impressively 20% parameter reduction, our model\nshows no loss of recognition performance on the 20,000-hour large-scale task.", "published": "2020-05-21 04:55:59", "link": "http://arxiv.org/abs/2005.10463v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASAPP-ASR: Multistream CNN and Self-Attentive SRU for SOTA Speech\n  Recognition", "abstract": "In this paper we present state-of-the-art (SOTA) performance on the\nLibriSpeech corpus with two novel neural network architectures, a multistream\nCNN for acoustic modeling and a self-attentive simple recurrent unit (SRU) for\nlanguage modeling. In the hybrid ASR framework, the multistream CNN acoustic\nmodel processes an input of speech frames in multiple parallel pipelines where\neach stream has a unique dilation rate for diversity. Trained with the\nSpecAugment data augmentation method, it achieves relative word error rate\n(WER) improvements of 4% on test-clean and 14% on test-other. We further\nimprove the performance via N-best rescoring using a 24-layer self-attentive\nSRU language model, achieving WERs of 1.75% on test-clean and 4.46% on\ntest-other.", "published": "2020-05-21 05:18:34", "link": "http://arxiv.org/abs/2005.10469v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multistream CNN for Robust Acoustic Modeling", "abstract": "This paper proposes multistream CNN, a novel neural network architecture for\nrobust acoustic modeling in speech recognition tasks. The proposed architecture\nprocesses input speech with diverse temporal resolutions by applying different\ndilation rates to convolutional neural networks across multiple streams to\nachieve the robustness. The dilation rates are selected from the multiples of a\nsub-sampling rate of 3 frames. Each stream stacks TDNN-F layers (a variant of\n1D CNN), and output embedding vectors from the streams are concatenated then\nprojected to the final layer. We validate the effectiveness of the proposed\nmultistream CNN architecture by showing consistent improvements against Kaldi's\nbest TDNN-F model across various data sets. Multistream CNN improves the WER of\nthe test-other set in the LibriSpeech corpus by 12% (relative). On custom data\nfrom ASAPP's production ASR system for a contact center, it records a relative\nWER improvement of 11% for customer channel audio to prove its robustness to\ndata in the wild. In terms of real-time factor, multistream CNN outperforms the\nbaseline TDNN-F by 15%, which also suggests its practicality on production\nsystems. When combined with self-attentive SRU LM rescoring, multistream CNN\ncontributes for ASAPP to achieve the best WER of 1.75% on test-clean in\nLibriSpeech.", "published": "2020-05-21 05:26:15", "link": "http://arxiv.org/abs/2005.10470v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hidden Markov Chains, Entropic Forward-Backward, and Part-Of-Speech\n  Tagging", "abstract": "The ability to take into account the characteristics - also called features -\nof observations is essential in Natural Language Processing (NLP) problems.\nHidden Markov Chain (HMC) model associated with classic Forward-Backward\nprobabilities cannot handle arbitrary features like prefixes or suffixes of any\nsize, except with an independence condition. For twenty years, this default has\nencouraged the development of other sequential models, starting with the\nMaximum Entropy Markov Model (MEMM), which elegantly integrates arbitrary\nfeatures. More generally, it led to neglect HMC for NLP. In this paper, we show\nthat the problem is not due to HMC itself, but to the way its restoration\nalgorithms are computed. We present a new way of computing HMC based\nrestorations using original Entropic Forward and Entropic Backward (EFB)\nprobabilities. Our method allows taking into account features in the HMC\nframework in the same way as in the MEMM framework. We illustrate the\nefficiency of HMC using EFB in Part-Of-Speech Tagging, showing its superiority\nover MEMM based restoration. We also specify, as a perspective, how HMCs with\nEFB might appear as an alternative to Recurrent Neural Networks to treat\nsequential data with a deep architecture.", "published": "2020-05-21 13:31:11", "link": "http://arxiv.org/abs/2005.10629v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for\n  Automatic Dialog Evaluation", "abstract": "Open Domain dialog system evaluation is one of the most important challenges\nin dialog research. Existing automatic evaluation metrics, such as BLEU are\nmostly reference-based. They calculate the difference between the generated\nresponse and a limited number of available references. Likert-score based\nself-reported user rating is widely adopted by social conversational systems,\nsuch as Amazon Alexa Prize chatbots. However, self-reported user rating suffers\nfrom bias and variance among different users. To alleviate this problem, we\nformulate dialog evaluation as a comparison task. We also propose an automatic\nevaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that\nautomatically cleans self-reported user ratings as it trains on them.\nSpecifically, we first use a self-supervised method to learn better dialog\nfeature representation, and then use KNN and Shapley to remove confusing\nsamples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog\ncomparison task.", "published": "2020-05-21 15:14:49", "link": "http://arxiv.org/abs/2005.10716v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time", "abstract": "We introduce Trialstreamer, a living database of clinical trial reports. Here\nwe mainly describe the evidence extraction component; this extracts from\nbiomedical abstracts key pieces of information that clinicians need when\nappraising the literature, and also the relations between these. Specifically,\nthe system extracts descriptions of trial participants, the treatments compared\nin each arm (the interventions), and which outcomes were measured. The system\nthen attempts to infer which interventions were reported to work best by\ndetermining their relationship with identified trial outcome measures. In\naddition to summarizing individual trials, these extracted data elements allow\nautomatic synthesis of results across many trials on the same topic. We apply\nthe system at scale to all reports of randomized controlled trials indexed in\nMEDLINE, powering the automatic generation of evidence maps, which provide a\nglobal view of the efficacy of different interventions combining data from all\nrelevant clinical trials on a topic. We make all code and models freely\navailable alongside a demonstration of the web interface.", "published": "2020-05-21 19:32:04", "link": "http://arxiv.org/abs/2005.10865v1", "categories": ["cs.IR", "cs.CL", "cs.HC", "cs.LG", "I.2.7; J.3"], "primary_category": "cs.IR"}
{"title": "Team Neuro at SemEval-2020 Task 8: Multi-Modal Fine Grain Emotion\n  Classification of Memes using Multitask Learning", "abstract": "In this article, we describe the system that we used for the memotion\nanalysis challenge, which is Task 8 of SemEval-2020. This challenge had three\nsubtasks where affect based sentiment classification of the memes was required\nalong with intensities. The system we proposed combines the three tasks into a\nsingle one by representing it as multi-label hierarchical classification\nproblem.Here,Multi-Task learning or Joint learning Procedure is used to train\nour model.We have used dual channels to extract text and image based features\nfrom separate Deep Neural Network Backbone and aggregate them to create task\nspecific features. These task specific aggregated feature vectors ware then\npassed on to smaller networks with dense layers, each one assigned for\npredicting one type of fine grain sentiment label. Our Proposed method show the\nsuperiority of this system in few tasks to other best models from the\nchallenge.", "published": "2020-05-21 21:29:44", "link": "http://arxiv.org/abs/2005.10915v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Conversational End-to-End TTS for Voice Agent", "abstract": "End-to-end neural TTS has achieved superior performance on reading style\nspeech synthesis. However, it's still a challenge to build a high-quality\nconversational TTS due to the limitations of the corpus and modeling\ncapability. This study aims at building a conversational TTS for a voice agent\nunder sequence to sequence modeling framework. We firstly construct a\nspontaneous conversational speech corpus well designed for the voice agent with\na new recording scheme ensuring both recording quality and conversational\nspeaking style. Secondly, we propose a conversation context-aware end-to-end\nTTS approach which has an auxiliary encoder and a conversational context\nencoder to reinforce the information about the current utterance and its\ncontext in a conversation as well. Experimental results show that the proposed\nmethods produce more natural prosody in accordance with the conversational\ncontext, with significant preference gains at both utterance-level and\nconversation-level. Moreover, we find that the model has the ability to express\nsome spontaneous behaviors, like fillers and repeated words, which makes the\nconversational speaking style more realistic.", "published": "2020-05-21 02:52:25", "link": "http://arxiv.org/abs/2005.10438v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pitchtron: Towards audiobook generation from ordinary people's voices", "abstract": "In this paper, we explore prosody transfer for audiobook generation under\nrather realistic condition where training DB is plain audio mostly from\nmultiple ordinary people and reference audio given during inference is from\nprofessional and richer in prosody than training DB. To be specific, we explore\ntransferring Korean dialects and emotive speech even though training set is\nmostly composed of standard and neutral Korean. We found that under this\nsetting, original global style token method generates undesirable glitches in\npitch, energy and pause length. To deal with this issue, we propose two models,\nhard and soft pitchtron and release the toolkit and corpus that we have\ndeveloped. Hard pitchtron uses pitch as input to the decoder while soft\npitchtron uses pitch as input to the prosody encoder. We verify the\neffectiveness of proposed models with objective and subjective tests. AXY score\nover GST is 2.01 and 1.14 for hard pitchtron and soft pitchtron respectively.", "published": "2020-05-21 04:11:15", "link": "http://arxiv.org/abs/2005.10456v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Far-Field Speech Recognition with Unified Dereverberation and\n  Beamforming", "abstract": "Despite successful applications of end-to-end approaches in multi-channel\nspeech recognition, the performance still degrades severely when the speech is\ncorrupted by reverberation. In this paper, we integrate the dereverberation\nmodule into the end-to-end multi-channel speech recognition system and explore\ntwo different frontend architectures. First, a multi-source mask-based weighted\nprediction error (WPE) module is incorporated in the frontend for\ndereverberation. Second, another novel frontend architecture is proposed, which\nextends the weighted power minimization distortionless response (WPD)\nconvolutional beamformer to perform simultaneous separation and\ndereverberation. We derive a new formulation from the original WPD, which can\nhandle multi-source input, and replace eigenvalue decomposition with the matrix\ninverse operation to make the back-propagation algorithm more stable. The above\ntwo architectures are optimized in a fully end-to-end manner, only using the\nspeech recognition criterion. Experiments on both spatialized wsj1-2mix corpus\nand REVERB show that our proposed model outperformed the conventional methods\nin reverberant scenarios.", "published": "2020-05-21 06:29:49", "link": "http://arxiv.org/abs/2005.10479v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Coswara -- A Database of Breathing, Cough, and Voice Sounds for COVID-19\n  Diagnosis", "abstract": "The COVID-19 pandemic presents global challenges transcending boundaries of\ncountry, race, religion, and economy. The current gold standard method for\nCOVID-19 detection is the reverse transcription polymerase chain reaction\n(RT-PCR) testing. However, this method is expensive, time-consuming, and\nviolates social distancing. Also, as the pandemic is expected to stay for a\nwhile, there is a need for an alternate diagnosis tool which overcomes these\nlimitations, and is deployable at a large scale. The prominent symptoms of\nCOVID-19 include cough and breathing difficulties. We foresee that respiratory\nsounds, when analyzed using machine learning techniques, can provide useful\ninsights, enabling the design of a diagnostic tool. Towards this, the paper\npresents an early effort in creating (and analyzing) a database, called\nCoswara, of respiratory sounds, namely, cough, breath, and voice. The sound\nsamples are collected via worldwide crowdsourcing using a website application.\nThe curated dataset is released as open access. As the pandemic is evolving,\nthe data collection and analysis is a work in progress. We believe that\ninsights from analysis of Coswara can be effective in enabling sound based\ntechnology solutions for point-of-care diagnosis of respiratory infection, and\nin the near future this can help to diagnose COVID-19.", "published": "2020-05-21 10:04:52", "link": "http://arxiv.org/abs/2005.10548v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Inaudible Adversarial Perturbations for Targeted Attack in Speaker\n  Recognition", "abstract": "Speaker recognition is a popular topic in biometric authentication and many\ndeep learning approaches have achieved extraordinary performances. However, it\nhas been shown in both image and speech applications that deep neural networks\nare vulnerable to adversarial examples. In this study, we aim to exploit this\nweakness to perform targeted adversarial attacks against the x-vector based\nspeaker recognition system. We propose to generate inaudible adversarial\nperturbations achieving targeted white-box attacks to speaker recognition\nsystem based on the psychoacoustic principle of frequency masking.\nSpecifically, we constrict the perturbation under the masking threshold of\noriginal audio, instead of using a common l_p norm to measure the\nperturbations. Experiments on Aishell-1 corpus show that our approach yields up\nto 98.5% attack success rate to arbitrary gender speaker targets, while\nretaining indistinguishable attribute to listeners. Furthermore, we also\nachieve an effective speaker attack when applying the proposed approach to a\ncompletely irrelevant waveform, such as music.", "published": "2020-05-21 13:37:50", "link": "http://arxiv.org/abs/2005.10637v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Formant Tracking Using Dilated Convolutional Networks Through Dense\n  Connection with Gating Mechanism", "abstract": "Formant tracking is one of the most fundamental problems in speech\nprocessing. Traditionally, formants are estimated using signal processing\nmethods. Recent studies showed that generic convolutional architectures can\noutperform recurrent networks on temporal tasks such as speech synthesis and\nmachine translation. In this paper, we explored the use of Temporal\nConvolutional Network (TCN) for formant tracking. In addition to the\nconventional implementation, we modified the architecture from three aspects.\nFirst, we turned off the \"causal\" mode of dilated convolution, making the\ndilated convolution see the future speech frames. Second, each hidden layer\nreused the output information from all the previous layers through dense\nconnection. Third, we also adopted a gating mechanism to alleviate the problem\nof gradient disappearance by selectively forgetting unimportant information.\nThe model was validated on the open access formant database VTR. The experiment\nshowed that our proposed model was easy to converge and achieved an overall\nmean absolute percent error (MAPE) of 8.2% on speech-labeled frames, compared\nto three competitive baselines of 9.4% (LSTM), 9.1% (Bi-LSTM) and 8.9% (TCN).", "published": "2020-05-21 17:32:39", "link": "http://arxiv.org/abs/2005.10803v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Reinforcement Learning with Pre-training for Time-efficient\n  Training of Automatic Speech Recognition", "abstract": "Deep reinforcement learning (deep RL) is a combination of deep learning with\nreinforcement learning principles to create efficient methods that can learn by\ninteracting with its environment. This has led to breakthroughs in many complex\ntasks, such as playing the game \"Go\", that were previously difficult to solve.\nHowever, deep RL requires significant training time making it difficult to use\nin various real-life applications such as Human-Computer Interaction (HCI). In\nthis paper, we study pre-training in deep RL to reduce the training time and\nimprove the performance of Speech Recognition, a popular application of HCI. To\nevaluate the performance improvement in training we use the publicly available\n\"Speech Command\" dataset, which contains utterances of 30 command keywords\nspoken by 2,618 speakers. Results show that pre-training with deep RL offers\nfaster convergence compared to non-pre-trained RL while achieving improved\nspeech recognition accuracy.", "published": "2020-05-21 06:07:30", "link": "http://arxiv.org/abs/2005.11172v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Streaming Chunk-Aware Multihead Attention for Online End-to-End Speech\n  Recognition", "abstract": "Recently, streaming end-to-end automatic speech recognition (E2E-ASR) has\ngained more and more attention. Many efforts have been paid to turn the\nnon-streaming attention-based E2E-ASR system into streaming architecture. In\nthis work, we propose a novel online E2E-ASR system by using Streaming\nChunk-Aware Multihead Attention(SCAMA) and a latency control memory equipped\nself-attention network (LC-SAN-M). LC-SAN-M uses chunk-level input to control\nthe latency of encoder. As to SCAMA, a jointly trained predictor is used to\ncontrol the output of encoder when feeding to decoder, which enables decoder to\ngenerate output in streaming manner. Experimental results on the open 170-hour\nAISHELL-1 and an industrial-level 20000-hour Mandarin speech recognition tasks\nshow that our approach can significantly outperform the MoChA-based baseline\nsystem under comparable setup. On the AISHELL-1 task, our proposed method\nachieves a character error rate (CER) of 7.39%, to the best of our knowledge,\nwhich is the best published performance for online ASR.", "published": "2020-05-21 03:35:15", "link": "http://arxiv.org/abs/2006.01712v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition", "abstract": "End-to-end speech recognition has become popular in recent years, since it\ncan integrate the acoustic, pronunciation and language models into a single\nneural network. Among end-to-end approaches, attention-based methods have\nemerged as being superior. For example, Transformer, which adopts an\nencoder-decoder architecture. The key improvement introduced by Transformer is\nthe utilization of self-attention instead of recurrent mechanisms, enabling\nboth encoder and decoder to capture long-range dependencies with lower\ncomputational complexity.In this work, we propose boosting the self-attention\nability with a DFSMN memory block, forming the proposed memory equipped\nself-attention (SAN-M) mechanism. Theoretical and empirical comparisons have\nbeen made to demonstrate the relevancy and complementarity between\nself-attention and the DFSMN memory block. Furthermore, the proposed SAN-M\nprovides an efficient mechanism to integrate these two modules. We have\nevaluated our approach on the public AISHELL-1 benchmark and an\nindustrial-level 20,000-hour Mandarin speech recognition task. On both tasks,\nSAN-M systems achieved much better performance than the self-attention based\nTransformer baseline system. Specially, it can achieve a CER of 6.46% on the\nAISHELL-1 task even without using any external LM, comfortably outperforming\nother state-of-the-art systems.", "published": "2020-05-21 03:33:09", "link": "http://arxiv.org/abs/2006.01713v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Text Data Using Hybrid Transformer-LSTM Based End-to-End ASR\n  in Transfer Learning", "abstract": "In this work, we study leveraging extra text data to improve low-resource\nend-to-end ASR under cross-lingual transfer learning setting. To this end, we\nextend our prior work [1], and propose a hybrid Transformer-LSTM based\narchitecture. This architecture not only takes advantage of the highly\neffective encoding capacity of the Transformer network but also benefits from\nextra text data due to the LSTM-based independent language model network. We\nconduct experiments on our in-house Malay corpus which contains limited labeled\ndata and a large amount of extra text. Results show that the proposed\narchitecture outperforms the previous LSTM-based architecture [1] by 24.2%\nrelative word error rate (WER) when both are trained using limited labeled\ndata. Starting from this, we obtain further 25.4% relative WER reduction by\ntransfer learning from another resource-rich language. Moreover, we obtain\nadditional 13.6% relative WER reduction by boosting the LSTM decoder of the\ntransferred model with the extra text data. Overall, our best model outperforms\nthe vanilla Transformer ASR by 11.9% relative WER. Last but not least, the\nproposed hybrid architecture offers much faster inference compared to both LSTM\nand Transformer architectures.", "published": "2020-05-21 00:56:42", "link": "http://arxiv.org/abs/2005.10407v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-lingual Multispeaker Text-to-Speech under Limited-Data Scenario", "abstract": "Modeling voices for multiple speakers and multiple languages in one\ntext-to-speech system has been a challenge for a long time. This paper presents\nan extension on Tacotron2 to achieve bilingual multispeaker speech synthesis\nwhen there are limited data for each language. We achieve cross-lingual\nsynthesis, including code-switching cases, between English and Mandarin for\nmonolingual speakers. The two languages share the same phonemic representations\nfor input, while the language attribute and the speaker identity are\nindependently controlled by language tokens and speaker embeddings,\nrespectively. In addition, we investigate the model's performance on the\ncross-lingual synthesis, with and without a bilingual dataset during training.\nWith the bilingual dataset, not only can the model generate high-fidelity\nspeech for all speakers concerning the language they speak, but also can\ngenerate accented, yet fluent and intelligible speech for monolingual speakers\nregarding non-native language. For example, the Mandarin speaker can speak\nEnglish fluently. Furthermore, the model trained with bilingual dataset is\nrobust for code-switching text-to-speech, as shown in our results and provided\nsamples.{https://caizexin.github.io/mlms-syn-samples/index.html}.", "published": "2020-05-21 03:03:34", "link": "http://arxiv.org/abs/2005.10441v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Robust Interpretable Deep Learning Classifier for Heart Anomaly\n  Detection Without Segmentation", "abstract": "Traditionally, abnormal heart sound classification is framed as a three-stage\nprocess. The first stage involves segmenting the phonocardiogram to detect\nfundamental heart sounds; after which features are extracted and classification\nis performed. Some researchers in the field argue the segmentation step is an\nunwanted computational burden, whereas others embrace it as a prior step to\nfeature extraction. When comparing accuracies achieved by studies that have\nsegmented heart sounds before analysis with those who have overlooked that\nstep, the question of whether to segment heart sounds before feature extraction\nis still open. In this study, we explicitly examine the importance of heart\nsound segmentation as a prior step for heart sound classification, and then\nseek to apply the obtained insights to propose a robust classifier for abnormal\nheart sound detection. Furthermore, recognizing the pressing need for\nexplainable Artificial Intelligence (AI) models in the medical domain, we also\nunveil hidden representations learned by the classifier using model\ninterpretation techniques. Experimental results demonstrate that the\nsegmentation plays an essential role in abnormal heart sound classification.\nOur new classifier is also shown to be robust, stable and most importantly,\nexplainable, with an accuracy of almost 100% on the widely used PhysioNet\ndataset.", "published": "2020-05-21 06:36:28", "link": "http://arxiv.org/abs/2005.10480v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "An approach to Beethoven's 10th Symphony", "abstract": "Ludwig van Beethoven composed his symphonies between 1799 and 1825, when he\nwas writing his Tenth symphony. As we dispose of a great amount of data\nbelonging to his work, the purpose of this paper is to investigate the\npossibility of extracting patterns on his compositional model from symbolic\ndata and generate what would have been his last symphony, the Tenth. A neural\nnetwork model has been built based on the Long Short-Therm Memory (LSTM) neural\nnetworks. After training the model, the generated music has been analysed by\ncomparing the input data with the results, and establishing differences between\nthe generated outputs based on the training data used to obtain them. The\nstructure of the outputs strongly depends on the symphonies used to train the\nnetwork.", "published": "2020-05-21 09:36:24", "link": "http://arxiv.org/abs/2005.10539v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
