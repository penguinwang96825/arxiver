{"title": "CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization", "abstract": "The quest for seeking health information has swamped the web with consumers'\nhealth-related questions. Generally, consumers use overly descriptive and\nperipheral information to express their medical condition or other healthcare\nneeds, contributing to the challenges of natural language understanding. One\nway to address this challenge is to summarize the questions and distill the key\ninformation of the original question. To address this issue, we introduce a new\ndataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health\nquestions and corresponding summaries. The dataset is derived from the\ncommunity question-answering forum and therefore provides a valuable resource\nfor understanding consumer health-related posts on social media. We benchmark\nthe dataset on multiple state-of-the-art summarization models to show the\neffectiveness of the dataset.", "published": "2022-06-14 03:49:03", "link": "http://arxiv.org/abs/2206.06581v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FreeTransfer-X: Safe and Label-Free Cross-Lingual Transfer from\n  Off-the-Shelf Models", "abstract": "Cross-lingual transfer (CLT) is of various applications. However, labeled\ncross-lingual corpus is expensive or even inaccessible, especially in the\nfields where labels are private, such as diagnostic results of symptoms in\nmedicine and user profiles in business. Nevertheless, there are off-the-shelf\nmodels in these sensitive fields. Instead of pursuing the original labels, a\nworkaround for CLT is to transfer knowledge from the off-the-shelf models\nwithout labels. To this end, we define a novel CLT problem named FreeTransfer-X\nthat aims to achieve knowledge transfer from the off-the-shelf models in\nrich-resource languages. To address the problem, we propose a 2-step knowledge\ndistillation (KD, Hinton et al., 2015) framework based on multilingual\npre-trained language models (mPLM). The significant improvement over strong\nneural machine translation (NMT) baselines demonstrates the effectiveness of\nthe proposed method. In addition to reducing annotation cost and protecting\nprivate labels, the proposed method is compatible with different networks and\neasy to be deployed. Finally, a range of analyses indicate the great potential\nof the proposed method.", "published": "2022-06-14 04:09:00", "link": "http://arxiv.org/abs/2206.06586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OSN Dashboard Tool For Sentiment Analysis", "abstract": "The amount of opinionated data on the internet is rapidly increasing. More\nand more people are sharing their ideas and opinions in reviews, discussion\nforums, microblogs and general social media. As opinions are central in all\nhuman activities, sentiment analysis has been applied to gain insights in this\ntype of data. There are proposed several approaches for sentiment\nclassification. The major drawback is the lack of standardized solutions for\nclassification and high-level visualization. In this study, a sentiment\nanalyzer dashboard for online social networking analysis is proposed. This, to\nenable people gaining insights in topics interesting to them. The tool allows\nusers to run the desired sentiment analysis algorithm in the dashboard. In\naddition to providing several visualization types, the dashboard facilitates\nraw data results from the sentiment classification which can be downloaded for\nfurther analysis.", "published": "2022-06-14 15:56:32", "link": "http://arxiv.org/abs/2206.06935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Experimental Investigation of Part-Of-Speech Taggers for Vietnamese", "abstract": "Part-of-speech (POS) tagging plays an important role in Natural Language\nProcessing (NLP). Its applications can be found in many NLP tasks such as named\nentity recognition, syntactic parsing, dependency parsing and text chunking. In\nthe investigation conducted in this paper, we utilize the technologies of two\nwidely-used toolkits, ClearNLP and Stanford POS Tagger, as well as develop two\nnew POS taggers for Vietnamese, then compare them to three well-known\nVietnamese taggers, namely JVnTagger, vnTagger and RDRPOSTagger. We make a\nsystematic comparison to find out the tagger having the best performance. We\nalso design a new feature set to measure the performance of the statistical\ntaggers. Our new taggers built from Stanford Tagger and ClearNLP with the new\nfeature set can outperform all other current Vietnamese taggers in term of\ntagging accuracy. Moreover, we also analyze the affection of some features to\nthe performance of statistical taggers. Lastly, the experimental results also\nreveal that the transformation-based tagger, RDRPOSTagger, can run\nsignificantly faster than any other statistical tagger.", "published": "2022-06-14 17:07:28", "link": "http://arxiv.org/abs/2206.06992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational linguistics and Natural Language Processing", "abstract": "This chapter provides an introduction to computational linguistics methods,\nwith focus on their applications to the practice and study of translation. It\ncovers computational models, methods and tools for collection, storage,\nindexing and analysis of linguistic data in the context of translation, and\ndiscusses the main methodological issues and challenges in this field. While an\nexhaustive review of existing computational linguistics methods and tools is\nbeyond the scope of this chapter, we describe the most representative\napproaches, and illustrate them with descriptions of typical applications.", "published": "2022-06-14 17:43:42", "link": "http://arxiv.org/abs/2206.07026v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Text Generation with Text-Editing Models", "abstract": "Text-editing models have recently become a prominent alternative to seq2seq\nmodels for monolingual text-generation tasks such as grammatical error\ncorrection, simplification, and style transfer. These tasks share a common\ntrait - they exhibit a large amount of textual overlap between the source and\ntarget texts. Text-editing models take advantage of this observation and learn\nto generate the output by predicting edit operations applied to the source\nsequence. In contrast, seq2seq models generate outputs word-by-word from\nscratch thus making them slow at inference time. Text-editing models provide\nseveral benefits over seq2seq models including faster inference speed, higher\nsample efficiency, and better control and interpretability of the outputs. This\ntutorial provides a comprehensive overview of text-editing models and current\nstate-of-the-art approaches, and analyzes their pros and cons. We discuss\nchallenges related to productionization and how these models can be used to\nmitigate hallucination and bias, both pressing challenges in the field of text\ngeneration.", "published": "2022-06-14 17:58:17", "link": "http://arxiv.org/abs/2206.07043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NewsEdits: A News Article Revision Dataset and a Document-Level\n  Reasoning Challenge", "abstract": "News article revision histories provide clues to narrative and factual\nevolution in news articles. To facilitate analysis of this evolution, we\npresent the first publicly available dataset of news revision histories,\nNewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million\narticles with 4.6 million versions from over 22 English- and French-language\nnewspaper sources based in three countries, spanning 15 years of coverage\n(2006-2021).\n  We define article-level edit actions: Addition, Deletion, Edit and Refactor,\nand develop a high-accuracy extraction algorithm to identify these actions. To\nunderscore the factual nature of many edit actions, we conduct analyses showing\nthat added and deleted sentences are more likely to contain updating events,\nmain content and quotes than unchanged sentences.\n  Finally, to explore whether edit actions are predictable, we introduce three\nnovel tasks aimed at predicting actions performed during version updates. We\nshow that these tasks are possible for expert humans but are challenging for\nlarge NLP models. We hope this can spur research in narrative framing and help\nprovide predictive tools for journalists chasing breaking news.", "published": "2022-06-14 18:47:13", "link": "http://arxiv.org/abs/2206.07106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "If it Bleeds, it Leads: A Computational Approach to Covering Crime in\n  Los Angeles", "abstract": "Developing and improving computational approaches to covering news can\nincrease journalistic output and improve the way stories are covered. In this\nwork we approach the problem of covering crime stories in Los Angeles. We\npresent a machine-in-the-loop system that covers individual crimes by (1)\nlearning the prototypical coverage archetypes from classical news articles on\ncrime to learn their structure and (2) using output from the Los Angeles Police\ndepartment to generate \"lede paragraphs\", first structural unit of\ncrime-articles. We introduce a probabilistic graphical model for learning\narticle structure and a rule-based system for generating ledes. We hope our\nwork can lead to systems that use these components together to form the\nskeletons of news articles covering crime.\n  This work was done for a class project in Jonathan May's Advanced Natural\nLanguage Processing Course, Fall, 2019.", "published": "2022-06-14 19:06:13", "link": "http://arxiv.org/abs/2206.07115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning\n  Tasks", "abstract": "Fine-tuning pretrained language models (LMs) without making any architectural\nchanges has become a norm for learning various language downstream tasks.\nHowever, for non-language downstream tasks, a common practice is to employ\ntask-specific designs for input, output layers, and loss functions. For\ninstance, it is possible to fine-tune an LM into an MNIST classifier by\nreplacing the word embedding layer with an image patch embedding layer, the\nword token output layer with a 10-way output layer, and the word prediction\nloss with a 10-way classification loss, respectively. A natural question\narises: Can LM fine-tuning solve non-language downstream tasks without changing\nthe model architecture or loss function? To answer this, we propose\nLanguage-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations\nby conducting an extensive empirical study on a suite of non-language\nclassification and regression tasks. LIFT does not make any changes to the\nmodel architecture or loss function, and it solely relies on the natural\nlanguage interface, enabling \"no-code machine learning with LMs.\" We find that\nLIFT performs comparably well across a wide range of low-dimensional\nclassification and regression tasks, matching the performances of the best\nbaselines in many cases, especially for the classification tasks. We also\nreport experimental results on the fundamental properties of LIFT, including\ninductive bias, robustness, and sample complexity. We also analyze the effect\nof pretraining on LIFT and a few properties/techniques specific to LIFT, e.g.,\ncontext-aware learning via appropriate prompting, calibrated predictions, data\ngeneration, and two-stage fine-tuning. Our code is available at\nhttps://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.", "published": "2022-06-14 02:41:41", "link": "http://arxiv.org/abs/2206.06565v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Astock: A New Dataset and Automated Stock Trading based on\n  Stock-specific News Analyzing Model", "abstract": "Natural Language Processing(NLP) demonstrates a great potential to support\nfinancial decision-making by analyzing the text from social media or news\noutlets. In this work, we build a platform to study the NLP-aided stock\nauto-trading algorithms systematically. In contrast to the previous work, our\nplatform is characterized by three features: (1) We provide financial news for\neach specific stock. (2) We provide various stock factors for each stock. (3)\nWe evaluate performance from more financial-relevant metrics. Such a design\nallows us to develop and evaluate NLP-aided stock auto-trading algorithms in a\nmore realistic setting. In addition to designing an evaluation platform and\ndataset collection, we also made a technical contribution by proposing a system\nto automatically learn a good feature representation from various input\ninformation. The key to our algorithm is a method called semantic role labeling\nPooling (SRLP), which leverages Semantic Role Labeling (SRL) to create a\ncompact representation of each news paragraph. Based on SRLP, we further\nincorporate other stock factors to make the final prediction. In addition, we\npropose a self-supervised learning strategy based on SRLP to enhance the\nout-of-distribution generalization performance of our system. Through our\nexperimental study, we show that the proposed method achieves better\nperformance and outperforms all the baselines' annualized rate of return as\nwell as the maximum drawdown of the CSI300 index and XIN9 index on real\ntrading. Our Astock dataset and code are available at\nhttps://github.com/JinanZou/Astock.", "published": "2022-06-14 05:55:23", "link": "http://arxiv.org/abs/2206.06606v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Task Transfer and Domain Adaptation for Zero-Shot Question Answering", "abstract": "Pretrained language models have shown success in various areas of natural\nlanguage processing, including reading comprehension tasks. However, when\napplying machine learning methods to new domains, labeled data may not always\nbe available. To address this, we use supervised pretraining on source-domain\ndata to reduce sample complexity on domain-specific downstream tasks. We\nevaluate zero-shot performance on domain-specific reading comprehension tasks\nby combining task transfer with domain adaptation to fine-tune a pretrained\nmodel with no labelled data from the target task. Our approach outperforms\nDomain-Adaptive Pretraining on downstream domain-specific reading comprehension\ntasks in 3 out of 4 domains.", "published": "2022-06-14 09:10:48", "link": "http://arxiv.org/abs/2206.06705v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "\"hasSignification()\": une nouvelle fonction de distance pour soutenir la\n  d\u00e9tection de donn\u00e9es personnelles", "abstract": "Today with Big Data and data lakes, we are faced of a mass of data that is\nvery difficult to manage it manually. The protection of personal data in this\ncontext requires an automatic analysis for data discovery. Storing the names of\nattributes already analyzed in a knowledge base could optimize this automatic\ndiscovery. To have a better knowledge base, we should not store any attributes\nwhose name does not make sense. In this article, to check if the name of an\nattribute has a meaning, we propose a solution that calculate the distances\nbetween this name and the words in a dictionary. Our studies on the distance\nfunctions like N-Gram, Jaro-Winkler and Levenshtein show limits to set an\nacceptance threshold for an attribute in the knowledge base. In order to\novercome these limitations, our solution aims to strengthen the score\ncalculation by using an exponential function based on the longest sequence. In\naddition, a double scan in dictionary is also proposed in order to process the\nattributes which have a compound name.", "published": "2022-06-14 13:31:26", "link": "http://arxiv.org/abs/2206.06836v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "RDU: A Region-based Approach to Form-style Document Understanding", "abstract": "Key Information Extraction (KIE) is aimed at extracting structured\ninformation (e.g. key-value pairs) from form-style documents (e.g. invoices),\nwhich makes an important step towards intelligent document understanding.\nPrevious approaches generally tackle KIE by sequence tagging, which faces\ndifficulty to process non-flatten sequences, especially for table-text mixed\ndocuments. These approaches also suffer from the trouble of pre-defining a\nfixed set of labels for each type of documents, as well as the label imbalance\nissue. In this work, we assume Optical Character Recognition (OCR) has been\napplied to input documents, and reformulate the KIE task as a region prediction\nproblem in the two-dimensional (2D) space given a target field. Following this\nnew setup, we develop a new KIE model named Region-based Document Understanding\n(RDU) that takes as input the text content and corresponding coordinates of a\ndocument, and tries to predict the result by localizing a bounding-box-like\nregion. Our RDU first applies a layout-aware BERT equipped with a soft layout\nattention masking and bias mechanism to incorporate layout information into the\nrepresentations. Then, a list of candidate regions is generated from the\nrepresentations via a Region Proposal Module inspired by computer vision models\nwidely applied for object detection. Finally, a Region Categorization Module\nand a Region Selection Module are adopted to judge whether a proposed region is\nvalid and select the one with the largest probability from all proposed regions\nrespectively. Experiments on four types of form-style documents show that our\nproposed method can achieve impressive results. In addition, our RDU model can\nbe trained with different document types seamlessly, which is especially\nhelpful over low-resource documents.", "published": "2022-06-14 14:47:48", "link": "http://arxiv.org/abs/2206.06890v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SBERT studies Meaning Representations: Decomposing Sentence Embeddings\n  into Explainable Semantic Features", "abstract": "Models based on large-pretrained language models, such as S(entence)BERT,\nprovide effective and efficient sentence embeddings that show high correlation\nto human similarity ratings, but lack interpretability. On the other hand,\ngraph metrics for graph-based meaning representations (e.g., Abstract Meaning\nRepresentation, AMR) can make explicit the semantic aspects in which two\nsentences are similar. However, such metrics tend to be slow, rely on parsers,\nand do not reach state-of-the-art performance when rating sentence similarity.\n  In this work, we aim at the best of both worlds, by learning to induce\n$S$emantically $S$tructured $S$entence BERT embeddings (S$^3$BERT). Our\nS$^3$BERT embeddings are composed of explainable sub-embeddings that emphasize\nvarious semantic sentence features (e.g., semantic roles, negation, or\nquantification). We show how to i) learn a decomposition of the sentence\nembeddings into semantic features, through approximation of a suite of\ninterpretable AMR graph metrics, and how to ii) preserve the overall power of\nthe neural embeddings by controlling the decomposition learning process with a\nsecond objective that enforces consistency with the similarity ratings of an\nSBERT teacher model. In our experimental studies, we show that our approach\noffers interpretability -- while fully preserving the effectiveness and\nefficiency of the neural sentence embeddings.", "published": "2022-06-14 17:37:18", "link": "http://arxiv.org/abs/2206.07023v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Narratives through Dimensions of Analogy", "abstract": "Analogical reasoning is a powerful qualitative reasoning tool that enables\nhumans to connect two situations, and to generalize their knowledge from\nfamiliar to novel situations. Cognitive Science research provides valuable\ninsights into the richness and complexity of analogical reasoning, together\nwith implementations of expressive analogical reasoners with limited\nscalability. Modern scalable AI techniques with the potential to reason by\nanalogy have been only applied to the special case of proportional analogy, and\nnot to understanding higher-order analogies. In this paper, we aim to bridge\nthe gap by: 1) formalizing six dimensions of analogy based on mature insights\nfrom Cognitive Science research, 2) annotating a corpus of fables with each of\nthese dimensions, and 3) defining four tasks with increasing complexity that\nenable scalable evaluation of AI techniques. Experiments with language models\nand neuro-symbolic AI reasoners on these tasks reveal that state-of-the-art\nmethods can be applied to reason by analogy with a limited success, motivating\nthe need for further research towards comprehensive and scalable analogical\nreasoning by AI. We make all our code and data available.", "published": "2022-06-14 20:56:26", "link": "http://arxiv.org/abs/2206.07167v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across\n  Modalities", "abstract": "Events describe happenings in our world that are of importance. Naturally,\nunderstanding events mentioned in multimedia content and how they are related\nforms an important way of comprehending our world. Existing literature can\ninfer if events across textual and visual (video) domains are identical (via\ngrounding) and thus, on the same semantic level. However, grounding fails to\ncapture the intricate cross-event relations that exist due to the same events\nbeing referred to on many semantic levels. For example, in Figure 1, the\nabstract event of \"war\" manifests at a lower semantic level through subevents\n\"tanks firing\" (in video) and airplane \"shot\" (in text), leading to a\nhierarchical, multimodal relationship between the events.\n  In this paper, we propose the task of extracting event hierarchies from\nmultimodal (video and text) data to capture how the same event manifests itself\nin different modalities at different semantic levels. This reveals the\nstructure of events and is critical to understanding them. To support research\non this task, we introduce the Multimodal Hierarchical Events (MultiHiEve)\ndataset. Unlike prior video-language datasets, MultiHiEve is composed of news\nvideo-article pairs, which makes it rich in event hierarchies. We densely\nannotate a part of the dataset to construct the test benchmark. We show the\nlimitations of state-of-the-art unimodal and multimodal baselines on this task.\nFurther, we address these limitations via a new weakly supervised model,\nleveraging only unannotated video-article pairs from MultiHiEve. We perform a\nthorough evaluation of our proposed method which demonstrates improved\nperformance on this task and highlight opportunities for future research.", "published": "2022-06-14 23:24:15", "link": "http://arxiv.org/abs/2206.07207v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Does Twitter know your political views? POLiTweets dataset and\n  semi-automatic method for political leaning discovery", "abstract": "Every day, the world is flooded by millions of messages and statements posted\non Twitter or Facebook. Social media platforms try to protect users' personal\ndata, but there still is a real risk of misuse, including elections\nmanipulation. Did you know, that only 13 posts addressing important or\ncontroversial topics for society are enough to predict one's political\naffiliation with a 0.85 F1-score? To examine this phenomenon, we created a\nnovel universal method of semi-automated political leaning discovery. It relies\non a heuristical data annotation procedure, which was evaluated to achieve 0.95\nagreement with human annotators (counted as an accuracy metric). We also\npresent POLiTweets - the first publicly open Polish dataset for political\naffiliation discovery in a multi-party setup, consisting of over 147k tweets\nfrom almost 10k Polish-writing users annotated heuristically and almost 40k\ntweets from 166 users annotated manually as a test set. We used our data to\nstudy the aspects of domain shift in the context of topics and the type of\ncontent writers - ordinary citizens vs. professional politicians.", "published": "2022-06-14 10:28:23", "link": "http://arxiv.org/abs/2207.07586v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The Causal Structure of Semantic Ambiguities", "abstract": "Ambiguity is a natural language phenomenon occurring at different levels of\nsyntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,\nfor instance, we have a variety of competing studies for the human\ndisambiguation processes. These studies are empirical and based on eye-tracking\nmeasurements. Here we take first steps towards formalizing these processes for\nsemantic ambiguities where we identified the presence of two features: (1)\njoint plausibility degrees of different possible interpretations, (2) causal\nstructures according to which certain words play a more substantial role in the\nprocesses. The novel sheaf-theoretic model of definite causality developed by\nGogioso and Pinzani in QPL 2021 offers tools to model and reason about these\nfeatures. We applied this theory to a dataset of ambiguous phrases extracted\nfrom Psycholinguistics literature and their human plausibility judgements\ncollected by us using the Amazon Mechanical Turk engine. We measured the causal\nfractions of different disambiguation orders within the phrases and discovered\ntwo prominent orders: from subject to verb in the subject-verb and from object\nto verb in the verb object phrases. We also found evidence for delay in the\ndisambiguation of polysemous vs homonymous verbs, again compatible with\nPsycholinguistic findings.", "published": "2022-06-14 12:56:34", "link": "http://arxiv.org/abs/2206.06807v3", "categories": ["cs.CL", "cs.AI", "quant-ph"], "primary_category": "cs.CL"}
{"title": "CERT: Continual Pre-Training on Sketches for Library-Oriented Code\n  Generation", "abstract": "Code generation is a longstanding challenge, aiming to generate a code\nsnippet based on a natural language description. Usually, expensive text-code\npaired data is essential for training a code generation model. Recently, thanks\nto the success of pre-training techniques, large language models are trained on\nlarge-scale unlabelled code corpora and perform well in code generation. In\nthis paper, we investigate how to leverage an unlabelled code corpus to train a\nmodel for library-oriented code generation. Since it is a common practice for\nprogrammers to reuse third-party libraries, in which case the text-code paired\ndata are harder to obtain due to the huge number of libraries. We observe that\nlibrary-oriented code snippets are more likely to share similar code sketches.\nHence, we present CERT with two steps: a sketcher generates the sketch, then a\ngenerator fills the details in the sketch. Both the sketcher and the generator\nare continually pre-trained upon a base model using unlabelled data.\nFurthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate\nlibrary-oriented code generation. Experimental results demonstrate the\nimpressive performance of CERT. For example, it surpasses the base model by an\nabsolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is\navailable at https://github.com/microsoft/PyCodeGPT.", "published": "2022-06-14 14:44:34", "link": "http://arxiv.org/abs/2206.06888v1", "categories": ["cs.SE", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "The Maximum Linear Arrangement Problem for trees under projectivity and\n  planarity", "abstract": "A linear arrangement is a mapping $\\pi$ from the $n$ vertices of a graph $G$\nto $n$ distinct consecutive integers. Linear arrangements can be represented by\ndrawing the vertices along a horizontal line and drawing the edges as\nsemicircles above said line. In this setting, the length of an edge is defined\nas the absolute value of the difference between the positions of its two\nvertices in the arrangement, and the cost of an arrangement as the sum of all\nedge lengths. Here we study two variants of the Maximum Linear Arrangement\nproblem (MaxLA), which consists of finding an arrangement that maximizes the\ncost. In the planar variant for free trees, vertices have to be arranged in\nsuch a way that there are no edge crossings. In the projective variant for\nrooted trees, arrangements have to be planar and the root of the tree cannot be\ncovered by any edge. In this paper we present algorithms that are linear in\ntime and space to solve planar and projective MaxLA for trees. We also prove\nseveral properties of maximum projective and planar arrangements, and show that\ncaterpillar trees maximize planar MaxLA over all trees of a fixed size thereby\ngeneralizing a previous extremal result on trees.", "published": "2022-06-14 15:43:44", "link": "http://arxiv.org/abs/2206.06924v5", "categories": ["cs.DS", "cs.CL", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Comprehending and Ordering Semantics for Image Captioning", "abstract": "Comprehending the rich semantics in an image and ordering them in linguistic\norder are essential to compose a visually-grounded and linguistically coherent\ndescription for image captioning. Modern techniques commonly capitalize on a\npre-trained object detector/classifier to mine the semantics in an image, while\nleaving the inherent linguistic ordering of semantics under-exploited. In this\npaper, we propose a new recipe of Transformer-style structure, namely\nComprehending and Ordering Semantics Networks (COS-Net), that novelly unifies\nan enriched semantic comprehending and a learnable semantic ordering processes\ninto a single architecture. Technically, we initially utilize a cross-modal\nretrieval model to search the relevant sentences of each image, and all words\nin the searched sentences are taken as primary semantic cues. Next, a novel\nsemantic comprehender is devised to filter out the irrelevant semantic words in\nprimary semantic cues, and meanwhile infer the missing relevant semantic words\nvisually grounded in the image. After that, we feed all the screened and\nenriched semantic words into a semantic ranker, which learns to allocate all\nsemantic words in linguistic order as humans. Such sequence of ordered semantic\nwords are further integrated with visual tokens of images to trigger sentence\ngeneration. Empirical evidences show that COS-Net clearly surpasses the\nstate-of-the-art approaches on COCO and achieves to-date the best CIDEr score\nof 141.1% on Karpathy test split. Source code is available at\n\\url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet}.", "published": "2022-06-14 15:51:14", "link": "http://arxiv.org/abs/2206.06930v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "FETILDA: An Effective Framework For Fin-tuned Embeddings For Long\n  Financial Text Documents", "abstract": "Unstructured data, especially text, continues to grow rapidly in various\ndomains. In particular, in the financial sphere, there is a wealth of\naccumulated unstructured financial data, such as the textual disclosure\ndocuments that companies submit on a regular basis to regulatory agencies, such\nas the Securities and Exchange Commission (SEC). These documents are typically\nvery long and tend to contain valuable soft information about a company's\nperformance. It is therefore of great interest to learn predictive models from\nthese long textual documents, especially for forecasting numerical key\nperformance indicators (KPIs). Whereas there has been a great progress in\npre-trained language models (LMs) that learn from tremendously large corpora of\ntextual data, they still struggle in terms of effective representations for\nlong documents. Our work fills this critical need, namely how to develop better\nmodels to extract useful information from long textual documents and learn\neffective features that can leverage the soft financial and risk information\nfor text regression (prediction) tasks. In this paper, we propose and implement\na deep learning framework that splits long documents into chunks and utilizes\npre-trained LMs to process and aggregate the chunks into vector\nrepresentations, followed by self-attention to extract valuable document-level\nfeatures. We evaluate our model on a collection of 10-K public disclosure\nreports from US banks, and another dataset of reports submitted by US\ncompanies. Overall, our framework outperforms strong baseline methods for\ntextual modeling as well as a baseline regression model using only numerical\ndata. Our work provides better insights into how utilizing pre-trained\ndomain-specific and fine-tuned long-input LMs in representing long documents\ncan improve the quality of representation of textual data, and therefore, help\nin improving predictive analyses.", "published": "2022-06-14 16:14:14", "link": "http://arxiv.org/abs/2206.06952v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Automatic Clipping: Differentially Private Deep Learning Made Easier and\n  Stronger", "abstract": "Per-example gradient clipping is a key algorithmic step that enables\npractical differential private (DP) training for deep learning models. The\nchoice of clipping threshold R, however, is vital for achieving high accuracy\nunder DP. We propose an easy-to-use replacement, called automatic clipping,\nthat eliminates the need to tune R for any DP optimizers, including DP-SGD,\nDP-Adam, DP-LAMB and many others. The automatic variants are as private and\ncomputationally efficient as existing DP optimizers, but require no DP-specific\nhyperparameters and thus make DP training as amenable as the standard\nnon-private training. We give a rigorous convergence analysis of automatic\nDP-SGD in the non-convex setting, showing that it can enjoy an asymptotic\nconvergence rate that matches the standard SGD, under a symmetric gradient\nnoise assumption of the per-sample gradients (commonly used in the non-DP\nliterature). We demonstrate on various language and vision tasks that automatic\nclipping outperforms or matches the state-of-the-art, and can be easily\nemployed with minimal changes to existing codebases.", "published": "2022-06-14 19:49:44", "link": "http://arxiv.org/abs/2206.07136v3", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Prioritized Training on Points that are Learnable, Worth Learning, and\n  Not Yet Learnt", "abstract": "Training on web-scale data can take months. But most computation and time is\nwasted on redundant and noisy points that are already learnt or not learnable.\nTo accelerate training, we introduce Reducible Holdout Loss Selection\n(RHO-LOSS), a simple but principled technique which selects approximately those\npoints for training that most reduce the model's generalization loss. As a\nresult, RHO-LOSS mitigates the weaknesses of existing data selection methods:\ntechniques from the optimization literature typically select 'hard' (e.g. high\nloss) points, but such points are often noisy (not learnable) or less\ntask-relevant. Conversely, curriculum learning prioritizes 'easy' points, but\nsuch points need not be trained on once learned. In contrast, RHO-LOSS selects\npoints that are learnable, worth learning, and not yet learnt. RHO-LOSS trains\nin far fewer steps than prior art, improves accuracy, and speeds up training on\na wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and\nBERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in\n18x fewer steps and reaches 2% higher final accuracy than uniform data\nshuffling.", "published": "2022-06-14 19:49:52", "link": "http://arxiv.org/abs/2206.07137v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Frequency-centroid features for word recognition of non-native English\n  speakers", "abstract": "The objective of this work is to investigate complementary features which can\naid the quintessential Mel frequency cepstral coefficients (MFCCs) in the task\nof closed, limited set word recognition for non-native English speakers of\ndifferent mother-tongues. Unlike the MFCCs, which are derived from the spectral\nenergy of the speech signal, the proposed frequency-centroids (FCs) encapsulate\nthe spectral centres of the different bands of the speech spectrum, with the\nbands defined by the Mel filterbank. These features, in combination with the\nMFCCs, are observed to provide relative performance improvement in English word\nrecognition, particularly under varied noisy conditions. A two-stage\nConvolution Neural Network (CNN) is used to model the features of the English\nwords uttered with Arabic, French and Spanish accents.", "published": "2022-06-14 21:19:49", "link": "http://arxiv.org/abs/2206.07176v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Codec at SemEval-2022 Task 5: Multi-Modal Multi-Transformer Misogynous\n  Meme Classification Framework", "abstract": "In this paper we describe our work towards building a generic framework for\nboth multi-modal embedding and multi-label binary classification tasks, while\nparticipating in task 5 (Multimedia Automatic Misogyny Identification) of\nSemEval 2022 competition.\n  Since pretraining deep models from scratch is a resource and data hungry\ntask, our approach is based on three main strategies. We combine different\nstate-of-the-art architectures to capture a wide spectrum of semantic signals\nfrom the multi-modal input. We employ a multi-task learning scheme to be able\nto use multiple datasets from the same knowledge domain to help increase the\nmodel's performance. We also use multiple objectives to regularize and fine\ntune different system components.", "published": "2022-06-14 22:37:25", "link": "http://arxiv.org/abs/2206.07190v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Foundation Models Talk Causality?", "abstract": "Foundation models are subject to an ongoing heated debate, leaving open the\nquestion of progress towards AGI and dividing the community into two camps: the\nones who see the arguably impressive results as evidence to the scaling\nhypothesis, and the others who are worried about the lack of interpretability\nand reasoning capabilities. By investigating to which extent causal\nrepresentations might be captured by these large scale language models, we make\na humble efforts towards resolving the ongoing philosophical conflicts.", "published": "2022-06-14 22:54:09", "link": "http://arxiv.org/abs/2206.10591v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Label-enhanced Prototypical Network with Contrastive Learning for\n  Multi-label Few-shot Aspect Category Detection", "abstract": "Multi-label aspect category detection allows a given review sentence to\ncontain multiple aspect categories, which is shown to be more practical in\nsentiment analysis and attracting increasing attention. As annotating large\namounts of data is time-consuming and labor-intensive, data scarcity occurs\nfrequently in real-world scenarios, which motivates multi-label few-shot aspect\ncategory detection. However, research on this problem is still in infancy and\nfew methods are available. In this paper, we propose a novel label-enhanced\nprototypical network (LPN) for multi-label few-shot aspect category detection.\nThe highlights of LPN can be summarized as follows. First, it leverages label\ndescription as auxiliary knowledge to learn more discriminative prototypes,\nwhich can retain aspect-relevant information while eliminating the harmful\neffect caused by irrelevant aspects. Second, it integrates with contrastive\nlearning, which encourages that the sentences with the same aspect label are\npulled together in embedding space while simultaneously pushing apart the\nsentences with different aspect labels. In addition, it introduces an adaptive\nmulti-label inference module to predict the aspect count in the sentence, which\nis simple yet effective. Extensive experimental results on three datasets\ndemonstrate that our proposed model LPN can consistently achieve\nstate-of-the-art performance.", "published": "2022-06-14 02:37:44", "link": "http://arxiv.org/abs/2206.13980v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech intelligibility of simulated hearing loss sounds and its\n  prediction using the Gammachirp Envelope Similarity Index (GESI)", "abstract": "In the present study, speech intelligibility (SI) experiments were performed\nusing simulated hearing loss (HL) sounds in laboratory and remote environments\nto clarify the effects of peripheral dysfunction. Noisy speech sounds were\nprocessed to simulate the average HL of 70- and 80-year-olds using Wadai\nHearing Impairment Simulator (WHIS). These sounds were presented to normal\nhearing (NH) listeners whose cognitive function could be assumed to be normal.\nThe results showed that the divergence was larger in the remote experiments\nthan in the laboratory ones. However, the remote results could be equalized to\nthe laboratory ones, mostly through data screening using the results of tone\npip tests prepared on the experimental web page. In addition, a newly proposed\nobjective intelligibility measure (OIM) called the Gammachirp Envelope\nSimilarity Index (GESI) explained the psychometric functions in the laboratory\nand remote experiments fairly well. GESI has the potential to explain the SI of\nHI listeners by properly setting HL parameters.", "published": "2022-06-14 03:22:30", "link": "http://arxiv.org/abs/2206.06573v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WHIS: Hearing impairment simulator based on the gammachirp auditory\n  filterbank", "abstract": "A new version of a hearing impairment simulator (WHIS) was implemented based\non a revised version of the gammachirp filterbank (GCFB), which incorporates\nfast frame-based processing, absolute threshold (AT), an audiogram of a\nhearing-impaired (HI) listener, and a parameter to control the cochlear\ninput-output (IO) function. The parameter referred to as the compression health\n$\\alpha$ controlled the slope of the IO function to range from normal hearing\n(NH) listeners to HI listeners, without largely changing the total hearing loss\n(HL). The new WHIS was designed provide an NH listener the same EPs as those of\na target HI listener.The analysis part of WHIS was almost the same as that of\nthe revised GCFB, except that the IO function was used instead of the gain\nfunction. We proposed two synthesis methods: a direct time-varying filter for\nperceptually small distortion and a filterbank analysis-synthesis for further\nHI simulations including temporal smearing. We evaluated the WHIS family and a\nCambridge version of the HL simulator (CamHLS) in terms of differences in the\nIO function and spectral distance. The IO functions were simulated fairly well\nat $\\alpha$ less than 0.5 but not at $\\alpha$ equal to 1. Thus, it is difficult\nto simulate the HL when the IO function is sufficiently healthy. This is a\nfundamental limit of any existing HL simulator as well as WHIS. The new WHIS\nyielded a smaller spectral distortion than CamHLS and was fairly compatible\nwith the previous version.", "published": "2022-06-14 05:54:04", "link": "http://arxiv.org/abs/2206.06604v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Audio Synthesis with Complex-valued Polynomial Networks", "abstract": "Time-frequency (TF) representations in audio synthesis have been increasingly\nmodeled with real-valued networks. However, overlooking the complex-valued\nnature of TF representations can result in suboptimal performance and require\nadditional modules (e.g., for modeling the phase). To this end, we introduce\ncomplex-valued polynomial networks, called APOLLO, that integrate such\ncomplex-valued representations in a natural way. Concretely, APOLLO captures\nhigh-order correlations of the input elements using high-order tensors as\nscaling parameters. By leveraging standard tensor decompositions, we derive\ndifferent architectures and enable modeling richer correlations. We outline\nsuch architectures and showcase their performance in audio generation across\nfour benchmarks. As a highlight, APOLLO results in $17.5\\%$ improvement over\nadversarial methods and $8.2\\%$ over the state-of-the-art diffusion models on\nSC09 dataset in audio generation. Our models can encourage the systematic\ndesign of other efficient architectures on the complex field.", "published": "2022-06-14 12:58:59", "link": "http://arxiv.org/abs/2206.06811v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "LPCSE: Neural Speech Enhancement through Linear Predictive Coding", "abstract": "The increasingly stringent requirement on quality-of-experience in 5G/B5G\ncommunication systems has led to the emerging neural speech enhancement\ntechniques, which however have been developed in isolation from the existing\nexpert-rule based models of speech pronunciation and distortion, such as the\nclassic Linear Predictive Coding (LPC) speech model because it is difficult to\nintegrate the models with auto-differentiable machine learning frameworks. In\nthis paper, to improve the efficiency of neural speech enhancement, we\nintroduce an LPC-based speech enhancement (LPCSE) architecture, which leverages\nthe strong inductive biases in the LPC speech model in conjunction with the\nexpressive power of neural networks. Differentiable end-to-end learning is\nachieved in LPCSE via two novel blocks: a block that utilizes the expert rules\nto reduce the computational overhead when integrating the LPC speech model into\nneural networks, and a block that ensures the stability of the model and avoids\nexploding gradients in end-to-end training by mapping the Linear prediction\ncoefficients to the filter poles. The experimental results show that LPCSE\nsuccessfully restores the formants of the speeches distorted by transmission\nloss, and outperforms two existing neural speech enhancement methods of\ncomparable neural network sizes in terms of the Perceptual evaluation of speech\nquality (PESQ) and Short-Time Objective Intelligibility (STOI) on the LJ Speech\ncorpus.", "published": "2022-06-14 15:04:33", "link": "http://arxiv.org/abs/2206.06908v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring speaker enrolment for few-shot personalisation in emotional\n  vocalisation prediction", "abstract": "In this work, we explore a novel few-shot personalisation architecture for\nemotional vocalisation prediction. The core contribution is an `enrolment'\nencoder which utilises two unlabelled samples of the target speaker to adjust\nthe output of the emotion encoder; the adjustment is based on dot-product\nattention, thus effectively functioning as a form of `soft' feature selection.\nThe emotion and enrolment encoders are based on two standard audio\narchitectures: CNN14 and CNN10. The two encoders are further guided to forget\nor learn auxiliary emotion and/or speaker information. Our best approach\nachieves a CCC of $.650$ on the ExVo Few-Shot dev set, a $2.5\\%$ increase over\nour baseline CNN14 CCC of $.634$.", "published": "2022-06-14 08:15:16", "link": "http://arxiv.org/abs/2206.06680v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
