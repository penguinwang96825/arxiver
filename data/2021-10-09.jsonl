{"title": "Detecting Community Sensitive Norm Violations in Online Conversations", "abstract": "Online platforms and communities establish their own norms that govern what\nbehavior is acceptable within the community. Substantial effort in NLP has\nfocused on identifying unacceptable behaviors and, recently, on forecasting\nthem before they occur. However, these efforts have largely focused on toxicity\nas the sole form of community norm violation. Such focus has overlooked the\nmuch larger set of rules that moderators enforce. Here, we introduce a new\ndataset focusing on a more complete spectrum of community norms and their\nviolations in the local conversational and global community contexts. We\nintroduce a series of models that use this data to develop context- and\ncommunity-sensitive norm violation detection, showing that these changes give\nhigh performance.", "published": "2021-10-09 00:39:35", "link": "http://arxiv.org/abs/2110.04419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Distantly-Supervised Named Entity Recognition with\n  Self-Collaborative Denoising Learning", "abstract": "Distantly supervised named entity recognition (DS-NER) efficiently reduces\nlabor costs but meanwhile intrinsically suffers from the label noise due to the\nstrong assumption of distant supervision. Typically, the wrongly labeled\ninstances comprise numbers of incomplete and inaccurate annotation noise, while\nmost prior denoising works are only concerned with one kind of noise and fail\nto fully explore useful information in the whole training set. To address this\nissue, we propose a robust learning paradigm named Self-Collaborative Denoising\nLearning (SCDL), which jointly trains two teacher-student networks in a\nmutually-beneficial manner to iteratively perform noisy label refinery. Each\nnetwork is designed to exploit reliable labels via self denoising, and two\nnetworks communicate with each other to explore unreliable annotations by\ncollaborative denoising. Extensive experimental results on five real-world\ndatasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising\nmethods.", "published": "2021-10-09 01:45:03", "link": "http://arxiv.org/abs/2110.04429v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging recent advances in Pre-Trained Language Models\n  forEye-Tracking Prediction", "abstract": "Cognitively inspired Natural Language Pro-cessing uses human-derived\nbehavioral datalike eye-tracking data, which reflect the seman-tic\nrepresentations of language in the humanbrain to augment the neural nets to\nsolve arange of tasks spanning syntax and semanticswith the aim of teaching\nmachines about lan-guage processing mechanisms. In this paper,we use the ZuCo\n1.0 and ZuCo 2.0 dataset con-taining the eye-gaze features to explore\ndiffer-ent linguistic models to directly predict thesegaze features for each\nword with respect to itssentence. We tried different neural networkmodels with\nthe words as inputs to predict thetargets. And after lots of experimentation\nandfeature engineering finally devised a novel ar-chitecture consisting of\nRoBERTa Token Clas-sifier with a dense layer on top for languagemodeling and a\nstand-alone model consistingof dense layers followed by a transformer layerfor\nthe extra features we engineered. Finally,we took the mean of the outputs of\nboth thesemodels to make the final predictions. We eval-uated the models using\nmean absolute error(MAE) and the R2 score for each target.", "published": "2021-10-09 06:46:48", "link": "http://arxiv.org/abs/2110.04475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bayesian Active Summarization", "abstract": "Bayesian Active Learning has had significant impact to various NLP problems,\nbut nevertheless it's application to text summarization has been explored very\nlittle. We introduce Bayesian Active Summarization (BAS), as a method of\ncombining active learning methods with state-of-the-art summarization models.\nOur findings suggest that BAS achieves better and more robust performance,\ncompared to random selection, particularly for small and very small data\nannotation budgets. Using BAS we showcase it is possible to leverage large\nsummarization models to effectively solve real-world problems with very limited\nannotated data.", "published": "2021-10-09 06:51:16", "link": "http://arxiv.org/abs/2110.04480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Isotropy Analysis in the Multilingual BERT Embedding Space", "abstract": "Several studies have explored various advantages of multilingual pre-trained\nmodels (such as multilingual BERT) in capturing shared linguistic knowledge.\nHowever, less attention has been paid to their limitations. In this paper, we\ninvestigate the multilingual BERT for two known issues of the monolingual\nmodels: anisotropic embedding space and outlier dimensions. We show that,\nunlike its monolingual counterpart, the multilingual BERT model exhibits no\noutlier dimension in its representations while it has a highly anisotropic\nspace. There are a few dimensions in the monolingual BERT with high\ncontributions to the anisotropic distribution. However, we observe no such\ndimensions in the multilingual BERT. Furthermore, our experimental results\ndemonstrate that increasing the isotropy of multilingual space can\nsignificantly improve its representation power and performance, similarly to\nwhat had been observed for monolingual CWRs on semantic similarity tasks. Our\nanalysis indicates that, despite having different degenerated directions, the\nembedding spaces in various languages tend to be partially similar with respect\nto their structures.", "published": "2021-10-09 08:29:49", "link": "http://arxiv.org/abs/2110.04504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations", "abstract": "NLP models that compare or consolidate information across multiple documents\noften struggle when challenged with recognizing substantial information\nredundancies across the texts. For example, in multi-document summarization it\nis crucial to identify salient information across texts and then generate a\nnon-redundant summary, while facing repeated and usually differently-phrased\nsalient content. To facilitate researching such challenges, the sentence-level\ntask of \\textit{sentence fusion} was proposed, yet previous datasets for this\ntask were very limited in their size and scope. In this paper, we revisit and\nsubstantially extend previous dataset creation efforts. With careful\nmodifications, relabeling and employing complementing data sources, we were\nable to triple the size of a notable earlier dataset. Moreover, we show that\nour extended version uses more representative texts for multi-document tasks\nand provides a larger and more diverse training set, which substantially\nimproves model training.", "published": "2021-10-09 09:15:05", "link": "http://arxiv.org/abs/2110.04517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DMRST: A Joint Framework for Document-Level Multilingual RST Discourse\n  Segmentation and Parsing", "abstract": "Text discourse parsing weighs importantly in understanding information flow\nand argumentative structure in natural language, making it beneficial for\ndownstream tasks. While previous work significantly improves the performance of\nRST discourse parsing, they are not readily applicable to practical use cases:\n(1) EDU segmentation is not integrated into most existing tree parsing\nframeworks, thus it is not straightforward to apply such models on newly-coming\ndata. (2) Most parsers cannot be used in multilingual scenarios, because they\nare developed only in English. (3) Parsers trained from single-domain treebanks\ndo not generalize well on out-of-domain inputs. In this work, we propose a\ndocument-level multilingual RST discourse parsing framework, which conducts EDU\nsegmentation and discourse tree parsing jointly. Moreover, we propose a\ncross-translation augmentation strategy to enable the framework to support\nmultilingual parsing and improve its domain generality. Experimental results\nshow that our model achieves state-of-the-art performance on document-level\nmultilingual RST parsing in all sub-tasks.", "published": "2021-10-09 09:15:56", "link": "http://arxiv.org/abs/2110.04518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rumor Detection on Twitter with Claim-Guided Hierarchical Graph\n  Attention Networks", "abstract": "Rumors are rampant in the era of social media. Conversation structures\nprovide valuable clues to differentiate between real and fake claims. However,\nexisting rumor detection methods are either limited to the strict relation of\nuser responses or oversimplify the conversation structure. In this study, to\nsubstantially reinforces the interaction of user opinions while alleviating the\nnegative impact imposed by irrelevant posts, we first represent the\nconversation thread as an undirected interaction graph. We then present a\nClaim-guided Hierarchical Graph Attention Network for rumor classification,\nwhich enhances the representation learning for responsive posts considering the\nentire social contexts and attends over the posts that can semantically infer\nthe target claim. Extensive experiments on three Twitter datasets demonstrate\nthat our rumor detection method achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for detecting rumors\nat early stages.", "published": "2021-10-09 09:24:11", "link": "http://arxiv.org/abs/2110.04522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multi-Party Dialogue Discourse Parsing via Domain Integration", "abstract": "While multi-party conversations are often less structured than monologues and\ndocuments, they are implicitly organized by semantic level correlations across\nthe interactive turns, and dialogue discourse analysis can be applied to\npredict the dependency structure and relations between the elementary discourse\nunits, and provide feature-rich structural information for downstream tasks.\nHowever, the existing corpora with dialogue discourse annotation are collected\nfrom specific domains with limited sample sizes, rendering the performance of\ndata-driven approaches poor on incoming dialogues without any domain\nadaptation. In this paper, we first introduce a Transformer-based parser, and\nassess its cross-domain performance. We next adopt three methods to gain domain\nintegration from both data and language modeling perspectives to improve the\ngeneralization capability. Empirical results show that the neural parser can\nbenefit from our proposed methods, and performs better on cross-domain dialogue\nsamples.", "published": "2021-10-09 09:36:22", "link": "http://arxiv.org/abs/2110.04526v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empathetic Response Generation through Graph-based Multi-hop Reasoning\n  on Emotional Causality", "abstract": "Empathetic response generation aims to comprehend the user emotion and then\nrespond to it appropriately. Most existing works merely focus on what the\nemotion is and ignore how the emotion is evoked, thus weakening the capacity of\nthe model to understand the emotional experience of the user for generating\nempathetic responses. To tackle this problem, we consider the emotional\ncausality, namely, what feelings the user expresses (i.e., emotion) and why the\nuser has such feelings (i.e., cause). Then, we propose a novel graph-based\nmodel with multi-hop reasoning to model the emotional causality of the\nempathetic conversation. Finally, we demonstrate the effectiveness of our model\non EMPATHETICDIALOGUES in comparison with several competitive models.", "published": "2021-10-09 17:12:41", "link": "http://arxiv.org/abs/2110.04614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangled Sequence to Sequence Learning for Compositional\n  Generalization", "abstract": "There is mounting evidence that existing neural network models, in particular\nthe very popular sequence-to-sequence architecture, struggle to systematically\ngeneralize to unseen compositions of seen components. We demonstrate that one\nof the reasons hindering compositional generalization relates to\nrepresentations being entangled. We propose an extension to\nsequence-to-sequence models which encourages disentanglement by adaptively\nre-encoding (at each time step) the source input. Specifically, we condition\nthe source representations on the newly decoded target context which makes it\neasier for the encoder to exploit specialized information for each prediction\nrather than capturing it all in a single forward pass. Experimental results on\nsemantic parsing and machine translation empirically show that our proposal\ndelivers more disentangled representations and better generalization.", "published": "2021-10-09 22:27:19", "link": "http://arxiv.org/abs/2110.04655v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Keyword Spotting using Xception-1d", "abstract": "The field of conversational agents is growing fast and there is an increasing\nneed for algorithms that enhance natural interaction. In this work we show how\nwe achieved state of the art results in the Keyword Spotting field by adapting\nand tweaking the Xception algorithm, which achieved outstanding results in\nseveral computer vision tasks. We obtained about 96\\% accuracy when classifying\naudio clips belonging to 35 different categories, beating human annotation at\nthe most complex tasks proposed.", "published": "2021-10-09 00:17:20", "link": "http://arxiv.org/abs/2110.07498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two-stage Visual Cues Enhancement Network for Referring Image\n  Segmentation", "abstract": "Referring Image Segmentation (RIS) aims at segmenting the target object from\nan image referred by one given natural language expression. The diverse and\nflexible expressions as well as complex visual contents in the images raise the\nRIS model with higher demands for investigating fine-grained matching behaviors\nbetween words in expressions and objects presented in images. However, such\nmatching behaviors are hard to be learned and captured when the visual cues of\nreferents (i.e. referred objects) are insufficient, as the referents with weak\nvisual cues tend to be easily confused by cluttered background at boundary or\neven overwhelmed by salient objects in the image. And the insufficient visual\ncues issue can not be handled by the cross-modal fusion mechanisms as done in\nprevious work. In this paper, we tackle this problem from a novel perspective\nof enhancing the visual information for the referents by devising a Two-stage\nVisual cues enhancement Network (TV-Net), where a novel Retrieval and\nEnrichment Scheme (RES) and an Adaptive Multi-resolution feature Fusion (AMF)\nmodule are proposed. Through the two-stage enhancement, our proposed TV-Net\nenjoys better performances in learning fine-grained matching behaviors between\nthe natural language expression and image, especially when the visual\ninformation of the referent is inadequate, thus produces better segmentation\nresults. Extensive experiments are conducted to validate the effectiveness of\nthe proposed method on the RIS task, with our proposed TV-Net surpassing the\nstate-of-the-art approaches on four benchmark datasets.", "published": "2021-10-09 02:53:39", "link": "http://arxiv.org/abs/2110.04435v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining\n  Example Design", "abstract": "Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pretraining example\nyields improved sentence representations and open domain question answering\nabilities. This theoretically motivated degree of freedom for pretraining\nexample design indicates new training schemes for self-improving\nrepresentations.", "published": "2021-10-09 11:05:16", "link": "http://arxiv.org/abs/2110.04541v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters", "abstract": "Large-scale contrastive vision-language pre-training has shown significant\nprogress in visual representation learning. Unlike traditional visual systems\ntrained by a fixed set of discrete labels, a new paradigm was introduced in\n\\cite{radford2021learning} to directly learn to align images with raw texts in\nan open-vocabulary setting. On downstream tasks, a carefully chosen text prompt\nis employed to make zero-shot predictions.~To avoid non-trivial prompt\nengineering, context optimization \\cite{zhou2021coop} has been proposed to\nlearn continuous vectors as task-specific prompts with few-shot training\nexamples.~In this paper, we show that there is an alternative path to achieve\nbetter vision-language models other than prompt tuning.~While prompt tuning is\nfor the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with\nfeature adapters on either visual or language branch. Specifically,\nCLIP-Adapter adopts an additional bottleneck layer to learn new features and\nperforms residual-style feature blending with the original pre-trained\nfeatures.~As a consequence, CLIP-Adapter is able to outperform context\noptimization while maintains a simple design. Experiments and extensive\nablation studies on various visual classification tasks demonstrate the\neffectiveness of our approach. Code is released at t\nhttps://github.com/gaopengcuhk/CLIP-Adapter.", "published": "2021-10-09 11:39:30", "link": "http://arxiv.org/abs/2110.04544v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Framework for Rationale Extraction for Deep QA models", "abstract": "As neural-network-based QA models become deeper and more complex, there is a\ndemand for robust frameworks which can access a model's rationale for its\nprediction. Current techniques that provide insights on a model's working are\neither dependent on adversarial datasets or are proposing models with explicit\nexplanation generation components. These techniques are time-consuming and\nchallenging to extend to existing models and new datasets. In this work, we use\n`Integrated Gradients' to extract rationale for existing state-of-the-art\nmodels in the task of Reading Comprehension based Question Answering (RCQA). On\ndetailed analysis and comparison with collected human rationales, we find that\nthough ~40-80% words of extracted rationale coincide with the human rationale\n(precision), only 6-19% of human rationale is present in the extracted\nrationale (recall).", "published": "2021-10-09 18:02:55", "link": "http://arxiv.org/abs/2110.04620v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Relation between Syntactic Divergence and Zero-Shot Performance", "abstract": "We explore the link between the extent to which syntactic relations are\npreserved in translation and the ease of correctly constructing a parse tree in\na zero-shot setting. While previous work suggests such a relation, it tends to\nfocus on the macro level and not on the level of individual edges-a gap we aim\nto address. As a test case, we take the transfer of Universal Dependencies (UD)\nparsing from English to a diverse set of languages and conduct two sets of\nexperiments. In one, we analyze zero-shot performance based on the extent to\nwhich English source edges are preserved in translation. In another, we apply\nthree linguistically motivated transformations to UD, creating more\ncross-lingually stable versions of it, and assess their zero-shot parsability.\nIn order to compare parsing performance across different schemes, we perform\nextrinsic evaluation on the downstream task of cross-lingual relation\nextraction (RE) using a subset of a popular English RE benchmark translated to\nRussian and Korean. In both sets of experiments, our results suggest a strong\nrelation between cross-lingual stability and zero-shot parsing performance.", "published": "2021-10-09 21:09:21", "link": "http://arxiv.org/abs/2110.04644v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Follow Language Instructions with Compositional Policies", "abstract": "We propose a framework that learns to execute natural language instructions\nin an environment consisting of goal-reaching tasks that share components of\ntheir task descriptions. Our approach leverages the compositionality of both\nvalue functions and language, with the aim of reducing the sample complexity of\nlearning novel tasks. First, we train a reinforcement learning agent to learn\nvalue functions that can be subsequently composed through a Boolean algebra to\nsolve novel tasks. Second, we fine-tune a seq2seq model pretrained on web-scale\ncorpora to map language to logical expressions that specify the required value\nfunction compositions. Evaluating our agent in the BabyAI domain, we observe a\ndecrease of 86% in the number of training steps needed to learn a second task\nafter mastering a single task. Results from ablation studies further indicate\nthat it is the combination of compositional value functions and language\nrepresentations that allows the agent to quickly generalize to new tasks.", "published": "2021-10-09 21:28:26", "link": "http://arxiv.org/abs/2110.04647v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Paperswithtopic: Topic Identification from Paper Title Only", "abstract": "The deep learning field is growing rapidly as witnessed by the exponential\ngrowth of papers submitted to journals, conferences, and pre-print servers. To\ncope with the sheer number of papers, several text mining tools from natural\nlanguage processing (NLP) have been proposed that enable researchers to keep\ntrack of recent findings. In this context, our paper makes two main\ncontributions: first, we collected and annotated a dataset of papers paired by\ntitle and sub-field from the field of artificial intelligence (AI), and,\nsecond, we present results on how to predict a paper's AI sub-field from a\ngiven paper title only. Importantly, for the latter, short-text classification\ntask we compare several algorithms from conventional machine learning all the\nway up to recent, larger transformer architectures. Finally, for the\ntransformer models, we also present gradient-based, attention visualizations to\nfurther explain the model's classification process. All code can be found at\n\\url{https://github.com/1pha/paperswithtopic}", "published": "2021-10-09 06:32:09", "link": "http://arxiv.org/abs/2110.15721v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Natural Language for Human-Robot Collaboration: Problems Beyond Language\n  Grounding", "abstract": "To enable robots to instruct humans in collaborations, we identify several\naspects of language processing that are not commonly studied in this context.\nThese include location, planning, and generation. We suggest evaluations for\neach task, offer baselines for simple methods, and close by discussing\nchallenges and opportunities in studying language for collaboration.", "published": "2021-10-09 03:24:38", "link": "http://arxiv.org/abs/2110.04441v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis", "abstract": "This work presents a lifelong learning approach to train a multilingual\nText-To-Speech (TTS) system, where each language was seen as an individual task\nand was learned sequentially and continually. It does not require pooled data\nfrom all languages altogether, and thus alleviates the storage and computation\nburden. One of the challenges of lifelong learning methods is \"catastrophic\nforgetting\": in TTS scenario it means that model performance quickly degrades\non previous languages when adapted to a new language. We approach this problem\nvia a data-replay-based lifelong learning method. We formulate the replay\nprocess as a supervised learning problem, and propose a simple yet effective\ndual-sampler framework to tackle the heavily language-imbalanced training\nsamples. Through objective and subjective evaluations, we show that this\nsupervised learning formulation outperforms other gradient-based and\nregularization-based lifelong learning methods, achieving 43% Mel-Cepstral\nDistortion reduction compared to a fine-tuning baseline.", "published": "2021-10-09 07:00:38", "link": "http://arxiv.org/abs/2110.04482v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR", "abstract": "Self-supervised pre-training could effectively improve the performance of\nlow-resource automatic speech recognition (ASR). However, existing\nself-supervised pre-training are task-agnostic, i.e., could be applied to\nvarious downstream tasks. Although it enlarges the scope of its application,\nthe capacity of the pre-trained model is not fully utilized for the ASR task,\nand the learned representations may not be optimal for ASR. In this work, in\norder to build a better pre-trained model for low-resource ASR, we propose a\npre-training approach called wav2vec-S, where we use task-specific\nsemi-supervised pre-training to refine the self-supervised pre-trained model\nfor the ASR task thus more effectively utilize the capacity of the pre-trained\nmodel to generate task-specific representations for ASR. Experiments show that\ncompared to wav2vec 2.0, wav2vec-S only requires a marginal increment of\npre-training time but could significantly improve ASR performance on in-domain,\ncross-domain and cross-lingual datasets. Average relative WER reductions are\n24.5% and 6.6% for 1h and 10h fine-tuning, respectively. Furthermore, we show\nthat semi-supervised pre-training could close the representation gap between\nthe self-supervised pre-trained model and the corresponding fine-tuned model\nthrough canonical correlation analysis.", "published": "2021-10-09 07:09:22", "link": "http://arxiv.org/abs/2110.04484v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS\n  With Accurate Phoneme Duration Control", "abstract": "Sequence expansion between encoder and decoder is a critical challenge in\nsequence-to-sequence TTS. Attention-based methods achieve great naturalness but\nsuffer from unstable issues like missing and repeating phonemes, not to mention\naccurate duration control. Duration-informed methods, on the contrary, seem to\neasily adjust phoneme duration but show obvious degradation in speech\nnaturalness. This paper proposes PAMA-TTS to address the problem. It takes the\nadvantage of both flexible attention and explicit duration models. Based on the\nmonotonic attention mechanism, PAMA-TTS also leverages token duration and\nrelative position of a frame, especially countdown information, i.e. in how\nmany future frames the present phoneme will end. They help the attention to\nmove forward along the token sequence in a soft but reliable control.\nExperimental results prove that PAMA-TTS achieves the highest naturalness,\nwhile has on-par or even better duration controllability than the\nduration-informed model.", "published": "2021-10-09 07:16:14", "link": "http://arxiv.org/abs/2110.04486v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Generating Disentangled Arguments with Prompts: A Simple Event\n  Extraction Framework that Works", "abstract": "Event Extraction bridges the gap between text and event signals. Based on the\nassumption of trigger-argument dependency, existing approaches have achieved\nstate-of-the-art performance with expert-designed templates or complicated\ndecoding constraints. In this paper, for the first time we introduce the\nprompt-based learning strategy to the domain of Event Extraction, which\nempowers the automatic exploitation of label semantics on both input and output\nsides. To validate the effectiveness of the proposed generative method, we\nconduct extensive experiments with 11 diverse baselines. Empirical results show\nthat, in terms of F1 score on Argument Extraction, our simple architecture is\nstronger than any other generative counterpart and even competitive with\nalgorithms that require template engineering. Regarding the measure of recall,\nit sets new overall records for both Argument and Trigger Extractions. We\nhereby recommend this framework to the community, with the code publicly\navailable at https://git.io/GDAP.", "published": "2021-10-09 09:36:08", "link": "http://arxiv.org/abs/2110.04525v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Exploration of Self-Supervised Pretrained Representations for\n  End-to-End Speech Recognition", "abstract": "Self-supervised pretraining on speech data has achieved a lot of progress.\nHigh-fidelity representation of the speech signal is learned from a lot of\nuntranscribed data and shows promising performance. Recently, there are several\nworks focusing on evaluating the quality of self-supervised pretrained\nrepresentations on various tasks without domain restriction, e.g. SUPERB.\nHowever, such evaluations do not provide a comprehensive comparison among many\nASR benchmark corpora. In this paper, we focus on the general applications of\npretrained speech representations, on advanced end-to-end automatic speech\nrecognition (E2E-ASR) models. We select several pretrained speech\nrepresentations and present the experimental results on various open-source and\npublicly available corpora for E2E-ASR. Without any modification of the\nback-end model architectures or training strategy, some of the experiments with\npretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or\noutperform current state-of-the-art (SOTA) recognition performance. Moreover,\nwe further explore more scenarios for whether the pretraining representations\nare effective, such as the cross-language or overlapped speech. The scripts,\nconfiguratons and the trained models have been released in ESPnet to let the\ncommunity reproduce our experiments and improve them.", "published": "2021-10-09 15:06:09", "link": "http://arxiv.org/abs/2110.04590v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Personalized Automatic Speech Recognition Trained on Small Disordered\n  Speech Datasets", "abstract": "This study investigates the performance of personalized automatic speech\nrecognition (ASR) for recognizing disordered speech using small amounts of\nper-speaker adaptation data. We trained personalized models for 195 individuals\nwith different types and severities of speech impairment with training sets\nranging in size from <1 minute to 18-20 minutes of speech data. Word error rate\n(WER) thresholds were selected to determine Success Percentage (the percentage\nof personalized models reaching the target WER) in different application\nscenarios. For the home automation scenario, 79% of speakers reached the target\nWER with 18-20 minutes of speech; but even with only 3-4 minutes of speech, 63%\nof speakers reached the target WER. Further evaluation found similar\nimprovement on test sets with conversational and out-of-domain, unprompted\nphrases. Our results demonstrate that with only a few minutes of recordings,\nindividuals with disordered speech could benefit from personalized ASR.", "published": "2021-10-09 17:11:17", "link": "http://arxiv.org/abs/2110.04612v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transformer Network for Semantically-Aware and Speech-Driven Upper-Face\n  Generation", "abstract": "We propose a semantically-aware speech driven model to generate expressive\nand natural upper-facial and head motion for Embodied Conversational Agents\n(ECA). In this work, we aim to produce natural and continuous head motion and\nupper-facial gestures synchronized with speech. We propose a model that\ngenerates these gestures based on multimodal input features: the first modality\nis text, and the second one is speech prosody. Our model makes use of\nTransformers and Convolutions to map the multimodal features that correspond to\nan utterance to continuous eyebrows and head gestures. We conduct subjective\nand objective evaluations to validate our approach and compare it with state of\nthe art.", "published": "2021-10-09 09:38:40", "link": "http://arxiv.org/abs/2110.04527v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Lightweight Applications: Asymmetric Enroll-Verify Structure for\n  Speaker Verification", "abstract": "With the development of deep learning, automatic speaker verification has\nmade considerable progress over the past few years. However, to design a\nlightweight and robust system with limited computational resources is still a\nchallenging problem. Traditionally, a speaker verification system is\nsymmetrical, indicating that the same embedding extraction model is applied for\nboth enrollment and verification in inference. In this paper, we come up with\nan innovative asymmetric structure, which takes the large-scale ECAPA-TDNN\nmodel for enrollment and the small-scale ECAPA-TDNNLite model for verification.\nAs a symmetrical system, our proposed ECAPA-TDNNLite model achieves an EER of\n3.07% on the Voxceleb1 original test set with only 11.6M FLOPS. Moreover, the\nasymmetric structure further reduces the EER to 2.31%, without increasing any\ncomputational costs during verification.", "published": "2021-10-09 03:08:01", "link": "http://arxiv.org/abs/2110.04438v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Mutual learning framework for Few-shot Sound Event Detection", "abstract": "Although prototypical network (ProtoNet) has proved to be an effective method\nfor few-shot sound event detection, two problems still exist. Firstly, the\nsmall-scaled support set is insufficient so that the class prototypes may not\nrepresent the class center accurately. Secondly, the feature extractor is\ntask-agnostic (or class-agnostic): the feature extractor is trained with\nbase-class data and directly applied to unseen-class data. To address these\nissues, we present a novel mutual learning framework with transductive\nlearning, which aims at iteratively updating the class prototypes and feature\nextractor. More specifically, we propose to update class prototypes with\ntransductive inference to make the class prototypes as close to the true class\ncenter as possible. To make the feature extractor to be task-specific, we\npropose to use the updated class prototypes to fine-tune the feature extractor.\nAfter that, a fine-tuned feature extractor further helps produce better class\nprototypes. Our method achieves the F-score of 38.4$\\%$ on the DCASE 2021 Task\n5 evaluation set, which won the first place in the few-shot bioacoustic event\ndetection task of Detection and Classification of Acoustic Scenes and Events\n(DCASE) 2021 Challenge.", "published": "2021-10-09 06:45:41", "link": "http://arxiv.org/abs/2110.04474v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data Augmentation with Locally-time Reversed Speech for Automatic Speech\n  Recognition", "abstract": "Psychoacoustic studies have shown that locally-time reversed (LTR) speech,\ni.e., signal samples time-reversed within a short segment, can be accurately\nrecognised by human listeners. This study addresses the question of how well a\nstate-of-the-art automatic speech recognition (ASR) system would perform on LTR\nspeech. The underlying objective is to explore the feasibility of deploying LTR\nspeech in the training of end-to-end (E2E) ASR models, as an attempt to data\naugmentation for improving the recognition performance. The investigation\nstarts with experiments to understand the effect of LTR speech on\ngeneral-purpose ASR. LTR speech with reversed segment duration of 5 ms - 50 ms\nis rendered and evaluated. For ASR training data augmentation with LTR speech,\ntraining sets are created by combining natural speech with different partitions\nof LTR speech. The efficacy of data augmentation is confirmed by ASR results on\nspeech corpora in various languages and speaking styles. ASR on LTR speech with\nreversed segment duration of 15 ms - 30 ms is found to have lower error rate\nthan with other segment duration. Data augmentation with these LTR speech\nachieves satisfactory and consistent improvement on ASR performance.", "published": "2021-10-09 09:00:39", "link": "http://arxiv.org/abs/2110.04511v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An evaluation of data augmentation methods for sound scene geotagging", "abstract": "Sound scene geotagging is a new topic of research which has evolved from\nacoustic scene classification. It is motivated by the idea of audio\nsurveillance. Not content with only describing a scene in a recording, a\nmachine which can locate where the recording was captured would be of use to\nmany. In this paper we explore a series of common audio data augmentation\nmethods to evaluate which best improves the accuracy of audio geotagging\nclassifiers. Our work improves on the state-of-the-art city geotagging method\nby 23% in terms of classification accuracy.", "published": "2021-10-09 14:50:28", "link": "http://arxiv.org/abs/2110.04585v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Approach for Assessing Neuromotor Coordination in\n  Schizophrenia Using Convolutional Neural Networks", "abstract": "This study investigates the speech articulatory coordination in schizophrenia\nsubjects exhibiting strong positive symptoms (e.g. hallucinations and\ndelusions), using two distinct channel-delay correlation methods. We show that\nthe schizophrenic subjects with strong positive symptoms and who are markedly\nill pose complex articulatory coordination pattern in facial and speech\ngestures than what is observed in healthy subjects. This distinction in speech\ncoordination pattern is used to train a multimodal convolutional neural network\n(CNN) which uses video and audio data during speech to distinguish\nschizophrenic patients with strong positive symptoms from healthy subjects. We\nalso show that the vocal tract variables (TVs) which correspond to place of\narticulation and glottal source outperform the Mel-frequency Cepstral\nCoefficients (MFCCs) when fused with Facial Action Units (FAUs) in the proposed\nmultimodal network. For the clinical dataset we collected, our best performing\nmultimodal network improves the mean F1 score for detecting schizophrenia by\naround 18% with respect to the full vocal tract coordination (FVTC) baseline\nmethod implemented with fusing FAUs and MFCCs.", "published": "2021-10-09 03:11:46", "link": "http://arxiv.org/abs/2110.04440v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using multiple reference audios and style embedding constraints for\n  speech synthesis", "abstract": "The end-to-end speech synthesis model can directly take an utterance as\nreference audio, and generate speech from the text with prosody and speaker\ncharacteristics similar to the reference audio. However, an appropriate\nacoustic embedding must be manually selected during inference. Due to the fact\nthat only the matched text and speech are used in the training process, using\nunmatched text and speech for inference would cause the model to synthesize\nspeech with low content quality. In this study, we propose to mitigate these\ntwo problems by using multiple reference audios and style embedding constraints\nrather than using only the target audio. Multiple reference audios are\nautomatically selected using the sentence similarity determined by\nBidirectional Encoder Representations from Transformers (BERT). In addition, we\nuse ''target'' style embedding from a Pre-trained encoder as a constraint by\nconsidering the mutual information between the predicted and ''target'' style\nembedding. The experimental results show that the proposed model can improve\nthe speech naturalness and content quality with multiple reference audios and\ncan also outperform the baseline model in ABX preference tests of style\nsimilarity.", "published": "2021-10-09 04:24:29", "link": "http://arxiv.org/abs/2110.04451v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visually Exploring Multi-Purpose Audio Data", "abstract": "We analyse multi-purpose audio using tools to visualise similarities within\nthe data that may be observed via unsupervised methods. The success of machine\nlearning classifiers is affected by the information contained within system\ninputs, so we investigate whether latent patterns within the data may explain\nperformance limitations of such classifiers. We use the visual assessment of\ncluster tendency (VAT) technique on a well known data set to observe how the\nsamples naturally cluster, and we make comparisons to the labels used for audio\ngeotagging and acoustic scene classification. We demonstrate that VAT helps to\nexplain and corroborate confusions observed in prior work to classify this\naudio, yielding greater insight into the performance - and limitations - of\nsupervised classification systems. While this exploratory analysis is conducted\non data for which we know the \"ground truth\" labels, this method of visualising\nthe natural groupings as dictated by the data leads to important questions\nabout unlabelled data that can help the evaluation and realistic expectations\nof future (including self-supervised) classification systems.", "published": "2021-10-09 14:46:18", "link": "http://arxiv.org/abs/2110.04584v1", "categories": ["eess.AS", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Universal Paralinguistic Speech Representations Using Self-Supervised\n  Conformers", "abstract": "Many speech applications require understanding aspects beyond the words being\nspoken, such as recognizing emotion, detecting whether the speaker is wearing a\nmask, or distinguishing real from synthetic speech. In this work, we introduce\na new state-of-the-art paralinguistic representation derived from large-scale,\nfully self-supervised training of a 600M+ parameter Conformer-based\narchitecture. We benchmark on a diverse set of speech tasks and demonstrate\nthat simple linear classifiers trained on top of our time-averaged\nrepresentation outperform nearly all previous results, in some cases by large\nmargins. Our analyses of context-window size demonstrate that, surprisingly, 2\nsecond context-windows achieve 96\\% the performance of the Conformers that use\nthe full long-term context on 7 out of 9 tasks. Furthermore, while the best\nper-task representations are extracted internally in the network, stable\nperformance across several layers allows a single universal representation to\nreach near optimal performance on all tasks.", "published": "2021-10-09 18:07:03", "link": "http://arxiv.org/abs/2110.04621v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Complex Network-Based Approach for Feature Extraction and Classification\n  of Musical Genres", "abstract": "Musical genre's classification has been a relevant research topic. The\nassociation between music and genres is fundamental for the media industry,\nwhich manages musical recommendation systems, and for music streaming services,\nwhich may appear classified by genres. In this context, this work presents a\nfeature extraction method for the automatic classification of musical genres,\nbased on complex networks and their topological measurements. The proposed\nmethod initially converts the musics into sequences of musical notes and then\nmaps the sequences as complex networks. Topological measurements are extracted\nto characterize the network topology, which composes a feature vector that\napplies to the classification of musical genres. The method was evaluated in\nthe classification of 10 musical genres by adopting the GTZAN dataset and 8\nmusical genres by adopting the FMA dataset. The results were compared with\nmethods in the literature. The proposed method outperformed all compared\nmethods by presenting high accuracy and low standard deviation, showing its\nsuitability for the musical genre's classification, which contributes to the\nmedia industry in the automatic classification with assertiveness and\nrobustness. The proposed method is implemented in an open source in the Python\nlanguage and freely available at https://github.com/omatheuspimenta/examinner.", "published": "2021-10-09 22:23:33", "link": "http://arxiv.org/abs/2110.04654v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Streaming on-device detection of device directed speech from voice and\n  touch-based invocation", "abstract": "When interacting with smart devices such as mobile phones or wearables, the\nuser typically invokes a virtual assistant (VA) by saying a keyword or by\npressing a button on the device. However, in many cases, the VA can\naccidentally be invoked by the keyword-like speech or accidental button press,\nwhich may have implications on user experience and privacy. To this end, we\npropose an acoustic false-trigger-mitigation (FTM) approach for on-device\ndevice-directed speech detection that simultaneously handles the voice-trigger\nand touch-based invocation. To facilitate the model deployment on-device, we\nintroduce a new streaming decision layer, derived using the notion of temporal\nconvolutional networks (TCN) [1], known for their computational efficiency. To\nthe best of our knowledge, this is the first approach that can detect\ndevice-directed speech from more than one invocation type in a streaming\nfashion. We compare this approach with streaming alternatives based on vanilla\nAverage layer, and canonical LSTMs, and show: (i) that all the models show only\na small degradation in accuracy compared with the invocation-specific models,\nand (ii) that the newly introduced streaming TCN consistently performs better\nor comparable with the alternatives, while mitigating device undirected speech\nfaster in time, and with (relative) reduction in runtime peak-memory over the\nLSTM-based approach of 33% vs. 7%, when compared to a non-streaming\ncounterpart.", "published": "2021-10-09 22:33:42", "link": "http://arxiv.org/abs/2110.04656v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
