{"title": "Reducing LLM Hallucinations using Epistemic Neural Networks", "abstract": "Reducing and detecting hallucinations in large language models is an open\nresearch problem. In this project, we attempt to leverage recent advances in\nthe field of uncertainty estimation to reduce hallucinations in frozen large\nlanguage models. Epistemic neural networks have recently been proposed to\nimprove output joint distributions for large pre-trained models. ENNs are small\nnetworks attached to large, frozen models to improve the model's joint\ndistributions and uncertainty estimates. In this work, we train an epistemic\nneural network on top of the Llama-2 7B model combined with a contrastive\ndecoding feature enhancement technique. We are the first to train an ENN for\nthe next token prediction task and explore the efficacy of this method in\nreducing hallucinations on the TruthfulQA dataset. In essence, we provide a\nmethod that leverages a pre-trained model's latent embeddings to reduce\nhallucinations.", "published": "2023-12-25 01:17:01", "link": "http://arxiv.org/abs/2312.15576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Split-and-Privatize Framework for Large Language Model Fine-Tuning", "abstract": "Fine-tuning is a prominent technique to adapt a pre-trained language model to\ndownstream scenarios. In parameter-efficient fine-tuning, only a small subset\nof modules are trained over the downstream datasets, while leaving the rest of\nthe pre-trained model frozen to save computation resources. In recent years, a\npopular productization form arises as Model-as-a-Service (MaaS), in which\nvendors provide abundant pre-trained language models, server resources and core\nfunctions, and customers can fine-tune, deploy and invoke their customized\nmodel by accessing the one-stop MaaS with their own private dataset. In this\npaper, we identify the model and data privacy leakage risks in MaaS\nfine-tuning, and propose a Split-and-Privatize (SAP) framework, which manage to\nmitigate the privacy issues by adapting the existing split learning\narchitecture. The proposed SAP framework is sufficiently investigated by\nexperiments, and the results indicate that it can enhance the empirical privacy\nby 62% at the cost of 1% model performance degradation on the Stanford\nSentiment Treebank dataset.", "published": "2023-12-25 03:53:33", "link": "http://arxiv.org/abs/2312.15603v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conditional Variational Autoencoder for Sign Language Translation with\n  Cross-Modal Alignment", "abstract": "Sign language translation (SLT) aims to convert continuous sign language\nvideos into textual sentences. As a typical multi-modal task, there exists an\ninherent modality gap between sign language videos and spoken language text,\nwhich makes the cross-modal alignment between visual and textual modalities\ncrucial. However, previous studies tend to rely on an intermediate sign gloss\nrepresentation to help alleviate the cross-modal problem thereby neglecting the\nalignment across modalities that may lead to compromised results. To address\nthis issue, we propose a novel framework based on Conditional Variational\nautoencoder for SLT (CV-SLT) that facilitates direct and sufficient cross-modal\nalignment between sign language videos and spoken language text. Specifically,\nour CV-SLT consists of two paths with two Kullback-Leibler (KL) divergences to\nregularize the outputs of the encoder and decoder, respectively. In the prior\npath, the model solely relies on visual information to predict the target text;\nwhereas in the posterior path, it simultaneously encodes visual information and\ntextual knowledge to reconstruct the target text. The first KL divergence\noptimizes the conditional variational autoencoder and regularizes the encoder\noutputs, while the second KL divergence performs a self-distillation from the\nposterior path to the prior path, ensuring the consistency of decoder outputs.\nWe further enhance the integration of textual information to the posterior path\nby employing a shared Attention Residual Gaussian Distribution (ARGD), which\nconsiders the textual information in the posterior path as a residual component\nrelative to the prior path. Extensive experiments conducted on public datasets\n(PHOENIX14T and CSL-daily) demonstrate the effectiveness of our framework,\nachieving new state-of-the-art results while significantly alleviating the\ncross-modal representation discrepancy.", "published": "2023-12-25 08:20:40", "link": "http://arxiv.org/abs/2312.15645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models\n  with Semi-structured Data", "abstract": "Large Language Models (LLMs) pre-trained on massive corpora have exhibited\nremarkable performance on various NLP tasks. However, applying these models to\nspecific domains still poses significant challenges, such as lack of domain\nknowledge, limited capacity to leverage domain knowledge and inadequate\nadaptation to domain-specific data formats. Considering the exorbitant cost of\ntraining LLMs from scratch and the scarcity of annotated data within particular\ndomains, in this work, we focus on domain-specific continual pre-training of\nLLMs using E-commerce domain as an exemplar. Specifically, we explore the\nimpact of continual pre-training on LLMs employing unlabeled general and\nE-commercial corpora. Furthermore, we design a mixing strategy among different\ndata sources to better leverage E-commercial semi-structured data. We construct\nmultiple tasks to assess LLMs' few-shot In-context Learning ability and their\nzero-shot performance after instruction tuning in E-commerce domain.\nExperimental results demonstrate the effectiveness of continual pre-training of\nE-commerce LLMs and the efficacy of our devised data mixing strategy.", "published": "2023-12-25 11:31:47", "link": "http://arxiv.org/abs/2312.15696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving Label Variation in Scientific Information Extraction via\n  Multi-Task Learning", "abstract": "Scientific Information Extraction (ScientificIE) is a critical task that\ninvolves the identification of scientific entities and their relationships. The\ncomplexity of this task is compounded by the necessity for domain-specific\nknowledge and the limited availability of annotated data. Two of the most\npopular datasets for ScientificIE are SemEval-2018 Task-7 and SciERC. They have\noverlapping samples and differ in their annotation schemes, which leads to\nconflicts. In this study, we first introduced a novel approach based on\nmulti-task learning to address label variations. We then proposed a soft\nlabeling technique that converts inconsistent labels into probabilistic\ndistributions. The experimental results demonstrated that the proposed method\ncan enhance the model robustness to label noise and improve the end-to-end\nperformance in both ScientificIE tasks. The analysis revealed that label\nvariations can be particularly effective in handling ambiguous instances.\nFurthermore, the richness of the information captured by label variations can\npotentially reduce data size requirements. The findings highlight the\nimportance of releasing variation labels and promote future research on other\ntasks in other domains. Overall, this study demonstrates the effectiveness of\nmulti-task learning and the potential of label variations to enhance the\nperformance of ScientificIE.", "published": "2023-12-25 15:24:41", "link": "http://arxiv.org/abs/2312.15751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Design and Implementation of a Tool for Extracting Uzbek Syllables", "abstract": "The accurate syllabification of words plays a vital role in various Natural\nLanguage Processing applications. Syllabification is a versatile linguistic\ntool with applications in linguistic research, language technology, education,\nand various fields where understanding and processing language is essential. In\nthis paper, we present a comprehensive approach to syllabification for the\nUzbek language, including rule-based techniques and machine learning\nalgorithms. Our rule-based approach utilizes advanced methods for dividing\nwords into syllables, generating hyphenations for line breaks and count of\nsyllables. Additionally, we collected a dataset for evaluating and training\nusing machine learning algorithms comprising word-syllable mappings,\nhyphenations, and syllable counts to predict syllable counts as well as for the\nevaluation of the proposed model. Our results demonstrate the effectiveness and\nefficiency of both approaches in achieving accurate syllabification. The\nresults of our experiments show that both approaches achieved a high level of\naccuracy, exceeding 99%. This study provides valuable insights and\nrecommendations for future research on syllabification and related areas in not\nonly the Uzbek language itself, but also in other closely-related Turkic\nlanguages with low-resource factor.", "published": "2023-12-25 17:46:58", "link": "http://arxiv.org/abs/2312.15779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization in Spoken Language Understanding", "abstract": "State-of-the-art spoken language understanding (SLU) models have shown\ntremendous success in benchmark SLU datasets, yet they still fail in many\npractical scenario due to the lack of model compositionality when trained on\nlimited training data. In this paper, we study two types of compositionality:\n(a) novel slot combination, and (b) length generalization. We first conduct\nin-depth analysis, and find that state-of-the-art SLU models often learn\nspurious slot correlations during training, which leads to poor performance in\nboth compositional cases. To mitigate these limitations, we create the first\ncompositional splits of benchmark SLU datasets and we propose the first\ncompositional SLU model, including compositional loss and paired training that\ntackle each compositional case respectively. On both benchmark and\ncompositional splits in ATIS and SNIPS, we show that our compositional SLU\nmodel significantly outperforms (up to $5\\%$ F1 score) state-of-the-art BERT\nSLU model.", "published": "2023-12-25 21:46:06", "link": "http://arxiv.org/abs/2312.15815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning", "abstract": "Conventional embedding-based models approach event time prediction in\ntemporal knowledge graphs (TKGs) as a ranking problem. However, they often fall\nshort in capturing essential temporal relationships such as order and distance.\nIn this paper, we propose TEILP, a logical reasoning framework that naturally\nintegrates such temporal elements into knowledge graph predictions. We first\nconvert TKGs into a temporal event knowledge graph (TEKG) which has a more\nexplicit representation of time in term of nodes of the graph. The TEKG equips\nus to develop a differentiable random walk approach to time prediction.\nFinally, we introduce conditional probability density functions, associated\nwith the logical rules involving the query interval, using which we arrive at\nthe time prediction. We compare TEILP with state-of-the-art methods on five\nbenchmark datasets. We show that our model achieves a significant improvement\nover baselines while providing interpretable explanations. In particular, we\nconsider several scenarios where training samples are limited, event types are\nimbalanced, and forecasting the time of future events based on only past events\nis desired. In all these cases, TEILP outperforms state-of-the-art methods in\nterms of robustness.", "published": "2023-12-25 21:54:56", "link": "http://arxiv.org/abs/2312.15816v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chatbot is Not All You Need: Information-rich Prompting for More\n  Realistic Responses", "abstract": "Recent Large Language Models (LLMs) have shown remarkable capabilities in\nmimicking fictional characters or real humans in conversational settings.\nHowever, the realism and consistency of these responses can be further enhanced\nby providing richer information of the agent being mimicked. In this paper, we\npropose a novel approach to generate more realistic and consistent responses\nfrom LLMs, leveraging five senses, attributes, emotional states, relationship\nwith the interlocutor, and memories. By incorporating these factors, we aim to\nincrease the LLM's capacity for generating natural and realistic reactions in\nconversational exchanges. Through our research, we expect to contribute to the\ndevelopment of LLMs that demonstrate improved capabilities in mimicking\nfictional characters. We release a new benchmark dataset and all our codes,\nprompts, and sample results on our Github:\nhttps://github.com/srafsasm/InfoRichBot", "published": "2023-12-25 02:18:58", "link": "http://arxiv.org/abs/2312.16233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TACIT: A Target-Agnostic Feature Disentanglement Framework for\n  Cross-Domain Text Classification", "abstract": "Cross-domain text classification aims to transfer models from label-rich\nsource domains to label-poor target domains, giving it a wide range of\npractical applications. Many approaches promote cross-domain generalization by\ncapturing domain-invariant features. However, these methods rely on unlabeled\nsamples provided by the target domains, which renders the model ineffective\nwhen the target domain is agnostic. Furthermore, the models are easily\ndisturbed by shortcut learning in the source domain, which also hinders the\nimprovement of domain generalization ability. To solve the aforementioned\nissues, this paper proposes TACIT, a target domain agnostic feature\ndisentanglement framework which adaptively decouples robust and unrobust\nfeatures by Variational Auto-Encoders. Additionally, to encourage the\nseparation of unrobust features from robust features, we design a feature\ndistillation task that compels unrobust features to approximate the output of\nthe teacher. The teacher model is trained with a few easy samples that are easy\nto carry potential unknown shortcuts. Experimental results verify that our\nframework achieves comparable results to state-of-the-art baselines while\nutilizing only source domain data.", "published": "2023-12-25 02:52:36", "link": "http://arxiv.org/abs/2312.17263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Abductive Reasoning in Knowledge Graphs through Complex\n  Logical Hypothesis Generation", "abstract": "Abductive reasoning is the process of making educated guesses to provide\nexplanations for observations. Although many applications require the use of\nknowledge for explanations, the utilization of abductive reasoning in\nconjunction with structured knowledge, such as a knowledge graph, remains\nlargely unexplored. To fill this gap, this paper introduces the task of complex\nlogical hypothesis generation, as an initial step towards abductive logical\nreasoning with KG. In this task, we aim to generate a complex logical\nhypothesis so that it can explain a set of observations. We find that the\nsupervised trained generative model can generate logical hypotheses that are\nstructurally closer to the reference hypothesis. However, when generalized to\nunseen observations, this training objective does not guarantee better\nhypothesis generation. To address this, we introduce the Reinforcement Learning\nfrom Knowledge Graph (RLF-KG) method, which minimizes differences between\nobservations and conclusions drawn from generated hypotheses according to the\nKG. Experiments show that, with RLF-KG's assistance, the generated hypotheses\nprovide better explanations, and achieve state-of-the-art results on three\nwidely used KGs.", "published": "2023-12-25 08:06:20", "link": "http://arxiv.org/abs/2312.15643v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Alleviating Hallucinations of Large Language Models through Induced\n  Hallucinations", "abstract": "Despite their impressive capabilities, large language models (LLMs) have been\nobserved to generate responses that include inaccurate or fabricated\ninformation, a phenomenon commonly known as ``hallucination''. In this work, we\npropose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to\nalleviate hallucinations. We first construct a factually weak LLM by inducing\nhallucinations from the original LLMs. Then, we penalize these induced\nhallucinations during decoding to enhance the factuality of the generated\ncontent. Concretely, we determine the final next-token predictions by\namplifying the predictions from the original model and downplaying the induced\nuntruthful predictions via contrastive decoding. Experimental results on both\ndiscrimination-based and generation-based hallucination evaluation benchmarks,\nsuch as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD\nmethods can effectively enhance the factuality of LLMs across various model\nsizes and families. For example, when equipped with ICD, Llama2-7B-Chat and\nMistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on\nTruthfulQA, respectively.", "published": "2023-12-25 12:32:49", "link": "http://arxiv.org/abs/2312.15710v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PersianLLaMA: Towards Building First Persian Large Language Model", "abstract": "Despite the widespread use of the Persian language by millions globally,\nlimited efforts have been made in natural language processing for this\nlanguage. The use of large language models as effective tools in various\nnatural language processing tasks typically requires extensive textual data and\nrobust hardware resources. Consequently, the scarcity of Persian textual data\nand the unavailability of powerful hardware resources have hindered the\ndevelopment of large language models for Persian. This paper introduces the\nfirst large Persian language model, named PersianLLaMA, trained on a collection\nof Persian texts and datasets. This foundational model comes in two versions,\nwith 7 and 13 billion parameters, trained on formal and colloquial Persian\ntexts using two different approaches. PersianLLaMA has been evaluated for\nnatural language generation tasks based on the latest evaluation methods,\nnamely using larger language models, and for natural language understanding\ntasks based on automated machine metrics. The results indicate that\nPersianLLaMA significantly outperforms its competitors in both understanding\nand generating Persian text. PersianLLaMA marks an important step in the\ndevelopment of Persian natural language processing and can be a valuable\nresource for the Persian-speaking community. This large language model can be\nused for various natural language processing tasks, especially text generation\nlike chatbots, question-answering, machine translation, and text summarization", "published": "2023-12-25 12:48:55", "link": "http://arxiv.org/abs/2312.15713v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AHAM: Adapt, Help, Ask, Model -- Harvesting LLMs for literature mining", "abstract": "In an era marked by a rapid increase in scientific publications, researchers\ngrapple with the challenge of keeping pace with field-specific advances. We\npresent the `AHAM' methodology and a metric that guides the domain-specific\n\\textbf{adapt}ation of the BERTopic topic modeling framework to improve\nscientific text analysis. By utilizing the LLaMa2 generative language model, we\ngenerate topic definitions via one-shot learning by crafting prompts with the\n\\textbf{help} of domain experts to guide the LLM for literature mining by\n\\textbf{asking} it to model the topic names. For inter-topic similarity\nevaluation, we leverage metrics from language generation and translation\nprocesses to assess lexical and semantic similarity of the generated topics.\nOur system aims to reduce both the ratio of outlier topics to the total number\nof topics and the similarity between topic definitions. The methodology has\nbeen assessed on a newly gathered corpus of scientific papers on\nliterature-based discovery. Through rigorous evaluation by domain experts, AHAM\nhas been validated as effective in uncovering intriguing and novel insights\nwithin broad research areas. We explore the impact of domain adaptation of\nsentence-transformers for the task of topic \\textbf{model}ing using two\ndatasets, each specialized to specific scientific domains within arXiv and\nmedarxiv. We evaluate the impact of data size, the niche of adaptation, and the\nimportance of domain adaptation. Our results suggest a strong interaction\nbetween domain adaptation and topic modeling precision in terms of outliers and\ntopic definitions.", "published": "2023-12-25 18:23:03", "link": "http://arxiv.org/abs/2312.15784v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ESGReveal: An LLM-based approach for extracting structured data from ESG\n  reports", "abstract": "ESGReveal is an innovative method proposed for efficiently extracting and\nanalyzing Environmental, Social, and Governance (ESG) data from corporate\nreports, catering to the critical need for reliable ESG information retrieval.\nThis approach utilizes Large Language Models (LLM) enhanced with Retrieval\nAugmented Generation (RAG) techniques. The ESGReveal system includes an ESG\nmetadata module for targeted queries, a preprocessing module for assembling\ndatabases, and an LLM agent for data extraction. Its efficacy was appraised\nusing ESG reports from 166 companies across various sectors listed on the Hong\nKong Stock Exchange in 2022, ensuring comprehensive industry and market\ncapitalization representation. Utilizing ESGReveal unearthed significant\ninsights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in\ndata extraction and 83.7% in disclosure analysis, which is an improvement over\nbaseline models. This highlights the framework's capacity to refine ESG data\nanalysis precision. Moreover, it revealed a demand for reinforced ESG\ndisclosures, with environmental and social data disclosures standing at 69.5%\nand 57.2%, respectively, suggesting a pursuit for more corporate transparency.\nWhile current iterations of ESGReveal do not process pictorial information, a\nfunctionality intended for future enhancement, the study calls for continued\nresearch to further develop and compare the analytical capabilities of various\nLLMs. In summary, ESGReveal is a stride forward in ESG data processing,\noffering stakeholders a sophisticated tool to better evaluate and advance\ncorporate sustainability efforts. Its evolution is promising in promoting\ntransparency in corporate reporting and aligning with broader sustainable\ndevelopment aims.", "published": "2023-12-25 06:44:32", "link": "http://arxiv.org/abs/2312.17264v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on\n  Software Engineering Tasks", "abstract": "Pre-trained models (PTMs) have achieved great success in various Software\nEngineering (SE) downstream tasks following the ``pre-train then fine-tune''\nparadigm. As fully fine-tuning all parameters of PTMs can be computationally\nexpensive, a widely used solution is parameter-efficient fine-tuning (PEFT),\nwhich freezes PTMs while introducing extra parameters. Though work has been\ndone to test PEFT methods in the SE field, a comprehensive evaluation is still\nlacking. This paper aims to fill in this gap by evaluating the effectiveness of\nfive PEFT methods on eight PTMs and four SE downstream tasks. For different\ntasks and PEFT methods, we seek answers to the following research questions: 1)\nIs it more effective to use PTMs trained specifically on source code, or is it\nsufficient to use PTMs trained on natural language text? 2) What is the impact\nof varying model sizes? 3) How does the model architecture affect the\nperformance? Besides effectiveness, we also discuss the efficiency of PEFT\nmethods, concerning the costs of required training time and GPU resource\nconsumption. We hope that our findings can provide a deeper understanding of\nPEFT methods on various PTMs and SE downstream tasks. All the codes and data\nare available at \\url{https://github.com/zwtnju/PEFT.git}.", "published": "2023-12-25 05:25:39", "link": "http://arxiv.org/abs/2312.15614v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "RDF-star2Vec: RDF-star Graph Embeddings for Data Mining", "abstract": "Knowledge Graphs (KGs) such as Resource Description Framework (RDF) data\nrepresent relationships between various entities through the structure of\ntriples (<subject, predicate, object>). Knowledge graph embedding (KGE) is\ncrucial in machine learning applications, specifically in node classification\nand link prediction tasks. KGE remains a vital research topic within the\nsemantic web community. RDF-star introduces the concept of a quoted triple\n(QT), a specific form of triple employed either as the subject or object within\nanother triple. Moreover, RDF-star permits a QT to act as compositional\nentities within another QT, thereby enabling the representation of recursive,\nhyper-relational KGs with nested structures. However, existing KGE models fail\nto adequately learn the semantics of QTs and entities, primarily because they\ndo not account for RDF-star graphs containing multi-leveled nested QTs and\nQT-QT relationships. This study introduces RDF-star2Vec, a novel KGE model\nspecifically designed for RDF-star graphs. RDF-star2Vec introduces graph walk\ntechniques that enable probabilistic transitions between a QT and its\ncompositional entities. Feature vectors for QTs, entities, and relations are\nderived from generated sequences through the structured skip-gram model.\nAdditionally, we provide a dataset and a benchmarking framework for data mining\ntasks focused on complex RDF-star graphs. Evaluative experiments demonstrated\nthat RDF-star2Vec yielded superior performance compared to recent extensions of\nRDF2Vec in various tasks including classification, clustering, entity\nrelatedness, and QT similarity.", "published": "2023-12-25 06:32:14", "link": "http://arxiv.org/abs/2312.15626v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.2.4; I.2.6"], "primary_category": "cs.AI"}
{"title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic\n  Data Selection in Instruction Tuning", "abstract": "Instruction tuning is a standard technique employed to align large language\nmodels to end tasks and user preferences after the initial pretraining phase.\nRecent research indicates the critical role of data engineering in instruction\ntuning -- when appropriately selected, only limited data is necessary to\nachieve superior performance. However, we still lack a principled understanding\nof what makes good instruction tuning data for alignment, and how we should\nselect data automatically and effectively. In this work, we delve deeply into\nautomatic data selection strategies for alignment. We start with controlled\nstudies to measure data across three dimensions: complexity, quality, and\ndiversity, along which we examine existing methods and introduce novel\ntechniques for enhanced data measurement. Subsequently, we propose a simple\nstrategy to select data samples based on the measurement. We present deita\n(short for Data-Efficient Instruction Tuning for Alignment), a series of models\nfine-tuned from LLaMA and Mistral models using data samples automatically\nselected with our proposed approach. Empirically, deita performs better or on\npar with the state-of-the-art open-source alignment models with only 6K SFT\ntraining data samples -- over 10x less than the data used in the baselines.\nWhen further trained with direct preference optimization (DPO),\ndeita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55\nMT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools\non automatic data selection, facilitating data-efficient alignment. We release\nour models as well as the selected datasets for future researches to\neffectively align models more efficiently.", "published": "2023-12-25 10:29:28", "link": "http://arxiv.org/abs/2312.15685v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Balanced SNR-Aware Distillation for Guided Text-to-Audio Generation", "abstract": "Diffusion models have demonstrated promising results in text-to-audio\ngeneration tasks. However, their practical usability is hindered by slow\nsampling speeds, limiting their applicability in high-throughput scenarios. To\naddress this challenge, progressive distillation methods have been effective in\nproducing more compact and efficient models. Nevertheless, these methods\nencounter issues with unbalanced weights at both high and low noise levels,\npotentially impacting the quality of generated samples. In this paper, we\npropose the adaptation of the progressive distillation method to text-to-audio\ngeneration tasks and introduce the Balanced SNR-Aware~(BSA) method, an enhanced\nloss-weighting mechanism for diffusion distillation. The BSA method employs a\nbalanced approach to weight the loss for both high and low noise levels. We\nevaluate our proposed method on the AudioCaps dataset and report experimental\nresults showing superior performance during the reverse diffusion process\ncompared to previous distillation methods with the same number of sampling\nsteps. Furthermore, the BSA method allows for a significant reduction in\nsampling steps from 200 to 25, with minimal performance degradation when\ncompared to the original teacher models.", "published": "2023-12-25 06:46:39", "link": "http://arxiv.org/abs/2312.15628v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DSNet: Disentangled Siamese Network with Neutral Calibration for Speech\n  Emotion Recognition", "abstract": "One persistent challenge in deep learning based speech emotion recognition\n(SER) is the unconscious encoding of emotion-irrelevant factors (e.g., speaker\nor phonetic variability), which limits the generalization of SER in practical\nuse. In this paper, we propose DSNet, a Disentangled Siamese Network with\nneutral calibration, to meet the demand for a more robust and explainable SER\nmodel. Specifically, we introduce an orthogonal feature disentanglement module\nto explicitly project the high-level representation into two distinct\nsubspaces. Later, we propose a novel neutral calibration mechanism to encourage\none subspace to capture sufficient emotion-irrelevant information. In this way,\nthe other one can better isolate and emphasize the emotion-relevant information\nwithin speech signals. Experimental results on two popular benchmark datasets\ndemonstrate the superiority of DSNet over various state-of-the-art methods for\nspeaker-independent SER.", "published": "2023-12-25 02:58:37", "link": "http://arxiv.org/abs/2312.15593v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Uncertainty as a Predictor: Leveraging Self-Supervised Learning for\n  Zero-Shot MOS Prediction", "abstract": "Predicting audio quality in voice synthesis and conversion systems is a\ncritical yet challenging task, especially when traditional methods like Mean\nOpinion Scores (MOS) are cumbersome to collect at scale. This paper addresses\nthe gap in efficient audio quality prediction, especially in low-resource\nsettings where extensive MOS data from large-scale listening tests may be\nunavailable. We demonstrate that uncertainty measures derived from\nout-of-the-box pretrained self-supervised learning (SSL) models, such as\nwav2vec, correlate with MOS scores. These findings are based on data from the\n2022 and 2023 VoiceMOS challenges. We explore the extent of this correlation\nacross different models and language contexts, revealing insights into how\ninherent uncertainties in SSL models can serve as effective proxies for audio\nquality assessment. In particular, we show that the contrastive wav2vec models\nare the most performant in all settings.", "published": "2023-12-25 05:35:28", "link": "http://arxiv.org/abs/2312.15616v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Audiobox: Unified Audio Generation with Natural Language Prompts", "abstract": "Audio is an essential part of our life, but creating it often requires\nexpertise and is time-consuming. Research communities have made great progress\nover the past year advancing the performance of large scale audio generative\nmodels for a single modality (speech, sound, or music) through adopting more\npowerful generative models and scaling data. However, these models lack\ncontrollability in several aspects: speech generation models cannot synthesize\nnovel styles based on text description and are limited on domain coverage such\nas outdoor environments; sound generation models only provide coarse-grained\ncontrol based on descriptions like \"a person speaking\" and would only generate\nmumbling human voices. This paper presents Audiobox, a unified model based on\nflow-matching that is capable of generating various audio modalities. We design\ndescription-based and example-based prompting to enhance controllability and\nunify speech and sound generation paradigms. We allow transcript, vocal, and\nother audio styles to be controlled independently when generating speech. To\nimprove model generalization with limited labels, we adapt a self-supervised\ninfilling objective to pre-train on large quantities of unlabeled audio.\nAudiobox sets new benchmarks on speech and sound generation (0.745 similarity\non Librispeech for zero-shot TTS; 0.77 FAD on AudioCaps for text-to-sound) and\nunlocks new methods for generating audio with novel vocal and acoustic styles.\nWe further integrate Bespoke Solvers, which speeds up generation by over 25\ntimes compared to the default ODE solver for flow-matching, without loss of\nperformance on several tasks. Our demo is available at\nhttps://audiobox.metademolab.com/", "published": "2023-12-25 22:24:49", "link": "http://arxiv.org/abs/2312.15821v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Learning for Few-Shot Bird Sound Classification", "abstract": "Self-supervised learning (SSL) in audio holds significant potential across\nvarious domains, particularly in situations where abundant, unlabeled data is\nreadily available at no cost. This is pertinent in bioacoustics, where\nbiologists routinely collect extensive sound datasets from the natural\nenvironment. In this study, we demonstrate that SSL is capable of acquiring\nmeaningful representations of bird sounds from audio recordings without the\nneed for annotations. Our experiments showcase that these learned\nrepresentations exhibit the capacity to generalize to new bird species in\nfew-shot learning (FSL) scenarios. Additionally, we show that selecting windows\nwith high bird activation for self-supervised learning, using a pretrained\naudio neural network, significantly enhances the quality of the learned\nrepresentations.", "published": "2023-12-25 22:33:45", "link": "http://arxiv.org/abs/2312.15824v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
