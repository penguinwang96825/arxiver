{"title": "Lawyers are Dishonest? Quantifying Representational Harms in Commonsense\n  Knowledge Resources", "abstract": "Warning: this paper contains content that may be offensive or upsetting.\n  Numerous natural language processing models have tried injecting commonsense\nby using the ConceptNet knowledge base to improve performance on different\ntasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect\nhuman biases such as \"lawyers are dishonest.\" It is important that these biases\nare not conflated with the notion of commonsense. We study this missing yet\nimportant problem by first defining and quantifying biases in ConceptNet as two\ntypes of representational harms: overgeneralization of polarized perceptions\nand representation disparity. We find that ConceptNet contains severe biases\nand disparities across four demographic categories. In addition, we analyze two\ndownstream models that use ConceptNet as a source for commonsense knowledge and\nfind the existence of biases in those models as well. We further propose a\nfiltered-based bias-mitigation approach and examine its effectiveness. We show\nthat our mitigation approach can reduce the issues in both resource and models\nbut leads to a performance drop, leaving room for future work to build fairer\nand stronger commonsense models.", "published": "2021-03-21 06:59:08", "link": "http://arxiv.org/abs/2103.11320v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive\n  Summarization", "abstract": "State-of-the-art abstractive summarization models generally rely on extensive\nlabeled data, which lowers their generalization ability on domains where such\ndata are not available. In this paper, we present a study of domain adaptation\nfor the abstractive summarization task across six diverse target domains in a\nlow-resource setting. Specifically, we investigate the second phase of\npre-training on large-scale generative models under three different settings:\n1) source domain pre-training; 2) domain-adaptive pre-training; and 3)\ntask-adaptive pre-training. Experiments show that the effectiveness of\npre-training is correlated with the similarity between the pre-training data\nand the target domain task. Moreover, we find that continuing pre-training\ncould lead to the pre-trained model's catastrophic forgetting, and a learning\nmethod with less forgetting can alleviate this issue. Furthermore, results\nillustrate that a huge gap still exists between the low-resource and\nhigh-resource settings, which highlights the need for more advanced domain\nadaptation methods for the abstractive summarization task.", "published": "2021-03-21 08:12:19", "link": "http://arxiv.org/abs/2103.11332v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NameRec*: Highly Accurate and Fine-grained Person Name Recognition", "abstract": "In this paper, we introduce the NameRec* task, which aims to do highly\naccurate and fine-grained person name recognition. Traditional Named Entity\nRecognition models have good performance in recognising well-formed person\nnames from text with consistent and complete syntax, such as news articles.\nHowever, there are rapidly growing scenarios where sentences are of incomplete\nsyntax and names are in various forms such as user-generated contents and\nacademic homepages. To address person name recognition in this context, we\npropose a fine-grained annotation scheme based on anthroponymy. To take full\nadvantage of the fine-grained annotations, we propose a Co-guided Neural\nNetwork (CogNN) for person name recognition. CogNN fully explores the\nintra-sentence context and rich training signals of name forms. To better\nutilize the inter-sentence context and implicit relations, which are extremely\nessential for recognizing person names in long documents, we further propose an\nInter-sentence BERT Model (IsBERT). IsBERT has an overlapped input processor,\nand an inter-sentence encoder with bidirectional overlapped contextual\nembedding learning and multi-hop inference mechanisms. To derive benefit from\ndifferent documents with a diverse abundance of context, we propose an advanced\nAdaptive Inter-sentence BERT Model (Ada-IsBERT) to dynamically adjust the\ninter-sentence overlapping ratio to different documents. We conduct extensive\nexperiments to demonstrate the superiority of the proposed methods on both\nacademic homepages and news articles.", "published": "2021-03-21 10:35:04", "link": "http://arxiv.org/abs/2103.11360v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques", "abstract": "Pre-trained language models of the BERT family have defined the\nstate-of-the-arts in a wide range of NLP tasks. However, the performance of\nBERT-based models is mainly driven by the enormous amount of parameters, which\nhinders their application to resource-limited scenarios. Faced with this\nproblem, recent studies have been attempting to compress BERT into a\nsmall-scale model. However, most previous work primarily focuses on a single\nkind of compression technique, and few attention has been paid to the\ncombination of different methods. When BERT is compressed with integrated\ntechniques, a critical question is how to design the entire compression\nframework to obtain the optimal performance. In response to this question, we\nintegrate three kinds of compression methods (weight pruning, low-rank\nfactorization and knowledge distillation (KD)) and explore a range of designs\nconcerning model architecture, KD strategy, pruning frequency and learning rate\nschedule. We find that a careful choice of the designs is crucial to the\nperformance of the compressed model. Based on the empirical findings, our best\ncompressed model, dubbed Refined BERT cOmpreSsion with InTegrAted techniques\n(ROSITA), is $7.5 \\times$ smaller than BERT while maintains $98.5\\%$ of the\nperformance on five tasks of the GLUE benchmark, outperforming the previous\nBERT compression methods with similar parameter budget. The code is available\nat https://github.com/llyx97/Rosita.", "published": "2021-03-21 11:33:33", "link": "http://arxiv.org/abs/2103.11367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SwissDial: Parallel Multidialectal Corpus of Spoken Swiss German", "abstract": "Swiss German is a dialect continuum whose natively acquired dialects\nsignificantly differ from the formal variety of the language. These dialects\nare mostly used for verbal communication and do not have standard orthography.\nThis has led to a lack of annotated datasets, rendering the use of many NLP\nmethods infeasible. In this paper, we introduce the first annotated parallel\ncorpus of spoken Swiss German across 8 major dialects, plus a Standard German\nreference. Our goal has been to create and to make available a basic dataset\nfor employing data-driven NLP applications in Swiss German. We present our data\ncollection procedure in detail and validate the quality of our corpus by\nconducting experiments with the recent neural models for speech synthesis.", "published": "2021-03-21 14:00:09", "link": "http://arxiv.org/abs/2103.11401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Method Names to Improve Code Summarization: A Deliberation\n  Multi-Task Learning Approach", "abstract": "Code summaries are brief natural language descriptions of source code pieces.\nThe main purpose of code summarization is to assist developers in understanding\ncode and to reduce documentation workload. In this paper, we design a novel\nmulti-task learning (MTL) approach for code summarization through mining the\nrelationship between method code summaries and method names. More specifically,\nsince a method's name can be considered as a shorter version of its code\nsummary, we first introduce the tasks of generation and informativeness\nprediction of method names as two auxiliary training objectives for code\nsummarization. A novel two-pass deliberation mechanism is then incorporated\ninto our MTL architecture to generate more consistent intermediate states fed\ninto a summary decoder, especially when informative method names do not exist.\nTo evaluate our deliberation MTL approach, we carried out a large-scale\nexperiment on two existing datasets for Java and Python. The experiment results\nshow that our technique can be easily applied to many state-of-the-art neural\nmodels for code summarization and improve their performance. Meanwhile, our\napproach shows significant superiority when generating summaries for methods\nwith non-informative names.", "published": "2021-03-21 17:52:21", "link": "http://arxiv.org/abs/2103.11448v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural block driven - enhanced convolutional neural representation\n  for relation extraction", "abstract": "In this paper, we propose a novel lightweight relation extraction approach of\nstructural block driven - convolutional neural learning. Specifically, we\ndetect the essential sequential tokens associated with entities through\ndependency analysis, named as a structural block, and only encode the block on\na block-wise and an inter-block-wise representation, utilizing multi-scale\nCNNs. This is to 1) eliminate the noisy from irrelevant part of a sentence;\nmeanwhile 2) enhance the relevant block representation with both block-wise and\ninter-block-wise semantically enriched representation. Our method has the\nadvantage of being independent of long sentence context since we only encode\nthe sequential tokens within a block boundary. Experiments on two datasets\ni.e., SemEval2010 and KBP37, demonstrate the significant advantages of our\nmethod. In particular, we achieve the new state-of-the-art performance on the\nKBP37 dataset; and comparable performance with the state-of-the-art on the\nSemEval2010 dataset.", "published": "2021-03-21 10:23:44", "link": "http://arxiv.org/abs/2103.11356v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Translation by Learning Target Categorical Codes", "abstract": "Non-autoregressive Transformer is a promising text generation model. However,\ncurrent non-autoregressive models still fall behind their autoregressive\ncounterparts in translation quality. We attribute this accuracy gap to the lack\nof dependency modeling among decoder inputs. In this paper, we propose CNAT,\nwhich learns implicitly categorical codes as latent variables into the\nnon-autoregressive decoding. The interaction among these categorical codes\nremedies the missing dependencies and improves the model capacity. Experiment\nresults show that our model achieves comparable or better performance in\nmachine translation tasks, compared with several strong baselines.", "published": "2021-03-21 14:12:34", "link": "http://arxiv.org/abs/2103.11405v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "L3CubeMahaSent: A Marathi Tweet-based Sentiment Analysis Dataset", "abstract": "Sentiment analysis is one of the most fundamental tasks in Natural Language\nProcessing. Popular languages like English, Arabic, Russian, Mandarin, and also\nIndian languages such as Hindi, Bengali, Tamil have seen a significant amount\nof work in this area. However, the Marathi language which is the third most\npopular language in India still lags behind due to the absence of proper\ndatasets. In this paper, we present the first major publicly available Marathi\nSentiment Analysis Dataset - L3CubeMahaSent. It is curated using tweets\nextracted from various Maharashtrian personalities' Twitter accounts. Our\ndataset consists of ~16,000 distinct tweets classified in three broad classes\nviz. positive, negative, and neutral. We also present the guidelines using\nwhich we annotated the tweets. Finally, we present the statistics of our\ndataset and baseline classification results using CNN, LSTM, ULMFiT, and\nBERT-based deep learning models.", "published": "2021-03-21 14:22:13", "link": "http://arxiv.org/abs/2103.11408v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SEMIE: SEMantically Infused Embeddings with Enhanced Interpretability\n  for Domain-specific Small Corpus", "abstract": "Word embeddings are a basic building block of modern NLP pipelines. Efforts\nhave been made to learn rich, efficient, and interpretable embeddings for large\ngeneric datasets available in the public domain. However, these embeddings have\nlimited applicability for small corpora from specific domains such as\nautomotive, manufacturing, maintenance and support, etc. In this work, we\npresent a comprehensive notion of interpretability for word embeddings and\npropose a novel method to generate highly interpretable and efficient\nembeddings for a domain-specific small corpus. We report the evaluation results\nof our resulting word embeddings and demonstrate their novel features for\nenhanced interpretability.", "published": "2021-03-21 16:28:08", "link": "http://arxiv.org/abs/2103.11431v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TextFlint: Unified Multilingual Robustness Evaluation Toolkit for\n  Natural Language Processing", "abstract": "Various robustness evaluation methodologies from different perspectives have\nbeen proposed for different natural language processing (NLP) tasks. These\nmethods have often focused on either universal or task-specific generalization\ncapabilities. In this work, we propose a multilingual robustness evaluation\nplatform for NLP tasks (TextFlint) that incorporates universal text\ntransformation, task-specific transformation, adversarial attack,\nsubpopulation, and their combinations to provide comprehensive robustness\nanalysis. TextFlint enables practitioners to automatically evaluate their\nmodels from all aspects or to customize their evaluations as desired with just\na few lines of code. To guarantee user acceptability, all the text\ntransformations are linguistically based, and we provide a human evaluation for\neach one. TextFlint generates complete analytical reports as well as targeted\naugmented data to address the shortcomings of the model's robustness. To\nvalidate TextFlint's utility, we performed large-scale empirical evaluations\n(over 67,000 evaluations) on state-of-the-art deep learning models, classic\nsupervised methods, and real-world systems. Almost all models showed\nsignificant performance degradation, including a decline of more than 50% of\nBERT's prediction accuracy on tasks such as aspect-level sentiment\nclassification, named entity recognition, and natural language inference.\nTherefore, we call for the robustness to be included in the model evaluation,\nso as to promote the healthy development of NLP technology.", "published": "2021-03-21 17:20:38", "link": "http://arxiv.org/abs/2103.11441v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "#PraCegoVer: A Large Dataset for Image Captioning in Portuguese", "abstract": "Automatically describing images using natural sentences is an important task\nto support visually impaired people's inclusion onto the Internet. It is still\na big challenge that requires understanding the relation of the objects present\nin the image and their attributes and actions they are involved in. Then,\nvisual interpretation methods are needed, but linguistic models are also\nnecessary to verbally describe the semantic relations. This problem is known as\nImage Captioning. Although many datasets were proposed in the literature, the\nmajority contains only English captions, whereas datasets with captions\ndescribed in other languages are scarce. Recently, a movement called PraCegoVer\narose on the Internet, stimulating users from social media to publish images,\ntag #PraCegoVer and add a short description of their content. Thus, inspired by\nthis movement, we have proposed the #PraCegoVer, a multi-modal dataset with\nPortuguese captions based on posts from Instagram. It is the first large\ndataset for image captioning in Portuguese with freely annotated images.\nFurther, the captions in our dataset bring additional challenges to the\nproblem: first, in contrast to popular datasets such as MS COCO Captions,\n#PraCegoVer has only one reference to each image; also, both mean and variance\nof our reference sentence length are significantly greater than those in the MS\nCOCO Captions. These two characteristics contribute to making our dataset\ninteresting due to the linguistic aspect and the challenges that it introduces\nto the image captioning problem. We publicly-share the dataset at\nhttps://github.com/gabrielsantosrv/PraCegoVer.", "published": "2021-03-21 19:55:46", "link": "http://arxiv.org/abs/2103.11474v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Comparative Study on Recent Neural Spoofing Countermeasures for\n  Synthetic Speech Detection", "abstract": "A great deal of recent research effort on speech spoofing countermeasures has\nbeen invested into back-end neural networks and training criteria. We\ncontribute to this effort with a comparative perspective in this study. Our\ncomparison of countermeasure models on the ASVspoof 2019 logical access task\ntakes into account recently proposed margin-based training criteria, widely\nused front ends, and common strategies to deal with varied-length input trials.\nWe also measured intra-model differences through multiple training-evaluation\nrounds with random initialization. Our statistical analysis demonstrates that\nthe performance of the same model may be significantly different when just\nchanging the random initial seed. Thus, we recommend similar analysis or\nmultiple training-evaluation rounds for further research on the database.\nDespite the intra-model differences, we observed a few promising techniques\nsuch as the average pooling to process varied-length inputs and a new\nhyper-parameter-free loss function. The two techniques led to the best single\nmodel in our experiment, which achieved an equal error rate of 1.92% and was\nsignificantly different in statistical sense from most of the other\nexperimental models.", "published": "2021-03-21 07:39:20", "link": "http://arxiv.org/abs/2103.11326v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
