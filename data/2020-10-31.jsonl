{"title": "Understanding Pre-trained BERT for Aspect-based Sentiment Analysis", "abstract": "This paper analyzes the pre-trained hidden representations learned from\nreviews on BERT for tasks in aspect-based sentiment analysis (ABSA). Our work\nis motivated by the recent progress in BERT-based language models for ABSA.\nHowever, it is not clear how the general proxy task of (masked) language model\ntrained on unlabeled corpus without annotations of aspects or opinions can\nprovide important features for downstream tasks in ABSA. By leveraging the\nannotated datasets in ABSA, we investigate both the attentions and the learned\nrepresentations of BERT pre-trained on reviews. We found that BERT uses very\nfew self-attention heads to encode context words (such as prepositions or\npronouns that indicating an aspect) and opinion words for an aspect. Most\nfeatures in the representation of an aspect are dedicated to the fine-grained\nsemantics of the domain (or product category) and the aspect itself, instead of\ncarrying summarized opinions from its context. We hope this investigation can\nhelp future research in improving self-supervised learning, unsupervised\nlearning and fine-tuning for ABSA. The pre-trained model and code can be found\nat https://github.com/howardhsu/BERT-for-RRC-ABSA.", "published": "2020-10-31 02:21:43", "link": "http://arxiv.org/abs/2011.00169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Bias In Dutch Word Embeddings", "abstract": "Recent research in Natural Language Processing has revealed that word\nembeddings can encode social biases present in the training data which can\naffect minorities in real world applications. This paper explores the gender\nbias implicit in Dutch embeddings while investigating whether English language\nbased approaches can also be used in Dutch. We implement the Word Embeddings\nAssociation Test (WEAT), Clustering and Sentence Embeddings Association Test\n(SEAT) methods to quantify the gender bias in Dutch word embeddings, then we\nproceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods\nand finally we evaluate the performance of the debiased embeddings in\ndownstream tasks. The results suggest that, among others, gender bias is\npresent in traditional and contextualized Dutch word embeddings. We highlight\nhow techniques used to measure and reduce bias created for English can be used\nin Dutch embeddings by adequately translating the data and taking into account\nthe unique characteristics of the language. Furthermore, we analyze the effect\nof the debiasing techniques on downstream tasks which show a negligible impact\non traditional embeddings and a 2% decrease in performance in contextualized\nembeddings. Finally, we release the translated Dutch datasets to the public\nalong with the traditional embeddings with mitigated bias.", "published": "2020-10-31 11:14:16", "link": "http://arxiv.org/abs/2011.00244v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution", "abstract": "Now that the performance of coreference resolvers on the simpler forms of\nanaphoric reference has greatly improved, more attention is devoted to more\ncomplex aspects of anaphora. One limitation of virtually all coreference\nresolution models is the focus on single-antecedent anaphors. Plural anaphors\nwith multiple antecedents-so-called split-antecedent anaphors (as in John met\nMary. They went to the movies) have not been widely studied, because they are\nnot annotated in ONTONOTES and are relatively infrequent in other corpora. In\nthis paper, we introduce the first model for unrestricted resolution of\nsplit-antecedent anaphors. We start with a strong baseline enhanced by BERT\nembeddings, and show that we can substantially improve its performance by\naddressing the sparsity issue. To do this, we experiment with auxiliary corpora\nwhere split-antecedent anaphors were annotated by the crowd, and with transfer\nlearning models using element-of bridging references and single-antecedent\ncoreference as auxiliary tasks. Evaluation on the gold annotated ARRAU corpus\nshows that the out best model uses a combination of three auxiliary corpora\nachieved F1 scores of 70% and 43.6% when evaluated in a lenient and strict\nsetting, respectively, i.e., 11 and 21 percentage points gain when compared\nwith our baseline.", "published": "2020-10-31 11:21:39", "link": "http://arxiv.org/abs/2011.00245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rumor Detection on Twitter Using Multiloss Hierarchical BiLSTM with an\n  Attenuation Factor", "abstract": "Social media platforms such as Twitter have become a breeding ground for\nunverified information or rumors. These rumors can threaten people's health,\nendanger the economy, and affect the stability of a country. Many researchers\nhave developed models to classify rumors using traditional machine learning or\nvanilla deep learning models. However, previous studies on rumor detection have\nachieved low precision and are time consuming. Inspired by the hierarchical\nmodel and multitask learning, a multiloss hierarchical BiLSTM model with an\nattenuation factor is proposed in this paper. The model is divided into two\nBiLSTM modules: post level and event level. By means of this hierarchical\nstructure, the model can extract deep in-formation from limited quantities of\ntext. Each module has a loss function that helps to learn bilateral features\nand reduce the training time. An attenuation fac-tor is added at the post level\nto increase the accuracy. The results on two rumor datasets demonstrate that\nour model achieves better performance than that of state-of-the-art machine\nlearning and vanilla deep learning models.", "published": "2020-10-31 12:29:25", "link": "http://arxiv.org/abs/2011.00259v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Coreference Resolution for Arabic", "abstract": "No neural coreference resolver for Arabic exists, in fact we are not aware of\nany learning-based coreference resolver for Arabic since (Bjorkelund and Kuhn,\n2014). In this paper, we introduce a coreference resolution system for Arabic\nbased on Lee et al's end to end architecture combined with the Arabic version\nof bert and an external mention detector. As far as we know, this is the first\nneural coreference resolution system aimed specifically to Arabic, and it\nsubstantially outperforms the existing state of the art on OntoNotes 5.0 with a\ngain of 15.2 points conll F1. We also discuss the current limitations of the\ntask for Arabic and possible approaches that can tackle these challenges.", "published": "2020-10-31 14:34:43", "link": "http://arxiv.org/abs/2011.00286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Method of the coherence evaluation of Ukrainian text", "abstract": "Due to the growing role of the SEO technologies, it is necessary to perform\nan automated analysis of the article's quality. Such approach helps both to\nreturn the most intelligible pages for the user's query and to raise the web\nsites positions to the top of query results. An automated assessment of a\ncoherence is a part of the complex analysis of the text. In this article, main\nmethods for text coherence measurements for Ukrainian language are analyzed.\nExpediency of using the semantic similarity graph method in comparison with\nother methods are explained. It is suggested the improvement of that method by\nthe pre-training of the neural network for vector representations of sentences.\nExperimental examination of the original method and its modifications is made.\nTraining and examination procedures are made on the corpus of Ukrainian texts,\nwhich were previously retrieved from abstracts and full texts of Ukrainian\nscientific articles. The testing procedure is implemented by performing of two\ntypical tasks for the text coherence assessment: document discrimination task\nand insertion task. Accordingly to the analysis it is defined the most\neffective combination of method's modification and its parameter for the\nmeasurement of the text coherence.", "published": "2020-10-31 16:48:55", "link": "http://arxiv.org/abs/2011.00310v1", "categories": ["cs.CL", "68U15", "I.7"], "primary_category": "cs.CL"}
{"title": "Effective Approach to Develop a Sentiment Annotator For Legal Domain in\n  a Low Resource Setting", "abstract": "Analyzing the sentiments of legal opinions available in Legal Opinion Texts\ncan facilitate several use cases such as legal judgement prediction,\ncontradictory statements identification and party-based sentiment analysis.\nHowever, the task of developing a legal domain specific sentiment annotator is\nchallenging due to resource constraints such as lack of domain specific\nlabelled data and domain expertise. In this study, we propose novel techniques\nthat can be used to develop a sentiment annotator for the legal domain while\nminimizing the need for manual annotations of data.", "published": "2020-10-31 17:12:32", "link": "http://arxiv.org/abs/2011.00318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pick a Fight or Bite your Tongue: Investigation of Gender Differences in\n  Idiomatic Language Usage", "abstract": "A large body of research on gender-linked language has established\nfoundations regarding cross-gender differences in lexical, emotional, and\ntopical preferences, along with their sociological underpinnings. We compile a\nnovel, large and diverse corpus of spontaneous linguistic productions annotated\nwith speakers' gender, and perform a first large-scale empirical study of\ndistinctions in the usage of \\textit{figurative language} between male and\nfemale authors. Our analyses suggest that (1) idiomatic choices reflect\ngender-specific lexical and semantic preferences in general language, (2) men's\nand women's idiomatic usages express higher emotion than their literal\nlanguage, with detectable, albeit more subtle, differences between male and\nfemale authors along the dimension of dominance compared to similar\ndistinctions in their literal utterances, and (3) contextual analysis of\nidiomatic expressions reveals considerable differences, reflecting subtle\ndivergences in usage environments, shaped by cross-gender communication styles\nand semantic biases.", "published": "2020-10-31 18:44:07", "link": "http://arxiv.org/abs/2011.00335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Arabic emotion recognition using deep neural networks", "abstract": "Emotion recognition from speech signal based on deep learning is an active\nresearch area. Convolutional neural networks (CNNs) may be the dominant method\nin this area. In this paper, we implement two neural architectures to address\nthis problem. The first architecture is an attention-based CNN-LSTM-DNN model.\nIn this novel architecture, the convolutional layers extract salient features\nand the bi-directional long short-term memory (BLSTM) layers handle the\nsequential phenomena of the speech signal. This is followed by an attention\nlayer, which extracts a summary vector that is fed to the fully connected dense\nlayer (DNN), which finally connects to a softmax output layer. The second\narchitecture is based on a deep CNN model. The results on an Arabic speech\nemotion recognition task show that our innovative approach can lead to\nsignificant improvements (2.2% absolute improvements) over a strong deep CNN\nbaseline system. On the other hand, the deep CNN models are significantly\nfaster than the attention based CNN-LSTM-DNN models in training and\nclassification.", "published": "2020-10-31 19:39:37", "link": "http://arxiv.org/abs/2011.00346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset for Tracking Entities in Open Domain Procedural Text", "abstract": "We present the first dataset for tracking state changes in procedural text\nfrom arbitrary domains by using an unrestricted (open) vocabulary. For example,\nin a text describing fog removal using potatoes, a car window may transition\nbetween being foggy, sticky,opaque, and clear. Previous formulations of this\ntask provide the text and entities involved,and ask how those entities change\nfor just a small, pre-defined set of attributes (e.g., location), limiting\ntheir fidelity. Our solution is a new task formulation where given just a\nprocedural text as input, the task is to generate a set of state change\ntuples(entity, at-tribute, before-state, after-state)for each step,where the\nentity, attribute, and state values must be predicted from an open vocabulary.\nUsing crowdsourcing, we create OPENPI1, a high-quality (91.5% coverage as\njudged by humans and completely vetted), and large-scale dataset comprising\n29,928 state changes over 4,050 sentences from 810 procedural real-world\nparagraphs from WikiHow.com. A current state-of-the-art generation model on\nthis task achieves 16.1% F1 based on BLEU metric, leaving enough room for novel\nmodel architectures.", "published": "2020-10-31 02:33:53", "link": "http://arxiv.org/abs/2011.08092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Multimodal Feedback Generation in Education", "abstract": "The automatic evaluation for school assignments is an important application\nof AI in the education field. In this work, we focus on the task of\npersonalized multimodal feedback generation, which aims to generate\npersonalized feedback for various teachers to evaluate students' assignments\ninvolving multimodal inputs such as images, audios, and texts. This task\ninvolves the representation and fusion of multimodal information and natural\nlanguage generation, which presents the challenges from three aspects: 1) how\nto encode and integrate multimodal inputs; 2) how to generate feedback specific\nto each modality; and 3) how to realize personalized feedback generation. In\nthis paper, we propose a novel Personalized Multimodal Feedback Generation\nNetwork (PMFGN) armed with a modality gate mechanism and a personalized bias\nmechanism to address these challenges. The extensive experiments on real-world\nK-12 education data show that our model significantly outperforms several\nbaselines by generating more accurate and diverse feedback. In addition,\ndetailed ablation experiments are conducted to deepen our understanding of the\nproposed framework.", "published": "2020-10-31 05:26:49", "link": "http://arxiv.org/abs/2011.00192v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Aspectuality Across Genre: A Distributional Semantics Approach", "abstract": "The interpretation of the lexical aspect of verbs in English plays a crucial\nrole for recognizing textual entailment and learning discourse-level\ninferences. We show that two elementary dimensions of aspectual class, states\nvs. events, and telic vs. atelic events, can be modelled effectively with\ndistributional semantics. We find that a verb's local context is most\nindicative of its aspectual class, and demonstrate that closed class words tend\nto be stronger discriminating contexts than content words. Our approach\noutperforms previous work on three datasets. Lastly, we contribute a dataset of\nhuman--human conversations annotated with lexical aspect and present\nexperiments that show the correlation of telicity with genre and discourse\ngoals.", "published": "2020-10-31 19:37:22", "link": "http://arxiv.org/abs/2011.00345v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The xx205 System for the VoxCeleb Speaker Recognition Challenge 2020", "abstract": "This report describes the systems submitted to the first and second tracks of\nthe VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020, which ranked second\nin both tracks. Three key points of the system pipeline are explored: (1)\ninvestigating multiple CNN architectures including ResNet, Res2Net and dual\npath network (DPN) to extract the x-vectors, (2) using a composite angular\nmargin softmax loss to train the speaker models, and (3) applying score\nnormalization and system fusion to boost the performance. Measured on the\nVoxSRC-20 Eval set, the best submitted systems achieve an EER of $3.808\\%$ and\na MinDCF of $0.1958$ in the close-condition track 1, and an EER of $3.798\\%$\nand a MinDCF of $0.1942$ in the open-condition track 2, respectively.", "published": "2020-10-31 06:36:26", "link": "http://arxiv.org/abs/2011.00200v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multilingual Bottleneck Features for Improving ASR Performance of\n  Code-Switched Speech in Under-Resourced Languages", "abstract": "In this work, we explore the benefits of using multilingual bottleneck\nfeatures (mBNF) in acoustic modelling for the automatic speech recognition of\ncode-switched (CS) speech in African languages. The unavailability of annotated\ncorpora in the languages of interest has always been a primary challenge when\ndeveloping speech recognition systems for this severely under-resourced type of\nspeech. Hence, it is worthwhile to investigate the potential of using speech\ncorpora available for other better-resourced languages to improve speech\nrecognition performance. To achieve this, we train a mBNF extractor using nine\nSouthern Bantu languages that form part of the freely available multilingual\nNCHLT corpus. We append these mBNFs to the existing MFCCs, pitch features and\ni-vectors to train acoustic models for automatic speech recognition (ASR) in\nthe target code-switched languages. Our results show that the inclusion of the\nmBNF features leads to clear performance improvements over a baseline trained\nwithout the mBNFs for code-switched English-isiZulu, English-isiXhosa,\nEnglish-Sesotho and English-Setswana speech.", "published": "2020-10-31 18:51:42", "link": "http://arxiv.org/abs/2011.03118v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Urban Sound Tagging with Spatiotemporal Context", "abstract": "Noise pollution significantly affects our daily life and urban development.\nUrban Sound Tagging (UST) has attracted much attention recently, which aims to\nanalyze and monitor urban noise pollution. One weakness of the previous UST\nstudies is that the spatial and temporal context of sound signals, which\ncontains complementary information about when and where the audio data was\nrecorded, has not been investigated. To address this problem, in this paper, we\npropose a multimodal UST system that deeply mines the audio and spatiotemporal\ncontext together. In order to incorporate characteristics of different acoustic\nfeatures, two sets of four spectrograms are first extracted as the inputs of\nresidual neural networks. Then, the spatiotemporal context is encoded and\ncombined with acoustic features to explore the efficiency of multimodal\nlearning for discriminating sound signals. Moreover, a data filtering approach\nis adopted in text processing to further improve the performance of\nmulti-modality. We evaluate the proposed method on the UST challenge (task 5)\nof DCASE2020. Experimental results demonstrate the effectiveness of the\nproposed method.", "published": "2020-10-31 03:07:57", "link": "http://arxiv.org/abs/2011.00175v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AGAIN-VC: A One-shot Voice Conversion using Activation Guidance and\n  Adaptive Instance Normalization", "abstract": "Recently, voice conversion (VC) has been widely studied. Many VC systems use\ndisentangle-based learning techniques to separate the speaker and the\nlinguistic content information from a speech signal. Subsequently, they convert\nthe voice by changing the speaker information to that of the target speaker. To\nprevent the speaker information from leaking into the content embeddings,\nprevious works either reduce the dimension or quantize the content embedding as\na strong information bottleneck. These mechanisms somehow hurt the synthesis\nquality. In this work, we propose AGAIN-VC, an innovative VC system using\nActivation Guidance and Adaptive Instance Normalization. AGAIN-VC is an\nauto-encoder-based model, comprising of a single encoder and a decoder. With a\nproper activation as an information bottleneck on content embeddings, the\ntrade-off between the synthesis quality and the speaker similarity of the\nconverted speech is improved drastically. This one-shot VC system obtains the\nbest performance regardless of the subjective or objective evaluations.", "published": "2020-10-31 17:09:19", "link": "http://arxiv.org/abs/2011.00316v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RespireNet: A Deep Neural Network for Accurately Detecting Abnormal Lung\n  Sounds in Limited Data Setting", "abstract": "Auscultation of respiratory sounds is the primary tool for screening and\ndiagnosing lung diseases. Automated analysis, coupled with digital\nstethoscopes, can play a crucial role in enabling tele-screening of fatal lung\ndiseases. Deep neural networks (DNNs) have shown a lot of promise for such\nproblems, and are an obvious choice. However, DNNs are extremely data hungry,\nand the largest respiratory dataset ICBHI has only 6898 breathing cycles, which\nis still small for training a satisfactory DNN model. In this work, RespireNet,\nwe propose a simple CNN-based model, along with a suite of novel techniques --\ndevice specific fine-tuning, concatenation-based augmentation, blank region\nclipping, and smart padding -- enabling us to efficiently use the small-sized\ndataset. We perform extensive evaluation on the ICBHI dataset, and improve upon\nthe state-of-the-art results for 4-class classification by 2.2%", "published": "2020-10-31 05:53:37", "link": "http://arxiv.org/abs/2011.00196v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
