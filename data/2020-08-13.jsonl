{"title": "Cognitive Representation Learning of Self-Media Online Article Quality", "abstract": "The automatic quality assessment of self-media online articles is an urgent\nand new issue, which is of great value to the online recommendation and search.\nDifferent from traditional and well-formed articles, self-media online articles\nare mainly created by users, which have the appearance characteristics of\ndifferent text levels and multi-modal hybrid editing, along with the potential\ncharacteristics of diverse content, different styles, large semantic spans and\ngood interactive experience requirements. To solve these challenges, we\nestablish a joint model CoQAN in combination with the layout organization,\nwriting characteristics and text semantics, designing different representation\nlearning subnetworks, especially for the feature learning process and\ninteractive reading habits on mobile terminals. It is more consistent with the\ncognitive style of expressing an expert's evaluation of articles. We have also\nconstructed a large scale real-world assessment dataset. Extensive experimental\nresults show that the proposed framework significantly outperforms\nstate-of-the-art methods, and effectively learns and integrates different\nfactors of the online article quality assessment.", "published": "2020-08-13 02:59:52", "link": "http://arxiv.org/abs/2008.05658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue State Induction Using Neural Latent Variable Models", "abstract": "Dialogue state modules are a useful component in a task-oriented dialogue\nsystem. Traditional methods find dialogue states by manually labeling training\ncorpora, upon which neural models are trained. However, the labeling process\ncan be costly, slow, error-prone, and more importantly, cannot cover the vast\nrange of domains in real-world dialogues for customer service. We propose the\ntask of dialogue state induction, building two neural latent variable models\nthat mine dialogue states automatically from unlabeled customer service\ndialogue records. Results show that the models can effectively find meaningful\nslots. In addition, equipped with induced dialogue states, a state-of-the-art\ndialogue system gives better performance compared with not using a dialogue\nstate module.", "published": "2020-08-13 03:14:25", "link": "http://arxiv.org/abs/2008.05666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ANDES at SemEval-2020 Task 12: A jointly-trained BERT multilingual model\n  for offensive language detection", "abstract": "This paper describes our participation in SemEval-2020 Task 12: Multilingual\nOffensive Language Detection. We jointly-trained a single model by fine-tuning\nMultilingual BERT to tackle the task across all the proposed languages:\nEnglish, Danish, Turkish, Greek and Arabic. Our single model had competitive\nresults, with a performance close to top-performing systems in spite of sharing\nthe same parameters across all languages. Zero-shot and few-shot experiments\nwere also conducted to analyze the transference performance among these\nlanguages. We make our code public for further research", "published": "2020-08-13 16:07:00", "link": "http://arxiv.org/abs/2008.06408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation\n  Objectively?", "abstract": "Misinformation is an ever increasing problem that is difficult to solve for\nthe research community and has a negative impact on the society at large. Very\nrecently, the problem has been addressed with a crowdsourcing-based approach to\nscale up labeling efforts: to assess the truthfulness of a statement, instead\nof relying on a few experts, a crowd of (non-expert) judges is exploited. We\nfollow the same approach to study whether crowdsourcing is an effective and\nreliable method to assess statements truthfulness during a pandemic. We\nspecifically target statements related to the COVID-19 health emergency, that\nis still ongoing at the time of the study and has arguably caused an increase\nof the amount of misinformation that is spreading online (a phenomenon for\nwhich the term \"infodemic\" has been used). By doing so, we are able to address\n(mis)information that is both related to a sensitive and personal issue like\nhealth and very recent as compared to when the judgment is done: two issues\nthat have not been analyzed in related work. In our experiment, crowd workers\nare asked to assess the truthfulness of statements, as well as to provide\nevidence for the assessments as a URL and a text justification. Besides showing\nthat the crowd is able to accurately judge the truthfulness of the statements,\nwe also report results on many different aspects, including: agreement among\nworkers, the effect of different aggregation functions, of scales\ntransformations, and of workers background / bias. We also analyze workers\nbehavior, in terms of queries submitted, URLs found / selected, text\njustifications, and other behavioral data like clicks and mouse actions\ncollected by means of an ad hoc logger.", "published": "2020-08-13 05:53:24", "link": "http://arxiv.org/abs/2008.05701v1", "categories": ["cs.IR", "cs.CL", "68P20", "H.3"], "primary_category": "cs.IR"}
{"title": "Exploration of Gender Differences in COVID-19 Discourse on Reddit", "abstract": "Decades of research on differences in the language of men and women have\nestablished postulates about preferences in lexical, topical, and emotional\nexpression between the two genders, along with their sociological\nunderpinnings. Using a novel dataset of male and female linguistic productions\ncollected from the Reddit discussion platform, we further confirm existing\nassumptions about gender-linked affective distinctions, and demonstrate that\nthese distinctions are amplified in social media postings involving\nemotionally-charged discourse related to COVID-19. Our analysis also confirms\nconsiderable differences in topical preferences between male and female authors\nin spontaneous pandemic-related discussions.", "published": "2020-08-13 06:29:24", "link": "http://arxiv.org/abs/2008.05713v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MICE: Mining Idioms with Contextual Embeddings", "abstract": "Idiomatic expressions can be problematic for natural language processing\napplications as their meaning cannot be inferred from their constituting words.\nA lack of successful methodological approaches and sufficiently large datasets\nprevents the development of machine learning approaches for detecting idioms,\nespecially for expressions that do not occur in the training set. We present an\napproach, called MICE, that uses contextual embeddings for that purpose. We\npresent a new dataset of multi-word expressions with literal and idiomatic\nmeanings and use it to train a classifier based on two state-of-the-art\ncontextual word embeddings: ELMo and BERT. We show that deep neural networks\nusing both embeddings perform much better than existing approaches, and are\ncapable of detecting idiomatic word use, even for expressions that were not\npresent in the training set. We demonstrate cross-lingual transfer of developed\nmodels and analyze the size of the required dataset.", "published": "2020-08-13 08:56:40", "link": "http://arxiv.org/abs/2008.05759v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MASRI-HEADSET: A Maltese Corpus for Speech Recognition", "abstract": "Maltese, the national language of Malta, is spoken by approximately 500,000\npeople. Speech processing for Maltese is still in its early stages of\ndevelopment. In this paper, we present the first spoken Maltese corpus designed\npurposely for Automatic Speech Recognition (ASR). The MASRI-HEADSET corpus was\ndeveloped by the MASRI project at the University of Malta. It consists of 8\nhours of speech paired with text, recorded by using short text snippets in a\nlaboratory environment. The speakers were recruited from different geographical\nlocations all over the Maltese islands, and were roughly evenly distributed by\ngender. This paper also presents some initial results achieved in baseline\nexperiments for Maltese ASR using Sphinx and Kaldi. The MASRI-HEADSET Corpus is\npublicly available for research/academic purposes.", "published": "2020-08-13 08:57:16", "link": "http://arxiv.org/abs/2008.05760v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continuous Speech Separation with Conformer", "abstract": "Continuous speech separation plays a vital role in complicated speech related\ntasks such as conversation transcription. The separation model extracts a\nsingle speaker signal from a mixed speech. In this paper, we use transformer\nand conformer in lieu of recurrent neural networks in the separation system, as\nwe believe capturing global information with the self-attention based method is\ncrucial for the speech separation. Evaluating on the LibriCSS dataset, the\nconformer separation model achieves state of the art results, with a relative\n23.5% word error rate (WER) reduction from bi-directional LSTM (BLSTM) in the\nutterance-wise evaluation and a 15.4% WER reduction in the continuous\nevaluation.", "published": "2020-08-13 09:36:05", "link": "http://arxiv.org/abs/2008.05773v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "On the Importance of Local Information in Transformer Based Models", "abstract": "The self-attention module is a key component of Transformer-based models,\nwherein each token pays attention to every other token. Recent studies have\nshown that these heads exhibit syntactic, semantic, or local behaviour. Some\nstudies have also identified promise in restricting this attention to be local,\ni.e., a token attending to other tokens only in a small neighbourhood around\nit. However, no conclusive evidence exists that such local attention alone is\nsufficient to achieve high accuracy on multiple NLP tasks. In this work, we\nsystematically analyse the role of locality information in learnt models and\ncontrast it with the role of syntactic information. More specifically, we first\ndo a sensitivity analysis and show that, at every layer, the representation of\na token is much more sensitive to tokens in a small neighborhood around it than\nto tokens which are syntactically related to it. We then define an attention\nbias metric to determine whether a head pays more attention to local tokens or\nto syntactically related tokens. We show that a larger fraction of heads have a\nlocality bias as compared to a syntactic bias. Having established the\nimportance of local attention heads, we train and evaluate models where varying\nfractions of the attention heads are constrained to be local. Such models would\nbe more efficient as they would have fewer computations in the attention layer.\nWe evaluate these models on 4 GLUE datasets (QQP, SST-2, MRPC, QNLI) and 2 MT\ndatasets (En-De, En-Ru) and clearly demonstrate that such constrained models\nhave comparable performance to the unconstrained models. Through this\nsystematic evaluation we establish that attention in Transformer-based models\ncan be constrained to be local without affecting performance.", "published": "2020-08-13 11:32:47", "link": "http://arxiv.org/abs/2008.05828v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Commonsense Knowledge Graph Reasoning by Selection or Generation? Why?", "abstract": "Commonsense knowledge graph reasoning(CKGR) is the task of predicting a\nmissing entity given one existing and the relation in a commonsense knowledge\ngraph (CKG). Existing methods can be classified into two categories generation\nmethod and selection method. Each method has its own advantage. We\ntheoretically and empirically compare the two methods, finding the selection\nmethod is more suitable than the generation method in CKGR. Given the\nobservation, we further combine the structure of neural Text Encoder and\nKnowledge Graph Embedding models to solve the selection method's two problems,\nachieving competitive results. We provide a basic framework and baseline model\nfor subsequent CKGR tasks by selection methods.", "published": "2020-08-13 14:13:30", "link": "http://arxiv.org/abs/2008.05925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ranking Enhanced Dialogue Generation", "abstract": "How to effectively utilize the dialogue history is a crucial problem in\nmulti-turn dialogue generation. Previous works usually employ various neural\nnetwork architectures (e.g., recurrent neural networks, attention mechanisms,\nand hierarchical structures) to model the history. However, a recent empirical\nstudy by Sankar et al. has shown that these architectures lack the ability of\nunderstanding and modeling the dynamics of the dialogue history. For example,\nthe widely used architectures are insensitive to perturbations of the dialogue\nhistory, such as words shuffling, utterances missing, and utterances\nreordering. To tackle this problem, we propose a Ranking Enhanced Dialogue\ngeneration framework in this paper. Despite the traditional representation\nencoder and response generation modules, an additional ranking module is\nintroduced to model the ranking relation between the former utterance and\nconsecutive utterances. Specifically, the former utterance and consecutive\nutterances are treated as query and corresponding documents, and both local and\nglobal ranking losses are designed in the learning process. In this way, the\ndynamics in the dialogue history can be explicitly captured. To evaluate our\nproposed models, we conduct extensive experiments on three public datasets,\ni.e., bAbI, PersonaChat, and JDC. Experimental results show that our models\nproduce better responses in terms of both quantitative measures and human\njudgments, as compared with the state-of-the-art dialogue generation models.\nFurthermore, we give some detailed experimental analysis to show where and how\nthe improvements come from.", "published": "2020-08-13 01:49:56", "link": "http://arxiv.org/abs/2008.05640v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prosody Learning Mechanism for Speech Synthesis System Without Text\n  Length Limit", "abstract": "Recent neural speech synthesis systems have gradually focused on the control\nof prosody to improve the quality of synthesized speech, but they rarely\nconsider the variability of prosody and the correlation between prosody and\nsemantics together. In this paper, a prosody learning mechanism is proposed to\nmodel the prosody of speech based on TTS system, where the prosody information\nof speech is extracted from the melspectrum by a prosody learner and combined\nwith the phoneme sequence to reconstruct the mel-spectrum. Meanwhile, the\nsematic features of text from the pre-trained language model is introduced to\nimprove the prosody prediction results. In addition, a novel self-attention\nstructure, named as local attention, is proposed to lift this restriction of\ninput text length, where the relative position information of the sequence is\nmodeled by the relative position matrices so that the position encodings is no\nlonger needed. Experiments on English and Mandarin show that speech with more\nsatisfactory prosody has obtained in our model. Especially in Mandarin\nsynthesis, our proposed model outperforms baseline model with a MOS gap of\n0.08, and the overall naturalness of the synthesized speech has been\nsignificantly improved.", "published": "2020-08-13 02:54:50", "link": "http://arxiv.org/abs/2008.05656v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Large-scale Transfer Learning for Low-resource Spoken Language\n  Understanding", "abstract": "End-to-end Spoken Language Understanding (SLU) models are made increasingly\nlarge and complex to achieve the state-ofthe-art accuracy. However, the\nincreased complexity of a model can also introduce high risk of over-fitting,\nwhich is a major challenge in SLU tasks due to the limitation of available\ndata. In this paper, we propose an attention-based SLU model together with\nthree encoder enhancement strategies to overcome data sparsity challenge. The\nfirst strategy focuses on the transferlearning approach to improve feature\nextraction capability of the encoder. It is implemented by pre-training the\nencoder component with a quantity of Automatic Speech Recognition annotated\ndata relying on the standard Transformer architecture and then fine-tuning the\nSLU model with a small amount of target labelled data. The second strategy\nadopts multitask learning strategy, the SLU model integrates the speech\nrecognition model by sharing the same underlying encoder, such that improving\nrobustness and generalization ability. The third strategy, learning from\nComponent Fusion (CF) idea, involves a Bidirectional Encoder Representation\nfrom Transformer (BERT) model and aims to boost the capability of the decoder\nwith an auxiliary network. It hence reduces the risk of over-fitting and\naugments the ability of the underlying encoder, indirectly. Experiments on the\nFluentAI dataset show that cross-language transfer learning and multi-task\nstrategies have been improved by up to 4:52% and 3:89% respectively, compared\nto the baseline.", "published": "2020-08-13 03:43:05", "link": "http://arxiv.org/abs/2008.05671v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable\n  End-to-End Speech Recognition", "abstract": "Transformer has achieved competitive performance against state-of-the-art\nend-to-end models in automatic speech recognition (ASR), and requires\nsignificantly less training time than RNN-based models. The original\nTransformer, with encoder-decoder architecture, is only suitable for offline\nASR. It relies on an attention mechanism to learn alignments, and encodes input\naudio bidirectionally. The high computation cost of Transformer decoding also\nlimits its use in production streaming systems. To make Transformer suitable\nfor streaming ASR, we explore Transducer framework as a streamable way to learn\nalignments. For audio encoding, we apply unidirectional Transformer with\ninterleaved convolution layers. The interleaved convolution layers are used for\nmodeling future context which is important to performance. To reduce\ncomputation cost, we gradually downsample acoustic input, also with the\ninterleaved convolution layers. Moreover, we limit the length of history\ncontext in self-attention to maintain constant computation cost for each\ndecoding step. We show that this architecture, named Conv-Transformer\nTransducer, achieves competitive performance on LibriSpeech dataset (3.6\\% WER\non test-clean) without external language models. The performance is comparable\nto previously published streamable Transformer Transducer and strong hybrid\nstreaming ASR systems, and is achieved with smaller look-ahead window (140~ms),\nfewer parameters and lower frame rate.", "published": "2020-08-13 08:20:02", "link": "http://arxiv.org/abs/2008.05750v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Survey on Knowledge integration techniques with Artificial Neural\n  Networks for seq-2-seq/time series models", "abstract": "In recent years, with the advent of massive computational power and the\navailability of huge amounts of data, Deep neural networks have enabled the\nexploration of uncharted areas in several domains. But at times, they\nunder-perform due to insufficient data, poor data quality, data that might not\nbe covering the domain broadly, etc. Knowledge-based systems leverage expert\nknowledge for making decisions and suitably take actions. Such systems retain\ninterpretability in the decision-making process. This paper focuses on\nexploring techniques to integrate expert knowledge to the Deep Neural Networks\nfor sequence-to-sequence and time series models to improve their performance\nand interpretability.", "published": "2020-08-13 15:40:38", "link": "http://arxiv.org/abs/2008.05972v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Studying Dishonest Intentions in Brazilian Portuguese Texts", "abstract": "Previous work in the social sciences, psychology and linguistics has show\nthat liars have some control over the content of their stories, however their\nunderlying state of mind may \"leak out\" through the way that they tell them. To\nthe best of our knowledge, no previous systematic effort exists in order to\ndescribe and model deception language for Brazilian Portuguese. To fill this\nimportant gap, we carry out an initial empirical linguistic study on false\nstatements in Brazilian news. We methodically analyze linguistic features using\na deceptive news corpus, which includes both fake and true news. The results\nshow that they present substantial lexical, syntactic and semantic variations,\nas well as punctuation and emotion distinctions.", "published": "2020-08-13 18:44:52", "link": "http://arxiv.org/abs/2008.06079v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cross attentive pooling for speaker verification", "abstract": "The goal of this paper is text-independent speaker verification where\nutterances come from 'in the wild' videos and may contain irrelevant signal.\nWhile speaker verification is naturally a pair-wise problem, existing methods\nto produce the speaker embeddings are instance-wise. In this paper, we propose\nCross Attentive Pooling (CAP) that utilizes the context information across the\nreference-query pair to generate utterance-level embeddings that contain the\nmost discriminative information for the pair-wise matching problem. Experiments\nare performed on the VoxCeleb dataset in which our method outperforms\ncomparable pooling strategies.", "published": "2020-08-13 15:59:23", "link": "http://arxiv.org/abs/2008.05983v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LSTM Acoustic Models Learn to Align and Pronounce with Graphemes", "abstract": "Automated speech recognition coverage of the world's languages continues to\nexpand. However, standard phoneme based systems require handcrafted lexicons\nthat are difficult and expensive to obtain. To address this problem, we propose\na training methodology for a grapheme-based speech recognizer that can be\ntrained in a purely data-driven fashion. Built with LSTM networks and trained\nwith the cross-entropy loss, the grapheme-output acoustic models we study are\nalso extremely practical for real-world applications as they can be decoded\nwith conventional ASR stack components such as language models and FST\ndecoders, and produce good quality audio-to-grapheme alignments that are useful\nin many speech applications. We show that the grapheme models are competitive\nin WER with their phoneme-output counterparts when trained on large datasets,\nwith the advantage that grapheme models do not require explicit linguistic\nknowledge as an input. We further compare the alignments generated by the\nphoneme and grapheme models to demonstrate the quality of the pronunciations\nlearnt by them using four Indian languages that vary linguistically in spoken\nand written forms.", "published": "2020-08-13 21:38:36", "link": "http://arxiv.org/abs/2008.06121v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "MLNET: An Adaptive Multiple Receptive-field Attention Neural Network for\n  Voice Activity Detection", "abstract": "Voice activity detection (VAD) makes a distinction between speech and\nnon-speech and its performance is of crucial importance for speech based\nservices. Recently, deep neural network (DNN)-based VADs have achieved better\nperformance than conventional signal processing methods. The existed DNNbased\nmodels always handcrafted a fixed window to make use of the contextual speech\ninformation to improve the performance of VAD. However, the fixed window of\ncontextual speech information can't handle various unpredicatable noise\nenvironments and highlight the critical speech information to VAD task. In\norder to solve this problem, this paper proposed an adaptive multiple\nreceptive-field attention neural network, called MLNET, to finish VAD task. The\nMLNET leveraged multi-branches to extract multiple contextual speech\ninformation and investigated an effective attention block to weight the most\ncrucial parts of the context for final classification. Experiments in\nreal-world scenarios demonstrated that the proposed MLNET-based model\noutperformed other baselines.", "published": "2020-08-13 02:24:28", "link": "http://arxiv.org/abs/2008.05650v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evolutionary Algorithm Enhanced Neural Architecture Search for\n  Text-Independent Speaker Verification", "abstract": "State-of-the-art speaker verification models are based on deep learning\ntechniques, which heavily depend on the handdesigned neural architectures from\nexperts or engineers. We borrow the idea of neural architecture search(NAS) for\nthe textindependent speaker verification task. As NAS can learn deep network\nstructures automatically, we introduce the NAS conception into the well-known\nx-vector network. Furthermore, this paper proposes an evolutionary algorithm\nenhanced neural architecture search method called Auto-Vector to automatically\ndiscover promising networks for the speaker verification task. The experimental\nresults demonstrate our NAS-based model outperforms state-of-the-art speaker\nverification models.", "published": "2020-08-13 05:34:52", "link": "http://arxiv.org/abs/2008.05695v1", "categories": ["eess.AS", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Speech Intelligibility in Text-To-Speech Synthesis using\n  Speaking Style Conversion", "abstract": "The increased adoption of digital assistants makes text-to-speech (TTS)\nsynthesis systems an indispensable feature of modern mobile devices. It is\nhence desirable to build a system capable of generating highly intelligible\nspeech in the presence of noise. Past studies have investigated style\nconversion in TTS synthesis, yet degraded synthesized quality often leads to\nworse intelligibility. To overcome such limitations, we proposed a novel\ntransfer learning approach using Tacotron and WaveRNN based TTS synthesis. The\nproposed speech system exploits two modification strategies: (a) Lombard\nspeaking style data and (b) Spectral Shaping and Dynamic Range Compression\n(SSDRC) which has been shown to provide high intelligibility gains by\nredistributing the signal energy on the time-frequency domain. We refer to this\nextension as Lombard-SSDRC TTS system. Intelligibility enhancement as\nquantified by the Intelligibility in Bits (SIIB-Gauss) measure shows that the\nproposed Lombard-SSDRC TTS system shows significant relative improvement\nbetween 110% and 130% in speech-shaped noise (SSN), and 47% to 140% in\ncompeting-speaker noise (CSN) against the state-of-the-art TTS approach.\nAdditional subjective evaluation shows that Lombard-SSDRC TTS successfully\nincreases the speech intelligibility with relative improvement of 455% for SSN\nand 104% for CSN in median keyword correction rate compared to the baseline TTS\nmethod.", "published": "2020-08-13 10:51:56", "link": "http://arxiv.org/abs/2008.05809v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Quality Assessment for Audio-Visual Verification Systems. The\n  LOVe submission to NIST SRE Challenge 2019", "abstract": "Fusion of scores is a cornerstone of multimodal biometric systems composed of\nindependent unimodal parts. In this work, we focus on quality-dependent fusion\nfor speaker-face verification. To this end, we propose a universal model which\ncan be trained for automatic quality assessment of both face and speaker\nmodalities. This model estimates the quality of representations produced by\nunimodal systems which are then used to enhance the score-level fusion of\nspeaker and face verification modules. We demonstrate the improvements brought\nby this quality-dependent fusion on the recent NIST SRE19 Audio-Visual\nChallenge dataset.", "published": "2020-08-13 13:21:48", "link": "http://arxiv.org/abs/2008.05889v2", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Textual Echo Cancellation", "abstract": "In this paper, we propose Textual Echo Cancellation (TEC) - a framework for\ncancelling the text-to-speech (TTS) playback echo from overlapping speech\nrecordings. Such a system can largely improve speech recognition performance\nand user experience for intelligent devices such as smart speakers, as the user\ncan talk to the device while the device is still playing the TTS signal\nresponding to the previous query. We implement this system by using a novel\nsequence-to-sequence model with multi-source attention that takes both the\nmicrophone mixture signal and source text of the TTS playback as inputs, and\npredicts the enhanced audio. Experiments show that the textual information of\nthe TTS playback is critical to enhancement performance. Besides, the text\nsequence is much smaller in size compared with the raw acoustic signal of the\nTTS playback, and can be immediately transmitted to the device or ASR server\neven before the playback is synthesized. Therefore, our proposed approach\neffectively reduces Internet communication and latency compared with\nalternative approaches such as acoustic echo cancellation (AEC).", "published": "2020-08-13 16:47:30", "link": "http://arxiv.org/abs/2008.06006v4", "categories": ["eess.AS", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Incorporating Broad Phonetic Information for Speech Enhancement", "abstract": "In noisy conditions, knowing speech contents facilitates listeners to more\neffectively suppress background noise components and to retrieve pure speech\nsignals. Previous studies have also confirmed the benefits of incorporating\nphonetic information in a speech enhancement (SE) system to achieve better\ndenoising performance. To obtain the phonetic information, we usually prepare a\nphoneme-based acoustic model, which is trained using speech waveforms and\nphoneme labels. Despite performing well in normal noisy conditions, when\noperating in very noisy conditions, however, the recognized phonemes may be\nerroneous and thus misguide the SE process. To overcome the limitation, this\nstudy proposes to incorporate the broad phonetic class (BPC) information into\nthe SE process. We have investigated three criteria to build the BPC, including\ntwo knowledge-based criteria: place and manner of articulatory and one\ndata-driven criterion. Moreover, the recognition accuracies of BPCs are much\nhigher than that of phonemes, thus providing more accurate phonetic information\nto guide the SE process under very noisy conditions. Experimental results\ndemonstrate that the proposed SE with the BPC information framework can achieve\nnotable performance improvements over the baseline system and an SE system\nusing monophonic information in terms of both speech quality intelligibility on\nthe TIMIT dataset.", "published": "2020-08-13 09:38:08", "link": "http://arxiv.org/abs/2008.07618v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Recognition using EEG signals recorded using dry electrodes", "abstract": "In this paper, we demonstrate speech recognition using electroencephalography\n(EEG) signals obtained using dry electrodes on a limited English vocabulary\nconsisting of three vowels and one word using a deep learning model. We\ndemonstrate a test accuracy of 79.07 percent on a subset vocabulary consisting\nof two English vowels. Our results demonstrate the feasibility of using EEG\nsignals recorded using dry electrodes for performing the task of speech\nrecognition.", "published": "2020-08-13 09:56:45", "link": "http://arxiv.org/abs/2008.07621v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
