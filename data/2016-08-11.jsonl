{"title": "Sex, drugs, and violence", "abstract": "Automatically detecting inappropriate content can be a difficult NLP task,\nrequiring understanding context and innuendo, not just identifying specific\nkeywords. Due to the large quantity of online user-generated content, automatic\ndetection is becoming increasingly necessary. We take a largely unsupervised\napproach using a large corpus of narratives from a community-based\nself-publishing website and a small segment of crowd-sourced annotations. We\nexplore topic modelling using latent Dirichlet allocation (and a variation),\nand use these to regress appropriateness ratings, effectively automating rating\nfor suitability. The results suggest that certain topics inferred may be useful\nin detecting latent inappropriateness -- yielding recall up to 96% and low\nregression errors.", "published": "2016-08-11 13:10:02", "link": "http://arxiv.org/abs/1608.03448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WikiReading: A Novel Large-scale Language Understanding Task over\n  Wikipedia", "abstract": "We present WikiReading, a large-scale natural language understanding task and\npublicly-available dataset with 18 million instances. The task is to predict\ntextual values from the structured knowledge base Wikidata by reading the text\nof the corresponding Wikipedia articles. The task contains a rich variety of\nchallenging classification and extraction sub-tasks, making it well-suited for\nend-to-end models such as deep neural networks (DNNs). We compare various\nstate-of-the-art DNN-based architectures for document classification,\ninformation extraction, and question answering. We find that models supporting\na rich answer space, such as word or character sequences, perform best. Our\nbest-performing model, a word-level sequence to sequence model with a mechanism\nto copy out-of-vocabulary words, obtains an accuracy of 71.8%.", "published": "2016-08-11 17:34:12", "link": "http://arxiv.org/abs/1608.03542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The statistical trade-off between word order and word structure -\n  large-scale evidence for the principle of least effort", "abstract": "Languages employ different strategies to transmit structural and grammatical\ninformation. While, for example, grammatical dependency relationships in\nsentences are mainly conveyed by the ordering of the words for languages like\nMandarin Chinese, or Vietnamese, the word ordering is much less restricted for\nlanguages such as Inupiatun or Quechua, as those languages (also) use the\ninternal structure of words (e.g. inflectional morphology) to mark grammatical\nrelationships in a sentence. Based on a quantitative analysis of more than\n1,500 unique translations of different books of the Bible in more than 1,100\ndifferent languages that are spoken as a native language by approximately 6\nbillion people (more than 80% of the world population), we present large-scale\nevidence for a statistical trade-off between the amount of information conveyed\nby the ordering of words and the amount of information conveyed by internal\nword structure: languages that rely more strongly on word order information\ntend to rely less on word structure information and vice versa. In addition, we\nfind that - despite differences in the way information is expressed - there is\nalso evidence for a trade-off between different books of the biblical canon\nthat recurs with little variation across languages: the more informative the\nword order of the book, the less informative its word structure and vice versa.\nWe argue that this might suggest that, on the one hand, languages encode\ninformation in very different (but efficient) ways. On the other hand,\ncontent-related and stylistic features are statistically encoded in very\nsimilar ways.", "published": "2016-08-11 09:01:04", "link": "http://arxiv.org/abs/1608.03587v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
