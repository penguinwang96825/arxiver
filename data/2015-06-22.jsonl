{"title": "Aligning Books and Movies: Towards Story-like Visual Explanations by\n  Watching Movies and Reading Books", "abstract": "Books are a rich source of both fine-grained information, how a character, an\nobject or a scene looks like, as well as high-level semantics, what someone is\nthinking, feeling and how these states evolve through a story. This paper aims\nto align books to their movie releases in order to provide rich descriptive\nexplanations for visual content that go semantically far beyond the captions\navailable in current datasets. To align movies and books we exploit a neural\nsentence embedding that is trained in an unsupervised way from a large corpus\nof books, as well as a video-text neural embedding for computing similarities\nbetween movie clips and sentences in the book. We propose a context-aware CNN\nto combine information from multiple sources. We demonstrate good quantitative\nperformance for movie/book alignment and show several qualitative examples that\nshowcase the diversity of tasks our model can be used for.", "published": "2015-06-22 19:26:56", "link": "http://arxiv.org/abs/1506.06724v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Skip-Thought Vectors", "abstract": "We describe an approach for unsupervised learning of a generic, distributed\nsentence encoder. Using the continuity of text from books, we train an\nencoder-decoder model that tries to reconstruct the surrounding sentences of an\nencoded passage. Sentences that share semantic and syntactic properties are\nthus mapped to similar vector representations. We next introduce a simple\nvocabulary expansion method to encode words that were not seen as part of\ntraining, allowing us to expand our vocabulary to a million words. After\ntraining our model, we extract and evaluate our vectors with linear models on 8\ntasks: semantic relatedness, paraphrase detection, image-sentence ranking,\nquestion-type classification and 4 benchmark sentiment and subjectivity\ndatasets. The end result is an off-the-shelf encoder that can produce highly\ngeneric sentence representations that are robust and perform well in practice.\nWe will make our encoder publicly available.", "published": "2015-06-22 19:33:40", "link": "http://arxiv.org/abs/1506.06726v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning", "abstract": "We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence\nlearning, which performs the task through a series of nonlinear transformations\nfrom the representation of the input sequence (e.g., a Chinese sentence) to the\nfinal output sequence (e.g., translation to English). Inspired by the recently\nproposed Neural Turing Machine (Graves et al., 2014), we store the intermediate\nrepresentations in stacked layers of memories, and use read-write operations on\nthe memories to realize the nonlinear transformations between the\nrepresentations. The types of transformations are designed in advance but the\nparameters are learned from data. Through layer-by-layer transformations,\nDEEPMEMORY can model complicated relations between sequences necessary for\napplications such as machine translation between distant languages. The\narchitecture can be trained with normal back-propagation on sequenceto-sequence\ndata, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is\nbroad enough to subsume the state-of-the-art neural translation model in\n(Bahdanau et al., 2015) as its special case, while significantly improving upon\nthe model with its deeper architecture. Remarkably, DEEPMEMORY, being purely\nneural network-based, can achieve performance comparable to the traditional\nphrase-based machine translation system Moses with a small vocabulary and a\nmodest parameter size.", "published": "2015-06-22 02:12:54", "link": "http://arxiv.org/abs/1506.06442v4", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Answer Sequence Learning with Neural Networks for Answer Selection in\n  Community Question Answering", "abstract": "In this paper, the answer selection problem in community question answering\n(CQA) is regarded as an answer sequence labeling task, and a novel approach is\nproposed based on the recurrent architecture for this problem. Our approach\napplies convolution neural networks (CNNs) to learning the joint representation\nof question-answer pair firstly, and then uses the joint representation as\ninput of the long short-term memory (LSTM) to learn the answer sequence of a\nquestion for labeling the matching quality of each answer. Experiments\nconducted on the SemEval 2015 CQA dataset shows the effectiveness of our\napproach.", "published": "2015-06-22 07:26:51", "link": "http://arxiv.org/abs/1506.06490v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Nonparametric Bayesian Double Articulation Analyzer for Direct Language\n  Acquisition from Continuous Speech Signals", "abstract": "Human infants can discover words directly from unsegmented speech signals\nwithout any explicitly labeled data. In this paper, we develop a novel machine\nlearning method called nonparametric Bayesian double articulation analyzer\n(NPB-DAA) that can directly acquire language and acoustic models from observed\ncontinuous speech signals. For this purpose, we propose an integrative\ngenerative model that combines a language model and an acoustic model into a\nsingle generative model called the \"hierarchical Dirichlet process hidden\nlanguage model\" (HDP-HLM). The HDP-HLM is obtained by extending the\nhierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by\nJohnson et al. An inference procedure for the HDP-HLM is derived using the\nblocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure\nenables the simultaneous and direct inference of language and acoustic models\nfrom continuous speech signals. Based on the HDP-HLM and its inference\nprocedure, we developed a novel double articulation analyzer. By assuming\nHDP-HLM as a generative model of observed time series data, and by inferring\nlatent variables of the model, the method can analyze latent double\narticulation structure, i.e., hierarchically organized latent words and\nphonemes, of the data in an unsupervised manner. The novel unsupervised double\narticulation analyzer is called NPB-DAA.\n  The NPB-DAA can automatically estimate double articulation structure embedded\nin speech signals. We also carried out two evaluation experiments using\nsynthetic data and actual human continuous speech signals representing Japanese\nvowel sequences. In the word acquisition and phoneme categorization tasks, the\nNPB-DAA outperformed a conventional double articulation analyzer (DAA) and\nbaseline automatic speech recognition system whose acoustic model was trained\nin a supervised manner.", "published": "2015-06-22 15:21:57", "link": "http://arxiv.org/abs/1506.06646v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "A Neural Network Approach to Context-Sensitive Generation of\n  Conversational Responses", "abstract": "We present a novel response generation system that can be trained end to end\non large quantities of unstructured Twitter conversations. A neural network\narchitecture is used to address sparsity issues that arise when integrating\ncontextual information into classic statistical models, allowing the system to\ntake into account previous dialog utterances. Our dynamic-context generative\nmodels show consistent gains over both context-sensitive and\nnon-context-sensitive Machine Translation and Information Retrieval baselines.", "published": "2015-06-22 18:29:03", "link": "http://arxiv.org/abs/1506.06714v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Distributional Sentence Entailment Using Density Matrices", "abstract": "Categorical compositional distributional model of Coecke et al. (2010)\nsuggests a way to combine grammatical composition of the formal, type logical\nmodels with the corpus based, empirical word representations of distributional\nsemantics. This paper contributes to the project by expanding the model to also\ncapture entailment relations. This is achieved by extending the representations\nof words from points in meaning space to density operators, which are\nprobability distributions on the subspaces of the space. A symmetric measure of\nsimilarity and an asymmetric measure of entailment is defined, where lexical\nentailment is measured using von Neumann entropy, the quantum variant of\nKullback-Leibler divergence. Lexical entailment, combined with the composition\nmap on word representations, provides a method to obtain entailment relations\non the level of sentences. Truth theoretic and corpus-based examples are\nprovided.", "published": "2015-06-22 10:14:47", "link": "http://arxiv.org/abs/1506.06534v2", "categories": ["cs.CL", "cs.IT", "cs.LO", "math.CT", "math.IT"], "primary_category": "cs.CL"}
