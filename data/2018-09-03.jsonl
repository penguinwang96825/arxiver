{"title": "Data Augmentation for Neural Online Chat Response Selection", "abstract": "Data augmentation seeks to manipulate the available data for training to\nimprove the generalization ability of models. We investigate two data\naugmentation proxies, permutation and flipping, for neural dialog response\nselection task on various models over multiple datasets, including both Chinese\nand English languages. Different from standard data augmentation techniques,\nour method combines the original and synthesized data for prediction. Empirical\nresults show that our approach can gain 1 to 3 recall-at-1 points over baseline\nmodels in both full-scale and small-scale settings.", "published": "2018-09-03 02:12:56", "link": "http://arxiv.org/abs/1809.00428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Semi-supervised Learning for Cross-domain Sentiment\n  Classification", "abstract": "We consider the cross-domain sentiment classification problem, where a\nsentiment classifier is to be learned from a source domain and to be\ngeneralized to a target domain. Our approach explicitly minimizes the distance\nbetween the source and the target instances in an embedded feature space. With\nthe difference between source and target minimized, we then exploit additional\ninformation from the target domain by consolidating the idea of semi-supervised\nlearning, for which, we jointly employ two regularizations -- entropy\nminimization and self-ensemble bootstrapping -- to incorporate the unlabeled\ntarget data for classifier refinement. Our experimental results demonstrate\nthat the proposed approach can better leverage unlabeled data from the target\ndomain and achieve substantial improvements over baseline methods in various\nexperimental settings.", "published": "2018-09-03 10:15:04", "link": "http://arxiv.org/abs/1809.00530v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing Semantic Label Propagation in Relation Classification", "abstract": "Distant supervision is a popular method for performing relation extraction\nfrom text that is known to produce noisy labels. Most progress in relation\nextraction and classification has been made with crowdsourced corrections to\ndistant-supervised labels, and there is evidence that indicates still more\nwould be better. In this paper, we explore the problem of propagating human\nannotation signals gathered for open-domain relation classification through the\nCrowdTruth methodology for crowdsourcing, that captures ambiguity in\nannotations by measuring inter-annotator disagreement. Our approach propagates\nannotations to sentences that are similar in a low dimensional embedding space,\nexpanding the number of labels by two orders of magnitude. Our experiments show\nsignificant improvement in a sentence-level multi-class relation classifier.", "published": "2018-09-03 10:29:09", "link": "http://arxiv.org/abs/1809.00537v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Argument Mining for Discussion Threads Based on Parallel\n  Constrained Pointer Architecture", "abstract": "Argument Mining (AM) is a relatively recent discipline, which concentrates on\nextracting claims or premises from discourses, and inferring their structures.\nHowever, many existing works do not consider micro-level AM studies on\ndiscussion threads sufficiently. In this paper, we tackle AM for discussion\nthreads. Our main contributions are follows: (1) A novel combination scheme\nfocusing on micro-level inner- and inter- post schemes for a discussion thread.\n(2) Annotation of large-scale civic discussion threads with the scheme. (3)\nParallel constrained pointer architecture (PCPA), a novel end-to-end technique\nto discriminate sentence types, inner-post relations, and inter-post\ninteractions simultaneously. The experimental results demonstrate that our\nproposed model shows better accuracy in terms of relations extraction, in\ncomparison to existing state-of-the-art models.", "published": "2018-09-03 11:48:28", "link": "http://arxiv.org/abs/1809.00563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-to-Text Generation with Content Selection and Planning", "abstract": "Recent advances in data-to-text generation have led to the use of large-scale\ndatasets and neural network models which are trained end-to-end, without\nexplicitly modeling what to say and in what order. In this work, we present a\nneural network architecture which incorporates content selection and planning\nwithout sacrificing end-to-end training. We decompose the generation task into\ntwo stages. Given a corpus of data records (paired with descriptive documents),\nwe first generate a content plan highlighting which information should be\nmentioned and in which order and then generate the document while taking the\ncontent plan into account. Automatic and human-based evaluation experiments\nshow that our model outperforms strong baselines improving the state-of-the-art\non the recently released RotoWire dataset.", "published": "2018-09-03 12:41:44", "link": "http://arxiv.org/abs/1809.00582v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Affordance Extraction and Inference based on Semantic Role Labeling", "abstract": "Common-sense reasoning is becoming increasingly important for the advancement\nof Natural Language Processing. While word embeddings have been very\nsuccessful, they cannot explain which aspects of 'coffee' and 'tea' make them\nsimilar, or how they could be related to 'shop'. In this paper, we propose an\nexplicit word representation that builds upon the Distributional Hypothesis to\nrepresent meaning from semantic roles, and allow inference of relations from\ntheir meshing, as supported by the affordance-based Indexical Hypothesis. We\nfind that our model improves the state-of-the-art on unsupervised word\nsimilarity tasks while allowing for direct inference of new relations from the\nsame vector space.", "published": "2018-09-03 13:11:07", "link": "http://arxiv.org/abs/1809.00589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep learning for language understanding of mental health concepts\n  derived from Cognitive Behavioural Therapy", "abstract": "In recent years, we have seen deep learning and distributed representations\nof words and sentences make impact on a number of natural language processing\ntasks, such as similarity, entailment and sentiment analysis. Here we introduce\na new task: understanding of mental health concepts derived from Cognitive\nBehavioural Therapy (CBT). We define a mental health ontology based on the CBT\nprinciples, annotate a large corpus where this phenomena is exhibited and\nperform understanding using deep learning and distributed representations. Our\nresults show that the performance of deep learning models combined with word\nembeddings or sentence embeddings significantly outperform non-deep-learning\nmodels in this difficult task. This understanding module will be an essential\ncomponent of a statistical dialogue system delivering therapy.", "published": "2018-09-03 16:17:11", "link": "http://arxiv.org/abs/1809.00640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A3Net: Adversarial-and-Attention Network for Machine Reading\n  Comprehension", "abstract": "In this paper, we introduce Adversarial-and-attention Network (A3Net) for\nMachine Reading Comprehension. This model extends existing approaches from two\nperspectives. First, adversarial training is applied to several target\nvariables within the model, rather than only to the inputs or embeddings. We\ncontrol the norm of adversarial perturbations according to the norm of original\ntarget variables, so that we can jointly add perturbations to several target\nvariables during training. As an effective regularization method, adversarial\ntraining improves robustness and generalization of our model. Second, we\npropose a multi-layer attention network utilizing three kinds of\nhigh-efficiency attention mechanisms. Multi-layer attention conducts\ninteraction between question and passage within each layer, which contributes\nto reasonable representation and understanding of the model. Combining these\ntwo contributions, we enhance the diversity of dataset and the information\nextracting ability of the model at the same time. Meanwhile, we construct A3Net\nfor the WebQA dataset. Results show that our model outperforms the\nstate-of-the-art models (improving Fuzzy Score from 73.50% to 77.0%).", "published": "2018-09-03 18:03:46", "link": "http://arxiv.org/abs/1809.00676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Level Structured Self-Attentions for Distantly Supervised Relation\n  Extraction", "abstract": "Attention mechanisms are often used in deep neural networks for distantly\nsupervised relation extraction (DS-RE) to distinguish valid from noisy\ninstances. However, traditional 1-D vector attention models are insufficient\nfor the learning of different contexts in the selection of valid instances to\npredict the relationship for an entity pair. To alleviate this issue, we\npropose a novel multi-level structured (2-D matrix) self-attention mechanism\nfor DS-RE in a multi-instance learning (MIL) framework using bidirectional\nrecurrent neural networks. In the proposed method, a structured word-level\nself-attention mechanism learns a 2-D matrix where each row vector represents a\nweight distribution for different aspects of an instance regarding two\nentities. Targeting the MIL issue, the structured sentence-level attention\nlearns a 2-D matrix where each row vector represents a weight distribution on\nselection of different valid in-stances. Experiments conducted on two publicly\navailable DS-RE datasets show that the proposed framework with a multi-level\nstructured self-attention mechanism significantly outperform state-of-the-art\nbaselines in terms of PR curves, P@N and F1 measures.", "published": "2018-09-03 19:39:20", "link": "http://arxiv.org/abs/1809.00699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit\n  Emotion Classification", "abstract": "In this paper we present our approach to tackle the Implicit Emotion Shared\nTask (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from\nwhich a certain word has been removed, we are asked to predict the emotion of\nthe missing word. In this work, we experiment with neural Transfer Learning\n(TL) methods. Our models are based on LSTM networks, augmented with a\nself-attention mechanism. We use the weights of various pretrained models, for\ninitializing specific layers of our networks. We leverage a big collection of\nunlabeled Twitter messages, for pretraining word2vec word embeddings and a set\nof diverse language models. Moreover, we utilize a sentiment analysis dataset\nfor pretraining a model, which encodes emotion related information. The\nsubmitted model consists of an ensemble of the aforementioned TL models. Our\nteam ranked 3rd out of 30 participants, achieving an F1 score of 0.703.", "published": "2018-09-03 21:00:10", "link": "http://arxiv.org/abs/1809.00717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "emrQA: A Large Corpus for Question Answering on Electronic Medical\n  Records", "abstract": "We propose a novel methodology to generate domain-specific large-scale\nquestion answering (QA) datasets by re-purposing existing annotations for other\nNLP tasks. We demonstrate an instance of this methodology in generating a\nlarge-scale QA dataset for electronic medical records by leveraging existing\nexpert annotations on clinical notes for various NLP tasks from the community\nshared i2b2 datasets. The resulting corpus (emrQA) has 1 million\nquestion-logical form and 400,000+ question-answer evidence pairs. We\ncharacterize the dataset and explore its learning potential by training\nbaseline models for question to logical form and question to answer mapping.", "published": "2018-09-03 21:56:47", "link": "http://arxiv.org/abs/1809.00732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Clustering of Streaming News", "abstract": "Clustering news across languages enables efficient media monitoring by\naggregating articles from multilingual sources into coherent stories. Doing so\nin an online setting allows scalable processing of massive news streams. To\nthis end, we describe a novel method for clustering an incoming stream of\nmultilingual documents into monolingual and crosslingual story clusters. Unlike\ntypical clustering approaches that consider a small and known number of labels,\nwe tackle the problem of discovering an ever growing number of cluster labels\nin an online fashion, using real news datasets in multiple languages. Our\nmethod is simple to implement, computationally efficient and produces\nstate-of-the-art results on datasets in German, English and Spanish.", "published": "2018-09-03 10:40:26", "link": "http://arxiv.org/abs/1809.00540v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Emergence of Communication in an Interactive World with Consistent\n  Speakers", "abstract": "Training agents to communicate with one another given task-based supervision\nonly has attracted considerable attention recently, due to the growing interest\nin developing models for human-agent interaction. Prior work on the topic\nfocused on simple environments, where training using policy gradient was\nfeasible despite the non-stationarity of the agents during training. In this\npaper, we present a more challenging environment for testing the emergence of\ncommunication from raw pixels, where training using policy gradient fails. We\npropose a new model and training algorithm, that utilizes the structure of a\nlearned representation space to produce more consistent speakers at the initial\nphases of training, which stabilizes learning. We empirically show that our\nalgorithm substantially improves performance compared to policy gradient. We\nalso propose a new alignment-based metric for measuring context-independence in\nemerged communication and find our method increases context-independence\ncompared to policy gradient and other competitive baselines.", "published": "2018-09-03 11:05:00", "link": "http://arxiv.org/abs/1809.00549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Event Salience Identification", "abstract": "Identifying the salience (i.e. importance) of discourse units is an important\ntask in language understanding. While events play important roles in text\ndocuments, little research exists on analyzing their saliency status. This\npaper empirically studies the Event Salience task and proposes two salience\ndetection models based on content similarities and discourse relations. The\nfirst is a feature based salience model that incorporates similarities among\ndiscourse units. The second is a neural model that captures more complex\nrelations between discourse units. Tested on our new large-scale event salience\ncorpus, both methods significantly outperform the strong frequency baseline,\nwhile our neural model further improves the feature based one by a large\nmargin. Our analyses demonstrate that our neural model captures interesting\nconnections between salience and discourse unit relations (e.g., scripts and\nframe structures).", "published": "2018-09-03 16:35:07", "link": "http://arxiv.org/abs/1809.00647v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hypernyms Through Intra-Article Organization in Wikipedia", "abstract": "We introduce a new measure for unsupervised hypernym detection and\ndirectionality. The motivation is to keep the measure computationally light and\nportatable across languages. We show that the relative physical location of\nwords in explanatory articles captures the directionality property. Further,\nthe phrases in section titles of articles about the word, capture the semantic\nsimilarity needed for hypernym detection task. We experimentally show that the\ncombination of features coming from these two simple measures suffices to\nproduce results comparable with the best unsupervised measures in terms of the\naverage precision.", "published": "2018-09-03 00:04:49", "link": "http://arxiv.org/abs/1809.00414v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Belittling the Source: Trustworthiness Indicators to Obfuscate Fake News\n  on the Web", "abstract": "With the growth of the internet, the number of fake-news online has been\nproliferating every year. The consequences of such phenomena are manifold,\nranging from lousy decision-making process to bullying and violence episodes.\nTherefore, fact-checking algorithms became a valuable asset. To this aim, an\nimportant step to detect fake-news is to have access to a credibility score for\na given information source. However, most of the widely used Web indicators\nhave either been shut-down to the public (e.g., Google PageRank) or are not\nfree for use (Alexa Rank). Further existing databases are short-manually\ncurated lists of online sources, which do not scale. Finally, most of the\nresearch on the topic is theoretical-based or explore confidential data in a\nrestricted simulation environment. In this paper we explore current research,\nhighlight the challenges and propose solutions to tackle the problem of\nclassifying websites into a credibility scale. The proposed model automatically\nextracts source reputation cues and computes a credibility factor, providing\nvaluable insights which can help in belittling dubious and confirming trustful\nunknown websites. Experimental results outperform state of the art in the\n2-classes and 5-classes setting.", "published": "2018-09-03 08:37:33", "link": "http://arxiv.org/abs/1809.00494v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "DeFactoNLP: Fact Verification using Entity Recognition, TFIDF Vector\n  Comparison and Decomposable Attention", "abstract": "In this paper, we describe DeFactoNLP, the system we designed for the FEVER\n2018 Shared Task. The aim of this task was to conceive a system that can not\nonly automatically assess the veracity of a claim but also retrieve evidence\nsupporting this assessment from Wikipedia. In our approach, the Wikipedia\ndocuments whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are\nmost similar to the vector of the claim and those documents whose names are\nsimilar to those of the named entities (NEs) mentioned in the claim are\nidentified as the documents which might contain evidence. The sentences in\nthese documents are then supplied to a textual entailment recognition module.\nThis module calculates the probability of each sentence supporting the claim,\ncontradicting the claim or not providing any relevant information to assess the\nveracity of the claim. Various features computed using these probabilities are\nfinally used by a Random Forest classifier to determine the overall\ntruthfulness of the claim. The sentences which support this classification are\nreturned as evidence. Our approach achieved a 0.4277 evidence F1-score, a\n0.5136 label accuracy and a 0.3833 FEVER score.", "published": "2018-09-03 09:07:17", "link": "http://arxiv.org/abs/1809.00509v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Towards Dynamic Computation Graphs via Sparse Latent Structure", "abstract": "Deep NLP models benefit from underlying structures in the data---e.g., parse\ntrees---typically extracted using off-the-shelf parsers. Recent attempts to\njointly learn the latent structure encounter a tradeoff: either make\nfactorization assumptions that limit expressiveness, or sacrifice end-to-end\ndifferentiability. Using the recently proposed SparseMAP inference, which\nretrieves a sparse distribution over latent structures, we propose a novel\napproach for end-to-end learning of latent structure predictors jointly with a\ndownstream predictor. To the best of our knowledge, our method is the first to\nenable unrestricted dynamic computation graph construction from the global\nlatent structure, while maintaining differentiability.", "published": "2018-09-03 16:52:19", "link": "http://arxiv.org/abs/1809.00653v1", "categories": ["cs.CL", "cs.LG", "stat.ML", "68T50", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring the Landscape of Relational Syllogistic Logics", "abstract": "This paper explores relational syllogistic logics, a family of logical\nsystems related to reasoning about relations in extensions of the classical\nsyllogistic. These are all decidable logical systems. We prove completeness\ntheorems and complexity results for a natural subfamily of relational\nsyllogistic logics, parametrized by constructors for terms and for sentences.", "published": "2018-09-03 16:57:54", "link": "http://arxiv.org/abs/1809.00656v2", "categories": ["math.LO", "cs.CL", "cs.LO", "03B20, 03B65"], "primary_category": "math.LO"}
{"title": "\"Read My Lips\": Using Automatic Text Analysis to Classify Politicians by\n  Party and Ideology", "abstract": "The increasing digitization of political speech has opened the door to\nstudying a new dimension of political behavior using text analysis. This work\ninvestigates the value of word-level statistical data from the US Congressional\nRecord--which contains the full text of all speeches made in the US\nCongress--for studying the ideological positions and behavior of senators.\nApplying machine learning techniques, we use this data to automatically\nclassify senators according to party, obtaining accuracy in the 70-95% range\ndepending on the specific method used. We also show that using text to predict\nDW-NOMINATE scores, a common proxy for ideology, does not improve upon these\nalready-successful results. This classification deteriorates when applied to\ntext from sessions of Congress that are four or more years removed from the\ntraining set, pointing to a need on the part of voters to dynamically update\nthe heuristics they use to evaluate party based on political speech. Text-based\npredictions are less accurate than those based on voting behavior, supporting\nthe theory that roll-call votes represent greater commitment on the part of\npoliticians and are thus a more accurate reflection of their ideological\npreferences. However, the overall success of the machine learning approaches\nstudied here demonstrates that political speeches are highly predictive of\npartisan affiliation. In addition to these findings, this work also introduces\nthe computational tools and methods relevant to the use of political speech\ndata.", "published": "2018-09-03 23:13:00", "link": "http://arxiv.org/abs/1809.00741v1", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification", "abstract": "The Fact Extraction and VERification (FEVER) shared task was launched to\nsupport the development of systems able to verify claims by extracting\nsupporting or refuting facts from raw text. The shared task organizers provide\na large-scale dataset for the consecutive steps involved in claim verification,\nin particular, document retrieval, fact extraction, and claim classification.\nIn this paper, we present our claim verification pipeline approach, which,\naccording to the preliminary results, scored third in the shared task, out of\n23 competing systems. For the document retrieval, we implemented a new entity\nlinking approach. In order to be able to rank candidate facts and classify a\nclaim on the basis of several selected facts, we introduce two extensions to\nthe Enhanced LSTM (ESIM).", "published": "2018-09-03 14:06:11", "link": "http://arxiv.org/abs/1809.01479v5", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Deep Room Recognition Using Inaudible Echos", "abstract": "Recent years have seen the increasing need of location awareness by mobile\napplications. This paper presents a room-level indoor localization approach\nbased on the measured room's echos in response to a two-millisecond single-tone\ninaudible chirp emitted by a smartphone's loudspeaker. Different from other\nacoustics-based room recognition systems that record full-spectrum audio for up\nto ten seconds, our approach records audio in a narrow inaudible band for 0.1\nseconds only to preserve the user's privacy. However, the short-time and\nnarrowband audio signal carries limited information about the room's\ncharacteristics, presenting challenges to accurate room recognition. This paper\napplies deep learning to effectively capture the subtle fingerprints in the\nrooms' acoustic responses. Our extensive experiments show that a two-layer\nconvolutional neural network fed with the spectrogram of the inaudible echos\nachieve the best performance, compared with alternative designs using other raw\ndata formats and deep models. Based on this result, we design a RoomRecognize\ncloud service and its mobile client library that enable the mobile application\ndevelopers to readily implement the room recognition functionality without\nresorting to any existing infrastructures and add-on hardware.\n  Extensive evaluation shows that RoomRecognize achieves 99.7%, 97.7%, 99%, and\n89% accuracy in differentiating 22 and 50 residential/office rooms, 19 spots in\na quiet museum, and 15 spots in a crowded museum, respectively. Compared with\nthe state-of-the-art approaches based on support vector machine, RoomRecognize\nsignificantly improves the Pareto frontier of recognition accuracy versus\nrobustness against interfering sounds (e.g., ambient music).", "published": "2018-09-03 10:17:14", "link": "http://arxiv.org/abs/1809.00531v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Learning of Human Perception in Audio Event Classification", "abstract": "In this paper, we introduce our recent studies on human perception in audio\nevent classification by different deep learning models. In particular, the\npre-trained model VGGish is used as feature extractor to process audio data,\nand DenseNet is trained by and used as feature extractor for our\nelectroencephalography (EEG) data. The correlation between audio stimuli and\nEEG is learned in a shared space. In the experiments, we record brain\nactivities (EEG signals) of several subjects while they are listening to music\nevents of 8 audio categories selected from Google AudioSet, using a 16-channel\nEEG headset with active electrodes. Our experimental results demonstrate that\ni) audio event classification can be improved by exploiting the power of human\nperception, and ii) the correlation between audio stimuli and EEG can be\nlearned to complement audio event understanding.", "published": "2018-09-03 08:47:32", "link": "http://arxiv.org/abs/1809.00502v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Three-Stage Speaker Verification Architecture in Emotional Talking\n  Environments", "abstract": "Speaker verification performance in neutral talking environment is usually\nhigh, while it is sharply decreased in emotional talking environments. This\nperformance degradation in emotional environments is due to the problem of\nmismatch between training in neutral environment while testing in emotional\nenvironments. In this work, a three-stage speaker verification architecture has\nbeen proposed to enhance speaker verification performance in emotional\nenvironments. This architecture is comprised of three cascaded stages: gender\nidentification stage followed by an emotion identification stage followed by a\nspeaker verification stage. The proposed framework has been evaluated on two\ndistinct and independent emotional speech datasets: in-house dataset and\nEmotional Prosody Speech and Transcripts dataset. Our results show that speaker\nverification based on both gender information and emotion information is\nsuperior to each of speaker verification based on gender information only,\nemotion information only, and neither gender information nor emotion\ninformation. The attained average speaker verification performance based on the\nproposed framework is very alike to that attained in subjective assessment by\nhuman listeners.", "published": "2018-09-03 09:25:35", "link": "http://arxiv.org/abs/1809.01721v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
