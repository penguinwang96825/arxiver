{"title": "Structured Interpretation of Temporal Relations", "abstract": "Temporal relations between events and time expressions in a document are\noften modeled in an unstructured manner where relations between individual\npairs of time expressions and events are considered in isolation. This often\nresults in inconsistent and incomplete annotation and computational modeling.\nWe propose a novel annotation approach where events and time expressions in a\ndocument form a dependency tree in which each dependency relation corresponds\nto an instance of temporal anaphora where the antecedent is the parent and the\nanaphor is the child. We annotate a corpus of 235 documents using this approach\nin the two genres of news and narratives, with 48 documents doubly annotated.\nWe report a stable and high inter-annotator agreement on the doubly annotated\nsubset, validating our approach, and perform a quantitative comparison between\nthe two genres of the entire corpus. We make this corpus publicly available.", "published": "2018-08-23 00:37:58", "link": "http://arxiv.org/abs/1808.07599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Review-Driven Multi-Label Music Style Classification by Exploiting Style\n  Correlations", "abstract": "This paper explores a new natural language processing task, review-driven\nmulti-label music style classification. This task requires the system to\nidentify multiple styles of music based on its reviews on websites. The biggest\nchallenge lies in the complicated relations of music styles. It has brought\nfailure to many multi-label classification methods. To tackle this problem, we\npropose a novel deep learning approach to automatically learn and exploit style\ncorrelations. The proposed method consists of two parts: a label-graph based\nneural network, and a soft training mechanism with correlation-based continuous\nlabel representation. Experimental results show that our approach achieves\nlarge improvements over the baselines on the proposed dataset. Especially, the\nmicro F1 is improved from 53.9 to 64.5, and the one-error is reduced from 30.5\nto 22.6. Furthermore, the visualized analysis shows that our approach performs\nwell in capturing style correlations.", "published": "2018-08-23 02:02:32", "link": "http://arxiv.org/abs/1808.07604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly-supervised Neural Semantic Parsing with a Generative Ranker", "abstract": "Weakly-supervised semantic parsers are trained on utterance-denotation pairs,\ntreating logical forms as latent. The task is challenging due to the large\nsearch space and spuriousness of logical forms. In this paper we introduce a\nneural parser-ranker system for weakly-supervised semantic parsing. The parser\ngenerates candidate tree-structured logical forms from utterances using clues\nof denotations. These candidates are then ranked based on two criterion: their\nlikelihood of executing to the correct denotation, and their agreement with the\nutterance semantics. We present a scheduled training procedure to balance the\ncontribution of the two objectives. Furthermore, we propose to use a neurally\nencoded lexicon to inject prior domain knowledge to the model. Experiments on\nthree Freebase datasets demonstrate the effectiveness of our semantic parser,\nachieving results within the state-of-the-art range.", "published": "2018-08-23 04:03:58", "link": "http://arxiv.org/abs/1808.07625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-Guided Answer Distillation for Machine Reading Comprehension", "abstract": "Despite that current reading comprehension systems have achieved significant\nadvancements, their promising performances are often obtained at the cost of\nmaking an ensemble of numerous models. Besides, existing approaches are also\nvulnerable to adversarial attacks. This paper tackles these problems by\nleveraging knowledge distillation, which aims to transfer knowledge from an\nensemble model to a single model. We first demonstrate that vanilla knowledge\ndistillation applied to answer span prediction is effective for reading\ncomprehension systems. We then propose two novel approaches that not only\npenalize the prediction on confusing answers but also guide the training with\nalignment information distilled from the ensemble. Experiments show that our\nbest student model has only a slight drop of 0.4% F1 on the SQuAD test set\ncompared to the ensemble teacher, while running 12x faster during inference. It\neven outperforms the teacher on adversarial SQuAD datasets and NarrativeQA\nbenchmark.", "published": "2018-08-23 06:27:58", "link": "http://arxiv.org/abs/1808.07644v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arap-Tweet: A Large Multi-Dialect Twitter Corpus for Gender, Age and\n  Language Variety Identification", "abstract": "In this paper, we present Arap-Tweet, which is a large-scale and\nmulti-dialectal corpus of Tweets from 11 regions and 16 countries in the Arab\nworld representing the major Arabic dialectal varieties. To build this corpus,\nwe collected data from Twitter and we provided a team of experienced annotators\nwith annotation guidelines that they used to annotate the corpus for age\ncategories, gender, and dialectal variety. During the data collection effort,\nwe based our search on distinctive keywords that are specific to the different\nArabic dialects and we also validated the location using Twitter API. In this\npaper, we report on the corpus data collection and annotation efforts. We also\npresent some issues that we encountered during these phases. Then, we present\nthe results of the evaluation performed to ensure the consistency of the\nannotation. The provided corpus will enrich the limited set of available\nlanguage resources for Arabic and will be an invaluable enabler for developing\nauthor profiling tools and NLP tools for Arabic.", "published": "2018-08-23 09:41:17", "link": "http://arxiv.org/abs/1808.07674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guidelines and Annotation Framework for Arabic Author Profiling", "abstract": "In this paper, we present the annotation pipeline and the guidelines we wrote\nas part of an effort to create a large manually annotated Arabic author\nprofiling dataset from various social media sources covering 16 Arabic\ncountries and 11 dialectal regions. The target size of the annotated ARAP-Tweet\ncorpus is more than 2.4 million words. We illustrate and summarize our general\nand dialect-specific guidelines for each of the dialectal regions selected. We\nalso present the annotation framework and logistics. We control the annotation\nquality frequently by computing the inter-annotator agreement during the\nannotation process. Finally, we describe the issues encountered during the\nannotation phase, especially those related to the peculiarities of Arabic\ndialectal varieties as used in social media.", "published": "2018-08-23 09:52:26", "link": "http://arxiv.org/abs/1808.07678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Role of Intonation in Scoring Spoken English", "abstract": "In this paper, we have introduced and evaluated intonation based feature for\nscoring the English speech of nonnative English speakers in Indian context. For\nthis, we created an automated spoken English scoring engine to learn from the\nmanual evaluation of spoken English. This involved using an existing Automatic\nSpeech Recognition (ASR) engine to convert the speech to text. Thereafter,\nmacro features like accuracy, fluency and prosodic features were used to build\na scoring model. In the process, we introduced SimIntonation, short for\nsimilarity between spoken intonation pattern and \"ideal\" i.e. training\nintonation pattern. Our results show that it is a highly predictive feature\nunder controlled environment. We also categorized interword pauses into 4\ndistinct types for a granular evaluation of pauses and their impact on speech\nevaluation. Moreover, we took steps to moderate test difficulty through its\nevaluation across parameters like difficult word count, average sentence\nreadability and lexical density. Our results show that macro features like\naccuracy and intonation, and micro features like pause-topography are strongly\npredictive. The scoring of spoken English is not within the purview of this\npaper.", "published": "2018-08-23 10:13:52", "link": "http://arxiv.org/abs/1808.07688v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Neural Entity Linking", "abstract": "Entity Linking (EL) is an essential task for semantic text understanding and\ninformation extraction. Popular methods separately address the Mention\nDetection (MD) and Entity Disambiguation (ED) stages of EL, without leveraging\ntheir mutual dependency. We here propose the first neural end-to-end EL system\nthat jointly discovers and links entities in a text document. The main idea is\nto consider all possible spans as potential mentions and learn contextual\nsimilarity scores over their entity candidates that are useful for both MD and\nED decisions. Key components are context-aware mention embeddings, entity\nembeddings and a probabilistic mention - entity map, without demanding other\nengineered features. Empirically, we show that our end-to-end method\nsignificantly outperforms popular systems on the Gerbil platform when enough\ntraining data is available. Conversely, if testing datasets follow different\nannotation conventions compared to the training set (e.g. queries/ tweets vs\nnews documents), our ED model coupled with a traditional NER system offers the\nbest or second best EL accuracy.", "published": "2018-08-23 11:16:57", "link": "http://arxiv.org/abs/1808.07699v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Importance of Encoding Logic Rules in Sentiment\n  Classification", "abstract": "We analyze the performance of different sentiment classification models on\nsyntactically complex inputs like A-but-B sentences. The first contribution of\nthis analysis addresses reproducible research: to meaningfully compare\ndifferent models, their accuracies must be averaged over far more random seeds\nthan what has traditionally been reported. With proper averaging in place, we\nnotice that the distillation model described in arXiv:1603.06318v4 [cs.LG],\nwhich incorporates explicit logic rules for sentiment classification, is\nineffective. In contrast, using contextualized ELMo embeddings\n(arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better\nperformance. Additionally, we provide analysis and visualizations that\ndemonstrate ELMo's ability to implicitly learn logic rules. Finally, a\ncrowdsourced analysis reveals how ELMo outperforms baseline models even on\nsentences with ambiguous sentiment labels.", "published": "2018-08-23 13:03:55", "link": "http://arxiv.org/abs/1808.07733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Index of the Russian Speaking Facebook", "abstract": "A sentiment index measures the average emotional level in a corpus. We\nintroduce four such indexes and use them to gauge average \"positiveness\" of a\npopulation during some period based on posts in a social network. This article\nfor the first time presents a text-, rather than word-based sentiment index.\nFurthermore, this study presents the first large-scale study of the sentiment\nindex of the Russian-speaking Facebook. Our results are consistent with the\nprior experiments for the English language.", "published": "2018-08-23 17:24:42", "link": "http://arxiv.org/abs/1808.07851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Transfer as Unsupervised Machine Translation", "abstract": "Language style transferring rephrases text with specific stylistic attributes\nwhile preserving the original attribute-independent content. One main challenge\nin learning a style transfer system is a lack of parallel data where the source\nsentence is in one style and the target sentence in another style. With this\nconstraint, in this paper, we adapt unsupervised machine translation methods\nfor the task of automatic style transfer. We first take advantage of\nstyle-preference information and word embedding similarity to produce\npseudo-parallel data with a statistical machine translation (SMT) framework.\nThen the iterative back-translation approach is employed to jointly train two\nneural machine translation (NMT) based transfer systems. To control the noise\ngenerated during joint training, a style classifier is introduced to guarantee\nthe accuracy of style transfer and penalize bad candidates in the generated\npseudo data. Experiments on benchmark datasets show that our proposed method\noutperforms previous state-of-the-art models in terms of both accuracy of style\ntransfer and quality of input-output correspondence.", "published": "2018-08-23 18:18:32", "link": "http://arxiv.org/abs/1808.07894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Abstraction in Text Summarization", "abstract": "Abstractive text summarization aims to shorten long text documents into a\nhuman readable form that contains the most important facts from the original\ndocument. However, the level of actual abstraction as measured by novel phrases\nthat do not appear in the source document remains low in existing approaches.\nWe propose two techniques to improve the level of abstraction of generated\nsummaries. First, we decompose the decoder into a contextual network that\nretrieves relevant parts of the source document, and a pretrained language\nmodel that incorporates prior knowledge about language generation. Second, we\npropose a novelty metric that is optimized directly through policy learning to\nencourage the generation of novel phrases. Our model achieves results\ncomparable to state-of-the-art models, as determined by ROUGE scores and human\nevaluations, while achieving a significantly higher level of abstraction as\nmeasured by n-gram overlap with the source document.", "published": "2018-08-23 19:19:27", "link": "http://arxiv.org/abs/1808.07913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Financial Aspect-Based Sentiment Analysis using Deep Representations", "abstract": "The topic of aspect-based sentiment analysis (ABSA) has been explored for a\nvariety of industries, but it still remains much unexplored in finance. The\nrecent release of data for an open challenge (FiQA) from the companion\nproceedings of WWW '18 has provided valuable finance-specific annotations. FiQA\ncontains high quality labels, but it still lacks data quantity to apply\ntraditional ABSA deep learning architecture. In this paper, we employ\nhigh-level semantic representations and methods of inductive transfer learning\nfor NLP. We experiment with extensions of recently developed domain adaptation\nmethods and target task fine-tuning that significantly improve performance on a\nsmall dataset. Our results show an 8.7% improvement in the F1 score for\nclassification and an 11% improvement over the MSE for regression on current\nstate-of-the-art results.", "published": "2018-08-23 20:23:36", "link": "http://arxiv.org/abs/1808.07931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Rich Syntactic Information for Semantic Parsing with\n  Graph-to-Sequence Model", "abstract": "Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a\nsequential LSTM, to extract word order features while neglecting other valuable\nsyntactic information such as dependency graph or constituent trees. In this\npaper, we first propose to use the \\textit{syntactic graph} to represent three\ntypes of syntactic information, i.e., word order, dependency and constituency\nfeatures. We further employ a graph-to-sequence model to encode the syntactic\ngraph and decode a logical form. Experimental results on benchmark datasets\nshow that our model is comparable to the state-of-the-art on Jobs640, ATIS and\nGeo880. Experimental results on adversarial examples demonstrate the robustness\nof the model is also improved by encoding more syntactic information.", "published": "2018-08-23 03:58:21", "link": "http://arxiv.org/abs/1808.07624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Shared Structures and Hierarchies for Multiple NLP Tasks", "abstract": "Designing shared neural architecture plays an important role in multi-task\nlearning. The challenge is that finding an optimal sharing scheme heavily\nrelies on the expert knowledge and is not scalable to a large number of diverse\ntasks. Inspired by the promising work of neural architecture search (NAS), we\napply reinforcement learning to automatically find possible shared architecture\nfor multi-task learning. Specifically, we use a controller to select from a set\nof shareable modules and assemble a task-specific architecture, and repeat the\nsame procedure for other tasks. The controller is trained with reinforcement\nlearning to maximize the expected accuracies for all tasks. We conduct\nextensive experiments on two types of tasks, text classification and sequence\nlabeling, which demonstrate the benefits of our approach.", "published": "2018-08-23 08:07:44", "link": "http://arxiv.org/abs/1808.07658v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Playing 20 Question Game with Policy-Based Reinforcement Learning", "abstract": "The 20 Questions (Q20) game is a well known game which encourages deductive\nreasoning and creativity. In the game, the answerer first thinks of an object\nsuch as a famous person or a kind of animal. Then the questioner tries to guess\nthe object by asking 20 questions. In a Q20 game system, the user is considered\nas the answerer while the system itself acts as the questioner which requires a\ngood strategy of question selection to figure out the correct object and win\nthe game. However, the optimal policy of question selection is hard to be\nderived due to the complexity and volatility of the game environment. In this\npaper, we propose a novel policy-based Reinforcement Learning (RL) method,\nwhich enables the questioner agent to learn the optimal policy of question\nselection through continuous interactions with users. To facilitate training,\nwe also propose to use a reward network to estimate the more informative\nreward. Compared to previous methods, our RL method is robust to noisy answers\nand does not rely on the Knowledge Base of objects. Experimental results show\nthat our RL method clearly outperforms an entropy-based engineering system and\nhas competitive performance in a noisy-free simulation environment.", "published": "2018-08-23 06:34:32", "link": "http://arxiv.org/abs/1808.07645v3", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs", "abstract": "This paper addresses the problem of mapping natural language text to\nknowledge base entities. The mapping process is approached as a composition of\na phrase or a sentence into a point in a multi-dimensional entity space\nobtained from a knowledge graph. The compositional model is an LSTM equipped\nwith a dynamic disambiguation mechanism on the input word embeddings (a\nMulti-Sense LSTM), addressing polysemy issues. Further, the knowledge base\nspace is prepared by collecting random walks from a graph enhanced with textual\nfeatures, which act as a set of semantic bridges between text and knowledge\nbase entities. The ideas of this work are demonstrated on large-scale\ntext-to-entity mapping and entity classification tasks, with state of the art\nresults.", "published": "2018-08-23 12:47:01", "link": "http://arxiv.org/abs/1808.07724v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Importance of Generation Order in Language Modeling", "abstract": "Neural language models are a critical component of state-of-the-art systems\nfor machine translation, summarization, audio transcription, and other tasks.\nThese language models are almost universally autoregressive in nature,\ngenerating sentences one token at a time from left to right. This paper studies\nthe influence of token generation order on model quality via a novel two-pass\nlanguage model that produces partially-filled sentence \"templates\" and then\nfills in missing tokens. We compare various strategies for structuring these\ntwo passes and observe a surprisingly large variation in model quality. We find\nthe most effective strategy generates function words in the first pass followed\nby content words in the second. We believe these experimental results justify a\nmore extensive investigation of generation order for neural language models.", "published": "2018-08-23 19:17:24", "link": "http://arxiv.org/abs/1808.07910v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Discussion of Parameters Setting for A Distributed Probabilistic\n  Modeling Algorithm", "abstract": "This manuscript provides additional case analysis for the parameters setting\nof the distributed probabilistic modeling algorithm for the aggregated wind\npower forecast error.", "published": "2018-08-23 03:38:22", "link": "http://arxiv.org/abs/1808.07620v2", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
