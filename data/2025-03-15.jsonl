{"title": "The Lucie-7B LLM and the Lucie Training Dataset: Open resources for multilingual language generation", "abstract": "We present both the Lucie Training Dataset and the Lucie-7B foundation model.\nThe Lucie Training Dataset is a multilingual collection of textual corpora\ncentered around French and designed to offset anglo-centric biases found in\nmany datasets for large language model pretraining. Its French data is pulled\nnot only from traditional web sources, but also from French cultural heritage\ndocuments, filling an important gap in modern datasets. Beyond French, which\nmakes up the largest share of the data, we added documents to support several\nother European languages, including English, Spanish, German, and Italian.\nApart from its value as a resource for French language and culture, an\nimportant feature of this dataset is that it prioritizes data rights by\nminimizing copyrighted material. In addition, building on the philosophy of\npast open projects, it is redistributed in the form used for training and its\nprocessing is described on Hugging Face and GitHub. The Lucie-7B foundation\nmodel is trained on equal amounts of data in French and English -- roughly 33%\neach -- in an effort to better represent cultural aspects of French-speaking\ncommunities. We also describe two instruction fine-tuned models,\nLucie-7B-Instruct-v1.1 and Lucie-7B-Instruct-human-data, which we release as\ndemonstrations of Lucie-7B in use. These models achieve promising results\ncompared to state-of-the-art models, demonstrating that an open approach\nprioritizing data rights can still deliver strong performance. We see these\nmodels as an initial step toward developing more performant, aligned models in\nthe near future. Model weights for Lucie-7B and the Lucie instruct models,\nalong with intermediate checkpoints for the former, are published on Hugging\nFace, while model training and data preparation code is available on GitHub.\nThis makes Lucie-7B one of the first OSI compliant language models according to\nthe new OSI definition.", "published": "2025-03-15 23:20:45", "link": "http://arxiv.org/abs/2503.12294v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes", "abstract": "Background: Several studies show that large language models (LLMs) struggle\nwith phenotype-driven gene prioritization for rare diseases. These studies\ntypically use Human Phenotype Ontology (HPO) terms to prompt foundation models\nlike GPT and LLaMA to predict candidate genes. However, in real-world settings,\nfoundation models are not optimized for domain-specific tasks like clinical\ndiagnosis, yet inputs are unstructured clinical notes rather than standardized\nterms. How LLMs can be instructed to predict candidate genes or disease\ndiagnosis from unstructured clinical notes remains a major challenge. Methods:\nWe introduce RAG-driven CoT and CoT-driven RAG, two methods that combine\nChain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze\nclinical notes. A five-question CoT protocol mimics expert reasoning, while RAG\nretrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in\nMan). We evaluated these approaches on rare disease datasets, including 5,980\nPhenopacket-derived notes, 255 literature-based narratives, and 220 in-house\nclinical notes from Childrens Hospital of Philadelphia. Results: We found that\nrecent foundations models, including Llama 3.3-70B-Instruct and\nDeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2\nand GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both\noutperform foundation models in candidate gene prioritization from clinical\nnotes; in particular, both methods with DeepSeek backbone resulted in a top-10\ngene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT\nworks better for high-quality notes, where early retrieval can anchor the\nsubsequent reasoning steps in domain-specific evidence, while CoT-driven RAG\nhas advantage when processing lengthy and noisy notes.", "published": "2025-03-15 22:57:31", "link": "http://arxiv.org/abs/2503.12286v1", "categories": ["cs.CL", "cs.AI", "q-bio.GN", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents", "abstract": "This article explores the gaps that can manifest when using a large language\nmodel (LLM) to obtain simplified interpretations of data practices from a\ncomplex privacy policy. We exemplify these gaps to showcase issues in accuracy,\ncompleteness, clarity and representation, while advocating for continued\nresearch to realize an LLM's true potential in revolutionizing privacy\nmanagement through personal assistants and automated compliance checking.", "published": "2025-03-15 18:43:13", "link": "http://arxiv.org/abs/2503.12225v1", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare SMEs", "abstract": "This study examines AI adoption among Finnish healthcare SMEs through\nsemi-structured interviews with six health-tech companies. We identify three AI\nengagement categories: AI-curious (exploring AI), AI-embracing (integrating\nAI), and AI-catering (providing AI solutions). Our proposed threefold model\nhighlights key adoption barriers, including regulatory complexities, technical\nexpertise gaps, and financial constraints. While SMEs recognize AI's potential,\nmost remain in early adoption stages. We provide actionable recommendations to\naccelerate AI integration, focusing on regulatory reforms, talent development,\nand inter-company collaboration, offering valuable insights for healthcare\norganizations, policymakers, and researchers.", "published": "2025-03-15 18:37:29", "link": "http://arxiv.org/abs/2503.14527v1", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "Medifact at PerAnsSumm 2025: Leveraging Lightweight Models for Perspective-Specific Summarization of Clinical Q&A Forums", "abstract": "The PerAnsSumm 2025 challenge focuses on perspective-aware healthcare answer\nsummarization (Agarwal et al., 2025). This work proposes a few-shot learning\nframework using a Snorkel-BART-SVM pipeline for classifying and summarizing\nopen-ended healthcare community question-answering (CQA). An SVM model is\ntrained with weak supervision via Snorkel, enhancing zero-shot learning.\nExtractive classification identifies perspective-relevant sentences, which are\nthen summarized using a pretrained BART-CNN model. The approach achieved 12th\nplace among 100 teams in the shared task, demonstrating computational\nefficiency and contextual accuracy. By leveraging pretrained summarization\nmodels, this work advances medical CQA research and contributes to clinical\ndecision support systems.", "published": "2025-03-15 17:36:02", "link": "http://arxiv.org/abs/2503.16513v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PLM: Efficient Peripheral Language Models Hardware-Co-Designed for Ubiquitous Computing", "abstract": "While scaling laws have been continuously validated in large language models\n(LLMs) with increasing model parameters, the inherent tension between the\ninference demands of LLMs and the limited resources of edge devices poses a\ncritical challenge to the development of edge intelligence. Recently, numerous\nsmall language models have emerged, aiming to distill the capabilities of LLMs\ninto smaller footprints. However, these models often retain the fundamental\narchitectural principles of their larger counterparts, still imposing\nconsiderable strain on the storage and bandwidth capacities of edge devices. In\nthis paper, we introduce the PLM, a Peripheral Language Model, developed\nthrough a co-design process that jointly optimizes model architecture and edge\nsystem constraints. The PLM utilizes a Multi-head Latent Attention mechanism\nand employs the squared ReLU activation function to encourage sparsity, thereby\nreducing peak memory footprint during inference. During training, we collect\nand reorganize open-source datasets, implement a multi-phase training strategy,\nand empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning\nrate scheduler. Additionally, we incorporate Reinforcement Learning from Human\nFeedback (RLHF) by adopting the ARIES preference learning approach. Following a\ntwo-phase SFT process, this method yields performance gains of 2% in general\ntasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel\narchitecture, evaluation results demonstrate that PLM outperforms existing\nsmall language models trained on publicly available data while maintaining the\nlowest number of activated parameters. Furthermore, deployment across various\nedge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,\nvalidates PLM's suitability for peripheral applications. The PLM series models\nare publicly available at https://github.com/plm-team/PLM.", "published": "2025-03-15 15:11:17", "link": "http://arxiv.org/abs/2503.12167v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving LLM-based Document-level Machine Translation with Multi-Knowledge Fusion", "abstract": "Recent studies in prompting large language model (LLM) for document-level\nmachine translation (DMT) primarily focus on the inter-sentence context by\nflatting the source document into a long sequence. This approach relies solely\non the sequence of sentences within the document. However, the complexity of\ndocument-level sequences is greater than that of shorter sentence-level\nsequences, which may limit LLM's ability in DMT when only this single-source\nknowledge is used. In this paper, we propose an enhanced approach by\nincorporating multiple sources of knowledge, including both the document\nsummarization and entity translation, to enhance the performance of LLM-based\nDMT. Given a source document, we first obtain its summarization and translation\nof entities via LLM as the additional knowledge. We then utilize LLMs to\ngenerate two translations of the source document by fusing these two single\nknowledge sources, respectively. Finally, recognizing that different sources of\nknowledge may aid or hinder the translation of different sentences, we refine\nand rank the translations by leveraging a multi-knowledge fusion strategy to\nensure the best results. Experimental results in eight document-level\ntranslation tasks show that our approach achieves an average improvement of\n0.8, 0.6, and 0.4 COMET scores over the baseline without extra knowledge for\nLLaMA3-8B-Instruct, Mistral-Nemo-Instruct, and GPT-4o-mini, respectively.", "published": "2025-03-15 14:18:45", "link": "http://arxiv.org/abs/2503.12152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models", "abstract": "With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps://github.com/CoderChen01/LVLMSarcasmAnalysis", "published": "2025-03-15 14:10:25", "link": "http://arxiv.org/abs/2503.12149v1", "categories": ["cs.CL", "cs.MM", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Enhanced Sentiment Analysis of Iranian Restaurant Reviews Utilizing Sentiment Intensity Analyzer & Fuzzy Logic", "abstract": "This research presents an advanced sentiment analysis framework studied on\nIranian restaurant reviews, combining fuzzy logic with conventional sentiment\nanalysis techniques to assess both sentiment polarity and intensity. A dataset\nof 1266 reviews, alongside corresponding star ratings, was compiled and\npreprocessed for analysis. Initial sentiment analysis was conducted using the\nSentiment Intensity Analyzer (VADER), a rule-based tool that assigns sentiment\nscores across positive, negative, and neutral categories. However, a noticeable\nbias toward neutrality often led to an inaccurate representation of sentiment\nintensity. To mitigate this issue, based on a fuzzy perspective, two refinement\ntechniques were introduced, applying square-root and fourth-root\ntransformations to amplify positive and negative sentiment scores while\nmaintaining neutrality. This led to three distinct methodologies: Approach 1,\nutilizing unaltered VADER scores; Approach 2, modifying sentiment values using\nthe square root; and Approach 3, applying the fourth root for further\nrefinement. A Fuzzy Inference System incorporating comprehensive fuzzy rules\nwas then developed to process these refined scores and generate a single,\ncontinuous sentiment value for each review based on each approach. Comparative\nanalysis, including human supervision and alignment with customer star ratings,\nrevealed that the refined approaches significantly improved sentiment analysis\nby reducing neutrality bias and better capturing sentiment intensity. Despite\nthese advancements, minor over-amplification and persistent neutrality in\ndomain-specific cases were identified, leading us to propose several future\nstudies to tackle these occasional barriers. The study's methodology and\noutcomes offer valuable insights for businesses seeking a more precise\nunderstanding of consumer sentiment, enhancing sentiment analysis across\nvarious industries.", "published": "2025-03-15 13:55:23", "link": "http://arxiv.org/abs/2503.12141v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Hyperbolic Safety-Aware Vision-Language Models", "abstract": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.", "published": "2025-03-15 13:18:04", "link": "http://arxiv.org/abs/2503.12127v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling", "abstract": "Process reward models (PRMs) have shown success in complex reasoning tasks\nfor large language models (LLMs). However, their application to machine\ntranslation (MT) remains underexplored due to the lack of systematic\nmethodologies and evaluation benchmarks. To address this gap, we introduce\n\\textbf{MT-RewardTree}, a comprehensive framework for constructing, evaluating,\nand deploying process reward models in MT. Unlike traditional vanilla\npreference pair construction, we propose a novel method for automatically\ngenerating token-level preference pairs using approximate Monte Carlo Tree\nSearch (MCTS), which mitigates the prohibitive cost of human annotation for\nfine-grained steps. Then, we establish the first MT-specific reward model\nbenchmark and provide a systematic comparison of different reward modeling\narchitectures, revealing that token-level supervision effectively captures\nfine-grained preferences. Experimental results demonstrate that our\nMT-PRM-Qwen-2.5-3B achieves state-of-the-art performance in both token-level\nand sequence-level evaluation given the same input prefix. Furthermore, we\nshowcase practical applications where PRMs enable test-time alignment for LLMs\nwithout additional alignment training and significantly improve performance in\nhypothesis ensembling. Our work provides valuable insights into the role of\nreward models in MT research. Our code and data are released in\n\\href{https://sabijun.github.io/MT_RewardTreePage/}{https://sabijun.github.io/MT\\_RewardTreePage}.", "published": "2025-03-15 13:04:51", "link": "http://arxiv.org/abs/2503.12123v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RECSIP: REpeated Clustering of Scores Improving the Precision", "abstract": "The latest research on Large Language Models (LLMs) has demonstrated\nsignificant advancement in the field of Natural Language Processing (NLP).\nHowever, despite this progress, there is still a lack of reliability in these\nmodels. This is due to the stochastic architecture of LLMs, which presents a\nchallenge for users attempting to ascertain the reliability of a model's\nresponse. These responses may cause serious harm in high-risk environments or\nexpensive failures in industrial contexts. Therefore, we introduce the\nframework REpeated Clustering of Scores Improving the Precision (RECSIP) which\nfocuses on improving the precision of LLMs by asking multiple models in\nparallel, scoring and clustering their responses to ensure a higher reliability\non the response. The evaluation of our reference implementation recsip on the\nbenchmark MMLU-Pro using the models GPT-4o, Claude and Gemini shows an overall\nincrease of 5.8 per cent points compared to the best used model.", "published": "2025-03-15 12:36:32", "link": "http://arxiv.org/abs/2503.12108v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Legislative Content Analysis: A Dataset from the Polish Parliament", "abstract": "Large language models (LLMs) are among the best methods for processing\nnatural language, partly due to their versatility. At the same time,\ndomain-specific LLMs are more practical in real-life applications. This work\nintroduces a novel natural language dataset created by acquired data from\nofficial legislative authorities' websites. The study focuses on formulating\nthree natural language processing (NLP) tasks to evaluate the effectiveness of\nLLMs on legislative content analysis within the context of the Polish legal\nsystem. Key findings highlight the potential of LLMs in automating and\nenhancing legislative content analysis while emphasizing specific challenges,\nsuch as understanding legal context. The research contributes to the\nadvancement of NLP in the legal field, particularly in the Polish language. It\nhas been demonstrated that even commonly accessible data can be practically\nutilized for legislative content analysis.", "published": "2025-03-15 12:10:20", "link": "http://arxiv.org/abs/2503.12100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models", "abstract": "High-quality training data has proven crucial for developing performant large\nlanguage models (LLMs). However, commercial LLM providers disclose few, if any,\ndetails about the data used for training. This lack of transparency creates\nmultiple challenges: it limits external oversight and inspection of LLMs for\nissues such as copyright infringement, it undermines the agency of data\nauthors, and it hinders scientific research on critical issues such as data\ncontamination and data selection. How can we recover what training data is\nknown to LLMs? In this work, we demonstrate a new method to identify training\ndata known to proprietary LLMs like GPT-4 without requiring any access to model\nweights or token probabilities, by using information-guided probes. Our work\nbuilds on a key observation: text passages with high surprisal are good search\nmaterial for memorization probes. By evaluating a model's ability to\nsuccessfully reconstruct high-surprisal tokens in text, we can identify a\nsurprising number of texts memorized by LLMs.", "published": "2025-03-15 10:19:15", "link": "http://arxiv.org/abs/2503.12072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark", "abstract": "Large language models (LLMs) have made tremendous progress in recent years,\nbut low-resource languages, such as Tibetan, remain significantly\nunderrepresented in their evaluation. Despite Tibetan being spoken by over\nseven million people, it has largely been neglected in the development and\nassessment of LLMs. To address this gap, we present TLUE (A Tibetan Language\nUnderstanding Evaluation Benchmark), the first large-scale benchmark for\nassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:\n(1) a comprehensive multi-task understanding benchmark spanning 5 domains and\n67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a\ndiverse set of state-of-the-art LLMs. Experimental results demonstrate that\nmost LLMs perform below the random baseline, highlighting the considerable\nchallenges LLMs face in processing Tibetan, a low-resource language. TLUE\nprovides an essential foundation for driving future research and progress in\nTibetan language understanding and underscores the need for greater inclusivity\nin LLM development.", "published": "2025-03-15 08:54:25", "link": "http://arxiv.org/abs/2503.12051v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applications of Large Language Model Reasoning in Feature Generation", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\nthrough their state of art reasoning capabilities. This paper explores the\nconvergence of LLM reasoning techniques and feature generation for machine\nlearning tasks. We examine four key reasoning approaches: Chain of Thought,\nTree of Thoughts, Retrieval-Augmented Generation, and Thought Space\nExploration. Our analysis reveals how these approaches can be used to identify\neffective feature generation rules without having to manually specify search\nspaces. The paper categorizes LLM-based feature generation methods across\nvarious domains including finance, healthcare, and text analytics. LLMs can\nextract key information from clinical notes and radiology reports in\nhealthcare, by enabling more efficient data utilization. In finance, LLMs\nfacilitate text generation, summarization, and entity extraction from complex\ndocuments. We analyze evaluation methodologies for assessing feature quality\nand downstream performance, with particular attention to OCTree's decision tree\nreasoning approach that provides language-based feedback for iterative\nimprovements. Current challenges include hallucination, computational\nefficiency, and domain adaptation. As of March 2025, emerging approaches\ninclude inference-time compute scaling, reinforcement learning, and supervised\nfine-tuning with model distillation. Future directions point toward multimodal\nfeature generation, self-improving systems, and neuro-symbolic approaches. This\npaper provides a detailed overview of an emerging field that promises to\nautomate and enhance feature engineering through language model reasoning.", "published": "2025-03-15 04:18:01", "link": "http://arxiv.org/abs/2503.11989v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models", "abstract": "Advancements in Large Language Models (LLMs) have increased the performance\nof different natural language understanding as well as generation tasks.\nAlthough LLMs have breached the state-of-the-art performance in various tasks,\nthey often reflect different forms of bias present in the training data. In the\nlight of this perceived limitation, we provide a unified evaluation of\nbenchmarks using a set of representative LLMs that cover different forms of\nbiases starting from physical characteristics to socio-economic categories.\nMoreover, we propose five prompting approaches to carry out the bias detection\ntask across different aspects of bias. Further, we formulate three research\nquestions to gain valuable insight in detecting biases in LLMs using different\napproaches and evaluation metrics across benchmarks. The results indicate that\neach of the selected LLMs suffer from one or the other form of bias with the\nLLaMA3.1-8B model being the least biased. Finally, we conclude the paper with\nthe identification of key challenges and possible future directions.", "published": "2025-03-15 03:58:14", "link": "http://arxiv.org/abs/2503.11985v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HInter: Exposing Hidden Intersectional Bias in Large Language Models", "abstract": "Large Language Models (LLMs) may portray discrimination towards certain\nindividuals, especially those characterized by multiple attributes (aka\nintersectional bias). Discovering intersectional bias in LLMs is challenging,\nas it involves complex inputs on multiple attributes (e.g. race and gender). To\naddress this challenge, we propose HInter, a test technique that\nsynergistically combines mutation analysis, dependency parsing and metamorphic\noracles to automatically detect intersectional bias in LLMs. HInter generates\ntest inputs by systematically mutating sentences using multiple mutations,\nvalidates inputs via a dependency invariant and detects biases by checking the\nLLM response on the original and mutated sentences. We evaluate HInter using\nsix LLM architectures and 18 LLM models (GPT3.5, Llama2, BERT, etc) and find\nthat 14.61% of the inputs generated by HInter expose intersectional bias.\nResults also show that our dependency invariant reduces false positives\n(incorrect test inputs) by an order of magnitude. Finally, we observed that\n16.62% of intersectional bias errors are hidden, meaning that their\ncorresponding atomic cases do not trigger biases. Overall, this work emphasize\nthe importance of testing LLMs for intersectional bias.", "published": "2025-03-15 02:10:38", "link": "http://arxiv.org/abs/2503.11962v1", "categories": ["cs.CL", "cs.AI", "68T50, 68T05"], "primary_category": "cs.CL"}
{"title": "Integration of Explainable AI Techniques with Large Language Models for Enhanced Interpretability for Sentiment Analysis", "abstract": "Interpretability remains a key difficulty in sentiment analysis with Large\nLanguage Models (LLMs), particularly in high-stakes applications where it is\ncrucial to comprehend the rationale behind forecasts. This research addressed\nthis by introducing a technique that applies SHAP (Shapley Additive\nExplanations) by breaking down LLMs into components such as embedding\nlayer,encoder,decoder and attention layer to provide a layer-by-layer knowledge\nof sentiment prediction. The approach offers a clearer overview of how model\ninterpret and categorise sentiment by breaking down LLMs into these parts. The\nmethod is evaluated using the Stanford Sentiment Treebank (SST-2) dataset,\nwhich shows how different sentences affect different layers. The effectiveness\nof layer-wise SHAP analysis in clarifying sentiment-specific token attributions\nis demonstrated by experimental evaluations, which provide a notable\nenhancement over current whole-model explainability techniques. These results\nhighlight how the suggested approach could improve the reliability and\ntransparency of LLM-based sentiment analysis in crucial applications.", "published": "2025-03-15 01:37:54", "link": "http://arxiv.org/abs/2503.11948v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token-Level Uncertainty-Aware Objective for Language Model Post-Training", "abstract": "In the current work, we connect token-level uncertainty in causal language\nmodeling to two types of training objectives: 1) masked maximum likelihood\n(MLE), 2) self-distillation. We show that masked MLE is effective in reducing\nepistemic uncertainty, and serve as an effective token-level automatic\ncurriculum learning technique. However, masked MLE is prone to overfitting and\nrequires self-distillation regularization to improve or maintain performance on\nout-of-distribution tasks. We demonstrate significant performance gain via the\nproposed training objective - combined masked MLE and self-distillation -\nacross multiple architectures (Gemma, LLaMA, Phi) and datasets (Alpaca,\nShareGPT, GSM8K), mitigating overfitting while maintaining adaptability during\npost-training. Our findings suggest that uncertainty-aware training provides an\neffective mechanism for enhancing language model training.", "published": "2025-03-15 00:32:14", "link": "http://arxiv.org/abs/2503.16511v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantum-enhanced quickest change detection of transmission loss", "abstract": "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.", "published": "2025-03-15 22:18:43", "link": "http://arxiv.org/abs/2503.12276v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "A Novel Double Pruning method for Imbalanced Data using Information Entropy and Roulette Wheel Selection for Breast Cancer Diagnosis", "abstract": "Accurate illness diagnosis is vital for effective treatment and patient\nsafety. Machine learning models are widely used for cancer diagnosis based on\nhistorical medical data. However, data imbalance remains a major challenge,\nleading to hindering classifier performance and reliability. The SMOTEBoost\nmethod addresses this issue by generating synthetic data to balance the\ndataset, but it may overlook crucial overlapping regions near the decision\nboundary and can produce noisy samples. This paper proposes RE-SMOTEBoost, an\nenhanced version of SMOTEBoost, designed to overcome these limitations.\nFirstly, RE-SMOTEBoost focuses on generating synthetic samples in overlapping\nregions to better capture the decision boundary using roulette wheel selection.\nSecondly, it incorporates a filtering mechanism based on information entropy to\nreduce noise, and borderline cases and improve the quality of generated data.\nThirdly, we introduce a double regularization penalty to control the synthetic\nsamples proximity to the decision boundary and avoid class overlap. These\nenhancements enable higher-quality oversampling of the minority class,\nresulting in a more balanced and effective training dataset. The proposed\nmethod outperforms existing state-of-the-art techniques when evaluated on\nimbalanced datasets. Compared to the top-performing sampling algorithms,\nRE-SMOTEBoost demonstrates a notable improvement of 3.22\\% in accuracy and a\nvariance reduction of 88.8\\%. These results indicate that the proposed model\noffers a solid solution for medical settings, effectively overcoming data\nscarcity and severe imbalance caused by limited samples, data collection\ndifficulties, and privacy constraints.", "published": "2025-03-15 19:34:15", "link": "http://arxiv.org/abs/2503.12239v1", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Robust Full-Space Physical Layer Security for STAR-RIS-Aided Wireless Networks: Eavesdropper with Uncertain Location and Channel", "abstract": "A robust full-space physical layer security (PLS) transmission scheme is\nproposed in this paper considering the full-space wiretapping challenge of\nwireless networks supported by simultaneous transmitting and reflecting\nreconfigurable intelligent surface (STAR-RIS). Different from the existing\nschemes, the proposed PLS scheme takes account of the uncertainty on the\neavesdropper's position within the 360$^\\circ$ service area offered by the\nSTAR-RIS. Specifically, the large system analytical method is utilized to\nderive the asymptotic expression of the average security rate achieved by the\nsecurity user, considering that the base station (BS) only has the statistical\ninformation of the eavesdropper's channel state information (CSI) and the\nuncertainty of its location. To evaluate the effectiveness of the proposed PLS\nscheme, we first formulate an optimization problem aimed at maximizing the\nweighted sum rate of the security user and the public user. This optimization\nis conducted under the power allocation constraint, and some practical\nlimitations for STAR-RIS implementation, through jointly designing the active\nand passive beamforming variables. A novel iterative algorithm based on the\nminimum mean-square error (MMSE) and cross-entropy optimization (CEO) methods\nis proposed to effectively address the established non-convex optimization\nproblem with discrete variables. Simulation results indicate that the proposed\nrobust PLS scheme can effectively mitigate the information leakage across the\nentire coverage area of the STAR-RIS-assisted system, leading to superior\nperformance gain when compared to benchmark schemes encompassing traditional\nRIS-aided scheme.", "published": "2025-03-15 18:57:35", "link": "http://arxiv.org/abs/2503.12233v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Unsupervised Learning for AoD Estimation in MISO Downlink LoS Transmissions", "abstract": "With the emerging of simultaneous localization and communication (SLAC), it\nbecomes more and more attractive to perform angle of departure (AoD) estimation\nat the receiving Internet of Thing (IoT) user end for improved positioning\naccuracy, flexibility and enhanced user privacy. To address challenges like\nlarge number of real-time measurements required for latency-critical\napplications and enormous data collection for training deep learning models in\nconventional AoD estimation methods, we propose in this letter an unsupervised\nlearning framework, which unifies training for both deterministic maximum\nlikelihood (DML) and stochastic maximum likelihood (SML) based AoD estimation\nin multiple-input single-output (MISO) downlink (DL) wireless transmissions.\nSpecifically, under the line-of-sight (LoS) assumption, we incorporate both the\nreceived signals and pilot-sequence information, as per its availability at the\nDL user, into the input of the deep learning model, and adopt a common neural\nnetwork architecture compatible with input data in both DML and SML cases.\nExtensive numerical results validate that the proposed unsupervised learning\nbased AoD estimation not only improves estimation accuracy, but also\nsignificantly reduces required number of observations, thereby reducing both\nestimation overhead and latency compared to various benchmarks.", "published": "2025-03-15 07:53:01", "link": "http://arxiv.org/abs/2503.12033v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Weighted cycle-based identification of influential node groups in complex networks", "abstract": "Identifying influential node groups in complex networks is crucial for\noptimizing information dissemination, epidemic control, and viral marketing.\nHowever, traditional centrality-based methods often focus on individual nodes,\nresulting in overlapping influence zones and diminished collective\neffectiveness. To overcome these limitations, we propose Weighted Cycle\n(WCycle), a novel indicator that incorporates basic cycle structures and node\nbehavior traits (edge weights) to comprehensively assess node importance.\nWCycle effectively identifies spatially dispersed and structurally diverse key\nnode group, thereby reducing influence redundancy and enhancing network-wide\npropagation. Extensive experiments on six real-world networks demonstrate\nWCycle's superior performance compared to five benchmark methods across\nmultiple evaluation dimensions, including influence propagation efficiency,\nstructural differentiation, and cost-effectiveness. The findings highlight\nWCycle's robustness and scalability, establishing it as a promising tool for\ncomplex network analysis and practical applications requiring effective\ninfluence maximization.", "published": "2025-03-15 03:08:22", "link": "http://arxiv.org/abs/2503.11974v1", "categories": ["cs.SI", "cs.IT", "math.IT"], "primary_category": "cs.SI"}
{"title": "Goal-Oriented Source Coding using LDPC Codes for Compressed-Domain Image Classification", "abstract": "In the emerging field of goal-oriented communications, the focus has shifted\nfrom reconstructing data to directly performing specific learning tasks, such\nas classification, segmentation, or pattern recognition, on the received coded\ndata. In the commonly studied scenario of classification from compressed\nimages, a key objective is to enable learning directly on entropy-coded data,\nthereby bypassing the computationally intensive step of data reconstruction.\nConventional entropy-coding methods, such as Huffman and Arithmetic coding, are\neffective for compression but disrupt the data structure, making them less\nsuitable for direct learning without decoding. This paper investigates the use\nof low-density parity-check (LDPC) codes -- originally designed for channel\ncoding -- as an alternative entropy-coding approach. It is hypothesized that\nthe structured nature of LDPC codes can be leveraged more effectively by deep\nlearning models for tasks like classification. At the receiver side, gated\nrecurrent unit (GRU) models are trained to perform image classification\ndirectly on LDPC-coded data. Experiments on datasets like MNIST, Fashion-MNIST,\nand CIFAR show that LDPC codes outperform Huffman and Arithmetic coding in\nclassification tasks, while requiring significantly smaller learning models.\nFurthermore, the paper analyzes why LDPC codes preserve data structure more\neffectively than traditional entropy-coding techniques and explores the impact\nof key code parameters on classification performance. These results suggest\nthat LDPC-based entropy coding offers an optimal balance between learning\nefficiency and model complexity, eliminating the need for prior decoding.", "published": "2025-03-15 01:52:09", "link": "http://arxiv.org/abs/2503.11954v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.IT", "cs.LG", "math.IT", "94A29, 94A08, 94B05, 68T01, 68P30", "I.4.2; E.4; I.2.10; I.5.4; I.5.1; I.4.1"], "primary_category": "eess.IV"}
{"title": "Decentralized Hidden Markov Modeling with Equal Exit Probabilities", "abstract": "Social learning strategies enable agents to infer the underlying true state\nof nature in a distributed manner by receiving private environmental signals\nand exchanging beliefs with their neighbors. Previous studies have extensively\nfocused on static environments, where the underlying true state remains\nunchanged over time. In this paper, we consider a dynamic setting where the\ntrue state evolves according to a Markov chain with equal exit probabilities.\nBased on this assumption, we present a social learning strategy for dynamic\nenvironments, termed Diffusion $\\alpha$-HMM. By leveraging a simplified\nparameterization, we derive a nonlinear dynamical system that governs the\nevolution of the log-belief ratio over time. This formulation further reveals\nthe relationship between the linearized form of Diffusion $\\alpha$-HMM and\nAdaptive Social Learning, a well-established social learning strategy for\ndynamic environments. Furthermore, we analyze the convergence and fixed-point\nproperties of a reference system, providing theoretical guarantees on the\nlearning performance of the proposed algorithm in dynamic settings. Numerical\nexperiments compare various distributed social learning strategies across\ndifferent dynamic environments, demonstrating the impact of nonlinearity and\nparameterization on learning performance in a range of dynamic scenarios.", "published": "2025-03-15 14:22:19", "link": "http://arxiv.org/abs/2503.12153v1", "categories": ["cs.MA", "cs.SI"], "primary_category": "cs.MA"}
{"title": "Vote Delegation in DeFi Governance", "abstract": "We investigate the drivers of vote delegation in Decentralized Autonomous\nOrganizations (DAOs), using the Uniswap governance DAO as a laboratory. We show\nthat parties with fewer self-owned votes and those affiliated with the\ncontrolling venture capital firm, Andreesen Horowitz (a16z), receive more vote\ndelegations. These patterns suggest that while the Uniswap ecosystem values\ndecentralization, a16z may engage in window-dressing around it. Moreover, we\nfind that an active and successful track record in submitting improvement\nproposals, especially in the final stage, leads to more vote delegations,\nindicating that delegation in DAOs is at least partly reputation- or\nmerit-based. Combined, our findings provide new insights into how governance\nand decentralization operate in DeFi.", "published": "2025-03-15 01:15:08", "link": "http://arxiv.org/abs/2503.11940v1", "categories": ["q-fin.RM", "q-fin.CP", "q-fin.GN"], "primary_category": "q-fin.RM"}
{"title": "United we stand, Divided we fall: Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition in Valence-Arousal Space", "abstract": "Audio and visual modalities are two predominant contact-free channels in\nvideos, which are often expected to carry a complementary relationship with\neach other. However, they may not always complement each other, resulting in\npoor audio-visual feature representations. In this paper, we introduce Gated\nRecursive Joint Cross Attention (GRJCA) using a gating mechanism that can\nadaptively choose the most relevant features to effectively capture the\nsynergic relationships across audio and visual modalities. Specifically, we\nimprove the performance of Recursive Joint Cross-Attention (RJCA) by\nintroducing a gating mechanism to control the flow of information between the\ninput features and the attended features of multiple iterations depending on\nthe strength of their complementary relationship. For instance, if the\nmodalities exhibit strong complementary relationships, the gating mechanism\nemphasizes cross-attended features, otherwise non-attended features. To further\nimprove the performance of the system, we also explored a hierarchical gating\napproach by introducing a gating mechanism at every iteration, followed by\nhigh-level gating across the gated outputs of each iteration. The proposed\napproach improves the performance of RJCA model by adding more flexibility to\ndeal with weak complementary relationships across audio and visual modalities.\nExtensive experiments are conducted on the challenging Affwild2 dataset to\ndemonstrate the robustness of the proposed approach. By effectively handling\nthe weak complementary relationships across the audio and visual modalities,\nthe proposed model achieves a Concordance Correlation Coefficient (CCC) of\n0.561 (0.623) and 0.620 (0.660) for valence and arousal respectively on the\ntest set (validation set).", "published": "2025-03-15 21:03:20", "link": "http://arxiv.org/abs/2503.12261v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap", "abstract": "Recent works in cross-modal understanding and generation, notably through\nmodels like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive\nAudio-Visual Pretraining), have significantly enhanced the alignment of text,\nvideo, and audio embeddings via a single contrastive loss. However, these\nmethods often overlook the bidirectional interactions and inherent noises\npresent in each modality, which can crucially impact the quality and efficacy\nof cross-modal integration. To address this limitation, we introduce DiffGAP, a\nnovel approach incorporating a lightweight generative module within the\ncontrastive space. Specifically, our DiffGAP employs a bidirectional diffusion\nprocess tailored to bridge the cross-modal gap more effectively. This involves\na denoising process on text and video embeddings conditioned on audio\nembeddings and vice versa, thus facilitating a more nuanced and robust\ncross-modal interaction. Our experimental results on VGGSound and AudioCaps\ndatasets demonstrate that DiffGAP significantly improves performance in\nvideo/text-audio generation and retrieval tasks, confirming its effectiveness\nin enhancing cross-modal understanding and generation capabilities.", "published": "2025-03-15 13:24:09", "link": "http://arxiv.org/abs/2503.12131v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations", "abstract": "Current large speech language models are mainly based on semantic tokens from\ndiscretization of self-supervised learned representations and acoustic tokens\nfrom a neural codec, following a semantic-modeling and acoustic-synthesis\nparadigm. However, semantic tokens discard paralinguistic attributes of\nspeakers that is important for natural spoken communication, while prompt-based\nacoustic synthesis from semantic tokens has limits in recovering paralinguistic\ndetails and suffers from robustness issues, especially when there are domain\ngaps between the prompt and the target. This paper unifies two types of tokens\nand proposes the UniCodec, a universal speech token learning that encapsulates\nall semantics of speech, including linguistic and paralinguistic information,\ninto a compact and semantically-disentangled unified token. Such a unified\ntoken can not only benefit speech language models in understanding with\nparalinguistic hints but also help speech generation with high-quality output.\nA low-bitrate neural codec is leveraged to learn such disentangled discrete\nrepresentations at global and local scales, with knowledge distilled from\nself-supervised learned features. Extensive evaluations on multilingual\ndatasets demonstrate its effectiveness in generating natural, expressive and\nlong-term consistent output quality with paralinguistic attributes well\npreserved in several speech processing tasks.", "published": "2025-03-15 12:50:43", "link": "http://arxiv.org/abs/2503.12115v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing", "abstract": "Movie dubbing describes the process of transforming a script into speech that\naligns temporally and emotionally with a given movie clip while exemplifying\nthe speaker's voice demonstrated in a short reference audio clip. This task\ndemands the model bridge character performances and complicated prosody\nstructures to build a high-quality video-synchronized dubbing track. The\nlimited scale of movie dubbing datasets, along with the background noise\ninherent in audio data, hinder the acoustic modeling performance of trained\nmodels. To address these issues, we propose an acoustic-prosody disentangled\ntwo-stage method to achieve high-quality dubbing generation with precise\nprosody alignment. First, we propose a prosody-enhanced acoustic pre-training\nto develop robust acoustic modeling capabilities. Then, we freeze the\npre-trained acoustic system and design a disentangled framework to model\nprosodic text features and dubbing style while maintaining acoustic quality.\nAdditionally, we incorporate an in-domain emotion analysis module to reduce the\nimpact of visual domain shifts across different movies, thereby enhancing\nemotion-prosody alignment. Extensive experiments show that our method performs\nfavorably against the state-of-the-art models on two primary benchmarks. The\ndemos are available at https://zzdoog.github.io/ProDubber/.", "published": "2025-03-15 08:25:57", "link": "http://arxiv.org/abs/2503.12042v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Mixture of Experts Learning for Robust Audio Spoofing Detection", "abstract": "In audio spoofing detection, most studies rely on clean datasets, making\nmodels susceptible to real-world post-processing attacks, such as channel\ncompression and noise. To overcome this challenge, we propose the Adaptive\nMixture of Experts Learning (AMEL) framework, which enhances resilience by\nleveraging attack-specific knowledge and adapting dynamically to varied attack\nconditions. Specifically, AMEL utilizes Attack-Specific Experts (ASE)\nfine-tuned with Low-Rank Adaptation (LoRA), enabling each expert to target\nspecific post-processing patterns while requiring only 1.12\\% of the parameters\nneeded for full fine-tuning. Furthermore, we introduce Dynamic Expert\nAggregation (DEA), which adaptively selects and integrates expert knowledge to\nenhance the robustness of spoofing detection. Experimental results demonstrate\nthat AMEL significantly enhances robustness by improving noise resilience and\nexhibiting greater adaptability to previously unseen post-processing methods\ncompared to models relying on full fine-tuning. Additionally, our framework\noutperforms both single expert and simple average ensemble under various mixed\nattacks, demonstrating its superior robustness and adaptability in managing\ncomplex, real-world conditions.", "published": "2025-03-15 06:23:13", "link": "http://arxiv.org/abs/2503.12010v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Optimization-Based Analysis of Music Intervals and Tuning Systems in Oral Traditions Using Pitch Histograms: A Case Study of Iranian Vocal Music", "abstract": "This paper presents a computational methodology for analyzing music intervals\nand tunings in microtonal oral traditions, utilizing pitch histograms, Dynamic\nTime Warping (DTW), and optimization techniques. By extracting pitch\nfrequencies directly from vocal performances and aligning them with MIDI notes\nvia DTW, we determine musical intervals using histograms. This approach offers\nan efficient, performance-based, and instrument-independent alternative to\ntraditional tuning system analysis. Optimization techniques are then employed\nto align intervals throughout the oral tradition repertoire, capturing the\nspecific tunings and modes involved. Our methodology demonstrates the potential\nof computational techniques in advancing musicological and ethnomusicological\nresearch, revealing new insights into the studied traditions.", "published": "2025-03-15 01:59:47", "link": "http://arxiv.org/abs/2503.11956v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
