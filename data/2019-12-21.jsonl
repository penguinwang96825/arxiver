{"title": "Candidate Fusion: Integrating Language Modelling into a\n  Sequence-to-Sequence Handwritten Word Recognition Architecture", "abstract": "Sequence-to-sequence models have recently become very popular for tackling\nhandwritten word recognition problems. However, how to effectively integrate an\nexternal language model into such recognizer is still a challenging problem.\nThe main challenge faced when training a language model is to deal with the\nlanguage model corpus which is usually different to the one used for training\nthe handwritten word recognition system. Thus, the bias between both word\ncorpora leads to incorrectness on the transcriptions, providing similar or even\nworse performances on the recognition task. In this work, we introduce\nCandidate Fusion, a novel way to integrate an external language model to a\nsequence-to-sequence architecture. Moreover, it provides suggestions from an\nexternal language knowledge, as a new input to the sequence-to-sequence\nrecognizer. Hence, Candidate Fusion provides two improvements. On the one hand,\nthe sequence-to-sequence recognizer has the flexibility not only to combine the\ninformation from itself and the language model, but also to choose the\nimportance of the information provided by the language model. On the other\nhand, the external language model has the ability to adapt itself to the\ntraining corpus and even learn the most commonly errors produced from the\nrecognizer. Finally, by conducting comprehensive experiments, the Candidate\nFusion proves to outperform the state-of-the-art language models for\nhandwritten word recognition tasks.", "published": "2019-12-21 18:14:32", "link": "http://arxiv.org/abs/1912.10308v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning and Evaluating Contextual Embedding of Source Code", "abstract": "Recent research has achieved impressive results on understanding and\nimproving source code by building up on machine-learning techniques developed\nfor natural languages. A significant advancement in natural-language\nunderstanding has come with the development of pre-trained contextual\nembeddings, such as BERT, which can be fine-tuned for downstream tasks with\nless labeled data and training budget, while achieving better accuracies.\nHowever, there is no attempt yet to obtain a high-quality contextual embedding\nof source code, and to evaluate it on multiple program-understanding tasks\nsimultaneously; that is the gap that this paper aims to mitigate. Specifically,\nfirst, we curate a massive, deduplicated corpus of 7.4M Python files from\nGitHub, which we use to pre-train CuBERT, an open-sourced code-understanding\nBERT model; and, second, we create an open-sourced benchmark that comprises\nfive classification tasks and one program-repair task, akin to\ncode-understanding tasks proposed in the literature before. We fine-tune CuBERT\non our benchmark tasks, and compare the resulting models to different variants\nof Word2Vec token embeddings, BiLSTM and Transformer models, as well as\npublished state-of-the-art models, showing that CuBERT outperforms them all,\neven with shorter training, and with fewer labeled examples. Future work on\nsource-code embedding can benefit from reusing our benchmark, and from\ncomparing against CuBERT models as a strong baseline.", "published": "2019-12-21 05:05:22", "link": "http://arxiv.org/abs/2001.00059v3", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "A Machine Learning Framework for Authorship Identification From Texts", "abstract": "Authorship identification is a process in which the author of a text is\nidentified. Most known literary texts can easily be attributed to a certain\nauthor because they are, for example, signed. Yet sometimes we find unfinished\npieces of work or a whole bunch of manuscripts with a wide variety of possible\nauthors. In order to assess the importance of such a manuscript, it is vital to\nknow who wrote it. In this work, we aim to develop a machine learning framework\nto effectively determine authorship. We formulate the task as a single-label\nmulti-class text categorization problem and propose a supervised machine\nlearning framework incorporating stylometric features. This task is highly\ninterdisciplinary in that it takes advantage of machine learning, information\nretrieval, and natural language processing. We present an approach and a model\nwhich learns the differences in writing style between $50$ different authors\nand is able to predict the author of a new text with high accuracy. The\naccuracy is seen to increase significantly after introducing certain linguistic\nstylometric features along with text features.", "published": "2019-12-21 05:47:58", "link": "http://arxiv.org/abs/1912.10204v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Predicting Heart Failure Readmission from Clinical Notes Using Deep\n  Learning", "abstract": "Heart failure hospitalization is a severe burden on healthcare. How to\npredict and therefore prevent readmission has been a significant challenge in\noutcomes research. To address this, we propose a deep learning approach to\npredict readmission from clinical notes. Unlike conventional methods that use\nstructured data for prediction, we leverage the unstructured clinical notes to\ntrain deep learning models based on convolutional neural networks (CNN). We\nthen use the trained models to classify and predict potentially high-risk\nadmissions/patients. For evaluation, we trained CNNs using the discharge\nsummary notes in the MIMIC III database. We also trained regular machine\nlearning models based on random forest using the same datasets. The result\nshows that deep learning models outperform the regular models in prediction\ntasks. CNN method achieves a F1 score of 0.756 in general readmission\nprediction and 0.733 in 30-day readmission prediction, while random forest only\nachieves a F1 score of 0.674 and 0.656 respectively. We also propose a\nchi-square test based method to interpret key features associated with deep\nlearning predicted readmissions. It reveals clinical insights about readmission\nembedded in the clinical notes. Collectively, our method can make the human\nevaluation process more efficient and potentially facilitate the reduction of\nreadmission rates.", "published": "2019-12-21 17:49:13", "link": "http://arxiv.org/abs/1912.10306v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Recurrent Hierarchical Topic-Guided RNN for Language Generation", "abstract": "To simultaneously capture syntax and global semantics from a text corpus, we\npropose a new larger-context recurrent neural network (RNN) based language\nmodel, which extracts recurrent hierarchical semantic structure via a dynamic\ndeep topic model to guide natural language generation. Moving beyond a\nconventional RNN-based language model that ignores long-range word dependencies\nand sentence order, the proposed model captures not only intra-sentence word\ndependencies, but also temporal transitions between sentences and\ninter-sentence topic dependencies. For inference, we develop a hybrid of\nstochastic-gradient Markov chain Monte Carlo and recurrent autoencoding\nvariational Bayes. Experimental results on a variety of real-world text corpora\ndemonstrate that the proposed model not only outperforms larger-context\nRNN-based language models, but also learns interpretable recurrent multilayer\ntopics and generates diverse sentences and paragraphs that are syntactically\ncorrect and semantically coherent.", "published": "2019-12-21 21:11:35", "link": "http://arxiv.org/abs/1912.10337v2", "categories": ["cs.CL", "cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Calibration and reference simulations for the auditory periphery model\n  of Verhulst et al. 2018 version 1.2", "abstract": "This document describes a comprehensive procedure of how the biophysical\nmodel published by Verhulst et al. (2018) can be calibrated on the basis of\nreference auditory brainstem responses. Additionally, the filter design used in\ntwo of the model stages, cochlear nucleus (CN) and inferior colliculus (IC), is\ndescribed in detail. These descriptions are valid for a new release of the\nVerhulst et al. model, version 1.2, as well as for previous versions of the\nmodel (version 1.1 or earlier). The differences between the model versions are\nexplicitly mentioned and simulations to basic auditory stimuli are shown for\nmodel versions 1.1 and 1.2. In short, version 1.2 of the model includes a new\nimplementation of the CN and IC stages (Stages 5 and 6). All previous model\nstages (Stages 1-4: outer and middle ear, transmission-line cochlear filter\nbank, inner hair cell model, and auditory nerve model) remained unchanged. In\nthe new release (model version 1.2), in addition to the updated CN and IC\nstages, we employed a different calibration procedure to match human reference\nABR amplitudes of waves I, III, and V more faithfully. This release note shows\nthe implications of these model adjustments on the simulations presented in the\noriginal 2018 model paper. For this purpose, results from two model versions\nare reported: (1) New model release (version 1.2), labelled as model v1.2; and\n(2) Previous model release as used by Verhulst et al., labelled as model v1.1.\nThe main difference between IC model stages relates to the degree of IC\ninhibition that was applied, with more inhibition in v1.2 than implemented in\nv1.1. The time domain simulations presented in this document show that this\nchange in inhibition strength does not drastically change the results presented\nin the original paper. However, v1.2 more correctly captures the\nphysiologically derived CN and IC inhibition/excitation strengths.", "published": "2019-12-21 22:44:22", "link": "http://arxiv.org/abs/1912.10026v1", "categories": ["eess.AS", "cs.SD", "I.6.4, I.6.5", "I.6.4; I.6.5"], "primary_category": "eess.AS"}
{"title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern\n  Recognition", "abstract": "Audio pattern recognition is an important research topic in the machine\nlearning area, and includes several tasks such as audio tagging, acoustic scene\nclassification, music classification, speech emotion classification and sound\nevent detection. Recently, neural networks have been applied to tackle audio\npattern recognition problems. However, previous systems are built on specific\ndatasets with limited durations. Recently, in computer vision and natural\nlanguage processing, systems pretrained on large-scale datasets have\ngeneralized well to several tasks. However, there is limited research on\npretraining systems on large-scale datasets for audio pattern recognition. In\nthis paper, we propose pretrained audio neural networks (PANNs) trained on the\nlarge-scale AudioSet dataset. These PANNs are transferred to other audio\nrelated tasks. We investigate the performance and computational complexity of\nPANNs modeled by a variety of convolutional neural networks. We propose an\narchitecture called Wavegram-Logmel-CNN using both log-mel spectrogram and\nwaveform as input feature. Our best PANN system achieves a state-of-the-art\nmean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the\nbest previous system of 0.392. We transfer PANNs to six audio pattern\nrecognition tasks, and demonstrate state-of-the-art performance in several of\nthose tasks. We have released the source code and pretrained models of PANNs:\nhttps://github.com/qiuqiangkong/audioset_tagging_cnn.", "published": "2019-12-21 06:53:14", "link": "http://arxiv.org/abs/1912.10211v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Audio Prior", "abstract": "Deep convolutional neural networks are known to specialize in distilling\ncompact and robust prior from a large amount of data. We are interested in\napplying deep networks in the absence of training dataset. In this paper, we\nintroduce deep audio prior (DAP) which leverages the structure of a network and\nthe temporal information in a single audio file. Specifically, we demonstrate\nthat a randomly-initialized neural network can be used with carefully designed\naudio prior to tackle challenging audio problems such as universal blind source\nseparation, interactive audio editing, audio texture synthesis, and audio\nco-separation. To understand the robustness of the deep audio prior, we\nconstruct a benchmark dataset \\emph{Universal-150} for universal sound source\nseparation with a diverse set of sources. We show superior audio results than\nprevious work on both qualitative and quantitative evaluations. We also perform\nthorough ablation study to validate our design choices.", "published": "2019-12-21 16:35:54", "link": "http://arxiv.org/abs/1912.10292v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
