{"title": "Multi-stage Information Retrieval for Vietnamese Legal Texts", "abstract": "This study deals with the problem of information retrieval (IR) for\nVietnamese legal texts. Despite being well researched in many languages,\ninformation retrieval has still not received much attention from the Vietnamese\nresearch community. This is especially true for the case of legal documents,\nwhich are hard to process. This study proposes a new approach for information\nretrieval for Vietnamese legal documents using sentence-transformer. Besides,\nvarious experiments are conducted to make comparisons between different\ntransformer models, ranking scores, syllable-level, and word-level training.\nThe experiment results show that the proposed model outperforms models used in\ncurrent research on information retrieval for Vietnamese documents.", "published": "2022-09-29 01:13:56", "link": "http://arxiv.org/abs/2209.14494v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Media Bias Detection Using Distant Supervision With BABE -- Bias\n  Annotations By Experts", "abstract": "Media coverage has a substantial effect on the public perception of events.\nNevertheless, media outlets are often biased. One way to bias news articles is\nby altering the word choice. The automatic identification of bias by word\nchoice is challenging, primarily due to the lack of a gold standard data set\nand high context dependencies. This paper presents BABE, a robust and diverse\ndata set created by trained experts, for media bias research. We also analyze\nwhy expert labeling is essential within this domain. Our data set offers better\nannotation quality and higher inter-annotator agreement than existing work. It\nconsists of 3,700 sentences balanced among topics and outlets, containing media\nbias labels on the word and sentence level. Based on our data, we also\nintroduce a way to detect bias-inducing sentences in news articles\nautomatically. Our best performing BERT-based model is pre-trained on a larger\ncorpus consisting of distant labels. Fine-tuning and evaluating the model on\nour proposed supervised data set, we achieve a macro F1-score of 0.804,\noutperforming existing methods.", "published": "2022-09-29 05:32:55", "link": "http://arxiv.org/abs/2209.14557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COLO: A Contrastive Learning based Re-ranking Framework for One-Stage\n  Summarization", "abstract": "Traditional training paradigms for extractive and abstractive summarization\nsystems always only use token-level or sentence-level training objectives.\nHowever, the output summary is always evaluated from summary-level which leads\nto the inconsistency in training and evaluation. In this paper, we propose a\nContrastive Learning based re-ranking framework for one-stage summarization\ncalled COLO. By modeling a contrastive objective, we show that the\nsummarization model is able to directly generate summaries according to the\nsummary-level score without additional modules and parameters. Extensive\nexperiments demonstrate that COLO boosts the extractive and abstractive results\nof one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1\nscore while preserving the parameter efficiency and inference efficiency.\nCompared with state-of-the-art multi-stage systems, we save more than 100 GPU\ntraining hours and obtaining 3~8 speed-up ratio during inference while\nmaintaining comparable results.", "published": "2022-09-29 06:11:21", "link": "http://arxiv.org/abs/2209.14569v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COMPILING: A Benchmark Dataset for Chinese Complexity Controllable\n  Definition Generation", "abstract": "The definition generation task aims to generate a word's definition within a\nspecific context automatically. However, owing to the lack of datasets for\ndifferent complexities, the definitions produced by models tend to keep the\nsame complexity level. This paper proposes a novel task of generating\ndefinitions for a word with controllable complexity levels. Correspondingly, we\nintroduce COMPILING, a dataset given detailed information about Chinese\ndefinitions, and each definition is labeled with its complexity levels. The\nCOMPILING dataset includes 74,303 words and 106,882 definitions. To the best of\nour knowledge, it is the largest dataset of the Chinese definition generation\ntask. We select various representative generation methods as baselines for this\ntask and conduct evaluations, which illustrates that our dataset plays an\noutstanding role in assisting models in generating different complexity-level\ndefinitions. We believe that the COMPILING dataset will benefit further\nresearch in complexity controllable definition generation.", "published": "2022-09-29 08:17:53", "link": "http://arxiv.org/abs/2209.14614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for\n  Explainable Fake News Detection", "abstract": "Existing fake news detection methods aim to classify a piece of news as true\nor false and provide veracity explanations, achieving remarkable performances.\nHowever, they often tailor automated solutions on manual fact-checked reports,\nsuffering from limited news coverage and debunking delays. When a piece of news\nhas not yet been fact-checked or debunked, certain amounts of relevant raw\nreports are usually disseminated on various media outlets, containing the\nwisdom of crowds to verify the news claim and explain its verdict. In this\npaper, we propose a novel Coarse-to-fine Cascaded Evidence-Distillation\n(CofCED) neural network for explainable fake news detection based on such raw\nreports, alleviating the dependency on fact-checked ones. Specifically, we\nfirst utilize a hierarchical encoder for web text representation, and then\ndevelop two cascaded selectors to select the most explainable sentences for\nverdicts on top of the selected top-K reports in a coarse-to-fine manner.\nBesides, we construct two explainable fake news datasets, which are publicly\navailable. Experimental results demonstrate that our model significantly\noutperforms state-of-the-art baselines and generates high-quality explanations\nfrom diverse evaluation perspectives.", "published": "2022-09-29 09:05:47", "link": "http://arxiv.org/abs/2209.14642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GROOT: Corrective Reward Optimization for Generative Sequential Labeling", "abstract": "Sequential labeling is a fundamental NLP task, forming the backbone of many\napplications. Supervised learning of Seq2Seq models has shown great success on\nthese problems. However, the training objectives are still significantly\ndisconnected with the metrics and desiderata we care about in practice. For\nexample, a practical sequence tagging application may want to optimize for a\ncertain precision-recall trade-off (of the top-k predictions) which is quite\ndifferent from the standard objective of maximizing the likelihood of the gold\nlabeled sequence. Thus to bridge this gap, we propose GROOT -- a simple yet\neffective framework for Generative Reward Optimization Of Text sequences. GROOT\nworks by training a generative sequential labeling model to match the decoder\noutput distribution with that of the (black-box) reward function. Using an\niterative training regime, we first generate prediction candidates, then\ncorrect errors in them, and finally contrast those candidates (based on their\nreward values). As demonstrated via extensive experiments on four public\nbenchmarks, GROOT significantly improves all reward metrics. Furthermore, GROOT\nleads to improvements of the overall decoder distribution as evidenced by the\nquality gains of the top-$k$ candidates.", "published": "2022-09-29 11:35:47", "link": "http://arxiv.org/abs/2209.14694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perturbations and Subpopulations for Testing Robustness in Token-Based\n  Argument Unit Recognition", "abstract": "Argument Unit Recognition and Classification aims at identifying argument\nunits from text and classifying them as pro or against. One of the design\nchoices that need to be made when developing systems for this task is what the\nunit of classification should be: segments of tokens or full sentences.\nPrevious research suggests that fine-tuning language models on the token-level\nyields more robust results for classifying sentences compared to training on\nsentences directly. We reproduce the study that originally made this claim and\nfurther investigate what exactly token-based systems learned better compared to\nsentence-based ones. We develop systematic tests for analysing the behavioural\ndifferences between the token-based and the sentence-based system. Our results\nshow that token-based models are generally more robust than sentence-based\nmodels both on manually perturbed examples and on specific subpopulations of\nthe data.", "published": "2022-09-29 13:44:28", "link": "http://arxiv.org/abs/2209.14780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TERMinator: A system for scientific texts processing", "abstract": "This paper is devoted to the extraction of entities and semantic relations\nbetween them from scientific texts, where we consider scientific terms as\nentities. In this paper, we present a dataset that includes annotations for two\ntasks and develop a system called TERMinator for the study of the influence of\nlanguage models on term recognition and comparison of different approaches for\nrelation extraction. Experiments show that language models pre-trained on the\ntarget language are not always show the best performance. Also adding some\nheuristic approaches may improve the overall quality of the particular task.\nThe developed tool and the annotated corpus are publicly available at\nhttps://github.com/iis-research-team/terminator and may be useful for other\nresearchers.", "published": "2022-09-29 15:14:42", "link": "http://arxiv.org/abs/2209.14854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generate-and-Retrieve: use your predictions to improve retrieval for\n  semantic parsing", "abstract": "A common recent approach to semantic parsing augments sequence-to-sequence\nmodels by retrieving and appending a set of training samples, called exemplars.\nThe effectiveness of this recipe is limited by the ability to retrieve\ninformative exemplars that help produce the correct parse, which is especially\nchallenging in low-resource settings. Existing retrieval is commonly based on\nsimilarity of query and exemplar inputs. We propose GandR, a retrieval\nprocedure that retrieves exemplars for which outputs are also similar.\nGandRfirst generates a preliminary prediction with input-based retrieval. Then,\nit retrieves exemplars with outputs similar to the preliminary prediction which\nare used to generate a final prediction. GandR sets the state of the art on\nmultiple low-resource semantic parsing tasks.", "published": "2022-09-29 16:03:29", "link": "http://arxiv.org/abs/2209.14899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unpacking Large Language Models with Conceptual Consistency", "abstract": "If a Large Language Model (LLM) answers \"yes\" to the question \"Are mountains\ntall?\" then does it know what a mountain is? Can you rely on it responding\ncorrectly or incorrectly to other questions about mountains? The success of\nLarge Language Models (LLMs) indicates they are increasingly able to answer\nqueries like these accurately, but that ability does not necessarily imply a\ngeneral understanding of concepts relevant to the anchor query. We propose\nconceptual consistency to measure a LLM's understanding of relevant concepts.\nThis novel metric measures how well a model can be characterized by finding out\nhow consistent its responses to queries about conceptually relevant background\nknowledge are. To compute it we extract background knowledge by traversing\npaths between concepts in a knowledge base and then try to predict the model's\nresponse to the anchor query from the background knowledge. We investigate the\nperformance of current LLMs in a commonsense reasoning setting using the CSQA\ndataset and the ConceptNet knowledge base. While conceptual consistency, like\nother metrics, does increase with the scale of the LLM used, we find that\npopular models do not necessarily have high conceptual consistency. Our\nanalysis also shows significant variation in conceptual consistency across\ndifferent kinds of relations, concepts, and prompts. This serves as a step\ntoward building models that humans can apply a theory of mind to, and thus\ninteract with intuitively.", "published": "2022-09-29 20:55:57", "link": "http://arxiv.org/abs/2209.15093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MUG: Interactive Multimodal Grounding on User Interfaces", "abstract": "We present MUG, a novel interactive task for multimodal grounding where a\nuser and an agent work collaboratively on an interface screen. Prior works\nmodeled multimodal UI grounding in one round: the user gives a command and the\nagent responds to the command. Yet, in a realistic scenario, a user command can\nbe ambiguous when the target action is inherently difficult to articulate in\nnatural language. MUG allows multiple rounds of interactions such that upon\nseeing the agent responses, the user can give further commands for the agent to\nrefine or even correct its actions. Such interaction is critical for improving\ngrounding performances in real-world use cases. To investigate the problem, we\ncreate a new dataset that consists of 77,820 sequences of human user-agent\ninteraction on mobile interfaces in which 20% involves multiple rounds of\ninteractions. To establish our benchmark, we experiment with a range of\nmodeling variants and evaluation strategies, including both offline and online\nevaluation-the online strategy consists of both human evaluation and automatic\nwith simulators. Our experiments show that allowing iterative interaction\nsignificantly improves the absolute task completion by 18% over the entire test\ndataset and 31% over the challenging subset. Our results lay the foundation for\nfurther investigation of the problem.", "published": "2022-09-29 21:08:18", "link": "http://arxiv.org/abs/2209.15099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConceptNet infused DialoGPT for Underlying Commonsense Understanding and\n  Reasoning in Dialogue Response Generation", "abstract": "The pre-trained conversational models still fail to capture the implicit\ncommonsense (CS) knowledge hidden in the dialogue interaction, even though they\nwere pre-trained with an enormous dataset. In order to build a dialogue agent\nwith CS capability, we firstly inject external knowledge into a pre-trained\nconversational model to establish basic commonsense through efficient Adapter\ntuning (Section 4). Secondly, we propose the ``two-way learning'' method to\nenable the bidirectional relationship between CS knowledge and sentence pairs\nso that the model can generate a sentence given the CS triplets, also generate\nthe underlying CS knowledge given a sentence (Section 5). Finally, we leverage\nthis integrated CS capability to improve open-domain dialogue response\ngeneration so that the dialogue agent is capable of understanding the CS\nknowledge hidden in dialogue history on top of inferring related other\nknowledge to further guide response generation (Section 6). The experiment\nresults demonstrate that CS\\_Adapter fusion helps DialoGPT to be able to\ngenerate series of CS knowledge. And the DialoGPT+CS\\_Adapter response model\nadapted from CommonGen training can generate underlying CS triplets that fits\nbetter to dialogue context.", "published": "2022-09-29 21:42:25", "link": "http://arxiv.org/abs/2209.15109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concepts and Experiments on Psychoanalysis Driven Computing", "abstract": "This research investigates the effective incorporation of the human factor\nand user perception in text-based interactive media. In such contexts, the\nreliability of user texts is often compromised by behavioural and emotional\ndimensions. To this end, several attempts have been made in the state of the\nart, to introduce psychological approaches in such systems, including\ncomputational psycholinguistics, personality traits and cognitive psychology\nmethods.\n  In contrast, our method is fundamentally different since we employ a\npsychoanalysis-based approach; in particular, we use the notion of Lacanian\ndiscourse types, to capture and deeply understand real (possibly elusive)\ncharacteristics, qualities and contents of texts, and evaluate their\nreliability. As far as we know, this is the first time computational methods\nare systematically combined with psychoanalysis. We believe such psychoanalytic\nframework is fundamentally more effective than standard methods, since it\naddresses deeper, quite primitive elements of human personality, behaviour and\nexpression which usually escape methods functioning at \"higher\", conscious\nlayers. In fact, this research is a first attempt to form a new paradigm of\npsychoanalysis-driven interactive technologies, with broader impact and diverse\napplications.\n  To exemplify this generic approach, we apply it to the case-study of fake\nnews detection; we first demonstrate certain limitations of the well-known\nMyers-Briggs Type Indicator (MBTI) personality type method, and then propose\nand evaluate our new method of analysing user texts and detecting fake news\nbased on the Lacanian discourses psychoanalytic approach.", "published": "2022-09-29 19:27:22", "link": "http://arxiv.org/abs/2210.00850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional Language Models Are Also Few-shot Learners", "abstract": "Large language models such as GPT-3 (Brown et al., 2020) can perform\narbitrary tasks without undergoing fine-tuning after being prompted with only a\nfew labeled examples. An arbitrary task can be reformulated as a natural\nlanguage prompt, and a language model can be asked to generate the completion,\nindirectly performing the task in a paradigm known as prompt-based learning. To\ndate, emergent prompt-based learning capabilities have mainly been demonstrated\nfor unidirectional language models. However, bidirectional language models\npre-trained on denoising objectives such as masked language modeling produce\nstronger learned representations for transfer learning. This motivates the\npossibility of prompting bidirectional models, but their pre-training\nobjectives have made them largely incompatible with the existing prompting\nparadigm. We present SAP (Sequential Autoregressive Prompting), a technique\nthat enables the prompting of bidirectional models. Utilizing the machine\ntranslation task as a case study, we prompt the bidirectional mT5 model (Xue et\nal., 2021) with SAP and demonstrate its few-shot and zero-shot translations\noutperform the few-shot translations of unidirectional models like GPT-3 and\nXGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We\nfurther show SAP is effective on question answering and summarization. For the\nfirst time, our results demonstrate prompt-based learning is an emergent\nproperty of a broader class of language models, rather than only unidirectional\nmodels.", "published": "2022-09-29 01:35:57", "link": "http://arxiv.org/abs/2209.14500v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Two-Stage Method for Chinese AMR Parsing", "abstract": "In this paper, we provide a detailed description of our system at CAMRP-2022\nevaluation. We firstly propose a two-stage method to conduct Chinese AMR\nParsing with alignment generation, which includes Concept-Prediction and\nRelation-Prediction stages. Our model achieves 0.7756 and 0.7074 Align-Smatch\nF1 scores on the CAMR 2.0 test set and the blind-test set of CAMRP-2022\nindividually. We also analyze the result and the limitation such as the error\npropagation and class imbalance problem we conclude in the current method. Code\nand the trained models are released at\nhttps://github.com/PKUnlp-icler/Two-Stage-CAMRP for reproduction.", "published": "2022-09-29 02:17:56", "link": "http://arxiv.org/abs/2209.14512v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition in Industrial Tables using Tabular Language\n  Models", "abstract": "Specialized transformer-based models for encoding tabular data have gained\ninterest in academia. Although tabular data is omnipresent in industry,\napplications of table transformers are still missing. In this paper, we study\nhow these models can be applied to an industrial Named Entity Recognition (NER)\nproblem where the entities are mentioned in tabular-structured spreadsheets.\nThe highly technical nature of spreadsheets as well as the lack of labeled data\npresent major challenges for fine-tuning transformer-based models. Therefore,\nwe develop a dedicated table data augmentation strategy based on available\ndomain-specific knowledge graphs. We show that this boosts performance in our\nlow-resource scenario considerably. Further, we investigate the benefits of\ntabular structure as inductive bias compared to tables as linearized sequences.\nOur experiments confirm that a table transformer outperforms other baselines\nand that its tabular inductive bias is vital for convergence of\ntransformer-based models.", "published": "2022-09-29 14:25:44", "link": "http://arxiv.org/abs/2209.14812v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language\n  Processing", "abstract": "The meaningful use of electronic health records (EHR) continues to progress\nin the digital era with clinical decision support systems augmented by\nartificial intelligence. A priority in improving provider experience is to\novercome information overload and reduce the cognitive burden so fewer medical\nerrors and cognitive biases are introduced during patient care. One major type\nof medical error is diagnostic error due to systematic or predictable errors in\njudgment that rely on heuristics. The potential for clinical natural language\nprocessing (cNLP) to model diagnostic reasoning in humans with forward\nreasoning from data to diagnosis and potentially reduce the cognitive burden\nand medical error has not been investigated. Existing tasks to advance the\nscience in cNLP have largely focused on information extraction and named entity\nrecognition through classification tasks. We introduce a novel suite of tasks\ncoined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for\ndeveloping and evaluating cNLP models with clinical diagnostic reasoning\nability. The suite includes six tasks from ten publicly available datasets\naddressing clinical text understanding, medical knowledge reasoning, and\ndiagnosis generation. DR.BENCH is the first clinical suite of tasks designed to\nbe a natural language generation framework to evaluate pre-trained language\nmodels. Experiments with state-of-the-art pre-trained generative language\nmodels using large general domain models and models that were continually\ntrained on a medical corpus demonstrate opportunities for improvement when\nevaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab\nrepository with a systematic approach to load and evaluate models for the cNLP\ncommunity.", "published": "2022-09-29 16:05:53", "link": "http://arxiv.org/abs/2209.14901v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chandojnanam: A Sanskrit Meter Identification and Utilization System", "abstract": "We present Chandoj\\~n\\=anam, a web-based Sanskrit meter (Chanda)\nidentification and utilization system. In addition to the core functionality of\nidentifying meters, it sports a friendly user interface to display the\nscansion, which is a graphical representation of the metrical pattern. The\nsystem supports identification of meters from uploaded images by using optical\ncharacter recognition (OCR) engines in the backend. It is also able to process\nentire text files at a time. The text can be processed in two modes, either by\ntreating it as a list of individual lines, or as a collection of verses. When a\nline or a verse does not correspond exactly to a known meter, Chandoj\\~n\\=anam\nis capable of finding fuzzy (i.e., approximate and close) matches based on\nsequence matching. This opens up the scope of a meter-based correction of\nerroneous digital corpora. The system is available for use at\nhttps://sanskrit.iitk.ac.in/jnanasangraha/chanda/, and the source code in the\nform of a Python library is made available at\nhttps://github.com/hrishikeshrt/chanda/.", "published": "2022-09-29 16:43:27", "link": "http://arxiv.org/abs/2209.14924v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Co-Writing Screenplays and Theatre Scripts with Language Models: An\n  Evaluation by Industry Professionals", "abstract": "Language models are increasingly attracting interest from writers. However,\nsuch models lack long-range semantic coherence, limiting their usefulness for\nlongform creative writing. We address this limitation by applying language\nmodels hierarchically, in a system we call Dramatron. By building structural\ncontext via prompt chaining, Dramatron can generate coherent scripts and\nscreenplays complete with title, characters, story beats, location\ndescriptions, and dialogue. We illustrate Dramatron's usefulness as an\ninteractive co-creative system with a user study of 15 theatre and film\nindustry professionals. Participants co-wrote theatre scripts and screenplays\nwith Dramatron and engaged in open-ended interviews. We report critical\nreflections both from our interviewees and from independent reviewers who\nwatched stagings of the works to illustrate how both Dramatron and hierarchical\ntext generation could be useful for human-machine co-creativity. Finally, we\ndiscuss the suitability of Dramatron for co-creativity, ethical considerations\n-- including plagiarism and bias -- and participatory models for the design and\ndeployment of such tools.", "published": "2022-09-29 17:26:22", "link": "http://arxiv.org/abs/2209.14958v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Compositional Semantic Parsing with Large Language Models", "abstract": "Humans can reason compositionally when presented with new tasks. Previous\nresearch shows that appropriate prompting techniques enable large language\nmodels (LLMs) to solve artificial compositional generalization tasks such as\nSCAN. In this work, we identify additional challenges in more realistic\nsemantic parsing tasks with larger vocabulary and refine these prompting\ntechniques to address them. Our best method is based on least-to-most\nprompting: it decomposes the problem using prompting-based syntactic parsing,\nthen uses this decomposition to select appropriate exemplars and to\nsequentially generate the semantic parse. This method allows us to set a new\nstate of the art for CFQ while requiring only 1% of the training data used by\ntraditional approaches. Due to the general nature of our approach, we expect\nsimilar efforts will lead to new results in other tasks and domains, especially\nfor knowledge-intensive applications.", "published": "2022-09-29 17:58:28", "link": "http://arxiv.org/abs/2209.15003v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured\n  Mathematical Reasoning", "abstract": "Mathematical reasoning, a core ability of human intelligence, presents unique\nchallenges for machines in abstract thinking and logical reasoning. Recent\nlarge pre-trained language models such as GPT-3 have achieved remarkable\nprogress on mathematical reasoning tasks written in text form, such as math\nword problems (MWP). However, it is unknown if the models can handle more\ncomplex problems that involve math reasoning over heterogeneous information,\nsuch as tabular data. To fill the gap, we present Tabular Math Word Problems\n(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that\nrequire mathematical reasoning on both textual and tabular data. Each question\nin TabMWP is aligned with a tabular context, which is presented as an image,\nsemi-structured text, and a structured table. There are two types of questions:\nfree-text and multi-choice, and each problem is annotated with gold solutions\nto reveal the multi-step reasoning process. We evaluate different pre-trained\nmodels on TabMWP, including the GPT-3 model in a few-shot setting. As earlier\nstudies suggest, since few-shot GPT-3 relies on the selection of in-context\nexamples, its performance is unstable and can degrade to near chance. The\nunstable issue is more severe when handling complex problems like TabMWP. To\nmitigate this, we further propose a novel approach, PromptPG, which utilizes\npolicy gradient to learn to select in-context examples from a small amount of\ntraining data and then constructs the corresponding prompt for the test\nexample. Experimental results show that our method outperforms the best\nbaseline by 5.31% on the accuracy metric and reduces the prediction variance\nsignificantly compared to random selection, which verifies its effectiveness in\nselecting in-context examples.", "published": "2022-09-29 08:01:04", "link": "http://arxiv.org/abs/2209.14610v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation", "abstract": "Open-domain dialogue systems aim to interact with humans through natural\nlanguage texts in an open-ended fashion. Despite the recent success of super\nlarge dialogue systems such as ChatGPT, using medium-to-small-sized dialogue\nsystems remains the common practice as they are more lightweight and\naccessible; however, generating diverse dialogue responses is challenging,\nespecially with smaller models. In this work, we propose an Equal-size Hard\nExpectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model\nfor diverse dialogue generation. Our algorithm assigns a sample to a decoder in\na hard manner and additionally imposes an equal-assignment constraint to ensure\nthat all decoders are well-trained. We provide detailed theoretical analysis to\njustify our approach. Further, experiments on two large-scale open-domain\ndialogue datasets verify that our EqHard-EM algorithm generates high-quality\ndiverse responses.", "published": "2022-09-29 08:41:32", "link": "http://arxiv.org/abs/2209.14627v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Domain-aware Self-supervised Pre-training for Label-Efficient Meme\n  Analysis", "abstract": "Existing self-supervised learning strategies are constrained to either a\nlimited set of objectives or generic downstream tasks that predominantly target\nuni-modal applications. This has isolated progress for imperative multi-modal\napplications that are diverse in terms of complexity and domain-affinity, such\nas meme analysis. Here, we introduce two self-supervised pre-training methods,\nnamely Ext-PIE-Net and MM-SimCLR that (i) employ off-the-shelf multi-modal\nhate-speech data during pre-training and (ii) perform self-supervised learning\nby incorporating multiple specialized pretext tasks, effectively catering to\nthe required complex multi-modal representation learning for meme analysis. We\nexperiment with different self-supervision strategies, including potential\nvariants that could help learn rich cross-modality representations and evaluate\nusing popular linear probing on the Hateful Memes task. The proposed solutions\nstrongly compete with the fully supervised baseline via label-efficient\ntraining while distinctly outperforming them on all three tasks of the Memotion\nchallenge with 0.18%, 23.64%, and 0.93% performance gain, respectively.\nFurther, we demonstrate the generalizability of the proposed solutions by\nreporting competitive performance on the HarMeme task. Finally, we empirically\nestablish the quality of the learned representations by analyzing task-specific\nlearning, using fewer labeled training samples, and arguing that the complexity\nof the self-supervision strategy and downstream task at hand are correlated.\nOur efforts highlight the requirement of better multi-modal self-supervision\nmethods involving specialized pretext tasks for efficient fine-tuning and\ngeneralizable performance.", "published": "2022-09-29 10:00:29", "link": "http://arxiv.org/abs/2209.14667v1", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "ConvRNN-T: Convolutional Augmented Recurrent Neural Network Transducers\n  for Streaming Speech Recognition", "abstract": "The recurrent neural network transducer (RNN-T) is a prominent streaming\nend-to-end (E2E) ASR technology. In RNN-T, the acoustic encoder commonly\nconsists of stacks of LSTMs. Very recently, as an alternative to LSTM layers,\nthe Conformer architecture was introduced where the encoder of RNN-T is\nreplaced with a modified Transformer encoder composed of convolutional layers\nat the frontend and between attention layers. In this paper, we introduce a new\nstreaming ASR model, Convolutional Augmented Recurrent Neural Network\nTransducers (ConvRNN-T) in which we augment the LSTM-based RNN-T with a novel\nconvolutional frontend consisting of local and global context CNN encoders.\nConvRNN-T takes advantage of causal 1-D convolutional layers,\nsqueeze-and-excitation, dilation, and residual blocks to provide both global\nand local audio context representation to LSTM layers. We show ConvRNN-T\noutperforms RNN-T, Conformer, and ContextNet on Librispeech and in-house data.\nIn addition, ConvRNN-T offers less computational complexity compared to\nConformer. ConvRNN-T's superior accuracy along with its low footprint make it a\npromising candidate for on-device streaming ASR technologies.", "published": "2022-09-29 15:33:41", "link": "http://arxiv.org/abs/2209.14868v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Few-shot Text Classification with Dual Contrastive Consistency", "abstract": "In this paper, we explore how to utilize pre-trained language model to\nperform few-shot text classification where only a few annotated examples are\ngiven for each class. Since using traditional cross-entropy loss to fine-tune\nlanguage model under this scenario causes serious overfitting and leads to\nsub-optimal generalization of model, we adopt supervised contrastive learning\non few labeled data and consistency-regularization on vast unlabeled data.\nMoreover, we propose a novel contrastive consistency to further boost model\nperformance and refine sentence representation. After conducting extensive\nexperiments on four datasets, we demonstrate that our model (FTCC) can\noutperform state-of-the-art methods and has better robustness.", "published": "2022-09-29 19:26:23", "link": "http://arxiv.org/abs/2209.15069v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to tackle an emerging topic? Combining strong and weak labels for\n  Covid news NER", "abstract": "Being able to train Named Entity Recognition (NER) models for emerging topics\nis crucial for many real-world applications especially in the medical domain\nwhere new topics are continuously evolving out of the scope of existing models\nand datasets. For a realistic evaluation setup, we introduce a novel COVID-19\nnews NER dataset (COVIDNEWS-NER) and release 3000 entries of hand annotated\nstrongly labelled sentences and 13000 auto-generated weakly labelled sentences.\nBesides the dataset, we propose CONTROSTER, a recipe to strategically combine\nweak and strong labels in improving NER in an emerging topic through transfer\nlearning. We show the effectiveness of CONTROSTER on COVIDNEWS-NER while\nproviding analysis on combining weak and strong labels for training. Our key\nfindings are: (1) Using weak data to formulate an initial backbone before\ntuning on strong data outperforms methods trained on only strong or weak data.\n(2) A combination of out-of-domain and in-domain weak label training is crucial\nand can overcome saturation when being training on weak labels from a single\nsource.", "published": "2022-09-29 21:33:02", "link": "http://arxiv.org/abs/2209.15108v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Creative Painting with Latent Diffusion Models", "abstract": "Artistic painting has achieved significant progress during recent years.\nUsing an autoencoder to connect the original images with compressed latent\nspaces and a cross attention enhanced U-Net as the backbone of diffusion,\nlatent diffusion models (LDMs) have achieved stable and high fertility image\ngeneration. In this paper, we focus on enhancing the creative painting ability\nof current LDMs in two directions, textual condition extension and model\nretraining with Wikiart dataset. Through textual condition extension, users'\ninput prompts are expanded with rich contextual knowledge for deeper\nunderstanding and explaining the prompts. Wikiart dataset contains 80K famous\nartworks drawn during recent 400 years by more than 1,000 famous artists in\nrich styles and genres. Through the retraining, we are able to ask these\nartists to draw novel and creative painting on modern topics. Direct\ncomparisons with the original model show that the creativity and artistry are\nenriched.", "published": "2022-09-29 11:49:07", "link": "http://arxiv.org/abs/2209.14697v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Detection of Prosodic Boundaries in Speech Using Wav2Vec 2.0", "abstract": "Prosodic boundaries in speech are of great relevance to both speech synthesis\nand audio annotation. In this paper, we apply the wav2vec 2.0 framework to the\ntask of detecting these boundaries in speech signal, using only acoustic\ninformation. We test the approach on a set of recordings of Czech broadcast\nnews, labeled by phonetic experts, and compare it to an existing text-based\npredictor, which uses the transcripts of the same data. Despite using a\nrelatively small amount of labeled data, the wav2vec2 model achieves an\naccuracy of 94% and F1 measure of 83% on within-sentence prosodic boundaries\n(or 95% and 89% on all prosodic boundaries), outperforming the text-based\napproach. However, by combining the outputs of the two different models we can\nimprove the results even further.", "published": "2022-09-29 18:12:26", "link": "http://arxiv.org/abs/2209.15032v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Classification of Vocal Bursts for ACII 2022 A-VB-Type Competition using\n  Convolutional Neural Networks and Deep Acoustic Embeddings", "abstract": "This report provides a brief description of our proposed solution for the\nVocal Burst Type classification task of the ACII 2022 Affective Vocal Bursts\n(A-VB) Competition. We experimented with two approaches as part of our solution\nfor the task at hand. The first of which is based on convolutional neural\nnetworks trained on Mel Spectrograms, and the second is based on average\npooling of deep acoustic embeddings from a pretrained wav2vec2 model. Our best\nperforming model achieves an unweighted average recall (UAR) of 0.5190 for the\ntest partition, compared to the chance-level UAR of 0.1250 and a baseline of\n0.4172. Thus, an improvement of around 20% over the challenge baseline. The\nresults reported in this document demonstrate the efficacy of our proposed\napproaches to solve the AV-B Type Classification task.", "published": "2022-09-29 14:58:23", "link": "http://arxiv.org/abs/2209.14842v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
