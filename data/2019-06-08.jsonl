{"title": "Clinical Concept Extraction for Document-Level Coding", "abstract": "The text of clinical notes can be a valuable source of patient information\nand clinical assessments. Historically, the primary approach for exploiting\nclinical notes has been information extraction: linking spans of text to\nconcepts in a detailed domain ontology. However, recent work has demonstrated\nthe potential of supervised machine learning to extract document-level codes\ndirectly from the raw text of clinical notes. We propose to bridge the gap\nbetween the two approaches with two novel syntheses: (1) treating extracted\nconcepts as features, which are used to supplement or replace the text of the\nnote; (2) treating extracted concepts as labels, which are used to learn a\nbetter representation of the text. Unfortunately, the resulting concepts do not\nyield performance gains on the document-level clinical coding task. We explore\npossible explanations and future research directions.", "published": "2019-06-08 03:32:00", "link": "http://arxiv.org/abs/1906.03380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Low-Resource Cross-lingual Document Retrieval by Reranking\n  with Deep Bilingual Representations", "abstract": "In this paper, we propose to boost low-resource cross-lingual document\nretrieval performance with deep bilingual query-document representations. We\nmatch queries and documents in both source and target languages with four\ncomponents, each of which is implemented as a term interaction-based deep\nneural network with cross-lingual word embeddings as input. By including query\nlikelihood scores as extra features, our model effectively learns to rerank the\nretrieved documents by using a small number of relevance labels for\nlow-resource language pairs. Due to the shared cross-lingual word embedding\nspace, the model can also be directly applied to another language pair without\nany training label. Experimental results on the MATERIAL dataset show that our\nmodel outperforms the competitive translation-based baselines on\nEnglish-Swahili, English-Tagalog, and English-Somali cross-lingual information\nretrieval tasks.", "published": "2019-06-08 17:40:33", "link": "http://arxiv.org/abs/1906.03492v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "This Email Could Save Your Life: Introducing the Task of Email Subject\n  Line Generation", "abstract": "Given the overwhelming number of emails, an effective subject line becomes\nessential to better inform the recipient of the email's content. In this paper,\nwe propose and study the task of email subject line generation: automatically\ngenerating an email subject line from the email body. We create the first\ndataset for this task and find that email subject line generation favor\nextremely abstractive summary which differentiates it from news headline\ngeneration or news single document summarization. We then develop a novel deep\nlearning method and compare it to several baselines as well as recent\nstate-of-the-art text summarization systems. We also investigate the efficacy\nof several automatic metrics based on correlations with human judgments and\npropose a new automatic evaluation metric. Our system outperforms competitive\nbaselines given both automatic and human evaluations. To our knowledge, this is\nthe first work to tackle the problem of effective email subject line\ngeneration.", "published": "2019-06-08 18:31:33", "link": "http://arxiv.org/abs/1906.03497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Centrality Revisited for Unsupervised Summarization", "abstract": "Single document summarization has enjoyed renewed interests in recent years\nthanks to the popularity of neural network models and the availability of\nlarge-scale datasets. In this paper we develop an unsupervised approach arguing\nthat it is unrealistic to expect large-scale and high-quality training data to\nbe available or created for different types of summaries, domains, or\nlanguages. We revisit a popular graph-based ranking algorithm and modify how\nnode (aka sentence) centrality is computed in two ways: (a)~we employ BERT, a\nstate-of-the-art neural representation learning model to better capture\nsentential meaning and (b)~we build graphs with directed edges arguing that the\ncontribution of any two nodes to their respective centrality is influenced by\ntheir relative position in a document. Experimental results on three news\nsummarization datasets representative of different languages and writing styles\nshow that our approach outperforms strong baselines by a wide margin.", "published": "2019-06-08 19:27:31", "link": "http://arxiv.org/abs/1906.03508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeing Things from a Different Angle: Discovering Diverse Perspectives\n  about Claims", "abstract": "One key consequence of the information revolution is a significant increase\nand a contamination of our information supply. The practice of fact checking\nwon't suffice to eliminate the biases in text data we observe, as the degree of\nfactuality alone does not determine whether biases exist in the spectrum of\nopinions visible to us. To better understand controversial issues, one needs to\nview them from a diverse yet comprehensive set of perspectives. For example,\nthere are many ways to respond to a claim such as \"animals should have lawful\nrights\", and these responses form a spectrum of perspectives, each with a\nstance relative to this claim and, ideally, with evidence supporting it.\nInherently, this is a natural language understanding task, and we propose to\naddress it as such. Specifically, we propose the task of substantiated\nperspective discovery where, given a claim, a system is expected to discover a\ndiverse set of well-corroborated perspectives that take a stance with respect\nto the claim. Each perspective should be substantiated by evidence paragraphs\nwhich summarize pertinent results and facts. We construct PERSPECTRUM, a\ndataset of claims, perspectives and evidence, making use of online debate\nwebsites to create the initial data collection, and augmenting it using search\nengines in order to expand and diversify our dataset. We use crowd-sourcing to\nfilter out noise and ensure high-quality data. Our dataset contains 1k claims,\naccompanied with pools of 10k and 8k perspective sentences and evidence\nparagraphs, respectively. We provide a thorough analysis of the dataset to\nhighlight key underlying language understanding challenges, and show that human\nbaselines across multiple subtasks far outperform ma-chine baselines built upon\nstate-of-the-art NLP techniques. This poses a challenge and opportunity for the\nNLP community to address.", "published": "2019-06-08 23:18:07", "link": "http://arxiv.org/abs/1906.03538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Contextualized Biomedical Abbreviation Expansion", "abstract": "Automatic identification and expansion of ambiguous abbreviations are\nessential for biomedical natural language processing applications, such as\ninformation retrieval and question answering systems. In this paper, we present\nDEep Contextualized Biomedical. Abbreviation Expansion (DECBAE) model. DECBAE\nautomatically collects substantial and relatively clean annotated contexts for\n950 ambiguous abbreviations from PubMed abstracts using a simple heuristic.\nThen it utilizes BioELMo to extract the contextualized features of words, and\nfeed those features to abbreviation-specific bidirectional LSTMs, where the\nhidden states of the ambiguous abbreviations are used to assign the exact\ndefinitions. Our DECBAE model outperforms other baselines by large margins,\nachieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset. It\nalso surpasses human performance for expanding a sample abbreviation, and\nremains robust in imbalanced, low-resources and clinical settings.", "published": "2019-06-08 00:01:08", "link": "http://arxiv.org/abs/1906.03360v1", "categories": ["cs.CL", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Making Asynchronous Stochastic Gradient Descent Work for Transformers", "abstract": "Asynchronous stochastic gradient descent (SGD) is attractive from a speed\nperspective because workers do not wait for synchronization. However, the\nTransformer model converges poorly with asynchronous SGD, resulting in\nsubstantially lower quality compared to synchronous SGD. To investigate why\nthis is the case, we isolate differences between asynchronous and synchronous\nmethods to investigate batch size and staleness effects. We find that summing\nseveral asynchronous updates, rather than applying them immediately, restores\nconvergence behavior. With this hybrid method, Transformer training for neural\nmachine translation task reaches a near-convergence level 1.36x faster in\nsingle-node multi-GPU training with no impact on model quality.", "published": "2019-06-08 18:19:50", "link": "http://arxiv.org/abs/1906.03496v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain Adaptive Dialog Generation via Meta Learning", "abstract": "Domain adaptation is an essential task in dialog system building because\nthere are so many new dialog tasks created for different needs every day.\nCollecting and annotating training data for these new tasks is costly since it\ninvolves real user interactions. We propose a domain adaptive dialog generation\nmethod based on meta-learning (DAML). DAML is an end-to-end trainable dialog\nsystem model that learns from multiple rich-resource tasks and then adapts to\nnew domains with minimal training samples. We train a dialog system model using\nmultiple rich-resource single-domain dialog data by applying the model-agnostic\nmeta-learning algorithm to dialog domain. The model is capable of learning a\ncompetitive dialog system on a new domain with only a few training examples in\nan efficient manner. The two-step gradient updates in DAML enable the model to\nlearn general features across multiple tasks. We evaluate our method on a\nsimulated dialog dataset and achieve state-of-the-art performance, which is\ngeneralizable to new tasks.", "published": "2019-06-08 20:54:02", "link": "http://arxiv.org/abs/1906.03520v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detection and Prediction of Users Attitude Based on Real-Time and Batch\n  Sentiment Analysis of Facebook Comments", "abstract": "The most of the people have their account on social networks (e.g. Facebook,\nVkontakte) where they express their attitude to different situations and\nevents. Facebook provides only the positive mark as a like button and share.\nHowever, it is important to know the position of a certain user on posts even\nthough the opinion is negative. Positive, negative and neutral attitude can be\nextracted from the comments of users. Overall information about positive,\nnegative and neutral opinion can bring the understanding of how people react in\na position. Moreover, it is important to know how attitude is changing during\nthe time period. The contribution of the paper is a new method based on\nsentiment text analysis for detection and prediction negative and positive\npatterns for Facebook comments which combines (i) real-time sentiment text\nanalysis for pattern discovery and (ii) batch data processing for creating\nopinion forecasting algorithm. To perform forecast we propose two-steps\nalgorithm where: (i) patterns are clustered using unsupervised clustering\ntechniques and (ii) trend prediction is performed based on finding the nearest\npattern from the certain cluster. Case studies show the efficiency and accuracy\n(Avg. MAE = 0.008) of the proposed method and its practical applicability.\nAlso, we discovered three types of users attitude patterns and described them.", "published": "2019-06-08 05:10:28", "link": "http://arxiv.org/abs/1906.03392v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Effective Use of Variational Embedding Capacity in Expressive End-to-End\n  Speech Synthesis", "abstract": "Recent work has explored sequence-to-sequence latent variable models for\nexpressive speech synthesis (supporting control and transfer of prosody and\nstyle), but has not presented a coherent framework for understanding the\ntrade-offs between the competing methods. In this paper, we propose embedding\ncapacity (the amount of information the embedding contains about the data) as a\nunified method of analyzing the behavior of latent variable models of speech,\ncomparing existing heuristic (non-variational) methods to variational methods\nthat are able to explicitly constrain capacity using an upper bound on\nrepresentational mutual information. In our proposed model (Capacitron), we\nshow that by adding conditional dependencies to the variational posterior such\nthat it matches the form of the true posterior, the same model can be used for\nhigh-precision prosody transfer, text-agnostic style transfer, and generation\nof natural-sounding prior samples. For multi-speaker models, Capacitron is able\nto preserve target speaker identity during inter-speaker prosody transfer and\nwhen drawing samples from the latent prior. Lastly, we introduce a method for\ndecomposing embedding capacity hierarchically across two sets of latents,\nallowing a portion of the latent variability to be specified and the remaining\nvariability sampled from a learned prior. Audio examples are available on the\nweb.", "published": "2019-06-08 06:59:56", "link": "http://arxiv.org/abs/1906.03402v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "News Labeling as Early as Possible: Real or Fake?", "abstract": "Making disguise between real and fake news propagation through online social\nnetworks is an important issue in many applications. The time gap between the\nnews release time and detection of its label is a significant step towards\nbroadcasting the real information and avoiding the fake. Therefore, one of the\nchallenging tasks in this area is to identify fake and real news in early\nstages of propagation. However, there is a trade-off between minimizing the\ntime gap and maximizing accuracy. Despite recent efforts in detection of fake\nnews, there has been no significant work that explicitly incorporates early\ndetection in its model. In this paper, we focus on accurate early labeling of\nnews, and propose a model by considering earliness both in modeling and\nprediction. The proposed method utilizes recurrent neural networks with a novel\nloss function, and a new stopping rule. Given the context of news, we first\nembed it with a class-specific text representation. Then, we utilize the\navailable public profile of users, and speed of news diffusion, for early\nlabeling of the news. Experiments on real datasets demonstrate the\neffectiveness of our model both in terms of early labelling and accuracy,\ncompared to the state of the art baseline and models.", "published": "2019-06-08 08:56:02", "link": "http://arxiv.org/abs/1906.03423v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Forward and Backward Knowledge Transfer for Sentiment Classification", "abstract": "This paper studies the problem of learning a sequence of sentiment\nclassification tasks. The learned knowledge from each task is retained and used\nto help future or subsequent task learning. This learning paradigm is called\nLifelong Learning (LL). However, existing LL methods either only transfer\nknowledge forward to help future learning and do not go back to improve the\nmodel of a previous task or require the training data of the previous task to\nretrain its model to exploit backward/reverse knowledge transfer. This paper\nstudies reverse knowledge transfer of LL in the context of naive Bayesian (NB)\nclassification. It aims to improve the model of a previous task by leveraging\nfuture knowledge without retraining using its training data. This is done by\nexploiting a key characteristic of the generative model of NB. That is, it is\npossible to improve the NB classifier for a task by improving its model\nparameters directly by using the retained knowledge from other tasks.\nExperimental results show that the proposed method markedly outperforms\nexisting LL baselines.", "published": "2019-06-08 19:18:18", "link": "http://arxiv.org/abs/1906.03506v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Adversarial Mahalanobis Distance-based Attentive Song Recommender for\n  Automatic Playlist Continuation", "abstract": "In this paper, we aim to solve the automatic playlist continuation (APC)\nproblem by modeling complex interactions among users, playlists, and songs\nusing only their interaction data. Prior methods mainly rely on dot product to\naccount for similarities, which is not ideal as dot product is not metric\nlearning, so it does not convey the important inequality property. Based on\nthis observation, we propose three novel deep learning approaches that utilize\nMahalanobis distance. Our first approach uses user-playlist-song interactions,\nand combines Mahalanobis distance scores between (i) a target user and a target\nsong, and (ii) between a target playlist and the target song to account for\nboth the user's preference and the playlist's theme. Our second approach\nmeasures song-song similarities by considering Mahalanobis distance scores\nbetween the target song and each member song (i.e., existing song) in the\ntarget playlist. The contribution of each distance score is measured by our\nproposed memory metric-based attention mechanism. In the third approach, we\nfuse the two previous models into a unified model to further enhance their\nperformance. In addition, we adopt and customize Adversarial Personalized\nRanking (APR) for our three approaches to further improve their robustness and\npredictive capabilities. Through extensive experiments, we show that our\nproposed models outperform eight state-of-the-art models in two large-scale\nreal-world datasets.", "published": "2019-06-08 12:53:23", "link": "http://arxiv.org/abs/1906.03450v1", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
