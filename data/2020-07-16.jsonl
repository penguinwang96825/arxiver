{"title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with\n  Logical Reasoning", "abstract": "Machine reading is a fundamental task for testing the capability of natural\nlanguage understanding, which is closely related to human cognition in many\naspects. With the rising of deep learning techniques, algorithmic models rival\nhuman performances on simple QA, and thus increasingly challenging machine\nreading datasets have been proposed. Though various challenges such as evidence\nintegration and commonsense knowledge have been integrated, one of the\nfundamental capabilities in human reading, namely logical reasoning, is not\nfully investigated. We build a comprehensive dataset, named LogiQA, which is\nsourced from expert-written questions for testing human Logical reasoning. It\nconsists of 8,678 QA instances, covering multiple types of deductive reasoning.\nResults show that state-of-the-art neural models perform by far worse than\nhuman ceiling. Our dataset can also serve as a benchmark for reinvestigating\nlogical AI under the deep learning NLP setting. The dataset is freely available\nat https://github.com/lgw863/LogiQA-dataset", "published": "2020-07-16 05:52:16", "link": "http://arxiv.org/abs/2007.08124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SLK-NER: Exploiting Second-order Lexicon Knowledge for Chinese NER", "abstract": "Although character-based models using lexicon have achieved promising results\nfor Chinese named entity recognition (NER) task, some lexical words would\nintroduce erroneous information due to wrongly matched words. Existing\nresearches proposed many strategies to integrate lexicon knowledge. However,\nthey performed with simple first-order lexicon knowledge, which provided\ninsufficient word information and still faced the challenge of matched word\nboundary conflicts; or explored the lexicon knowledge with graph where\nhigher-order information introducing negative words may disturb the\nidentification. To alleviate the above limitations, we present new insight into\nsecond-order lexicon knowledge (SLK) of each character in the sentence to\nprovide more lexical word information including semantic and word boundary\nfeatures. Based on these, we propose a SLK-based model with a novel strategy to\nintegrate the above lexicon knowledge. The proposed model can exploit more\ndiscernible lexical words information with the help of global context.\nExperimental results on three public datasets demonstrate the validity of SLK.\nThe proposed model achieves more excellent performance than the\nstate-of-the-art comparison methods.", "published": "2020-07-16 15:53:02", "link": "http://arxiv.org/abs/2007.08416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Pretrained Language Models for Graph-to-Text Generation", "abstract": "Graph-to-text generation aims to generate fluent texts from graph-based data.\nIn this paper, we investigate two recently proposed pretrained language models\n(PLMs) and analyze the impact of different task-adaptive pretraining strategies\nfor PLMs in graph-to-text generation. We present a study across three graph\ndomains: meaning representations, Wikipedia knowledge graphs (KGs) and\nscientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art\nresults and that task-adaptive pretraining strategies improve their performance\neven further. In particular, we report new state-of-the-art BLEU scores of\n49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative\nimprovement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,\nwe identify possible reasons for the PLMs' success on graph-to-text tasks. We\nfind evidence that their knowledge about true facts helps them perform well\neven when the input graph representation is reduced to a simple bag of node and\nedge labels.", "published": "2020-07-16 16:05:34", "link": "http://arxiv.org/abs/2007.08426v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Interaction Networks with Rethinking Mechanism for\n  Document-level Sentiment Analysis", "abstract": "Document-level Sentiment Analysis (DSA) is more challenging due to vague\nsemantic links and complicate sentiment information. Recent works have been\ndevoted to leveraging text summarization and have achieved promising results.\nHowever, these summarization-based methods did not take full advantage of the\nsummary including ignoring the inherent interactions between the summary and\ndocument. As a result, they limited the representation to express major points\nin the document, which is highly indicative of the key sentiment. In this\npaper, we study how to effectively generate a discriminative representation\nwith explicit subject patterns and sentiment contexts for DSA. A Hierarchical\nInteraction Networks (HIN) is proposed to explore bidirectional interactions\nbetween the summary and document at multiple granularities and learn\nsubject-oriented document representations for sentiment classification.\nFurthermore, we design a Sentiment-based Rethinking mechanism (SR) by refining\nthe HIN with sentiment label information to learn a more sentiment-aware\ndocument representation. We extensively evaluate our proposed models on three\npublic datasets. The experimental results consistently demonstrate the\neffectiveness of our proposed models and show that HIN-SR outperforms various\nstate-of-the-art methods.", "published": "2020-07-16 16:27:38", "link": "http://arxiv.org/abs/2007.08445v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Experimental Study of The Effects of Position Bias on Emotion\n  CauseExtraction", "abstract": "Emotion Cause Extraction (ECE) aims to identify emotion causes from a\ndocument after annotating the emotion keywords. Some baselines have been\nproposed to address this problem, such as rule-based, commonsense based and\nmachine learning methods. We show, however, that a simple random selection\napproach toward ECE that does not require observing the text achieves similar\nperformance compared to the baselines. We utilized only position information\nrelative to the emotion cause to accomplish this goal. Since position\ninformation alone without observing the text resulted in higher F-measure, we\ntherefore uncovered a bias in the ECE single genre Sina-news benchmark. Further\nanalysis showed that an imbalance of emotional cause location exists in the\nbenchmark, with a majority of cause clauses immediately preceding the central\nemotion clause. We examine the bias from a linguistic perspective, and show\nthat high accuracy rate of current state-of-art deep learning models that\nutilize location information is only evident in datasets that contain such\nposition biases. The accuracy drastically reduced when a dataset with balanced\nlocation distribution is introduced. We therefore conclude that it is the\ninnate bias in this benchmark that caused high accuracy rate of these deep\nlearning models in ECE. We hope that the case study in this paper presents both\na cautionary lesson, as well as a template for further studies, in interpreting\nthe superior fit of deep learning models without checking for bias.", "published": "2020-07-16 08:02:36", "link": "http://arxiv.org/abs/2007.15066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Debiasing Sentence Representations", "abstract": "As natural language processing methods are increasingly deployed in\nreal-world scenarios such as healthcare, legal systems, and social science, it\nbecomes necessary to recognize the role they potentially play in shaping social\nbiases and stereotypes. Previous work has revealed the presence of social\nbiases in widely used word embeddings involving gender, race, religion, and\nother social constructs. While some methods were proposed to debias these\nword-level embeddings, there is a need to perform debiasing at the\nsentence-level given the recent shift towards new contextualized sentence\nrepresentations such as ELMo and BERT. In this paper, we investigate the\npresence of social biases in sentence-level representations and propose a new\nmethod, Sent-Debias, to reduce these biases. We show that Sent-Debias is\neffective in removing biases, and at the same time, preserves performance on\nsentence-level downstream tasks such as sentiment analysis, linguistic\nacceptability, and natural language understanding. We hope that our work will\ninspire future research on characterizing and removing social biases from\nwidely adopted sentence representations for fairer NLP.", "published": "2020-07-16 04:22:30", "link": "http://arxiv.org/abs/2007.08100v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coupling Distant Annotation and Adversarial Training for Cross-Domain\n  Chinese Word Segmentation", "abstract": "Fully supervised neural approaches have achieved significant progress in the\ntask of Chinese word segmentation (CWS). Nevertheless, the performance of\nsupervised models tends to drop dramatically when they are applied to\nout-of-domain data. Performance degradation is caused by the distribution gap\nacross domains and the out of vocabulary (OOV) problem. In order to\nsimultaneously alleviate these two issues, this paper proposes to couple\ndistant annotation and adversarial training for cross-domain CWS. For distant\nannotation, we rethink the essence of \"Chinese words\" and design an automatic\ndistant annotation mechanism that does not need any supervision or pre-defined\ndictionaries from the target domain. The approach could effectively explore\ndomain-specific words and distantly annotate the raw texts for the target\ndomain. For adversarial training, we develop a sentence-level training\nprocedure to perform noise reduction and maximum utilization of the source\ndomain information. Experiments on multiple real-world datasets across various\ndomains show the superiority and robustness of our model, significantly\noutperforming previous state-of-the-art cross-domain CWS methods.", "published": "2020-07-16 08:54:17", "link": "http://arxiv.org/abs/2007.08186v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translate Reverberated Speech to Anechoic Ones: Speech Dereverberation\n  with BERT", "abstract": "Single channel speech dereverberation is considered in this work. Inspired by\nthe recent success of Bidirectional Encoder Representations from Transformers\n(BERT) model in the domain of Natural Language Processing (NLP), we investigate\nits applicability as backbone sequence model to enhance reverberated speech\nsignal. We present a variation of the basic BERT model: a pre-sequence network,\nwhich extracts local spectral-temporal information and/or provides order\ninformation, before the backbone sequence model. In addition, we use\npre-trained neural vocoder for implicit phase reconstruction. To evaluate our\nmethod, we used the data from the 3rd CHiME challenge, and compare our results\nwith other methods. Experiments show that the proposed method outperforms\ntraditional method WPE, and achieve comparable performance with\nstate-of-the-art BLSTM-based sequence models.", "published": "2020-07-16 00:45:27", "link": "http://arxiv.org/abs/2007.08052v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval", "abstract": "The abundance of multimodal data (e.g. social media posts) has inspired\ninterest in cross-modal retrieval methods. Popular approaches rely on a variety\nof metric learning losses, which prescribe what the proximity of image and text\nshould be, in the learned space. However, most prior methods have focused on\nthe case where image and text convey redundant information; in contrast,\nreal-world image-text pairs convey complementary information with little\noverlap. Further, images in news articles and media portray topics in a\nvisually diverse fashion; thus, we need to take special care to ensure a\nmeaningful image representation. We propose novel within-modality losses which\nencourage semantic coherency in both the text and image subspaces, which does\nnot necessarily align with visual coherency. Our method ensures that not only\nare paired images and texts close, but the expected image-image and text-text\nrelationships are also observed. Our approach improves the results of\ncross-modal retrieval on four datasets compared to five baselines.", "published": "2020-07-16 20:32:54", "link": "http://arxiv.org/abs/2007.08617v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enabling Morally Sensitive Robotic Clarification Requests", "abstract": "The design of current natural language oriented robot architectures enables\ncertain architectural components to circumvent moral reasoning capabilities.\nOne example of this is reflexive generation of clarification requests as soon\nas referential ambiguity is detected in a human utterance. As shown in previous\nresearch, this can lead robots to (1) miscommunicate their moral dispositions\nand (2) weaken human perception or application of moral norms within their\ncurrent context. We present a solution to these problems by performing moral\nreasoning on each potential disambiguation of an ambiguous human utterance and\nresponding accordingly, rather than immediately and naively requesting\nclarification. We implement our solution in the DIARC robot architecture,\nwhich, to our knowledge, is the only current robot architecture with both moral\nreasoning and clarification request generation capabilities. We then evaluate\nour method with a human subjects experiment, the results of which indicate that\nour approach successfully ameliorates the two identified concerns.", "published": "2020-07-16 22:12:35", "link": "http://arxiv.org/abs/2007.08670v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Toward Forgetting-Sensitive Referring Expression Generationfor\n  Integrated Robot Architectures", "abstract": "To engage in human-like dialogue, robots require the ability to describe the\nobjects, locations, and people in their environment, a capability known as\n\"Referring Expression Generation.\" As speakers repeatedly refer to similar\nobjects, they tend to re-use properties from previous descriptions, in part to\nhelp the listener, and in part due to cognitive availability of those\nproperties in working memory (WM). Because different theories of working memory\n\"forgetting\" necessarily lead to differences in cognitive availability, we\nhypothesize that they will similarly result in generation of different\nreferring expressions. To design effective intelligent agents, it is thus\nnecessary to determine how different models of forgetting may be differentially\neffective at producing natural human-like referring expressions. In this work,\nwe computationalize two candidate models of working memory forgetting within a\nrobot cognitive architecture, and demonstrate how they lead to cognitive\navailability-based differences in generated referring expressions.", "published": "2020-07-16 22:20:15", "link": "http://arxiv.org/abs/2007.08672v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Hopfield Networks is All You Need", "abstract": "We introduce a modern Hopfield network with continuous states and a\ncorresponding update rule. The new Hopfield network can store exponentially\n(with the dimension of the associative space) many patterns, retrieves the\npattern with one update, and has exponentially small retrieval errors. It has\nthree types of energy minima (fixed points of the update): (1) global fixed\npoint averaging over all patterns, (2) metastable states averaging over a\nsubset of patterns, and (3) fixed points which store a single pattern. The new\nupdate rule is equivalent to the attention mechanism used in transformers. This\nequivalence enables a characterization of the heads of transformer models.\nThese heads perform in the first layers preferably global averaging and in\nhigher layers partial averaging via metastable states. The new modern Hopfield\nnetwork can be integrated into deep learning architectures as layers to allow\nthe storage of and access to raw input data, intermediate results, or learned\nprototypes. These Hopfield layers enable new ways of deep learning, beyond\nfully-connected, convolutional, or recurrent networks, and provide pooling,\nmemory, association, and attention mechanisms. We demonstrate the broad\napplicability of the Hopfield layers across various domains. Hopfield layers\nimproved state-of-the-art on three out of four considered multiple instance\nlearning problems as well as on immune repertoire classification with several\nhundreds of thousands of instances. On the UCI benchmark collections of small\nclassification tasks, where deep learning methods typically struggle, Hopfield\nlayers yielded a new state-of-the-art when compared to different machine\nlearning methods. Finally, Hopfield layers achieved state-of-the-art on two\ndrug design datasets. The implementation is available at:\nhttps://github.com/ml-jku/hopfield-layers", "published": "2020-07-16 17:52:37", "link": "http://arxiv.org/abs/2008.02217v3", "categories": ["cs.NE", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.NE"}
{"title": "Automatic Detection of Cue Points for DJ Mixing", "abstract": "The automatic identification of cue points is a central task in applications\nas diverse as music thumbnailing, mash-ups generation, and DJ mixing. Our focus\nlies in electronic dance music and in specific cue points, the \"switch points\",\nthat make it possible to automatically construct transitions among tracks,\nmimicking what professional DJs do. We present an approach for the detection of\nswitch points that embody a few general rules we established from interviews\nwith professional DJs; the implementation of these rules is based on features\nextraction and novelty analysis. The quality of the generated switch points is\nassessed both by comparing them with a manually annotated dataset that we\ncurated, and by evaluating them individually. We found that about 96\\% of the\npoints generated by our methodology are of good quality for use in a DJ mix.", "published": "2020-07-16 15:45:25", "link": "http://arxiv.org/abs/2007.08411v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Tagging by Cross Filtering Noisy Labels", "abstract": "High quality labeled datasets have allowed deep learning to achieve\nimpressive results on many sound analysis tasks. Yet, it is labor-intensive to\naccurately annotate large amount of audio data, and the dataset may contain\nnoisy labels in the practical settings. Meanwhile, the deep neural networks are\nsusceptive to those incorrect labeled data because of their outstanding\nmemorization ability. In this paper, we present a novel framework, named\nCrossFilter, to combat the noisy labels problem for audio tagging. Multiple\nrepresentations (such as, Logmel and MFCC) are used as the input of our\nframework for providing more complementary information of the audio. Then,\nthough the cooperation and interaction of two neural networks, we divide the\ndataset into curated and noisy subsets by incrementally pick out the possibly\ncorrectly labeled data from the noisy data. Moreover, our approach leverages\nthe multi-task learning on curated and noisy subsets with different loss\nfunction to fully utilize the entire dataset. The noisy-robust loss function is\nemployed to alleviate the adverse effects of incorrect labels. On both the\naudio tagging datasets FSDKaggle2018 and FSDKaggle2019, empirical results\ndemonstrate the performance improvement compared with other competing\napproaches. On FSDKaggle2018 dataset, our method achieves state-of-the-art\nperformance and even surpasses the ensemble models.", "published": "2020-07-16 07:55:04", "link": "http://arxiv.org/abs/2007.08165v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Neural MOS Prediction for Synthesized Speech Using Multi-Task Learning\n  With Spoofing Detection and Spoofing Type Classification", "abstract": "Several studies have proposed deep-learning-based models to predict the mean\nopinion score (MOS) of synthesized speech, showing the possibility of replacing\nhuman raters. However, inter- and intra-rater variability in MOSs makes it hard\nto ensure the high performance of the models. In this paper, we propose a\nmulti-task learning (MTL) method to improve the performance of a MOS prediction\nmodel using the following two auxiliary tasks: spoofing detection (SD) and\nspoofing type classification (STC). Besides, we use the focal loss to maximize\nthe synergy between SD and STC for MOS prediction. Experiments using the MOS\nevaluation results of the Voice Conversion Challenge 2018 show that proposed\nMTL with two auxiliary tasks improves MOS prediction. Our proposed model\nachieves up to 11.6% relative improvement in performance over the baseline\nmodel.", "published": "2020-07-16 11:38:08", "link": "http://arxiv.org/abs/2007.08267v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Device-Robust Acoustic Scene Classification Based on Two-Stage\n  Categorization and Data Augmentation", "abstract": "In this technical report, we present a joint effort of four groups, namely\nGT, USTC, Tencent, and UKE, to tackle Task 1 - Acoustic Scene Classification\n(ASC) in the DCASE 2020 Challenge. Task 1 comprises two different sub-tasks:\n(i) Task 1a focuses on ASC of audio signals recorded with multiple (real and\nsimulated) devices into ten different fine-grained classes, and (ii) Task 1b\nconcerns with classification of data into three higher-level classes using\nlow-complexity solutions. For Task 1a, we propose a novel two-stage ASC system\nleveraging upon ad-hoc score combination of two convolutional neural networks\n(CNNs), classifying the acoustic input according to three classes, and then ten\nclasses, respectively. Four different CNN-based architectures are explored to\nimplement the two-stage classifiers, and several data augmentation techniques\nare also investigated. For Task 1b, we leverage upon a quantization method to\nreduce the complexity of two of our top-accuracy three-classes CNN-based\narchitectures. On Task 1a development data set, an ASC accuracy of 76.9\\% is\nattained using our best single classifier and data augmentation. An accuracy of\n81.9\\% is then attained by a final model fusion of our two-stage ASC\nclassifiers. On Task 1b development data set, we achieve an accuracy of 96.7\\%\nwith a model size smaller than 500KB. Code is available:\nhttps://github.com/MihawkHu/DCASE2020_task1.", "published": "2020-07-16 15:07:14", "link": "http://arxiv.org/abs/2007.08389v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
