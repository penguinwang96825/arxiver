{"title": "Automatic Short Answer Grading and Feedback Using Text Mining Methods", "abstract": "Automatic grading is not a new approach but the need to adapt the latest\ntechnology to automatic grading has become very important. As the technology\nhas rapidly became more powerful on scoring exams and essays, especially from\nthe 1990s onwards, partially or wholly automated grading systems using\ncomputational methods have evolved and have become a major area of research. In\nparticular, the demand of scoring of natural language responses has created a\nneed for tools that can be applied to automatically grade these responses. In\nthis paper, we focus on the concept of automatic grading of short answer\nquestions such as are typical in the UK GCSE system, and providing useful\nfeedback on their answers to students. We present experimental results on a\ndataset provided from the introductory computer science class in the University\nof North Texas. We first apply standard data mining techniques to the corpus of\nstudent answers for the purpose of measuring similarity between the student\nanswers and the model answer. This is based on the number of common words. We\nthen evaluate the relation between these similarities and marks awarded by\nscorers. We then consider an approach that groups student answers into\nclusters. Each cluster would be awarded the same mark, and the same feedback\ngiven to each answer in a cluster. In this manner, we demonstrate that clusters\nindicate the groups of students who are awarded the same or the similar scores.\nWords in each cluster are compared to show that clusters are constructed based\non how many and which words of the model answer have been used. The main\nnovelty in this paper is that we design a model to predict marks based on the\nsimilarities between the student answers and the model answer.", "published": "2018-07-27 12:00:21", "link": "http://arxiv.org/abs/1807.10543v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auto-Encoding Variational Neural Machine Translation", "abstract": "We present a deep generative model of bilingual sentence pairs for machine\ntranslation. The model generates source and target sentences jointly from a\nshared latent representation and is parameterised by neural networks. We\nperform efficient training using amortised variational inference and\nreparameterised gradients. Additionally, we discuss the statistical\nimplications of joint modelling and propose an efficient approximation to\nmaximum a posteriori decoding for fast test-time predictions. We demonstrate\nthe effectiveness of our model in three machine translation scenarios:\nin-domain training, mixed-domain training, and learning from a mix of\ngold-standard and synthetic data. Our experiments show consistently that our\njoint formulation outperforms conditional modelling (i.e. standard neural\nmachine translation) in all such scenarios.", "published": "2018-07-27 13:03:06", "link": "http://arxiv.org/abs/1807.10564v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concept Tagging for Natural Language Understanding: Two Decadelong\n  Algorithm Development", "abstract": "Concept tagging is a type of structured learning needed for natural language\nunderstanding (NLU) systems. In this task, meaning labels from a domain\nontology are assigned to word sequences. In this paper, we review the\nalgorithms developed over the last twenty five years. We perform a comparative\nevaluation of generative, discriminative and deep learning methods on two\npublic datasets. We report on the statistical variability performance\nmeasurements. The third contribution is the release of a repository of the\nalgorithms, datasets and recipes for NLU evaluation.", "published": "2018-07-27 14:40:32", "link": "http://arxiv.org/abs/1807.10661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A small Griko-Italian speech translation corpus", "abstract": "This paper presents an extension to a very low-resource parallel corpus\ncollected in an endangered language, Griko, making it useful for computational\nresearch. The corpus consists of 330 utterances (about 20 minutes of speech)\nwhich have been transcribed and translated in Italian, with annotations for\nword-level speech-to-transcription and speech-to-translation alignments. The\ncorpus also includes morphosyntactic tags and word-level glosses. Applying an\nautomatic unit discovery method, pseudo-phones were also generated. We detail\nhow the corpus was collected, cleaned and processed, and we illustrate its use\non zero-resource tasks by presenting some baseline results for the task of\nspeech-to-translation alignment and unsupervised word discovery. The dataset is\navailable online, aiming to encourage replicability and diversity in\ncomputational language documentation experiments.", "published": "2018-07-27 17:29:20", "link": "http://arxiv.org/abs/1807.10740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering Prominent People and Organizations in Topic-Specific Text\n  Corpora", "abstract": "Named entities in text documents are the names of people, organization,\nlocation or other types of objects in the documents that exist in the real\nworld. A persisting research challenge is to use computational techniques to\nidentify such entities in text documents. Once identified, several text mining\ntools and algorithms can be utilized to leverage these discovered named\nentities and improve NLP applications. In this paper, a method that clusters\nprominent names of people and organizations based on their semantic similarity\nin a text corpus is proposed. The method relies on common named entity\nrecognition techniques and on recent word embeddings models. The semantic\nsimilarity scores generated using the word embeddings models for the named\nentities are used to cluster similar entities of the people and organizations\ntypes. Two human judges evaluated ten variations of the method after it was run\non a corpus that consists of 4,821 articles on a specific topic. The\nperformance of the method was measured using three quantitative measures. The\nresults of these three metrics demonstrate that the method is effective in\nclustering semantically similar named entities.", "published": "2018-07-27 19:00:01", "link": "http://arxiv.org/abs/1807.10800v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of the Usages of Deep Learning in Natural Language Processing", "abstract": "Over the last several years, the field of natural language processing has\nbeen propelled forward by an explosion in the use of deep learning models. This\nsurvey provides a brief introduction to the field and a quick overview of deep\nlearning architectures and methods. It then sifts through the plethora of\nrecent studies and summarizes a large assortment of relevant contributions.\nAnalyzed research areas include several core linguistic processing issues in\naddition to a number of applications of computational linguistics. A discussion\nof the current state of the art is then provided along with recommendations for\nfuture research in the field.", "published": "2018-07-27 23:11:39", "link": "http://arxiv.org/abs/1807.10854v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Neural Sequence Labelling using Additional Linguistic\n  Information", "abstract": "Sequence labelling is the task of assigning categorical labels to a data\nsequence. In Natural Language Processing, sequence labelling can be applied to\nvarious fundamental problems, such as Part of Speech (POS) tagging, Named\nEntity Recognition (NER), and Chunking. In this study, we propose a method to\nadd various linguistic features to the neural sequence framework to improve\nsequence labelling. Besides word level knowledge, sense embeddings are added to\nprovide semantic information. Additionally, selective readings of character\nembeddings are added to capture contextual as well as morphological features\nfor each word in a sentence. Compared to previous methods, these added\nlinguistic features allow us to design a more concise model and perform more\nefficient training. Our proposed architecture achieves state of the art results\non the benchmark datasets of POS, NER, and chunking. Moreover, the convergence\nrate of our model is significantly better than the previous state of the art\nmodels.", "published": "2018-07-27 19:07:33", "link": "http://arxiv.org/abs/1807.10805v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Sentence Embedding using Only In-domain Sentences for\n  Out-of-domain Sentence Detection in Dialog Systems", "abstract": "To ensure satisfactory user experience, dialog systems must be able to\ndetermine whether an input sentence is in-domain (ID) or out-of-domain (OOD).\nWe assume that only ID sentences are available as training data because\ncollecting enough OOD sentences in an unbiased way is a laborious and\ntime-consuming job. This paper proposes a novel neural sentence embedding\nmethod that represents sentences in a low-dimensional continuous vector space\nthat emphasizes aspects that distinguish ID cases from OOD cases. We first used\na large set of unlabeled text to pre-train word representations that are used\nto initialize neural sentence embedding. Then we used domain-category analysis\nas an auxiliary task to train neural sentence embedding for OOD sentence\ndetection. After the sentence representations were learned, we used them to\ntrain an autoencoder aimed at OOD sentence detection. We evaluated our method\nby experimentally comparing it to the state-of-the-art methods in an\neight-domain dialog system; our proposed method achieved the highest accuracy\nin all tests.", "published": "2018-07-27 15:31:15", "link": "http://arxiv.org/abs/1807.11567v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comparison of Techniques for Language Model Integration in\n  Encoder-Decoder Speech Recognition", "abstract": "Attention-based recurrent neural encoder-decoder models present an elegant\nsolution to the automatic speech recognition problem. This approach folds the\nacoustic model, pronunciation model, and language model into a single network\nand requires only a parallel corpus of speech and text for training. However,\nunlike in conventional approaches that combine separate acoustic and language\nmodels, it is not clear how to use additional (unpaired) text. While there has\nbeen previous work on methods addressing this problem, a thorough comparison\namong methods is still lacking. In this paper, we compare a suite of past\nmethods and some of our own proposed methods for using unpaired text data to\nimprove encoder-decoder models. For evaluation, we use the medium-sized\nSwitchboard data set and the large-scale Google voice search and dictation data\nsets. Our results confirm the benefits of using unpaired text across a range of\nmethods and data sets. Surprisingly, for first-pass decoding, the rather simple\napproach of shallow fusion performs best across data sets. However, for Google\ndata sets we find that cold fusion has a lower oracle error rate and\noutperforms other approaches after second-pass rescoring on the Google voice\nsearch data set.", "published": "2018-07-27 23:33:33", "link": "http://arxiv.org/abs/1807.10857v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Hierarchical Approach to Neural Context-Aware Modeling", "abstract": "We present a new recurrent neural network topology to enhance\nstate-of-the-art machine learning systems by incorporating a broader context.\nOur approach overcomes recent limitations with extended narratives through a\nmulti-layered computational approach to generate an abstract context\nrepresentation. Therefore, the developed system captures the narrative on\nword-level, sentence-level, and context-level. Through the hierarchical set-up,\nour proposed model summarizes the most salient information on each level and\ncreates an abstract representation of the extended context. We subsequently use\nthis representation to enhance neural language processing systems on the task\nof semantic error detection. To show the potential of the newly introduced\ntopology, we compare the approach against a context-agnostic set-up including a\nstandard neural language model and a supervised binary classification network.\nThe performance measures on the error detection task show the advantage of the\nhierarchical context-aware topologies, improving the baseline by 12.75%\nrelative for unsupervised models and 20.37% relative for supervised models.", "published": "2018-07-27 11:10:03", "link": "http://arxiv.org/abs/1807.11582v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Identifying Protein-Protein Interaction using Tree LSTM and Structured\n  Attention", "abstract": "Identifying interactions between proteins is important to understand\nunderlying biological processes. Extracting a protein-protein interaction (PPI)\nfrom the raw text is often very difficult. Previous supervised learning methods\nhave used handcrafted features on human-annotated data sets. In this paper, we\npropose a novel tree recurrent neural network with structured attention\narchitecture for doing PPI. Our architecture achieves state of the art results\n(precision, recall, and F1-score) on the AIMed and BioInfer benchmark data\nsets. Moreover, our models achieve a significant improvement over previous best\nmodels without any explicit feature extraction. Our experimental results show\nthat traditional recurrent networks have inferior performance compared to tree\nrecurrent networks for the supervised PPI problem.", "published": "2018-07-27 19:08:12", "link": "http://arxiv.org/abs/1808.03227v1", "categories": ["q-bio.QM", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "q-bio.QM"}
{"title": "Large-Scale Weakly Labeled Semi-Supervised Sound Event Detection in\n  Domestic Environments", "abstract": "This paper presents DCASE 2018 task 4. The task evaluates systems for the\nlarge-scale detection of sound events using weakly labeled data (without time\nboundaries). The target of the systems is to provide not only the event class\nbut also the event time boundaries given that multiple events can be present in\nan audio recording. Another challenge of the task is to explore the possibility\nto exploit a large amount of unbalanced and unlabeled training data together\nwith a small weakly labeled training set to improve system performance. The\ndata are Youtube video excerpts from domestic context which have many\napplications such as ambient assisted living. The domain was chosen due to the\nscientific challenges (wide variety of sounds, time-localized events.. .) and\npotential industrial applications .", "published": "2018-07-27 09:15:50", "link": "http://arxiv.org/abs/1807.10501v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
