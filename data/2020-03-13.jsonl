{"title": "MixPoet: Diverse Poetry Generation via Learning Controllable Mixed\n  Latent Space", "abstract": "As an essential step towards computer creativity, automatic poetry generation\nhas gained increasing attention these years. Though recent neural models make\nprominent progress in some criteria of poetry quality, generated poems still\nsuffer from the problem of poor diversity. Related literature researches show\nthat different factors, such as life experience, historical background, etc.,\nwould influence composition styles of poets, which considerably contributes to\nthe high diversity of human-authored poetry. Inspired by this, we propose\nMixPoet, a novel model that absorbs multiple factors to create various styles\nand promote diversity. Based on a semi-supervised variational autoencoder, our\nmodel disentangles the latent space into some subspaces, with each conditioned\non one influence factor by adversarial training. In this way, the model learns\na controllable latent variable to capture and mix generalized factor-related\nproperties. Different factor mixtures lead to diverse styles and hence further\ndifferentiate generated poems from each other. Experiment results on Chinese\npoetry demonstrate that MixPoet improves both diversity and quality against\nthree state-of-the-art models.", "published": "2020-03-13 03:31:29", "link": "http://arxiv.org/abs/2003.06094v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "WAC: A Corpus of Wikipedia Conversations for Online Abuse Detection", "abstract": "With the spread of online social networks, it is more and more difficult to\nmonitor all the user-generated content. Automating the moderation process of\nthe inappropriate exchange content on Internet has thus become a priority task.\nMethods have been proposed for this purpose, but it can be challenging to find\na suitable dataset to train and develop them. This issue is especially true for\napproaches based on information derived from the structure and the dynamic of\nthe conversation. In this work, we propose an original framework, based on the\nWikipedia Comment corpus, with comment-level abuse annotations of different\ntypes. The major contribution concerns the reconstruction of conversations, by\ncomparison to existing corpora, which focus only on isolated messages (i.e.\ntaken out of their conversational context). This large corpus of more than 380k\nannotated messages opens perspectives for online abuse detection and especially\nfor context-based approaches. We also propose, in addition to this corpus, a\ncomplete benchmarking platform to stimulate and fairly compare scientific works\naround the problem of content abuse detection, trying to avoid the recurring\nproblem of result replication. Finally, we apply two classification methods to\nour dataset to demonstrate its potential.", "published": "2020-03-13 10:26:45", "link": "http://arxiv.org/abs/2003.06190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Review-guided Helpful Answer Identification in E-commerce", "abstract": "Product-specific community question answering platforms can greatly help\naddress the concerns of potential customers. However, the user-provided answers\non such platforms often vary a lot in their qualities. Helpfulness votes from\nthe community can indicate the overall quality of the answer, but they are\noften missing. Accurately predicting the helpfulness of an answer to a given\nquestion and thus identifying helpful answers is becoming a demanding need.\nSince the helpfulness of an answer depends on multiple perspectives instead of\nonly topical relevance investigated in typical QA tasks, common answer\nselection algorithms are insufficient for tackling this task. In this paper, we\npropose the Review-guided Answer Helpfulness Prediction (RAHP) model that not\nonly considers the interactions between QA pairs but also investigates the\nopinion coherence between the answer and crowds' opinions reflected in the\nreviews, which is another important factor to identify helpful answers.\nMoreover, we tackle the task of determining opinion coherence as a language\ninference problem and explore the utilization of pre-training strategy to\ntransfer the textual inference knowledge obtained from a specifically designed\ntrained network. Extensive experiments conducted on real-world data across\nseven product categories show that our proposed model achieves superior\nperformance on the prediction task.", "published": "2020-03-13 11:34:29", "link": "http://arxiv.org/abs/2003.06209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Level Human Translation Quality Estimation with Attention-based\n  Neural Networks", "abstract": "This paper explores the use of Deep Learning methods for automatic estimation\nof quality of human translations. Automatic estimation can provide useful\nfeedback for translation teaching, examination and quality control.\nConventional methods for solving this task rely on manually engineered features\nand external knowledge. This paper presents an end-to-end neural model without\nfeature engineering, incorporating a cross attention mechanism to detect which\nparts in sentence pairs are most relevant for assessing quality. Another\ncontribution concerns of prediction of fine-grained scores for measuring\ndifferent aspects of translation quality. Empirical results on a large human\nannotated dataset show that the neural model outperforms feature-based methods\nsignificantly. The dataset and the tools are available.", "published": "2020-03-13 16:57:55", "link": "http://arxiv.org/abs/2003.06381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Know thy corpus! Robust methods for digital curation of Web corpora", "abstract": "This paper proposes a novel framework for digital curation of Web corpora in\norder to provide robust estimation of their parameters, such as their\ncomposition and the lexicon. In recent years language models pre-trained on\nlarge corpora emerged as clear winners in numerous NLP tasks, but no proper\nanalysis of the corpora which led to their success has been conducted. The\npaper presents a procedure for robust frequency estimation, which helps in\nestablishing the core lexicon for a given corpus, as well as a procedure for\nestimating the corpus composition via unsupervised topic models and via\nsupervised genre classification of Web pages. The results of the digital\ncuration study applied to several Web-derived corpora demonstrate their\nconsiderable differences. First, this concerns different frequency bursts which\nimpact the core lexicon obtained from each corpus. Second, this concerns the\nkinds of texts they contain. For example, OpenWebText contains considerably\nmore topical news and political argumentation in comparison to ukWac or\nWikipedia. The tools and the results of analysis have been released.", "published": "2020-03-13 17:21:57", "link": "http://arxiv.org/abs/2003.06389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Major Types of Chinese Classical Poetry in a Uniformed\n  Framework", "abstract": "Poetry generation is an interesting research topic in the field of text\ngeneration. As one of the most valuable literary and cultural heritages of\nChina, Chinese classical poetry is very familiar and loved by Chinese people\nfrom generation to generation. It has many particular characteristics in its\nlanguage structure, ranging from form, sound to meaning, thus is regarded as an\nideal testing task for text generation. In this paper, we propose a GPT-2 based\nuniformed framework for generating major types of Chinese classical poems. We\ndefine a unified format for formulating all types of training samples by\nintegrating detailed form information, then present a simple form-stressed\nweighting method in GPT-2 to strengthen the control to the form of the\ngenerated poems, with special emphasis on those forms with longer body length.\nPreliminary experimental results show this enhanced model can generate Chinese\nclassical poems of major types with high quality in both form and content,\nvalidating the effectiveness of the proposed strategy. The model has been\nincorporated into Jiuge, the most influential Chinese classical poetry\ngeneration system developed by Tsinghua University (Guo et al., 2019).", "published": "2020-03-13 14:16:25", "link": "http://arxiv.org/abs/2003.11528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Masakhane -- Machine Translation For Africa", "abstract": "Africa has over 2000 languages. Despite this, African languages account for a\nsmall portion of available resources and publications in Natural Language\nProcessing (NLP). This is due to multiple factors, including: a lack of focus\nfrom government and funding, discoverability, a lack of community, sheer\nlanguage complexity, difficulty in reproducing papers and no benchmarks to\ncompare techniques. To begin to address the identified problems, MASAKHANE, an\nopen-source, continent-wide, distributed, online research effort for machine\ntranslation for African languages, was founded. In this paper, we discuss our\nmethodology for building the community and spurring research from the African\ncontinent, as well as outline the success of the community in terms of\naddressing the identified problems affecting African NLP.", "published": "2020-03-13 09:01:02", "link": "http://arxiv.org/abs/2003.11529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using word embeddings to improve the discriminability of co-occurrence\n  text networks", "abstract": "Word co-occurrence networks have been employed to analyze texts both in the\npractical and theoretical scenarios. Despite the relative success in several\napplications, traditional co-occurrence networks fail in establishing links\nbetween similar words whenever they appear distant in the text. Here we\ninvestigate whether the use of word embeddings as a tool to create virtual\nlinks in co-occurrence networks may improve the quality of classification\nsystems. Our results revealed that the discriminability in the stylometry task\nis improved when using Glove, Word2Vec and FastText. In addition, we found that\noptimized results are obtained when stopwords are not disregarded and a simple\nglobal thresholding strategy is used to establish virtual links. Because the\nproposed approach is able to improve the representation of texts as complex\nnetworks, we believe that it could be extended to study other natural language\nprocessing tasks. Likewise, theoretical languages studies could benefit from\nthe adopted enriched representation of word co-occurrence networks.", "published": "2020-03-13 13:35:44", "link": "http://arxiv.org/abs/2003.06279v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "DAN: Dual-View Representation Learning for Adapting Stance Classifiers\n  to New Domains", "abstract": "We address the issue of having a limited number of annotations for stance\nclassification in a new domain, by adapting out-of-domain classifiers with\ndomain adaptation. Existing approaches often align different domains in a\nsingle, global feature space (or view), which may fail to fully capture the\nrichness of the languages used for expressing stances, leading to reduced\nadaptability on stance data. In this paper, we identify two major types of\nstance expressions that are linguistically distinct, and we propose a tailored\ndual-view adaptation network (DAN) to adapt these expressions across domains.\nThe proposed model first learns a separate view for domain transfer in each\nexpression channel and then selects the best adapted parts of both views for\noptimal transfer. We find that the learned view features can be more easily\naligned and more stance-discriminative in either or both views, leading to more\ntransferable overall features after combining the views. Results from extensive\nexperiments show that our method can enhance the state-of-the-art single-view\nmethods in matching stance data across different domains, and that it\nconsistently improves those methods on various adaptation tasks.", "published": "2020-03-13 23:56:37", "link": "http://arxiv.org/abs/2003.06514v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Alignment using String Edit Distance", "abstract": "In this work, we propose a novel knowledge graph alignment technique based\nupon string edit distance that exploits the type information between entities\nand can find similarity between relations of any arity", "published": "2020-03-13 22:11:39", "link": "http://arxiv.org/abs/2003.12145v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Learning Instantiated Logical Rules from Knowledge Graphs", "abstract": "Efficiently inducing high-level interpretable regularities from knowledge\ngraphs (KGs) is an essential yet challenging task that benefits many downstream\napplications. In this work, we present GPFL, a probabilistic rule learner\noptimized to mine instantiated first-order logic rules from KGs. Instantiated\nrules contain constants extracted from KGs. Compared to abstract rules that\ncontain no constants, instantiated rules are capable of explaining and\nexpressing concepts in more details. GPFL utilizes a novel two-stage rule\ngeneration mechanism that first generalizes extracted paths into templates that\nare acyclic abstract rules until a certain degree of template saturation is\nachieved, then specializes the generated templates into instantiated rules.\nUnlike existing works that ground every mined instantiated rule for evaluation,\nGPFL shares groundings between structurally similar rules for collective\nevaluation. Moreover, we reveal the presence of overfitting rules, their impact\non the predictive performance, and the effectiveness of a simple validation\nmethod filtering out overfitting rules. Through extensive experiments on public\nbenchmark datasets, we show that GPFL 1.) significantly reduces the runtime on\nevaluating instantiated rules; 2.) discovers much more quality instantiated\nrules than existing works; 3.) improves the predictive performance of learned\nrules by removing overfitting rules via validation; 4.) is competitive on\nknowledge graph completion task compared to state-of-the-art baselines.", "published": "2020-03-13 00:32:46", "link": "http://arxiv.org/abs/2003.06071v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LSCP: Enhanced Large Scale Colloquial Persian Language Understanding", "abstract": "Language recognition has been significantly advanced in recent years by means\nof modern machine learning methods such as deep learning and benchmarks with\nrich annotations. However, research is still limited in low-resource formal\nlanguages. This consists of a significant gap in describing the colloquial\nlanguage especially for low-resourced ones such as Persian. In order to target\nthis gap for low resource languages, we propose a \"Large Scale Colloquial\nPersian Dataset\" (LSCP). LSCP is hierarchically organized in a semantic\ntaxonomy that focuses on multi-task informal Persian language understanding as\na comprehensive problem. This encompasses the recognition of multiple semantic\naspects in the human-level sentences, which naturally captures from the\nreal-world sentences. We believe that further investigations and processing, as\nwell as the application of novel algorithms and methods, can strengthen\nenriching computerized understanding and processing of low resource languages.\nThe proposed corpus consists of 120M sentences resulted from 27M tweets\nannotated with parsing tree, part-of-speech tags, sentiment polarity and\ntranslation in five different languages.", "published": "2020-03-13 22:24:14", "link": "http://arxiv.org/abs/2003.06499v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50 (Primary) 68T09, 68T07 (Secondary)", "A.0; E.0; I.2.0; I.2.6; I.2.7; I.7.1; I.7.2"], "primary_category": "cs.CL"}
{"title": "Learning to Encode Position for Transformer with Continuous Dynamical\n  Model", "abstract": "We introduce a new way of learning to encode position information for\nnon-recurrent models, such as Transformer models. Unlike RNN and LSTM, which\ncontain inductive bias by loading the input tokens sequentially, non-recurrent\nmodels are less sensitive to position. The main reason is that position\ninformation among input units is not inherently encoded, i.e., the models are\npermutation equivalent; this problem justifies why all of the existing models\nare accompanied by a sinusoidal encoding/embedding layer at the input. However,\nthis solution has clear limitations: the sinusoidal encoding is not flexible\nenough as it is manually designed and does not contain any learnable\nparameters, whereas the position embedding restricts the maximum length of\ninput sequences. It is thus desirable to design a new position layer that\ncontains learnable parameters to adjust to different datasets and different\narchitectures. At the same time, we would also like the encodings to\nextrapolate in accordance with the variable length of inputs. In our proposed\nsolution, we borrow from the recent Neural ODE approach, which may be viewed as\na versatile continuous version of a ResNet. This model is capable of modeling\nmany kinds of dynamical systems. We model the evolution of encoded results\nalong position index by such a dynamical system, thereby overcoming the above\nlimitations of existing methods. We evaluate our new position layers on a\nvariety of neural machine translation and language understanding tasks, the\nexperimental results show consistent improvements over the baselines.", "published": "2020-03-13 00:41:41", "link": "http://arxiv.org/abs/2003.09229v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Predicting Legal Proceedings Status: Approaches Based on Sequential Text\n  Data", "abstract": "The objective of this paper is to develop predictive models to classify\nBrazilian legal proceedings in three possible classes of status: (i) archived\nproceedings, (ii) active proceedings, and (iii) suspended proceedings. This\nproblem's resolution is intended to assist public and private institutions in\nmanaging large portfolios of legal proceedings, providing gains in scale and\nefficiency. In this paper, legal proceedings are made up of sequences of short\ntexts called \"motions.\" We combined several natural language processing (NLP)\nand machine learning techniques to solve the problem. Although working with\nPortuguese NLP, which can be challenging due to lack of resources, our\napproaches performed remarkably well in the classification task, achieving\nmaximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93\n(weighted). Furthermore, we could extract and interpret the patterns learned by\none of our models besides quantifying how those patterns relate to the\nclassification task. The interpretability step is important among machine\nlearning legal applications and gives us an exciting insight into how black-box\nmodels make decisions.", "published": "2020-03-13 19:40:57", "link": "http://arxiv.org/abs/2003.11561v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "End-to-end Recurrent Denoising Autoencoder Embeddings for Speaker\n  Identification", "abstract": "Speech 'in-the-wild' is a handicap for speaker recognition systems due to the\nvariability induced by real-life conditions, such as environmental noise and\nthe emotional state of the speaker. Taking advantage of the principles of\nrepresentation learning, we aim to design a recurrent denoising autoencoder\nthat extracts robust speaker embeddings from noisy spectrograms to perform\nspeaker identification. The end-to-end proposed architecture uses a feedback\nloop to encode information regarding the speaker into low-dimensional\nrepresentations extracted by a spectrogram denoising autoencoder. We employ\ndata augmentation techniques by additively corrupting clean speech with\nreal-life environmental noise in a database containing real stressed speech.\nOur study presents that the joint optimization of both the denoiser and speaker\nidentification modules outperforms independent optimization of both components\nunder stress and noise distortions as well as hand-crafted features.", "published": "2020-03-13 13:12:48", "link": "http://arxiv.org/abs/2003.07688v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Wide Dataset of Ear Shapes and Pinna-Related Transfer Functions\n  Generated by Random Ear Drawings", "abstract": "Head-related transfer functions (HRTFs) individualization is a key matter in\nbinaural synthesis. However, currently available databases are limited in size\ncompared to the high dimensionality of the data. Hereby, we present the process\nof generating a synthetic dataset of 1000 ear shapes and matching sets of\npinna-related transfer functions (PRTFs), named WiDESPREaD (wide dataset of ear\nshapes and pinna-related transfer functions obtained by random ear drawings)\nand made freely available to other researchers. Contributions in this article\nare three-fold. First, from a proprietary dataset of 119 three-dimensional\nleft-ear scans, we build a matching dataset of PRTFs by performing\nfast-multipole boundary element method (FM-BEM) calculations. Second, we\ninvestigate the underlying geometry of each type of high-dimensional data using\nprincipal component analysis (PCA). We find that this linear machine learning\ntechnique performs better at modeling and reducing data dimensionality on ear\nshapes than on matching PRTF sets. Third, based on these findings, we devise a\nmethod to generate an arbitrarily large synthetic database of PRTF sets that\nrelies on the random drawing of ear shapes and subsequent FM-BEM computations.", "published": "2020-03-13 10:10:59", "link": "http://arxiv.org/abs/2003.06182v1", "categories": ["eess.AS", "cs.SD", "physics.class-ph"], "primary_category": "eess.AS"}
{"title": "HRTF Individualization: A Survey", "abstract": "The individuality of head-related transfer functions (HRTFs) is a key issue\nfor binaural synthesis. While, over the years, a lot of work has been\naccomplished to propose end-user-friendly solutions to HRTF personalization, it\nremains a challenge. In this article we establish a state-of-the-art of that\nwork. We classify the various proposed methods, review their respective\nadvantages and disadvantages, and, above all, methodically check if and how the\nperceptual validity of the resulting HRTFs was assessed.", "published": "2020-03-13 10:13:17", "link": "http://arxiv.org/abs/2003.06183v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "ASR Error Correction and Domain Adaptation Using Machine Translation", "abstract": "Off-the-shelf pre-trained Automatic Speech Recognition (ASR) systems are an\nincreasingly viable service for companies of any size building speech-based\nproducts. While these ASR systems are trained on large amounts of data, domain\nmismatch is still an issue for many such parties that want to use this service\nas-is leading to not so optimal results for their task. We propose a simple\ntechnique to perform domain adaptation for ASR error correction via machine\ntranslation. The machine translation model is a strong candidate to learn a\nmapping from out-of-domain ASR errors to in-domain terms in the corresponding\nreference files. We use two off-the-shelf ASR systems in this work: Google ASR\n(commercial) and the ASPIRE model (open-source). We observe 7% absolute\nimprovement in word error rate and 4 point absolute improvement in BLEU score\nin Google ASR output via our proposed method. We also evaluate ASR error\ncorrection via a downstream task of Speaker Diarization that captures speaker\nstyle, syntax, structure and semantic improvements we obtain via ASR\ncorrection.", "published": "2020-03-13 20:05:38", "link": "http://arxiv.org/abs/2003.07692v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Audio inpainting with generative adversarial network", "abstract": "We study the ability of Wasserstein Generative Adversarial Network (WGAN) to\ngenerate missing audio content which is, in context, (statistically similar) to\nthe sound and the neighboring borders. We deal with the challenge of audio\ninpainting long range gaps (500 ms) using WGAN models. We improved the quality\nof the inpainting part using a new proposed WGAN architecture that uses a\nshort-range and a long-range neighboring borders compared to the classical WGAN\nmodel. The performance was compared with two different audio instruments (piano\nand guitar) and on virtuoso pianists together with a string orchestra. The\nobjective difference grading (ODG) was used to evaluate the performance of both\narchitectures. The proposed model outperforms the classical WGAN model and\nimproves the reconstruction of high-frequency content. Further, we got better\nresults for instruments where the frequency spectrum is mainly in the lower\nrange where small noises are less annoying for human ear and the inpainting\npart is more perceptible. Finally, we could show that better test results for\naudio dataset were reached where a particular instrument is accompanist by\nother instruments if we train the network only on this particular instrument\nneglecting the other instruments.", "published": "2020-03-13 09:17:01", "link": "http://arxiv.org/abs/2003.07704v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
