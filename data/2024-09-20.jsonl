{"title": "A Comparison between Financial and Gambling Markets", "abstract": "Financial and gambling markets are ostensibly similar and hence strategies\nfrom one could potentially be applied to the other. Financial markets have been\nextensively studied, resulting in numerous theorems and models, while gambling\nmarkets have received comparatively less attention and remain relatively\nundocumented. This study conducts a comprehensive comparison of both markets,\nfocusing on trading rather than regulation. Five key aspects are examined:\nplatform, product, procedure, participant and strategy. The findings reveal\nnumerous similarities between these two markets. Financial exchanges resemble\nonline betting platforms, such as Betfair, and some financial products,\nincluding stocks and options, share speculative traits with sports betting. We\nexamine whether well-established models and strategies from financial markets\ncould be applied to the gambling industry, which lacks comparable frameworks.\nFor example, statistical arbitrage from financial markets has been effectively\napplied to gambling markets, particularly in peer-to-peer betting exchanges,\nwhere bettors exploit odds discrepancies for risk-free profits using\nquantitative models. Therefore, exploring the strategies and approaches used in\nboth markets could lead to new opportunities for innovation and optimization in\ntrading and betting activities.", "published": "2024-09-20 14:09:15", "link": "http://arxiv.org/abs/2409.13528v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "RRM: Robust Reward Model Training Mitigates Reward Hacking", "abstract": "Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human preferences. However, traditional RM training, which relies\non response pairs tied to specific prompts, struggles to disentangle\nprompt-driven preferences from prompt-independent artifacts, such as response\nlength and format. In this work, we expose a fundamental limitation of current\nRM training methods, where RMs fail to effectively distinguish between\ncontextual signals and irrelevant artifacts when determining preferences. To\naddress this, we introduce a causal framework that learns preferences\nindependent of these artifacts and propose a novel data augmentation technique\ndesigned to eliminate them. Extensive experiments show that our approach\nsuccessfully filters out undesirable artifacts, yielding a more robust reward\nmodel (RRM). Our RRM improves the performance of a pairwise reward model\ntrained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to\n84.15%. Additionally, we train two DPO policies using both the RM and RRM,\ndemonstrating that the RRM significantly enhances DPO-aligned policies,\nimproving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in\nAlpacaEval-2 from 33.46% to 52.49%.", "published": "2024-09-20 01:46:07", "link": "http://arxiv.org/abs/2409.13156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\textit{SKIntern}$: Internalizing Symbolic Knowledge for Distilling\n  Better CoT Capabilities into Small Language Models", "abstract": "Small Language Models (SLMs) are attracting attention due to the high\ncomputational demands and privacy concerns of Large Language Models (LLMs).\nSome studies fine-tune SLMs using Chains of Thought (CoT) data distilled from\nLLMs, aiming to enhance their reasoning ability. Furthermore, Some CoT\ndistillation methods introduce external symbolic knowledge into the generation\nprocess to improve the limited knowledge memory, reasoning ability and\nout-of-domain (OOD) generalization of SLMs. However, the introduction of\nsymbolic knowledge increases computational overhead and introduces potential\nnoise. In this paper, we introduce $\\textit{SKIntern}$, an innovative approach\nthat empowers SLMs to internalize symbolic knowledge and few-shot examples\ngradually through a progressive fine-tuning process, guided by a predefined\nlinear decay schedule under curriculum learning. By efficiently internalizing\nknowledge, $\\textit{SKIntern}$ reduces computational overhead and speeds up the\nreasoning process by focusing solely on the question during inference. It\noutperforms state-of-the-art baselines by over 5\\%, while reducing inference\ncosts (measured in FLOPs) by up to $4\\times$ across a wide range of SLMs in\nboth in-domain (ID) and out-of-domain (OOD) tasks. Our code will be available\nat \\url{https://github.com/Xnhyacinth/SKIntern}.", "published": "2024-09-20 03:23:20", "link": "http://arxiv.org/abs/2409.13183v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CFSP: An Efficient Structured Pruning Framework for LLMs with\n  Coarse-to-Fine Activation Information", "abstract": "The colossal parameters and computational overhead of Large Language Models\n(LLMs) challenge their real-world applications. Network pruning, which targets\nunstructured or structured sparsity by removing redundant parameters, has\nrecently been explored for LLM acceleration. Existing LLM pruning works focus\non unstructured pruning, which typically requires special hardware support for\na practical speed-up. In contrast, structured pruning can reduce latency on\ngeneral devices. However, it remains a challenge to perform structured pruning\nefficiently and maintain performance, especially at high sparsity ratios. To\nthis end, we introduce an efficient structured pruning framework named CFSP,\nwhich leverages both Coarse (interblock) and Fine-grained (intrablock)\nactivation information as an importance criterion to guide pruning. The pruning\nis highly efficient, as it only requires one forward pass to compute feature\nactivations. Specifically, we first allocate the sparsity budget across blocks\nbased on their importance and then retain important weights within each block.\nIn addition, we introduce a recovery fine-tuning strategy that adaptively\nallocates training overhead based on coarse-grained importance to further\nimprove performance. Experimental results demonstrate that CFSP outperforms\nexisting methods on diverse models across various sparsity budgets. Our code\nwill be available at https://github.com/wyxscir/CFSP.", "published": "2024-09-20 04:03:27", "link": "http://arxiv.org/abs/2409.13199v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CITI: Enhancing Tool Utilizing Ability in Large Language Models without\n  Sacrificing General Performance", "abstract": "Tool learning enables the Large Language Models (LLMs) to interact with the\nexternal environment by invoking tools, enriching the accuracy and capability\nscope of LLMs. However, previous works predominantly focus on improving model's\ntool-utilizing accuracy and the ability to generalize to new, unseen tools,\nexcessively forcing LLMs to adjust specific tool-invoking pattern without\nconsidering the harm to model's general performance. This deviates from the\nactual applications and original intention of integrating tools to enhance\nmodel. To tackle this problem, we dissect the capability trade-offs by\nexamining the hidden representation changes and the gradient-based importance\nscore of model's components. Based on the analysis result, we propose a\nComponent Importance-based Tool-utilizing ability Injection method (CITI).\nAccording to the gradient-based importance score of different components, it\nalleviates the capability conflicts caused by fine-tuning process by applying\ndistinct training strategies to different components. CITI applies\nMixture-Of-LoRA (MOLoRA) for important components. Meanwhile, it fine-tunes the\nparameters of few components deemed less important in the backbone of the LLM,\nwhile keeping other parameters frozen. CITI can effectively enhance the model's\ntool-utilizing capability without excessively compromising its general\nperformance. Experimental results demonstrate that our approach achieves\noutstanding performance across a range of evaluation metrics.", "published": "2024-09-20 04:06:28", "link": "http://arxiv.org/abs/2409.13202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural-Symbolic Collaborative Distillation: Advancing Small Language\n  Models for Complex Reasoning Tasks", "abstract": "In this paper, we propose $\\textbf{Ne}$ural-$\\textbf{Sy}$mbolic\n$\\textbf{C}$ollaborative $\\textbf{D}$istillation ($\\textbf{NesyCD}$), a novel\nknowledge distillation method for learning the complex reasoning abilities of\nLarge Language Models (LLMs, e.g., \\textgreater 13B). We argue that complex\nreasoning tasks are difficult for Small Language Models (SLMs, e.g., $\\leq$\n7B), as these tasks demand not only general cognitive abilities but also\nspecialized knowledge, which is often sparse and difficult for these\nneural-based SLMs to effectively capture. Therefore, NesyCD distills the\ngeneral capabilities and specialized knowledge in LLMs using different manners.\nOn the one hand, we distill only general abilities from teacher LLMs into the\nstudent SLMs of parameterized neural networks. On the other hand, for the\nspecialized abilities and uncommon knowledge of a complex reasoning task, we\nemploy a symbolic knowledge distillation approach to obtain and store the\nspecialized knowledge within a symbolic knowledge base (KB). By decoupling\ngeneral and specialized capabilities, the proposed NesyCD can achieve superior\nperformance cost-effectively, utilizing smaller models and blending\nparameterized neural networks with symbolic KB. Moreover, the specialized KB\ngeneralizes well and is comprehended and manipulated by humans. Our experiments\nshow that NesyCD significantly boosts SLMs' complex reasoning performance on\nin-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our\napproach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in\nperformance and come close to matching LLaMA3-70B, despite the latter having\nnine times more parameters. Our code will be available at\nhttps://github.com/Xnhyacinth/NesyCD.", "published": "2024-09-20 04:17:13", "link": "http://arxiv.org/abs/2409.13203v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards LifeSpan Cognitive Systems", "abstract": "Building a human-like system that continuously interacts with complex\nenvironments -- whether simulated digital worlds or human society -- presents\nseveral key challenges. Central to this is enabling continuous, high-frequency\ninteractions, where the interactions are termed experiences. We refer to this\nenvisioned system as the LifeSpan Cognitive System (LSCS). A critical feature\nof LSCS is its ability to engage in incremental and rapid updates while\nretaining and accurately recalling past experiences. In this paper we focus on\nthe domain of Large Language Models (LLMs), where we identify two major\nchallenges: (1) Abstraction and Experience Merging, and (2) Long-term Retention\nwith Accurate Recall. These properties are essential for storing new\nexperiences, organizing past experiences, and responding to the environment in\nways that leverage relevant historical data. Unlike language models with\ncontinual learning, which typically rely on large corpora for fine-tuning and\nfocus on improving performance within specific domains or tasks, LSCS must\nrapidly and incrementally update with new information from its environment at a\nhigh frequency. Existing technologies with the potential of solving the above\ntwo major challenges can be classified into four classes based on a conceptual\nmetric called Storage Complexity, which measures the relative space required to\nstore past experiences. Each of these four classes of technologies has its own\nstrengths and limitations while we argue none of them alone can achieve LSCS\nalone. To this end, we propose a potential instantiation for LSCS that can\nintegrate all four classes of technologies. The new instantiation, serving as a\nconjecture, operates through two core processes: Absorbing Experiences and\nGenerating Responses.", "published": "2024-09-20 06:54:00", "link": "http://arxiv.org/abs/2409.13265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation for Keyphrase Generation using Citation\n  Contexts", "abstract": "Adapting keyphrase generation models to new domains typically involves\nfew-shot fine-tuning with in-domain labeled data. However, annotating documents\nwith keyphrases is often prohibitively expensive and impractical, requiring\nexpert annotators. This paper presents silk, an unsupervised method designed to\naddress this issue by extracting silver-standard keyphrases from citation\ncontexts to create synthetic labeled data for domain adaptation. Extensive\nexperiments across three distinct domains demonstrate that our method yields\nhigh-quality synthetic samples, resulting in significant and consistent\nimprovements in in-domain performance over strong baselines.", "published": "2024-09-20 06:56:14", "link": "http://arxiv.org/abs/2409.13266v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language\n  Models", "abstract": "Recent developments in Japanese large language models (LLMs) primarily focus\non general domains, with fewer advancements in Japanese biomedical LLMs. One\nobstacle is the absence of a comprehensive, large-scale benchmark for\ncomparison. Furthermore, the resources for evaluating Japanese biomedical LLMs\nare insufficient. To advance this field, we propose a new benchmark including\neight LLMs across four categories and 20 Japanese biomedical datasets across\nfive tasks. Experimental results indicate that: (1) LLMs with a better\nunderstanding of Japanese and richer biomedical knowledge achieve better\nperformance in Japanese biomedical tasks, (2) LLMs that are not mainly designed\nfor Japanese biomedical domains can still perform unexpectedly well, and (3)\nthere is still much room for improving the existing LLMs in certain Japanese\nbiomedical tasks. Moreover, we offer insights that could further enhance\ndevelopment in this field. Our evaluation tools tailored to our benchmark as\nwell as the datasets are publicly available in\nhttps://huggingface.co/datasets/Coldog2333/JMedBench to facilitate future\nresearch.", "published": "2024-09-20 08:25:16", "link": "http://arxiv.org/abs/2409.13317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AQA: Adaptive Question Answering in a Society of LLMs via Contextual\n  Multi-Armed Bandit", "abstract": "In question answering (QA), different questions can be effectively addressed\nwith different answering strategies. Some require a simple lookup, while others\nneed complex, multi-step reasoning to be answered adequately. This observation\nmotivates the development of a dynamic method that adaptively selects the most\nsuitable QA strategy for each question, enabling more efficient and effective\nsystems capable of addressing a broader range of question types. To this aim,\nwe build on recent advances in the orchestration of multiple large language\nmodels (LLMs) and formulate adaptive QA as a dynamic orchestration challenge.\nWe define this as a contextual multi-armed bandit problem, where the context is\ndefined by the characteristics of the incoming question and the action space\nconsists of potential communication graph configurations among the LLM agents.\nWe then train a linear upper confidence bound model to learn an optimal mapping\nbetween different question types and their corresponding optimal multi-LLM\ncommunication graph representation. Our experiments show that the proposed\nsolution is viable for adaptive orchestration of a QA system with multiple\nmodules, as it combines the superior performance of more complex strategies\nwhile avoiding their costs when simpler strategies suffice.", "published": "2024-09-20 12:28:18", "link": "http://arxiv.org/abs/2409.13447v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minstrel: Structural Prompt Generation with Multi-Agents Coordination\n  for Non-AI Experts", "abstract": "LLMs have demonstrated commendable performance across diverse domains.\nNevertheless, formulating high-quality prompts to assist them in their work\nposes a challenge for non-AI experts. Existing research in prompt engineering\nsuggests somewhat scattered optimization principles and designs empirically\ndependent prompt optimizers. Unfortunately, these endeavors lack a structural\ndesign, incurring high learning costs and it is not conducive to the iterative\nupdating of prompts, especially for non-AI experts. Inspired by structured\nreusable programming languages, we propose LangGPT, a structural prompt design\nframework. Furthermore, we introduce Minstrel, a multi-generative agent system\nwith reflection to automate the generation of structural prompts. Experiments\nand the case study illustrate that structural prompts generated by Minstrel or\nwritten manually significantly enhance the performance of LLMs. Furthermore, we\nanalyze the ease of use of structural prompts through a user survey in our\nonline community.", "published": "2024-09-20 12:30:03", "link": "http://arxiv.org/abs/2409.13449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constrained Reasoning Chains for Enhancing Theory-of-Mind in Large\n  Language Models", "abstract": "Theory-of-Mind (ToM) ability possessed by Large Language Models (LLMs) has\nbeen shown to be limited. Most existing methods for improving ToM in LLMs adopt\nzero-shot prompting, and they face challenges including poor performance in\ncomplex ToM reasoning tasks and an inability to handle non-narrative contexts.\nWe propose a zero-shot prompting method named Constrained Chain-of-ToM (CCoToM)\nthat leverages domain knowledge and the causal relations between ToM dimensions\nto address these limitations. Specifically, CCoToM guides LLMs to construct\nexplicit reasoning chains by first prompting LLMs to infer related ToM\ndimensions (e.g., belief). Afterward, CCoToM prompts LLMs to infer the queried\nToM dimension based on the generated related ToM dimensions and corresponding\ncausal relations. Additionally, CCoToM adaptively imposes constraints on\nprompts to introduce inductive biases and improve consistency between ToM\ndimensions. Besides narratives, CCoToM can also handle non-narrative contexts\nlike conversations. Extensive experiments show that CCoToM consistently\noutperforms previous state-of-the-art methods by large margins across all LLMs\nand datasets used. We also conduct in-depth analyses to gain deeper insights\ninto CCoToM. We have made our code publicly available.", "published": "2024-09-20 13:27:11", "link": "http://arxiv.org/abs/2409.13490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"I Never Said That\": A dataset, taxonomy and baselines on response\n  clarity classification", "abstract": "Equivocation and ambiguity in public speech are well-studied discourse\nphenomena, especially in political science and analysis of political\ninterviews. Inspired by the well-grounded theory on equivocation, we aim to\nresolve the closely related problem of response clarity in questions extracted\nfrom political interviews, leveraging the capabilities of Large Language Models\n(LLMs) and human expertise. To this end, we introduce a novel taxonomy that\nframes the task of detecting and classifying response clarity and a\ncorresponding clarity classification dataset which consists of question-answer\n(QA) pairs drawn from political interviews and annotated accordingly. Our\nproposed two-level taxonomy addresses the clarity of a response in terms of the\ninformation provided for a given question (high-level) and also provides a\nfine-grained taxonomy of evasion techniques that relate to unclear, ambiguous\nresponses (lower-level). We combine ChatGPT and human annotators to collect,\nvalidate and annotate discrete QA pairs from political interviews, to be used\nfor our newly introduced response clarity task. We provide a detailed analysis\nand conduct several experiments with different model architectures, sizes and\nadaptation methods to gain insights and establish new baselines over the\nproposed dataset and task.", "published": "2024-09-20 20:15:06", "link": "http://arxiv.org/abs/2409.13879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning with Clinical Concept Embeddings from Large Language\n  Models", "abstract": "Knowledge sharing is crucial in healthcare, especially when leveraging data\nfrom multiple clinical sites to address data scarcity, reduce costs, and enable\ntimely interventions. Transfer learning can facilitate cross-site knowledge\ntransfer, but a major challenge is heterogeneity in clinical concepts across\ndifferent sites. Large Language Models (LLMs) show significant potential of\ncapturing the semantic meaning of clinical concepts and reducing heterogeneity.\nThis study analyzed electronic health records from two large healthcare systems\nto assess the impact of semantic embeddings from LLMs on local, shared, and\ntransfer learning models. Results indicate that domain-specific LLMs, such as\nMed-BERT, consistently outperform in local and direct transfer scenarios, while\ngeneric models like OpenAI embeddings require fine-tuning for optimal\nperformance. However, excessive tuning of models with biomedical embeddings may\nreduce effectiveness, emphasizing the need for balance. This study highlights\nthe importance of domain-specific embeddings and careful model tuning for\neffective knowledge transfer in healthcare.", "published": "2024-09-20 20:50:55", "link": "http://arxiv.org/abs/2409.13893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "abstract": "In natural human-to-human conversations, participants often receive feedback\nsignals from one another based on their follow-up reactions. These reactions\ncan include verbal responses, facial expressions, changes in emotional state,\nand other non-verbal cues. Similarly, in human-machine interactions, the\nmachine can leverage the user's follow-up utterances as feedback signals to\nassess whether it has appropriately addressed the user's request. Therefore, we\npropose using the likelihood of follow-up utterances as rewards to\ndifferentiate preferred responses from less favored ones, without relying on\nhuman or commercial LLM-based preference annotations. Our proposed reward\nmechanism, ``Follow-up Likelihood as Reward\" (FLR), matches the performance of\nstrong reward models trained on large-scale human or GPT-4 annotated data on 8\npairwise-preference and 4 rating-based benchmarks. Building upon the FLR\nmechanism, we propose to automatically mine preference data from the online\ngenerations of a base policy model. The preference data are subsequently used\nto boost the helpfulness of the base model through direct alignment from\npreference (DAP) methods, such as direct preference optimization (DPO). Lastly,\nwe demonstrate that fine-tuning the language model that provides follow-up\nlikelihood with natural language feedback significantly enhances FLR's\nperformance on reward modeling benchmarks and effectiveness in aligning the\nbase policy model's helpfulness.", "published": "2024-09-20 23:47:25", "link": "http://arxiv.org/abs/2409.13948v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM", "abstract": "Multilingual large language models (LLMs) are great translators, but this is\nlargely limited to high-resource languages. For many LLMs, translating in and\nout of low-resource languages remains a challenging task. To maximize data\nefficiency in this low-resource setting, we introduce Mufu, which includes a\nselection of automatically generated multilingual candidates and an instruction\nto correct inaccurate translations in the prompt. Mufu prompts turn a\ntranslation task into a postediting one, and seek to harness the LLM's\nreasoning capability with auxiliary translation candidates, from which the\nmodel is required to assess the input quality, align the semantics\ncross-lingually, copy from relevant inputs and override instances that are\nincorrect. Our experiments on En-XX translations over the Flores-200 dataset\nshow LLMs finetuned against Mufu-style prompts are robust to poor quality\nauxiliary translation candidates, achieving performance superior to NLLB 1.3B\ndistilled model in 64% of low- and very-low-resource language pairs. We then\ndistill these models to reduce inference cost, while maintaining on average 3.1\nchrF improvement over finetune-only baseline in low-resource translations.", "published": "2024-09-20 23:48:47", "link": "http://arxiv.org/abs/2409.13949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GAProtoNet: A Multi-head Graph Attention-based Prototypical Network for\n  Interpretable Text Classification", "abstract": "Pretrained transformer-based Language Models (LMs) are well-known for their\nability to achieve significant improvement on text classification tasks with\ntheir powerful word embeddings, but their black-box nature, which leads to a\nlack of interpretability, has been a major concern. In this work, we introduce\nGAProtoNet, a novel white-box Multi-head Graph Attention-based Prototypical\nNetwork designed to explain the decisions of text classification models built\nwith LM encoders. In our approach, the input vector and prototypes are regarded\nas nodes within a graph, and we utilize multi-head graph attention to\nselectively construct edges between the input node and prototype nodes to learn\nan interpretable prototypical representation. During inference, the model makes\ndecisions based on a linear combination of activated prototypes weighted by the\nattention score assigned for each prototype, allowing its choices to be\ntransparently explained by the attention weights and the prototypes projected\ninto the closest matching training examples. Experiments on multiple public\ndatasets show our approach achieves superior results without sacrificing the\naccuracy of the original black-box LMs. We also compare with four alternative\nprototypical network variations and our approach achieves the best accuracy and\nF1 among all. Our case study and visualization of prototype clusters also\ndemonstrate the efficiency in explaining the decisions of black-box models\nbuilt with LMs.", "published": "2024-09-20 08:15:17", "link": "http://arxiv.org/abs/2409.13312v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Applying Pre-trained Multilingual BERT in Embeddings for Improved\n  Malicious Prompt Injection Attacks Detection", "abstract": "Large language models (LLMs) are renowned for their exceptional capabilities,\nand applying to a wide range of applications. However, this widespread use\nbrings significant vulnerabilities. Also, it is well observed that there are\nhuge gap which lies in the need for effective detection and mitigation\nstrategies against malicious prompt injection attacks in large language models,\nas current approaches may not adequately address the complexity and evolving\nnature of these vulnerabilities in real-world applications. Therefore, this\nwork focuses the impact of malicious prompt injection attacks which is one of\nmost dangerous vulnerability on real LLMs applications. It examines to apply\nvarious BERT (Bidirectional Encoder Representations from Transformers) like\nmultilingual BERT, DistilBert for classifying malicious prompts from legitimate\nprompts. Also, we observed how tokenizing the prompt texts and generating\nembeddings using multilingual BERT contributes to improve the performance of\nvarious machine learning methods: Gaussian Naive Bayes, Random Forest, Support\nVector Machine, and Logistic Regression. The performance of each model is\nrigorously analyzed with various parameters to improve the binary\nclassification to discover malicious prompts. Multilingual BERT approach to\nembed the prompts significantly improved and outperformed the existing works\nand achieves an outstanding accuracy of 96.55% by Logistic regression.\nAdditionally, we investigated the incorrect predictions of the model to gain\ninsights into its limitations. The findings can guide researchers in tuning\nvarious BERT for finding the most suitable model for diverse LLMs\nvulnerabilities.", "published": "2024-09-20 08:48:51", "link": "http://arxiv.org/abs/2409.13331v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Time Awareness in Large Language Models: Benchmarking Fact Recall Across\n  Time", "abstract": "Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge.", "published": "2024-09-20 08:57:20", "link": "http://arxiv.org/abs/2409.13338v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recent Advancement of Emotion Cognition in Large Language Models", "abstract": "Emotion cognition in large language models (LLMs) is crucial for enhancing\nperformance across various applications, such as social media, human-computer\ninteraction, and mental health assessment. We explore the current landscape of\nresearch, which primarily revolves around emotion classification, emotionally\nrich response generation, and Theory of Mind assessments, while acknowledge the\nchallenges like dependency on annotated data and complexity in emotion\nprocessing. In this paper, we present a detailed survey of recent progress in\nLLMs for emotion cognition. We explore key research studies, methodologies,\noutcomes, and resources, aligning them with Ulric Neisser's cognitive stages.\nAdditionally, we outline potential future directions for research in this\nevolving field, including unsupervised learning approaches and the development\nof more complex and interpretable emotion cognition LLMs. We also discuss\nadvanced methods such as contrastive learning used to improve LLMs' emotion\ncognition capabilities.", "published": "2024-09-20 09:34:58", "link": "http://arxiv.org/abs/2409.13354v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EmotionQueen: A Benchmark for Evaluating Empathy of Large Language\n  Models", "abstract": "Emotional intelligence in large language models (LLMs) is of great importance\nin Natural Language Processing. However, the previous research mainly focus on\nbasic sentiment analysis tasks, such as emotion recognition, which is not\nenough to evaluate LLMs' overall emotional intelligence. Therefore, this paper\npresents a novel framework named EmotionQueen for evaluating the emotional\nintelligence of LLMs. The framework includes four distinctive tasks: Key Event\nRecognition, Mixed Event Recognition, Implicit Emotional Recognition, and\nIntention Recognition. LLMs are requested to recognize important event or\nimplicit emotions and generate empathetic response. We also design two metrics\nto evaluate LLMs' capabilities in recognition and response for emotion-related\nstatements. Experiments yield significant conclusions about LLMs' capabilities\nand limitations in emotion intelligence.", "published": "2024-09-20 09:44:51", "link": "http://arxiv.org/abs/2409.13359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1\n  on PlanBench", "abstract": "The ability to plan a course of action that achieves a desired state of\naffairs has long been considered a core competence of intelligent agents and\nhas been an integral part of AI research since its inception. With the advent\nof large language models (LLMs), there has been considerable interest in the\nquestion of whether or not they possess such planning abilities. PlanBench, an\nextensible benchmark we developed in 2022, soon after the release of GPT3, has\nremained an important tool for evaluating the planning abilities of LLMs.\nDespite the slew of new private and open source LLMs since GPT3, progress on\nthis benchmark has been surprisingly slow. OpenAI claims that their recent o1\n(Strawberry) model has been specifically constructed and trained to escape the\nnormal limitations of autoregressive LLMs--making it a new kind of model: a\nLarge Reasoning Model (LRM). Using this development as a catalyst, this paper\ntakes a comprehensive look at how well current LLMs and new LRMs do on\nPlanBench. As we shall see, while o1's performance is a quantum improvement on\nthe benchmark, outpacing the competition, it is still far from saturating it.\nThis improvement also brings to the fore questions about accuracy, efficiency,\nand guarantees which must be considered before deploying such systems.", "published": "2024-09-20 10:20:46", "link": "http://arxiv.org/abs/2409.13373v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Contextual Compression in Retrieval-Augmented Generation for Large\n  Language Models: A Survey", "abstract": "Large Language Models (LLMs) showcase remarkable abilities, yet they struggle\nwith limitations such as hallucinations, outdated knowledge, opacity, and\ninexplicable reasoning. To address these challenges, Retrieval-Augmented\nGeneration (RAG) has proven to be a viable solution, leveraging external\ndatabases to improve the consistency and coherence of generated content,\nespecially valuable for complex, knowledge-rich tasks, and facilitates\ncontinuous improvement by leveraging domain-specific insights. By combining the\nintrinsic knowledge of LLMs with the vast, dynamic repositories of external\ndatabases, RAG achieves a synergistic effect. However, RAG is not without its\nlimitations, including a limited context window, irrelevant information, and\nthe high processing overhead for extensive contextual data. In this\ncomprehensive work, we explore the evolution of Contextual Compression\nparadigms, providing an in-depth examination of the field. Finally, we outline\nthe current challenges and suggest potential research and development\ndirections, paving the way for future advancements in this area.", "published": "2024-09-20 10:36:49", "link": "http://arxiv.org/abs/2409.13385v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Selective Exploration and Information Gathering in Search and Rescue\n  Using Hierarchical Learning Guided by Natural Language Input", "abstract": "In recent years, robots and autonomous systems have become increasingly\nintegral to our daily lives, offering solutions to complex problems across\nvarious domains. Their application in search and rescue (SAR) operations,\nhowever, presents unique challenges. Comprehensively exploring the\ndisaster-stricken area is often infeasible due to the vastness of the terrain,\ntransformed environment, and the time constraints involved. Traditional robotic\nsystems typically operate on predefined search patterns and lack the ability to\nincorporate and exploit ground truths provided by human stakeholders, which can\nbe the key to speeding up the learning process and enhancing triage. Addressing\nthis gap, we introduce a system that integrates social interaction via large\nlanguage models (LLMs) with a hierarchical reinforcement learning (HRL)\nframework. The proposed system is designed to translate verbal inputs from\nhuman stakeholders into actionable RL insights and adjust its search strategy.\nBy leveraging human-provided information through LLMs and structuring task\nexecution through HRL, our approach not only bridges the gap between autonomous\ncapabilities and human intelligence but also significantly improves the agent's\nlearning efficiency and decision-making process in environments characterised\nby long horizons and sparse rewards.", "published": "2024-09-20 12:27:47", "link": "http://arxiv.org/abs/2409.13445v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Alternate Preference Optimization for Unlearning Factual Knowledge in\n  Large Language Models", "abstract": "Machine unlearning aims to efficiently eliminate the influence of specific\ntraining data, known as the forget set, from the model. However, existing\nunlearning methods for Large Language Models (LLMs) face a critical challenge:\nthey rely solely on negative feedback to suppress responses related to the\nforget set, which often results in nonsensical or inconsistent outputs,\ndiminishing model utility and posing potential privacy risks. To address this\nlimitation, we propose a novel approach called Alternate Preference\nOptimization (AltPO), which combines negative feedback with in-domain positive\nfeedback on the forget set. Additionally, we introduce new evaluation metrics\nto assess the quality of responses related to the forget set. Extensive\nexperiments show that our approach not only enables effective unlearning but\nalso avoids undesirable model behaviors while maintaining overall model\nperformance. Our implementation can be found at\nhttps://github.com/molereddy/Alternate-Preference-Optimization.", "published": "2024-09-20 13:05:07", "link": "http://arxiv.org/abs/2409.13474v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain\n  Question Answering", "abstract": "Speech-based open-domain question answering (QA over a large corpus of text\npassages with spoken questions) has emerged as an important task due to the\nincreasing number of users interacting with QA systems via speech interfaces.\nPassage retrieval is a key task in speech-based open-domain QA. So far,\nprevious works adopted pipelines consisting of an automatic speech recognition\n(ASR) model that transcribes the spoken question before feeding it to a dense\ntext retriever. Such pipelines have several limitations. The need for an ASR\nmodel limits the applicability to low-resource languages and specialized\ndomains with no annotated speech data. Furthermore, the ASR model propagates\nits errors to the retriever. In this work, we try to alleviate these\nlimitations by proposing an ASR-free, end-to-end trained multimodal dense\nretriever that can work directly on spoken questions. Our experimental results\nshowed that, on shorter questions, our retriever is a promising alternative to\nthe \\textit{ASR and Retriever} pipeline, achieving better retrieval performance\nin cases where ASR would have mistranscribed important words in the question or\nhave produced a transcription with a high word error rate.", "published": "2024-09-20 13:15:53", "link": "http://arxiv.org/abs/2409.13483v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "HUT: A More Computation Efficient Fine-Tuning Method With Hadamard\n  Updated Transformation", "abstract": "Fine-tuning pre-trained language models for downstream tasks has achieved\nimpressive results in NLP. However, fine-tuning all parameters becomes\nimpractical due to the rapidly increasing size of model parameters. To address\nthis, Parameter Efficient Fine-Tuning (PEFT) methods update only a subset of\nparameters. Most PEFT methods, such as LoRA, use incremental updates, which\ninvolve adding learned weight matrix increments to the original parameters.\nAlthough effective, these methods face limitations in capturing complex\nparameter dynamics and do not maintain a strong correlation between the\noriginal and updated parameters. To overcome these challenges, we propose the\ndirect Updated Transformation (UT) paradigm, which constructs a transformation\ndirectly from the original to the updated parameters. This approach ensures\nthat the correlation between the original and updated parameters is preserved,\nleveraging the semantic features learned during pre-training. Building on this\nparadigm, we present the Hadamard Updated Transformation (HUT) method. HUT\nefficiently updates the original weight matrix using the Hadamard\ntransformation with two low-rank matrices, offering a more expressive and\nflexible update mechanism. This allows HUT to capture richer parameter features\nthrough functional transformations, reducing computational complexity while\nmaintaining or improving model quality. Theoretical analysis and extensive\nexperiments on RoBERTa and GPT-2 validate the effectiveness of HUT. Results\nshow that HUT performs on par with or better than other PEFT methods in terms\nof model quality, while significantly reducing computational complexity.", "published": "2024-09-20 13:42:17", "link": "http://arxiv.org/abs/2409.13501v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ShizishanGPT: An Agricultural Large Language Model Integrating Tools and\n  Resources", "abstract": "Recent developments in large language models (LLMs) have led to significant\nimprovements in intelligent dialogue systems'ability to handle complex\ninquiries. However, current LLMs still exhibit limitations in specialized\ndomain knowledge, particularly in technical fields such as agriculture. To\naddress this problem, we propose ShizishanGPT, an intelligent question\nanswering system for agriculture based on the Retrieval Augmented Generation\n(RAG) framework and agent architecture. ShizishanGPT consists of five key\nmodules: including a generic GPT-4 based module for answering general\nquestions; a search engine module that compensates for the problem that the\nlarge language model's own knowledge cannot be updated in a timely manner; an\nagricultural knowledge graph module for providing domain facts; a retrieval\nmodule which uses RAG to supplement domain knowledge; and an agricultural agent\nmodule, which invokes specialized models for crop phenotype prediction, gene\nexpression analysis, and so on. We evaluated the ShizishanGPT using a dataset\ncontaining 100 agricultural questions specially designed for this study. The\nexperimental results show that the tool significantly outperforms general LLMs\nas it provides more accurate and detailed answers due to its modular design and\nintegration of different domain knowledge sources. Our source code, dataset,\nand model weights are publicly available at https://github.com/Zaiwen/CropGPT.", "published": "2024-09-20 14:30:45", "link": "http://arxiv.org/abs/2409.13537v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Visual Stories with Grounded and Coreferent Characters", "abstract": "Characters are important in narratives. They move the plot forward, create\nemotional connections, and embody the story's themes. Visual storytelling\nmethods focus more on the plot and events relating to it, without building the\nnarrative around specific characters. As a result, the generated stories feel\ngeneric, with character mentions being absent, vague, or incorrect. To mitigate\nthese issues, we introduce the new task of character-centric story generation\nand present the first model capable of predicting visual stories with\nconsistently grounded and coreferent character mentions. Our model is finetuned\non a new dataset which we build on top of the widely used VIST benchmark.\nSpecifically, we develop an automated pipeline to enrich VIST with visual and\ntextual character coreference chains. We also propose new evaluation metrics to\nmeasure the richness of characters and coreference in stories. Experimental\nresults show that our model generates stories with recurring characters which\nare consistent and coreferent to larger extent compared to baselines and\nstate-of-the-art systems.", "published": "2024-09-20 14:56:33", "link": "http://arxiv.org/abs/2409.13555v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Demystifying and Extracting Fault-indicating Information from Logs for\n  Failure Diagnosis", "abstract": "Logs are imperative in the maintenance of online service systems, which often\nencompass important information for effective failure mitigation. While\nexisting anomaly detection methodologies facilitate the identification of\nanomalous logs within extensive runtime data, manual investigation of log\nmessages by engineers remains essential to comprehend faults, which is\nlabor-intensive and error-prone. Upon examining the log-based troubleshooting\npractices at CloudA, we find that engineers typically prioritize two categories\nof log information for diagnosis. These include fault-indicating descriptions,\nwhich record abnormal system events, and fault-indicating parameters, which\nspecify the associated entities. Motivated by this finding, we propose an\napproach to automatically extract such faultindicating information from logs\nfor fault diagnosis, named LoFI. LoFI comprises two key stages. In the first\nstage, LoFI performs coarse-grained filtering to collect logs related to the\nfaults based on semantic similarity. In the second stage, LoFI leverages a\npre-trained language model with a novel prompt-based tuning method to extract\nfine-grained information of interest from the collected logs. We evaluate LoFI\non logs collected from Apache Spark and an industrial dataset from CloudA. The\nexperimental results demonstrate that LoFI outperforms all baseline methods by\na significant margin, achieving an absolute improvement of 25.8~37.9 in F1 over\nthe best baseline method, ChatGPT. This highlights the effectiveness of LoFI in\nrecognizing fault-indicating information. Furthermore, the successful\ndeployment of LoFI at CloudA and user studies validate the utility of our\nmethod. The code and data are available at\nhttps://github.com/Jun-jie-Huang/LoFI.", "published": "2024-09-20 15:00:47", "link": "http://arxiv.org/abs/2409.13561v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Cross-Target Stance Detection: A Survey of Techniques, Datasets, and\n  Challenges", "abstract": "Stance detection is the task of determining the viewpoint expressed in a text\ntowards a given target. A specific direction within the task focuses on\ncross-target stance detection, where a model trained on samples pertaining to\ncertain targets is then applied to a new, unseen target. With the increasing\nneed to analyze and mining viewpoints and opinions online, the task has\nrecently seen a significant surge in interest. This review paper examines the\nadvancements in cross-target stance detection over the last decade,\nhighlighting the evolution from basic statistical methods to contemporary\nneural and LLM-based models. These advancements have led to notable\nimprovements in accuracy and adaptability. Innovative approaches include the\nuse of topic-grouped attention and adversarial learning for zero-shot\ndetection, as well as fine-tuning techniques that enhance model robustness.\nAdditionally, prompt-tuning methods and the integration of external knowledge\nhave further refined model performance. A comprehensive overview of the\ndatasets used for evaluating these models is also provided, offering valuable\ninsights into the progress and challenges in the field. We conclude by\nhighlighting emerging directions of research and by suggesting avenues for\nfuture work in the task.", "published": "2024-09-20 15:49:14", "link": "http://arxiv.org/abs/2409.13594v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Beyond Accuracy Optimization: Computer Vision Losses for Large Language\n  Model Fine-Tuning", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks. However, current training approaches combine standard\ncross-entropy loss with extensive data, human feedback, or ad hoc methods to\nenhance performance. These solutions are often not scalable or feasible due to\ntheir associated costs, complexity, or resource requirements. This study\ninvestigates the use of established semantic segmentation loss functions in\nnatural language generation to create a versatile, practical, and scalable\nsolution for fine-tuning different architectures. We evaluate their\neffectiveness in solving Math Word Problems and question answering across\ndifferent models of varying sizes. For the analyzed tasks, we found that the\ntraditional Cross-Entropy loss represents a sub-optimal choice, while models\ntrained to minimize alternative (task-dependent) losses, such as Focal or\nLov\\'asz, achieve a mean improvement of +42% on exact match without requiring\nadditional data or human feedback. These findings suggest a promising pathway\nfor more efficient and accessible training processes.", "published": "2024-09-20 16:46:17", "link": "http://arxiv.org/abs/2409.13641v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Do language models practice what they preach? Examining language\n  ideologies about gendered language reform encoded in LLMs", "abstract": "We study language ideologies in text produced by LLMs through a case study on\nEnglish gendered language reform (related to role nouns like\ncongressperson/-woman/-man, and singular they). First, we find political bias:\nwhen asked to use language that is \"correct\" or \"natural\", LLMs use language\nmost similarly to when asked to align with conservative (vs. progressive)\nvalues. This shows how LLMs' metalinguistic preferences can implicitly\ncommunicate the language ideologies of a particular political group, even in\nseemingly non-political contexts. Second, we find LLMs exhibit internal\ninconsistency: LLMs use gender-neutral variants more often when more explicit\nmetalinguistic context is provided. This shows how the language ideologies\nexpressed in text produced by LLMs can vary, which may be unexpected to users.\nWe discuss the broader implications of these findings for value alignment.", "published": "2024-09-20 18:55:48", "link": "http://arxiv.org/abs/2409.13852v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM for Everyone: Representing the Underrepresented in Large Language\n  Models", "abstract": "Natural language processing (NLP) has witnessed a profound impact of large\nlanguage models (LLMs) that excel in a multitude of tasks. However, the\nlimitation of LLMs in multilingual settings, particularly in underrepresented\nlanguages, remains a significant hurdle. This thesis aims to bridge the gap in\nNLP research and development by focusing on underrepresented languages. A\ncomprehensive evaluation of LLMs is conducted to assess their capabilities in\nthese languages, revealing the challenges of multilingual and multicultural\ngeneralization. Addressing the multilingual generalization gap, this thesis\nproposes data-and-compute-efficient methods to mitigate the disparity in LLM\nability in underrepresented languages, allowing better generalization on\nunderrepresented languages without the loss of task generalization ability. The\nproposed solutions cover cross-lingual continual instruction tuning,\nretrieval-based cross-lingual in-context learning, and in-context query\nalignment. Furthermore, a novel method to measure cultural values alignment\nbetween LLMs operating in different languages is proposed, ensuring cultural\nsensitivity and inclusivity. These contributions aim to enhance the\nmultilingual and multicultural alignment of LLMs in underrepresented languages,\nultimately advancing the NLP field toward greater equality and inclusiveness.", "published": "2024-09-20 20:53:22", "link": "http://arxiv.org/abs/2409.13897v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Models with Domain-specific Retrieval Augment\n  Generation: A Case Study on Long-form Consumer Health Question Answering in\n  Ophthalmology", "abstract": "Despite the potential of Large Language Models (LLMs) in medicine, they may\ngenerate responses lacking supporting evidence or based on hallucinated\nevidence. While Retrieval Augment Generation (RAG) is popular to address this\nissue, few studies implemented and evaluated RAG in downstream domain-specific\napplications. We developed a RAG pipeline with 70,000 ophthalmology-specific\ndocuments that retrieve relevant documents to augment LLMs during inference\ntime. In a case study on long-form consumer health questions, we systematically\nevaluated the responses including over 500 references of LLMs with and without\nRAG on 100 questions with 10 healthcare professionals. The evaluation focuses\non factuality of evidence, selection and ranking of evidence, attribution of\nevidence, and answer accuracy and completeness. LLMs without RAG provided 252\nreferences in total. Of which, 45.3% hallucinated, 34.1% consisted of minor\nerrors, and 20.6% were correct. In contrast, LLMs with RAG significantly\nimproved accuracy (54.5% being correct) and reduced error rates (18.8% with\nminor hallucinations and 26.7% with errors). 62.5% of the top 10 documents\nretrieved by RAG were selected as the top references in the LLM response, with\nan average ranking of 4.9. The use of RAG also improved evidence attribution\n(increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight\ndecreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47\nto 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited\nhallucinated and erroneous evidence in the responses, raising concerns for\ndownstream applications in the medical domain. RAG substantially reduced the\nproportion of such evidence but encountered challenges.", "published": "2024-09-20 21:06:00", "link": "http://arxiv.org/abs/2409.13902v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit\n  NLP Tasks", "abstract": "Morphologically rich languages are notoriously challenging to process for\ndownstream NLP applications. This paper presents a new pretrained language\nmodel, ByT5-Sanskrit, designed for NLP applications involving the\nmorphologically rich language Sanskrit. We evaluate ByT5-Sanskrit on\nestablished Sanskrit word segmentation tasks, where it outperforms previous\ndata-driven approaches by a considerable margin and matches the performance of\nthe current best lexicon-based model. It is easier to deploy and more robust to\ndata not covered by external linguistic resources. It also achieves new\nstate-of-the-art results in Vedic Sanskrit dependency parsing and OCR\npost-correction tasks. Additionally, based on the Digital Corpus of Sanskrit,\nwe introduce a novel multitask dataset for the joint training of Sanskrit word\nsegmentation, lemmatization, and morphosyntactic tagging tasks. We fine-tune\nByT5-Sanskrit on this dataset, creating a versatile multitask model for various\ndownstream Sanskrit applications. We have used this model in Sanskrit\nlinguistic annotation projects, in information retrieval setups, and as a\npreprocessing step in a Sanskrit machine translation pipeline. We also show\nthat our approach yields new best scores for lemmatization and dependency\nparsing of other morphologically rich languages. We thus demonstrate that\nbyte-level pretrained language models can achieve excellent performance for\nmorphologically rich languages, outperforming tokenizer-based models and\npresenting an important vector of exploration when constructing NLP pipelines\nfor such languages.", "published": "2024-09-20 22:02:26", "link": "http://arxiv.org/abs/2409.13920v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On-Device Collaborative Language Modeling via a Mixture of Generalists\n  and Specialists", "abstract": "On-device LLMs have gained increasing attention for their ability to enhance\nprivacy and provide a personalized user experience. To facilitate private\nlearning with scarce data, Federated Learning has become a standard approach.\nHowever, it faces challenges such as computational resource heterogeneity and\ndata heterogeneity among end users. We propose CoMiGS ($\\textbf{Co}$llaborative\nlearning with a $\\textbf{Mi}$xture of $\\textbf{G}$eneralists and\n$\\textbf{S}$pecialists), the first approach to address both challenges. A key\ninnovation of our method is the bi-level optimization formulation of the\nMixture-of-Experts learning objective, where the router is optimized using a\nseparate validation set to ensure alignment with the target distribution. We\nsolve our objective with alternating minimization, for which we provide a\ntheoretical analysis. Our method shares generalist experts across users while\nlocalizing a varying number of specialist experts, thereby adapting to users'\ncomputational resources and preserving privacy. Through extensive experiments,\nwe show CoMiGS effectively balances general and personalized knowledge for each\ntoken generation. We demonstrate that CoMiGS remains robust against\noverfitting-due to the generalists' regularizing effect-while adapting to local\ndata through specialist expertise. We open source our codebase for\ncollaborative LLMs.", "published": "2024-09-20 22:34:37", "link": "http://arxiv.org/abs/2409.13931v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prompting Large Language Models for Supporting the Differential\n  Diagnosis of Anemia", "abstract": "In practice, clinicians achieve a diagnosis by following a sequence of steps,\nsuch as laboratory exams, observations, or imaging. The pathways to reach\ndiagnosis decisions are documented by guidelines authored by expert\norganizations, which guide clinicians to reach a correct diagnosis through\nthese sequences of steps. While these guidelines are beneficial for following\nmedical reasoning and consolidating medical knowledge, they have some\ndrawbacks. They often fail to address patients with uncommon conditions due to\ntheir focus on the majority population, and are slow and costly to update,\nmaking them unsuitable for rapidly emerging diseases or new practices. Inspired\nby clinical guidelines, our study aimed to develop pathways similar to those\nthat can be obtained in clinical guidelines. We tested three Large Language\nModels (LLMs) -Generative Pretrained Transformer 4 (GPT-4), Large Language\nModel Meta AI (LLaMA), and Mistral -on a synthetic yet realistic dataset to\ndifferentially diagnose anemia and its subtypes. By using advanced prompting\ntechniques to enhance the decision-making process, we generated diagnostic\npathways using these models. Experimental results indicate that LLMs hold huge\npotential in clinical pathway discovery from patient data, with GPT-4\nexhibiting the best performance in all conducted experiments.", "published": "2024-09-20 06:47:36", "link": "http://arxiv.org/abs/2409.15377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino", "abstract": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.", "published": "2024-09-20 15:01:21", "link": "http://arxiv.org/abs/2409.15380v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multiple-Fill-in-the-Blank Exam Approach for Enhancing Zero-Resource\n  Hallucination Detection in Large Language Models", "abstract": "Large language models (LLMs) often fabricate a hallucinatory text. Several\nmethods have been developed to detect such text by semantically comparing it\nwith the multiple versions probabilistically regenerated. However, a\nsignificant issue is that if the storyline of each regenerated text changes,\nthe generated texts become incomparable, which worsen detection accuracy. In\nthis paper, we propose a hallucination detection method that incorporates a\nmultiple-fill-in-the-blank exam approach to address this storyline-changing\nissue. First, our method creates a multiple-fill-in-the-blank exam by masking\nmultiple objects from the original text. Second, prompts an LLM to repeatedly\nanswer this exam. This approach ensures that the storylines of the exam answers\nalign with the original ones. Finally, quantifies the degree of hallucination\nfor each original sentence by scoring the exam answers, considering the\npotential for \\emph{hallucination snowballing} within the original text itself.\nExperimental results show that our method alone not only outperforms existing\nmethods, but also achieves clearer state-of-the-art performance in the\nensembles with existing methods.", "published": "2024-09-20 04:34:30", "link": "http://arxiv.org/abs/2409.17173v1", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal\n  Significance and Consistency", "abstract": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal hallucinations between a step of reasoning and corresponding state\ntransitions are becoming a significant obstacle to advancing LLMs' reasoning\ncapabilities, especially in long-range reasoning tasks. This paper proposes a\nnon-chain-based reasoning framework for simultaneous consideration of causal\nsignificance and consistency, i.e., the Causal Significance and Consistency\nEnhancer (CSCE). We customize LLM's loss function utilizing treatment effect\nassessments to enhance its reasoning ability from two aspects: causal\nsignificance and consistency. This ensures that the model captures essential\ncausal relationships and maintains robust and consistent performance across\nvarious scenarios. Additionally, we transform the reasoning process from the\ncascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks.", "published": "2024-09-20 08:28:23", "link": "http://arxiv.org/abs/2409.17174v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical\n  Tasks in Diabetes Care and Management", "abstract": "Diabetes is a chronic disease with a significant global health burden,\nrequiring multi-stakeholder collaboration for optimal management. Large\nlanguage models (LLMs) have shown promise in various healthcare scenarios, but\ntheir effectiveness across diverse diabetes tasks remains unproven. Our study\nintroduced a framework to train and validate diabetes-specific LLMs. We first\ndeveloped a comprehensive data processing pipeline that includes data\ncollection, filtering, augmentation and refinement. This created a\nhigh-quality, diabetes-specific dataset and evaluation benchmarks from scratch.\nFine-tuned on the collected training dataset, our diabetes-specific LLM family\ndemonstrated state-of-the-art proficiency in processing various diabetes tasks\ncompared to other LLMs. Furthermore, clinical studies revealed the potential\napplications of our models in diabetes care, including providing personalized\nhealthcare, assisting medical education, and streamlining clinical tasks.\nGenerally, our introduced framework helps develop diabetes-specific LLMs and\nhighlights their potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes management across different end\nusers. Our codes, benchmarks and models are available at\nhttps://github.com/waltonfuture/Diabetica.", "published": "2024-09-20 03:47:54", "link": "http://arxiv.org/abs/2409.13191v2", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChemDFM-X: Towards Large Multimodal Model for Chemistry", "abstract": "Rapid developments of AI tools are expected to offer unprecedented assistance\nto the research of natural science including chemistry. However, neither\nexisting unimodal task-specific specialist models nor emerging general large\nmultimodal models (LMM) can cover the wide range of chemical data modality and\ntask categories. To address the real demands of chemists, a cross-modal\nChemical General Intelligence (CGI) system, which serves as a truly practical\nand useful research assistant utilizing the great potential of LMMs, is in\ngreat need. In this work, we introduce the first Cross-modal Dialogue\nFoundation Model for Chemistry (ChemDFM-X). Diverse multimodal data are\ngenerated from an initial modality by approximate calculations and\ntask-specific model predictions. This strategy creates sufficient chemical\ntraining corpora, while significantly reducing excessive expense, resulting in\nan instruction-tuning dataset containing 7.6M data. After instruction\nfinetuning, ChemDFM-X is evaluated on extensive experiments of different\nchemical tasks with various data modalities. The results demonstrate the\ncapacity of ChemDFM-X for multimodal and inter-modal knowledge comprehension.\nChemDFM-X marks a significant milestone toward aligning all modalities in\nchemistry, a step closer to CGI.", "published": "2024-09-20 03:55:34", "link": "http://arxiv.org/abs/2409.13194v2", "categories": ["cs.LG", "cs.CL", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Exploring Scaling Laws for Local SGD in Large Language Model Training", "abstract": "This paper investigates scaling laws for local SGD in LLM training, a\ndistributed optimization algorithm that facilitates training on loosely\nconnected devices. Through extensive experiments, we show that local SGD\nachieves competitive results compared to conventional methods, given equivalent\nmodel parameters, datasets, and computational resources. Furthermore, we\nexplore the application of local SGD in various practical scenarios, including\nmulti-cluster setups and edge computing environments. Our findings elucidate\nthe necessary conditions for effective multi-cluster LLM training and examine\nthe potential and limitations of leveraging edge computing resources in the LLM\ntraining process. This demonstrates its viability as an alternative to single\nlarge-cluster training.", "published": "2024-09-20 04:02:48", "link": "http://arxiv.org/abs/2409.13198v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "RLHFuse: Efficient RLHF Training for Large Language Models with Inter-\n  and Intra-Stage Fusion", "abstract": "Reinforcement Learning from Human Feedback (RLHF) enhances the alignment\nbetween LLMs and human preference. The workflow of RLHF typically involves\nseveral models and tasks in a series of distinct stages. Existing RLHF training\nsystems view each task as the smallest execution unit thus overlooking the\nopportunities for subtask-level optimizations. Due to the intrinsic nature of\nRLHF training, i.e., the data skewness in the generation stage, and the\npipeline bubbles in the training stage, existing RLHF systems suffer from low\nGPU utilization in production deployments.\n  RLHFuse breaks the traditional view of RLHF workflow as a composition of\nindividual tasks, splitting each task into finer-grained subtasks, and\nperforming stage fusion to improve GPU utilization. RLHFuse contains two key\nideas. First, for generation and inference tasks, RLHFuse splits them into\nsample-level subtasks, enabling efficient inter-stage fusion to mitigate the\noriginal generation bottleneck dominated by long-tailed samples. Second, for\ntraining tasks, RLHFuse breaks them into subtasks of micro-batches. By\nleveraging the intuition that pipeline execution can be essentially\ncomplemented by another pipeline, RLHFuse performs intra-stage fusion to\nconcurrently execute these subtasks in the training stage with a fused pipeline\nschedule, resulting in fewer pipeline bubbles. In addition, RLHFuse\nincorporates a series of system optimizations tailored for each stage of RLHF,\nmaking it efficient and scalable for our internal product usage. We evaluate\nRLHFuse on various popular LLMs and the results show that RLHFuse increases the\ntraining throughput by up to 3.7x, compared to existing state-of-the-art\nsystems.", "published": "2024-09-20 05:15:38", "link": "http://arxiv.org/abs/2409.13221v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Large Language Model Should Understand Pinyin for Chinese ASR Error\n  Correction", "abstract": "Large language models can enhance automatic speech recognition systems\nthrough generative error correction. In this paper, we propose Pinyin-enhanced\nGEC, which leverages Pinyi, the phonetic representation of Mandarin Chinese, as\nsupplementary information to improve Chinese ASR error correction. Our approach\nonly utilizes synthetic errors for training and employs the one-best hypothesis\nduring inference. Additionally, we introduce a multitask training approach\ninvolving conversion tasks between Pinyin and text to align their feature\nspaces. Experiments on the Aishell-1 and the Common Voice datasets demonstrate\nthat our approach consistently outperforms GEC with text-only input. More\nimportantly, we provide intuitive explanations for the effectiveness of PY-GEC\nand multitask training from two aspects: 1) increased attention weight on\nPinyin features; and 2) aligned feature space between Pinyin and text hidden\nstates.", "published": "2024-09-20 06:50:56", "link": "http://arxiv.org/abs/2409.13262v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SLaVA-CXR: Small Language and Vision Assistant for Chest X-ray Report\n  Automation", "abstract": "Inspired by the success of large language models (LLMs), there is growing\nresearch interest in developing LLMs in the medical domain to assist\nclinicians. However, for hospitals, using closed-source commercial LLMs\ninvolves privacy issues, and developing open-source public LLMs requires\nlarge-scale computational resources, which are usually limited, especially in\nresource-efficient regions and low-income countries. We propose an open-source\nSmall Language and Vision Assistant (SLaVA-CXR) that can be used for Chest\nX-Ray report automation. To efficiently train a small assistant, we first\npropose the Re$^3$Training method, which simulates the cognitive development of\nradiologists and optimizes the model in the Recognition, Reasoning, and\nReporting training manner. Then, we introduce a data synthesis method, RADEX,\nwhich can generate a high-quality and diverse training corpus with privacy\nregulation compliance. The extensive experiments show that our SLaVA-CXR built\non a 2.7B backbone not only outperforms but also achieves 6 times faster\ninference efficiency than previous state-of-the-art larger models.", "published": "2024-09-20 08:28:46", "link": "http://arxiv.org/abs/2409.13321v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Beyond the binary: Limitations and possibilities of gender-related\n  speech technology research", "abstract": "This paper presents a review of 107 research papers relating to speech and\nsex or gender in ISCA Interspeech publications between 2013 and 2023. We note\nthe scarcity of work on this topic and find that terminology, particularly the\nword gender, is used in ways that are underspecified and often out of step with\nthe prevailing view in social sciences that gender is socially constructed and\nis a spectrum as opposed to a binary category. We draw attention to the\npotential problems that this can cause for already marginalised groups, and\nsuggest some questions for researchers to ask themselves when undertaking work\non speech and gender.", "published": "2024-09-20 08:56:09", "link": "http://arxiv.org/abs/2409.13335v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "'Since Lawyers are Males..': Examining Implicit Gender Bias in Hindi\n  Language Generation by LLMs", "abstract": "Large Language Models (LLMs) are increasingly being used to generate text\nacross various languages, for tasks such as translation, customer support, and\neducation. Despite these advancements, LLMs show notable gender biases in\nEnglish, which become even more pronounced when generating content in\nrelatively underrepresented languages like Hindi. This study explores implicit\ngender biases in Hindi text generation and compares them to those in English.\nWe developed Hindi datasets inspired by WinoBias to examine stereotypical\npatterns in responses from models like GPT-4o and Claude-3 sonnet. Our results\nreveal a significant gender bias of 87.8% in Hindi, compared to 33.4% in\nEnglish GPT-4o generation, with Hindi responses frequently relying on gender\nstereotypes related to occupations, power hierarchies, and social class. This\nresearch underscores the variation in gender biases across languages and\nprovides considerations for navigating these biases in generative AI systems.", "published": "2024-09-20 13:16:58", "link": "http://arxiv.org/abs/2409.13484v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation\n  with Whisper", "abstract": "The training of automatic speech recognition (ASR) with little to no\nsupervised data remains an open question. In this work, we demonstrate that\nstreaming Transformer-Transducer (TT) models can be trained from scratch in\nconsumer and accessible GPUs in their entirety with pseudo-labeled (PL) speech\nfrom foundational speech models (FSM). This allows training a robust ASR model\njust in one stage and does not require large data and computational budget\ncompared to the two-step scenario with pre-training and fine-tuning. We perform\na comprehensive ablation on different aspects of PL-based streaming TT models\nsuch as the impact of (1) shallow fusion of n-gram LMs, (2) contextual biasing\nwith named entities, (3) chunk-wise decoding for low-latency streaming\napplications, and (4) TT overall performance as the function of the FSM size.\nOur results demonstrate that TT can be trained from scratch without supervised\ndata, even with very noisy PLs. We validate the proposed framework on 6\nlanguages from CommonVoice and propose multiple heuristics to filter out\nhallucinated PLs.", "published": "2024-09-20 13:38:59", "link": "http://arxiv.org/abs/2409.13499v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LM-assisted keyword biasing with Aho-Corasick algorithm for\n  Transducer-based ASR", "abstract": "Despite the recent success of end-to-end models for automatic speech\nrecognition, recognizing special rare and out-of-vocabulary words, as well as\nfast domain adaptation with text, are still challenging. It often happens that\nbiasing to the special entities leads to a degradation in the overall\nperformance. We propose a light on-the-fly method to improve automatic speech\nrecognition performance by combining a bias list of named entities with a\nword-level n-gram language model with the shallow fusion approach based on the\nAho-Corasick string matching algorithm. The Aho-Corasick algorithm has proved\nto be more efficient than other methods and allows fast context adaptation. An\nn-gram language model is introduced as a graph with fail and output arcs, where\nthe arc weights are adapted from the n-gram probabilities. The language model\nis used as an additional support to keyword biasing when the language model is\ncombined with bias entities in a single context graph to take care of the\noverall performance. We demonstrate our findings on 4 languages, 2 public and 1\nprivate datasets including performance on named entities and out-of-vocabulary\nentities. We achieve up to 21.6% relative improvement in the general word error\nrate with no practical difference in the inverse real-time factor.", "published": "2024-09-20 13:53:37", "link": "http://arxiv.org/abs/2409.13514v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EMMeTT: Efficient Multimodal Machine Translation Training", "abstract": "A rising interest in the modality extension of foundation language models\nwarrants discussion on the most effective, and efficient, multimodal training\napproach. This work focuses on neural machine translation (NMT) and proposes a\njoint multimodal training regime of Speech-LLM to include automatic speech\ntranslation (AST). We investigate two different foundation model architectures,\ndecoder-only GPT and encoder-decoder T5, extended with Canary-1B's speech\nencoder. To handle joint multimodal training, we propose a novel training\nframework called EMMeTT. EMMeTT improves training efficiency with the\nfollowing: balanced sampling across languages, datasets, and modalities;\nefficient sequential data iteration; and a novel 2D bucketing scheme for\nmultimodal data, complemented by a batch size optimizer (OOMptimizer). We show\nthat a multimodal training consistently helps with both architectures.\nMoreover, SALM-T5 trained with EMMeTT retains the original NMT capability while\noutperforming AST baselines on four-language subsets of FLORES and FLEURS. The\nresultant Multimodal Translation Model produces strong text and speech\ntranslation results at the same time.", "published": "2024-09-20 14:03:23", "link": "http://arxiv.org/abs/2409.13523v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Contextualized Data-Wrangling Code Generation in Computational Notebooks", "abstract": "Data wrangling, the process of preparing raw data for further analysis in\ncomputational notebooks, is a crucial yet time-consuming step in data science.\nCode generation has the potential to automate the data wrangling process to\nreduce analysts' overhead by translating user intents into executable code.\nPrecisely generating data wrangling code necessitates a comprehensive\nconsideration of the rich context present in notebooks, including textual\ncontext, code context and data context. However, notebooks often interleave\nmultiple non-linear analysis tasks into linear sequence of code blocks, where\nthe contextual dependencies are not clearly reflected. Directly training models\nwith source code blocks fails to fully exploit the contexts for accurate\nwrangling code generation.\n  To bridge the gap, we aim to construct a high quality datasets with clear and\nrich contexts to help training models for data wrangling code generation tasks.\nIn this work, we first propose an automated approach, CoCoMine to mine\ndata-wrangling code generation examples with clear multi-modal contextual\ndependency. It first adopts data flow analysis to identify the code blocks\ncontaining data wrangling codes. Then, CoCoMine extracts the contextualized\ndatawrangling code examples through tracing and replaying notebooks. With\nCoCoMine, we construct CoCoNote, a dataset containing 58,221 examples for\nContextualized Data-wrangling Code generation in Notebooks. To demonstrate the\neffectiveness of our dataset, we finetune a range of pretrained code models and\nprompt various large language models on our task. Furthermore, we also propose\nDataCoder, which encodes data context and code&textual contexts separately to\nenhance code generation. Experiment results demonstrate the significance of\nincorporating data context in data-wrangling code generation and the\neffectiveness of our model. We release code and data at url...", "published": "2024-09-20 14:49:51", "link": "http://arxiv.org/abs/2409.13551v1", "categories": ["cs.SE", "cs.CL", "cs.DB"], "primary_category": "cs.SE"}
{"title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating\n  Satire Comprehension capability of Vision-Language Models", "abstract": "Understanding satire and humor is a challenging task for even current\nVision-Language models. In this paper, we propose the challenging tasks of\nSatirical Image Detection (detecting whether an image is satirical),\nUnderstanding (generating the reason behind the image being satirical), and\nCompletion (given one half of the image, selecting the other half from 2 given\noptions, such that the complete image is satirical) and release a high-quality\ndataset YesBut, consisting of 2547 images, 1084 satirical and 1463\nnon-satirical, containing different artistic styles, to evaluate those tasks.\nEach satirical image in the dataset depicts a normal scenario, along with a\nconflicting scenario which is funny or ironic. Despite the success of current\nVision-Language Models on multimodal tasks such as Visual QA and Image\nCaptioning, our benchmarking experiments show that such models perform poorly\non the proposed tasks on the YesBut Dataset in Zero-Shot Settings w.r.t both\nautomated as well as human evaluation. Additionally, we release a dataset of\n119 real, satirical photographs for further research. The dataset and code are\navailable at https://github.com/abhi1nandy2/yesbut_dataset.", "published": "2024-09-20 15:45:29", "link": "http://arxiv.org/abs/2409.13592v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring\n  Expression Comprehension", "abstract": "Referring Expression Comprehension (REC), which aims to ground a local visual\nregion via natural language, is a task that heavily relies on multimodal\nalignment. Most existing methods utilize powerful pre-trained models to\ntransfer visual/linguistic knowledge by full fine-tuning. However, full\nfine-tuning the entire backbone not only breaks the rich prior knowledge\nembedded in the pre-training, but also incurs significant computational costs.\nMotivated by the recent emergence of Parameter-Efficient Transfer Learning\n(PETL) methods, we aim to solve the REC task in an effective and efficient\nmanner. Directly applying these PETL methods to the REC task is inappropriate,\nas they lack the specific-domain abilities for precise local visual perception\nand visual-language alignment. Therefore, we propose a novel framework of\nMultimodal Prior-guided Parameter Efficient Tuning, namely MaPPER.\nSpecifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned\nprior, and Local Convolution Adapters to extract precise local semantics for\nbetter visual perception. Moreover, the Prior-Guided Text module is proposed to\nfurther utilize the prior for facilitating the cross-modal alignment.\nExperimental results on three widely-used benchmarks demonstrate that MaPPER\nachieves the best accuracy compared to the full fine-tuning and other PETL\nmethods with only 1.41% tunable backbone parameters. Our code is available at\nhttps://github.com/liuting20/MaPPER.", "published": "2024-09-20 16:12:26", "link": "http://arxiv.org/abs/2409.13609v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Advancing Event Causality Identification via Heuristic Semantic\n  Dependency Inquiry Network", "abstract": "Event Causality Identification (ECI) focuses on extracting causal relations\nbetween events in texts. Existing methods for ECI primarily rely on causal\nfeatures and external knowledge. However, these approaches fall short in two\ndimensions: (1) causal features between events in a text often lack explicit\nclues, and (2) external knowledge may introduce bias, while specific problems\nrequire tailored analyses. To address these issues, we propose SemDI - a simple\nand effective Semantic Dependency Inquiry Network for ECI. SemDI captures\nsemantic dependencies within the context using a unified encoder. Then, it\nutilizes a Cloze Analyzer to generate a fill-in token based on comprehensive\ncontext understanding. Finally, this fill-in token is used to inquire about the\ncausal relation between two events. Extensive experiments demonstrate the\neffectiveness of SemDI, surpassing state-of-the-art methods on three widely\nused benchmarks. Code is available at https://github.com/hrlics/SemDI.", "published": "2024-09-20 16:32:54", "link": "http://arxiv.org/abs/2409.13621v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory\n  for Robot Navigation", "abstract": "Navigating and understanding complex environments over extended periods of\ntime is a significant challenge for robots. People interacting with the robot\nmay want to ask questions like where something happened, when it occurred, or\nhow long ago it took place, which would require the robot to reason over a long\nhistory of their deployment. To address this problem, we introduce a\nRetrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed\nfor long-horizon video question answering for robot navigation. To evaluate\nReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal,\nand descriptive questions to long-horizon robot navigation videos. ReMEmbR\nemploys a structured approach involving a memory building and a querying phase,\nleveraging temporal information, spatial information, and images to efficiently\nhandle continuously growing robot histories. Our experiments demonstrate that\nReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve\neffective long-horizon reasoning with low latency. Additionally, we deploy\nReMEmbR on a robot and show that our approach can handle diverse queries. The\ndataset, code, videos, and other material can be found at the following link:\nhttps://nvidia-ai-iot.github.io/remembr", "published": "2024-09-20 17:50:07", "link": "http://arxiv.org/abs/2409.13682v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Learning to Generalize Unseen Domains via Multi-Source Meta Learning for\n  Text Classification", "abstract": "With the rapid development of deep learning methods, there have been many\nbreakthroughs in the field of text classification. Models developed for this\ntask have been shown to achieve high accuracy. However, most of these models\nare trained using labeled data from seen domains. It is difficult for these\nmodels to maintain high accuracy in a new challenging unseen domain, which is\ndirectly related to the generalization of the model. In this paper, we study\nthe multi-source Domain Generalization of text classification and propose a\nframework to use multiple seen domains to train a model that can achieve high\naccuracy in an unseen domain. Specifically, we propose a multi-source\nmeta-learning Domain Generalization framework to simulate the process of model\ngeneralization to an unseen domain, so as to extract sufficient domain-related\nfeatures. We introduced a memory mechanism to store domain-specific features,\nwhich coordinate with the meta-learning framework. Besides, we adopt the novel\n\"jury\" mechanism that enables the model to learn sufficient domain-invariant\nfeatures. Experiments demonstrate that our meta-learning framework can\neffectively enhance the ability of the model to generalize to an unseen domain\nand can outperform the state-of-the-art methods on multi-source text\nclassification datasets.", "published": "2024-09-20 07:46:21", "link": "http://arxiv.org/abs/2409.13787v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Measuring Copyright Risks of Large Language Model via Partial\n  Information Probing", "abstract": "Exploring the data sources used to train Large Language Models (LLMs) is a\ncrucial direction in investigating potential copyright infringement by these\nmodels. While this approach can identify the possible use of copyrighted\nmaterials in training data, it does not directly measure infringing risks.\nRecent research has shifted towards testing whether LLMs can directly output\ncopyrighted content. Addressing this direction, we investigate and assess LLMs'\ncapacity to generate infringing content by providing them with partial\ninformation from copyrighted materials, and try to use iterative prompting to\nget LLMs to generate more infringing content. Specifically, we input a portion\nof a copyrighted text into LLMs, prompt them to complete it, and then analyze\nthe overlap between the generated content and the original copyrighted\nmaterial. Our findings demonstrate that LLMs can indeed generate content highly\noverlapping with copyrighted materials based on these partial inputs.", "published": "2024-09-20 18:16:05", "link": "http://arxiv.org/abs/2409.13831v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music\n  Scores for All Singing Tasks", "abstract": "The scarcity of high-quality and multi-task singing datasets significantly\nhinders the development of diverse controllable and personalized singing tasks,\nas existing singing datasets suffer from low quality, limited diversity of\nlanguages and singers, absence of multi-technique information and realistic\nmusic scores, and poor task suitability. To tackle these problems, we present\nGTSinger, a large global, multi-technique, free-to-use, high-quality singing\ncorpus with realistic music scores, designed for all singing tasks, along with\nits benchmarks. Particularly, (1) we collect 80.59 hours of high-quality\nsinging voices, forming the largest recorded singing dataset; (2) 20\nprofessional singers across nine widely spoken languages offer diverse timbres\nand styles; (3) we provide controlled comparison and phoneme-level annotations\nof six commonly used singing techniques, helping technique modeling and\ncontrol; (4) GTSinger offers realistic music scores, assisting real-world\nmusical composition; (5) singing voices are accompanied by manual\nphoneme-to-audio alignments, global style labels, and 16.16 hours of paired\nspeech for various singing tasks. Moreover, to facilitate the use of GTSinger,\nwe conduct four benchmark experiments: technique-controllable singing voice\nsynthesis, technique recognition, style transfer, and speech-to-singing\nconversion. The corpus and demos can be found at\nhttp://aaronz345.github.io/GTSingerDemo/. We provide the dataset and the code\nfor processing data and conducting benchmarks at\nhttps://huggingface.co/datasets/GTSinger/GTSinger and\nhttps://github.com/AaronZ345/GTSinger.", "published": "2024-09-20 18:18:14", "link": "http://arxiv.org/abs/2409.13832v5", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on\n  Offensive Progressions", "abstract": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has\nbecome a critical focus in the field of natural language processing. However,\nmany current methodologies evaluate scenarios in isolation, without considering\nthe broader context or the spectrum of potential biases within each situation.\nTo address this, we introduce the Sensitivity Testing on Offensive Progressions\n(STOP) dataset, which includes 450 offensive progressions containing 2,700\nunique sentences of varying severity that progressively escalate from less to\nmore explicitly offensive. Covering a broad spectrum of 9 demographics and 46\nsub-demographics, STOP ensures inclusivity and comprehensive coverage. We\nevaluate several leading closed- and open-source models, including GPT-4,\nMixtral, and Llama 3. Our findings reveal that even the best-performing models\ndetect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We\nalso demonstrate how aligning models with human judgments on STOP can improve\nmodel answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs\nby up to 191%, while maintaining or even improving performance. STOP presents a\nnovel framework for assessing the complex nature of biases in LLMs, which will\nenable more effective bias mitigation strategies and facilitates the creation\nof fairer language models.", "published": "2024-09-20 18:34:38", "link": "http://arxiv.org/abs/2409.13843v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Unlocking Memorization in Large Language Models with Dynamic Soft\n  Prompting", "abstract": "Pretrained large language models (LLMs) have revolutionized natural language\nprocessing (NLP) tasks such as summarization, question answering, and\ntranslation. However, LLMs pose significant security risks due to their\ntendency to memorize training data, leading to potential privacy breaches and\ncopyright infringement. Accurate measurement of this memorization is essential\nto evaluate and mitigate these potential risks. However, previous attempts to\ncharacterize memorization are constrained by either using prefixes only or by\nprepending a constant soft prompt to the prefixes, which cannot react to\nchanges in input. To address this challenge, we propose a novel method for\nestimating LLM memorization using dynamic, prefix-dependent soft prompts. Our\napproach involves training a transformer-based generator to produce soft\nprompts that adapt to changes in input, thereby enabling more accurate\nextraction of memorized data. Our method not only addresses the limitations of\nprevious methods but also demonstrates superior performance in diverse\nexperimental settings compared to state-of-the-art techniques. In particular,\nour method can achieve the maximum relative improvement of 112.75% and 32.26%\nover the vanilla baseline in terms of discoverable memorization rate for the\ntext generation task and code generation task respectively.", "published": "2024-09-20 18:56:32", "link": "http://arxiv.org/abs/2409.13853v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative AI Carries Non-Democratic Biases and Stereotypes:\n  Representation of Women, Black Individuals, Age Groups, and People with\n  Disability in AI-Generated Images across Occupations", "abstract": "AI governance and ethics in AI development have become critical concerns,\nprompting active discussions among tech companies, governments, and researchers\nabout the potential risks AI poses to our democracies. This short essay aims to\nhighlight one such risk: how generative AI includes or excludes\nequity-deserving groups in its outputs. The findings reveal that generative AI\nis not equitably inclusive regarding gender, race, age, and visible disability.", "published": "2024-09-20 19:47:31", "link": "http://arxiv.org/abs/2409.13869v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Instruct-Tuning Pretrained Causal Language Models for Ancient Greek\n  Papyrology and Epigraphy", "abstract": "This article presents an experiment in fine-tuning a pretrained causal\nlanguage model (Meta's Llama 3.1 8B Instruct) to assist with restoring missing\nor illegible characters in ancient Greek inscriptions and documentary papyri.\nUtilizing a straightforward instruction-based approach and a 95%/5% train/test\nsplit, the papyrus restoration model achieved a character error rate (CER) of\n14.9%, a top-1 accuracy of 73.5%, and a top-20 accuracy of 86.0% for sequences\nup to 10 characters. A model was also fine-tuned for geographic attribution,\nreaching a top-1 accuracy of 66.4% and a top-3 accuracy of 79.9%. In\nchronological attribution, it demonstrated an average deviation of 21.7 years\nfrom the actual terminus post/ante quem, with a median deviation of 0 years.\nFor inscriptions, the restoration model achieved a CER of 20.5%, a top-1\naccuracy of 63.7%, and a top-20 accuracy of 83.0% for sequences up to 10\ncharacters. In geographic attribution, it attained a top-1 accuracy of 75.0%\nand a top-3 accuracy of 83.7%, while in dating, it had an average deviation of\n37.1 years and a median deviation of 3 years from the actual date range.\nBenchmarked against the state-of-the-art model (Ithaca) on a shared test set\nand on recently edited inscriptions, the instruction-tuned models excelled in\ntext restoration, while also offering the practical advantage of ignoring\nspaces during reconstruction, which aligns with the scriptio continua of\nancient textual artifacts. However, their performance in geographic and\nchronological attribution was lower than Ithaca's. To evaluate the approach in\na more even setup, the instruction model was retrained with an 80%/10%/10%\ntrain-validation-test split, and still outperformed Ithaca in text restoration.\nThe results suggest that fine-tuning larger pretrained causal language models\nusing instruction templates for emendations and conjectures to ancient texts\nholds promise.", "published": "2024-09-20 19:49:45", "link": "http://arxiv.org/abs/2409.13870v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multi-LLM Debiasing Framework", "abstract": "Large Language Models (LLMs) are powerful tools with the potential to benefit\nsociety immensely, yet, they have demonstrated biases that perpetuate societal\ninequalities. Despite significant advancements in bias mitigation techniques\nusing data augmentation, zero-shot prompting, and model fine-tuning, biases\ncontinuously persist, including subtle biases that may elude human detection.\nRecent research has shown a growing interest in multi-LLM approaches, which\nhave been demonstrated to be effective in improving the quality of reasoning\nand factuality in LLMs. Building on this approach, we propose a novel multi-LLM\ndebiasing framework aimed at reducing bias in LLMs. Our work is the first to\nintroduce and evaluate two distinct approaches within this framework for\ndebiasing LLMs: a centralized method, where the conversation is facilitated by\na single central LLM, and a decentralized method, where all models communicate\ndirectly. Our findings reveal that our multi-LLM framework significantly\nreduces bias in LLMs, outperforming the baseline method across several social\ngroups.", "published": "2024-09-20 20:24:50", "link": "http://arxiv.org/abs/2409.13884v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Target word activity detector: An approach to obtain ASR word boundaries\n  without lexicon", "abstract": "Obtaining word timestamp information from end-to-end (E2E) ASR models remains\nchallenging due to the lack of explicit time alignment during training. This\nissue is further complicated in multilingual models. Existing methods, either\nrely on lexicons or introduce additional tokens, leading to scalability issues\nand increased computational costs. In this work, we propose a new approach to\nestimate word boundaries without relying on lexicons. Our method leverages word\nembeddings from sub-word token units and a pretrained ASR model, requiring only\nword alignment information during training. Our proposed method can scale-up to\nany number of languages without incurring any additional cost. We validate our\napproach using a multilingual ASR model trained on five languages and\ndemonstrate its effectiveness against a strong baseline.", "published": "2024-09-20 21:40:18", "link": "http://arxiv.org/abs/2409.13913v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Eliciting Instruction-tuned Code Language Models' Capabilities to\n  Utilize Auxiliary Function for Code Generation", "abstract": "We study the code generation behavior of instruction-tuned models built on\ntop of code pre-trained language models when they could access an auxiliary\nfunction to implement a function. We design several ways to provide auxiliary\nfunctions to the models by adding them to the query or providing a response\nprefix to incorporate the ability to utilize auxiliary functions with the\ninstruction-following capability. Our experimental results show the\neffectiveness of combining the base models' auxiliary function utilization\nability with the instruction following ability. In particular, the performance\nof adopting our approaches with the open-sourced language models surpasses that\nof the recent powerful proprietary language models, i.e., gpt-4o.", "published": "2024-09-20 22:28:20", "link": "http://arxiv.org/abs/2409.13928v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "MirrorStories: Reflecting Diversity through Personalized Narrative\n  Generation with Large Language Models", "abstract": "This study explores the effectiveness of Large Language Models (LLMs) in\ncreating personalized \"mirror stories\" that reflect and resonate with\nindividual readers' identities, addressing the significant lack of diversity in\nliterature. We present MirrorStories, a corpus of 1,500 personalized short\nstories generated by integrating elements such as name, gender, age, ethnicity,\nreader interest, and story moral. We demonstrate that LLMs can effectively\nincorporate diverse identity elements into narratives, with human evaluators\nidentifying personalized elements in the stories with high accuracy. Through a\ncomprehensive evaluation involving 26 diverse human judges, we compare the\neffectiveness of MirrorStories against generic narratives. We find that\npersonalized LLM-generated stories not only outscore generic human-written and\nLLM-generated ones across all metrics of engagement (with average ratings of\n4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity\nwhile preserving the intended moral. We also provide analyses that include bias\nassessments and a study on the potential for integrating images into\npersonalized stories.", "published": "2024-09-20 22:43:13", "link": "http://arxiv.org/abs/2409.13935v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist\n  with Tertiary Memory", "abstract": "Mental health issues, particularly depressive disorders, present significant\nchallenges in contemporary society, necessitating the development of effective\nautomated diagnostic methods. This paper introduces the Agent Mental Clinic\n(AMC), a self-improving conversational agent system designed to enhance\ndepression diagnosis through simulated dialogues between patient and\npsychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we\ndesign a psychiatrist agent consisting of a tertiary memory structure, a\ndialogue control and reflect plugin that acts as ``supervisor'' and a memory\nsampling module, fully leveraging the skills reflected by the psychiatrist\nagent, achieving great accuracy on depression risk and suicide risk diagnosis\nvia conversation. Experiment results on datasets collected in real-life\nscenarios demonstrate that the system, simulating the procedure of training\npsychiatrists, can be a promising optimization method for aligning LLMs with\nreal-life distribution in specific domains without modifying the weights of\nLLMs, even when only a few representative labeled cases are available.", "published": "2024-09-20 14:25:08", "link": "http://arxiv.org/abs/2409.15084v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ControlMath: Controllable Data Generation Promotes Math Generalist\n  Models", "abstract": "Utilizing large language models (LLMs) for data augmentation has yielded\nencouraging results in mathematical reasoning. However, these approaches face\nconstraints in problem diversity, potentially restricting them to\nin-domain/distribution data generation. To this end, we propose ControlMath, an\niterative method involving an equation-generator module and two LLM-based\nagents. The module creates diverse equations, which the Problem-Crafter agent\nthen transforms into math word problems. The Reverse-Agent filters and selects\nhigh-quality data, adhering to the \"less is more\" principle, achieving better\nresults with fewer data points. This approach enables the generation of diverse\nmath problems, not limited to specific domains or distributions. As a result,\nwe collect ControlMathQA, which involves 190k math word problems. Extensive\nresults prove that combining our dataset with in-domain datasets like GSM8K can\nhelp improve the model's mathematical ability to generalize, leading to\nimproved performances both within and beyond specific domains.", "published": "2024-09-20 03:58:26", "link": "http://arxiv.org/abs/2409.15376v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Toward Automated Clinical Transcriptions", "abstract": "Administrative documentation is a major driver of rising healthcare costs and\nis linked to adverse outcomes, including physician burnout and diminished\nquality of care. This paper introduces a secure system that applies recent\nadvancements in speech-to-text transcription and speaker-labeling (diarization)\nto patient-provider conversations. This system is optimized to produce accurate\ntranscriptions and highlight potential errors to promote rapid human\nverification, further reducing the necessary manual effort. Applied to over 40\nhours of simulated conversations, this system offers a promising foundation for\nautomating clinical transcriptions.", "published": "2024-09-20 13:12:11", "link": "http://arxiv.org/abs/2409.15378v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity", "abstract": "Recently, when dealing with high-resolution images, dominant LMMs usually\ndivide them into multiple local images and one global image, which will lead to\na large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM\nthat can adaptively select the appropriate visual granularity based on the\ninput image and instruction. This approach not only reduces the number of\nvisual tokens and speeds up inference, but also improves the overall model\nperformance. Specifically, we introduce the following modules based on\nLLaVA-NeXT: (a) a visual granularity scaler that includes multiple pooling\nlayers to obtain visual tokens with different granularities; (b) a visual\ngranularity router, which includes a Transformer layer, an MLP layer, and a\nvoter layer, used to select the appropriate visual granularity based on the\nimage and instruction. Furthermore, we propose RGLF, a novel training paradigm\nthat aims at aligning the granularity predicted by the router with the\npreferences of the LMM, without the need for additional manually annotated\ndata. Extensive experiments and analysis show that AVG-LLaVA achieves superior\nperformance across 11 benchmarks, as well as significantly reduces the number\nof visual tokens and speeds up inference (e.g., an 85.3% reduction in visual\ntokens and a 2.53$\\times$ increase in inference speed on the AI2D benchmark).", "published": "2024-09-20 10:50:21", "link": "http://arxiv.org/abs/2410.02745v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Utility of Multimodal Large Language Models in Analyzing Chest X-ray\n  with Incomplete Contextual Information", "abstract": "Background: Large language models (LLMs) are gaining use in clinical\nsettings, but their performance can suffer with incomplete radiology reports.\nWe tested whether multimodal LLMs (using text and images) could improve\naccuracy and understanding in chest radiography reports, making them more\neffective for clinical decision support.\n  Purpose: To assess the robustness of LLMs in generating accurate impressions\nfrom chest radiography reports using both incomplete data and multimodal data.\nMaterial and Methods: We used 300 radiology image-report pairs from the\nMIMIC-CXR database. Three LLMs (OpenFlamingo, MedFlamingo, IDEFICS) were tested\nin both text-only and multimodal formats. Impressions were first generated from\nthe full text, then tested by removing 20%, 50%, and 80% of the text. The\nimpact of adding images was evaluated using chest x-rays, and model performance\nwas compared using three metrics with statistical analysis.\n  Results: The text-only models (OpenFlamingo, MedFlamingo, IDEFICS) had\nsimilar performance (ROUGE-L: 0.39 vs. 0.21 vs. 0.21; F1RadGraph: 0.34 vs. 0.17\nvs. 0.17; F1CheXbert: 0.53 vs. 0.40 vs. 0.40), with OpenFlamingo performing\nbest on complete text (p<0.001). Performance declined with incomplete data\nacross all models. However, adding images significantly boosted the performance\nof MedFlamingo and IDEFICS (p<0.001), equaling or surpassing OpenFlamingo, even\nwith incomplete text. Conclusion: LLMs may produce low-quality outputs with\nincomplete radiology data, but multimodal LLMs can improve reliability and\nsupport clinical decision-making.\n  Keywords: Large language model; multimodal; semantic analysis; Chest\nRadiography; Clinical Decision Support;", "published": "2024-09-20 01:42:53", "link": "http://arxiv.org/abs/2410.07111v1", "categories": ["eess.IV", "cs.CL", "cs.CV"], "primary_category": "eess.IV"}
{"title": "End-Cloud Collaboration Framework for Advanced AI Customer Service in\n  E-commerce", "abstract": "In recent years, the e-commerce industry has seen a rapid increase in the\ndemand for advanced AI-driven customer service solutions. Traditional\ncloud-based models face limitations in terms of latency, personalized services,\nand privacy concerns. Furthermore, end devices often lack the computational\nresources to deploy large AI models effectively. In this paper, we propose an\ninnovative End-Cloud Collaboration (ECC) framework for advanced AI customer\nservice in e-commerce. This framework integrates the advantages of large cloud\nmodels and mid/small-sized end models by deeply exploring the generalization\npotential of cloud models and effectively utilizing the computing power\nresources of terminal chips, alleviating the strain on computing resources to\nsome extent. Specifically, the large cloud model acts as a teacher, guiding and\npromoting the learning of the end model, which significantly reduces the end\nmodel's reliance on large-scale, high-quality data and thereby addresses the\ndata bottleneck in traditional end model training, offering a new paradigm for\nthe rapid deployment of industry applications. Additionally, we introduce an\nonline evolutive learning strategy that enables the end model to continuously\niterate and upgrade based on guidance from the cloud model and real-time user\nfeedback. This strategy ensures that the model can flexibly adapt to the rapid\nchanges in application scenarios while avoiding the uploading of sensitive\ninformation by performing local fine-tuning, achieving the dual goals of\nprivacy protection and personalized service. %We make systematic contributions\nto the customized model fine-tuning methods in the e-commerce domain. To\nconclude, we implement in-depth corpus collection (e.g., data organization,\ncleaning, and preprocessing) and train an ECC-based industry-specific model for\ne-commerce customer service.", "published": "2024-09-20 13:46:54", "link": "http://arxiv.org/abs/2410.07122v1", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "Sketching With Your Voice: \"Non-Phonorealistic\" Rendering of Sounds via\n  Vocal Imitation", "abstract": "We present a method for automatically producing human-like vocal imitations\nof sounds: the equivalent of \"sketching,\" but for auditory rather than visual\nrepresentation. Starting with a simulated model of the human vocal tract, we\nfirst try generating vocal imitations by tuning the model's control parameters\nto make the synthesized vocalization match the target sound in terms of\nperceptually-salient auditory features. Then, to better match human intuitions,\nwe apply a cognitive theory of communication to take into account how human\nspeakers reason strategically about their listeners. Finally, we show through\nseveral experiments and user studies that when we add this type of\ncommunicative reasoning to our method, it aligns with human intuitions better\nthan matching auditory features alone does. This observation has broad\nimplications for the study of depiction in computer graphics.", "published": "2024-09-20 13:48:48", "link": "http://arxiv.org/abs/2409.13507v1", "categories": ["cs.GR", "cs.CL", "cs.HC", "cs.SD", "eess.AS", "I.3.8"], "primary_category": "cs.GR"}
{"title": "A Survey on Moral Foundation Theory and Pre-Trained Language Models:\n  Current Advances and Challenges", "abstract": "Moral values have deep roots in early civilizations, codified within norms\nand laws that regulated societal order and the common good. They play a crucial\nrole in understanding the psychological basis of human behavior and cultural\norientation. The Moral Foundation Theory (MFT) is a well-established framework\nthat identifies the core moral foundations underlying the manner in which\ndifferent cultures shape individual and social lives. Recent advancements in\nnatural language processing, particularly Pre-trained Language Models (PLMs),\nhave enabled the extraction and analysis of moral dimensions from textual data.\nThis survey presents a comprehensive review of MFT-informed PLMs, providing an\nanalysis of moral tendencies in PLMs and their application in the context of\nthe MFT. We also review relevant datasets and lexicons and discuss trends,\nlimitations, and future directions. By providing a structured overview of the\nintersection between PLMs and MFT, this work bridges moral psychology insights\nwithin the realm of PLMs, paving the way for further research and development\nin creating morally aware AI systems.", "published": "2024-09-20 14:03:06", "link": "http://arxiv.org/abs/2409.13521v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Impact of Large Language Models in Academia: from Writing to\n  Speaking", "abstract": "Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.", "published": "2024-09-20 17:54:16", "link": "http://arxiv.org/abs/2409.13686v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Audio-Only Data for Text-Queried Target Sound Extraction", "abstract": "The goal of text-queried target sound extraction (TSE) is to extract from a\nmixture a sound source specified with a natural-language caption. While it is\npreferable to have access to large-scale text-audio pairs to address a variety\nof text prompts, the limited number of available high-quality text-audio pairs\nhinders the data scaling. To this end, this work explores how to leverage\naudio-only data without any captions for the text-queried TSE task to\npotentially scale up the data amount. A straightforward way to do so is to use\na joint audio-text embedding model, such as the contrastive language-audio\npre-training (CLAP) model, as a query encoder and train a TSE model using audio\nembeddings obtained from the ground-truth audio. The TSE model can then accept\ntext queries at inference time by switching to the text encoder. While this\napproach should work if the audio and text embedding spaces in CLAP were well\naligned, in practice, the embeddings have domain-specific information that\ncauses the TSE model to overfit to audio queries. We investigate several\nmethods to avoid overfitting and show that simple embedding-manipulation\nmethods such as dropout can effectively alleviate this issue. Extensive\nexperiments demonstrate that using audio-only data with embedding dropout is as\neffective as using text captions during training, and audio-only data can be\neffectively leveraged to improve text-queried TSE models.", "published": "2024-09-20 01:31:29", "link": "http://arxiv.org/abs/2409.13152v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MuCodec: Ultra Low-Bitrate Music Codec", "abstract": "Music codecs are a vital aspect of audio codec research, and ultra\nlow-bitrate compression holds significant importance for music transmission and\ngeneration. Due to the complexity of music backgrounds and the richness of\nvocals, solely relying on modeling semantic or acoustic information cannot\neffectively reconstruct music with both vocals and backgrounds. To address this\nissue, we propose MuCodec, specifically targeting music compression and\nreconstruction tasks at ultra low bitrates. MuCodec employs MuEncoder to\nextract both acoustic and semantic features, discretizes them with RVQ, and\nobtains Mel-VAE features via flow-matching. The music is then reconstructed\nusing a pre-trained MEL-VAE decoder and HiFi-GAN. MuCodec can reconstruct\nhigh-fidelity music at ultra low (0.35kbps) or high bitrates (1.35kbps),\nachieving the best results to date in both subjective and objective metrics.\nCode and Demo: https://xuyaoxun.github.io/MuCodec_demo/.", "published": "2024-09-20 05:06:49", "link": "http://arxiv.org/abs/2409.13216v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Text-Queried Sound Event Detection with Audio Source\n  Separation", "abstract": "In sound event detection (SED), overlapping sound events pose a significant\nchallenge, as certain events can be easily masked by background noise or other\nevents, resulting in poor detection performance. To address this issue, we\npropose the text-queried SED (TQ-SED) framework. Specifically, we first\npre-train a language-queried audio source separation (LASS) model to separate\nthe audio tracks corresponding to different events from the input audio. Then,\nmultiple target SED branches are employed to detect individual events. AudioSep\nis a state-of-the-art LASS model, but has limitations in extracting dynamic\naudio information because of its pure convolutional structure for separation.\nTo address this, we integrate a dual-path recurrent neural network block into\nthe model. We refer to this structure as AudioSep-DP, which achieves the first\nplace in DCASE 2024 Task 9 on language-queried audio source separation\n(objective single model track). Experimental results show that TQ-SED can\nsignificantly improve the SED performance, with an improvement of 7.22\\% on F1\nscore over the conventional framework. Additionally, we setup comprehensive\nexperiments to explore the impact of model complexity. The source code and\npre-trained model are released at https://github.com/apple-yinhan/TQ-SED.", "published": "2024-09-20 07:42:04", "link": "http://arxiv.org/abs/2409.13292v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiffSound: Differentiable Modal Sound Rendering and Inverse Rendering\n  for Diverse Inference Tasks", "abstract": "Accurately estimating and simulating the physical properties of objects from\nreal-world sound recordings is of great practical importance in the fields of\nvision, graphics, and robotics. However, the progress in these directions has\nbeen limited -- prior differentiable rigid or soft body simulation techniques\ncannot be directly applied to modal sound synthesis due to the high sampling\nrate of audio, while previous audio synthesizers often do not fully model the\naccurate physical properties of the sounding objects. We propose DiffSound, a\ndifferentiable sound rendering framework for physics-based modal sound\nsynthesis, which is based on an implicit shape representation, a new high-order\nfinite element analysis module, and a differentiable audio synthesizer. Our\nframework can solve a wide range of inverse problems thanks to the\ndifferentiability of the entire pipeline, including physical parameter\nestimation, geometric shape reasoning, and impact position prediction.\nExperimental results demonstrate the effectiveness of our approach,\nhighlighting its ability to accurately reproduce the target sound in a\nphysics-based manner. DiffSound serves as a valuable tool for various sound\nsynthesis and analysis applications.", "published": "2024-09-20 13:18:45", "link": "http://arxiv.org/abs/2409.13486v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Directional Filtering: Far-Field Directivity Control With a Small\n  Microphone Array", "abstract": "Capturing audio signals with specific directivity patterns is essential in\nspeech communication. This study presents a deep neural network (DNN)-based\napproach to directional filtering, alleviating the need for explicit signal\nmodels. More specifically, our proposed method uses a DNN to estimate a\nsingle-channel complex mask from the signals of a microphone array. This mask\nis then applied to a reference microphone to render a signal that exhibits a\ndesired directivity pattern. We investigate the training dataset composition\nand its effect on the directivity realized by the DNN during inference. Using a\nrelatively small DNN, the proposed method is found to approximate the desired\ndirectivity pattern closely. Additionally, it allows for the realization of\nhigher-order directivity patterns using a small number of microphones, which is\na difficult task for linear and parametric directional filtering.", "published": "2024-09-20 13:42:50", "link": "http://arxiv.org/abs/2409.13502v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-shot Cross-lingual Voice Transfer for TTS", "abstract": "In this paper, we introduce a zero-shot Voice Transfer (VT) module that can\nbe seamlessly integrated into a multi-lingual Text-to-speech (TTS) system to\ntransfer an individual's voice across languages. Our proposed VT module\ncomprises a speaker-encoder that processes reference speech, a bottleneck\nlayer, and residual adapters, connected to preexisting TTS layers. We compare\nthe performance of various configurations of these components and report Mean\nOpinion Score (MOS) and Speaker Similarity across languages. Using a single\nEnglish reference speech per speaker, we achieve an average voice transfer\nsimilarity score of 73% across nine target languages. Vocal characteristics\ncontribute significantly to the construction and perception of individual\nidentity. The loss of one's voice, due to physical or neurological conditions,\ncan lead to a profound sense of loss, impacting one's core identity. As a case\nstudy, we demonstrate that our approach can not only transfer typical speech\nbut also restore the voices of individuals with dysarthria, even when only\natypical speech samples are available - a valuable utility for those who have\nnever had typical speech or banked their voice. Cross-lingual typical audio\nsamples, plus videos demonstrating voice restoration for dysarthric speakers\nare available here\n(google.github.io/tacotron/publications/zero_shot_voice_transfer).", "published": "2024-09-20 21:27:13", "link": "http://arxiv.org/abs/2409.13910v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LiSenNet: Lightweight Sub-band and Dual-Path Modeling for Real-Time\n  Speech Enhancement", "abstract": "Speech enhancement (SE) aims to extract the clean waveform from\nnoise-contaminated measurements to improve the speech quality and\nintelligibility. Although learning-based methods can perform much better than\ntraditional counterparts, the large computational complexity and model size\nheavily limit the deployment on latency-sensitive and low-resource edge\ndevices. In this work, we propose a lightweight SE network (LiSenNet) for\nreal-time applications. We design sub-band downsampling and upsampling blocks\nand a dual-path recurrent module to capture band-aware features and\ntime-frequency patterns, respectively. A noise detector is developed to detect\nnoisy regions in order to perform SE adaptively and save computational costs.\nCompared to recent higher-resource-dependent baseline models, the proposed\nLiSenNet can achieve a competitive performance with only 37k parameters (half\nof the state-of-the-art model) and 56M multiply-accumulate (MAC) operations per\nsecond.", "published": "2024-09-20 07:29:44", "link": "http://arxiv.org/abs/2409.13285v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Audio Codec Augmentation for Robust Collaborative Watermarking of Speech\n  Synthesis", "abstract": "Automatic detection of synthetic speech is becoming increasingly important as\ncurrent synthesis methods are both near indistinguishable from human speech and\nwidely accessible to the public. Audio watermarking and other active disclosure\nmethods of are attracting research activity, as they can complement traditional\ndeepfake defenses based on passive detection. In both active and passive\ndetection, robustness is of major interest. Traditional audio watermarks are\nparticularly susceptible to removal attacks by audio codec application. Most\ngenerated speech and audio content released into the wild passes through an\naudio codec purely as a distribution method. We recently proposed collaborative\nwatermarking as method for making generated speech more easily detectable over\na noisy but differentiable transmission channel. This paper extends the channel\naugmentation to work with non-differentiable traditional audio codecs and\nneural audio codecs and evaluates transferability and effect of codec bitrate\nover various configurations. The results show that collaborative watermarking\ncan be reliably augmented by black-box audio codecs using a waveform-domain\nstraight-through-estimator for gradient approximation. Furthermore, that\nresults show that channel augmentation with a neural audio codec transfers well\nto traditional codecs. Listening tests demonstrate collaborative watermarking\nincurs negligible perceptual degradation with high bitrate codecs or DAC at\n8kbps.", "published": "2024-09-20 10:33:17", "link": "http://arxiv.org/abs/2409.13382v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Time and Tokens: Benchmarking End-to-End Speech Dysfluency Detection", "abstract": "Speech dysfluency modeling is a task to detect dysfluencies in speech, such\nas repetition, block, insertion, replacement, and deletion. Most recent\nadvancements treat this problem as a time-based object detection problem. In\nthis work, we revisit this problem from a new perspective: tokenizing\ndysfluencies and modeling the detection problem as a token-based automatic\nspeech recognition (ASR) problem. We propose rule-based speech and text\ndysfluency simulators and develop VCTK-token, and then develop a Whisper-like\nseq2seq architecture to build a new benchmark with decent performance. We also\nsystematically compare our proposed token-based methods with time-based\nmethods, and propose a unified benchmark to facilitate future research\nendeavors. We open-source these resources for the broader scientific community.\nThe project page is available at https://rorizzz.github.io/", "published": "2024-09-20 15:35:32", "link": "http://arxiv.org/abs/2409.13582v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Temporally Aligned Audio for Video with Autoregression", "abstract": "We introduce V-AURA, the first autoregressive model to achieve high temporal\nalignment and relevance in video-to-audio generation. V-AURA uses a\nhigh-framerate visual feature extractor and a cross-modal audio-visual feature\nfusion strategy to capture fine-grained visual motion events and ensure precise\ntemporal alignment. Additionally, we propose VisualSound, a benchmark dataset\nwith high audio-visual relevance. VisualSound is based on VGGSound, a video\ndataset consisting of in-the-wild samples extracted from YouTube. During the\ncuration, we remove samples where auditory events are not aligned with the\nvisual ones. V-AURA outperforms current state-of-the-art models in temporal\nalignment and semantic relevance while maintaining comparable audio quality.\nCode, samples, VisualSound and models are available at\nhttps://v-aura.notion.site", "published": "2024-09-20 17:59:01", "link": "http://arxiv.org/abs/2409.13689v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "On the Feasibility of Fully AI-automated Vishing Attacks", "abstract": "A vishing attack is a form of social engineering where attackers use phone\ncalls to deceive individuals into disclosing sensitive information, such as\npersonal data, financial information, or security credentials. Attackers\nexploit the perceived urgency and authenticity of voice communication to\nmanipulate victims, often posing as legitimate entities like banks or tech\nsupport. Vishing is a particularly serious threat as it bypasses security\ncontrols designed to protect information. In this work, we study the potential\nfor vishing attacks to escalate with the advent of AI. In theory, AI-powered\nsoftware bots may have the ability to automate these attacks by initiating\nconversations with potential victims via phone calls and deceiving them into\ndisclosing sensitive information. To validate this thesis, we introduce ViKing,\nan AI-powered vishing system developed using publicly available AI technology.\nIt relies on a Large Language Model (LLM) as its core cognitive processor to\nsteer conversations with victims, complemented by a pipeline of speech-to-text\nand text-to-speech modules that facilitate audio-text conversion in phone\ncalls. Through a controlled social experiment involving 240 participants, we\ndiscovered that ViKing has successfully persuaded many participants to reveal\nsensitive information, even those who had been explicitly warned about the risk\nof vishing campaigns. Interactions with ViKing's bots were generally considered\nrealistic. From these findings, we conclude that tools like ViKing may already\nbe accessible to potential malicious actors, while also serving as an\ninvaluable resource for cyber awareness programs.", "published": "2024-09-20 10:47:09", "link": "http://arxiv.org/abs/2409.13793v1", "categories": ["cs.CR", "cs.AI", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Cross-Domain Knowledge Transfer for Underwater Acoustic Classification\n  Using Pre-trained Models", "abstract": "Transfer learning is commonly employed to leverage large, pre-trained models\nand perform fine-tuning for downstream tasks. The most prevalent pre-trained\nmodels are initially trained using ImageNet. However, their ability to\ngeneralize can vary across different data modalities. This study compares\npre-trained Audio Neural Networks (PANNs) and ImageNet pre-trained models\nwithin the context of underwater acoustic target recognition (UATR). It was\nobserved that the ImageNet pre-trained models slightly out-perform pre-trained\naudio models in passive sonar classification. We also analyzed the impact of\naudio sampling rates for model pre-training and fine-tuning. This study\ncontributes to transfer learning applications of UATR, illustrating the\npotential of pre-trained models to address limitations caused by scarce,\nlabeled data in the UATR domain.", "published": "2024-09-20 20:13:45", "link": "http://arxiv.org/abs/2409.13878v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigation of Time-Frequency Feature Combinations with Histogram\n  Layer Time Delay Neural Networks", "abstract": "While deep learning has reduced the prevalence of manual feature extraction,\ntransformation of data via feature engineering remains essential for improving\nmodel performance, particularly for underwater acoustic signals. The methods by\nwhich audio signals are converted into time-frequency representations and the\nsubsequent handling of these spectrograms can significantly impact performance.\nThis work demonstrates the performance impact of using different combinations\nof time-frequency features in a histogram layer time delay neural network. An\noptimal set of features is identified with results indicating that specific\nfeature combinations outperform single data features.", "published": "2024-09-20 20:22:24", "link": "http://arxiv.org/abs/2409.13881v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PTQ4ADM: Post-Training Quantization for Efficient Text Conditional Audio\n  Diffusion Models", "abstract": "Denoising diffusion models have emerged as state-of-the-art in generative\ntasks across image, audio, and video domains, producing high-quality, diverse,\nand contextually relevant data. However, their broader adoption is limited by\nhigh computational costs and large memory footprints. Post-training\nquantization (PTQ) offers a promising approach to mitigate these challenges by\nreducing model complexity through low-bandwidth parameters. Yet, direct\napplication of PTQ to diffusion models can degrade synthesis quality due to\naccumulated quantization noise across multiple denoising steps, particularly in\nconditional tasks like text-to-audio synthesis. This work introduces PTQ4ADM, a\nnovel framework for quantizing audio diffusion models(ADMs). Our key\ncontributions include (1) a coverage-driven prompt augmentation method and (2)\nan activation-aware calibration set generation algorithm for text-conditional\nADMs. These techniques ensure comprehensive coverage of audio aspects and\nmodalities while preserving synthesis fidelity. We validate our approach on\nTANGO, Make-An-Audio, and AudioLDM models for text-conditional audio\ngeneration. Extensive experiments demonstrate PTQ4ADM's capability to reduce\nthe model size by up to 70\\% while achieving synthesis quality metrics\ncomparable to full-precision models($<$5\\% increase in FD scores). We show that\nspecific layers in the backbone network can be quantized to 4-bit weights and\n8-bit activations without significant quality loss. This work paves the way for\nmore efficient deployment of ADMs in resource-constrained environments.", "published": "2024-09-20 20:52:56", "link": "http://arxiv.org/abs/2409.13894v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
