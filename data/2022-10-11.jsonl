{"title": "Improving Robustness of Retrieval Augmented Translation via Shuffling of\n  Suggestions", "abstract": "Several recent studies have reported dramatic performance improvements in\nneural machine translation (NMT) by augmenting translation at inference time\nwith fuzzy-matches retrieved from a translation memory (TM). However, these\nstudies all operate under the assumption that the TMs available at test time\nare highly relevant to the testset. We demonstrate that for existing retrieval\naugmented translation methods, using a TM with a domain mismatch to the test\nset can result in substantially worse performance compared to not using a TM at\nall. We propose a simple method to expose fuzzy-match NMT systems during\ntraining and show that it results in a system that is much more tolerant\n(regaining up to 5.8 BLEU) to inference with TMs with domain mismatch. Also,\nthe model is still competitive to the baseline when fed with suggestions from\nrelevant TMs.", "published": "2022-10-11 00:09:51", "link": "http://arxiv.org/abs/2210.05059v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset", "abstract": "In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.", "published": "2022-10-11 02:52:31", "link": "http://arxiv.org/abs/2210.05109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HUE: Pretrained Model and Dataset for Understanding Hanja Documents of\n  Ancient Korea", "abstract": "Historical records in Korea before the 20th century were primarily written in\nHanja, an extinct language based on Chinese characters and not understood by\nmodern Korean or Chinese speakers. Historians with expertise in this time\nperiod have been analyzing the documents, but that process is very difficult\nand time-consuming, and language models would significantly speed up the\nprocess. Toward building and evaluating language models for Hanja, we release\nthe Hanja Understanding Evaluation dataset consisting of chronological\nattribution, topic classification, named entity recognition, and summary\nretrieval tasks. We also present BERT-based models continued training on the\ntwo major corpora from the 14th to the 19th centuries: the Annals of the Joseon\nDynasty and Diaries of the Royal Secretariats. We compare the models with\nseveral baselines on all tasks and show there are significant improvements\ngained by training on the two corpora. Additionally, we run zero-shot\nexperiments on the Daily Records of the Royal Court and Important Officials\n(DRRI). The DRRI dataset has not been studied much by the historians, and not\nat all by the NLP community.", "published": "2022-10-11 03:04:28", "link": "http://arxiv.org/abs/2210.05112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture of Attention Heads: Selecting Attention Heads Per Token", "abstract": "Mixture-of-Experts (MoE) networks have been proposed as an efficient way to\nscale up model capacity and implement conditional computing. However, the study\nof MoE components mostly focused on the feedforward layer in Transformer\narchitecture. This paper proposes the Mixture of Attention Heads (MoA), a new\narchitecture that combines multi-head attention with the MoE mechanism. MoA\nincludes a set of attention heads that each has its own set of parameters.\nGiven an input, a router dynamically selects a subset of $k$ attention heads\nper token. This conditional computation schema allows MoA to achieve stronger\nperformance than the standard multi-head attention layer. Furthermore, the\nsparsely gated MoA can easily scale up the number of attention heads and the\nnumber of parameters while preserving computational efficiency. In addition to\nthe performance improvements, MoA also automatically differentiates heads'\nutilities, providing a new perspective to discuss the model's interpretability.\nWe conducted experiments on several important tasks, including Machine\nTranslation and Masked Language Modeling. Experiments have shown promising\nresults on several tasks against strong baselines that involve large and very\ndeep models.", "published": "2022-10-11 04:54:05", "link": "http://arxiv.org/abs/2210.05144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CSS: Combining Self-training and Self-supervised Learning for Few-shot\n  Dialogue State Tracking", "abstract": "Few-shot dialogue state tracking (DST) is a realistic problem that trains the\nDST model with limited labeled data. Existing few-shot methods mainly transfer\nknowledge learned from external labeled dialogue data (e.g., from question\nanswering, dialogue summarization, machine reading comprehension tasks, etc.)\ninto DST, whereas collecting a large amount of external labeled data is\nlaborious, and the external data may not effectively contribute to the\nDST-specific task. In this paper, we propose a few-shot DST framework called\nCSS, which Combines Self-training and Self-supervised learning methods. The\nunlabeled data of the DST task is incorporated into the self-training\niterations, where the pseudo labels are predicted by a DST model trained on\nlimited labeled data in advance. Besides, a contrastive self-supervised method\nis used to learn better representations, where the data is augmented by the\ndropout operation to train the model. Experimental results on the MultiWOZ\ndataset show that our proposed CSS achieves competitive performance in several\nfew-shot scenarios.", "published": "2022-10-11 04:55:16", "link": "http://arxiv.org/abs/2210.05146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for\n  Open-Domain Question Answering", "abstract": "Given its effectiveness on knowledge-intensive natural language processing\ntasks, dense retrieval models have become increasingly popular. Specifically,\nthe de-facto architecture for open-domain question answering uses two\nisomorphic encoders that are initialized from the same pretrained model but\nseparately parameterized for questions and passages. This bi-encoder\narchitecture is parameter-inefficient in that there is no parameter sharing\nbetween encoders. Further, recent studies show that such dense retrievers\nunderperform BM25 in various settings. We thus propose a new architecture,\nTask-aware Specialization for dense Retrieval (TASER), which enables parameter\nsharing by interleaving shared and specialized blocks in a single encoder. Our\nexperiments on five question answering datasets show that TASER can achieve\nsuperior accuracy, surpassing BM25, while using about 60% of the parameters as\nbi-encoder dense retrievers. In out-of-domain evaluations, TASER is also\nempirically more robust than bi-encoder dense retrievers. Our code is available\nat https://github.com/microsoft/taser.", "published": "2022-10-11 05:33:25", "link": "http://arxiv.org/abs/2210.05156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive\n  Machine Translation", "abstract": "Non-autoregressive models achieve significant decoding speedup in neural\nmachine translation but lack the ability to capture sequential dependency.\nDirected Acyclic Transformer (DA-Transformer) was recently proposed to model\nsequential dependency with a directed acyclic graph. Consequently, it has to\napply a sequential decision process at inference time, which harms the global\ntranslation accuracy. In this paper, we present a Viterbi decoding framework\nfor DA-Transformer, which guarantees to find the joint optimal solution for the\ntranslation and decoding path under any length constraint. Experimental results\ndemonstrate that our approach consistently improves the performance of\nDA-Transformer while maintaining a similar decoding speedup.", "published": "2022-10-11 06:53:34", "link": "http://arxiv.org/abs/2210.05193v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIGAT: Modeling News Recommendation with Dual-Graph Interaction", "abstract": "News recommendation (NR) is essential for online news services. Existing NR\nmethods typically adopt a news-user representation learning framework, facing\ntwo potential limitations. First, in news encoder, single candidate news\nencoding suffers from an insufficient semantic information problem. Second,\nexisting graph-based NR methods are promising but lack effective news-user\nfeature interaction, rendering the graph-based recommendation suboptimal. To\novercome these limitations, we propose dual-interactive graph attention\nnetworks (DIGAT) consisting of news- and user-graph channels. In the news-graph\nchannel, we enrich the semantics of single candidate news by incorporating the\nsemantically relevant news information with a semantic-augmented graph (SAG).\nIn the user-graph channel, multi-level user interests are represented with a\nnews-topic graph. Most notably, we design a dual-graph interaction process to\nperform effective feature interaction between the news and user graphs, which\nfacilitates accurate news-user representation matching. Experiment results on\nthe benchmark dataset MIND show that DIGAT outperforms existing news\nrecommendation methods. Further ablation studies and analyses validate the\neffectiveness of (1) semantic-augmented news graph modeling and (2) dual-graph\ninteraction.", "published": "2022-10-11 07:01:40", "link": "http://arxiv.org/abs/2210.05196v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Do Multi-hop Reading Comprehension Models Understand Date\n  Information?", "abstract": "Several multi-hop reading comprehension datasets have been proposed to\nresolve the issue of reasoning shortcuts by which questions can be answered\nwithout performing multi-hop reasoning. However, the ability of multi-hop\nmodels to perform step-by-step reasoning when finding an answer to a comparison\nquestion remains unclear. It is also unclear how questions about the internal\nreasoning process are useful for training and evaluating question-answering\n(QA) systems. To evaluate the model precisely in a hierarchical manner, we\nfirst propose a dataset, \\textit{HieraDate}, with three probing tasks in\naddition to the main question: extraction, reasoning, and robustness. Our\ndataset is created by enhancing two previous multi-hop datasets, HotpotQA and\n2WikiMultiHopQA, focusing on multi-hop questions on date information that\ninvolve both comparison and numerical reasoning. We then evaluate the ability\nof existing models to understand date information. Our experimental results\nreveal that the multi-hop models do not have the ability to subtract two dates\neven when they perform well in date comparison and number subtraction tasks.\nOther results reveal that our probing questions can help to improve the\nperformance of the models (e.g., by +10.3 F1) on the main QA task and our\ndataset can be used for data augmentation to improve the robustness of the\nmodels.", "published": "2022-10-11 07:24:07", "link": "http://arxiv.org/abs/2210.05208v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models", "abstract": "Despite the remarkable success of pre-trained language models (PLMs), they\nstill face two challenges: First, large-scale PLMs are inefficient in terms of\nmemory footprint and computation. Second, on the downstream tasks, PLMs tend to\nrely on the dataset bias and struggle to generalize to out-of-distribution\n(OOD) data. In response to the efficiency problem, recent studies show that\ndense PLMs can be replaced with sparse subnetworks without hurting the\nperformance. Such subnetworks can be found in three scenarios: 1) the\nfine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even\ninside 3) PLMs without any parameter fine-tuning. However, these results are\nonly obtained in the in-distribution (ID) setting. In this paper, we extend the\nstudy on PLMs subnetworks to the OOD setting, investigating whether sparsity\nand robustness to dataset bias can be achieved simultaneously. To this end, we\nconduct extensive experiments with the pre-trained BERT model on three natural\nlanguage understanding (NLU) tasks. Our results demonstrate that \\textbf{sparse\nand robust subnetworks (SRNets) can consistently be found in BERT}, across the\naforementioned three scenarios, using different training and compression\nmethods. Furthermore, we explore the upper bound of SRNets using the OOD\ninformation and show that \\textbf{there exist sparse and almost unbiased BERT\nsubnetworks}. Finally, we present 1) an analytical study that provides insights\non how to promote the efficiency of SRNets searching process and 2) a solution\nto improve subnetworks' performance at high sparsity. The code is available at\nhttps://github.com/llyx97/sparse-and-robust-PLM.", "published": "2022-10-11 07:26:34", "link": "http://arxiv.org/abs/2210.05211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Neural Network Policies and Imitation Learning for Multi-Domain\n  Task-Oriented Dialogues", "abstract": "Task-oriented dialogue systems are designed to achieve specific goals while\nconversing with humans. In practice, they may have to handle simultaneously\nseveral domains and tasks. The dialogue manager must therefore be able to take\ninto account domain changes and plan over different domains/tasks in order to\ndeal with multidomain dialogues. However, learning with reinforcement in such\ncontext becomes difficult because the state-action dimension is larger while\nthe reward signal remains scarce. Our experimental results suggest that\nstructured policies based on graph neural networks combined with different\ndegrees of imitation learning can effectively handle multi-domain dialogues.\nThe reported experiments underline the benefit of structured policies over\nstandard policies.", "published": "2022-10-11 08:29:10", "link": "http://arxiv.org/abs/2210.05252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting and Advancing Chinese Natural Language Understanding with\n  Accelerated Heterogeneous Knowledge Pre-training", "abstract": "Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve\ncontext-aware representations via learning from structured relations in\nknowledge graphs, and/or linguistic knowledge from syntactic or dependency\nanalysis. Unlike English, there is a lack of high-performing open-source\nChinese KEPLMs in the natural language processing (NLP) community to support\nvarious language understanding applications. In this paper, we revisit and\nadvance the development of Chinese natural language understanding with a series\nof novel Chinese KEPLMs released in various parameter sizes, namely CKBERT\n(Chinese knowledge-enhanced BERT).Specifically, both relational and linguistic\nknowledge is effectively injected into CKBERT based on two novel pre-training\ntasks, i.e., linguistic-aware masked language modeling and contrastive\nmulti-hop relation modeling. Based on the above two pre-training paradigms and\nour in-house implemented TorchAccelerator, we have pre-trained base (110M),\nlarge (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters.\nExperiments demonstrate that CKBERT outperforms strong baselines for Chinese\nover various benchmark NLP tasks and in terms of different model sizes.", "published": "2022-10-11 09:34:21", "link": "http://arxiv.org/abs/2210.05287v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for Cognitive Analysis of Emotions", "abstract": "Emotion analysis in texts suffers from two major limitations: annotated\ngold-standard corpora are mostly small and homogeneous, and emotion\nidentification is often simplified as a sentence-level classification problem.\nTo address these issues, we introduce a new annotation scheme for exploring\nemotions and their causes, along with a new French dataset composed of\nautobiographical accounts of an emotional scene. The texts were collected by\napplying the Cognitive Analysis of Emotions developed by A. Finkel to help\npeople improve on their emotion management. The method requires the manual\nanalysis of an emotional event by a coach trained in Cognitive Analysis. We\npresent a rule-based approach to automatically annotate emotions and their\nsemantic roles (e.g. emotion causes) to facilitate the identification of\nrelevant aspects by the coach. We investigate future directions for emotion\nanalysis using graph structures.", "published": "2022-10-11 09:47:00", "link": "http://arxiv.org/abs/2210.05296v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Structure-aware Paraphrase Identification with Phrase Alignment\n  Using Sentence Encoders", "abstract": "Previous works have demonstrated the effectiveness of utilising pre-trained\nsentence encoders based on their sentence representations for meaning\ncomparison tasks. Though such representations are shown to capture hidden\nsyntax structures, the direct similarity comparison between them exhibits weak\nsensitivity to word order and structural differences in given sentences. A\nsingle similarity score further makes the comparison process hard to interpret.\nTherefore, we here propose to combine sentence encoders with an alignment\ncomponent by representing each sentence as a list of predicate-argument spans\n(where their span representations are derived from sentence encoders), and\ndecomposing the sentence-level meaning comparison into the alignment between\ntheir spans for paraphrase identification tasks. Empirical results show that\nthe alignment component brings in both improved performance and\ninterpretability for various sentence encoders. After closer investigation, the\nproposed approach indicates increased sensitivity to structural difference and\nenhanced ability to distinguish non-paraphrases with high lexical overlap.", "published": "2022-10-11 09:52:52", "link": "http://arxiv.org/abs/2210.05302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation between Spoken Languages and Signed Languages\n  Represented in SignWriting", "abstract": "This paper presents work on novel machine translation (MT) systems between\nspoken and signed languages, where signed languages are represented in\nSignWriting, a sign language writing system. Our work seeks to address the lack\nof out-of-the-box support for signed languages in current MT systems and is\nbased on the SignBank dataset, which contains pairs of spoken language text and\nSignWriting content. We introduce novel methods to parse, factorize, decode,\nand evaluate SignWriting, leveraging ideas from neural factored MT. In a\nbilingual setup--translating from American Sign Language to (American)\nEnglish--our method achieves over 30 BLEU, while in two multilingual\nsetups--translating in both directions between spoken languages and signed\nlanguages--we achieve over 20 BLEU. We find that common MT techniques used to\nimprove spoken language translation similarly affect the performance of sign\nlanguage translation. These findings validate our use of an intermediate text\nrepresentation for signed languages to include them in natural language\nprocessing research.", "published": "2022-10-11 12:28:06", "link": "http://arxiv.org/abs/2210.05404v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Sense Induction with Hierarchical Clustering and Mutual Information\n  Maximization", "abstract": "Word sense induction (WSI) is a difficult problem in natural language\nprocessing that involves the unsupervised automatic detection of a word's\nsenses (i.e. meanings). Recent work achieves significant results on the WSI\ntask by pre-training a language model that can exclusively disambiguate word\nsenses, whereas others employ previously pre-trained language models in\nconjunction with additional strategies to induce senses. In this paper, we\npropose a novel unsupervised method based on hierarchical clustering and\ninvariant information clustering (IIC). The IIC is used to train a small model\nto optimize the mutual information between two vector representations of a\ntarget word occurring in a pair of synthetic paraphrases. This model is later\nused in inference mode to extract a higher quality vector representation to be\nused in the hierarchical clustering. We evaluate our method on two WSI tasks\nand in two distinct clustering configurations (fixed and dynamic number of\nclusters). We empirically demonstrate that, in certain cases, our approach\noutperforms prior WSI state-of-the-art methods, while in others, it achieves a\ncompetitive performance.", "published": "2022-10-11 13:04:06", "link": "http://arxiv.org/abs/2210.05422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Pretrained Multilingual Models Equally Fair Across Languages?", "abstract": "Pretrained multilingual language models can help bridge the digital language\ndivide, enabling high-quality NLP models for lower resourced languages. Studies\nof multilingual models have so far focused on performance, consistency, and\ncross-lingual generalisation. However, with their wide-spread application in\nthe wild and downstream societal impact, it is important to put multilingual\nmodels under the same scrutiny as monolingual models. This work investigates\nthe group fairness of multilingual models, asking whether these models are\nequally fair across languages. To this end, we create a new four-way\nmultilingual dataset of parallel cloze test examples (MozArt), equipped with\ndemographic information (balanced with regard to gender and native tongue)\nabout the test participants. We evaluate three multilingual models on MozArt --\nmBERT, XLM-R, and mT5 -- and show that across the four target languages, the\nthree models exhibit different levels of group disparity, e.g., exhibiting\nnear-equal risk for Spanish, but high levels of disparity for German.", "published": "2022-10-11 13:59:19", "link": "http://arxiv.org/abs/2210.05457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T5 for Hate Speech, Augmented Data and Ensemble", "abstract": "We conduct relatively extensive investigations of automatic hate speech (HS)\ndetection using different state-of-the-art (SoTA) baselines over 11 subtasks of\n6 different datasets. Our motivation is to determine which of the recent SoTA\nmodels is best for automatic hate speech detection and what advantage methods\nlike data augmentation and ensemble may have on the best model, if any. We\ncarry out 6 cross-task investigations. We achieve new SoTA on two subtasks -\nmacro F1 scores of 91.73% and 53.21% for subtasks A and B of the HASOC 2020\ndataset, where previous SoTA are 51.52% and 26.52%, respectively. We achieve\nnear-SoTA on two others - macro F1 scores of 81.66% for subtask A of the OLID\n2019 dataset and 82.54% for subtask A of the HASOC 2021 dataset, where SoTA are\n82.9% and 83.05%, respectively. We perform error analysis and use two\nexplainable artificial intelligence (XAI) algorithms (IG and SHAP) to reveal\nhow two of the models (Bi-LSTM and T5) make the predictions they do by using\nexamples. Other contributions of this work are 1) the introduction of a simple,\nnovel mechanism for correcting out-of-class (OOC) predictions in T5, 2) a\ndetailed description of the data augmentation methods, 3) the revelation of the\npoor data annotations in the HASOC 2021 dataset by using several examples and\nXAI (buttressing the need for better quality control), and 4) the public\nrelease of our model checkpoints and codes to foster transparency.", "published": "2022-10-11 14:32:39", "link": "http://arxiv.org/abs/2210.05480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Like a bilingual baby: The advantage of visually grounding a bilingual\n  language model", "abstract": "Unlike most neural language models, humans learn language in a rich,\nmulti-sensory and, often, multi-lingual environment. Current language models\ntypically fail to fully capture the complexities of multilingual language use.\nWe train an LSTM language model on images and captions in English and Spanish\nfrom MS-COCO-ES. We find that the visual grounding improves the model's\nunderstanding of semantic similarity both within and across languages and\nimproves perplexity. However, we find no significant advantage of visual\ngrounding for abstract words. Our results provide additional evidence of the\nadvantages of visually grounded language models and point to the need for more\nnaturalistic language data from multilingual speakers and multilingual datasets\nwith perceptual grounding.", "published": "2022-10-11 14:43:26", "link": "http://arxiv.org/abs/2210.05487v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Sharpness-Aware Minimization with Fisher Mask for Better\n  Generalization on Language Models", "abstract": "Fine-tuning large pretrained language models on a limited training corpus\nusually suffers from poor generalization. Prior works show that the\nrecently-proposed sharpness-aware minimization (SAM) optimization method can\nimprove the model generalization. However, SAM adds a perturbation to each\nmodel parameter equally (but not all parameters contribute equally to the\noptimization of training), which we argue is sub-optimal and will lead to\nexcessive computation. In this paper, we propose a novel optimization\nprocedure, namely FSAM, which introduces a Fisher mask to improve the\nefficiency and performance of SAM. In short, instead of adding perturbation to\nall parameters, FSAM uses the Fisher information to identity the important\nparameters and formulates a Fisher mask to obtain the sparse perturbation,\ni.e., making the optimizer focus on these important parameters. Experiments on\nvarious tasks in GLUE and SuperGLUE benchmarks show that FSAM consistently\noutperforms the vanilla SAM by 0.67~1.98 average score among four different\npretrained models. We also empirically show that FSAM works well in other\ncomplex scenarios, e.g., fine-tuning on generation tasks or limited training\ndata. Encouragingly, when training data is limited, FSAM improves the SAM by a\nlarge margin, i.e., up to 15.1.", "published": "2022-10-11 14:53:58", "link": "http://arxiv.org/abs/2210.05497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing Global Structural Information in Long Document Question\n  Answering with Compressive Graph Selector Network", "abstract": "Long document question answering is a challenging task due to its demands for\ncomplex reasoning over long text. Previous works usually take long documents as\nnon-structured flat texts or only consider the local structure in long\ndocuments. However, these methods usually ignore the global structure of the\nlong document, which is essential for long-range understanding. To tackle this\nproblem, we propose Compressive Graph Selector Network (CGSN) to capture the\nglobal structure in a compressive and iterative manner. The proposed model\nmainly focuses on the evidence selection phase of long document question\nanswering. Specifically, it consists of three modules: local graph network,\nglobal graph network and evidence memory network. Firstly, the local graph\nnetwork builds the graph structure of the chunked segment in token, sentence,\nparagraph and segment levels to capture the short-term dependency of the text.\nSecondly, the global graph network selectively receives the information of each\nlevel from the local graph, compresses them into the global graph nodes and\napplies graph attention to the global graph nodes to build the long-range\nreasoning over the entire text in an iterative way. Thirdly, the evidence\nmemory network is designed to alleviate the redundancy problem in the evidence\nselection by saving the selected result in the previous steps. Extensive\nexperiments show that the proposed model outperforms previous methods on two\ndatasets.", "published": "2022-10-11 14:55:12", "link": "http://arxiv.org/abs/2210.05499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Hierarchical Attention Transformers for Efficient Long\n  Document Classification", "abstract": "Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.", "published": "2022-10-11 15:17:56", "link": "http://arxiv.org/abs/2210.05529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aggregating Crowdsourced and Automatic Judgments to Scale Up a Corpus of\n  Anaphoric Reference for Fiction and Wikipedia Texts", "abstract": "Although several datasets annotated for anaphoric reference/coreference\nexist, even the largest such datasets have limitations in terms of size, range\nof domains, coverage of anaphoric phenomena, and size of documents included.\nYet, the approaches proposed to scale up anaphoric annotation haven't so far\nresulted in datasets overcoming these limitations. In this paper, we introduce\na new release of a corpus for anaphoric reference labelled via a\ngame-with-a-purpose. This new release is comparable in size to the largest\nexisting corpora for anaphoric reference due in part to substantial activity by\nthe players, in part thanks to the use of a new resolve-and-aggregate paradigm\nto 'complete' markable annotations through the combination of an anaphoric\nresolver and an aggregation method for anaphoric reference. The proposed method\ncould be adopted to greatly speed up annotation time in other projects\ninvolving games-with-a-purpose. In addition, the corpus covers genres for which\nno comparable size datasets exist (Fiction and Wikipedia); it covers singletons\nand non-referring expressions; and it includes a substantial number of long\ndocuments (> 2K in length).", "published": "2022-10-11 16:13:57", "link": "http://arxiv.org/abs/2210.05581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual BERT has an accent: Evaluating English influences on\n  fluency in multilingual models", "abstract": "While multilingual language models can improve NLP performance on\nlow-resource languages by leveraging higher-resource languages, they also\nreduce average performance on all languages (the 'curse of multilinguality').\nHere we show another problem with multilingual models: grammatical structures\nin higher-resource languages bleed into lower-resource languages, a phenomenon\nwe call grammatical structure bias. We show this bias via a novel method for\ncomparing the fluency of multilingual models to the fluency of monolingual\nSpanish and Greek models: testing their preference for two carefully-chosen\nvariable grammatical structures (optional pronoun-drop in Spanish and optional\nSubject-Verb ordering in Greek). We find that multilingual BERT is biased\ntoward the English-like setting (explicit pronouns and Subject-Verb-Object\nordering) as compared to our monolingual control language model. With our case\nstudies, we hope to bring to light the fine-grained ways in which multilingual\nmodels can be biased,and encourage more linguistically-aware fluency\nevaluation.", "published": "2022-10-11 17:06:38", "link": "http://arxiv.org/abs/2210.05619v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEE-Few: Seed, Expand and Entail for Few-shot Named Entity Recognition", "abstract": "Few-shot named entity recognition (NER) aims at identifying named entities\nbased on only few labeled instances. Current few-shot NER methods focus on\nleveraging existing datasets in the rich-resource domains which might fail in a\ntraining-from-scratch setting where no source-domain data is used. To tackle\ntraining-from-scratch setting, it is crucial to make full use of the annotation\ninformation (the boundaries and entity types). Therefore, in this paper, we\npropose a novel multi-task (Seed, Expand and Entail) learning framework,\nSEE-Few, for Few-shot NER without using source domain data. The seeding and\nexpanding modules are responsible for providing as accurate candidate spans as\npossible for the entailing module. The entailing module reformulates span\nclassification as a textual entailment task, leveraging both the contextual\nclues and entity type information. All the three modules share the same text\nencoder and are jointly learned. Experimental results on four benchmark\ndatasets under the training-from-scratch setting show that the proposed method\noutperformed state-of-the-art few-shot NER methods with a large margin. Our\ncode is available at https://github.com/unveiled-the-red-hat/SEE-Few.", "published": "2022-10-11 17:20:47", "link": "http://arxiv.org/abs/2210.05632v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For\n  Social Influence Tasks", "abstract": "Dialogue systems capable of social influence such as persuasion, negotiation,\nand therapy, are essential for extending the use of technology to numerous\nrealistic scenarios. However, existing research primarily focuses on either\ntask-oriented or open-domain scenarios, a categorization that has been\ninadequate for capturing influence skills systematically. There exists no\nformal definition or category for dialogue systems with these skills and\ndata-driven efforts in this direction are highly limited. In this work, we\nformally define and introduce the category of social influence dialogue systems\nthat influence users' cognitive and emotional responses, leading to changes in\nthoughts, opinions, and behaviors through natural conversations. We present a\nsurvey of various tasks, datasets, and methods, compiling the progress across\nseven diverse domains. We discuss the commonalities and differences between the\nexamined systems, identify limitations, and recommend future directions. This\nstudy serves as a comprehensive reference for social influence dialogue systems\nto inspire more dedicated research and discussion in this emerging area.", "published": "2022-10-11 17:57:23", "link": "http://arxiv.org/abs/2210.05664v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Text Representations under Tight Annotation Budgets: Measuring\n  Structural Alignment", "abstract": "Annotating large collections of textual data can be time consuming and\nexpensive. That is why the ability to train models with limited annotation\nbudgets is of great importance. In this context, it has been shown that under\ntight annotation budgets the choice of data representation is key. The goal of\nthis paper is to better understand why this is so. With this goal in mind, we\npropose a metric that measures the extent to which a given representation is\nstructurally aligned with a task. We conduct experiments on several text\nclassification datasets testing a variety of models and representations. Using\nour proposed metric we show that an efficient representation for a task (i.e.\none that enables learning from few samples) is a representation that induces a\ngood alignment between latent input structure and class structure.", "published": "2022-10-11 18:28:19", "link": "http://arxiv.org/abs/2210.05721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring and Improving Semantic Diversity of Dialogue Generation", "abstract": "Response diversity has become an important criterion for evaluating the\nquality of open-domain dialogue generation models. However, current evaluation\nmetrics for response diversity often fail to capture the semantic diversity of\ngenerated responses, as they mainly consider lexical aspects of the generated\nresponses. In this paper, we introduce a new automatic evaluation metric to\nmeasure the semantic diversity of generated responses. Through human\nevaluation, we demonstrate that our proposed metric captures human judgments on\nresponse diversity better than existing lexical-level diversity metrics.\nFurthermore, motivated by analyzing an existing dialogue dataset, we propose a\nsimple yet effective learning method that improves the semantic diversity of\ngenerated responses. Our learning method weights training samples based on the\nsemantic distribution of the training set. We show that our learning method\nimproves response diversity and coherency better than other baseline methods\nthrough automatic and human evaluation.", "published": "2022-10-11 18:36:54", "link": "http://arxiv.org/abs/2210.05725v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Speech Recognition of Low-Resource Languages Based on Chukchi", "abstract": "The following paper presents a project focused on the research and creation\nof a new Automatic Speech Recognition (ASR) based in the Chukchi language.\nThere is no one complete corpus of the Chukchi language, so most of the work\nconsisted in collecting audio and texts in the Chukchi language from open\nsources and processing them. We managed to collect 21:34:23 hours of audio\nrecordings and 112,719 sentences (or 2,068,273 words) of text in the Chukchi\nlanguage. The XLSR model was trained on the obtained data, which showed good\nresults even with a small amount of data. Besides the fact that the Chukchi\nlanguage is a low-resource language, it is also polysynthetic, which\nsignificantly complicates any automatic processing. Thus, the usual WER metric\nfor evaluating ASR becomes less indicative for a polysynthetic language.\nHowever, the CER metric showed good results. The question of metrics for\npolysynthetic languages remains open.", "published": "2022-10-11 18:37:15", "link": "http://arxiv.org/abs/2210.05726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Streaming Punctuation for Long-form Dictation with Transformers", "abstract": "While speech recognition Word Error Rate (WER) has reached human parity for\nEnglish, long-form dictation scenarios still suffer from segmentation and\npunctuation problems resulting from irregular pausing patterns or slow\nspeakers. Transformer sequence tagging models are effective at capturing long\nbi-directional context, which is crucial for automatic punctuation. Automatic\nSpeech Recognition (ASR) production systems, however, are constrained by\nreal-time requirements, making it hard to incorporate the right context when\nmaking punctuation decisions. In this paper, we propose a streaming approach\nfor punctuation or re-punctuation of ASR output using dynamic decoding windows\nand measure its impact on punctuation and segmentation accuracy across\nscenarios. The new system tackles over-segmentation issues, improving\nsegmentation F0.5-score by 13.9%. Streaming punctuation achieves an average\nBLEU-score improvement of 0.66 for the downstream task of Machine Translation\n(MT).", "published": "2022-10-11 20:03:03", "link": "http://arxiv.org/abs/2210.05756v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Speaker Identification Using Distant Supervision", "abstract": "Speaker identification, determining which character said each utterance in\nliterary text, benefits many downstream tasks. Most existing approaches use\nexpert-defined rules or rule-based features to directly approach this task, but\nthese approaches come with significant drawbacks, such as lack of contextual\nreasoning and poor cross-lingual generalization. In this work, we propose a\nspeaker identification framework that addresses these issues. We first extract\nlarge-scale distant supervision signals in English via general-purpose tools\nand heuristics, and then apply these weakly-labeled instances with a focus on\nencouraging contextual reasoning to train a cross-lingual language model. We\nshow that the resulting model outperforms previous state-of-the-art methods on\ntwo English speaker identification benchmarks by up to 9% in accuracy and 5%\nwith only distant supervision, as well as two Chinese speaker identification\ndatasets by up to 4.7%.", "published": "2022-10-11 20:49:44", "link": "http://arxiv.org/abs/2210.05780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLIP also Understands Text: Prompting CLIP for Phrase Understanding", "abstract": "Contrastive Language-Image Pretraining (CLIP) efficiently learns visual\nconcepts by pre-training with natural language supervision. CLIP and its visual\nencoder have been explored on various vision and language tasks and achieve\nstrong zero-shot or transfer learning performance. However, the application of\nits text encoder solely for text understanding has been less explored. In this\npaper, we find that the text encoder of CLIP actually demonstrates strong\nability for phrase understanding, and can even significantly outperform popular\nlanguage models such as BERT with a properly designed prompt. Extensive\nexperiments validate the effectiveness of our method across different datasets\nand domains on entity clustering and entity set expansion tasks.", "published": "2022-10-11 23:35:18", "link": "http://arxiv.org/abs/2210.05836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Segmentation Approaches for Neural Machine Translation of\n  Code-Switched Egyptian Arabic-English Text", "abstract": "Data sparsity is one of the main challenges posed by code-switching (CS),\nwhich is further exacerbated in the case of morphologically rich languages. For\nthe task of machine translation (MT), morphological segmentation has proven\nsuccessful in alleviating data sparsity in monolingual contexts; however, it\nhas not been investigated for CS settings. In this paper, we study the\neffectiveness of different segmentation approaches on MT performance, covering\nmorphology-based and frequency-based segmentation techniques. We experiment on\nMT from code-switched Arabic-English to English. We provide detailed analysis,\nexamining a variety of conditions, such as data size and sentences with\ndifferent degrees of CS. Empirical results show that morphology-aware\nsegmenters perform the best in segmentation tasks but under-perform in MT.\nNevertheless, we find that the choice of the segmentation setup to use for MT\nis highly dependent on the data size. For extreme low-resource scenarios, a\ncombination of frequency and morphology-based segmentations is shown to perform\nthe best. For more resourced settings, such a combination does not bring\nsignificant improvements over the use of frequency-based segmentation.", "published": "2022-10-11 23:20:12", "link": "http://arxiv.org/abs/2210.06990v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alzheimer's Diagnosis and Generation-Based Chatbot Using Hierarchical\n  Attention and Transformer", "abstract": "In this paper, we propose a natural language processing architecture that can\nhandle tasks that previously required two models as one model. With a single\nmodel, we analyze the language patterns and conversational context of\nAlzheimer's patients and derive answers from two results: patient\nclassification and chatbot. If the patient's language characteristics are\nidentified by chatbots in daily life, doctors can plan more precise diagnosis\nand treatment for early diagnosis. The proposed model is used to develop\nchatbots that replace questionnaires that required experts. There are two\nnatural language processing tasks performed by the model. The first is a\n'natural language classification' that indicates with probability whether the\npatient has an illness, and the second is to generate the next 'answer' of the\nchatbot to the patient's answer. In the first half, a context vector, which is\na characteristic of patient utterance, is extracted through a self-attention\nneural network. This context vector and chatbot (expert, moderator) questions\nare entered together into the encoder to obtain a matrix containing the\ncharacteristics of the interaction between the questioner and the patient. The\nvectorized matrix becomes a probability value for classification of patients.\nEnter the matrix into the decoder with the next answer from the chatbot (the\nmoderator) to generate the next utterance. As a result of learning this\nstructure with DmentiaBank's cookie theft description corpus, it was confirmed\nthat the value of the loss function of the encoder and decoder was\nsignificantly reduced and converged. This shows that capturing the speech\nlanguage pattern of Alzheimer's disease patients can contribute to early\ndiagnosis and longitudinal studies of the disease in the future.", "published": "2022-10-11 07:00:49", "link": "http://arxiv.org/abs/2211.07703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Discourse Annotation Reference Manual", "abstract": "This document provides extensive guidelines and examples for Rhetorical\nStructure Theory (RST) annotation in Mandarin Chinese. The guideline is divided\ninto three sections. We first introduce preprocessing steps to prepare data for\nRST annotation. Secondly, we discuss syntactic criteria to segment texts into\nElementary Discourse Units (EDUs). Lastly, we provide examples to define and\ndistinguish discourse relations in different genres. We hope that this\nreference manual can facilitate RST annotations in Chinese and accelerate the\ndevelopment of the RST framework across languages.", "published": "2022-10-11 11:02:42", "link": "http://arxiv.org/abs/2212.06037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces", "abstract": "The ability to extract high-quality translation dictionaries from monolingual\nword embedding spaces depends critically on the geometric similarity of the\nspaces -- their degree of \"isomorphism.\" We address the root-cause of faulty\ncross-lingual mapping: that word embedding training resulted in the underlying\nspaces being non-isomorphic. We incorporate global measures of isomorphism\ndirectly into the Skip-gram loss function, successfully increasing the relative\nisomorphism of trained word embedding spaces and improving their ability to be\nmapped to a shared cross-lingual space. The result is improved bilingual\nlexicon induction in general data conditions, under domain mismatch, and with\ntraining algorithm dissimilarities. We release IsoVec at\nhttps://github.com/kellymarchisio/isovec.", "published": "2022-10-11 02:29:34", "link": "http://arxiv.org/abs/2210.05098v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Time-aware topic identification in social media with pre-trained\n  language models: A case study of electric vehicles", "abstract": "Recent extensively competitive business environment makes companies to keep\ntheir eyes on social media, as there is a growing recognition over customer\nlanguages (e.g., needs, interests, and complaints) as source of future\nopportunities. This research avenue analysing social media data has received\nmuch attention in academia, but their utilities are limited as most of methods\nprovide retrospective results. Moreover, the increasing number of\ncustomer-generated contents and rapidly varying topics have made the necessity\nof time-aware topic evolution analyses. Recently, several researchers have\nshowed the applicability of pre-trained semantic language models to social\nmedia as an input feature, but leaving limitations in understanding evolving\ntopics. In this study, we propose a time-aware topic identification approach\nwith pre-trained language models. The proposed approach consists of two stages:\nthe dynamics-focused function for tracking time-varying topics with language\nmodels and the emergence-scoring function to examine future promising topics.\nHere we apply the proposed approach to reddit data on electric vehicles, and\nour findings highlight the feasibility of capturing emerging customer topics\nfrom voluminous social media in a time-aware manner.", "published": "2022-10-11 04:50:10", "link": "http://arxiv.org/abs/2210.05143v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Retrieval Augmentation for T5 Re-ranker using External Sources", "abstract": "Retrieval augmentation has shown promising improvements in different tasks.\nHowever, whether such augmentation can assist a large language model based\nre-ranker remains unclear. We investigate how to augment T5-based re-rankers\nusing high-quality information retrieved from two external corpora -- a\ncommercial web search engine and Wikipedia. We empirically demonstrate how\nretrieval augmentation can substantially improve the effectiveness of T5-based\nre-rankers for both in-domain and zero-shot out-of-domain re-ranking tasks.", "published": "2022-10-11 04:54:19", "link": "http://arxiv.org/abs/2210.05145v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Understanding the Failure of Batch Normalization for Transformers in NLP", "abstract": "Batch Normalization (BN) is a core and prevalent technique in accelerating\nthe training of deep neural networks and improving the generalization on\nComputer Vision (CV) tasks. However, it fails to defend its position in Natural\nLanguage Processing (NLP), which is dominated by Layer Normalization (LN). In\nthis paper, we are trying to answer why BN usually performs worse than LN in\nNLP tasks with Transformer models. We find that the inconsistency between\ntraining and inference of BN is the leading cause that results in the failure\nof BN in NLP. We define Training Inference Discrepancy (TID) to quantitatively\nmeasure this inconsistency and reveal that TID can indicate BN's performance,\nsupported by extensive experiments, including image classification, neural\nmachine translation, language modeling, sequence labeling, and text\nclassification tasks. We find that BN can obtain much better test performance\nthan LN when TID keeps small through training. To suppress the explosion of\nTID, we propose Regularized BN (RBN) that adds a simple regularization term to\nnarrow the gap between batch statistics and population statistics of BN. RBN\nimproves the performance of BN consistently and outperforms or is on par with\nLN on 17 out of 20 settings, involving ten datasets and two common variants of\nTransformer\n  Our code is available at https://github.com/wjxts/RegularizedBN.", "published": "2022-10-11 05:18:47", "link": "http://arxiv.org/abs/2210.05153v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Language Models Be Specific? How?", "abstract": "\"He is a person\", \"Paris is located on the earth\". Both statements are\ncorrect but meaningless - due to lack of specificity. In this paper, we propose\nto measure how specific the language of pre-trained language models (PLMs) is.\nTo achieve this, we introduce a novel approach to build a benchmark for\nspecificity testing by forming masked token prediction tasks with prompts. For\ninstance, given \"Toronto is located in [MASK].\", we want to test whether a more\nspecific answer will be better filled in by PLMs, e.g., Ontario instead of\nCanada. From our evaluations, we show that existing PLMs have only a slight\npreference for more specific answers. We identify underlying factors affecting\nthe specificity and design two prompt-based methods to improve the specificity.\nResults show that the specificity of the models can be improved by the proposed\nmethods without additional training. We hope this work can bring to awareness\nthe notion of specificity of language models and encourage the research\ncommunity to further explore this important but understudied problem.", "published": "2022-10-11 05:38:27", "link": "http://arxiv.org/abs/2210.05159v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for\n  Legal Case Retrieval", "abstract": "Legal case retrieval, which aims to retrieve relevant cases given a query\ncase, plays an essential role in the legal system. While recent research\nefforts improve the performance of traditional ad-hoc retrieval models, legal\ncase retrieval is still challenging since queries are legal cases, which\ncontain hundreds of tokens. Legal cases are much longer and more complicated\nthan keywords queries. Apart from that, the definition of legal relevance is\nbeyond the general definition. In addition to general topical relevance, the\nrelevant cases also involve similar situations and legal elements, which can\nsupport the judgment of the current case. In this paper, we propose an\ninteraction-focused network for legal case retrieval with a multi-view\ncontrastive learning objective. The contrastive learning views, including\ncase-view and element-view, aim to overcome the above challenges. The case-view\ncontrastive learning minimizes the hidden space distance between relevant legal\ncase representations produced by a pre-trained language model (PLM) encoder.\nThe element-view builds positive and negative instances by changing legal\nelements of cases to help the network better compute legal relevance. To\nachieve this, we employ a legal element knowledge-aware indicator to detect\nlegal elements of cases. We conduct extensive experiments on the benchmark of\nrelevant case retrieval. Evaluation results indicate our proposed method\nobtains significant improvement over the existing methods.", "published": "2022-10-11 06:47:23", "link": "http://arxiv.org/abs/2210.05188v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CHAE: Fine-Grained Controllable Story Generation with Characters,\n  Actions and Emotions", "abstract": "Story generation has emerged as an interesting yet challenging NLP task in\nrecent years. Some existing studies aim at generating fluent and coherent\nstories from keywords and outlines; while others attempt to control the global\nfeatures of the story, such as emotion, style and topic. However, these works\nfocus on coarse-grained control on the story, neglecting control on the details\nof the story, which is also crucial for the task. To fill the gap, this paper\nproposes a model for fine-grained control on the story, which allows the\ngeneration of customized stories with characters, corresponding actions and\nemotions arbitrarily assigned. Extensive experimental results on both automatic\nand human manual evaluations show the superiority of our method. It has strong\ncontrollability to generate stories according to the fine-grained personalized\nguidance, unveiling the effectiveness of our methodology. Our code is available\nat https://github.com/victorup/CHAE.", "published": "2022-10-11 07:37:50", "link": "http://arxiv.org/abs/2210.05221v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Mimicking to Integrating: Knowledge Integration for Pre-Trained\n  Language Models", "abstract": "Investigating better ways to reuse the released pre-trained language models\n(PLMs) can significantly reduce the computational cost and the potential\nenvironmental side-effects. This paper explores a novel PLM reuse paradigm,\nKnowledge Integration (KI). Without human annotations available, KI aims to\nmerge the knowledge from different teacher-PLMs, each of which specializes in a\ndifferent classification problem, into a versatile student model. To achieve\nthis, we first derive the correlation between virtual golden supervision and\nteacher predictions. We then design a Model Uncertainty--aware Knowledge\nIntegration (MUKI) framework to recover the golden supervision for the student.\nSpecifically, MUKI adopts Monte-Carlo Dropout to estimate model uncertainty for\nthe supervision integration. An instance-wise re-weighting mechanism based on\nthe margin of uncertainty scores is further incorporated, to deal with the\npotential conflicting supervision from teachers. Experimental results\ndemonstrate that MUKI achieves substantial improvements over baselines on\nbenchmark datasets. Further analysis shows that MUKI can generalize well for\nmerging teacher models with heterogeneous architectures, and even teachers\nmajor in cross-lingual datasets.", "published": "2022-10-11 07:59:08", "link": "http://arxiv.org/abs/2210.05230v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PatternRank: Leveraging Pretrained Language Models and Part of Speech\n  for Unsupervised Keyphrase Extraction", "abstract": "Keyphrase extraction is the process of automatically selecting a small set of\nmost relevant phrases from a given text. Supervised keyphrase extraction\napproaches need large amounts of labeled training data and perform poorly\noutside the domain of the training data. In this paper, we present PatternRank,\nwhich leverages pretrained language models and part-of-speech for unsupervised\nkeyphrase extraction from single documents. Our experiments show PatternRank\nachieves higher precision, recall and F1-scores than previous state-of-the-art\napproaches. In addition, we present the KeyphraseVectorizers package, which\nallows easy modification of part-of-speech patterns for candidate keyphrase\nselection, and hence adaptation of our approach to any domain.", "published": "2022-10-11 08:23:54", "link": "http://arxiv.org/abs/2210.05245v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair\n  Modeling", "abstract": "Transformer-based models have achieved great success on sentence pair\nmodeling tasks, such as answer selection and natural language inference (NLI).\nThese models generally perform cross-attention over input pairs, leading to\nprohibitive computational costs. Recent studies propose dual-encoder and late\ninteraction architectures for faster computation. However, the balance between\nthe expressive of cross-attention and computation speedup still needs better\ncoordinated. To this end, this paper introduces a novel paradigm MixEncoder for\nefficient sentence pair modeling. MixEncoder involves a light-weight\ncross-attention mechanism. It conducts query encoding only once while modeling\nthe query-candidate interaction in parallel. Extensive experiments conducted on\nfour tasks demonstrate that our MixEncoder can speed up sentence pairing by\nover 113x while achieving comparable performance as the more expensive\ncross-attention models.", "published": "2022-10-11 08:44:03", "link": "http://arxiv.org/abs/2210.05261v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mind's Eye: Grounded Language Model Reasoning through Simulation", "abstract": "Successful and effective communication between humans and AI relies on a\nshared experience of the world. By training solely on written text, current\nlanguage models (LMs) miss the grounded experience of humans in the real-world\n-- their failure to relate language to the physical world causes knowledge to\nbe misrepresented and obvious mistakes in their reasoning. We present Mind's\nEye, a paradigm to ground language model reasoning in the physical world. Given\na physical reasoning question, we use a computational physics engine\n(DeepMind's MuJoCo) to simulate the possible outcomes, and then use the\nsimulation results as part of the input, which enables language models to\nperform reasoning. Experiments on 39 tasks in a physics alignment benchmark\ndemonstrate that Mind's Eye can improve reasoning ability by a large margin\n(27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average).\nSmaller language models armed with Mind's Eye can obtain similar performance to\nmodels that are 100x larger. Finally, we confirm the robustness of Mind's Eye\nthrough ablation studies.", "published": "2022-10-11 11:39:23", "link": "http://arxiv.org/abs/2210.05359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Locate Visual Answer in Video Corpus Using Question", "abstract": "We introduce a new task, named video corpus visual answer localization\n(VCVAL), which aims to locate the visual answer in a large collection of\nuntrimmed instructional videos using a natural language question. This task\nrequires a range of skills - the interaction between vision and language, video\nretrieval, passage comprehension, and visual answer localization. In this\npaper, we propose a cross-modal contrastive global-span (CCGS) method for the\nVCVAL, jointly training the video corpus retrieval and visual answer\nlocalization subtasks with the global-span matrix. We have reconstructed a\ndataset named MedVidCQA, on which the VCVAL task is benchmarked. Experimental\nresults show that the proposed method outperforms other competitive methods\nboth in the video corpus retrieval and visual answer localization subtasks.\nMost importantly, we perform detailed analyses on extensive experiments, paving\na new path for understanding the instructional videos, which ushers in further\nresearch.", "published": "2022-10-11 13:04:59", "link": "http://arxiv.org/abs/2210.05423v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Instance Regularization for Discriminative Language Model Pre-training", "abstract": "Discriminative pre-trained language models (PrLMs) can be generalized as\ndenoising auto-encoders that work with two procedures, ennoising and denoising.\nFirst, an ennoising process corrupts texts with arbitrary noising functions to\nconstruct training instances. Then, a denoising language model is trained to\nrestore the corrupted tokens. Existing studies have made progress by optimizing\nindependent strategies of either ennoising or denosing. They treat training\ninstances equally throughout the training process, with little attention on the\nindividual contribution of those instances. To model explicit signals of\ninstance contribution, this work proposes to estimate the complexity of\nrestoring the original sentences from corrupted ones in language model\npre-training. The estimations involve the corruption degree in the ennoising\ndata construction process and the prediction confidence in the denoising\ncounterpart. Experimental results on natural language understanding and reading\ncomprehension benchmarks show that our approach improves pre-training\nefficiency, effectiveness, and robustness. Code is publicly available at\nhttps://github.com/cooelf/InstanceReg", "published": "2022-10-11 14:16:37", "link": "http://arxiv.org/abs/2210.05471v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Better Than Whitespace: Information Retrieval for Languages without\n  Custom Tokenizers", "abstract": "Tokenization is a crucial step in information retrieval, especially for\nlexical matching algorithms, where the quality of indexable tokens directly\nimpacts the effectiveness of a retrieval system. Since different languages have\nunique properties, the design of the tokenization algorithm is usually\nlanguage-specific and requires at least some lingustic knowledge. However, only\na handful of the 7000+ languages on the planet benefit from specialized,\ncustom-built tokenization algorithms, while the other languages are stuck with\na \"default\" whitespace tokenizer, which cannot capture the intricacies of\ndifferent languages. To address this challenge, we propose a different approach\nto tokenization for lexical matching retrieval algorithms (e.g., BM25): using\nthe WordPiece tokenizer, which can be built automatically from unsupervised\ndata. We test the approach on 11 typologically diverse languages in the MrTyDi\ncollection: results show that the mBERT tokenizer provides strong relevance\nsignals for retrieval \"out of the box\", outperforming whitespace tokenization\non most languages. In many cases, our approach also improves retrieval\neffectiveness when combined with existing custom-built tokenizers.", "published": "2022-10-11 14:32:46", "link": "http://arxiv.org/abs/2210.05481v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Adversarial Contrastive Learning for Evidence-aware Fake News Detection\n  with Graph Neural Networks", "abstract": "The prevalence and perniciousness of fake news have been a critical issue on\nthe Internet, which stimulates the development of automatic fake news detection\nin turn. In this paper, we focus on evidence-based fake news detection, where\nseveral evidences are utilized to probe the veracity of news (i.e., a claim).\nMost previous methods first employ sequential models to embed the semantic\ninformation and then capture the claim-evidence interaction based on attention\nmechanisms. Despite their effectiveness, they still suffer from three\nweaknesses. Firstly, sequential models fail to integrate the relevant\ninformation that is scattered far apart in evidences. Secondly, they\nunderestimate much redundant information in evidences may be useless or\nharmful. Thirdly, insufficient data utilization limits the separability and\nreliability of representations captured by the model. To solve these problems,\nwe propose a unified Graph-based sEmantic structure mining framework with\nConTRAstive Learning, namely GETRAL in short. Specifically, we first model\nclaims and evidences as graph-structured data to capture the long-distance\nsemantic dependency. Consequently, we reduce information redundancy by\nperforming graph structure learning. Then the fine-grained semantic\nrepresentations are fed into the claim-evidence interaction module for\npredictions. Finally, an adversarial contrastive learning module is applied to\nmake full use of data and strengthen representation learning. Comprehensive\nexperiments have demonstrated the superiority of GETRAL over the\nstate-of-the-arts and validated the efficacy of semantic mining with graph\nstructure and contrastive learning.", "published": "2022-10-11 14:54:37", "link": "http://arxiv.org/abs/2210.05498v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Model Cascading: Towards Jointly Improving Efficiency and Accuracy of\n  NLP Systems", "abstract": "Do all instances need inference through the big models for a correct\nprediction? Perhaps not; some instances are easy and can be answered correctly\nby even small capacity models. This provides opportunities for improving the\ncomputational efficiency of systems. In this work, we present an explorative\nstudy on 'model cascading', a simple technique that utilizes a collection of\nmodels of varying capacities to accurately yet efficiently output predictions.\nThrough comprehensive experiments in multiple task settings that differ in the\nnumber of models available for cascading (K value), we show that cascading\nimproves both the computational efficiency and the prediction accuracy. For\ninstance, in K=3 setting, cascading saves up to 88.93% computation cost and\nconsistently achieves superior prediction accuracy with an improvement of up to\n2.18%. We also study the impact of introducing additional models in the cascade\nand show that it further increases the efficiency improvements. Finally, we\nhope that our work will facilitate development of efficient NLP systems making\ntheir widespread adoption in real-world applications possible.", "published": "2022-10-11 15:17:52", "link": "http://arxiv.org/abs/2210.05528v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ViLPAct: A Benchmark for Compositional Generalization on Multimodal\n  Human Activities", "abstract": "We introduce ViLPAct, a novel vision-language benchmark for human activity\nplanning. It is designed for a task where embodied AI agents can reason and\nforecast future actions of humans based on video clips about their initial\nactivities and intents in text. The dataset consists of 2.9k videos from\n\\charades extended with intents via crowdsourcing, a multi-choice question test\nset, and four strong baselines. One of the baselines implements a neurosymbolic\napproach based on a multi-modal knowledge base (MKB), while the other ones are\ndeep generative models adapted from recent state-of-the-art (SOTA) methods.\nAccording to our extensive experiments, the key challenges are compositional\ngeneralization and effective use of information from both modalities.", "published": "2022-10-11 15:50:51", "link": "http://arxiv.org/abs/2210.05556v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enriching Biomedical Knowledge for Low-resource Language Through\n  Large-Scale Translation", "abstract": "Biomedical data and benchmarks are highly valuable yet very limited in\nlow-resource languages other than English such as Vietnamese. In this paper, we\nmake use of a state-of-the-art translation model in English-Vietnamese to\ntranslate and produce both pretrained as well as supervised data in the\nbiomedical domains. Thanks to such large-scale translation, we introduce\nViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20\nmillion translated abstracts from the high-quality public PubMed corpus.\nViPubMedT5 demonstrates state-of-the-art results on two different biomedical\nbenchmarks in summarization and acronym disambiguation. Further, we release\nViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the\nrecently public En-vi translation model and carefully refined by human experts,\nwith evaluations of existing methods against ViPubmedT5.", "published": "2022-10-11 16:35:10", "link": "http://arxiv.org/abs/2210.05598v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MTet: Multi-domain Translation for English and Vietnamese", "abstract": "We introduce MTet, the largest publicly available parallel corpus for\nEnglish-Vietnamese translation. MTet consists of 4.2M high-quality training\nsentence pairs and a multi-domain test set refined by the Vietnamese research\ncommunity. Combining with previous works on English-Vietnamese translation, we\ngrow the existing parallel dataset to 6.2M sentence pairs. We also release the\nfirst pretrained model EnViT5 for English and Vietnamese languages. Combining\nboth resources, our model significantly outperforms previous state-of-the-art\nresults by up to 2 points in translation BLEU score, while being 1.6 times\nsmaller.", "published": "2022-10-11 16:55:21", "link": "http://arxiv.org/abs/2210.05610v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Training Improves Zero-Shot Classification of\n  Semi-structured Documents", "abstract": "We investigate semi-structured document classification in a zero-shot\nsetting. Classification of semi-structured documents is more challenging than\nthat of standard unstructured documents, as positional, layout, and style\ninformation play a vital role in interpreting such documents. The standard\nclassification setting where categories are fixed during both training and\ntesting falls short in dynamic environments where new document categories could\npotentially emerge. We focus exclusively on the zero-shot setting where\ninference is done on new unseen classes. To address this task, we propose a\nmatching-based approach that relies on a pairwise contrastive objective for\nboth pretraining and fine-tuning. Our results show a significant boost in Macro\nF$_1$ from the proposed pretraining step in both supervised and unsupervised\nzero-shot settings.", "published": "2022-10-11 16:55:47", "link": "http://arxiv.org/abs/2210.05613v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Kernel-Based View of Language Model Fine-Tuning", "abstract": "It has become standard to solve NLP tasks by fine-tuning pre-trained language\nmodels (LMs), especially in low-data settings. There is minimal theoretical\nunderstanding of empirical success, e.g., why fine-tuning a model with $10^8$\nor more parameters on a couple dozen training points does not result in\noverfitting. We investigate whether the Neural Tangent Kernel (NTK) - which\noriginated as a model to study the gradient descent dynamics of infinitely wide\nnetworks with suitable random initialization - describes fine-tuning of\npre-trained LMs. This study was inspired by the decent performance of NTK for\ncomputer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam\nand use Tensor Programs (Yang, 2020) to characterize conditions under which the\nNTK lens may describe fine-tuning updates to pre-trained language models.\nExtensive experiments on 14 NLP tasks validate our theory and show that\nformulating the downstream task as a masked word prediction problem through\nprompting often induces kernel-based dynamics during fine-tuning. Finally, we\nuse this kernel view to propose an explanation for the success of\nparameter-efficient subspace-based fine-tuning methods.", "published": "2022-10-11 17:34:32", "link": "http://arxiv.org/abs/2210.05643v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Shapley Head Pruning: Identifying and Removing Interference in\n  Multilingual Transformers", "abstract": "Multilingual transformer-based models demonstrate remarkable zero and\nfew-shot transfer across languages by learning and reusing language-agnostic\nfeatures. However, as a fixed-size model acquires more languages, its\nperformance across all languages degrades, a phenomenon termed interference.\nOften attributed to limited model capacity, interference is commonly addressed\nby adding additional parameters despite evidence that transformer-based models\nare overparameterized. In this work, we show that it is possible to reduce\ninterference by instead identifying and pruning language-specific parameters.\nFirst, we use Shapley Values, a credit allocation metric from coalitional game\ntheory, to identify attention heads that introduce interference. Then, we show\nthat removing identified attention heads from a fixed model improves\nperformance for a target language on both sentence classification and\nstructural prediction, seeing gains as large as 24.7\\%. Finally, we provide\ninsights on language-agnostic and language-specific attention heads using\nattention visualization.", "published": "2022-10-11 18:11:37", "link": "http://arxiv.org/abs/2210.05709v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Independent Stance Detection: Social Interaction-based\n  Embeddings and Large Language Models", "abstract": "The large majority of the research performed on stance detection has been\nfocused on developing more or less sophisticated text classification systems,\neven when many benchmarks are based on social network data such as Twitter.\nThis paper aims to take on the stance detection task by placing the emphasis\nnot so much on the text itself but on the interaction data available on social\nnetworks. More specifically, we propose a new method to leverage social\ninformation such as friends and retweets by generating Relational Embeddings,\nnamely, dense vector representations of interaction pairs. Our experiments on\nseven publicly available datasets and four different languages (Basque,\nCatalan, Italian, and Spanish) show that combining our relational embeddings\nwith discriminative textual methods helps to substantially improve performance,\nobtaining state-of-the-art results for six out of seven evaluation settings,\noutperforming strong baselines based on Large Language Models, or other popular\ninteraction-based approaches such as DeepWalk or node2vec.", "published": "2022-10-11 18:13:43", "link": "http://arxiv.org/abs/2210.05715v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Propagators of Disinformation on Twitter Using Quantitative\n  Discursive Analysis", "abstract": "Efforts by foreign actors to influence public opinion have gained\nconsiderable attention because of their potential to impact democratic\nelections. Thus, the ability to identify and counter sources of disinformation\nis increasingly becoming a top priority for government entities in order to\nprotect the integrity of democratic processes. This study presents a method of\nidentifying Russian disinformation bots on Twitter using centering resonance\nanalysis and Clauset-Newman-Moore community detection. The data reflect a\nsignificant degree of discursive dissimilarity between known Russian\ndisinformation bots and a control set of Twitter users during the timeframe of\nthe 2016 U.S. Presidential Election. The data also demonstrate statistically\nsignificant classification capabilities (MCC = 0.9070) based on community\nclustering. The prediction algorithm is very effective at identifying true\npositives (bots), but is not able to resolve true negatives (non-bots) because\nof the lack of discursive similarity between control users. This leads to a\nhighly sensitive means of identifying propagators of disinformation with a high\ndegree of discursive similarity on Twitter, with implications for limiting the\nspread of disinformation that could impact democratic processes.", "published": "2022-10-11 20:11:50", "link": "http://arxiv.org/abs/2210.05760v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Vote'n'Rank: Revision of Benchmarking with Social Choice Theory", "abstract": "The development of state-of-the-art systems in different applied areas of\nmachine learning (ML) is driven by benchmarks, which have shaped the paradigm\nof evaluating generalisation capabilities from multiple perspectives. Although\nthe paradigm is shifting towards more fine-grained evaluation across diverse\ntasks, the delicate question of how to aggregate the performances has received\nparticular interest in the community. In general, benchmarks follow the\nunspoken utilitarian principles, where the systems are ranked based on their\nmean average score over task-specific metrics. Such aggregation procedure has\nbeen viewed as a sub-optimal evaluation protocol, which may have created the\nillusion of progress. This paper proposes Vote'n'Rank, a framework for ranking\nsystems in multi-task benchmarks under the principles of the social choice\ntheory. We demonstrate that our approach can be efficiently utilised to draw\nnew insights on benchmarking in several ML sub-fields and identify the\nbest-performing systems in research and development case studies. The\nVote'n'Rank's procedures are more robust than the mean average while being able\nto handle missing performance scores and determine conditions under which the\nsystem becomes the winner.", "published": "2022-10-11 20:19:11", "link": "http://arxiv.org/abs/2210.05769v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Bil-DOS: A Bi-lingual Dialogue Ordering System (for Subway)", "abstract": "Due to the unfamiliarity to particular words(or proper nouns) for\ningredients, non-native English speakers can be extremely confused about the\nordering process in restaurants like Subway. Thus, We developed a dialogue\nsystem, which supports Chinese(Mandarin)1 and English2 at the same time. In\nother words, users can switch arbitrarily between Chinese(Mandarin) and English\nas the conversation is being conducted. This system is specifically designed\nfor Subway ordering3. In BilDOS, we designed a Discriminator module to tell the\nlanguage is being used in inputted user utterance, a Translator module to\ntranslate used language into English if it is not English, and a Dialogue\nManager module to detect the intention within inputted user utterances, handle\noutlier inputs by throwing clarification requests, map detected Intention and\ndetailed Keyword4 into a particular intention class, locate the current\nordering process, continue to give queries to finish the order, conclude the\norder details once the order is completed, activate the evaluation process when\nthe conversation is done.", "published": "2022-10-11 20:32:02", "link": "http://arxiv.org/abs/2210.05773v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Underspecification in Scene Description-to-Depiction Tasks", "abstract": "Questions regarding implicitness, ambiguity and underspecification are\ncrucial for understanding the task validity and ethical concerns of multimodal\nimage+text systems, yet have received little attention to date. This position\npaper maps out a conceptual framework to address this gap, focusing on systems\nwhich generate images depicting scenes from scene descriptions. In doing so, we\naccount for how texts and images convey meaning differently. We outline a set\nof core challenges concerning textual and visual ambiguity, as well as risks\nthat may be amplified by ambiguous and underspecified elements. We propose and\ndiscuss strategies for addressing these challenges, including generating\nvisually ambiguous images, and generating a set of diverse images.", "published": "2022-10-11 22:51:24", "link": "http://arxiv.org/abs/2210.05815v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SEAL : Interactive Tool for Systematic Error Analysis and Labeling", "abstract": "With the advent of Transformers, large language models (LLMs) have saturated\nwell-known NLP benchmarks and leaderboards with high aggregate performance.\nHowever, many times these models systematically fail on tail data or rare\ngroups not obvious in aggregate evaluation. Identifying such problematic data\ngroups is even more challenging when there are no explicit labels (e.g.,\nethnicity, gender, etc.) and further compounded for NLP datasets due to the\nlack of visual features to characterize failure modes (e.g., Asian males,\nanimals indoors, waterbirds on land, etc.). This paper introduces an\ninteractive Systematic Error Analysis and Labeling (\\seal) tool that uses a\ntwo-step approach to first identify high error slices of data and then, in the\nsecond step, introduce methods to give human-understandable semantics to those\nunderperforming slices. We explore a variety of methods for coming up with\ncoherent semantics for the error groups using language models for semantic\nlabeling and a text-to-image model for generating visual features. SEAL toolkit\nand demo screencast is available at https://huggingface.co/spaces/nazneen/seal.", "published": "2022-10-11 23:51:44", "link": "http://arxiv.org/abs/2210.05839v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "PromptEHR: Conditional Electronic Healthcare Records Generation with\n  Prompt Learning", "abstract": "Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is\nchallenging due to privacy concerns, which hinders the use of ML for healthcare\napplications. Synthetic EHRs generation bypasses the need to share sensitive\nreal patient records. However, existing methods generate single-modal EHRs by\nunconditional generation or by longitudinal inference, which falls short of low\nflexibility and makes unrealistic EHRs. In this work, we propose to formulate\nEHRs generation as a text-to-text translation task by language models (LMs),\nwhich suffices to highly flexible event imputation during generation. We also\ndesign prompt learning to control the generation conditioned by numerical and\ncategorical demographic features. We evaluate synthetic EHRs quality by two\nperplexity measures accounting for their longitudinal pattern (longitudinal\nimputation perplexity, lpl) and the connections cross modalities\n(cross-modality imputation perplexity, mpl). Moreover, we utilize two\nadversaries: membership and attribute inference attacks for privacy-preserving\nevaluation. Experiments on MIMIC-III data demonstrate the superiority of our\nmethods on realistic EHRs generation (53.1\\% decrease of lpl and 45.3\\%\ndecrease of mpl on average compared to the best baselines) with low privacy\nrisks. Software is available at https://github.com/RyanWangZf/PromptEHR.", "published": "2022-10-11 14:48:15", "link": "http://arxiv.org/abs/2211.01761v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reflection of Thought: Inversely Eliciting Numerical Reasoning in\n  Language Models via Solving Linear Systems", "abstract": "Numerical reasoning over natural language has been a long-standing goal for\nthe research community. However, cutting-edge language models have proven\ndifficult to reliably generalize to a broad range of numbers, although they\nhave shown proficiency in reasoning over common and simple numbers. In this\npaper, we propose a novel method to elicit and exploit the numerical reasoning\nknowledge hidden in pre-trained language models using simple anchor numbers.\nConcretely, we first leverage simple numbers as anchors to probe the implicitly\ninferred arithmetic expressions from language models, and then explicitly apply\nthe expressions on complex numbers to get corresponding answers. To inversely\nelicit arithmetic expressions, we transform and formulate the task as an\nanalytically solvable linear system. Experimental results on several numerical\nreasoning benchmarks demonstrate that our approach significantly improves\nnumerical reasoning capabilities of existing LMs. More importantly, our\napproach is training-free and simply works in the inference phase, making it\nhighly portable and achieving consistent performance benefits across a variety\nof language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and\nfine-tuning scenarios.", "published": "2022-10-11 00:57:19", "link": "http://arxiv.org/abs/2210.05075v1", "categories": ["cs.CL", "cs.IR", "cs.NA", "math.NA"], "primary_category": "cs.CL"}
{"title": "Checks and Strategies for Enabling Code-Switched Machine Translation", "abstract": "Code-switching is a common phenomenon among multilingual speakers, where\nalternation between two or more languages occurs within the context of a single\nconversation. While multilingual humans can seamlessly switch back and forth\nbetween languages, multilingual neural machine translation (NMT) models are not\nrobust to such sudden changes in input. This work explores multilingual NMT\nmodels' ability to handle code-switched text. First, we propose checks to\nmeasure switching capability. Second, we investigate simple and effective data\naugmentation methods that can enhance an NMT model's ability to support\ncode-switching. Finally, by using a glass-box analysis of attention modules, we\ndemonstrate the effectiveness of these methods in improving robustness.", "published": "2022-10-11 02:25:21", "link": "http://arxiv.org/abs/2210.05096v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Markup-to-Image Diffusion Models with Scheduled Sampling", "abstract": "Building on recent advances in image generation, we present a fully\ndata-driven approach to rendering markup into images. The approach is based on\ndiffusion models, which parameterize the distribution of data using a sequence\nof denoising operations on top of a Gaussian noise distribution. We view the\ndiffusion denoising process as a sequential decision making process, and show\nthat it exhibits compounding errors similar to exposure bias issues in\nimitation learning problems. To mitigate these issues, we adapt the scheduled\nsampling algorithm to diffusion training. We conduct experiments on four markup\ndatasets: mathematical formulas (LaTeX), table layouts (HTML), sheet music\n(LilyPond), and molecular images (SMILES). These experiments each verify the\neffectiveness of the diffusion process and the use of scheduled sampling to fix\ngeneration issues. These results also show that the markup-to-image task\npresents a useful controlled compositional setting for diagnosing and analyzing\ngenerative image models.", "published": "2022-10-11 04:56:12", "link": "http://arxiv.org/abs/2210.05147v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Mixed-modality Representation Learning and Pre-training for Joint\n  Table-and-Text Retrieval in OpenQA", "abstract": "Retrieving evidences from tabular and textual resources is essential for\nopen-domain question answering (OpenQA), which provides more comprehensive\ninformation. However, training an effective dense table-text retriever is\ndifficult due to the challenges of table-text discrepancy and data sparsity\nproblem. To address the above challenges, we introduce an optimized OpenQA\nTable-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences.\nFirstly, we propose to enhance mixed-modality representation learning via two\nmechanisms: modality-enhanced representation and mixed-modality negative\nsampling strategy. Secondly, to alleviate data sparsity problem and enhance the\ngeneral retrieval ability, we conduct retrieval-centric mixed-modality\nsynthetic pre-training. Experimental results demonstrate that OTTeR\nsubstantially improves the performance of table-and-text retrieval on the\nOTT-QA dataset. Comprehensive analyses examine the effectiveness of all the\nproposed mechanisms. Besides, equipped with OTTeR, our OpenQA system achieves\nthe state-of-the-art result on the downstream QA task, with 10.1% absolute\nimprovement in terms of the exact match over the previous best system.\n  All the code and data are available at\nhttps://github.com/Jun-jie-Huang/OTTeR.", "published": "2022-10-11 07:04:39", "link": "http://arxiv.org/abs/2210.05197v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CTC Alignments Improve Autoregressive Translation", "abstract": "Connectionist Temporal Classification (CTC) is a widely used approach for\nautomatic speech recognition (ASR) that performs conditionally independent\nmonotonic alignment. However for translation, CTC exhibits clear limitations\ndue to the contextual and non-monotonic nature of the task and thus lags behind\nattentional decoder approaches in terms of translation quality. In this work,\nwe argue that CTC does in fact make sense for translation if applied in a joint\nCTC/attention framework wherein CTC's core properties can counteract several\nkey weaknesses of pure-attention models during training and decoding. To\nvalidate this conjecture, we modify the Hybrid CTC/Attention model originally\nproposed for ASR to support text-to-text translation (MT) and speech-to-text\ntranslation (ST). Our proposed joint CTC/attention models outperform\npure-attention baselines across six benchmark translation tasks.", "published": "2022-10-11 07:13:50", "link": "http://arxiv.org/abs/2210.05200v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rethinking the Event Coding Pipeline with Prompt Entailment", "abstract": "For monitoring crises, political events are extracted from the news. The\nlarge amount of unstructured full-text event descriptions makes a case-by-case\nanalysis unmanageable, particularly for low-resource humanitarian aid\norganizations. This creates a demand to classify events into event types, a\ntask referred to as event coding. Typically, domain experts craft an event type\nontology, annotators label a large dataset and technical experts develop a\nsupervised coding system. In this work, we propose PR-ENT, a new event coding\napproach that is more flexible and resource-efficient, while maintaining\ncompetitive accuracy: first, we extend an event description such as \"Military\ninjured two civilians'' by a template, e.g. \"People were [Z]\" and prompt a\npre-trained (cloze) language model to fill the slot Z. Second, we select answer\ncandidates Z* = {\"injured'', \"hurt\"...} by treating the event description as\npremise and the filled templates as hypothesis in a textual entailment task.\nThis allows domain experts to draft the codebook directly as labeled prompts\nand interpretable answer candidates. This human-in-the-loop process is guided\nby our interactive codebook design tool. We evaluate PR-ENT in several\nrobustness checks: perturbing the event description and prompt template,\nrestricting the vocabulary and removing contextual information.", "published": "2022-10-11 08:38:48", "link": "http://arxiv.org/abs/2210.05257v2", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Use of Semantically-Aligned Speech Representations for Spoken\n  Language Understanding", "abstract": "In this paper we examine the use of semantically-aligned speech\nrepresentations for end-to-end spoken language understanding (SLU). We employ\nthe recently-introduced SAMU-XLSR model, which is designed to generate a single\nembedding that captures the semantics at the utterance level, semantically\naligned across different languages. This model combines the acoustic\nframe-level speech representation learning model (XLS-R) with the Language\nAgnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the\nSAMU-XLSR model instead of the initial XLS-R model improves significantly the\nperformance in the framework of end-to-end SLU. Finally, we present the\nbenefits of using this model towards language portability in SLU.", "published": "2022-10-11 09:40:34", "link": "http://arxiv.org/abs/2210.05291v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model", "abstract": "Multimodal semantic understanding often has to deal with uncertainty, which\nmeans the obtained messages tend to refer to multiple targets. Such uncertainty\nis problematic for our interpretation, including inter- and intra-modal\nuncertainty. Little effort has studied the modeling of this uncertainty,\nparticularly in pre-training on unlabeled datasets and fine-tuning in\ntask-specific downstream datasets. In this paper, we project the\nrepresentations of all modalities as probabilistic distributions via a\nProbability Distribution Encoder (PDE) by utilizing sequence-level\ninteractions. Compared to the existing deterministic methods, such uncertainty\nmodeling can convey richer multimodal semantic information and more complex\nrelationships. Furthermore, we integrate uncertainty modeling with popular\npre-training frameworks and propose suitable pre-training tasks:\nDistribution-based Vision-Language Contrastive learning (D-VLC),\nDistribution-based Masked Language Modeling (D-MLM), and Distribution-based\nImage-Text Matching (D-ITM). The fine-tuned models are applied to challenging\ndownstream tasks, including image-text retrieval, visual question answering,\nvisual reasoning, and visual entailment, and achieve state-of-the-art results.", "published": "2022-10-11 10:54:54", "link": "http://arxiv.org/abs/2210.05335v3", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "MiDe22: An Annotated Multi-Event Tweet Dataset for Misinformation\n  Detection", "abstract": "The rapid dissemination of misinformation through online social networks\nposes a pressing issue with harmful consequences jeopardizing human health,\npublic safety, democracy, and the economy; therefore, urgent action is required\nto address this problem. In this study, we construct a new human-annotated\ndataset, called MiDe22, having 5,284 English and 5,064 Turkish tweets with\ntheir misinformation labels for several recent events between 2020 and 2022,\nincluding the Russia-Ukraine war, COVID-19 pandemic, and Refugees. The dataset\nincludes user engagements with the tweets in terms of likes, replies, retweets,\nand quotes. We also provide a detailed data analysis with descriptive\nstatistics and the experimental results of a benchmark evaluation for\nmisinformation detection.", "published": "2022-10-11 12:25:26", "link": "http://arxiv.org/abs/2210.05401v2", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "COVID-19-related Nepali Tweets Classification in a Low Resource Setting", "abstract": "Billions of people across the globe have been using social media platforms in\ntheir local languages to voice their opinions about the various topics related\nto the COVID-19 pandemic. Several organizations, including the World Health\nOrganization, have developed automated social media analysis tools that\nclassify COVID-19-related tweets into various topics. However, these tools that\nhelp combat the pandemic are limited to very few languages, making several\ncountries unable to take their benefit. While multi-lingual or low-resource\nlanguage-specific tools are being developed, they still need to expand their\ncoverage, such as for the Nepali language. In this paper, we identify the eight\nmost common COVID-19 discussion topics among the Twitter community using the\nNepali language, set up an online platform to automatically gather Nepali\ntweets containing the COVID-19-related keywords, classify the tweets into the\neight topics, and visualize the results across the period in a web-based\ndashboard. We compare the performance of two state-of-the-art multi-lingual\nlanguage models for Nepali tweet classification, one generic (mBERT) and the\nother Nepali language family-specific model (MuRIL). Our results show that the\nmodels' relative performance depends on the data size, with MuRIL doing better\nfor a larger dataset. The annotated data, models, and the web-based dashboard\nare open-sourced at https://github.com/naamiinepal/covid-tweet-classification.", "published": "2022-10-11 13:08:37", "link": "http://arxiv.org/abs/2210.05425v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Training of Language Models for Few-Shot Learning", "abstract": "Recent work on applying large language models (LMs) achieves impressive\nperformance in many NLP applications. Adapting or posttraining an LM using an\nunlabeled domain corpus can produce even better performance for end-tasks in\nthe domain. This paper proposes the problem of continually extending an LM by\nincrementally post-train the LM with a sequence of unlabeled domain corpora to\nexpand its knowledge without forgetting its previous skills. The goal is to\nimprove the few-shot end-task learning in these domains. The resulting system\nis called CPT (Continual PostTraining), which to our knowledge, is the first\ncontinual post-training system. Experimental results verify its effectiveness.", "published": "2022-10-11 15:43:58", "link": "http://arxiv.org/abs/2210.05549v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Entity Disambiguation with Entity Definitions", "abstract": "Local models have recently attained astounding performances in Entity\nDisambiguation (ED), with generative and extractive formulations being the most\npromising research directions. However, previous works limited their studies to\nusing, as the textual representation of each candidate, only its Wikipedia\ntitle. Although certainly effective, this strategy presents a few critical\nissues, especially when titles are not sufficiently informative or\ndistinguishable from one another. In this paper, we address this limitation and\ninvestigate to what extent more expressive textual representations can mitigate\nit. We thoroughly evaluate our approach against standard benchmarks in ED and\nfind extractive formulations to be particularly well-suited to these\nrepresentations: we report a new state of the art on 2 out of 6 benchmarks we\nconsider and strongly improve the generalization capability over unseen\npatterns. We release our code, data and model checkpoints at\nhttps://github.com/SapienzaNLP/extend.", "published": "2022-10-11 17:46:28", "link": "http://arxiv.org/abs/2210.05648v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers generalize differently from information stored in context\n  vs in weights", "abstract": "Transformer models can use two fundamentally different kinds of information:\ninformation stored in weights during training, and information provided\n``in-context'' at inference time. In this work, we show that transformers\nexhibit different inductive biases in how they represent and generalize from\nthe information in these two sources. In particular, we characterize whether\nthey generalize via parsimonious rules (rule-based generalization) or via\ndirect comparison with observed examples (exemplar-based generalization). This\nis of important practical consequence, as it informs whether to encode\ninformation in weights or in context, depending on how we want models to use\nthat information. In transformers trained on controlled stimuli, we find that\ngeneralization from weights is more rule-based whereas generalization from\ncontext is largely exemplar-based. In contrast, we find that in transformers\npre-trained on natural language, in-context learning is significantly\nrule-based, with larger models showing more rule-basedness. We hypothesise that\nrule-based generalization from in-context information might be an emergent\nconsequence of large-scale training on language, which has sparse rule-like\nstructure. Using controlled stimuli, we verify that transformers pretrained on\ndata containing sparse rule-like structure exhibit more rule-based\ngeneralization.", "published": "2022-10-11 09:29:19", "link": "http://arxiv.org/abs/2210.05675v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoupled Context Processing for Context Augmented Language Modeling", "abstract": "Language models can be augmented with a context retriever to incorporate\nknowledge from large external databases. By leveraging retrieved context, the\nneural network does not have to memorize the massive amount of world knowledge\nwithin its internal parameters, leading to better parameter efficiency,\ninterpretability and modularity. In this paper we examined a simple yet\neffective architecture for incorporating external context into language models\nbased on decoupled Encoder Decoder architecture. We showed that such a simple\narchitecture achieves competitive results on auto-regressive language modeling\nand open domain question answering tasks. We also analyzed the behavior of the\nproposed model which performs grounded context transfer. Finally we discussed\nthe computational implications of such retrieval augmented models.", "published": "2022-10-11 20:05:09", "link": "http://arxiv.org/abs/2210.05758v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Applying FrameNet to Chinese(Poetry)", "abstract": "FrameNet( Fillmore and Baker [2009] ) is well-known for its wide use for\nknowledge representation in the form of inheritance-based ontologies and\nlexica( Trott et al. [2020] ). Although FrameNet is usually applied to\nlanguages like English, Spanish and Italian, there are still plenty of FrameNet\ndata sets available for other languages like Chinese, which differs\nsignificantly from those languages based on Latin alphabets. In this paper, the\ntranslation from ancient Chinese Poetry to modern Chinese will be first\nconducted to further apply the Chinese FrameNet(CFN, provided by Shanxi\nUniversity). Afterwards, the translation from modern Chinese will be conducted\nas well for the comparison between the applications of CFN and English\nFrameNet. Finally, the overall comparison will be draw between CFN to modern\nChinese and English FrameNet.", "published": "2022-10-11 20:28:20", "link": "http://arxiv.org/abs/2210.05772v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Scaling Up Deliberation for Multilingual ASR", "abstract": "Multilingual end-to-end automatic speech recognition models are attractive\ndue to its simplicity in training and deployment. Recent work on large-scale\ntraining of such models has shown promising results compared to monolingual\nmodels. However, the work often focuses on multilingual models themselves in a\nsingle-pass setup. In this work, we investigate second-pass deliberation for\nmultilingual speech recognition. Our proposed deliberation is multilingual,\ni.e., the text encoder encodes hypothesis text from multiple languages, and the\ndecoder attends to multilingual text and audio. We investigate scaling the\ndeliberation text encoder and decoder, and compare scaling the deliberation\ndecoder and the first-pass cascaded encoder. We show that deliberation improves\nthe average WER on 9 languages by 4% relative compared to the single-pass\nmodel. By increasing the size of the deliberation up to 1B parameters, the\naverage WER improvement increases to 9%, with up to 14% for certain languages.\nOur deliberation rescorer is based on transformer layers and can be\nparallelized during rescoring.", "published": "2022-10-11 21:07:00", "link": "http://arxiv.org/abs/2210.05785v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Transfer Learning with Joint Fine-Tuning for Multimodal Sentiment\n  Analysis", "abstract": "Most existing methods focus on sentiment analysis of textual data. However,\nrecently there has been a massive use of images and videos on social platforms,\nmotivating sentiment analysis from other modalities. Current studies show that\nexploring other modalities (e.g., images) increases sentiment analysis\nperformance. State-of-the-art multimodal models, such as CLIP and VisualBERT,\nare pre-trained on datasets with the text paired with images. Although the\nresults obtained by these models are promising, pre-training and sentiment\nanalysis fine-tuning tasks of these models are computationally expensive. This\npaper introduces a transfer learning approach using joint fine-tuning for\nsentiment analysis. Our proposal achieved competitive results using a more\nstraightforward alternative fine-tuning strategy that leverages different\npre-trained unimodal models and efficiently combines them in a multimodal\nspace. Moreover, our proposal allows flexibility when incorporating any\npre-trained model for texts and images during the joint fine-tuning stage,\nbeing especially interesting for sentiment classification in low-resource\nscenarios.", "published": "2022-10-11 21:16:14", "link": "http://arxiv.org/abs/2210.05790v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Comparison of Soft and Hard Target RNN-T Distillation for Large-scale\n  ASR", "abstract": "Knowledge distillation is an effective machine learning technique to transfer\nknowledge from a teacher model to a smaller student model, especially with\nunlabeled data. In this paper, we focus on knowledge distillation for the RNN-T\nmodel, which is widely used in state-of-the-art (SoTA) automatic speech\nrecognition (ASR). Specifically, we compared using soft and hard target\ndistillation to train large-scaleRNN-T models on the LibriSpeech/LibriLight\npublic dataset (60k hours) and our in-house data (600k hours). We found that\nhard tar-gets are more effective when the teacher and student have different\narchitecture, such as large teacher and small streaming student. On the other\nhand, soft target distillation works better in self-training scenario like\niterative large teacher training. For a large model with0.6B weights, we\nachieve a new SoTA word error rate (WER) on LibriSpeech (8% relative\nimprovement on dev-other) using Noisy Student Training with soft target\ndistillation. It also allows our production teacher to adapt new data domain\ncontinuously.", "published": "2022-10-11 21:32:34", "link": "http://arxiv.org/abs/2210.05793v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Designing Robust Transformers using Robust Kernel Density Estimation", "abstract": "Recent advances in Transformer architectures have empowered their empirical\nsuccess in a variety of tasks across different domains. However, existing works\nmainly focus on predictive accuracy and computational cost, without considering\nother practical issues, such as robustness to contaminated samples. Recent work\nby Nguyen et al., (2022) has shown that the self-attention mechanism, which is\nthe center of the Transformer architecture, can be viewed as a non-parametric\nestimator based on kernel density estimation (KDE). This motivates us to\nleverage a set of robust kernel density estimation methods for alleviating the\nissue of data contamination. Specifically, we introduce a series of\nself-attention mechanisms that can be incorporated into different Transformer\narchitectures and discuss the special properties of each method. We then\nperform extensive empirical studies on language modeling and image\nclassification tasks. Our methods demonstrate robust performance in multiple\nscenarios while maintaining competitive results on clean datasets.", "published": "2022-10-11 21:39:52", "link": "http://arxiv.org/abs/2210.05794v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Social-Group-Agnostic Word Embedding Debiasing via the Stereotype\n  Content Model", "abstract": "Existing word embedding debiasing methods require social-group-specific word\npairs (e.g., \"man\"-\"woman\") for each social attribute (e.g., gender), which\ncannot be used to mitigate bias for other social groups, making these methods\nimpractical or costly to incorporate understudied social groups in debiasing.\nWe propose that the Stereotype Content Model (SCM), a theoretical framework\ndeveloped in social psychology for understanding the content of stereotypes,\nwhich structures stereotype content along two psychological dimensions -\n\"warmth\" and \"competence\" - can help debiasing efforts to become\nsocial-group-agnostic by capturing the underlying connection between bias and\nstereotypes. Using only pairs of terms for warmth (e.g., \"genuine\"-\"fake\") and\ncompetence (e.g.,\"smart\"-\"stupid\"), we perform debiasing with established\nmethods and find that, across gender, race, and age, SCM-based debiasing\nperforms comparably to group-specific debiasing", "published": "2022-10-11 23:26:23", "link": "http://arxiv.org/abs/2210.05831v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Language Maps for Robot Navigation", "abstract": "Grounding language to the visual observations of a navigating agent can be\nperformed using off-the-shelf visual-language models pretrained on\nInternet-scale data (e.g., image captions). While this is useful for matching\nimages to natural language descriptions of object goals, it remains disjoint\nfrom the process of mapping the environment, so that it lacks the spatial\nprecision of classic geometric maps. To address this problem, we propose\nVLMaps, a spatial map representation that directly fuses pretrained\nvisual-language features with a 3D reconstruction of the physical world. VLMaps\ncan be autonomously built from video feed on robots using standard exploration\napproaches and enables natural language indexing of the map without additional\nlabeled data. Specifically, when combined with large language models (LLMs),\nVLMaps can be used to (i) translate natural language commands into a sequence\nof open-vocabulary navigation goals (which, beyond prior work, can be spatial\nby construction, e.g., \"in between the sofa and TV\" or \"three meters to the\nright of the chair\") directly localized in the map, and (ii) can be shared\namong multiple robots with different embodiments to generate new obstacle maps\non-the-fly (by using a list of obstacle categories). Extensive experiments\ncarried out in simulated and real world environments show that VLMaps enable\nnavigation according to more complex language instructions than existing\nmethods. Videos are available at https://vlmaps.github.io.", "published": "2022-10-11 18:13:20", "link": "http://arxiv.org/abs/2210.05714v4", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "The DKU-Tencent System for the VoxCeleb Speaker Recognition Challenge\n  2022", "abstract": "This paper is the system description of the DKU-Tencent System for the\nVoxCeleb Speaker Recognition Challenge 2022 (VoxSRC22). In this challenge, we\nfocus on track1 and track3. For track1, multiple backbone networks are adopted\nto extract frame-level features. Since track1 focus on the cross-age scenarios,\nwe adopt the cross-age trials and perform QMF to calibrate score. The\nmagnitude-based quality measures achieve a large improvement. For track3, the\nsemi-supervised domain adaptation task, the pseudo label method is adopted to\nmake domain adaptation. Considering the noise labels in clustering, the ArcFace\nis replaced by Sub-center ArcFace. The final submission achieves 0.107 mDCF in\ntask1 and 7.135% EER in task3.", "published": "2022-10-11 02:09:40", "link": "http://arxiv.org/abs/2210.05092v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MFCCA:Multi-Frame Cross-Channel attention for multi-speaker ASR in\n  Multi-party meeting scenario", "abstract": "Recently cross-channel attention, which better leverages multi-channel\nsignals from microphone array, has shown promising results in the multi-party\nmeeting scenario. Cross-channel attention focuses on either learning global\ncorrelations between sequences of different channels or exploiting fine-grained\nchannel-wise information effectively at each time step. Considering the delay\nof microphone array receiving sound, we propose a multi-frame cross-channel\nattention, which models cross-channel information between adjacent frames to\nexploit the complementarity of both frame-wise and channel-wise knowledge.\nBesides, we also propose a multi-layer convolutional mechanism to fuse the\nmulti-channel output and a channel masking strategy to combat the channel\nnumber mismatch problem between training and inference. Experiments on the\nAliMeeting, a real-world corpus, reveal that our proposed model outperforms\nsingle-channel model by 31.7\\% and 37.0\\% CER reduction on Eval and Test sets.\nMoreover, with comparable model parameters and training data, our proposed\nmodel achieves a new SOTA performance on the AliMeeting corpus, as compared\nwith the top ranking systems in the ICASSP2022 M2MeT challenge, a recently held\nmulti-channel multi-speaker ASR challenge.", "published": "2022-10-11 08:54:17", "link": "http://arxiv.org/abs/2210.05265v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ConchShell: A Generative Adversarial Networks that Turns Pictures into\n  Piano Music", "abstract": "We present ConchShell, a multi-modal generative adversarial framework that\ntakes pictures as input to the network and generates piano music samples that\nmatch the picture context. Inspired by I3D, we introduce a novel image feature\nrepresentation method: time-convolutional neural network (TCNN), which is used\nto forge features for images in the temporal dimension. Although our image data\nconsists of only six categories, our proposed framework will be innovative and\ncommercially meaningful. The project will provide technical ideas for work such\nas 3D game voice overs, short-video soundtracks, and real-time generation of\nmetaverse background music.We have also released a new dataset, the\nBeach-Ocean-Piano Dataset (BOPD) 1, which contains more than 3,000 images and\nmore than 1,500 piano pieces. This dataset will support multimodal\nimage-to-music research.", "published": "2022-10-11 01:04:39", "link": "http://arxiv.org/abs/2210.05076v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiffRoll: Diffusion-based Generative Music Transcription with\n  Unsupervised Pretraining Capability", "abstract": "In this paper we propose a novel generative approach, DiffRoll, to tackle\nautomatic music transcription (AMT). Instead of treating AMT as a\ndiscriminative task in which the model is trained to convert spectrograms into\npiano rolls, we think of it as a conditional generative task where we train our\nmodel to generate realistic looking piano rolls from pure Gaussian noise\nconditioned on spectrograms. This new AMT formulation enables DiffRoll to\ntranscribe, generate and even inpaint music. Due to the classifier-free nature,\nDiffRoll is also able to be trained on unpaired datasets where only piano rolls\nare available. Our experiments show that DiffRoll outperforms its\ndiscriminative counterpart by 19 percentage points (ppt.) and our ablation\nstudies also indicate that it outperforms similar existing methods by 4.8 ppt.\n  Source code and demonstration are available https://sony.github.io/DiffRoll/.", "published": "2022-10-11 05:02:11", "link": "http://arxiv.org/abs/2210.05148v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Spectro-temporal Artifacts for Detecting Synthesized Speech", "abstract": "The Audio Deep Synthesis Detection (ADD) Challenge has been held to detect\ngenerated human-like speech. With our submitted system, this paper provides an\noverall assessment of track 1 (Low-quality Fake Audio Detection) and track 2\n(Partially Fake Audio Detection). In this paper, spectro-temporal artifacts\nwere detected using raw temporal signals, spectral features, as well as deep\nembedding features. To address track 1, low-quality data augmentation, domain\nadaptation via finetuning, and various complementary feature information fusion\nwere aggregated in our system. Furthermore, we analyzed the clustering\ncharacteristics of subsystems with different features by visualization method\nand explained the effectiveness of our proposed greedy fusion strategy. As for\ntrack 2, frame transition and smoothing were detected using self-supervised\nlearning structure to capture the manipulation of PF attacks in the time\ndomain. We ranked 4th and 5th in track 1 and track 2, respectively.", "published": "2022-10-11 08:31:30", "link": "http://arxiv.org/abs/2210.05254v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GAN You Hear Me? Reclaiming Unconditional Speech Synthesis from\n  Diffusion Models", "abstract": "We propose AudioStyleGAN (ASGAN), a new generative adversarial network (GAN)\nfor unconditional speech synthesis. As in the StyleGAN family of image\nsynthesis models, ASGAN maps sampled noise to a disentangled latent vector\nwhich is then mapped to a sequence of audio features so that signal aliasing is\nsuppressed at every layer. To successfully train ASGAN, we introduce a number\nof new techniques, including a modification to adaptive discriminator\naugmentation to probabilistically skip discriminator updates. ASGAN achieves\nstate-of-the-art results in unconditional speech synthesis on the Google Speech\nCommands dataset. It is also substantially faster than the top-performing\ndiffusion models. Through a design that encourages disentanglement, ASGAN is\nable to perform voice conversion and speech editing without being explicitly\ntrained to do so. ASGAN demonstrates that GANs are still highly competitive\nwith diffusion models. Code, models, samples:\nhttps://github.com/RF5/simple-asgan/.", "published": "2022-10-11 09:12:29", "link": "http://arxiv.org/abs/2210.05271v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Experimental Study on Private Aggregation of Teacher Ensemble\n  Learning for End-to-End Speech Recognition", "abstract": "Differential privacy (DP) is one data protection avenue to safeguard user\ninformation used for training deep models by imposing noisy distortion on\nprivacy data. Such a noise perturbation often results in a severe performance\ndegradation in automatic speech recognition (ASR) in order to meet a privacy\nbudget $\\varepsilon$. Private aggregation of teacher ensemble (PATE) utilizes\nensemble probabilities to improve ASR accuracy when dealing with the noise\neffects controlled by small values of $\\varepsilon$. We extend PATE learning to\nwork with dynamic patterns, namely speech utterances, and perform a first\nexperimental demonstration that it prevents acoustic data leakage in ASR\ntraining. We evaluate three end-to-end deep models, including LAS, hybrid\nCTC/attention, and RNN transducer, on the open-source LibriSpeech and TIMIT\ncorpora. PATE learning-enhanced ASR models outperform the benchmark DP-SGD\nmechanisms, especially under strict DP budgets, giving relative word error rate\nreductions between 26.2% and 27.5% for an RNN transducer model evaluated with\nLibriSpeech. We also introduce a DP-preserving ASR solution for pretraining on\npublic speech corpora.", "published": "2022-10-11 16:55:54", "link": "http://arxiv.org/abs/2210.05614v2", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Inner speech recognition through electroencephalographic signals", "abstract": "This work focuses on inner speech recognition starting from EEG signals.\nInner speech recognition is defined as the internalized process in which the\nperson thinks in pure meanings, generally associated with an auditory imagery\nof own inner \"voice\". The decoding of the EEG into text should be understood as\nthe classification of a limited number of words (commands) or the presence of\nphonemes (units of sound that make up words). Speech-related BCIs provide\neffective vocal communication strategies for controlling devices through speech\ncommands interpreted from brain signals, improving the quality of life of\npeople who have lost the capability to speak, by restoring communication with\ntheir environment. Two public inner speech datasets are analysed. Using this\ndata, some classification models are studied and implemented starting from\nbasic methods such as Support Vector Machines, to ensemble methods such as the\neXtreme Gradient Boosting classifier up to the use of neural networks such as\nLong Short Term Memory (LSTM) and Bidirectional Long Short Term Memory\n(BiLSTM). With the LSTM and BiLSTM models, generally not used in the literature\nof inner speech recognition, results in line with or superior to those present\nin the stateof-the-art are obtained.", "published": "2022-10-11 08:29:12", "link": "http://arxiv.org/abs/2210.06472v1", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.HC"}
